       Grammatical Bracketing Determines Learning of Non-adjacent Dependencies
    Felix Hao Wang (wang970@usc.edu)a, Jason Zevin (zevin@usc.edu)a,b,c, Toby Mintz (tmintz@usc.edu)a,b,c
          a
            Department of Psychology, University of Southern California, 3620 McClintock Ave, Los Angeles, CA, 90089
                      b
                        Department of Linguistics, University of Southern California, Los Angeles, CA, 90089
                       c
                         Program in Neuroscience, University of Southern California, Los Angeles, CA, 90089
                            Abstract                               on non-adjacent dependency learning to date have only
                                                                   found evidence in limited situations, with some reporting
  Grammatical dependencies often involve elements that
  are not adjacent. However, most experiments in which             success in learning and others reporting failure. One
  non-adjacent dependencies are learned bracketed the              consistent characteristic of experiments that showed
  dependent material with pauses, which is not how                 successful learning is that the minimal sequences that
  dependencies appear in natural language. Here we                 contained a dependency were presented as discrete chunks.
  report successful learning of embedded NAD without               In other words, the chunks were surrounded by silences, and
  pause bracketing. Instead, we induce learners to                 the edges of such a chunk consisted of the (non-adjacent)
  compute structure in an artificial language by entraining
  them through processing English sentences. We also               dependent elements. For example, studies that have probed
  found that learning becomes difficult when grammatical           non-adjacent dependency learning between words in
  entrainment causes learners to compute boundaries that           artificial languages typically have used trigrams in which the
  are misaligned with NAD structures. In sum, we                   dependent words were at the trigram edges, and subjects
  demonstrated that grammatical entrainment can induce             were presented the trigrams one at a time, with silence
  boundaries that can carry over to reveal structures in           intervening between presentations (Gómez, 2002; Romberg
  novel language materials, and this effect can be used to         & Saffran, 2013). Similarly, in experiments investigating
  induce learning of non-adjacent dependencies.
                                                                   non-adjacent dependencies between syllables in syllable
   Keywords:       non-adjacent     dependency     learning;       sequences, learning occurred only when brief pauses were
   grammatical entrainment                                         introduced before (and after) each syllable trigram (Peña et
                                                                   al.,2002). When syllables were concatenated continuously,
                         Introduction                              participants showed no learning (see also Newport & Aslin,
Due to the hierarchical organization of the syntax of natural      2004). In the studies just discussed, the fact that subjects’
languages, lexical items (and morphemes) that are                  success in learning non-adjacent dependencies was
syntactically related are not always linearly adjacent. Thus,      correlated with whether the trigrams containing the
to acquire the specifics of the hierarchical grammar, learners     dependency were pre-segmented suggests that the chunked
must be able to track dependencies of linguistic items that        presentation might have played an important role in learning.
are both linearly adjacent and non-adjacent. For example,          One reason in which pre-segmenting the material in this way
given the dependency between the singular subject child and        could be helpful is that it places one or both dependent
the agreeing inflected verb runs, the subject and the verb are     elements in an edge position. Indeed, Endress, Nespor &
adjacent in the child runs, and non-adjacent in the child          Mehler (2009) argued that edges are privileged in the kind of
always runs. Therefore, an important question in language          position-related computations they afford, and placement at
acquisition is how learners acquire adjacent and non-              edges could be an important constraint for learning non-
adjacent grammatical dependencies, as they could help              adjacent dependencies.
learners understand how their language is structured. There           But in natural languages, non-adjacent dependencies are
has been considerable interest in investigating learning           not restricted to positions of silences (e.g., utterance
mechanisms that could detect these dependencies in linear          boundaries), and are often embedded in longer sequences.
sequences within spoken utterances. These mechanisms               Learning the dependency relations of a natural language
could be useful for discovering syntactic structures when          generally require learning non-adjacent dependencies of
children start to acquire their first language, and facilitate     items that may not always occur at silence boundaries. This
building syntactic parses later in life. For example, many         may pose a problem for the learning theories mentioned
studies employing artificial and natural languages have            above. One possibility is that the non-adjacent dependency
investigated how language learners acquire non-adjacent            learning with spoken language critically requires the
dependencies (e.g., Gómez, 2002; Peña, Bonatti, Nespor &           presence of pauses as a prosodic cue. According to this
Mehler, 2002; Newport & Aslin, 2004; Romberg & Saffran,            interpretation (see Peña et al. 2002, for a discussion),
2013), and how early in the acquisition process such               successful learning requires an interaction between prosody
dependencies are detected (Santelmann & Jusczyk, 1998;             and syntactic analysis. However, it is also possible that the
Gómez, 2002).                                                      edges that the learning mechanism requires are broader in
  Whereas most studies on adjacent dependencies have               scope, including boundaries in representations rather than
found success in a variety of learning paradigms, the studies      directly in the signal (pauses being the latter). For example,
                                                               2561

edges or boundaries that are represented as the result of a        For English words, we recorded 5 names (Brian, John, Kate,
syntactic parse might play a similar role to pauses. Under         Nate, Clair), 5 monosyllabic verbs in 3rd person singular
this interpretation, prosody per se might not be critical for      form (turns, keeps, puts, lets, has), 5 pronouns (these, those,
learning non-adjacent dependencies. But rather, the                this, that, it) and 5 adverbs (down, on, up, off, in). For the
availability of any kind of boundary, in the signal or             non-adjacent dependency, we used 12 novel words: 3 at
computed, could facilitate the detection of non-adjacent           position 1 (pel, tink, blit), 3 at position 2 (swech, voy, rud), 3
dependencies when the boundaries are in close alignment            at position 3 (dap, tood, wesh) and 3 at position 4 (ghire, jub,
with them.                                                         tiv).
   In order to distinguish these hypotheses, we sought a way          After all the words were recorded in list intonation in a
to induce edges and bracketing via syntactic analysis without      random order, we spliced the words from the recording.
resorting to pauses. To this end, we made use of a recently        Each word by itself from the recording lasted between
published phenomenon described as grammatical                      300ms to 737ms, and we used the lengthen function in Praat
entrainment. When sentences with the same syntactic                (Boersma, 2001) to shorten all the words into approximately
structures are presented repeatedly in a cyclical pattern,         250ms. An additional 83ms of silence was added to the end
MEG recordings identified cortical regions which track the         of each word to increase intelligibility and such that when
syntactic structures and compute syntactic boundaries when         words are concatenated in a continuous stream, they would
structures repeat (Ding, Melloni, Zhang, Tian & Poeppel,           occur at 3Hz. This was not intended to be a manipulation,
2015). One of the proposed mechanisms is that the repeated         and is certainly not the same as the design from Peña, et al.,
presentation of the same syntactic structure makes it possible     2002 because the pauses do not pre-segment phrases into
to predict the syntax of the sentence that comes next.             four-syllable chunks that line up with the dependency. The
   We reasoned that such entrainment of structural                 pauses are after every word, making it uninformative of the
processing might carry over to an artificial language that is      dependency structure.
presented immediately following cyclic presentation of
structurally similar English sentences presented as in Ding et     Design and procedure. There were three blocks of training
al. (2015). If this process induces syntactic edges in a           phase and testing phase, with each testing phase following a
predictable, cyclic way, it could potentially allow the same       training phase. The training materials and testing materials
analysis to be applied and transferred to new linguistic           were divided into 3 equal proportions for the 3 blocks.
materials that are presented also in the same frequency
(words-per-second) and phase as the English material. That         Training phase. To create the training stream, we mixed
is, perhaps repeated structure building of familiar material       English sentences and artificial sentences together. A total of
(i.e., English) can be used to guide upcoming parsing of           432 novel word sentences and 858 English sentences were
novel material (i.e., and artificial language). To this end, we    randomly concatenated together in the following fashion.
placed novel artificial language after repetitions of the same     Each English sentence was created by randomly picking a
English syntactic structure across a variety of English            name, a verb, a pronoun, and an adverb, in that order. As
sentences, matched in word length (4 words). At the very           such, each sentence consisted of 4 syllables (with the
least, we expected that the English sentences would entrain        exception of sentences containing the word Brian), and
listeners to segment into 4-word sequences, effectively            lasted 1.33 seconds. Since words were randomly selected
inducing virtual boundaries in the novel artificial language.      and constrained only by position, there was no statistical
Specifically, we predicted that when English grammatical           dependency on the level of words.
dependencies are repeated in phase with the artificial                Each novel sentence was a concatenation of 4 novel
language dependencies, they would facilitate detection of the      words, 1 each from choices of 3 for each position, as
artificial non-adjacent dependencies (Experiment 1). We            specified in the Stimuli section. We represent this pattern as
further predicted that when the phase relationship between         YAXB, with one letter for each class of novel word. The
English and artificial language was mismatched, the                second position word predicted the fourth position word
facilitation effect would disappear (Experiment 2).                (YAiXBi), so the fourth word is predictable from the second
                                                                   word. All the other words (at positions 1, 2 and 3) cannot be
                       Experiment 1                                predicted. Given that there are 3 different words for each
                                                                   non-dependent position (1, 2, & 3), there were 27 possible
Methods                                                            different quadruplet artificial sentences.
Participants. Twenty-four undergraduate students at                   The training stream was made by concatenating
University of Southern California were recruited from              alternating English and artificial language sentences in the
Psychology Department subject pool. Half of them                   following way. We concatenated 5, 6 or 7 English sentences
participated in each counterbalancing condition. The sample        together to create the entrainment effect. After the English
size was based on previous studies (Newport & Aslin 2004).         sentences, three artificial language sentences were presented,
                                                                   followed by more English sentences (see Figure 1 for a
Stimuli. We recorded speech from a native English speaker          demonstration). There are no additional pauses between
and digitized the recording at a rate of 44.1 kHz. We              English sentences, between novel words of artificial
recorded 2 types of words: English words and novel words.
                                                                2562

language, or the boundary between English words or novel              ratings were the dependent variable, where the interaction
words.                                                                and main effects of item type (correct vs. incorrect) and
                                                                      block number (1 through 3) was entered in the fixed effect
                                                                      with subject as the random effect. The interaction was found
                                                                      to be significant (chi2(2) = 8.37, p=0.0152). For this
                                                                      interaction, the regression revealed that the difference score
                                                                      between correct items and incorrect items in block 2 was not
                                                                      significantly different from block 1 (β=0.019, z=0.13,
                                                                      p=0.895) but block 3 is (β=-0.343, z=-2.44, p= 0.015). We
                                                                      conclude that the difference of ratings for correct vs.
    Figure 1. The design of language materials in the
                                                                      incorrect items was found to be significant. A plot of the
    training phase of Experiment 1. The English sentence
                                                                      data and comparisons can be found in Figure 2.
    and the artificial dependencies are in phase. If the third
    sentence is [pel swech dap ghire] and the following
    sentence is [blit rud dap tiv], learners may learn the
    dependency between swech and ghire (as well as rud &                          Experiment 1 Result
    tiv).
                                                                        2.5
   A counterbalancing condition was created such that the
                                                                        2.3
ungrammatical strings that occurred in the test are
grammatical in the training sequence in the counterbalancing            2.1
condition, similar to Gómez (2002). There were three Ai_Bi                                                                Incorrect
frames (i.e., Ai_Bi, where i=1-3). The two counterbalancing             1.9
                                                                                                                          Correct
languages were created by taking three pairs of A_B for the
three frames in one training language (A1_B1, A2_B2,                    1.7
A3_B3), and three different pairs for the other training
language (A1_B2, A2_B3, A3_B1). This design allowed the                 1.5
use of the same set of test items for the two counterbalancing                   Block 1     Block 2     Block 3
training languages, where the set included both frames from
the two training languages.                                              Figure 2. Ratings of test items by block for correct and
   Participants listened to the sound stream passively through           incorrect items. Smaller rating indicate acceptance of the
the headphones while the screen was blank. The listening                 item. Error bars represent 95% confidence intervals
part lasted about 9 minutes per block.                                   around the mean.
Test phase. Immediately after each training block, we
                                                                      Discussion
showed instructions for the test phase on the screen. The
instruction made it clear that participants would hear sound          In Experiment 1, we introduced a window of analysis from
sequences and make judgment about the sequences. There                English that participants can use to break into the artificial
were a total of 18 test trials per block, half of which were          language. Given the syntactic structures in English, the
from the correct dependency, and the other half from                  bracketing of artificial language can be induced such that
incorrect ones. The sequence of presenting the test trials was        structural analysis can be applied to artificial language
randomized for each participant.                                      sequences. Applying the boundary every 4 words into the
   Participants initiated each test trial. Per trial, we played an    artificial language, we get the chunks [Kate keeps that off]
artificial language sentence, and asked the participant to            [YAiXBi] [YAjXBj], which facilitates dependency learning.
indicate whether some sequences are from the previous                    It’s worth characterizing the way we constructed the
section that they have heard. A scale showed up after                 training material that produced grammatical entrainment.
playing the sentence and participants were asked to answer            The English sentences we used did not contain any acoustic
the question “Do you think that you heard this sequence in            information regarding its structure unless the knowledge of
the previous section?” There were five possible items to              English is applied. The construction was such that all the
choose from, “Definitely”, “Maybe”, “Not Sure”, “Maybe                English sentences had constant transitional probability
Not”, “Definitely Not”. Participants could click on any of            between each word, and the words between each sentences,
the choices, and the trial ended and next trial began.                given that the English sentences were made from a random
                                                                      Markov process. That is, each English word just predicts a
                            Results                                   word from a set of the next words with equal probability. If
We coded the scale of “Definitely”, “Maybe”, “Not Sure”,              no knowledge of English is present, words come randomly
“Maybe Not” and “Definitely Not” into numeric values of 1             without informative landmarks. Without English grammar,
through 5. To compare ratings statistically, we ran mixed             listening to the English words would not result in any kind
effect linear regressions with the data. In the regression,           of boundaries that can be used for processing of later
                                                                   2563

artificial language. The fact that our participants were native    3-4); rather, it would restart from the last word of the
speakers of English made grammatical entrainment possible,         quadruplet, making use of the name in the current
which facilitated the detection of non-adjacent dependencies.      quadruplet, and continuing to the next quadruplet, making a
   There are some alternative accounts to our current              sentence out of the current 4th word and the first 3 from the
proposal that listeners transferred syntactic boundaries from      next (4-1-2-3; see Figure 3 for a demonstration). The
English to the artificial language. One possibility is that the    purpose of this deliberate phase shift is to change where the
length of the English sentences (4 words) encouraged               syntactic boundaries come in the parsing process, which will
listeners to process the artificial language in smaller chunks,    become important for the artificial language parsing later.
and that was sufficient to ease processing and facilitate          Similar as in Experiment 1, there is no statistical dependency
detection of non-adjacent dependencies. In other words, the        on the level of words, given that any one of the 4 words can
alignment of the chunk boundaries with the non-adjacent            appear at its position. Dependency can only be defined
patterns may have been irrelevant, and all that was necessary      grammatically, given listener’s knowledge of English and
was processing shorter sequences. A related possibility is         the parsing of the sentences.
that presenting the same syntactic structures induced                 The novel sentences were exactly the same as in
syntactic priming (Bock 1986), whereby the repeated                Experiment 1, and the pattern can be similarly represented as
presentation of the same syntactic structure makes it easier       YAiXBi. Since the phase of the English sentences was such
to reuse/reactivate structures of the same type. In our case,      that the first position and the third position contain the
the fact that we present one syntactic structure (verb + prep.     dependency (verb and verb particle), the dependency in the
+ verb particle) over and over again may have sensitized           artificial language is out of phase with respect to the English
participants to dependency lengths of three if syntactic           sentences, and instead is aligned with the Y_X structures,
priming is operating. Furthermore, given that the boundaries       which are independent.
induced from English align with the switch from English to
artificial words, there might be boundaries arising from this
shift. These boundaries may simply serve as a starting
counter for the novel word sequences, from which edge-
based computations can be performed. We rule out these
alternatives in Experiment 2.
                       Experiment 2
Methods                                                               Figure 3. The design of language materials in the training
                                                                      phase of Experiment 2. If the third sentence is [Kate pel
Participants. Twenty-four undergraduate students at                   swech dap] and the following sentence is [ghire blit rud
University of Southern California were recruited from                 dap], learners fail to learn the dependency between swech
Psychology Department subject pool. Half of them                      and ghire.
participated in each counterbalancing condition. None of the
participants participated in Experiment 1.                            Similarly to Experiment 1, a counterbalancing condition
                                                                   was created such that the ungrammatical strings that
Stimuli. We used the same stimuli from Experiment 1.               occurred in the test are grammatical in the training sequence
                                                                   in the counterbalancing condition.
Design and procedure. Similar to Experiment 1, there were             The language stream started with a name, making the first
three blocks of training phase and testing phase, with each        English sentence grammatical. Participants listened to the
testing phase following a training phase. The training             alternating English and artificial language sentence stream
materials and testing materials were divided into 3 equal          passively through the headphones while the screen was
proportions for the 3 blocks, and the only difference between      blank.
Experiment 2 and Experiment 1 is the training materials            Test phase. The Test phase is exactly the same as
(described below).                                                 Experiment 1, with 3 blocks, each block containing the same
                                                                   number of correct and incorrect items to be rated.
Training phase. The training phase is similar to Experiment
1 except for one key difference in how the English words
                                                                   Results
were ordered. We mixed a total of 432 novel word sentences
and 858 English sentences were randomly concatenated               We coded the scale of “Definitely”, “Maybe”, “Not Sure”,
together in the same fashion as in Experiment 1, except for        “Maybe Not” and “Definitely Not” into numeric values of 1
the order of the English words. Each English ‘sentence’ was        through 5. To compare ratings statistically, we ran mixed
created by randomly picking a verb, a pronoun, an adverb,          effect linear regressions with the data. In the regression,
and a name, in that order. Given such ordering of English          ratings were the dependent variable, where the interaction
words, parsing of these sentences described above will not         and main effects of item type (correct vs. incorrect) and
start with the beginning to the end of each quadruplets (1-2-      block number (1 through 3) was entered in the fixed effect
                                                                   with subject as the random intercept. The interaction was
                                                                2564

found to be not significant (chi2(2) = 2.50, p= 0.287). For            Rather our interpretation is that the English sentences
this interaction, the regression revealed that the difference       provided edges in specific locations that were sufficiently
score between correct items and incorrect items in block 2          aligned with the dependencies in the artificial language. If
was not significantly different from block 1 (β=-0.157, z=          English parsing carried over to parsing the artificial
-1.16, p=0.244), nor is in block 3 (β=0.046, z= 0.34, p=            language, even at the coarsest ‘sentence’ level, the
0.732). Dropping the fixed effect of block, I rerun the             dependencies in Experiment 2 would have been chunked as
regression with only item type as the fixed effect with             [Kate YAiX] [BiYAjX]…, and the AiXBi dependency would
subjects as random effects. Again, there is no evidence for         never have been considered, and never be learned as a result.
learning, as correct items were not rated differently from the      We argue that the reason for failure to learn the dependency
incorrect items (β=-0.078, z=-1.42, p=0.154). We conclude           is a result of bracketing elements across boundaries.
that there is no significant difference in ratings for correct      Experiment 2 contrasts with Experiment 1 in that the lack of
vs. incorrect items. We plot the data in Figure 4.                  facilitation effect given the mismatch with phase support the
                                                                    presence of grammatical entrainment, given all other factors
                                                                    (acoustic, transitional probability) are controlled for.
           Experiment 2 Result
                                                                                        General Discussion
   2.5                                                              In this paper, we report our first attempt at inducing non-
   2.3                                                              adjacent dependency learning with grammatical bracketing.
                                                                    As we mentioned, non-adjacent dependencies are generally
   2.1                                                              hard to learn for a variety of reasons. For the most part, past
                                                   Incorrect
                                                                    literature suggested (Newport & Aslin, 2004; Peña et al.,
   1.9
                                                   Correct          2002) that pauses are critical to the learning of syllable-level
   1.7                                                              non-adjacent dependencies. Our design does not contain
                                                                    pauses, which makes our study the first we know of that
   1.5                                                              addresses the issue of handling the window of analysis in
           Block 1      Block 2     Block 3                         non-adjacent dependency with language learning without
                                                                    resorting to pauses. We show that this hard problem of
  Figure 4. Ratings of test items by block for correct and          learning of non-adjacent dependency can be solved when the
  incorrect items. Smaller rating indicate acceptance of the        non-adjacent dependency is entrained to an English rhythm
  item. Error bars represent 95% confidence intervals               that provided syntactic edges to the novel artificial language
  around the mean.                                                  in such a way that the edges line up with the dependencies to
                                                                    be learned. Furthermore, we show that when the syntactic
Discussion                                                          edges slice the novel language into chunks that do not
In Experiment 2, the phase of the English sentences was             contain the intended dependency, it leads to failure of
mismatched to the phase of the artificial language                  learning.
dependency whereby the non-adjacent dependency was not                 Existing theories posits that perceptual or memory
within each chunk of the segmented material. Participants in        primitives guide aspects of statistical based learning, and
Experiment 2 failed to learn the artificial language                more specifically, that edge-based computations are
dependency.                                                         critically required for computing non-adjacent dependencies
  We can eliminate some of the possibilities mentioned in           (Peña et al., 2002). However, this theory left the description
the discussion of Experiment 1. First, similar to Experiment        of edges rather vague. In the studies cited here on non-
1, Experiment 2 also used English sentences that were 4             adjacent dependencies from Endress and colleagues, the
words long. If this helps the artificial language parse into 4-     implementation of an edge has been a period of silence. As
word chunks (as we argue it does, see below), this factor           we mentioned, this points to a possibility that the non-
alone is not sufficient to support successful detection.            adjacent dependency learning mechanisms critically require
Similarly, a syntactic priming account would not entirely           the presence of pauses as a prosodic cue, whereby an
account for the result, given that the same verb to verb            interaction between prosody and syntactic analysis.
particle dependency exists in Experiment 2 as well. We              However, our data suggest that this may not be the case. One
cannot eliminate the potential influence of syntactic priming,      of the advantages of an edge-based computation is that it
given that we presented the same syntactic structure (verb +        shortens the sentence length that the computations are
prep. + verb particle) over and over again, but again, it is not    performed on. This has great implication for how long-
enough to facilitate detection. Lastly, if there were               distance dependencies can be detected from a computational
boundaries that were due to changes of English words to             perspective. Linguistic dependency can exist between any
artificial words, these boundaries are still present in             element within a sentence to any other element the number
Experiment 2. Experiment 2 thus suggests that the success           of dependencies grows factorially with the number of
of learning in Experiment 1 is not a result merely of               elements within a sentence (between-sentence dependencies
computing boundaries in sequences of novel words.                   also exist in language, which is not considered here). If
                                                                 2565

boundaries are only at utterance boundaries, detection of             There are many future directions to this preliminary work.
long-distance dependencies may quickly become intractable          As we just discussed, it is possible that entrainment and
as the length of sentence increases (see Wang & Mintz,             transfer involved the internal syntactic dependency.
under review, for a demonstration).                                Moreover, it is conceivable the syntactic categories have
   It follows that the learning mechanism for non-adjacent         carried over to the artificial words. There could be multiple
dependencies may require edges. Yet non-adjacent                   reasons this can happen. Given the cyclical nature of the
dependencies are not limited to start or end of sentences in       syntactic assignment of English words, the syntactic
natural languages, and as such, the learning mechanism in          assignment can potentially carry over. That is to say,
natural language for effective detection of non-adjacent           different artificial words may become more verb like or
dependencies may resort to edges beyond utterance                  pronoun like, depending on its position and the non-adjacent
boundaries or silences. To solve this problem, we                  dependency may facilitate syntactic categorization (Mintz,
demonstrate that non-adjacent dependencies are learnable           Wang & Li, 2014). Another direction to go is to examine
when the boundaries in a syntactic sense brackets the              how syntactic bracketing happens in artificial language in
continuous speech stream into chunks that contain the              general. Does the presence of adjacent and non-adjacent
dependency. Our study shows that prosody by itself is not          dependencies alone induce bracketing? What kind of
critical for learning non-adjacent dependencies. But rather,       mechanism is involved in deducing chunks from statistical
computing structural boundaries at the beginning and end of        information?
chunks that contain the dependency also facilitate their              In sum, we have argued that correct bracketing is crucial
detection. In this sense, the pauses are serving the same          to learn about elements within a chunk. We propose that
function as the syntactic boundaries here, perhaps restricting     thinking about bracketing is a useful way of puzzle solving
the window of analysis the detection mechanism operate             around learning linguistic dependencies, and that having a
within. However, characterizing that the learning mechanism        correct window of analysis is crucial for such purposes.
requires some kind of prosodic processes such as pauses
would be an under-specification.                                                            References
   We can speculate about the role of the English sentences
                                                                   Bock, J. K. (1986). Syntactic persistence in language
in Experiment 1 further. One possibility is that the English
                                                                      production. Cognitive psychology, 18(3), 355-387.
sentence simply sets a pace at every 4 word, bracketing
                                                                   Boersma, Paul (2001). Praat, a system for doing phonetics
artificial language to 4-word chunks. Potentially, having a
                                                                      by computer. Glot International (2001): 341-345.
chunk close to the size of dependency is enough for learning
                                                                   Ding, N., Melloni, L., Zhang, H., Tian, X., & Poeppel, D.
to happen, though given the low variability in the middle
                                                                      (2015). Cortical tracking of hierarchical linguistic
element, learning would occur slowly. Existing theories
                                                                      structures in connected speech. Nature neuroscience.
(Gomez, 2002, among others) suggest that the dependency is
                                                                   Endress, A. D., Nespor, M., & Mehler, J. (2009). Perceptual
hard to detect without highly variable middle elements. In
                                                                      and memory constraints on language acquisition. Trends
our design, the variability of the middle elements (n=3) is
                                                                      in cognitive sciences, 13(8), 348-353.
very low according to Gomez 2002, making the dependency
                                                                   Gómez, R. L. (2002). Variability and detection of invariant
hard to learn. Alternatively, processing the English sentences
                                                                      structure. Psychological Science, 13(5), 431-436.
result in a hierarchical parse, such that the verb and the verb
                                                                   Mintz, T. H., Wang, F. H., & Li, J. (2014). Word
particle have a syntactic non-adjacent dependency (e.g., puts
                                                                      categorization from distributional information: Frames
… on). If this parse transfers to new linguistic material, it
                                                                      confer more than the sum of their (Bigram) parts.
would imply that the detection of non-adjacent dependency
                                                                      Cognitive psychology, 75, 1-27.
is facilitated by this narrow window of in-phase pattern
                                                                   Newport, E.L., & Aslin, R.N. (2004). Learning at a distance:
matching. Learning is a result of many factors, and future
                                                                      I. Statistical learning of non-adjacent dependencies.
research will separate these possibilities in detail. Although
                                                                      Cognitive Psychology, 48, 127-162.
this is an intriguing possibility, from our data we cannot
                                                                   Peña, M., Bonatti, L. L., Nespor, M., & Mehler, J. (2002).
determine if the internal dependency is playing a role, as its
                                                                      Signal-driven computations in speech processing. Science,
alignment to the artificial dependency is confounded with
                                                                      298(5593), 604-607.
the alignment of the of the sentence boundary and the
                                                                   Romberg, A. R., & Saffran, J. R. (2013). All together now:
artificial dependency.
                                                                      Concurrent learning of multiple structures in an artificial
   We recognize that various aspects of our design are
                                                                      language. Cognitive Science, 37(7), 1290-1320.
artificial, especially in terms of the temporal nature of the
                                                                   Santelmann, L. M., & Jusczyk, P. W. (1998). Sensitivity to
English and artificial language material. The goal was to use
                                                                      discontinuous dependencies in language learners:
the rhythmic properties as a tool to stimulate syntactic
                                                                      Evidence for limitations in processing space. Cognition,
transfer from processing of known structures to novel ones.
                                                                      69(2), 105-134.
At one level, this could be viewed as a kind of syntactic
                                                                   Wang, F. H, & Mintz, T. M. (under review). Learning Non-
bootstrapping, either at a coarse grain level (the sentence
                                                                      Adjacent Dependencies Embedded in Sentences of an
level), or fine grain level (phrase level), where prior
                                                                      Artificial Language: When Learning Breaks Down.
structures organize the interpretation of novel material.
                                                                2566

