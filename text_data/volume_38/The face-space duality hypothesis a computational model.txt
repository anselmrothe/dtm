                      The face-space duality hypothesis: a computational model
                                     Jonathan Vitale (jonathan.vitale@student.uts.edu.au)
                                    Mary-Anne Williams (mary-anne.williams@uts.edu.au)
                                      Benjamin Johnston (benjamin.johnston@uts.edu.au)
                                                     University of Technology, Sydney
                     QCIS Centre - Innovation and Enterprise Research Lab, 15 Broadway - Ultimo NSW 2007
                               Abstract                                       Can Valentine’s framework support an integral understand-
   Valentine’s face-space suggests that faces are represented in a         ing of identity and expression processing? Calder and col-
   psychological multidimensional space according to their per-            leagues (2001) demonstrated that a multidimensional space
   ceived properties. However, the proposed framework was ini-             derived from a principal component analysis (PCA) (Turk
   tially designed as an account of invariant facial features only,
   and explanations for dynamic features representation were ne-           & Pentland, 1991) can provide a set of components be-
   glected. In this paper we propose, develop and evaluate a com-          ing either identity-independent, expression-independent or
   putational model for a twofold structure of the face-space, able        identity-expression-interdependent (Calder & Young, 2005).
   to unify both identity and expression representations in a single
   implemented model. To capture both invariant and dynamic                However, in order to perform identity and expression recog-
   facial features we introduce the face-space duality hypothesis          nition, the authors used two distinct latent discriminant anal-
   and subsequently validate it through a mathematical presen-             ysis (LDA) modules, one choosing the best components to
   tation using a general approach to dimensionality reduction.
   Two experiments with real facial images show that the pro-              support identity recognition, whereas the other choosing the
   posed face-space: (1) supports both identity and expression             best components to support expression classification.
   recognition, and (2) has a twofold structure anticipated by our            In this paper, we show that a single face-space with a
   formal argument.
                                                                           twofold representation supports both identity and expression
   Keywords: face perception; face processing; face-space; du-
   ality hypothesis; dimensionality reduction                              recognition. The structure of this face-space can be realized
                                                                           in a parsimonious way by integrating both identity and ex-
                           Introduction                                    pression in a single model. We demonstrate the computa-
As an explanation of findings in face perception, Valentine                tional validity of our hypothesis through a rigorous mathe-
used formal models of concept representations to propose that              matical presentation and related experiments. This work fur-
faces are represented in a psychologically plausible multidi-              ther supports the integral nature of identity and expression, at
mensional space, i.e. the face-space (Valentine, 1991). Faces              least from a computational perspective.
are points of this space based on their perceived properties.
Valentine’s formal models have been used to explain results                              Findings in Face Perception
of many human experimental studies, as well as computa-                    Valentine’s ‘face-space’ framework (Valentine, 1991) is a no-
tional simulations (Calder, Burton, Miller, Young, & Aka-                  table cognitive model for face representation. According to
matsu, 2001; Lee, Byatt, & Rhodes, 2000; Rhodes et al.,                    this framework, facial representations are encoded in a mul-
2011).                                                                     tidimensional psychological space. The dimensions of this
   These models were initially designed to only account for                space are assumed to encode properties of the facial sig-
coding identity-related features, such as sex, distinctiveness,            nals that better discriminate one face from another. The dis-
age and attractiveness (Calder et al., 2001). For example,                 tance between two representations underlies their dissimilar-
the feature ‘eyebrows’ can vary from marked to delicate, thus              ity from a psychological perspective.
possibly being one of the perceivable features crucial for cod-               Identity and expression are two forms of facial information
ing the sex of a face. However, dynamic aspects of faces,                  crucial for many social skills. Identity is considered an invari-
such as facial expressions, were neglected. Early brain lesion             ant feature of face, whereas expression a dynamic one. Tra-
and neuroimaging studies suggested that face identity and ex-              ditional cognitive models of face perception suggest a com-
pression are not integral dimensions1 , instead they are repre-            plete separation of identity and expression systems after the
sented and processed by separate systems that process faces                completion of a structural encoding stage (Bruce & Young,
in parallel (Bruyer et al., 1983; Tranel, Damasio, & Dama-                 1986). Accordingly, Haxby et al. (2000) argue that invariant
sio, 1988). However, contemporary understanding indicates                  features, such as identity, and dynamic features, such as facial
that identity and expression are more closely connected than               expression, are computed by separate regions of the brain.
previously thought, suggesting that common-codings can re-                    However, new evidence from recent findings indicates that
spond to both of them and so processes of their perception                 these systems operate interdependently (Pell & Richards,
can interact (Ganel, Valyear, Goshen-Gottstein, & Goodale,                 2013). For example, Ganel and Goshen-Gottstein (2004)
2005; Kadosh et al., 2016; Rhodes et al., 2015).                           found that familiarity of faces increases the perceptual in-
    1 Here and for the rest of the paper we use the term ‘integral’ to     terdependence of identity and expression recognition. They
define properties exhibiting inter-dependencies and not completely         suggested that differences between the facial configurations
separable.                                                                 of individuals should lead to systematic differences in the
                                                                       514

way emotions are expressed by these individuals. For this             functions CE (·) → R and CI (·) → R, respectively provid-
reason, every individual can express each facial expression           ing the number of correctly classified facial expressions and
in a unique way. Knowledge of the identity of the observed            identities from a set of observations encoding faces, and a
subject can therefore facilitate the process of his or her facial     permutation function σ over the coordinates (y1 , y2 , . . . , yd )
expression.                                                           of a point y defined as:
    We found a similar effect in developing a computational                              1
                                                                                                  y2     y3    . . . yd
                                                                                                                              
model inspired by simulation-theories (Vitale, Williams,                                    y
                                                                                    σ=                                                   (1)
Johnston, & Boccignone, 2014). In this endeavour, we sug-                                   yd yd−1 yd−2 . . . y1
gested to first pre-process the observed face so to reduce its
identity-related information, while preserving motor compo-           We make use, here and for the rest of the paper, of the super-
nents information (i.e. its dynamic features). This facilitates       script ˜· to denote a point or set of points to which is applied
the recognition of facial expression based on feed-forward            the permutation in (1). Then, given a set of perceived obser-
mechanisms of internal motor simulation. A similar account            vations Φ and the associated multidimensional spatial repre-
was recently supported by a human study of Ipser and Cook             sentations Y , the mapping function S is defined such that:
(2015).                                                              1. S(Φ) 7→ Y ;
    Calder et al. (2001) demonstrated the validity of inte-
gral identity and expression representations from a compu-           2. CE (Y )  CE (Φ);
tational perspective. They submitted digital images of faces
showing different identities and facial expressions to PCA           3. CI (Ỹ )  CI (Φ);
thus obtaining representations based on components of a low-
                                                                         In other words, the face-space duality hypothesis assumes
dimensional space. Their results demonstrated that this com-
                                                                      a function S mapping the perceived faces onto a psychologi-
mon representation can support both identity and expression
                                                                      cal multidimensional space having a twofold structure. This
recognition and that the representations of identity and ex-
                                                                      new representation allows the encodings Y = {y1 , y2 , . . . , yn }
pression partially overlap.
                                                                      and associated permutations Ỹ = {ỹ1 , ỹ2 , . . . , ỹn } to support
          The Face-Space Duality Hypothesis                           significantly higher recognition rates than the original input
                                                                      representation Φ.
In the previous section we provided studies supporting:                  The rationale behind this idea is that the dual face-space,
                                                                      by maximising the separation between dynamic and invari-
   i A multidimensional spatial representation of faces as a
                                                                      ant features of the face in a single multidimensional repre-
     plausible model for explaining many crucial effects in
                                                                      sentation, will order the components of the resulting space
     face perception;
                                                                      in such a way that the first ones will mostly correlate with
  ii An interdependence between representations of invariant          dynamic features of the face, whereas the latter will mostly
     and dynamic facial features;                                     correlate with invariant features of the face. Therefore, the
                                                                      resulting face-space would provide a single multidimensional
 iii That enhancing the separation between invariant and dy-          representation (as per point (i)) where invariant and dynamic
     namic components of the face during face processing fa-          features of the face are interdependent (as per point (ii)), but
     cilitates their classification.                                  preserving a certain degree of separation able to support sub-
                                                                      sequent classification processes (as per point (iii)).
    Given this brief summary, we introduce the ‘face-space du-           We investigate our hypothesis from a computational per-
ality hypothesis’, suggesting that faces: (i) are encoded in          spective, validating it using a mathematical analysis of a gen-
a multidimensional face-space, (ii) under a common integral           eral dimensionality reduction framework used in face recog-
representation (iii) having a twofold structure: one support-         nition, by limiting the analysis to facial identity and expres-
ing invariant features of the face (e.g. identity), whereas the       sion only. We further confirm our approach with experimental
other supporting dynamic features of the face (e.g. expres-           results.
sion), thus contributing to their correct classification.
    This hypothesis arises from the need to accommodate the                      Dimensionality Reduction Models
apparently contrasting points (ii) and (iii) under a single rep-      The function S introduced above can be modeled as a di-
resentation based on a multidimensional space (i), which is an        mensionality reduction function. A dimensionality reduction
extension of the face-space framework proposed by Valentine           function maps a high-dimensional signal onto a point of a
(1991).                                                               low-dimensional space. For example, consider an image of a
    Consider a perceived face φi and a corresponding d-               face having resolution 100 × 100 pixels. This observed signal
dimensional point yi of a multidimensional psychological              is represented by a set of pixels and can be posed as a column
space. The spatial representation yi encodes most of the orig-        vector φi of dimension D = 10000. Dimensionality reduc-
inal information of the input face φi and can be obtained             tion models provide a mapping function S : RD ×1 → Rd×1 ,
through the mapping function S(φi ) 7→ yi . We introduce the          with d  D, such that the low-dimensional representation
                                                                  515

yi = S(φi ) is able to explain the observed data φi (Yan et al.,         estimated. Finally, the overall mapping matrix able to reduce
2007).                                                                   the dimensionality from dimension D to dimension d and per-
   Linear dimensionality reduction techniques make use of a              forming the constraints specified in the objective function (2)
linear projection matrix V ∈ RD ×d in order to map the high              is given by:
dimensional observed sample onto the low-dimensional tar-                                        Voverall = VPCAV ?                    (3)
get space. So the projection yi of an observation φi can be
computed as yi = V > φi . When V is an orthogonal matrix,                                  Model implementation
an approximation of the original observation can be recon-               Consider a set Φ of N observations of frontal faces. Each
structed from its projection: φi ≈ V yi . The projection matrix          observation consists of D pixel values, represented by a D ×1
V can be estimated by solving an objective function. This ob-            vector. In this work we limit the investigation of observations
jective function models desired constraints that the structure           varying only in identity and facial expression.
of the target low-dimensional space is required to satisfy.                 We set a dimension d < N  D and we estimate a map-
   For the purposes of this paper we limit our investigation             ping matrix VPCA ∈ RD ×d by submitting the samples Φ to a
to provide an implementation of our model through linear                 PCA, thus obtaining the corresponding PCA-encodings X =
dimensionality reduction techniques.                                       > Φ. We aim to estimate another mapping matrix V ? ∈
                                                                         VPCA
Graph-based dimensionality reduction framework                           Rd×d , such that the final overall matrix Voverall = VPCAV ? val-
                                                                         idates our hypothesis.
Yan et al. (2007) provided a general framework for unifying                 We denote the identity class of the sample xi with I (xi )
many dimensionality reductions models. Since our approach                and the facial expression class of the sample xi with E (xi ).
makes use of this framework, we first briefly introduce it be-              Considering the previously introduced scenario, we start
low.                                                                     by designing the appropriate similarity and penalty matrices.
   Let Φ = [φi , . . . , φn ] be a matrix of the N observations rep-     Invariant features of the face extend to more regions of the
resented as column vectors with dimension D. The struc-                  face than dynamic ones, thus explaining most of the variance
ture of the target low-dimensional space can be constrained              in the considered dataset of observations (Turk & Pentland,
by a similarity matrix W and a penalty matrix W (p) . For                1991). This means that during facial expression classification
each pair of samples (φi , φ j ), the similarity matrix Wi j en-         the identity can potentially introduce a bias on the samples
codes the associated non-negative similarity measure, whilst             (the identity-bias), thus increasing their similarity and reduc-
                            (p)
the penalty matrix Wi j encodes the associated penalty mea-              ing their distance in the face-space even when they belong to
sure. This penalty measure can be used as a repulsive force              a different class of facial expression (Sariyanidi, Gunes, &
between pairs of samples to prevent samples with high simi-              Cavallaro, 2015).
larity but belonging to different classes from being placed in              Therefore, as a first step we design our similarity matrix
close proximity in the low-dimensional space (Kokiopoulou                to encourage pairs of samples associated with the same facial
& Saad, 2009).                                                           expression to be in close proximity in the resulting space, and
   Given these two graph structures, the optimal mapping ma-             our penalty matrix to provide a repulsive force between pairs
trix V ? can be found by solving the following objective func-           of samples belonging to the same identity. This would result
tion:                                                                    in maximising the separation between dynamic and invariant
                                                                         components of the face, thus facilitating the classification of
                                    Tr(V > ΦLΦ>V )
                V ? = arg min                                    (2)     facial expression.
                          V ∈RD ×d Tr(V > ΦL(p) Φ>V )                       Accordingly, we define the similarity matrix W E as:
with Tr(·) denoting the matrix trace operator and L, L(p) re-
                                                                                                      , if E (xi ) = E (x j )
                                                                                                (
                                                                                                    1
                                                                                            E
spectively being the resulting Laplacian matrices computed                               Wi j = nEi                                    (4)
from the similarity and penalty matrices W and W (p) (Yan et                                       0,     otherwise.
al., 2007).
   Unfortunately, there is no closed-form solution to this op-           where nEi is the number of samples in X belonging to facial
timization problem (Ngo, Bellalij, & Saad, 2012). However,               expression class E (xi ).
the problem can be solved numerically with iterative algo-                  Similarly, we define the penalty matrix W I as:
rithms whenever the matrix ΦL(p) Φ> is positive definite. The
                                                                                                      , if I (xi ) = I (x j )
                                                                                                (
                                                                                                    1
resulting optimal solution V ? is unique up to unitary trans-                             I
                                                                                        Wi j = nIi                                     (5)
forms of the columns (Ngo et al., 2012).                                                          0,      otherwise.
   To ensure that matrix ΦL(p) Φ> is positive definite, the pro-
cess is usually split into two phases. First, given a dimension          where nIi is the number of samples in X belonging to identity
D 0 < N, the observations Φ are provided in input to a PCA,              class I (xi ).
                                                0
and a first mapping matrix VPCA ∈ RD ×D is estimated. Then,                 By using the proposed similarity and penalty matrices,
                        > Φ are provided as input to the objective
the samples X = VPCA                                                     the resulting Laplacians becomes L = IN − W E and L(p) =
                                                              0
function in (2) and the optimal mapping matrix V ? ∈ RD ×d is            IN − W I , with IN a N × N identity matrix. Hence, these
                                                                     516

                                                                           lowing objective function:
                                                                                                  Tr(V > X(IN −W E )X >V )
                                                                                          arg max                                         (7)
                                                                                          V ∈Rd×d Tr(V > X(IN −W I )X >V )
Figure 1: Some examples of prototypes. On the left are two                    Using simple properties of trace and eigenvalues, it follows
prototypical identities (F05 and M07) in which expression-                 that the objective function in (7) can be equivalently posed as:
related features are reduced, whereas on the right are two ex-
amples of prototypical facial expressions (happiness and sur-                                     Tr(V > X(IN −W I )X >V )
                                                                                          arg min                                         (8)
prise).                                                                                   V ∈Rd×d Tr(V > X(IN −W E )X >V )
                                                                              By reminding that a centring matrix is symmetric idempo-
                                                                           tent and Tr(AA> ) = kAk22 , it is possible to note that the ob-
Laplacians behave as block centring matrices. These matrices               jective function (6) attempts in minimising the distances be-
remove respectively the corresponding prototypical facial ex-              tween the encodings Y = V > X and their corresponding proto-
pression (i.e. an average identity showing the averaged facial                                           E
                                                                           typical facial expressions Yproto  = V > XW E , while maximis-
expression) and the corresponding prototypical identity (i.e.              ing their distances with respect to the prototypical identity
the considered identity showing a neutral facial expression)                 I
                                                                           Yproto = V > XW I , overall facilitating expression recognition.
from samples X (see figure (1)).                                           Conversely, the objective functions (7, 8) attempts in min-
    The objective function in (2) becomes:                                 imising the distances between the encodings Y = V > X and
                                                                                                                          I
                                                                           their corresponding prototypical identities Yproto = V > XW I ,
                             Tr(V > X(IN −W E )X >V )                      while maximising their distances with respect to the proto-
                  arg min                                          (6)                                 E
                                                                                                            = V > XW E , overall facilitating
                  V ∈Rd×d    Tr(V > X(IN −W I )X >V )                      typical facial expression Yproto
                                                                           identity recognition.
                                                                              Since the eigenvectors of V ? are estimated from the same
    We solve the objective function in (6) with the iterative al-
                                                                           matrix G (ρ? ) = M E − ρ? M I in both the objective functions
gorithm proposed in (Ngo et al., 2012). Given the matrices
                                                                           (6,7), the components of the two spaces are the same, but
M E = X(IN − W E )X > and M I = X(IN − W I )X > the opti-
                                                                           differing by order. In other words, given V ? as the optimal
mal mapping matrix V ? can be found through the algorithm
                                                                           matrix resulting from objective function (6) we can easily get
(1).
                                                                           the optimal matrix of the objective function (7) Ṽ ? , defined
                                                                           as a matrix with the same columns of V ? , but arranged in an
    Data: Matrices M E , M I , a maximum number of                         inverse order.
            iterations K and a tolerance ε.                                   Finally, given the matrix VPCA ∈ RD ×d and the matrix
                                                                             ?
                                                                           V ∈ Rd×d we can estimate the final mapping matrix Voverall
    Result: A mapping matrix V of dimension D × d.
    V ← ID×d ;                                                             of the face-space through equation (3), which leads respec-
                                                                                                         >
                                                                           tively to the mapping Y = Voverall  X and the associated permu-
    for i ← 1 to K do
                                                                                                  >
                Tr(V > M E V )                                             tated mappings Ỹ = Ṽoverall X as suggested by our hypothesis.
        ρ←     Tr(V > M I V )
                               ;
                                                                              Note that we are not claiming that this is the only way to
        G (ρ) ← M E − ρM I ;                                               implement the proposed mapping function S (for example it
        Compute the smallest (for minimisation) or largest                 can be generalised to non linear models), and neither that hu-
        (for maximisation) d eigenvalues [λ1 , . . . , λd ] ≡ Λ of         man brain implement the suggested dual face-space in this
        G (ρ) and associated eigenvectors [v1 , . . . , vd ] ≡ V ;         way. However, this is a viable computational implementa-
        if | ∑dj=1 Λ| < ε then                                             tion of the proposed model able to support our hypothesis. In
              break;                                                       the remainder of this paper we further support the face-space
        end                                                                duality hypothesis with experimental data.
    end
                                                                                                   Experiments
  Algorithm 1: Newton-Lanczos algorithm for optimiza-
  tion of objective function (6).                                          We further validate our hypothesis using images from
                                                                           the Karolinska Directed Emotional Faces (KDEF) dataset
                                                                           (Lundqvist, Flykt, & Öhman, 1998). The dataset contains
    From the algorithm (1), it can be seen that the optimal map-           static images of 70 subjects—35 female and 35 male—
ping matrix V ? is the set of eigenvectors associated with the             exhibiting 7 different prototypical facial expressions of basic
smallest eigenvalues of G (ρ? ), with ρ? being the result of the           emotions (anger, disgust, fear, happiness, neutral, sadness and
trace ratio in (6) when posing the optimal solution V ? . If,              surprise). The pictures are taken in different face orientations
instead of taking the eigenvectors associated with the small-              and in two different sessions (A and B).
est eigenvalues, we take the eigenvectors associated with the                 We used the frontal pictures taken in session A. The fa-
largest eigenvalues, we get the optimal solution for the fol-              cial region was extracted from the images and its resolution
                                                                       517

         (a) Expression recognition task                (b) Identity recognition task                   (c) Components used in recognition tasks
                                             Figure 2: Results of the proposed experiments.
reduced to 80 × 80 pixels. Eyes and mouth were at approx-                 space. The recognition rates in each dimension were aver-
imately the same position. Illumination variations were re-               aged among each cross-validation test.
duced by applying a simple equalization process to the im-                   The results for facial expression and identity recognition
ages.                                                                     are shown, respectively, in figures (2a) and (2b). It is clear
    We first pre-processed the data by submitting the pixels of           that this face-space derived by a single integrated process can
the images in input to a PCA as explained previously. In the              support both identity and expression recognition, whereas a
first experiment we retained the components able to explain               simple PCA cannot overcome the baseline performance.
95% of the variance of the original data resulting in 200 com-
ponents, while in the second experiment we retained the data              Validation of face-space twofold structure
explaining the 85% (so reducing the number of components                  We were able to confirm our hypothesis on the twofold struc-
and allowing better readability) resulting in 100 components.             ture of the face-space using data.
    We performed two experiments: the first to test the ability              We estimated the mapping matrix Voverall as per equations
of the proposed face-space in supporting identity and expres-             (6, 3) using the full dataset as training data.
sion recognition, and the second to demonstrate the twofold                  Given the matrix Voverall = [v1 , . . . , vd ] the minimum set
nature of the resulting face-space.                                       of expression components for a sample φi is the smallest set
                                                                          Vimin = [v1 , . . . , vk ] such that yi = Vi>   φ is classified with the
                                                                                                                       min i
Support of identity and facial expression recognition                     correct expression label E (φi ) through nearest neighbour al-
In the first experiment we test the ability of our model to sup-          gorithm, as in the previous experiment.
port subsequent processes of identity and facial expression                  Similarly, given the matrix Ṽoverall = [vd , . . . , v1 ] the mini-
recognition. We used a 10-fold cross validation approach.                 mum set of identity components for a sample φi is the smallest
For each iteration we divided the data by taking one fold as              set Ṽimin = [vk , . . . , v1 ] such that yi = Ṽi>  φ is classified with
                                                                                                                            min i
test and the rest for model training.                                     the correct identity label I (φi ) through nearest neighbour al-
    With each training data we estimated the mapping matrix               gorithm, as in the previous experiment.
Voverall as per equations (6, 3). Then we mapped each test                   For each sample φi we computed its minimum set of ex-
data onto the corresponding face-space, thus obtaining the en-            pression and identity components. Then, we set nEk the num-
codings Y E = Voverall
                   >     Φ and Ỹ I = Ṽoverall
                                         >      Φ respectively used       ber of times the component k was included in the mini-
during the expression and identity recognition tasks.                     mum sets of expression components and nI                k the number of
    The classification was performed using the nearest neigh-             times the component k was included in the minimum sets
bour algorithm. For each sample φi , xi and yi we computed                of identity components. We computed the final results for
the Euclidean distances with respect to the centroids of each             expression and identity and for each component k as fkE =
                                                                               nE                                      nI
class in the corresponding space (i.e. the prototypical iden-             log( Nk × 100 + 1) and fkI = log( Nk × 100 + 1). Here N is
tities in the case of identity recognition or the prototypical            the number of samples in the dataset (i.e. 490) and we used
expressions in the case of facial expression recognition) and             the logarithm for better readability of the results. The result-
selected the label associated with the centroid having lower              ing log-frequencies are illustrated in Figure (2c).
distance to the sample. We repeated this process for each                    From the results two peaks placed in the extremes of the
dimension k by taking only the first k components of the en-              face-space components are clearly visible. There are ex-
codings xi and yi . In classifying the raw observations φi for            pression components clearly independent from identity ones
a baseline comparison, we considered all the pixel values of              (components #1 to #52), components shared among expres-
the input images as coordinates of points in a D-dimensional              sion and identity classification tasks (components #53 to #99)
                                                                      518

and just one identity component independent from expres-               Ipser, A., & Cook, R. (2015). Inducing a concurrent mo-
sion ones (component #100). These results are in agreement               tor load reduces categorization precision for facial expres-
with the study of Ganel and Goshen-Gottstein (2004), which               sions. Journal of Experimental Psychology Human Percep-
suggest that expression is perceptually separable from iden-             tion & Performance.
tity, but identity is not perceptually separable from expres-          Kadosh, K. C., Luo, Q., de Burca, C., Sokunbi, M. O., Feng,
sion. This experiment further supports the proposed twofold              J., Linden, D. E., & Lau, J. Y. (2016). Using real-time
structure of face-space as suggested by the face-space duality           fmri to influence effective connectivity in the developing
hypothesis.                                                              emotion regulation network. NeuroImage, 125, 616–626.
                                                                       Kokiopoulou, E., & Saad, Y. (2009). Enhanced graph-based
                         Conclusions                                     dimensionality reduction with repulsion laplaceans. Pat-
In this paper we extended Valentine’s face-space framework               tern Recognition, 42(11), 2392–2402.
to better explain recent findings in face perception.                  Lee, K., Byatt, G., & Rhodes, G. (2000). Caricature effects,
   In alignment with recent studies in face processing, we               distinctiveness, and identification: Testing the face-space
suggested that the nature of identity and expression dimen-              framework. Psychological Science, 11(5), 379–385.
sions is highly interdependent. We proposed that the structure         Lundqvist, D., Flykt, A., & Öhman, A. (1998). The karolin-
of the face-space can result from a single process integrating           ska directed emotional faces (kdef). CD ROM from De-
invariant and dynamic features of the face. From this process            partment of Clinical Neuroscience, Psychology section,
results a twofold structure. Reading the components of this              Karolinska Institutet, 91–630.
space from the first to the last facilitates classification of dy-     Ngo, T. T., Bellalij, M., & Saad, Y. (2012). The trace ratio
namic features of the face, such as facial expression, while             optimization problem. SIAM review, 54(3), 545–569.
reading them from the last to the first better supports classifi-      Pell, P. J., & Richards, A. (2013). Overlapping facial ex-
cation of invariant features, such as the identity.                      pression representations are identity-dependent. Vision re-
   We validated the face-space duality hypothesis, from a                search, 79, 1–7.
computational perspective, through a formal mathematical               Rhodes, G., Jaquet, E., Jeffery, L., Evangelista, E., Keane,
presentation, by considering a general framework of dimen-               J., & Calder, A. J. (2011). Sex-specific norms code face
sionality reduction. The hypothesis was further supported by             identity. Journal of Vision, 11(1), 1.
experimental data.                                                     Rhodes, G., Pond, S., Burton, N., Kloth, N., Jeffery, L., Bell,
   This framework might serve as the basis and inspiration for           J., . . . Palermo, R. (2015). How distinct is the coding of
future empirical work on face processing and linked to neural            face identity and expression? evidence for some common
approaches.                                                              dimensions in face space. Cognition, 142, 123–137.
                                                                       Sariyanidi, E., Gunes, H., & Cavallaro, A. (2015). Automatic
                         References                                      analysis of facial affect: A survey of registration, repre-
Bruce, V., & Young, A. (1986). Understanding face recogni-               sentation, and recognition. Pattern Analysis and Machine
   tion. British journal of psychology, 77, 305–327.                     Intelligence, IEEE Transactions on, 37(6), 1113–1133.
Bruyer, R., Laterre, C., Seron, X., Feyereisen, P., Strypstein,        Tranel, D., Damasio, A. R., & Damasio, H. (1988). In-
   E., Pierrard, E., & Rectem, D. (1983). A case of prosopag-            tact recognition of facial expression, gender, and age in
   nosia with some preserved covert remembrance of familiar              patients with impaired recognition of face identity. Neu-
   faces. Brain and cognition, 2(3), 257–284.                            rology, 38(5), 690–690.
Calder, A. J., Burton, A. M., Miller, P., Young, A. W., &              Turk, M., & Pentland, A. (1991). Eigenfaces for recognition.
   Akamatsu, S. (2001). A principal component analysis of                Journal of cognitive neuroscience, 3(1), 71–86.
   facial expressions. Vision research, 41(9), 1179–1208.              Valentine, T. (1991). A unified account of the effects
Calder, A. J., & Young, A. W. (2005). Understanding the                  of distinctiveness, inversion, and race in face recognition.
   recognition of facial identity and facial expression. Nature          The Quarterly Journal of Experimental Psychology, 43(2),
   Reviews Neuroscience, 6(8), 641–651.                                  161–204.
Ganel, T., & Goshen-Gottstein, Y. (2004). Effects of fa-               Vitale, J., Williams, M.-A., Johnston, B., & Boccignone, G.
   miliarity on the perceptual integrality of the identity and           (2014). Affective facial expression processing via simula-
   expression of faces: the parallel-route hypothesis revisited.         tion: A probabilistic model. Biologically Inspired Cogni-
   Journal of Experimental Psychology: Human Perception                  tive Architectures, 10, 30–41.
   and Performance, 30(3), 583.                                        Yan, S., Xu, D., Zhang, B., Zhang, H.-J., Yang, Q., & Lin,
Ganel, T., Valyear, K. F., Goshen-Gottstein, Y., & Goodale,              S. (2007). Graph embedding and extensions: a general
   M. A. (2005). The involvement of the fusiform face area               framework for dimensionality reduction. Pattern Analy-
   in processing facial expression. Neuropsychologia, 43(11),            sis and Machine Intelligence, IEEE Transactions on, 29(1),
   1645–1654.                                                            40–51.
Haxby, J. V., Hoffman, E. A., & Gobbini, M. I. (2000).
   The distributed human neural system for face perception.
   Trends in cognitive sciences, 4(6), 223–233.
                                                                   519

