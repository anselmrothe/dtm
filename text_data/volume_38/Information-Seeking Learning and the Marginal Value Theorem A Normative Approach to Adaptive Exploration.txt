                 Information-Seeking, Learning and the Marginal Value Theorem:
                                A Normative Approach to Adaptive Exploration
                Andra Geana (ageana@princeton.edu)*+ Robert C. Wilson (bob@email.arizona.edu)°
                  Nathaniel Daw (ndaw@princeton.edu)*+Jonathan D. Cohen (jdc@princeton.edu)*+
          *
            Department of Psychology/Princeton Neuroscience Institute, Princeton University, Princeton, NJ 08540 USA
                                °Department of Psychology, University of Arizona, Tucson, AZ 85721
                              Abstract                                     The first scenario has been extensively studied in the
                                                                         foraging literature. When choosing among behaviors with
   Daily life often makes us decide between two goals:
   maximizing immediate rewards (exploitation) and learning              different reward opportunities (e.g. foraging patches) that
   about the environment so as to improve our options for future         are progressively depleting, or in circumstances in which
   rewards (exploration). An adaptive organism therefore should          (estimable) changes can happen outside the local
   place value on information independent of immediate reward,           environment, it is optimal to switch behavior when it is
   and affective states may signal such value (e.g., curiosity vs.       estimated that the value of the current behavior falls below
   boredom: Hill & Perkins, 1985; Eastwood et al. 2012). This            the mean expected value of the available alternatives (Krebs
   tradeoff has been well studied in “bandit” tasks involving
   choice among a fixed number of options, but is equally
                                                                         & Inman, 1992). This policy can be shown to optimize
   pertinent in situations such as foraging, hunting, or job search,     immediate reward rate, as described by the marginal value
   where one encounters a series of new options sequentially.            theorem (MVT, Charnov, 1976), and numerous studies have
   Here, we augment the classic serial foraging scenario to more         found that animals’ foraging behavior approximates this
   explicitly reward the development of knowledge. We develop            (Krebs, Kacelnik & Taylor 1978).
   a formal model that quantifies the value of information in this         Most foraging scenarios of this type occur in
   setting and how it should impact decision making, paralleling         environments with well-specified rewards, in which
   the treatment of reward by the marginal value theorem (MVT)
   in the foraging literature. We then present the results of an         uncertainty usually stems from stable variance or hazard
   experiment designed to provide an initial test of this model,         rates (risk), so it is possible to incorporate it into reward
   and discuss the implications of this information-foraging             expectations through estimates of expected utility and the
   framework on boredom and task disengagement.                          switching policy given by the MVT is optimal
   Keywords:         exploration,     explore-exploit      tradeoff,     asymptotically (following all possible learning). However,
   information-seeking, decision making.                                 in many circumstances reward opportunities may not only
                                                                         be stochastic, but the properties of this stochasticity may be
                           Introduction                                  unknown (Payzan-LeNestour et al., 2013). That is,
                                                                         uncertainty may stem from ambiguity rather than risk. In
   All organisms face the frequent need to decide between
                                                                         such circumstances, sampling the environment can provide
persisting with one behavior (and the known rewards it
                                                                         information that, even if it is associated with immediate
brings), or switching to another. This tradeoff is well
                                                                         sacrifices in predictable reward rate, can be used to learn
documented in the literature: stay-or-switch behavior has
                                                                         about the environment and improve reward rate over the
been studied in humans (Behrens et al., 2007) and non-
                                                                         longer term. We refer to such information-seeking as
human animals such as primates (Pearson et al. 2009), birds
                                                                         “exploration," to distinguish it from foraging choices that
(Krebs, Kacelnik & Taylor 1978), rodents, and even non-
                                                                         we define as the pursuit of alternative behaviors based on
vertebrates (Gallistel 1990) including the extent to which
                                                                         decisions involving reward opportunities with known
this follows optimal sampling strategies (Goldstone &
                                                                         distributional properties (e.g., mean and variance)1.
Ashpole, 2004; Daw et al. 2006). When examining such
                                                                           The drive toward exploration has been well-documented
decisions, it is helpful to distinguish between at least two
                                                                         in both human and animal literatures (e.g., Cohen, McClure
types of circumstances under which an organism might
                                                                         & Yu, 2007), and there is rich evidence that under many
choose to persist in or change its behavior: one involves
                                                                         scenarios, organisms will choose to sample the environment
situations in which rewards are which rewards are known
                                                                         for useful information even at the cost of current or
up to stochasticity but either changing (as when foraging
                                                                         predictable reward (Wilson et al., 2014). This is the case, for
from a depleting patch) or varying in quality across options
                                                                         instance, when we choose to try out the new special at a
(as in encountering a series of candidate prey), so the
                                                                         restaurant instead of sticking with our favorite dish, or when
decision whether to switch to other alternatives is a way to
maximize current reward rate. The other involves situations
in which the options’ values are imperfectly known (as in                  1
                                                                              We make this qualitative distinction largely for the
bandit tasks) so that switching behavior may not yield                   purpose of clarity, and to guide formal treatment, fully
immediate benefit, but may provide new information that                  recognizing that real-world circumstances almost certainly
can support learning and improvements in reward-rate over                fall along a continuum between these extremes and involve
the longer term.                                                         a mix of these two type of decisions.
                                                                     1793

we choose to watch a new show instead of rewatching an             but at the same time we are also learning about the overall
episode of an old favorite. In this way, exploration is            qualities of the orchard, so next time we go apple-picking
different from foraging: though both involve the choice            me might choose an altogether different orchard.
between persisting with the current action or switching               Under this framework, exploiting a local patch obtained
away to something else, exploration is geared toward               increasing local reward (fig. 1A), but exploring many local
acquiring new information, while foraging is geared toward         patches helps the agent learn the global structure faster,
acquiring predictable sources of reward.                           which would in turn allow it to make better choices earlier
  However, exploration of this sort has been studied largely       in the local patches. Depending on goals, therefore, it could
in the context of “bandit” tasks – choice among a fixed set        be optimal to quit a local patch (even if it was yielding a
of options, whose properties must be learned from sampling         high reward) and move to another patch, at the potential cost
– and not in the serial switching scenarios modeled by the         of a lower reward, for the sake of learning about the global
MVT. Here, we propose a formal model for exploration that          environment that could improve returns in the future.
parallels the formulation of the MVT for reward, but applies          This local/global structure allowed us to model an
it to maximizing information alongside reward. In                  environment with a distribution of available information
particular, we augment the patch foraging scenario to more         paralleling the distribution of available reward in standard
explicitly reward information gathering – this models, for         foraging environments. In other words, each patch held not
instance, development of expertise when encountering a             only reward (which increased with time spent in patch), but
series of options, for instance, learning a trade over             also information (which decreased with time spent in patch).
successive jobs or improving one’s dating skills – and study       This generated a canonical explore-exploit decision
the behavior of optimal agents. We then present the results        tradeoff: maximize known rewards by staying within a
of an experiment designed to provide an initial test of this       patch (exploitation), or switch patches to acquire
model, and discuss the implications of this information-           information (exploration). We constructed a model of this
foraging framework on boredom and task disengagement.              process, and used it to compare performance with a pure
                                                                   exploitation strategy, a strict foraging model, and human
         A Normative Model of Exploration                          behavior in an empirical version of the task.
Paralleling circumstances to which the MVT has been
applied, we model an environment consisting of local               Model Assumptions
reward patches that offer different reward rates, with the         Each patch represented a stochastic environment in which
model agent free to either stay within a patch to reap reward      the agent could earn rewards by making accurate
(exploit), or switch away to search for other patches, in this     predictions. Each patch had a hidden distribution with mean
case with only partially or unknown characteristics                𝜇𝑖 and standard deviation σ (which was the same between
(explore). Each patch is comprised of an environment in            patches). On each time step spent inside the patch, the agent
which the agent can earn rewards by making accurate                had to make a prediction relating to this distribution. The
predictions of the outcome of a stable stochastic process.
                                                                   agent’s reward 𝑟𝑡,𝑖 was proportional to the accuracy of the
Upon "entering" a patch, the agent does not know the
parameters of the stochastic process, but these can be             prediction (for a similar task design, see Nassar et al.’s
learned by sampling. On each time step spent within a              (2010) “estimation task”), according to
patch, the agent makes a prediction, and receives a reward                              𝑟𝑡,𝑖 = 𝜌 − 𝑃𝐸𝑡 ,                      (1)
proportional to the accuracy of the prediction. The longer an
agent spends learning about a patch, the better its estimates      where ρ represented the maximum amount of reward that an
of the underlying structure can become, and higher the             agent could earn (if its predictions were fully accurate), and
reward it can receive. This distinguishes this task                𝑃𝐸𝑡 represented the prediction error, computed as the
environment from the environment assumed in most studies           difference between the agent’s prediction Pr and the actual
of foraging: here, the patch becomes more rewarding with           number generated in the patch on time step t:
the passage of time (and sampling), rather than depleting.
   An additional important assumption is that the properties
                                                                                       𝑃𝐸𝑡 = 𝑃𝑟𝑡 − 𝑁(𝜇𝑖 , σ)                 (2)
of patches are not independent of one another, but rather
reflect properties of a global environment from which they         Figure 1A shows the increase in reward with time spent in
are drawn. Thus, within limits, sampling a local patch can         patch. An agent could spend as long as it wanted exploiting
provide information that is relevant to other patches. This is     a patch, but each patch had a fixed chance of termination λ,
a property of many real-life environments, in which humans         meaning that on every time step the patch would end with
sequentially sampling different “patches” learn about the          probability λ, and continue with probability (1 – λ).
local structure while simultaneously learning about an             As explained in the previous section, all patches were
overarching global structure (Diuk et al 2013). For instance,      connected under a higher-level, global structure. In other
when going apple-picking, we learn about the quality and           words, the underlying patch distribution parameter 𝜇𝑖 came
availability of fruit in each individual tree (so we could         from a global distribution with a (fixed) grand mean M and
choose to move from a smaller, poorer tree to a better one),       standard deviation S. Exploiting a local patch obtained
                                                               1794

increasing local reward (fig. 1A), but decreasing                   future predictions. Accordingly, we approximated the
information (fig. 1B), while exploring more local patches           information value of staying in a patch with
                                                                                                    1
helps the agent learn the global structure faster. There was                           𝑉𝑠𝑡𝑎𝑦 =            𝑃𝐸𝑖,𝑡               (6)
                                                                                               (𝑁−1)(𝑛−1)
also a global reward R associated with learning the global
mean M. Our model tracked several quantities of interest as
an agent exploited a patch with the above structure; for            while the value of leaving the patch was
                                                                                                   1
simplicity, we approximate the hierarchical estimation                                 𝑉𝑠𝑤𝑖𝑡𝑐ℎ =       𝑃𝐸𝑖+1,𝑡                (7)
                                                                                                 (𝑁−1)
problem with nested error-driven updates. First, at each time
step it computed an estimate of the local mean for patch i at       where N was the current number of patches exploited so far,
time t,     , as the average of all data points 𝑥𝑖,𝑡 observed in    n the current data points observed in the current patch, 𝑃𝐸𝑖,𝑡
that patch up to the current time:                                  the next estimated prediction error within the current patch,
                          ∑𝑡 𝑥𝑖,𝑡                                   and 𝑃𝐸𝑖+1,𝑡 the next estimated prediction error assuming
                  𝜇𝑖,𝑡 =                       (3)                  that the agent explored a new patch. This relative value
                            𝑛
                                                                    between staying (exploiting) and switching (exploring)
which can also be written in terms of the prediction error PE       depended therefore on the current position within the game
and a learning rate of 1/n, as                                      (n), the current position within the patch (N), and the two
                                    1                               variance estimates for the patch mean (𝜎 2 ) and the global
                𝜇𝑖,𝑡 = 𝜇𝑖,𝑡−1 + ∗ 𝑃𝐸𝑡 (4)                           mean (𝑆 2 ), as those variance estimates were used to
                                    𝑛
                                                                    compute the two prediction errors of interest in the above
(Given the structure of the task, the optimal prediction at         equation. Given a fixed number of available timesteps, the
                                                                    assumption that the agent could not return to a patch once it
any time step was the current estimate of the mean,            ,    switched away, and values for the rewards ρ, R and the
and our model assumed that the agent would always predict           termination probability λ, we used dynamic programming to
that mean). In addition to tracking the mean estimate for the       compute the value of staying or going at every time step
patch, the model also used error-driven updating to estimate        (combining the immediate rewards for estimation within a
the variance of the local patch,                                    patch with the approximate future value of learning the
         2        2        1 (𝑛−1)∗𝑃𝐸 2         2                   global mean, V), as a function of how well both the local
       𝜎𝑖,𝑡 = 𝜎𝑖,𝑡−1    +𝑛(               − 𝜎𝑖,𝑡−1   ) (5)
                                   𝑛                                patch and the global mean were known.
which allowed computation of how informative each new
data point was, in terms of how much it could reduce
variance about the local patch. As the above equation
shows, the informativeness of each new data point
decreased proportionally to 1/n (see fig 1B). The model also
tracked an estimate of the global mean, in terms of the
history of visited patches. Each final mean estimate, 𝜇𝑖 , for
the distribution within a patch served as an additional data
point for inferring the grand mean M , in the same way that
each within-patch data point served to estimate 𝜇𝑖 .
   Crucially, the model assumed that upon first entering a          Figure 1: Model results. A. Reward increases with time
new patch, the initial prediction regarding the distribution of     spent in a game (dotted line), and average reward in a game
that patch (essentially, the prior, before any data points from     increases as more patches are explored (solid black line). B.
that patch were observed) was set to the current estimate of        The amount of information obtained from each new sample
the global mean M. This provided a way to quantify the              decreases with trials spent in a game (i.e. patch). C.
value of information in each patch, in terms of expected            Information-foraging agent that leaves patches early (blue)
reward, as the estimated improvement in initial predictions         learns the global structure parameters earlier than an agent
on future patches. That is, the better the estimate of the          that exploits a patch for all its reward (red). D. Model
global mean, the better the agent could do, on average, when        predicts later patch leaving times as a function of how well
entering a new patch. This is because the mean for each             the world is known (i.e. how many patches have already
patch was drawn from a distribution centered on the global          been explored).
mean, and thus the optimal initial guess (prior) for a given
patch was the global mean. Thus, at each time step t, the           Results
value of acquiring one extra data point in the current patch i
could be estimated in terms of how much it improved future          Figure 1 shows model results (5000 simulations of twenty-
                                                                    five games each, with a maximum of twenty-five trials per
predictions (i.e. how much closer it moved them to M),
relative to how much sampling a new patch would improve             game). As per the task structure, reward increases with time
                                                                    in game (fig. 1A, dotted line); furthermore the reward
                                                                1795

increases across games as global structure is learned (solid         be defeated by it (i.e. miss). A reward of 30 points was
line). The change in prediction error (PE) as well as the            available for defeating the overlord.
reduction in uncertainty from each new data point decrease              Participants were informed that each wave of minions was
with time spent in game (fig 1B). Compared to an agent that          trained by a different commander, and that all of the
exhausts all available trials in a patch, our “information-          commanders had been trained by the overlord.
foraging” agent that leaves a patch depending on the relative        Commanders shared, but did not perfectly imitate the
informativeness of an extra data point within the patch              overlord's preference for location of appearance. Similarly.
versus a data point in a new patch showed faster learning of         minions shared, but did not perfectly imitate their
the global mean (i.e., learned it both in a shorter number of        commander’s location of appearance. These instructions
trials, and approached it faster across games, fig. 1C). Under       reflected the generative properties of the environment to be
certain model parameters, it also earned more average                learned: the location of appearance of each minion within a
reward when compared to a model that only takes into                 wave was drawn from a distribution with a fixed mean and
account local reward (i.e. current prediction error). Looking        variance, and the means for each wave were drawn from a
at model predictions for the optimal time to switch away             distribution with the same variance and a mean equal to the
from a patch, as a function of both mean estimated variance          preferred location of the overlord.
of the global mean and, and as a function of time in game               Before encountering each minion in a wave, participants
(though the two measures are somewhat correlated), the               had two options: They could choose to stay and confront
model predicted longer dwell times later in the game, when           that minion, or choose to “run away” by pressing the large
the uncertainty about the global mean had been significantly         “RUN” button at the top left of the screen. If they chose to
reduced (figure 1D).                                                 run, that wave of minions would end, a screen appeared
                                                                     announcing a new wave (with a new distribution of
                        Experiment 1                                 locations, and they would then have the same two options
The following experiment was designed to test model                  for each minion in the new wave. Given this task structure,
predictions from our information-foraging model. The main            sampling within a wave could lead to progressive
prediction from the model, given the task structure                  improvement in performance (and reward) for a given wave.
presented to the subjects, was that they would quit high-            However, learning the preference of the overlord (associated
reward, low-information patches early when the value of              with a much larger reward) required an appropriate balance
gaining information from new patches was higher (i.e., near          of sampling within and across waves, as the informativeness
the beginning of the task, when they had not learned much            of each data point decreased (see fig. 1B) within a wave,
about the global environment), but spend longer and longer           and other waves provided additional information.
in patches as the usefulness of new information decreased.              Importantly, participants were told that they had only one
                                                                     hundred and fifty arrows to use on the minions – this
Methods                                                              operationalized the finite number of steps in our model – so
                                                                     they would have to decide how to use those arrows in a way
   Participants Twenty-five participants were recruited
                                                                     that would give them the best shot of defeating the overlord.
from the Princeton community. They gave informed
                                                                     The model predicted that they should use fewer arrows
consent, and were compensated for their time at a rate of
                                                                     (confront fewer minions) in earlier waves, switching waves
$12/hour, plus a bonus of up to $5 for performance.
                                                                     (i.e., exploring) to maximize information about average
                                                                     locations of the waves (as a predictor of the overlord). At
   Task Participants played a game in which they controlled
                                                                     the same time, the model predicted that, as information
a virtual archer that made his way through enemy territory
                                                                     accrued, and future opportunities to do so diminished (i.e.,
toward a castle (fig 2A, below). The goal was to defeat an
                                                                     task horizon shortened), they should use arrows more
“evil overlord,” and the ability to do so could be enhanced
                                                                     liberally to earn points within each wave (i.e., exploit).
by facing waves of “minions” trained by the overlord, and
learning about their behavior as an indicator of his. Minions
                                                                        Results Twenty-five Participants learned the task, as
appeared sequentially on the screen, the archer had to fire an
                                                                     evidenced both by their increasing accuracy in targeting the
arrow to hit the minion, and doing so earned one point. A hit
                                                                     minions, within a wave (figure 2B) and by the increasingly
and miss counter was available on the bottom left of the
                                                                     accurate first location estimate – i.e., change of hitting the
screen, indicating to participants how well they were doing.
                                                                     first minion – in later waves compared earlier waves
   Participants were informed that the archer had to confront
                                                                     (significant linear trend, F(1,6) = 9.42, p = 0.02, figure 2C).
seven waves of minions before facing the overlord. Each
                                                                        No participants attempted to defeat all minions in a wave.
wave consisted of a maximum of thirty-five minions,
                                                                     However, participants’ average number of minions
appearing one by one, from the right of the screen (fig 1A),
                                                                     attempted within a wave increased in later waves compared
at a variable location. Participants could adjust the archer’s
                                                                     to earlier waves (figure 2C). This equates to earlier quitting
firing position on each trial, to best anticipate where the next
                                                                     times earlier in the game. Average likelihood to “run” (i.e.
minion would appear. At the end of seven waves, the archer
                                                                     quit the current wave and move on to the next one)
confronted the overlord, and had only one shot to either
                                                                     increased, for all participants, as a function of the number of
defeat it (i.e., aim the arrow accurately enough to hit it), or
                                                                 1796

minions they had confronted in the current wave (i.e the            adaptive exploration. The model is based on a variant of the
number of time steps they had spent there, fig. 2D).                Marginal Value Theorem (MVT; Charnov, 1976), in which
                                                                    explore-exploit decisions are driven by estimates of the
                                                                    relative information (rather than reward) associated with
                                                                    each option. The model balances the goals of learning about
                                                                    the properties of the local and global environments, and we
                                                                    showed that in doing so it is capable of optimizing overall
                                                                    reward rate. We tested, and found support for qualitative
                                                                    predictions of the model in an empirical study: human
                                                                    participants exhibited behavior consistent with information-
                                                                    seeking, and stay-leave (explore-exploit) decisions that were
                                                                    sensitive to estimates not only of current reward, but also
                                                                    current information, and that these were integrated into their
                                                                    decision to stay or leave.
                                                                       The model predictions are consistent with theories of
                                                                    intrinsic motivation stating that the drive to explore arises
                                                                    from an innate need to interact efficiently with the
                                                                    environment (Deci & Ryan, 1985; White, 1959), as well as
                                                                    with the notion of “flow” and the optimal arousal theory of
                                                                    motivation, according to which organisms seek to balance
                                                                    an internal need for optimal levels of information and
                                                                    stimulation (Carrol, Zuckerman & Voegel, 1982;
                                                                    Csizentmihalyi, 2000). Furthermore, results here show that
                                                                    quitting a current high-reward but low-information patch
                                                                    can in fact still lead to higher overall reward than staying in
                                                                    the uninformative patch. This is consistent with model
                                                                    findings that average within-game reward increases across
                                                                    games, as the global structure is learned (Fig 1A). Similar
                                                                    findings have been presented previously in the machine
                                                                    learning literature, in studies showing that artificial agents
Figure 2: A. The archer task: participants adjust the position      capable of penalizing a too-well-known option’s value can
of the archer using the mouse, then release the arrow. The          learn a complex grid environment faster and earn higher
minion then comes from the right of the screen. B.                  overall reward (Simsek & Barto, 2006). Ecological models
Participants’ chances of hitting a minion increase with time        of optimal foraging have also mentioned the “penalty of
spent in a minion wave, as they learn to predict the locations      ignorance”, i.e. the benefits that a forager could lose by not
of the minions better. C. Leaving times (i.e. the number of         improving its information about the world over time (Olsson
minions attempted) increase in later waves compared to              & Brown, 2006). However, to our knowledge, the model we
earlier waves. D. The chance of hitting the first minion in a       present here is the first to cast exploration in terms that
wave (i.e. the first sample of that particular wave’s location)     parallel the role of reward in optimal foraging theory. In
increases across the game. E. Participants are more likely to       this respect, the model provides a bridge between the
run after facing more minions (black line) and after hitting        closely-related literatures on foraging and exploration, and a
more minions (blue line).                                           path toward theoretical integration.
   Likelihood to run also increased with the number of                 Along similar lines, our model provides a mechanistic,
minions actually defeated in the current wave (fig. 2E, blue        and potentially normative account of the phenomenology
line), which is well-correlated with how well the                   associated with boredom. The link between exploratory
participants had learned that particular wave; comparatively,       behavior and boredom has been suggested many times in
the likelihood to run was slower to increase as a function of       both human and animal literature (Fowler 1959; Cohen,
faced minions (black line). Finally, in line with optimal           McClure & Yu, 2007; Meagher & Mason 2012; Kurzban,
performance, participants’ accuracy in later waves improved         2013). Our model formalizes this idea, suggesting that
on the first trial of a wave (before they got any data from the     boredom might be considered as signaling the value of
current minion wave), indicating that they generalized the          exploration; that is, that information provided by the
knowledge about the structure of previous games to make             current behavior is below what can be expected from
better predictions in the current one (Fig. 2B).                    alternatives, and therefore that a switch in behavior is
                                                                    warranted.
Discussion                                                          Consistent with this suggestion, we have found in a separate
Building on ideas from optimal foraging theory and                  set of experiments that boredom is negatively correlated
reinforcement learning, we proposed a normative model of            with the rate of information acquisition (learning) in the
                                                                1797

current task, is influenced by context, and that participants          Diuk, C., Tsai, K., Wallis, J., Botvinick, M., & Niv, Y.
are willing to sacrifice reward in order to avoid boredom            (2013). Hierarchical learning induces two simultaneous, but
and increase the rate of information acquisition and                 separable, prediction errors in human basal ganglia. The
learning. These observations are consistent with ones from           Journal of Neuroscience, 33(13), 5797-5805.
the study reported here. At least initially, participants chose        Eastwood, J. D., Frischen, A., Fenske, M. J., & Smilek,
to switch away from a given wave of minions, even as their           D. (2012). The Unengaged mind defining boredom in terms
performance improved, reflecting a valuation of information          of attention. Perspectives on Psychological Science, 7(5),
and learning (and the diminution of “boredom”) over                  482-495.
immediate reward. This echoes a pervasive pattern of                   Fowler, H. (1967). Satiation and Curiosity: Constructs for
behavior in video games, and explains the need for “levels”          a Drive and Incentive-Motivational Theory of
to maintain gamers’ interest — a phenomenon that is                  Exploration. Psychology of learning and motivation, 1, 157-
consistent with the current model. Interestingly, however,           227.
participants in our experiment chose to stay with a wave of            Gallistel, C.R. (1990). The Organization of Learning.
minions longer as the task progressed; that is, they appear to       Cambridge, MA: Bradford Books/MIT Press.
have become less “bored” as overall time-on-task increased.            Goldstone, R. L., & Ashpole, B. C. (2004). Human
This seemingly counterintuitive effect was predicted by the          foraging behavior in a virtual environment. Psychonomic
theory: as the task horizon shortened, the worth of                  bulletin & review, 11(3), 508-514.
information diminished relative to immediate reward, thus              Hill, A. B., & Perkins, R. E. (1985). Towards a model of
driving participants to persist (exploit) as the task neared an      boredom. British Journal of Psychology, 76(2), 235-240.
end.                                                                   Kaelbling, L., Littman, M., & Moore, A. W. (1996).
It is important to note that, from the vantage of the model          Reinforcement learning: A survey. Journal of Artificial
proposed, the value of exploration is dependent on the               Intelligence Research, 4, 237–285.
structure of the environment (e.g., the amount of time                 Krebs, J. R., Kacelnik, A. & Taylor, P. (1978) Tests of
available, as well as the difficulty of the learning problem)        optimal sampling by foraging great tits. Nature 275, 27–
and on how well the agent has learned the environment. Our           31Lai & Robbins 1985
findings apply to the pre-asymptotic phase(s) of learning,             Kurzban, R., Duckworth, A., Kable, J. W., & Myers, J.
when gaining information from exploration can contribute             (2013). An opportunity cost model of subjective effort and
to forming better representations of the environment and             task performance. Behavioral and Brain Sciences, 36(06),
ultimately to better strategies for gaining reward. If the           661-679.
world were fully known, the same findings would not hold;              Nassar, M. R., Wilson, R. C., Heasly, B., & Gold, J. I.
however, given the complexity of the real world, it seems            (2010). An approximately Bayesian delta-rule model
likely that organisms spend a considerable fraction of their         explains the dynamics of belief updating in a changing
time in pre-asymptotic phases of learning. That certainly            environment. The Journal of Neuroscience, 30(37), 12366-
appears to be the case for our collective understanding of           12378.
how natural agents learn about their environment, and along            Olsson, O., & S Brown, J. (2006). The foraging benefits
similar lines, we hope that the work we describe here offers         of information and the penalty of ignorance. Oikos, 112(2),
a useful new path for exploring this domain of                       260-273.
understanding.                                                         Payzan-LeNestour, E., & Bossaerts, P. (2012). Do not bet
                                                                     on the unknown versus try to find out more: Estimation
                        References                                   uncertainty and “unexpected uncertainty” both modulate
   Behrens. T.E.J., Woolrich, M.W., Walton, M.E. *                   exploration. Frontiers in Neuroscience
Rushworth, M.F.S. (2007). Learning the value of                        Pearson, J. M., Hayden, B. Y., Raghavachari, S., & Platt,
information in an uncertain world. Nature Neuroscience 10:           M. L. (2009). Neurons in posterior cingulate cortex signal
1214 – 1221                                                          exploratory decisions in a dynamic multioption choice
   Charnov, E. L. (1976). Optimal foraging, the marginal             task. Current biology, 19(18), 1532-1537.
value theorem.Theoretical population biology, 9(2), 129-               Schmidhuber, J. (1991). A possibility for implementing
136.                                                                 curiosity and boredom in model-building neural controllers.
   Cohen, J. D., McClure, S. M. & Yu, A. J. (2007).Should I          In From animals to animats: proceedings of the first
stay or should I go? How the human brain manages the                 international conference on simulation of adaptive behavior
trade-off     between     exploitation     and      exploration.     (SAB90).
Philosophical Transactions of the Royal Society: Biological            Şimşek, O., & Barto, A. (2006). An intrinsic reward
Sciences. 362, 933–942                                               mechanism for efficient exploration. Proceedings of the
   Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B., &           23rd international conference, 833-840.
Dolan, R. J. (2006). Cortical substrates for exploratory               Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A., &
decisions in humans. Nature, 441, 876 – 879                          Cohen, J. D. (2014). Humans use directed and random
   Deci, E. L., & Ryan, R. M. (1985). Intrinsic motivation           exploration to solve the explore–exploit dilemma. Journal
and self-determination in human behavior.                            of Experimental Psychology: General, 143(6), 2074.
                                                                 1798

