      Modeling N400 amplitude using vector space models of word representation
                                                 Allyson Ettinger1 (aetting@umd.edu)
                                                 Naomi H. Feldman1,2 (nhf@umd.edu)
                                                   Philip Resnik1,2 (resnik@umd.edu)
                                                     Colin Phillips1 (colin@umd.edu)
                                1 Department of Linguistics, 2 Institute for Advanced Computer Studies
                           1401 Marie Mount Hall, University of Maryland, College Park, MD 20742 USA
                              Abstract                                   representation of the context. For instance, adding negation
   We use a vector space model (VSM) to simulate semantic relat-
                                                                         to a context should dramatically change likely continuations,
   edness effects in sentence processing, and use this connection        but negation has little effect on the N400 (Fischler, Childers,
   to predict N400 amplitude in an ERP study by Federmeier               Achariyapaopan, & Perry, 1985) unless additional contextual
   and Kutas (1999). We find that the VSM-based model is able            support is provided (Nieuwland & Kuperberg, 2008). Simi-
   to capture key elements of the authors’ manipulations and re-
   sults, accounting for aspects of the results that are unexplained     larly, in the absence of extended processing time, the N400
   by cloze probability. This demonstration provides a proof of          appears to be less sensitive to structural information about the
   concept for use of VSMs in modeling the particular context            agent and recipient of an event (Chow, Smith, Lau, & Phillips,
   representations and corresponding facilitation processes that
   seem to influence non-cloze-like behavior in the N400.                2015). Specifically, in such cases the N400 amplitude can fail
   Keywords: N400, vector space models, semantic relatedness             to reflect the low cloze probability of completions such as “A
                                                                         robin is not a bird” (negation violation) or “He forgot which
   Humans process words in relation to a context. During                 waitress the customer had served” (agent/recipient swap). In
sentence comprehension, incoming words are processed with                these cases, the N400 seems to reflect fit to a less structured
respect to mental states elicited by preceding words. How                context representation: more like general lexical fit, or collec-
words and their preceding contexts are represented, and how              tive facilitation by semantically related words. Evidence has
these representations interact during processing, is not yet             long indicated that the N400 is sensitive to semantic related-
understood—but questions underlying this puzzle are relevant             ness facilitation, both by sentence contexts (Kutas & Hillyard,
both to cognitive neuroscience and to computer science. In the          1980, 1984) and by single-word contexts (e.g. Bentin, Mc-
present paper we bring together lines of research from both              Carthy, & Wood, 1985; Kutas & Hillyard, 1989; Holcomb,
domains to enable explicit modeling of a particular class of            1988; Brown & Hagoort, 1993). Semantic relatedness effects
context representation and corresponding influence on word               on the N400 are broadly accepted and frequently cited, but
processing: the representations that seem to underlie facilita-          to our knowledge no explicit models generating quantitative
tion by lexical semantic relatedness.                                    predictions of these effects have yet been proposed.
   In cognitive neuroscience, a measure often used to study
                                                                            Meanwhile, in computer science, explicit models have
the effects of context on word processing is the N400 compo-
                                                                         emerged that allow straightforward computation of relations
nent, a brain response detectable by the event-related potential
                                                                         between words—and by extension, of relations between in-
(ERP) technique. The N400 is elicited by every word of a
                                                                         dividual words and groups of words. Vector space models
sentence, occurring approximately 400 milliseconds after the
                                                                         (VSMs), now widely used for natural language processing in
word is encountered. Its amplitude appears to be modulated
                                                                         computer science, use distributional characteristics of words
by the relation of the word to its context: the worse the fit to
                                                                         in text (that is, the types of contexts that words tend to occur
context, the larger the N400 amplitude. However, the exact
                                                                         in) to form representations for individual words in the form of
nature of the relation reflected by the N400 is complex and
                                                                         numeric vectors. A prominent early example of this concept
likely varies based on particular circumstances. A widespread
                                                                         in cognitive science is that of latent semantic analysis (LSA),
generalization is that the N400 amplitude tracks “cloze” prob-
                                                                         discussed extensively by Landauer and Dumais (1997). Since
ability (Kutas & Hillyard, 1984; Kutas, Lindamood, & Hill-
                                                                         the development of LSA, much continued progress has been
yard, 1984), a measure based on the proportion of people who
                                                                         made in building and optimizing such VSMs.
choose a word in a given context during an untimed fill-in-the-
blank task. To the extent that N400 amplitudes track cloze                  Once a word is represented by a vector, we can think of
probability, we may assume that all information used in an               this representation as being situated in a multi-dimensional
untimed fill-in-the-blank task can exert processing influence            space, and we can compute relations between different word
very shortly after the context is presented (400 milliseconds            representations based on their orientations or locations in that
after arrival of the next word). This plausibly includes a rich          space. Most common is the use of cosine similarity, a measure
and syntactically-composed representation of the context.                based on the angle between vectors. VSMs also allow flexible
   In other cases, however, N400 amplitude does not track                computation of the relation between a word and a group of
cloze probability, suggesting that it does not straightforwardly         words—for instance, using a very simple combination function
reflect fit between the incoming word and a fully composed               such as an average, one can generate a single vector represen-
                                                                     1445

                                                                                           Table 1: Sample stimuli.
tation reflecting characteristics of multiple words, without ad-
ditional information (such as agent/recipient roles) that would         Stimulus (expected/within/between)
be encoded in a representation arrived at through full syn-             He caught the pass and scored another touchdown. There
tactic composition. Such a representation could be used to              was nothing he enjoyed more than a good game of foot-
simulate the types of representations hypothesized to underlie          ball/baseball/monopoly.
non-cloze-like behavior in the N400, when amplitude seems
                                                                        The day before the wedding, the kitchen was just covered
to reflect sensitivity to general semantic relatedness and not to
                                                                        with frosting. Annette’s sister was responsible for making the
other information present in fully composed representations.
                                                                        cake/cookies/toast.
   There is one study to our knowledge that finds a correspon-          He complained that after she kissed him, he couldn’t get the
dence between VSM relations and N400 amplitude: Parviz,                 red color off his face. He finally just asked her to stop wearing
Johnson, Johnson, and Brock (2011) assess a suite of vari-              that lipstick/mascara/earring.
ables as possible predictors of N400 amplitude in a non-linear
regression model, including cosine similarity from an LSA-
type VSM, which they find to be significant. Several studies
have also shown correspondence between semantic priming               strongly predictive contexts, the N400 amplitude is reduced.
and VSM measures (Mandera, Keuleers, & Brysbaert, 2016;                  To accomplish this, Federmeier and Kutas constructed
Lapesa & Evert, 2013; Jones, Kintsch, & Mewhort, 2006;                two-sentence contexts with three possible ending types: “ex-
Padó & Lapata, 2007; Herdağdelen, Erk, & Baroni, 2009;              pected”, “within-category”, and “between-category”. Ex-
McDonald & Brew, 2004), suggesting that VSMs can predict              pected targets are predicted by the context, with high cloze
cognitive semantic relatedness effects more generally.                probability. Within-category and between-category targets are
                                                                      both unexpected in the context—cloze probability of approx-
   In the present paper we build on this foundation to imple-
                                                                      imately zero—but within-category targets share a category
ment a VSM-based model of aspects of sentence processing,
                                                                      with the expected target.1 If N400 amplitude were to track
intended to test whether a model with simple, averaging-based
                                                                      cloze probability, then we would see reduced N400 amplitude
context representations can successfully simulate non-cloze-
                                                                      for the expected target condition, and roughly identical, unre-
like N400 results, as the above reasoning would predict. We
                                                                      duced N400 amplitude for the two unexpected target types,
choose to model the results of the Federmeier and Kutas (1999)
                                                                      regardless of category relationship to the expected target.
N400 study. This study is one in which certain results deviate
from predictions of cloze probability, making it a valuable              The stimuli were furthermore binned into two conditions
testing ground for a model intended to capture semantic re-           based on the extent to which the context constrained toward
latedness effects believed to underlie non-cloze-like N400            the expected word: stimuli were classified as either “high-
behavior. The study is also ideal because it explicitly manip-        constraint” and “low-constraint”, according to a median split
ulates relations between target words and their contexts, but         on cloze probability of the expected target.
bases assumptions about these relations on measures such as              Figure 1 shows the results of Federmeier and Kutas’s study.
cloze probability and plausibility ratings. We use this oppor-        Negative voltages are plotted upward, with higher N400 am-
tunity to test whether relations computed based simply on             plitude (corresponding to a word that is less expected or facili-
collective effects of context words will generate better predic-      tated) represented by a greater negativity. In both constraint
tions of the observed results. We show that this VSM-based            conditions, we see that the expected target has extremely low
model captures many major characteristics of the study’s N400         N400 amplitude, compatible with strong facilitation. Addition-
results, including the key result that deviates from cloze pre-       ally, in both constraint conditions, between-category targets
dictions. The model’s relation computations also largely align        show very high N400 amplitude, compatible with lack of facil-
with the assumptions made by the authors about their stimu-           itation. There is also a main effect of constraint level, but the
lus manipulations—with one main exception, which is also              key difference emerges for within-category targets: in high-
the key factor accounting for the deviation from cloze. This          constraint contexts only, within-category targets show reduced
suggests that the model has successfully captured aspects of          N400 amplitude—despite the fact that within-category targets
semantic relatedness-based processes exerting influence on the        (like between-category targets) have roughly zero cloze proba-
N400, and that access to predictions based on such a model            bility. Federmeier and Kutas interpret this result as evidence of
can lend useful perspective in interpreting N400 results.             a mediating influence of the expected target in high-constraint
                                                                      contexts: strong prediction of the expected target in these
                                                                      contexts causes features of that target to be pre-activated, and
             Federmeier and Kutas (1999)
                                                                      because of semantic overlap between expected and within-
Federmeier and Kutas (1999) investigated N400 effects in              category targets, the latter targets are facilitated as well.
contexts that predict a particular completion word, and are              Federmeier and Kutas’s interpretation operates on the as-
then completed by words with varying levels of similarity
                                                                          1 Federmeier  and Kutas explain that “Categories were chosen to
to that predicted word. The authors found that unpredicted
                                                                      be those at the lowest level of inclusion for which the average under-
(zero-cloze) words elicit larger N400s, as expected. However,         graduate student could be expected to readily differentiate several
when the unpredicted item is similar to the predicted word in         exemplars.” See Table 1 for examples.
                                                                  1446

       Figure 1: Federmeier and Kutas (1999) N400 results. Left: original results as reported by the authors. Right: Results
       re-plotted as points representing peak N400 amplitude, for greater ease of comparison to simulation results below.
       Arrows indicate key facilitation in high-constraint within-category condition.
sumption that high- and low-constraint contexts are equally           For a sentence context, we will refer to vectors for the ex-
related to the within-category targets, such that the observed        pected target, within-category target, and between-category
facilitation of within-category targets in high-constraint con-       target as vectors E, W , and B, respectively.
texts must be explained by some additional factor. This need             We model the mental state induced by preceding context
motivates their hypothesized mediation by pre-activation of           words through a simple averaging procedure: vectors for se-
expected target features in high-constraint contexts. An alter-       lected context words are averaged to obtain a single context
native explanation would offer itself if high-constraint contexts     vector C. This representation reflects the collective effect of
were directly more facilitative of within-category targets than       the included words, without many of the additional structural
are low-constraint contexts. Federmeier and Kutas assume              cues that might inform a cloze decision. In selecting con-
that this is not the case, based on cloze probability measures        text words, we attempt to isolate the most informative words,
and plausibility ratings. However, there are other ways that          which we hypothesize will have the strongest influence upon
we might conceive of relation to context—in particular, we            the context representation. We try two selection methods:
should consider relations based simply on the collective ef-          anchored and agnostic.
fect of context words (as opposed to the fully structured and            In the anchored setting, we use relation to the expected
compositional context representations likely to be informing          target as a proxy for informativeness: using the expected
untimed cloze and plausibility decisions). In the next section,       target as an anchor, we select the four context words with
with the help of VSMs, we explore whether a notion of fit to          highest cosine similarity to that expected target.2 We employ
context based on collective semantic relatedness can explain          a minimum cosine similarity of 0.2 (chosen by examination of
the facilitation where cloze and plausibility do not.                 context word cosine similarities in a small subset of stimuli)
   Federmeier and Kutas make available a sample of 40 of their        to further filter words bearing little relation to the target.
experimental stimuli; we run our simulation on that sample.              In the agnostic setting, we take the top four words based
                                                                      on negative log frequency (that is, the least frequent words),
                             Model                                    excluding person names (e.g., Annette). This is equivalent
For testing assumptions and modeling the results of this study,       to choosing words based on maximum surprisal (information
we choose a VSM generated by the word2vec model (Mikolov,             content) as determined by a unigram probability model.
Chen, Corrado, & Dean, 2013). Unlike LSA, word2vec uses a                The modeling results suggest prima facie that the anchored
neural network to optimize word vectors based on their ability        setting is more successful in isolating the most significant
to predict nearby words. In systematic comparisons of VSM             words of the context. If so, this is likely due to the fact that the
performance on various semantic tasks, this model has shown           frequency metric underlying the agnostic setting, while rea-
consistently strong and often superior performance (Baroni,           sonable, is a rather blunt tool for assessing informativeness.3
Dinu, & Kruszewski, 2014; Levy, Goldberg, & Dagan, 2015).                Within these settings, we test two types of average: un-
For this reason, we select word2vec as a state-of-the-art VSM         weighted, and weighted inversely by linear distance. The latter
of word representations. We train the model on approximately          average aims to instantiate the hypothesis that the effect of
2 billion words of semantically diverse web data from the             a context word would decay over time, with earlier words
ukWaC corpus (Ferraresi, Zanchetta, Baroni, & Bernardini,             having less influence than later words.
2008), training vectors of 100 dimensions using the skip-gram
                                                                          2 One target. polar bear, is made up of two words; this is repre-
architecture, which maximizes the probability of surrounding
                                                                      sented as the average of the two separate word vectors.
words given the current word.                                             3 As we caution below, however, at the current stage we should
   Once we have trained this VSM, each word of the vocabu-            not be overzealous in making fine-grained modeling decisions based
lary is represented as a point within the resulting vector space.     on the linear fit of only six datapoints.
                                                                  1447

                                                                     captured across settings: for each ending type, average cosine
                                                                     similarity to context is higher in the high-constraint condition,
                                                                     corresponding to greater facilitation (lower N400 amplitude).
                                                                     This is consistent with the main effect observed in Federmeier
                                                                     and Kutas’s N400 results.4
                                                                        In addition, we see that for the most part, looking indepen-
                                                                     dently at the high- and low-constraint conditions, the three
                                                                     ending types pattern as the experimental paradigm predicts:
                                                                     expected targets are most facilitated by the context, while
                                                                     within- and between-category targets are less facilitated. We
                                                                     also see that under all settings, in the high-constraint condi-
                                                                     tion the within-category target falls at an intermediate position
                                                                     between the other two target types. In the low-constraint con-
         Figure 2: Cosine similarity to expected target              dition, however, three of the four settings have within- and
                                                                     between-category conditions in reversed or roughly identical
   Once we have obtained this context vector C, it can be            positions. The fact that between-category targets in the low-
represented as a point within the space that contains vectors        constraint condition fail to fall farthest from the context, often
E, W , and B, and its relation to these vectors can be computed.     switching with within-category targets, may reflect similar
For every stimulus, we take the cosine similarity between            factors to those that lead to within- and between-category con-
C and each of E, W , and B, and we average these cosine              ditions having statistically indistinguishable N400 amplitudes
similarity values across stimuli within each condition, in order     in Federmeier and Kutas’s results.
to simulate average N400 amplitude.                                     Returning to our main effect of constraint: recall Federmeier
   We also compute cosine similarity between E and W and             and Kutas’s assumption that facilitation of high-constraint
between E and B. This allows us to assess the model’s repre-         within-category targets cannot be explained by direct relation
sentation of the relations between different completion words.       to context. We see in Figure 3 that the VSM-based represen-
                                                                     tation of context—under both anchored and agnostic word
                    Simulation Results                               selection settings—does predict greater facilitation of within-
                                                                     category targets in the high-constraint as compared to the low-
Figure 2 shows the results of the comparison between target          constraint condition, suggesting that direct relation to context
types E, W , and B—this test simply serves as a control, to          could offer a valid explanation for this deviation from cloze
compare the model’s relation computations against those as-          probability. This result both lends support for the explanatory
sumed by Federmeier and Kutas, and to check for confounds.           power of our simple non-syntactically-composed context rep-
In Figure 2 and those that follow, cosine similarity is plotted      resentations, and gives us reason to consider direct facilitation
on the y-axis with the negative direction upward, to facilitate      by contextual semantic relatedness as an alternative account
comparison to N400 plots in Figure 1: higher cosine simi-            for Federmeier and Kutas’s results.
larity predicts lower N400 amplitude. Note in Figure 2 that
the expected word vector E is at cosine similarity of 1, as                                      Discussion
this is a comparison of a vector to itself. As for the other
two comparisons, we see that the model predicts on average a         In this study, we used a vector space model to predict N400
nearly identical level of relation between expected words and        amplitudes observed in Federmeier and Kutas (1999). We find
within-category words in both constraint conditions. We see a        that by representing words in a vector space, averaging vectors
slightly greater distance between the expected word E and the        of informative context words, and taking cosine similarity
between-category word B in the high- than the low-constraint         measures between the averaged context vector and each of its
condition. In both cases the model’s relations are roughly           possible completions, we are able to simulate key aspects of
consistent with the categorical relations assumed by the ex-         Federmeier and Kutas’s N400 results: the basic patterning of
perimental manipulation: within-category items are indeed            expected, within-category, and between-category items within
represented as being closer to the expected targets than are the     constraint conditions, as well as the main effect of constraint.
between-category items. The lack of any discernible differ-          Our model accounts for the deviation from predictions of cloze
ence in the expected/within-category target relation between         probability in the high-constraint condition, and in doing so
constraint conditions also rules out the possible confound of        calls into question the assumption that the key result of this
differing relation strengths between the targets themselves.         study cannot be explained by a direct facilitation between
   Figure 3 shows the full simulations under the anchored and        context words and the within-category targets.
agnostic settings, respectively. (The right hand side of Figure 1
                                                                         4 Having access only to 40 items of the original Federmeier and
presents Federmeier and Kutas’s results in the same plotting
                                                                     Kutas study, we are not making claims of statistical significance for
format, for ease of comparison.) In these figures we see several     this pattern of results in the models. This simulation is intended
things. First, we see a main effect of constraint consistently       instead as an exploratory proof of concept.
                                                                 1448

       Figure 3: Simulations in four settings. A) Context average unweighted by linear distance and words selected with
       expected target as anchor. B) Context average weighted by linear distance and words selected with expected target
       as anchor. C) Context average unweighted by linear distance and words selected by low frequency. D) Context
       average weighted by linear distance and words selected by low frequency.
   At face value, if we assume a linear relation between cosine       To understand what the N400 can tell us about contextual
similarity and N400 amplitude, then Figure 3B is the most             representations and their influence on incoming words, we
faithful simulation of the N400 results. We might take this           need to be able to tease apart the contributing factors at play
as evidence in favor of a cognitive model in which activation         when N400 amplitude tracks untimed measures such as cloze,
spreads from informative words (with relation to expected             versus the factors at play when it deviates from such measures.
target being a better proxy for informativeness), and in which        VSMs allow for explicit modeling of collective word relations,
a word’s influence decays over time. However, we caution              and as a result they are a promising tool for generating quan-
against drawing strong cognitive conclusions from this single         titative predictions from a range of hypotheses regarding the
set of simulations. First, we are modeling only six datapoints,       semantic relatedness-based processes that may underlie devia-
without claims of statistical significance. Second, we are for        tions of N400 amplitude from cloze. In the above simulations,
the moment assuming a linear relation between cosine similar-         we indeed find support for the ability of these models to use
ity and N400 amplitude, which is very likely an oversimplifica-       averaging-based context representation and simple relation
tion. Consider ceiling and floor effects, which are understood        computations to capture aspects of N400 behavior that deviate
to influence N400 amplitude. Floor effects, at least, are likely      from the predictions of cloze probability.
a factor in Federmeier and Kutas’s results, as the study finds           It should be noted that our results need not be in direct con-
no significant effect of constraint on N400 to expected tar-          flict with Federmeier and Kutas’s general framework. Since
gets, despite the fact that high- and low-constraint contexts are     cosine similarity is computed by dimension-wise comparison
defined precisely by how predictive they are of the expected          of one vector to another, one could think of higher cosine
target. The fact that our cosine similarity measure does reflect      similarity between context and target as representing greater
an effect of constraint on expected targets suggests that we are      pre-activation of target features as a result of context. As for
capturing important aspects of the context-to-target relation         Federmeier and Kutas’s hypothesis of mediating influence
with this measure. However, it also suggests that we will need        by pre-activation of the expected target: in our anchored set-
a nonlinear linking hypothesis to predict the N400 with more          ting, one might argue that selecting context words based on
precision. This means that we should not be quick to dismiss          the expected target instantiates a version of Federmeier and
the other settings in Figure 3, as they could ultimately prove to     Kutas’s mediation account. However, though the expected
be the more accurate simulations once we identify the proper          target does have an increased role in this setting, it is still
linking hypothesis.                                                   the context words, and not the expected target, that have the
   We see these simulations as a valuable proof of concept.           relevant relations to incoming target words in our simulations.
                                                                  1449

So although this demonstration does not discredit the validity             of English. In Proceedings of the 4th Web as Corpus Workshop
of Federmeier and Kutas’s account, it does illustrate a genuine            (wac-4) Can we beat Google (pp. 47–54).
                                                                         Fischler, I., Childers, D. G., Achariyapaopan, T., & Perry, N. W.
alternative.                                                               (1985). Brain potentials during sentence verification: Automatic
   It is also important to clarify that our claim is not that our av-      aspects of comprehension. Biological Psychology, 21(2), 83–105.
eraging procedure—and the representation that it produces—is             Fyshe, A., Wehbe, L., Talukdar, P. P., Murphy, B., & Mitchell, T. M.
                                                                           (2015). A compositional and interpretable semantic space. In Pro-
an appropriate reflection of the full extent of language process-          ceedings of the 2015 Conference of the North American Chapter of
ing. We are, however, positing that less structured representa-            the Association for Computational Linguistics: Human Language
tions of this kind are likely to underlie the N400 under some              Technologies.
                                                                         Herdağdelen, A., Erk, K., & Baroni, M. (2009). Measuring seman-
circumstances. As discussed above, many aspects of language                tic relatedness with vector space models and random walks. In
processing that we know, a priori, will be overlooked by this              Proceedings of the 2009 Workshop on Graph-based Methods for
averaging process, are also aspects of language processing that            Natural Language Processing (pp. 50–53).
                                                                         Holcomb, P. J. (1988). Automatic and attentional processing: An
we have seen the N400 at times to be insensitive to: for in-               event-related brain potential analysis of semantic priming. Brain
stance, this averaging process will not encode agent/recipient             and Language, 35(1), 66–85.
information, and it will also fail to capture effects of negation.       Jones, M. N., Kintsch, W., & Mewhort, D. J. (2006). High-
                                                                           dimensional semantic space accounts of priming. Journal of Mem-
Such selective insensitivities are in line with N400 studies               ory and Language, 55(4), 534–552.
cited above. It seems not unreasonable, therefore, to suppose            Kutas, M., & Hillyard, S. (1989). An electrophysiological probe of
that this simple averaging procedure may be approximating a                incidental semantic association. Cognitive Neuroscience, Journal
                                                                           of , 1(1), 38–49.
real representational stage tapped into by the N400.                     Kutas, M., & Hillyard, S. A. (1980). Reading senseless sentences:
   Using the N400 as a probe into online language processing,              Brain potentials reflect semantic incongruity. Science, 207(4427),
                                                                           203–205.
our results suggest that VSMs are well positioned to capture             Kutas, M., & Hillyard, S. A. (1984). Brain potentials during reading
elements of language interpretation that are driven by lexical             reflect word expectancy and semantic association. Nature.
semantic relatedness effects. A question that arises now is              Kutas, M., Lindamood, T. E., & Hillyard, S. A. (1984). Word
                                                                           expectancy and event-related brain potentials during sentence pro-
whether VSMs can also help us to model the more structured                 cessing. In S. Kornblum & J. Requin (Eds.), Preparatory states
compositional processes that seem to underlie the N400 when                and processes (pp. 217–237).
it does track cloze probability. Structured semantic compo-              Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s prob-
                                                                           lem: The latent semantic analysis theory of acquisition, induction,
sition with VSMs is an active area of current research (e.g.,              and representation of knowledge. Psychological review, 104(2),
Mitchell & Lapata, 2008; Socher, Huval, Manning, & Ng,                     211.
2012; Fyshe, Wehbe, Talukdar, Murphy, & Mitchell, 2015),                 Lapesa, G., & Evert, S. (2013). Evaluating neighbor rank and distance
                                                                           measures as predictors of semantic priming. In Proceedings of
and as progress continues in this area, it will be interesting to          the ACL Workshop on Cognitive Modeling and Computational
investigate whether the influences of more structured context              Linguistics (CMCL 2013) (pp. 66–74).
representations can also be captured through VSMs. Other                 Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving distri-
                                                                           butional similarity with lessons learned from word embeddings.
interesting questions will include whether these results extend            Transactions of the Association for Computational Linguistics, 3,
to other types of VSMs, or to different approaches to semantic             211–225.
similarity, such as manual feature generation.                           Mandera, P., Keuleers, E., & Brysbaert, M. (2016). Explaining
                                                                           human performance in psycholinguistic tasks with models of se-
Acknowledgments This material is based upon work supported by              mantic similarity based on prediction and counting: A review and
the National Science Foundation Graduate Research Fellowship un-           empirical validation. Journal of Memory and Language.
der Grant No. DGE-1322106. This work benefited from helpful              McDonald, S., & Brew, C. (2004). A distributional model of semantic
comments of three anonymous reviewers, and discussions with Tal            context effects in lexical processing. In Proceedings of the 42nd
Linzen as well as audiences and visitors across the University of          Annual Meeting on Association for Computational Linguistics
Maryland language science community.                                       (p. 17).
                                                                         Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient
                             References                                    estimation of word representations in vector space. arXiv preprint
                                                                           arXiv:1301.3781.
Baroni, M., Dinu, G., & Kruszewski, G. (2014). Don’t count,              Mitchell, J., & Lapata, M. (2008). Vector-based models of semantic
   predict! A systematic comparison of context-counting vs. context-       composition. In Acl (pp. 236–244).
   predicting semantic vectors. In Proceedings of the 52nd Annual        Nieuwland, M. S., & Kuperberg, G. R. (2008). When the truth is
   Meeting of the Association for Computational Linguistics (Vol. 1,       not too hard to handle: An event-related potential study on the
   pp. 238–247).                                                           pragmatics of negation. Psychological Science, 19(12), 1213–
Bentin, S., McCarthy, G., & Wood, C. C. (1985). Event-related po-          1218.
   tentials, lexical decision and semantic priming. Electroencephalog-   Padó, S., & Lapata, M. (2007). Dependency-based construction of
   raphy and Clinical Neurophysiology, 60(4), 343–355.                     semantic space models. Computational Linguistics, 33(2), 161–
Brown, C., & Hagoort, P. (1993). The processing nature of the              199.
   n400: Evidence from masked priming. Journal of Cognitive Neu-         Parviz, M., Johnson, M., Johnson, B., & Brock, J. (2011). Us-
   roscience, 5(1), 34–44.                                                 ing language models and latent semantic analysis to characterise
Chow, W.-Y., Smith, C., Lau, E., & Phillips, C. (2015). A ‘bag-            the N400m neural response. In Proceedings of the Australasian
   of-arguments’ mechanism for initial verb predictions. Language,         Language Technology Association Workshop 2011 (pp. 38–46).
   Cognition, and Neuroscience.                                          Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Se-
Federmeier, K. D., & Kutas, M. (1999). A rose by any other name:           mantic compositionality through recursive matrix-vector spaces.
   Long-term memory structure and sentence processing. Journal of          In Proceedings of the 2012 Joint Conference on Empirical Meth-
   Memory and Language, 41(4), 469–495.                                    ods in Natural Language Processing and Computational Natural
Ferraresi, A., Zanchetta, E., Baroni, M., & Bernardini, S. (2008). In-     Language Learning (pp. 1201–1211).
   troducing and evaluating ukWaC, a very large web-derived corpus
                                                                     1450

