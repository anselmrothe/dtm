                           Human Reinforcement Learning of Sequential Action
                       George Kachergis2 , Floris Berends1 , Roy de Kleijn1 , & Bernhard Hommel1
                         1 Institute of Psychology / Leiden Institute for Brain & Cognition, Leiden University
                                                            Leiden, the Netherlands
                                                         2 george.kachergis@nyu.edu
                                                            Psychology Department
                                                             New York University
                               Abstract                                   ior. The role of attention in the SRT task was further stud-
                                                                          ied in Fu, Fu, and Dienes (2008), finding that reward moti-
   Learning sequential actions is an essential human ability, for
   most daily activities are sequential. We modify the serial reac-       vation can improve the development of awareness of the se-
   tion time (SRT) task, originally used to teach people a con-           quence. Fu et al. (2008) reasoned reward motivation regulates
   sistent sequence of button presses by cueing them with the             the amount of attention paid to the stimuli, which in turn fa-
   next target response, to record mouse movements, collecting
   continuous response trajectories. Further, we introduce a rein-        cilitates sequence learning.
   forcement learning version of the paradigm in which the next              Cleeremans and McClelland (1991) demonstrated that as-
   target is not cued. Instead, learners must explore response al-        sociative processes could account for the improvement in
   ternatives, and receive a penalty for each incorrect response,
   as well as a reward for a correct response. Participants are           performance. They adapted the SRT paradigm to include
   not told that they are to learn a single deterministic sequence        a sequence derived from a ‘noisy’ finite-state grammar, and
   of responses, nor that it will repeat (nor how often), nor how         showed that the presence of grammatical structure facilitates
   long it is. Given the difficulty of the task, it is unsurprising
   that some learners performed poorly. However, many learn-              sequence acquisition. Cleeremans and McClelland (1991) ex-
   ers performed remarkably well, and some acquired the full 10-          plained their findings with a simulation of the learning pro-
   item sequence within 10 repetitions. We compare the high- and          cess. A Simple Recurrent Network (SRN; Elman (1990))
   low-performers’ detailed results in this reinforcement learning
   (RL) task with a cued trajectory SRT task, finding both simi-          was able to produce results similar to human performance,
   larities and discrepancies. Finally, we note that humans in this       findings that were later confirmed by Boyer, Destrebecqz,
   task outperform three standard RL models and have different            and Cleeremans (2005). The SRN demonstrates that asso-
   patterns of errors that suggest future modeling directions.
                                                                          ciative processes are sensitive to the statistical structure of the
   Keywords: Sequence learning; serial reaction time task; se-            training material, implying that rule-like behavior can emerge
   quential action; reinforcement learning; movement trajectory
                                                                          from networks trained on structured sequences. Indeed,
                                                                          Botvinick and Plaut (2004) showed that the SRN is capable of
                           Introduction                                   producing everyday hierarchical actions such as coffee- and
Traditionally, the bulk of cognitive psychology studies deal              tea-making after training on a set of valid examples that var-
with single stimulus-response actions or decisions. How-                  ied in order and complexity (e.g., sugar then milk or vice-
ever, much of human behavior can better be described as se-               versa, or only milk). Motivated by a study (Stadler, 1995)
quential action, consisting of partially-ordered hierarchies of           showing that introducing a longer delay (2000 ms instead of
simple actions–from cooking and cleaning to speaking and                  400 ms) to a random selection of the response-stimulus in-
sports. More recently, several diverse lines of research have             tervals reduced sequence learning, Dominey (1998) proposed
considered sequential action learning, stemming from various              another recurrent network model that is able to account for
domains including linguistics (Elman, 1990; Saffran, New-                 both serial structure effects and temporal structure (i.e., pat-
port, & Aslin, 1996), implicit learning (Nissen & Bullemer,               terns of delays).
1987; Cleeremans & McClelland, 1991), and everyday ac-                       To better reveal the mechanisms behind human sequen-
tions (Cooper & Shallice, 2000; Botvinick & Plaut, 2004).                 tial action learning, more information is needed than just the
   Throughout these lines of research, a recurring topic of in-           speedup of keypresses across an experiment. To measure
terest has been the process by which action-sequences are                 real-time dynamics and uncertainty during learning, the SRT
acquired, stored, and executed. Early work by Nissen and                  task was adapted to require and record movement trajecto-
Bullemer (1987) focused on the modulatory role of attention,              ries in (Kachergis, Berends, de Kleijn, & Hommel, 2014b,
and introduced the Serial Reaction Time (SRT) task. Un-                   2014a). That is, instead of measuring discrete button-presses,
beknownst to the participants, the task utilized a 10-symbol              continuous recordings were made during a mouse tracking
sequence, with each successive symbol indicating which of                 task that replaced the original SRT’s buttons with locations
four corresponding keys was to be pressed next. Participants              on a computer screen. The trajectory SRT paradigm not only
showed improved reaction times across training, although the              replicated earlier findings, but also found evidence of sequen-
knowledge gains seemed to be implicit: when explicitly asked              tial context effects (e.g., predictive movements towards the
at the end of the experiment, they were unable to reproduce               next response location), and unveiled changes in the move-
the sequence. Nissen and Bullemer (1987) concluded that at-               ment dynamics (e.g., pre- and post-stimulus onset).
tention is critical in developing awareness of learned behav-                Paradigms such as artificial language learning tasks and the
                                                                      193

SRT task have demonstrated that sequence learning is depen-          penalty (-1) that was accumulated throughout the experiment
dent upon the statistical structure within the training mate-        and displayed continuously. Upon reaching a valid target, it
rial, and attention towards the stimuli which is modulated by        would change color to green, add to the score by +1, and al-
reward motivation. However, rewards do not only provide a            low the participant to continue exploring. Reaching for an in-
source of motivation: in many situations they are an irreplace-      valid target caused it to change to red, subtract from the score
able source of information–perhaps even the only feedback.           by 1, while the cursor was relocated to the previously oc-
Contrary to the SRT task, everyday human action learning is          cupied target, effectively resetting the participant’s progress.
often not characterized by cued responses but by exploring           Target validity was determined by a recurrent sequence, taken
the environment and learning which actions result in positive        from the Nissen and Bullemer (1987) study, and adapted to
effects, and which result in negative effects. We believe that,      fit the trajectory SRT paradigm. Designating the stimuli as
in order to investigate human sequence learning in an eco-           numbers from left to right, top to bottom, the sequence read
logically valid manner, it is necessary to draw in another line      4-2-3-1-3-2-4-3-2-1.
of research: reinforcement learning (RL), a well-established
paradigm in the field of machine learning (Sutton & Barto,           Methods
1998), which of course was originally motivated by much              Participants Participants in this experiment were 13 Lei-
earlier behaviorist stimulus-response learning studies (e.g.,        den University students and employees (age: M = 23.9, sd =
Skinner (1950). RL paradigms allow learning agents to in-            6.4) who participated in exchange for 3.5 euros or for course
teract with a task solely through observations, actions, and         credit.
rewards. The rewards validate the actions, without the need          Procedure Participants were instructed that they would be
for explicit cueing or other forms of instruction. Thus, learn-      presented with four target squares in the corners of the screen
ing is exploratory, and accomplished via trial-and-error.            which they were to explore by moving the mouse, each time
   Although much reinforcement learning research is con-             resulting in either a gain or loss of one point. Participants
ducted in computer simulation, the inspiration for the ap-           were told to try to maximize their score, which was displayed
proach and many algorithms is in fact rooted in animal be-           continuously at the top of the screen. Unbeknownst to the
havior (Sutton & Barto, 1998) and there is evidence that             participants, only one of the four targets would be valid at
similar processes play a role in human learning. For in-             any given moment, but all were colored blue, so the target
stance, the error-related negativity (ERN) event-related po-         could not be visually distinguished. Upon reaching a valid
tential (Falkenstein, Hohnsbein, Hoormann, & Blanke, 1991;           target, its color would change to green momentarily and the
Gehring, 1992) has been studied extensively as a component           score would increase by one. The participant would be able
of error processing. The ERN originates in the brain when-           to continue exploring for the next target. Arriving at an in-
ever task-relevant errors are committed. Holroyd and Coles           valid target caused it to change to red momentarily and the
(2002) links the ERN to the mesencephalic dopamine system,           score was decreased by one, while the cursor was relocated
and proposes it is the result of a negative reinforcement signal     to the previously occupied target. Thus, although there were
which it conveys to the anterior cingulate cortex.                   no instructions explicitly indicating it, participants likely in-
   The current study adapts the trajectory SRT task to allow         ferred that they had chosen the incorrect stimulus, and should
for free movement and limited instruction, allowing learners         choose one of the remaining two–if they also assumed the
to explore and learn from trial-and-error. This RL sequence          same target was never repeated immediately, which was true.
learning paradigm allows us to study the effect of rewards on        In the absence of a previous target (i.e., at the beginning of
sequence acquisition in more detail, yielding not only correct       the experiment or after a rest break) the cursor was moved
response times but also mistakes over time, which may be             back to the middle of the screen.
indicators of distinct mechanisms. For example, committing              Unbeknownst to the participants, each trial consisted of
an error indicates incomplete knowledge (or a lapse in mem-          a series of 10 targets (labeled 1-4 left-to-right and top-
ory), whereas RT may be correlated with overall certainty or         to-bottom: 4-2-3-1-3-2-4-3-2-1) that repeated continuously,
fluency at that point in the action sequence. Thus, we investi-      with no indication where one trial stopped and the next be-
gate the RL paradigm data both in terms of earlier trajectory        gan. Participants completed eight blocks of 10 such trials,
SRT data and in comparison to three standard RL models.              with a short rest break after every 2 blocks (i.e., 200 cor-
                                                                     rect movements). A participant who somehow knew the se-
                         Experiment                                  quence before entering the experiment and never made a mis-
The goal of the current study is to examine sequence learning        take would therefore make 800 movements to valid targets,
within the trajectory SRT paradigm, and to compare human             receiving a theoretical maximum of 800 points. At worst, a
performance to basic baseline reinforcement learning mod-            participant with no memory of even the previous target they
els. The trajectory SRT task was adapted to no longer cue            had tried may make an infinite number of mistakes, and may
participants with the next target position, forcing them to in-      never finish the experiment. Assuming enough memory to not
stead explore the response alternatives until the correct one        repeat the same invalid target more than once when seeking
was found. Moving the mouse cursor from the previous tar-            each target (i.e., an elimination strategy), a participant using
get to another response alternative resulted in a reward (+1) or     this elimination strategy would expect on average to score 0
                                                                 194

points, as the expected value of completing one movement                                     possible explanation is that low performers are simply not try-
successfully is 0.1 Note that participants were not told that                                ing to learn a sequence, or do not expect it to to be determin-
there was a single deterministic sequence, let alone details                                 istic, whereas high performers explicitly learn the sequence,
such as how long the sequence was.                                                           and when they are uncertain they must pause to try to recall
                                                                                             the next target.
Results
The data from all 13 participants were analyzed. Figure 1                                                                                                   Correct                           Incorrect
shows a histogram of the final score achieved by each par-
ticipant. The distribution of scores is non-normal (Shapiro-
                                                                                                    Mean of Median Response Time (ms)
Wilk’s W = 0.87, p < .05), instead looking bimodal, with                                                                                                                                                          ●
four participants collecting less than 300 points and all but                                                                           10000
one of the rest accumulating more than 500 points each.                                                                                                                                                   ●
Given the bimodal score distribution, a median split was used                                                                                                                                       ●                 Group
                                                                                                                                                                                                                      ●   High
to divide the participants into high-performing (≥ 526; 7                                                                                                                                                     ●           Low
people) and low-performing (< 526; 6 people) groups. In                                                                                 5000                                              ●
                                                                                                                                                                                                ●
the high-scoring group, participants achieved almost flawless
                                                                                                                                                                                      ●
performance after only approximately 30 trials, with a final                                                                                                                      ●
mean score of 652 (max: 725), while the low-scoring group                                                                                       ●
                                                                                                                                                        ●
only gradually increased their score (final mean score: 287).                                                                                       ●
                                                                                                                                                             ●    ●   ●   ●   ●
                                                                                                                                            0
The remaining analyses are carried out for each group in an                                                                                     1   2   3    4    5   6   7   8   1   2   3    4    5     6   7   8
                                                                                                                                                                 Blocks of 10 Training Trials
attempt to understand the great variability in performance—
and the impressive success of the high-scoring group.
                                                                                             Figure 2: The mean of subjects’ median correct RTs by block (left
Figure 1: The histogram of                     simple elimination            perfect         panel) shows that high-performers’ RTs improved more than the
                                           3
participants’ final scores after                                                             low-performers’ RTs over training. The mean of subjects’ me-
completing 80 sequence rep-                                                                  dian incorrect RTs by block (right panel) shows that the high-
etitions (800 targets) shows                                                                 performing group’s incorrect RTs actually increased, whereas the
a bimodal distribution (lines:             2
                                                                                             low-performing group’s stayed roughly the same across the experi-
elimination strategy EV=0;                                                                   ment. Error bars show +/-1SE.
perfect knowledge EV=800).         Count
                                           1
                                                                                             Accuracy The mean number of mistakes made over the
                                                                                             entire experiment was 19.8 (sd: 21.3) for the high-scoring
                                                                                             group, and 63.5 (sd: 11.9) for the low-scoring group. Over
                                           0
                                                   0        250       500       750
                                                                                             time, the number of mistakes decreased especially for the
                                                               Final Score                   high scoring group. Examining the mistakes made by each
                                                                                             group of participants according to where they were in the se-
Response Times The overall median response time (RT)                                         quence revealed that for both groups the fifth stimulus was
for all stimulus arrivals was 1,401 ms (sd: 4,980). Of 10,400                                particularly challenging. This is reflected in the mean num-
correct target arrival times (median: 1,078 ms, sd: 2,216),                                  ber of mistakes for each group (see Figure 4, as well as in the
317 (3%) were trimmed for being too slow (median + 2 · sd).                                  mean RT to the target by sequence position (see Figure 3).
Of the 4,117 incorrect stimulus arrival times (median: 2,397
                                                                                             Comparison to previous research The pattern we observe
ms, sd: 8,401), 100 were trimmed for being too slow (2.4%).
                                                                                             in the accuracy and response time data bears some resem-
Each subject’s median RT for correct and incorrect move-
                                                                                             blance to the pattern observed in a previous trajectory SRT
ments was computed for each 10-trial block. Figure 2 shows
                                                                                             study that used cues (Kachergis et al., 2014b). Although the
the mean of subjects’ median correct and incorrect RTs over
                                                                                             task in this study (RL) was fundamentally different from the
the experiment, split into high- and low-performing group.
                                                                                             previous study (cued SRT), the same sequence was used in
RTs for correct movements improve in both groups during the
                                                                                             both experiments which enables us to compare the scaled re-
first few blocks, but the high-scoring group speeds up more
                                                                                             sponse times from the former and the accuracy from high- and
than the low-scoring group. Figure 2 also shows that the rare
                                                                                             low-performers in the latter experiment. Shown in Figure 5,
incorrect RTs for the high-performing group get slower over
                                                                                             we see a similar pattern across experiments, and the over-
the course of the experiment, whereas the low-performing
                                                                                             all mistakes per position in the RL experiment and the cor-
group’s incorrect RTs only increase a bit. The strikingly slow
                                                                                             rect RTs in the cued experiment are significantly correlated
mistakes of high-performing participants, compared to mis-
                                                                                             (r = 0.64, t(8)=2.36, p < .05), with detailed comparisons be-
takes that are barely slower than correct movements for the
                                                                                             low. We also compared the data to the Simple Condensator
low performers may indicate a different mode of behavior. A
                                                                                             Model (SCM) proposed by Boyer et al. (2005), which imple-
   1 33% of chance success in one try (+1), 33% chance of success in                         ments a negative recency bias: expectation (activation) for ev-
two tries (-1+1), and 33% chance of success in three tries (-1-1+1).                         ery response builds at each step until a given response occurs,
                                                                                       195

                                                                                                                                                                                          125
                                           2500
       Mean of Median Response Time (ms)
                                                                                                                                                                                          100
                                                                                                                                                                Mean Number of Mistakes
                                           2000
                                                                                                                                                                                          75
                                                                                                 Group                                                                                                                                                                Group
                                                                                                  ●   High                                                                                                                                                             ●   High
                                           1500                                                       Low                                                                                                                                                                  Low
                                                                                                                                                                                          50
                                                  ●
                                           1000
                                                                                                                                                                                          25                                       ●
                                                      ●   ●         ●    ●              ●   ●
                                                                                    ●                                                                                                                                                              ●
                                                                                                                                                                                                    ●       ●                                               ●
                                                               ●                                                                                                                                                     ●
                                                                                                                                                                                                                                                       ●
                                                                             ●                                                                                                                                                ●         ●
                                                                                                                                                                                                                                             ●
                                            500
                                                  1   2   3    4    5    6    7     8   9   10                                                                                                      1       2        3        4    5    6    7     8   9    10
                                                              Sequential Position                                                                                                                                            Sequential Position
Figure 3: Mean of subjects’ median correct response times by me-                                                   Figure 4: The mean number of mistakes made per block by position
dian split and sequential position. The correct RTs for the two                                                    in the sequence split by performance group. The errors are highly
performance groups were not significantly correlated (r = 0.17,                                                    correlated (r =.79, t(8)=3.68, p < .01), though note how much worse
t(8)=0.48, p = 0.65). Low-scorers were slowest at position 5, fol-                                                 sequence position 5 was for the low-performing group relative to
lowed by 2 and 8, whereas high-scorers were worst at position 1,                                                   the next-worst position (8). Low-performers showed twice as many
and almost consistently fast besides that. Error bars reflect +/-1SE.                                              errors in position 5 as in 8, while the high-performing group showed
                                                                                                                   only a 25% increase in errors. Error bars reflect +/-1SE.
and activation resets. Thus, stimuli that have been used least
recently have highest activation—and fastest RTs. The SCM
                                                                                                                                                                            3
                                                                                                                        Standardized Cued RTs and RL Mistakes
has previously been shown to correspond closely to human
cued SRT responses, and Figure 5 shows it mirrors mistakes
in both groups of the RL experiment quite well.                                                                                                                             2
   We examined mistakes and correct response times by their                                                                                                                                                                                                     Experiment
sequential position, and compared these to RTs from the pre-                                                                                                                                                                                                     ●   Cued
vious cued SRT study. Overall, there is a significant correla-                                                                                                              1                                                                                        RL High
                                                                                                                                                                                                    ●                                                                RL Low
tion (r = .88, t(8)=5.37, p < .001) between correct RTs from                                                                                                                                                                  ●
the RL study and RTs from the cued SRT study. Comparing                                                                                                                                     ●                                               ●                        SCM
                                                                                                                                                                            0                                                                      ●
the cued RTs to the high- and low-scoring groups separately                                                                                                                                                 ● ●                                        ●
                                                                                                                                                                                                                                   ●
revealed a difference between the groups. The cued SRT RTs                                                                                                                                                                              ●
do not correlate significantly with the high-scoring group’s                                                                                                    −1
RTs (r = .51, t(8)=1.68, p = .13), but do correlate signifi-
cantly with the number of mistakes made in the RL study
                                                                                                                                                                                                1       2       3        4     5    6   7    8     9   10
(r = .83, t(8)=4.18, p < .01). The low-scoring group shows                                                                                                                                                          Sequential Position
the opposite pattern. The cued SRT RTs correlated signifi-
cantly with the RL correct RTs (r = .80, t(8)=3.79, p < .01)                                                       Figure 5: Scaled mean number of mistakes in the current experiment
but not with the RL mistakes (r = .57, t(8)=1.96, p = .09).                                                        against scaled correct RTs from a cued SRT study Kachergis et al.
                                                                                                                   (2014b) by sequence position. Error bars show +/-1SE.
Comparing the two groups with each other revealed a signifi-
cant correlation in mistakes (r = .79, t(8)=3.68, p < .01), but
no significant correlation in RT (r = .17, t(8)=0.48, p > .05).                                                    setup). The environment contains all data regarding the tar-
   The SCM was strongly correlated with the low-scoring RL                                                         gets, which it passes to the task, which in turn passes the cur-
group’s RTs (r = .93, t(8) = 6.93, p < .001), but not with the                                                     rent state of the environment to the agent, which selects the
high-scoring RL group’s RTs (r = .16, t(8) = 0.45, p = .67).                                                       relevant action. The action is evaluated by the environment,
The low-scorers, failing to discern the repeating sequence,                                                        which updates itself and passes a reward to the agent. The
may have mostly used a negative recency bias. The SCM was                                                          reward is used to update the agent’s strategy, and the model
correlated with the mistakes of both RL groups (low: r = .89,                                                      continues with the next step. We defined the reinforcement
t(8) = 5.52, p < .001; high: r = .74, t(8) = 3.12, p = .01).                                                       learning SRT task in this framework for our simulations.
Models of sequence learning To compare human sequence                                                                 As in the human experiment, the data regarding the targets
acquisition with existing reinforcement learning models, we                                                        was only partially-visible to the agent. The task acted as a
implemented the models using PyBrain (Schaul et al., 2010;                                                         veil through which a certain state would be observable. To a
see Figure 6 for an overview of the modeling experiment’s                                                          human participant, the current position in the sequence would
                                                                                                             196

                             Experiment                                 γ) was used to find optimal values.
                                                                           The best parameters found for the SARSA model (α = .01,
                 Task
                             Environment
                                                                        γ = .98) achieved a mean final score of 200 (sd=218). The
                                                                        best parameters found for Q-learning (α = .38, γ = .98)
                                                                        yielded a mean final score of 290 (sd=116), while Q(λ)
               observation    action     reward
                                                                        reached a mean final score of 451 (sd=34, parameters: α =
                 Agent                                                  .001, γ = .95, λ = .99). However, despite considerable learn-
                                                                        ing by the end of the experiment, none of the models per-
                                                                        formed as well as the high-performing human learners, who
Figure 6: Overview of the experimental setup for the RL models.         averaged a final score of 652. Even the maximum scores
Each plated component is a PyBrain class, which interact with each      achieved by the models were below the high-scoring hu-
other according to the arrows to simulate the same trial-and-error
learning process that humans undergo.                                   mans average or maximum (human= 725; Q-learning= 518,
                                                                        Q(λ)=557; SARSA= 546).
                                                                           Although these common RL models were unable to reach
be obvious, as it was colored differently from the other stim-          human-level performance, we thought it worthwhile to exam-
uli. At a minimum, the immediately prior occupied position              ine whether their error patterns resemble those of people. The
was probably obvious as well, readily available in memory.              mistakes made by the SARSA and Q-learning algorithms did
Positions preceding that, however, might not be reliably ac-            not vary much by sequence position, and while Q(λ) made
cessible in memory. In the sequence we used (4231324321),               more mistakes in the middle of the sequence (vaguely like hu-
following (Nissen & Bullemer, 1987), each position’s iden-              mans), none of the models’ error patterns were significantly
tity is fully determined by the previous two positions. That            correlated with humans.
is, one could perfectly predict the next position given only                                      Discussion
the two prior to it–assuming one has determined that there              We adapted the trajectory SRT paradigm to be a reinforce-
is a deterministic, periodically-repeating sequence. The RL             ment learning task. The task proved to be more challeng-
models we use rely on a set of third-order observations, as-            ing for some than for others, as indicated by a bimodal dis-
suming that the models know their current position and the              tribution of scores, and differences in the high- and low-
two prior positions.                                                    performing groups’ response times (RTs) and mistakes by se-
   The models differ in their learning component, which is              quence position. These data may suggest that participants
contained within the agent and maintains a mapping be-                  adopt different strategies, discussed in greater detail below.
tween states and action-values. For each given input-state              Overall, the findings of the reinforcement learning paradigm
there are three action-values, corresponding to the number              are similar to a previous trajectory SRT experiment with cued
of movements that can be made by the agent. Upon re-                    targets: RT and accuracy were correlated across experiments.
ceiving a reward, the agent updates the action-values using             In particular, data from the high-performing participants com-
its learning algorithm. We tested three learning algorithms:            pared remarkably well to the previous trajectory SRT study,
SARSA (Rummery & Niranjan, 1994), standard Q-learning,                  despite the task differences. The most notable similarity was
and Q(λ)–Q-learning with eligibility traces (Watkins, 1989).            the difficulty participants experienced with the fifth stimulus
Q-learning is an off-policy algorithm, learning about the               position.
greedy policy, updating old action-values using the maximum                The better-performing half of participants made very few
of all action-values for the current state, while it stochastically     errors after as little as 10 repetitions of the length 10 se-
selects actions, sometimes exploring. SARSA is on-policy:               quence. Block-by-block analysis of the RTs by performance
instead of the maximum, it also takes into account the ac-              group showed a difference in speed-up across the experi-
tion it has selected for the current state. The eligibility traces      ment: the high-performing group already made faster correct
in Q(λ) are temporary records of an event (e.g., an action or           responses in the second block of ten repetitions, and main-
state) that help with temporal credit assignment by adding a            tained this advantage. The difference in response times to
trace to events that are eligible for learning updates. Theoret-        incorrect targets suggests the two groups might have used
ically, eligibility traces link RL temporal difference methods          different strategies. The rare but increasingly-slow mistakes
(like Q-learning and SARSA) to Monte Carlo methods.                     in the high-performing group may indicate retrieval attempts
   These algorithms were chosen as simple baselines that dif-           and an awareness of their uncertainty as to the next step.
fer somewhat in exploratory behavior and learning speed, and            In contrast, the persistent and relatively fast mistakes of the
thus may be suitable to compare to human behavior which                 low-performing group suggest these participants may have
varied widely. As with the human participants, the simu-                adopted a probabilistic view of the task, randomly trying op-
lated SARSA and Q-learners were tasked with iterating over              tions instead of trying to learn a deterministic pattern. This
the repeated sequence until the successful completion of 800            was corroborated by the particular success of the parameter-
movements. For each model, a grid search over the parame-               free Simple Condensator Model (SCM) in matching the low-
ters (learning rate α and discounting factor for future rewards         scoring group’s RTs using a simple negative recency bias. A
                                                                    197

distinction of stimulus-based and plan-based control (Tubau,                                Acknowledgments
Hommel, & López-Moliner, 2007) may capture the apparent             The preparation of this work was supported by the Euro-
differences between the low- and high-performing groups.             pean Commission (EU Cognitive Systems project ROBO-
   The results by sequence position showed that participants         HOW.COG; FP7-ICT-2011).
in both groups had more trouble with the target in sequence                                     References
position five than any other target. This is similar to the pat-     Botvinick, M., & Plaut, D. C. (2004). Doing without schema hier-
tern observed in previous studies (Nissen & Bullemer, 1987;            archies: A recurrent connectionist approach to routine sequential
                                                                       action and its pathologies. Psychological Review, 111, 395–429.
Kachergis et al., 2014b), and has previously been taken to           Boyer, M., Destrebecqz, A., & Cleeremans, A. (2005). Processing
indicate that participants chunk the sequence into two parts:          abstract sequence structure: Learning without knowing, or know-
the initial 4-2-3-1 and the final 4-3-2-1, with positions 5 and        ing without learning? Psychological Research, 69, 383–398.
                                                                     Cleeremans, A., & McClelland, J. L. (1991). Learning the structure
6 bridging the two chunks. We note that the only repeated              of event sequences. Journal of Experimental Psychology: Gen-
transition in the sequence (3-2, at positions 6 and 9)–which           eral, 120, 235–253.
might be expected to be worse due to the higher transition           Cooper, R. P., & Shallice, T. (2000). Contention scheduling and the
                                                                       control contention scheduling and the control of routine activities.
probability–shows neither slow correct responses nor more              Cognitive Neuropsychology, 17(4), 2987–338.
mistakes for either position it occurs in, somewhat unlike           Dominey, P. F. (1998). A shared system for learning serial and
(Nissen & Bullemer, 1987). Models make the clearest predic-            temporal sturcture of sensori-motor sequences? evidence from
                                                                       simulation and human experiments. Cognitive Brain Research,
tions, but in the absence of a winning theory, a comparison to         6(163), 163–172.
other paradigms can also be illuminating.                            Elman, J. L. (1990). Finding structure in time. Cognitive Science,
                                                                       14(2), 179–211.
   Despite the major difference of no cueing of the next re-         Falkenstein, M., Hohnsbein, J., Hoormann, J., & Blanke, L. (1991).
sponse, performance in the RL experiment was quite compa-              Effects of crossmodal divided attention on late {ERP} compo-
rable to performance in the cued SRT study. The correlations           nents. ii. error processing in choice reaction tasks. Electroen-
                                                                       cephalography and Clinical Neurophysiology, 78(6), 447–455.
of mistakes and RTs by sequence position indicated a differ-         Fu, Q., Fu, X., & Dienes, Z. (2008). Implicit sequence learning and
ence between the low- and high-performing groups that was              conscious awareness. Consciousness and Cognition, 17, 185–
not immediately obvious. Overall, the cued SRT response                202.
                                                                     Gehring, W. J. (1992). The error-related negativity: Evidence for a
times are correlated to RTs and accuracy in the RL exper-              neural mechanism for error-related processing (Unpublished doc-
iment, whereas this is not true for both the low- and high-            toral dissertation). University of Illinois at Urbana-Champaign.
performing groups separately. The low-scoring group closely          Holroyd, C. B., & Coles, M. G. (2002). The neural basis of hu-
                                                                       man error processing: Reinforcement learning, dopamine, and the
matched the negative recency bias of the SCM (in mistakes,             error-related negativity. Psychological Review, 109(4), 679 - 709.
but especially in RT), but the pattern and strategy of the high-     Hommel, B., Müsseler, J., Aschersleben, G., & Prinz, W. (2001).
scoring group is less clear.                                           The theory of event coding (TEC): A framework for perception
                                                                       and action planning. Behavioral and Brain Sciences, 24, 849–
   Hoping to better understand especially the high-                    937.
performing group, we developed a simple reinforcement                Kachergis, G., Berends, F., de Kleijn, R., & Hommel, B. (2014a).
                                                                       Reward effects on sequential action learning in a trajectory serial
learning model and tested three different learning algo-               reaction time task. IEEE Conference on Development and Learn-
rithms. High-performing humans were still far better than              ing / EpiRob 2014.
the models, which on average scored roughly as well as               Kachergis, G., Berends, F., de Kleijn, R., & Hommel, B. (2014b).
                                                                       Trajectory effects in a novel serial reaction time task. Proceedings
the low-performing humans. SARSA had quite variable                    of the 36th Annual Conference of the Cognitive Science Society.
performance, but was lowest on average, while Q-learning             Nissen, M. J., & Bullemer, P. (1987). Attentional requirements of
with eligibility traces fared the best. Examining the models’          learning: evidence from performance measures. Cognitive Psy-
                                                                       chology, 19, 1–32.
performance by sequence position showed they did not                 Rummery, G. A., & Niranjan, M. (1994). On-line q-learning us-
correspond well with errors in either group of humans. This            ing connectionist systems (Tech. Rep. No. CUED/F-INFENG/TR
suggests that simple model-free reinforcement algorithms               166). Cambridge University.
                                                                     Saffran, J., Newport, E., & Aslin, R. (1996). Word Segmentation:
do not capture the process by which humans learn action                The Role of Distributional Cues. Journal of Memory and Lan-
sequences, even though they eventually converge on a proper            guage.
solution. One explanation for this is the fact that the task         Schaul, T., Bayer, J., Wierstra, D., Sun, Y., Felder, M., Sehnke, F., . . .
                                                                       Schmidhuber, J. (2010). PyBrain. Journal of Machine Learning
and models used in studies like this do not fully capture the          Research, 11, 743–746.
essence of human action learning, which is goal-directed by          Skinner, B. F. (1950). Are theories of learning necessary? Psycho-
nature. Future studies could shed light on the role of goals in        logical Review, 57(4), 193–216.
                                                                     Stadler, M. A. (1995). The role of attention in implicity learning.
the acquisition of such action sequences, as has been shown            Journal of Experimental Psychology: Learning, Memory, & Cog-
to exist for single-step action (see, for example, Hommel,             nition, 21, 674–685.
Müsseler, Aschersleben, and Prinz (2001) for one proposed           Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An
                                                                       introduction. Cambridge, MA: MIT Press.
mechanism of goal-directed action). The process by which             Tubau, E., Hommel, B., & López-Moliner, J. (2007). Modes of
humans acquire action sequences is subtle, can yield quite             executive control in sequence learning: From stimulus-based to
variable performance, and is not easily captured by simple             plan-based control. Journal of Experimental Psychology: Gen-
                                                                       eral, 136, 43–63.
learning algorithms. However, studying it is important, as           Watkins, C. J. C. H. (1989). Learning from delayed rewards (Un-
most of human behavior is essentially sequential in nature.            published doctoral dissertation). Cambridge University.
                                                                 198

