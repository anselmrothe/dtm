   Combining Multiple Perspectives in Language Production: A Probabilistic Model
           Mindaugas Mozuraitis                              Suzanne Stevenson                     Daphna Heller
           Dept. of Computational Linguistics                Dept. of Computer Science             Dept. of Linguistics
             & Phonetics, Saarland University                University of Toronto                 University of Toronto
           mindauga@coli.uni-saarland.de                     suzanne@cs.toronto.edu                daphna.heller@utoronto.ca
                              Abstract                                  referent (Isaacs & Clark, 1987; Heller et al., 2012). In sum,
   While speakers tailor referring expressions to the knowledge
                                                                        speakers do not fully adapt to addressees’ knowledge, and
   of their addressees, they do so imperfectly. Our goal here is to     production often reflects some egocentric tendencies.
   provide an explanation for this type of pattern by extending a          The goal of this paper is to propose an explanation for
   probabilistic model introduced to explain perspective-taking         these “mixed” patterns. Specifically, we extend to cover
   behavior in comprehension. Using novel production data from          language production a computational model previously
   a type of knowledge mismatch not previously investigated in          introduced to explain reference comprehension through the
   production, we show that production patterns can also be             probabilistic combination of speakers’ and addressees’
   explained as arising from the probabilistic combination of the
   speaker’s and the addressee’s perspectives. These results            perspectives (Heller et al., 2016). We first present a new
   show the applicability of the multiple-perspectives approach         production experiment, and then turn to modeling its results.
   to language production, and to different types of knowledge          While much research on reference production has
   mismatch between conversational partners.                            considered knowledge mismatch due to differences in visual
   Keywords: language production; computational modeling;               perspectives (e.g., Horton & Keysar, 1996; Nadig & Sedivy,
   reference; audience design; common ground; perspective-              2002; Wardlow Lane & Ferreira, 2008; Yoon et al., 2012),
   taking; pragmatics; probabilistic models.                            our experiment involves a case where knowledge differs
                                                                        with respect to the function of objects. The predictions of
                          Introduction                                  our model, which combines the contribution of the speaker’s
The process whereby speakers tailor their utterances to the             perspective and the addressee’s perspective, are a good fit to
knowledge state of their addressee is known as “audience                the human data.
design”. Much research on audience design has focused on
reference – i.e., the labeling of objects. Reference is an ideal                         Production Experiment
test bed for audience design, because of the clear action               Our experiment investigates the production of referring
associated with referring: the speaker needs to produce a               expressions in cases where the speaker and the addressee
linguistic form that enables her addressee to identify the              have differing knowledge about the function of an object.
intended object. For example, a speaker should only use the             To create such knowledge mismatch, we use Visually-
name Aloysius if she can assume that her addressee will be              Misleading Objects [VMOs] whose appearance does not
able to map this name onto the intended person. More                    match their function, such as a crayon that is shaped to look
generally, in order to be understood, a speaker should rely             like a Lego block (similar objects were used in Mozuraitis et
on shared information when choosing a referring expression              al., 2015, a comprehension study). One novel aspect of our
(Clark & Marshall, 1981).                                               design is that it examines the effects of knowledge
   Indeed, psycholinguistic research has shown that speakers            mismatch about the VMOs’ function indirectly: As shown
rely on shared information when choosing a referring                    in Figure 1(a), these objects were not described by
expression (e.g., Nadig & Sedivy, 2002). However, their                 participants (i.e., they were not the target). Instead, they
referring expressions are not always based on shared                    were paired with a target (a typical object) that either
information alone. For example, when producing language                 matched their appearance (e.g., an actual Lego block) or that
under time pressure, speakers do not distinguish between                shared their function (e.g., a regular crayon).
shared information and their own privileged knowledge                      What happens when both interlocutors have been
(Horton & Keysar, 1996). But even when speakers do                      demonstrated the function of the VMO (the shared
distinguish the two, their utterances are sometimes formed              condition)? If the VMO is categorized based on its
using privileged information. For example, when there is                appearance (e.g., as a Lego), then there are two Legos in the
one shared triangle and a larger triangle known only to the             appearance condition (Fig. 1(a-1)), and a referring
speaker, speakers sometimes say the small triangle, even                expression should include a modifier to distinguish the
though for the addressee it would be sufficient to use the              appearance-target from the contrast (e.g., the Lego on your
unmodified expression the triangle (Wardlow Lane &                      left). If the VMO is categorized based on its function (e.g.,
Ferreira, 2008; Yoon et al., 2012). Furthermore, when their             as a crayon), then there are two crayons in the function
addressee does not know a name, speakers sometimes                      condition (Fig. 1(a-3)), thus a modified referring expression
include it nonetheless, along with the descriptive                      should be used to distinguish the function-target from the
information that would allow the addressee to identify the              contrast (e.g., the longish crayon). Importantly, these are not
                                                                        mutually exclusive: it is possible that both appearance and
                                                                    2123

function play into the categorization of objects, and speakers           conditions, the target shared the function of the VMO (e.g.,
will use a modified expression in both cases.                            a regular crayon with the Lego-crayon; see Fig. 1(a-3)).
    (a) Target x Contrast conditions (b) Example critical display
                                                                            Type of contrast. In the visually-misleading (critical)
                                                                         conditions, the contrast was one of 8 VMOs (appearance-
             Target      Contrast
           Appearance     VMO
                                                                         function): Lego-crayon, paintbrush-eraser, baseball-yoyo,
      1)                Lego-crayon                                      cigarette-pencil,
                                                                            distractor&1&        book-box,
                                                                                                 distractor&2&   lighter-pencil sharpener, pen-
                                                                         screwdriver, lightbulb-candle. In the control conditions, the
           Appearance    Control                                         VMOs were replaced by objects from an unrelated category
      2)
                                    A"                                B" that were similar to the VMOs in size and color (Fig. 1(a-2)
            Function      VMO                                            and 1(a-4)). The control conditions were intended to provide
      3)                Lego-crayon
                                        Contrast
                                                                         baseline
                                                                               v.m&
                                                                                         modification rates for the target objects when the
                                                            Target                                  target&
                                          VMO /         Appearance /     display
                                                                             object& does not contain
                                                                                                    func+on& any contrasting objects.
                                         control           Function
            Function     Control                                            A list design cycled the pairing of target and contrast
      4)
                                                                         objects such that participants saw a given array only once.
                                                                         Across participants, each array occurred in all of the
                   Figure 1: Sample materials
                                                                         experimental conditions. Across the experiment, target and
   How will this pattern change (if at all) when only the                contrast objects appeared in each of the four display
speaker has been shown the function of the VMOs (the                     positions equally often. To counteract any contingencies
privileged condition)? As demonstrated in Mozuraitis et al.,             created by the critical items, we added 24 filler displays. To
(2015), when people are not demonstrated the function of                 ensure that VMOs are not always relevant for the
these objects, they categorize them based on appearance                  instruction, two fillers had a VMO paired with an object
(e.g., as a Lego). Thus, if speakers (participants) tailor the           matching its appearance or function, but neither was the
referring expression to the perspective of the addressee                 target. To have speakers refer to strange objects, eight fillers
(who can be assumed to think that the VMO is a Lego), they               had a difficult-to-name object (not VMOs), with six of those
should use a modified expression when referring to the                   as the target. To divert attention from appearance and
appearance-target (e.g., the Lego on your left), but not with            function contrasts, ten fillers had a pair of objects
the function-target (which they will simply call the crayon).            contrasting in materials, with four as the target. The final
                                                                         four fillers had four unrelated objects. Trials were presented
Method                                                                   in random order with no two consecutive critical trials.
Participants We report data from 80 native English                       Procedure Participants (always the speaker) were led to
speakers from the University of Toronto community, paid                  believe the confederate was a naïve participant, and that
$10 each. An additional 7 participants were excluded                     they were assigned to their respective roles arbitrarily.
because they failed to follow instructions (n = 3) or they               Following Kuhlen & Brennan (2013), we used confederates
reported a suspicion that their partner was not naïve (n = 4).           blind to the purpose of the experiment. Two confederates
Materials and design All displays contained four objects                 participated in the shared condition. A third confederate
(e.g., in Figure 1b the distractor objects are a pinecone and a          participated in the privileged condition, to ensure she had
wand). Three factors were manipulated in a 2 x 2 x 2 design:             not previously seen the function of the VMOs.
Knowledge state (shared vs. privileged; between-                            Partners were seated on opposite sides of a table with a
participants), type of target (appearance vs. function; within-          3x3 cubbyhole display (Fig. 1(b)); the middle cubbyhole
participants), type of contrast (visually-misleading vs.                 was covered to avoid eye contact that might reveal
control; within-participants).                                           referential intent. Each trial began by the experimenter
   Knowledge state. In the shared conditions (n=40), at the              demonstrating the function of objects non-verbally, and
beginning of each trial, the function of the VMO (and other              placing them in the display. Then, the speaker saw a picture
objects) was demonstrated non-verbally: e.g., the                        on a computer monitor (not visible to the addressee),
experimenter drew on a piece of paper with the crayon that               indicating the object to be moved and its target location.
looks like a Lego, while both the speaker (participant) and              Speakers were given no further instructions, except to
the addressee (confederate) were facing the objects. In the              refrain from pointing or other gestures. Confederates did not
privileged conditions (n=40), the function of objects was                act until speakers completed instructions (they were
demonstrated only to the speaker, while the addressee                    instructed to this effect during their training).
(confederate) faced away from the display (and wore
headphones). In the latter condition, the goal was to give               Results
clear and consistent cues to the speaker that the addressee is           The dependent variable was whether speakers’ referring
unaware of the true (unexpected) function of VMOs.                       expressions for the target included a modifier (e.g., the Lego
   Type of target. In the appearance conditions, the target              on your left or the smaller Lego, coded as 1) or not (e.g., the
shared the appearance of the VMO (e.g., a regular Lego                   Lego, coded as 0). The data were analyzed using a mixed-
with the Lego-crayon; see Fig. 1(a-1)). In the function                  effects logistic regression model with participants and items
                                                                         as crossed, independent, random effects. We used models
                                                                    2124

with the maximal random-effect structure that led to                 addressee did not know the real function of the VMO, and
convergence. The model that converged included random                thus would expect the VMO to have a function consistent
intercept for participants and random intercept for items.           with its appearance.
   The 2x2x2 model (summarized in Table 1) revealed a                                                        Contrast = Control                                                    Contrast = VMO
main effect of contrast type: as expected, speakers were                                          1.0
                                                                                                  0.9
                                                                                                                                                                          1.0
                                                                                                                                                                          0.9
                                                                                                                                                                                       0.94    0.92
overall more likely to use modifiers in referring to the target                                   0.8                                                                     0.8   0.75
                                                                     Proportion of Modification                                              Proportion of Modification
when it occurred with a VMO than with a control object                                            0.7                                                                     0.7                              0.65
(.82 vs. .16). There were 3 significant interactions: Target                                      0.6                                                                     0.6
type x Contrast type, Target type x Knowledge state, and the                                      0.5
                                                                                                  0.4
                                                                                                                                                                          0.5
                                                                                                                                                                          0.4
3-way interaction. Here we focus on unpacking the 3-way                                           0.3                                        0.26
                                                                                                                                               0.3
interaction, both for theoretical reasons, and because the 2-                                     0.2                0.15        0.18                                     0.2
way interactions are subsumed by it.                                                              0.1     0.06                                                            0.1
                                                                                                  0.0                                             0.0
  Table 1: The 2x2x2 model. Significant effects are bolded.                                              Shared− Privileged−
                                                                                                        Appearance Appearance
                                                                                                                                Shared−
                                                                                                                                Function
                                                                                                                                           Privileged− Shared− Privileged−
                                                                                                                                            Function Appearance Appearance
                                                                                                                                                                                              Shared−
                                                                                                                                                                                              Function
                                                                                                                                                                                                         Privileged−
                                                                                                                                                                                                          Function
 Effect                               β     SE    z       p                                                         Figure 2: Modification rates produced.
Knowledge                           0.29   0.32 0.90 0.367
Target                             -0.32   0.26 -1.24 0.215            Taken together, the crossover interaction indicates that
Contrast                            4.06   0.33 12.46 <0.001         speakers were sensitive to the knowledge state of their
Knowledge x Target                  2.20   0.53 4.18 <0.001          addressee in tailoring referring expressions. But note that if
Knowledge x Contrast               -0.93   0.53 -1.74 0.081          speakers in the privileged condition completely adapted to
Target x Contrast                   1.40   0.52 2.69 0.007           the addressee’s perspective, they should not modify at all in
Knowledge x Target x Contrast       3.50   1.05 3.33 <0.001          the VMO-privileged-function condition (rightmost column
   Separate follow-up analyses were conducted for the two            in Figure 2): this is because the function-target (a crayon)
types of contrasts – see Figure 2. When the contrast was a           would not need to be distinguished from the VMO (the
control object, there was a main effect of target type               Lego-crayon that the addressee did not know was a crayon).
(β=-1.07, SE=0.37, z=2.88, p=0.004), indicating that                 However, speakers used more modifiers in this case than in
participants were overall more likely to use a modified              the corresponding control condition (.65 vs. .26; β=1.84,
expression to refer to the function-target rather than the           SE=0.41, z=4.52, p<0.001). This difference reveals that
appearance-target (.22 vs. .11); recall these are simply             speakers did not completely adapt to the addressee’s
different objects and thus the pattern is not meaningful             perspective. Instead, this pattern reflects the consideration of
beyond providing a baseline. The main effect of knowledge            the addressee’s perspective along with their own.
state was marginal (p=0.074): there was a trend for speakers
modifying more in the privileged conditions. However, the                                                             The Probabilistic Model
Target type X Knowledge state interaction was not                    The probabilistic model of Heller et al. (2016) was
significant (p=0.581), indicating that, in the absence of a          developed to account for apparently-inconsistent results in
contrasting object, the knowledge state manipulation did not         the literature on perspective-taking in comprehension. The
change the modification pattern differentially for the               approach models reference resolution (i.e., the
function and appearance targets.                                     comprehension of a referring expression) as the probability
   When the contrast was a VMO, the Target type X                    P(obj|RE) that a certain object obj is the referent intended by
Knowledge state was significant (β=4.08, SE=0.83, z=4.90,            the speaker, given a referring expression RE. Following
p<0.001; main effects were not, ps>0.250). Pairwise                  standard practice, the probability on the left is rewritten
comparisons revealed that when the target matched the                using Bayes rule, as a product of two probabilities;
VMO in function, participants were less likely to use                importantly, the component probabilities are then
modified expressions to describe it in the privileged                conditioned on the domain of reference (d):
condition (.65 vs. .92; β= -1.95, SE=0.49, z=-3.95, p<0.001).
This pattern indicates that speakers adapted to the                                                              P(obj|RE) =def αP(RE|obj,d=e)P(obj|d=e) +
addressee’s perspective: they were less likely to use a                                                                         (1-α)P(RE|obj,d=c)P(obj|d=c) (1)
modifier to differentiate the target (e.g., a typical crayon)        An important aspect of this model is the conditioning on the
from the VMO (e.g., the Lego-crayon) when they had no                domain d. This conditioning captures the fact that reference
reason to assume that the addressee knew these shared their          depends not just on what the intended referent is, but also on
function. When the target matched the VMO in                         what other objects need to be distinguished from the
appearance, the pattern was reversed: participants were              referent. The likelihood P(RE|obj,d) captures the preference
more likely to use modifiers in the privileged condition (.94        for using the referring expression RE to describe the various
vs. .75; β=2.33, SE=0.92, z=2.53, p=0.012). This pattern             objects obj in domain d; the prior P(obj|d) captures the prob-
further demonstrates that speakers adapted to the                    ability of obj being referred to in the context of domain d.
addressee’s perspective: they were more likely to use a                A second important aspect of the model is that reference
modifier to differentiate the target (e.g., a regular Lego)          resolution is guided by both the common ground perspective
from the VMO (e.g., the Lego-crayon), when they knew the             c (those entities which both the speaker and the addressee
                                                                  2125

can see; which was identical to the speaker’s perspective),         calculate the probability of using modification,
and the egocentric perspective e (all the objects the               P(RE=MOD|obj=target), abbreviated as P(MOD|target).
addressee can see). This contrasts with other Bayesian
                                                                    The Domains D We consider the set D of multiple
models of reference, which have not considered the
                                                                    domains that influence the speaker’s formulation of
combined influence of the speaker’s and addressee’s
                                                                    referring expressions to consist of two domains: the domain
perspectives (e.g., Frank & Goodman, 2012; Goodman and
                                                                    s is the speaker’s (egocentric) perspective, and the domain a
Stuhlmüller, 2013; Kehler & Rohde, 2013). These two
                                                                    is the addressee’s perspective. Thus, D={s, a}. In general,
domains are weighed in formula (1) by α and (1-α),
                                                                    d=s corresponds to the objects and their properties known
respectively, to ensure the combination of the component
                                                                    by the speaker, and d=a corresponds to the objects and
probabilities forms a probability distribution.
                                                                    properties known by the addressee. Importantly, when the
   In order to model language production, we extend Heller
                                                                    addressee did not know the true function of a VMO
et al.’s (2016) model in two ways. First, since we are
                                                                    (privileged condition), we assume the domain a reflects the
modeling the speaker’s choice of referring expression rather
                                                                    speaker’s assumption about the addressee’s false belief.
than the addressee’s search for a referent, the probability of
                                                                    (Note that Heller et al. (2016) used different labels for the
interest is P(RE|obj) rather than P(obj|RE) – that is, we
                                                                    perspectives of the interlocutors in modeling comprehen-
directly model the preference for various referring
                                                                    sion: domain e for the egocentric domain of the addressee
expressions RE assuming that the object obj to be referred to
                                                                    and domain c for the common ground, which was identical
is a given. In this case, there is no need to apply Bayes rule.
                                                                    to the speaker’s perspective.)
Second, we observe that the use of the domain-weighting
constant α in the Heller et al. formulation can be avoided:         The Resulting Probability Formula With the above
we can rewrite P(RE|obj) by marginalizing over all possible         variable values, we can instantiate formula (2) as:
values of the domain variable d (where D is the set of                  P(MOD|target) = ∑ d∈{s, a} P(MOD|target, d)P(d)         (4)
possible referential domains for production):
                                                                    Showing the sum over the two possible domains yields the
             P(RE|obj) = ∑ d∈D P(RE|obj,d)P(d)              (2)
                                                                    following formula to model the experimental data:
Marginalizing over d provides a well-motivated way within                  P(MOD|target) = P(MOD|target, d=s)P(d=s)
probability theory to condition the probability of the RE on                                + P(MOD|target, d=a)P(d=a)         (5)
the domain d when its value is not a given. By capturing the
degree of influence of each domain d in the probability P(d),       Estimating the Probabilities
the formula in (2) more naturally encodes the kind of               P(MOD|target, d) Because our goal is to use the multiple
domain weighting that Heller et al. achieved with the ad hoc        domains approach to explain data patterns obtained under
constant α in formula (1). This approach also provides a            knowledge mismatch (i.e., those in the critical Privileged
more general formulation that can be readily extended to the        conditions) we derive probabilities from the Control and
case of more than two domains. It is important to note that         Shared conditions, and see whether their application in the
formula (1) of Heller et al. can be recast within this same         Privileged conditions obtains the observed data pattern.
marginalization approach as:                                           Specifically, we take each probability P(MOD|target,d) in
                                                                    one of the critical experimental conditions to reflect two
         P(obj|RE) ∝ ∑ d∈D P(RE|obj,d)P(obj|d)P(d)          (3)
                                                                    influences. First, a “baseline” level of modification holds for
where D={e, c}, and P(d=e) replaces α and P(d=c) replaces           different objects regardless of the presence of a VMO; this
(1-α). Thus, together the two probability formulas we               baseline is taken directly from the modification rates in the
propose, (2) and (3), provide a unified way of modeling             four Control conditions (we include these because of the
both production and comprehension of referring expressions          variation). Second, when the target needs to be
under the influence of multiple perspectives.                       distinguished from a contrasting VMO, as in the Shared
                                                                    conditions, there is an additional level of modification. We
          Instantiating the Probability Model                       thus take the Shared experimental conditions to reflect a
The Variables in the Probability Formula                            “typical” modification rate for when the target shares only
                                                                    appearance or only function with the VMO (Shared-
The Object obj The given object obj in our probability              Appearance and Shared-Function, respectively).
formula of Eqn. (2) will denote the referent: the object the           Next, we use these values to predict the behavior
speaker will label with the goal of the addressee choosing it.      observed in the two VMO-Privileged conditions. We set the
                                                                    value of the probability P(MOD|target,d) in each of the
The Referring Expression RE Because the dependent
                                                                    Privileged conditions to be the sum of:
variable we used in the experimental results was
                                                                       (i) the baseline level of modification obtained in the
modification rate, we consider RE for the purposes of
                                                                           corresponding Control condition, plus:
modeling to represent modified referring expressions. That
                                                                       (ii) the amount of modification due to the target sharing
is, instead of determining the preference for a specific
                                                                           either the Appearance or the Function of the contrast
referring expression, we use the probability formula to
                                                                           object, from the corresponding Shared condition.
                                                                2126

These component estimates are shown in Fig. 3a. The                                                        Fig. 3b and 3c are weighted equally in Eqn. 5
resulting values for the Privileged conditions are shown                                                   (P(d=s)=P(d=a)=0.5), we obtain the values for
below in Modeling the Production Data, where we consider                                                   P(MOD|target) across the four experimental conditions
the speaker’s and the addressee’s perspectives (other panels                                               illustrated in Fig. 3d. The levels of probability for the
of Fig. 3, described below).                                                                               Privileged conditions (which are the values we aim to
                                                                                                           predict) have a very good fit to the behavioral data (Fig. 2;
P(d) As in Heller et al. (2016), we consider that the
                                                                                                           recall that the values for the Shared conditions are set
language user’s weighting of each domain is not directly
                                                                                                           according to the human data.) With 0.25 < P(d=a) < 0.65,
observable. We determine the range of values of P(d) that
                                                                                                           the model yields values in the ranges consistent with the
yields a fit to the empirical data, and see if this range fits our
                                                                                                           95% confidence intervals for the Privileged conditions; the
hypothesis that speakers use both domains in formulating
                                                                                                           best fit is obtained with P(d=a)=0.47, which is very close to
referring expressions. Because we assume that d can only
                                                                                                           the equal combination in Fig. 3d. Importantly, because we
take on the values s and a, those values exhaust the
                                                                                                           are predicting two values (Privileged-Appearance and
probability space, and so P(d=s)+P(d=a)=1, or
                                                                                                           Privileged-Function) with one parameter (P(d=a)), this is
P(d=a)=1–P(d=b). Given that there is only one parameter to
                                                                                                           not trivial (if it were one data point, there would always be
consider here (the other value is the additive inverse), the
                                                                                                           some value for P(d=a) that would achieve the fit).
model needs to account for two patterns of modification
                                                                                                                 (a) Estimates from human data                                                                          (b) Speaker’s perspective
(i.e., in Privileged-Appearance and in Privileged-Function)                                                1.0                                                                                                    1.0                                              1.00
with one parameter setting for weighing perspectives. Thus,                                                0.9
                                                                                                                                                        0.92
                                                                                                                                                                                                                  0.9                  0.84
                                                                                                                                                                                                                                                    0.92
our experimental design provides a critical test of the model.
                                                                                                                                                                                     Proportion of Modification
                                                                                                           0.8                 0.75                                                                               0.8       0.75
                                                                              Proportion of Modification
                                                                                                           0.7                                                                                                    0.7
                                                                                                                                                                                                                                                                   0.74
           Modeling the Production Data                                                                    0.6
                                                                                                           0.5
                                                                                                                                          +0.74
                                                                                                                                                                                                                  0.6
                                                                                                                                                                                                                  0.5                  0.69
                                                                                                                                                                                                                                                    0.74
To model the data, we must consider what the speaker’s and                                                 0.4     +0.69                                                                                          0.4       0.69
addressee’s perspectives are regarding the objects and their                                               0.3                                                                                                    0.3
                                                                                                                                           0.18                                                                   0.2
relevant properties, with attention to the relation of the                                                 0.2
                                                                                                                                                                                                                  0.1                                              0.26
                                                                                                           0.1      0.06                                                                                                               0.15         0.18
VMO to the target. This relation dictates the component                                                    0.0
                                                                                                                                                                                                                  0.0       0.06
probabilities that must be added to the baseline preference                                                        Control    Shared
                                                                                                                 Appearance Appearance
                                                                                                                                          Control
                                                                                                                                         Function
                                                                                                                                                       Shared
                                                                                                                                                      Function
                                                                                                                                                                                                                          Shared− Privileged−
                                                                                                                                                                                                                         Appearance Appearance
                                                                                                                                                                                                                                                   Shared−
                                                                                                                                                                                                                                                   Function
                                                                                                                                                                                                                                                                Privileged−
                                                                                                                                                                                                                                                                 Function
for modification. Specifically, does the target share
                                                                                                                 (c) Addressee’s perspective                                                                            (d) Combined perspective
Appearance with the VMO, share Function with the VMO,                                                 1.0                                                                                         1.0
or share neither? The answer depends on the perspective.                                              0.9                     0.89        0.92
                                                                                                                                                                                                  0.9                                0.87
                                                                                                                                                                                                                                                  0.92
Speaker’s Perspective (P(MOD|target,d=s): Fig. 3b)
                                                                      Proportion of Modification                                                                  Proportion of Modification
                                                                                                      0.8          0.75                                                                           0.8                     0.75
                                                                                                      0.7                                                                                         0.7
Because the speaker always knows the true function of the                                             0.6                                                                                         0.6
                                                                                                                                                                                                                                                                0.63
VMOs, the speaker’s perspective is identical in the Shared                                            0.5                     0.74        0.74
                                                                                                                                                                                                  0.5                                0.72
                                                                                                                                                                                                                                                  0.74
                                                                                                                                                                                                                                                                0.37
and Privileged conditions. We calculate P(MOD|target,                                                 0.4          0.69
                                                                                                                                                      0.26
                                                                                                                                                                                                  0.4                     0.69
d=s) in the Appearance conditions by adding the shared-                                               0.3                                              0.00                                       0.3
                                                                                                                                                                                                  0.2
appearance probability (.69) to each corresponding baseline,                                          0.2
                                                                                                      0.1                                             0.26                                        0.1                                                           0.26
                                                                                                                              0.15        0.18                                                                                       0.15         0.18
and in the Function conditions by adding the shared-                                                  0.0          0.06                                                                           0.0                     0.06
function probability (.74) to each corresponding baseline.                                                        Shared− Privileged−
                                                                                                                 Appearance Appearance
                                                                                                                                         Shared−
                                                                                                                                         Function
                                                                                                                                                    Privileged−
                                                                                                                                                     Function
                                                                                                                                                                                                                         Shared− Privileged−
                                                                                                                                                                                                                        Appearance Appearance
                                                                                                                                                                                                                                                 Shared−
                                                                                                                                                                                                                                                 Function
                                                                                                                                                                                                                                                              Privileged−
                                                                                                                                                                                                                                                               Function
   Addressee’s Perspective (P(MOD|target,d=a): Fig. 3c)                                                                                             Figure 3: Modeling
In contrast to speakers, the addressee’s perspective changes
across the knowledge state manipulation. In Privileged-                                                                                               Discussion
Appearance, the similarity in appearance between the target
                                                                                                           We present the first model of reference production that
and the VMO (e.g., a Lego and a Lego-crayon) should lead
                                                                                                           probabilistically combines the influence of multiple
the addressee to assume that the objects also share function.
                                                                                                           perspectives. Our model explains the modification patterns
Thus, we add the shared-function probability (.74) to the
                                                                                                           observed in our production experiment as a result of
baseline to mimic the addressee’s assumed perspective
                                                                                                           speakers considering both their own perspective and the
(despite the fact that in actuality the objects share only their
                                                                                                           addressee’s perspective in choosing the form of a referring
appearance). In Privileged-Function (e.g., a crayon target
                                                                                                           expression. The modeling results support the hypothesis that
and Lego-crayon contrast), the addressee’s false belief
                                                                                                           speakers use both perspectives, because weighing the two
should lead to an assumption that the VMO shares neither
                                                                                                           perspectives about equally gives a good match to the human
appearance nor function with the target, and thus a value of
                                                                                                           data. A model where speakers are fully egocentric
0 is added to the baseline.
                                                                                                           (equivalent to P(d=a) being close to 0) or fully adapt to the
   Combining the Two Perspectives (P(MOD|target): Fig.
                                                                                                           addressee’s perspective (equivalent to P(d=a) being close to
3d) Our proposal is that the speaker probabilistically
                                                                                                           1) cannot predict the pattern observed. Furthermore, this
combines both their own perspective with the addressee’s
                                                                                                           result is attractive because we are using the same approach
perspective in considering the preference P(MOD|target) for
                                                                                                           that     successfully    modeled     perspective-taking    in
a referring expression. Assuming that the probabilities in
                                                                                                           comprehension (Heller et al., 2016).
                                                                     2127

   Why would interlocutors use both their own perspective                               Acknowledgments
along with their partner’s? One reason is that it may be
                                                                     We are grateful to Rafiya Asad for her help with data
cognitively taxing to completely suppress one’s own
                                                                     collection and coding, and we acknowledge support from
perspective. It has already been shown in the literature that
                                                                     NSERC and SSHRC of Canada.
perspective-taking abilities are tied to inhibition control
(Brown-Schmidt, 2009; Nilsen & Graham, 2009), and to the                                      References
salience of one’s own perspective in the situational context
                                                                     Brown-Schmidt, S. (2009). The role of executive function in
(Wardlow Lane & Ferreira, 2008).
                                                                       perspective-taking       during      on-line       language
   There are also linguistic reasons, however, for why
                                                                       comprehension. Psycho. Bulletin & Review 16, 893-900.
interlocutors might use both perspectives, namely
                                                                     Clark, H. H., & Marshall, C. R. (1981). Definite reference
performing felicitous moves in conversation. For example,
                                                                       and mutual knowledge. In A. Joshi, B. Webber, & I. Sag
producing a meaningful assertion requires choosing
                                                                       (Eds.), Elements of discourse understanding (pp. 10-63).
information from one’s own perspective and assessing that
                                                                     Frank, M. C. & Goodman, N. D. (2012). Predicting
one’s partner does not know the information; formulating a
                                                                       pragmatic reasoning in language games. Science 336, 998.
question, in contrast, involves identifying a knowledge gap
                                                                     Goodman, D. N., & Stuhlmüller, A. (2013). Knowledge and
in one’s perspective and assessing that the partner does have
                                                                       implicature: Modeling language understanding as social
this information.
                                                                       cognition. Topics in Cognitive Science, 5, 173–184.
   Finally, even when tailoring referring expressions, it may
                                                                     Heller, D., Gorman, K. S. & Tanenhaus, M. K. (2012). “To
not be ideal for speakers to focus on the addressee’s
                                                                       name or to describe: shared knowledge affects referential
perspective alone: what we have been calling “the
                                                                       form”. Topics in Cognitive Science, 4, 290-305.
addressee’s perspective” is just a hypothesis on the part of
                                                                     Heller, D., Parisien, C. & Stevenson, S. (2016). Perspective-
the speaker about the addressee’s knowledge. While this
                                                                       taking behavior as the probabilistic weighing of multiple
hypothesis may have been developed based on strong cues
                                                                       domains. Cognition, 149, 104–120.
(e.g., the fact that the addressee did not see the experimenter
                                                                     Horton, W. S. & Keysar, B. (1996). When do speakers take
demonstrate the function of VMOs), other cues in the
                                                                       into account common ground? Cognition, 59, 91-117.
situation may suggest to the speaker that the addressee
                                                                     Isaacs, E. A., & Clark, H. H. (1987). References in
actually shares their knowledge (e.g., they may note small
                                                                       conversation between experts and novices. Journal of
perceptual cues on the VMOs that are suggestive of their
                                                                       Experimental Psychology: General, 116, 26–37.
unexpected function). Given the uncertainty in assessing
                                                                     Kehler, A. & Rohde, H. (2013). A Probabilistic
the addressee’s knowledge state, the speaker may do well to
                                                                       Reconciliation of Coherence-Driven and Centering-
consider their own perspective as relevant to expressing
                                                                       Driven Theories of Pronoun Interpretation, Theoretical
their intent.
                                                                       Linguistics, 39, 1-37.
   Note that this consideration and integration of multiple
                                                                     Kuhlen, A. K. & Brennan, S. E. (2013). Language in
communicative contexts is different from that explored in
                                                                       dialogue: When confederates might be hazardous to your
Goodman and Stuhlmüller (2013). There, probabilistic
                                                                       data. Psychonomic Bulletin & Review, 20, 54-72.
weighing is used to model uncertainty about the other
                                                                     Mozuraitis, M., Chambers, G. C., & Daneman, M. (2015).
partner’s state of knowledge, with the goal of maximizing
                                                                       Privileged vs. shared knowledge about object identity in
adaptation to the partner. This fundamentally differs from
                                                                       real-time referential processing. Cognition, 142, 148-165.
our approach of weighing and integrating the differing
                                                                     Nadig, A. S., & Sedivy, J. C. (2002). Evidence of
perspectives of the two partners. In future work, insights
                                                                       perspective-taking constraints in children’s on-line
from the Goodman and Stuhlmüller model of uncertainty
                                                                       reference resolution. Psychological Science, 13, 329–336.
with respect to a single perspective can be integrated within
                                                                     Nilsen, E., & Graham, S. (2009). The relations between
our approach that combines multiple perspectives.
                                                                       children’s communicative perspective-taking and
   Thus, although on the surface it may seem ideal to choose
                                                                       executive functioning. Cognitive Psychology 58, 220-249.
a referring expression that is fully tailored to the addressee’s
                                                                     Wardlow Lane, L. & Ferreira, V. S. (2008). Speaker-
perspective (cf. Clark & Marshall, 1981), our view is that
                                                                       external versus speaker-internal forces on utterance form:
balancing the different demands of conversation requires
                                                                       Do cognitive demands override threats to referential
actively maintaining – and integrating – a representation of
                                                                       success? Journal of Experimental Psychology: Learning,
both partners’ perspectives. In other words, perspective-
                                                                       Memory, and Cognition, 6, 1466-1481.
taking behavior is achieved neither by focusing on shared
                                                                     Yoon, S. O., Koh, S., & Brown-Schmidt, S. (2012).
information nor by trying to fully adapt to one’s partner, but
                                                                       Influence of perspective and goals on reference
rather by probabilistically integrating the perspectives of all
                                                                       production in conversation. Psychonomic Bulletin &
interlocutors. Future work will aim to disentangle the
                                                                       Review, 19, 699–707.
considerations that lead to the weighing of perspectives, as
well as the effect of feedback on weighing and re-weighing.
                                                                 2128

