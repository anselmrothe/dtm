              Constraining the Search Space in Cross-Situational Word Learning:
                                  Different Models Make Different Predictions
                          Giovanni Cassani, Robert Grimm, Steven Gillis, and Walter Daelemans
                              Computational Linguistics and Psycholinguistics (CLiPS) Research Center
                                    Department of Linguistics, University of Antwerp, 13 Prinsstraat
                                                        B-2000 Antwerpen, Belgium
                                                     {name.surname}@uantwerpen.be
                              Abstract                                     In this paper, we compare behavioral evidence to four dif-
                                                                        ferent models that exploit cross-situational regularities to in-
   We test the predictions of different computational models of         fer word-referent mappings from the data, to analyze what
   cross-situational word learning that have been proposed in the
   literature by comparing their behavior to that of young children     predictions each model makes and whether they fit with what
   and adults in the word learning task conducted by Ramscar,           children and adults do when asked to map a referent to a
   Dye, and Klein (2013). Our experimental results show that a          word. Our aim is to provide evidence about which learning
   Hebbian learner and a model that relies on hypothesis testing
   fail to account for the behavioral data obtained from both pop-      mechanisms proposed in the literature can explain behavioral
   ulations. Ruling out such accounts might help reducing the           evidence and which cannot, in order to constrain the search
   search space and better focus on the most relevant aspects of        space of possible models to the learning strategies that ex-
   the problem, in order to disentangle the mechanisms used dur-
   ing language acquisition to map words and referents in a highly      ploit cross-situational information in the same way humans
   noisy environment.                                                   do. Carefully controlled laboratory settings in which specific
   Keywords: cross-situational learning; word learning; compu-          features of the word learning task are manipulated can help
   tational modeling; language acquisition                              to achieve this goal, by isolating the information from the in-
                                                                        put that makes learning possible or impossible and providing
   Ever since the gavagai example provided by Quine (1960)              valuable data to test computational simulations in a variety
to describe the huge amount of referential uncertainty that any         of situations (Ramscar, Dye, & Klein, 2013; Kachergis et al.,
language learner has to face while inducing word-object map-            2016).
pings, researchers took an interest in which mechanisms can                While many learning mechanisms can mirror certain be-
be exploited to solve this crucial task. In the last twenty years,      havioral patterns (Yu & Smith, 2012), some may not be able
computational modeling has proven extremely useful in ex-               to learn the correct word-referent mappings in specific, con-
ploring what information encoded in the input children re-              trolled paradigms in which subjects do learn such mappings
ceive might allow them to correctly map referents and words,            robustly. Identifying these situations and showing why cer-
and which learning mechanisms might best exploit the rele-              tain mechanisms fail to account for successful learning will
vant information (Frank, Goodman, & Tenenbaum, 2009).                   help the researchers to constrain the hypothesis space and dis-
   Cross-situational learning posits that children keep track           card mechanisms that make incorrect predictions.
of co-occurrences of referents in the world and words uttered
to them in several situations to establish unambiguous map-                                        Dataset
pings: while the single situation might be ambiguous, the co-
occurrences of words and referents across many different sit-           In order to evaluate the predictions of different models of
uations help the learner figure out the correct mappings. Start-        cross-situational learning we make use of the evidence pre-
ing from the work of Siskind (1996), many different learning            sented in Ramscar, Dye, and Klein (2013). The experiment
mechanisms that exploit this basic principle have been pro-             they reported was conducted with a group of children (mean
posed that show comparable performances to many behav-                  age 28 months) and two groups of adults, undergraduates and
ioral data from both children (Ramscar, Dye, & Klein, 2013;             developmental psychologists.
Smith & Yu, 2008; Suanda, Mugwanya, & Namy, 2014)                          The setting included three objects, [ObjA, ObjB, ObjC],
and adults (Dautriche & Chemla, 2014; Fazly, Alishahi, &                and three labels, {Dax,Wug, Pid}; during 18 learning trials,
Stevenson, 2010; Medina, Snedeker, Trueswell, & Gleitman,               each subject saw two objects and then heard one label. Of the
2011; Trueswell, Medina, Hafri, & Gleitman, 2013; Yu &                  three objects, ObjA and ObjC were presented 9 times, never
Smith, 2007; Yurovsky & Frank, 2015; Yurovsky, Yu, &                    together; ObjB, however, was present in all trials, occurring
Smith, 2013), using both corpus studies and laboratory exper-           half of the times with ObjA and half of the times with ObjC.
iments, covering many different conditions. Differences and             Crucially, ObjA was always presented together with the same
similarities across models have been explored, with the main            label, e.g. Dax, and ObjC was always presented with the same
goal of showing how apparently different proposals can yield            label, e.g. Pid. Consequently, ObjB occurred half of the times
comparable results and make similar predictions when cer-               with the label Dax and half of the times with the label Pid.
tain components of the learning algorithm are modified (Yu              The third label, Wug, never occurred during training.
& Smith, 2012; Kachergis, Yu, & Shiffrin, 2016).                           During testing, the subjects heard one of the three labels
                                                                    1152

    3-year-old participants did. They were tested individually                          Althoughsituational   studies
                                                                                                    the children                           which to
                                                                                                                                 learning,objects
                                                                                                                           wordmatched
                                                                                                                    weoftested                      have shown                                                                                              ing is ofte
    and told that they were assisting in a pilot test of a task                                 thatbasis
                                                                                     labels on the    children                can14
                                                                                                                 and adults the
                                                                                                           of informativity,      learn the meaning
                                                                                                                                    Stanford under- of words                                                                                                literature
    that was subsequently to be conducted with children.                             graduates by   “accruing
                                                                                                 tested         statistical
                                                                                                         in exactly   the same   way across
                                                                                                                            evidence         multiple
                                                                                                                                      did not. They and indi-                                                                                               ing is sen
    They were told that although the task might seem trivial,                        agreed withvidually
                                                                                                    the children   aboutword-scene
                                                                                                           ambiguous                   pairings”A (Smith
                                                                                                                            A and C, selecting     as     & Yu,                                                                                             (Rescorla,
                                                                                                2008, p. 1559). However, these findings, and many other                                                                                                     nisms in c
    their answers were important and they should give the                            the dax (M = 86%) and C as the pid (M = 79%) at above-
                                                                                                similar findings in the lexical-acquisition literature, may                                                                                                 baffling. F
    answers that seemed most natural to them.                                        chance levels, t(13) = 5.401, p < .001, and t(13) = 3.421,
         a                                           Object A
                                                     Object B                        b                                                           a
                                                                                                                                                 Object A
                                                                                                                                                 Object B
                                                                                                                                                                                               Object A
                                                                                                                                                                                               Object B                                                   b
                                                80                                                                                         60
                                                                                         Consistent Choice in Duplicate Tests (% trials)
                                                     Object C                                                                                    Object C100                                   Object C                                                                                            100
                                                70                                                                                                                                        90                                                                                                       90
           Object Matched to Label (% trials)
                                                                                                                                           50
                                                                                                                                                     Object Matched to Label (% trials)                                                                       Object Matched to Label (% trials)
                                                                                                                                                                                          80                                                                                                       80
                                                60
                                                                                                                                           40                                             70                                                                                                       70
                                                50
                                                                                                                                                                                          60                                                                                                       60
                                                40                                                                                         30                                             50                                                                                                       50
                                                                            Chance
                                                30                                                                                                                                        40                                                                                                       40
                                                                                                                                           20
                                                                                                                                                                                          30                                                                                                       30
                                                20
                                                                                                                                                                                          20                                                                                                       20
                                                                                                                                           10
                                                10                                                                                                                                        10                                                                                                       10
                                                0                                                                                           0                                             0                                                                                                         0
                                                       Dax      Pid   Wug                                                                          Dax                                            Pid
                                                                                                                                                                                                  Dax     Wug
                                                                                                                                                                                                           Pid                       Wug
         Fig. 2. Average percentage of trials on which the children (n = 21) selected each of the three
                                                                                                      Fig. objects as matching
                                                                                                             3. Results from 14 each  labelundergraduates
                                                                                                                                            over repeated and 20 developmental psychologi
                                                                                                                                Stanford
         test trials (a) and the rate of consistent responses across the duplicate tests (b). Error bars represent
                                                                                                      trials       standard
                                                                                                             on which        errors of the mean.
                                                                                                                        the undergraduates   selected each of the three objects as matching
                                                                                                                                                 predictions for the percentage of trials on which they expected a healthy 2- or 3-year
Figure 1: Children learning patterns on the word-learning ex-                              Figure each     2: label.Undergraduates       learning
                                                                                                                       Error bars represent          patterns
                                                                                                                                                            of theon
                                                                                                                                            standard errors           the word-
                                                                                                                                                                   mean.
periment from Ramscar, Dye, and Klein (2013). The plot is                                  learning experiment from Ramscar, Dye, and Klein (2013).
taken from the original paper (Figure 2a).                                                 The plot is taken from the original paper (Figure 3a).
                                      Downloaded from pss.sagepub.com at Stanford University Libraries on May 8, 2013
                                                                                                                                                                                                                 Downloaded from pss.sagepub.com at Stanford University Librar
and were asked to point to the object to which they thought                                                                                order of presentation for learning to take place, when we eval-
the label referred to. Two labels occurred during training,                                                                                uate a model that was designed to map words to referents, we
one did not - words and objects were counterbalanced and                                                                                   switch the two layers and make it learn the opposite mapping.
learning trials were randomized across participants.
                                                                                                                                                 Models of Cross-situational Learning
Behavioral Evidence
                                                                                                                                           We compare simple, basic implementations1 of four differ-
Results of the experiment are provided in Figure 1 for chil-                                                                               ent learning mechanisms to highlight what predictions are
dren and Figure 2 for undergraduates - the plots show the                                                                                  made by each of them, and whether they match behavioral
case in which Dax was always presented with ObjA, Pid with                                                                                 evidence. We introduce each model separately and briefly
ObjC, and Wug was only showed during testing.                                                                                              discuss its main features; for more detailed explanations, we
   Both groups mapped ObjA and ObjC to the labels that only                                                                                refer to the cited publications.
occurred with each of them. Interestingly, however, under-
graduates showed a mutual exclusivity bias and mapped ObjB                                                                                 Hebbian Learner This model implements the law of con-
to Wug, which was not presented during training; on the con-                                                                               tiguity (Warren, 1921), according to which the association
trary, children picked ObjA and ObjC at comparable rates as                                                                                between two items becomes stronger when they consistently
referent for the new label. The developmental psychologists                                                                                occur together in the environment. It is usually implemented
were asked to predict the behavior of children but ended up                                                                                as a neural network with no hidden layer that incrementally
predicting that of undergraduates. The authors of the study                                                                                establishes associations between an input and an output layer
conclude that children are more sensitive to the informativity                                                                             (Hebb, 1949). An input-to-output association is strengthened
of cues than to logical principles, which on the contrary play                                                                             by a constant quantity whenever the two co-occur within a
a role in adults.                                                                                                                          learning trial. Associations from inputs that occur in a learn-
                                                                                                                                           ing trial and outputs that do not are left unchanged, as are
Feature-Label-Order Effects In this experiment, and in                                                                                     associations from absent inputs to all output units.
many others that address cross-situational word learning, ob-                                                                                 The way associations are updated is summarized in equa-
jects are presented before their labels are uttered. Far from                                                                              tion (1), where t represents a learning trial, ci indicates an
being irrelevant to the task, evidence from Ramscar, Yarlett,                                                                              input item, or cue, o j indicates an output, or outcome, and
Dye, Denny, and Thorpe (2010) shows that different learning                                                                                ∆Vi j indicates the value of the update from ci to o j after ex-
outcomes arise in behavioral experiments where this order                                                                                  periencing the learning trial t:
is manipulated. This difference is unfortunately not always
considered in cross-situational learning studies: as a conse-
                                                                                                                                                                   (
                                                                                                                                                                    k if ci ∈ t and o j ∈ t
quence, certain models are defined as mapping referents to                                                                                                 ∆Vi j =                                      (1)
                                                                                                                                                                    0 else
words and others do the opposite. Moreover, the behavioral
data we use were obtained using a paradigm in which the sub-                                                                                  1 The code of our own re-implementations of each model
jects first saw an object and then heard a label. Thus, consid-                                                                            is available at https://github.com/GiovanniCassani/cross
ering the experimental paradigm and the importance of the                                                                                   situational learning, commit n. 2a9dbaa
                                                                                      1153

   ∆Vi j is then added to the current association from ci to o j ;    ∆Vi j is added to the current association value of cue ci for each
k is a strictly positive constant which only affects the ab-          outcome o j encountered up to trial t. The same happens for
solute value of the associations but not the relations among          all ci ∈ t. For the reported simulations we selected standard
them, thus changing its value does not affect the learning out-       parameter values that allow to make minimal assumptions,
come. This model was showed to successfully model behav-              setting all αs = 0.2; β1 = β2 = 0.1; λ = 1.
ioral data in the study by Yu and Smith (2012) and for this
                                                                      Probabilistic learner In its original formulation (Fazly et
reason it is evaluated here. However, the risk exists that every
                                                                      al., 2010), this model computes a posterior probability distri-
input becomes associated with every output, making it impos-
                                                                      bution over referents for each word, updating the probability
sible to learn unambiguous input-output mappings (Dawson,
                                                                      mass allocated to each referent in the light of new evidence.
2008).
                                                                      A referent r that seldom occurs with a word w but often oc-
Naı̈ve Discriminative Learning (NDL) In this model,                   curs with many other words will get a small probability for w,
input-output associations are updated according to the                while a referent r0 that often occurs with word w and rarely
Rescorla-Wagner equations (Rescorla & Wagner, 1972), de-              with others will have a high probability of being the correct
veloped in the context of animal learning and condition-              referent for w. The model incrementally updates associations
ing. This model is often referred to as Naı̈ve Discrimina-            between words and referents and uses them to compute the
tive Learning (NDL, (Baayen, Hendrix, & Ramscar, 2013))               conditional probability of a referent given a word for all the
and its relevance to language has been established in different       referents that occurred with the word up to the present learn-
aspects of language learning and processing (Baayen, Milin,           ing trial.
Durdević, Hendrix, & Marelli, 2011; Baayen, Shaoul, Willits,            More generally, this model can be thought of as computing
& Ramscar, 2015; Ramscar, Dye, & McCauley, 2013; Ram-                 a posterior distribution over all possible outcomes for each
scar, Hendrix, Shaoul, Milin, & Baayen, 2014).                        cue. Associations between cues and outcomes are computed
   Its architecture closely resembles the Hebbian learner, as it      as specified in equations (3-5), where t is a learning trial, o is
is a neural network with no hidden layer that incrementally           an outcome from the set of outcomes in the learning trial, Ot ,
establishes associations between cues and outcomes, where             c is a cue, from the set of cues in the learning trial, Ct , paired
the first constitute the input layer and the latter the output        with Ot , and C is the set of cues encountered up to t:
nodes. As for the Hebbian learner, when a cue co-occurs with
an outcome, the association between them becomes stronger;                                                   pt−1 (o|c)
                                                                                      a(c|o, Ot ,Ct ) =                               (3)
moreover, associations from absent cues (in a learning trial)                                           ∑c0 ∈Ct pt−1 (o|c0 )
to all outcomes are left unchanged. However, in the NDL
model, associations from present cues to absent outcomes are                    assoct (c, o) = assoct−1 (c, o) + a(c|o, Ot ,Ct )     (4)
weakened, and can eventually become negative. The model
is naı̈ve because every outcome is updated independently of                                           assoct (c, o) + λ
                                                                                    pt (o|c) =                                        (5)
all other outcomes.                                                                             ∑o0 ∈O assoct (c, o0 ) + β · λ
   The update in associations is summarized in equation (2),             This model has 3 free parameters. λ is a small smooth-
where t is a learning trial, ∆Vi j is the change in association       ing factor; β is the upper bound on the expected lexicon;
involving a cue ci and an outcome o j .                               pt=0 (o|c) is the initial value of the probability of an outcome
                                                                      given a cue, before they are encountered in a learning trial.
                                                                     In the simulations reported by Fazly et al. (2010), β = 8.500,
                αi β1 (λ − ∑c∈t Vc ) if ci ∈ t and o j ∈ t
                
                                                                      λ = 10−5 , and pt=0 (o|c) = 1/8, 500. We kept the same value
        ∆Vi j = αi β2 (0 − ∑c∈t Vc ) if ci ∈ t and o j ∈/t   (2)      for λ, set β = 104 and pt=0 (o|c) = 10−4 .
                
                  0                    if ci ∈
                                             /t                          Equation (3) computes the update in association between a
                
                                                                      cue and an outcome from the current learning trial: this up-
   αi is a parameter modifying the salience of an input unit,         date is proportional to p(o|c) at the previous learning trial and
or cue: while a different value can be set for each cue, this         depends on the number of cues in the current trial: more cues
parameter is usually kept constant to remain agnostic with re-        cause a lower change, due to higher noise and uncertainty in
spect to cue importance. β1 and β2 specify the importance             the current trial. The update computed in (3) is added to the
of positive and negative evidence respectively. These two pa-         corresponding cue-outcome association, as specified in (4).
rameters can again take different values but are usually set          Associations are not exploited directly but rather used to up-
to the same quantity to reduce the initial assumptions. λ is          date a probability distribution over outcomes for each cue.
the maximum amount of association that each outcome can               More evidence makes the learner allocate a higher posterior
receive from all inputs and operates as a simple linear scal-         probability to a specific outcome. In (5), the denominator acts
ing factor (Evert & Arppe, 2015). Finally, ∑c∈t Vc is the total       as a scaling factor that implements within-trial competition:
association supported by the cues present in the current learn-       if a cue c is already associated to one of the previously en-
ing trial: this evidence is used to predict the outcome, and the      countered outcomes, the probability that c maps to another
prediction error is used to update cue-outcome associations.          outcome does not receive strong support.
                                                                  1154

   In the original formulation, words were cues and referents           ently from the task faced by the subjects. There was no way
were outcomes; however, considering what we discussed in                they could expect a third label to be presented during testing
the section about Feature-Label-Order effects (Ramscar et al.,          and thus update connections from objects to that label during
2010), we flipped the encoding so that this algorithm learns            training.
a probability distribution for words over referents, coding                Here, we focus on the situation where children and under-
words as outcomes and referents as cues2 .                              gradutes showed consistent behaviors, i.e. in retrieving an
                                                                        object when presented with a label they encountered during
Hypothesis-Testing Model (HTM) The HTM model se-
                                                                        training. If a model fails to account for this aspect of the
lects, stores and updates a single hypothesis for each learn-
                                                                        data, it can be hardly justified as a model of human cross-
ing trial. Initially, it randomly picks a word-referent map-
                                                                        situational learning, during acquisition as well as in adult-
ping from the possible ones in the learning trial. When an al-
                                                                        hood. Accordingly, we train each simulation using the input
ready encountered word is presented in a subsequent trial, the
                                                                        presented to the subjects and evaluate the final state of learn-
model looks in memory to retrieve the hypothesized referent
                                                                        ing. However, since different models learn different things
for the word and may retrieve it or not. If it does, the hy-
                                                                        (associations, probabilities, hypotheses), it is hard to directly
pothesis is strengthened when confirming evidence is found
                                                                        compare them. We do not assume any linking mechanism
in the current trial and discarded otherwise, in which case a
                                                                        that converts internal representations to behavior; we simply
new referent is hypothesized at random for the word being
                                                                        look at the learned representations and evaluate whether un-
considered. If no hypothesis is recalled, a new referent is hy-
                                                                        ambiguous mappings were learned, that could allow subjects
pothesized at random for the word being considered and the
                                                                        to retrieve an object, consistently with learning displayed by
old one fades away. As is specified in Medina et al. (2011)
                                                                        human subjects.
and Trueswell et al. (2013), the model depends on one main
                                                                           In each learning trial, simulated learners were given a set of
parameter, α, which models the probability that a formed hy-
                                                                        objects and a word: beside ObjA, ObjB, and ObjC, the set of
pothesis is retrieved from memory. However, Trueswell et al.
                                                                        cues also contained other cues that account for the whole ex-
(2013) argue that the value of this parameter changes when
                                                                        perimental context,3 for consistency with the original simula-
a hypothesis is recalled: the next time the label appears, the
                                                                        tion in Ramscar, Dye, and Klein (2013). Table 1 summarizes
hypothesized referent should be retrieved with a higher prob-
                                                                        the input to the computational models.
ability if it was already retrieved. Unfortunately, however, no
function was specified to model the change of α after suc-
cessful retrievals. Therefore, we set the initial and second            Table 1: Training trials, as described in Ramscar, Dye, and
values for α at 0.6 and 0.81, following the third experiment            Klein (2013)
in Trueswell et al. (2013), were this model was shown to
fit behavioral results. Accordingly, the first time a hypoth-              Cues                                     Outcomes       Freq
esis can be retrieved with probability equal to 0.6; if it gets            ObjA ObjB Context1 ExptContext                Dax          9
confirmed, the next time it will be retrieved with probability             ObjB ObjC Context2 ExptContext                Pid          9
equal to 0.81. Since we have many more trials, we set fur-
ther values, 0.9, 0.95, and 0.99, to model the probability that            For all models, we ran 200 simulations randomizing the
a hypothesis is retrieved after the third, fourth or fifth time it      order of presentation of the learning trials: since no model
was retrieved and confirmed. After the fifth time α does not            depends on initial random values, the order of the trials is the
change anymore: we stopped at 0.99 to exclude certainty of              only potential source of bias. We report referent-word asso-
recall.                                                                 ciations at the end of training for the four models in Table 24 .
                                                                           Successful learning happens when, for each label, the value
                Computational Simulations
                                                                        corresponding to an object is consistently higher than the val-
In order to closely mimic the learning task that was faced by           ues of the other two objects, given that the test procedure
children in the study by Ramscar, Dye, and Klein (2013), we             consisted of presenting a label and asking for the matching
implemented incremental learners: the connection between a              object. In this setting, the Hebbian learner would choose ran-
referent and a word is only updated when both have been en-             domly and is not learning much, since, in both the Dax and
countered in a learning trial. This is crucially different from         Pid columns, two objects have the same association to each
the simulations implemented by Ramscar, Dye, and Klein                  label. On the contrary, the NDL model would retrieve the
(2013), where the equilibrium equations (Danks, 2003) of the            correct object given the two words provided during training,
NDL model (Baayen et al., 2011) were used. In this case, the            since the ObjA-Dax and ObjC-Pid associations are higher
end state of the model is computed when no more learning tri-           than any other. Another interesting feature is that it learns
als are available. Equilibrium equations have the advantage             that ObjA does not come with the label Pid, forming a neg-
of not depending on any free parameter, but all cues and all
                                                                            3 This was not the case for the HTM model, in which only ObjA,
outcomes are simultaneously available to the learner, differ-
                                                                        ObjB, and ObjC were provided as input.
    2 Personal communication with one of the authors confirmed that         4 For explanatory purposes we will focus on the three objects,
the learning mechanism is not altered by switching the mapping.         leaving the other cues out.
                                                                    1155

                                                                     than specific features. However, even with such a bias, the
Table 2: Referent-word associations after 18 training trials
                                                                     HTM would fail to match the behavioral data. Consider the
(200 simulated learners). For the Probabilistic Learner, con-
                                                                     situation in which the model first sees a Dax trial and it ran-
ditional probabilities of label given object are showed; for the
                                                                     domly picks ObjB as a referent. When a Pid trial is presented,
Hypothesis Testing Model, the proportion of learners that se-
                                                                     the learner searches in memory, finds a Dax-ObjB hypothe-
lected each hypothesis is showed.
                                                                     sis, decides that Pid-ObjB is not legitimate, and maps Pid
      Model           Cue          Dax              Pid              to ObjC. If the HTM starts with a wrong mapping for Dax,
                      ObjA                9                .         it will only find the correct mapping for Pid, but will keep
      Hebbian                                                        failing at relating ObjA to Dax. The problem lies in the sin-
                      ObjB                9               9
      Learner                                                        gle hypothesis assumption, not in the absence of the mutual
                      ObjC                 .              9
                      ObjA      .127 ±.003     -.052 ±.004           exclusivity bias. In order to account for this behavioral evi-
      NDL             ObjB      .076 ±.003      .076 ±.003           dence, a model should hold in memory the two possible hy-
                      ObjC     -.051 ±.005      .127 ±.002           potheses. Only then could it appreciate the fact that ObjB
                      ObjA      .967 ±.003                 .         occurs with both labels while ObjA and ObjC consistently
      Probabilistic                                                  occur with one. The same problem of failing to appreciate
                      ObjB      .484 ±.085      .485 ±.085
      Learner                                                        the different background rates of the three objects affects the
                      ObjC                 .    .967 ±.003
                                                                     Hebbian learner, but results from an entirely different archi-
                      ObjA             .465                .
                                                                     tecture, since it only focuses on co-occurrences to update as-
      HTM             ObjB             .535             .53
                                                                     sociations. However, the behavioral evidence suggests that
                      ObjC                 .            .47
                                                                     subjects do assign importance to missing co-occurrences too,
                                                                     and our simulations show that successful learning is only pos-
                                                                     sible when a model is sensitive to both positive and negative
ative association. The Probabilistic Learner makes similar           co-occurrences. Taken together, the failures of the HTM and
predictions to the NDL model, except for the negative asso-          the Hebbian learner point to the importance of storing multi-
ciations. Finally, the HTM performs close to chance, with as         ple mappings and being sensitive to both things that co-occur
many simulated learners mapping Dax to ObjA as to ObjB,              and things that fail to co-occur in the environment (Ramscar,
and Pid to ObjB and ObjC, again showing no sign of learning,         Dye, & McCauley, 2013).
inconsistently with the behavioral evidence we considered.
                                                                        Unlike Trueswell et al. (2013) and Dautriche and Chemla
                          Discussion                                 (2014), we only evaluated the end-state of learning and did
                                                                     not consider trial-to-trial patterns, due to the behavioral data
Our results show that some of the proposed learning mech-
                                                                     we used for comparison. This analysis would have certainly
anisms fail to account for the behavioral data obtained by
                                                                     been useful because it allows to follow the learning trajec-
Ramscar, Dye, and Klein (2013), for both children and adults:
                                                                     tory. However, if a model does not account for the end state
specifically, a Hebbian learner (Hebb, 1949) and the HTM
                                                                     of learning it can hardly explain the mid-states, while a model
(Trueswell et al., 2013) fail to learn robust object-label map-
                                                                     that fits the final picture might have done so in different ways
pings. Two other models, the Probabilistic Learner (Fazly et
                                                                     than the subjects. Thus, the reported evidence appears to be
al., 2010) and the NDL model (Baayen et al., 2011), show
                                                                     strong enough to make a case against the psychological plau-
remarkably similar patterns to the behavioral data from both
                                                                     sibility of a model, while more evidence is needed about mod-
children and adults. The behavioral evidence also makes it
                                                                     els that fit the behavioral data.
clear that it is not necessary for successful cross-situational
learning that true word-referent associations are more fre-             Finally, we did not evaluate any specification of which
quent than spurious associations. As a matter of fact, in            mechanism can make use of the associations learned during
the dataset each word-referent pair occurs with the same fre-        training to actually decide which object to retrieve when pre-
quency, defying the very notion of a spurious pairing: ObjA          sented with a new label. While this is an interesting com-
could be paired to Dax just as ObjB could, if we only consider       ponent of the paradigm in Ramscar, Dye, and Klein (2013)
frequency of co-occurrence of objects and words. Nonethe-            and it is crucial to investigate how learning mechanisms dif-
less, humans learned consistent mappings, suggesting that            fer between young children and adults, we provided evidence
simply tracking co-occurrence frequencies is a poor candi-           that some learning mechanisms fail to account for behavioral
date mechanism to explain cross-situational word learning.           data from both groups even when the much simpler condition
   As is often the case in attempts to compare models, many          of retrieving a referent when presented with a known word
decisions need to be taken and different choices can result          is considered. Further analyses are required to identify those
in different outcomes. For example, we did not equip the             mechanisms that can both i) form the correct associations dur-
HTM with a mutual exclusivity bias, mainly because it is             ing training and ii) use such associations to retrieve a known
not specified in the paper where the model was proposed and          referent for an unknown word, in the same way children and
also because we wanted to evaluate basic versions of each            adults do, to highlight where their learning mechanisms differ
model to focus on the proposed learning mechanisms rather            and where they are comparable.
                                                                 1156

                         Conclusion                                 Kachergis, G., Yu, C., & Shiffrin, R. M. (2016). A boot-
The evidence we provided in this paper complements the                strapping model of frequency and context effects in word
study by Yu and Smith (2012) by showing that not every                learning. Cognitive Science.
learning mechanism can be instantiated in an algorithm that         Medina, T. N., Snedeker, J., Trueswell, J. C., & Gleitman,
accounts for behavioral data in cross-situational word learn-         L. R. (2011). How words can and cannot be learned by
ing. A single-hypothesis learning strategy (Medina et al.,            observation. Proc. Natl. Acad. Sci., 108(22), 9014-9019.
2011; Trueswell et al., 2013) and an associative model that         Quine, W. V. O. (1960). Word and object. Cambridge, MA:
only relies on Hebbian learning (Hebb, 1949) fail to fit be-          MIT Press.
havioral data. The jury is still out about the Probabilistic        Ramscar, M., Dye, M., & Klein, J. (2013). Children value in-
Learner (Fazly et al., 2010) and the Naive Discriminative             formativity over logic in word learning. Psychol Sci, 24(6),
Learner (NDL, (Baayen et al., 2011)): both models fit the             1017-1023.
results by Ramscar, Dye, and Klein (2013), but they behave          Ramscar, M., Dye, M., & McCauley, S. M. (2013). Error and
differently, prompting for further research on which mecha-           expectation in language learning: The curious absence of
nisms underpin cross-situational learning in humans.                  mouses in adult speech. Language, 89(4), 760-793.
                                                                    Ramscar, M., Hendrix, P., Shaoul, C., Milin, P., & Baayen,
                     Acknowledgments                                  R. H. (2014). The myth of cognitive decline: non-linear
                                                                      dynamics of lifelong learning. Top Cogn Sci, 6(1), 5-42.
We are grateful to Michael Ramscar and Konstantin Sering            Ramscar, M., Yarlett, D., Dye, M., Denny, K., & Thorpe,
for the discussion over the NDL model; to Aida Nematzadeh             K. (2010). The effects of Feature-Label-Order and their
for her help in better understanding the Probabilistic Learner;       implications for symbolic learning. Cognitive Science, 34,
to Chen Yu for sharing his latest research with us.                   909-957.
This research was supported by a BOF/TOP grant (ID 29072)           Rescorla, R. A., & Wagner, A. R. (1972). A theory of
of the Research Council of the University of Antwerp.                 Pavlovian conditioning: Variations in the effectiveness of
                                                                      reinforcement and nonreinforcement. In A. H. Black &
                          References
                                                                      W. F. Prokasy (Eds.), Classical conditioning II: current
Baayen, R. H., Hendrix, P., & Ramscar, M. (2013). Sidestep-           research and theory (p. 497). New York, NY: Appleton-
   ping the combinatorial explosion: An explanation of n-             Century-Crofts.
   gram frequency effects based on naı̈ve discriminative learn-     Siskind, J. M. (1996). A computational study of cross-
   ing. Lang Speech, 56(3), 329-347.                                  situational techniques for learning word-to-meaning map-
Baayen, R. H., Milin, P., Durdević, D. F., Hendrix, P., &            pings. Cognition, 61(1), 39-91.
   Marelli, M. (2011). An amorphous model for morpho-               Smith, L. B., & Yu, C. (2008). Infants rapidly learn word-
   logical processing in visual comprehension based on naı̈ve         referent mappings via cross- situational statistics. Cogni-
   discriminative learning. Psychol Rev, 118(3), 438-481.             tion, 106(1), 1558-1568.
Baayen, R. H., Shaoul, C., Willits, J., & Ramscar, M. (2015).       Suanda, S. H., Mugwanya, N., & Namy, L. L. (2014). Cross-
   Comprehension without segmentation: a proof of concept             situational statistical word learning in young children. J
   with naı̈ve discriminative learning. Lang Cogn Neurosci,           Exp Child Psychol, 126(1), 395–411.
   1-23.                                                            Trueswell, J. C., Medina, T. N., Hafri, A., & Gleitman, L. R.
Danks, D. (2003). Equilibria of the Rescorla–Wagner model.            (2013). Propose but verify: Fast mapping meets cross-
   J Math Psychol, 47(2), 109-121.                                    situational word learning. Cogn Psychol, 66(1), 126-156.
Dautriche, I., & Chemla, E. (2014). Cross-situational word          Warren, H. C. (1921). A history of the association psychol-
   learning in the right situations. J Exp Psychol Learn Mem          ogy. New York, NY: Charles Scribner’s Sons.
   Cogn, 40(3), 892.                                                Yu, C., & Smith, L. B. (2007). Rapid word learning under
Dawson, M. R. W. (2008). Connectionism and classical con-             uncertainty via cross-situational statistics. Psychol Sci, 18,
   ditioning. Comp Cogn Behav Rev, 3. Monograph, 1-115.               414-420.
Evert, S., & Arppe, A. (2015). Some theoretical and exper-          Yu, C., & Smith, L. B. (2012). Modeling cross-situational
   imental observations on naı̈ve discriminative learning. In         word-referent learning: Prior questions. Psychol Rev,
   Proceedings of the 6th Conference on Quantitative Investi-         119(1), 21-39.
   gations in Theoretical Linguistics.                              Yurovsky, D., & Frank, M. C. (2015). An integrative account
Fazly, A., Alishahi, A., & Stevenson, S. (2010). A probabilis-        of constraints on cross-situational learning. Cognition, 145,
   tic computational model of cross-situational word learning.        53-62.
   Cognitive Science, 34, 1017-1063.                                Yurovsky, D., Yu, C., & Smith, L. B. (2013). Competitive
Frank, M. C., Goodman, N. D., & Tenenbaum, J. B. (2009).              processes in cross-situational word learning. Cognitive Sci-
   Using speakers’ referential intentions to model early cross-       ence, 37(5), 891-921.
   situational word learning. Psychol Sci, 20(5), 578-585.
Hebb, D. O. (1949). The organization of behavior. New
   York, NY: John Wiley and Sons.
                                                                1157

