              Experience as a Free Parameter in the Cognitive Modeling of Language
                                Brendan T. Johns,1 Michael N. Jones, 2 & Douglas J. K. Mewhort3
                                btjohns@buffalo.edu, jonesmn@indiana.edu, mewhortd@queensu.ca
                    1
                      Department of Communicative Disorders and Sciences, University at Buffalo, Buffalo, NY
                              2
                               Department of Psychological and Brain Sciences, Indiana University, IN
                                    3
                                      Department of Psychology, Queen’s University, Kingston, ON
                              Abstract                                applies to memory (Anderson & Schooler, 1991), perception
                                                                      (Barsalou, 1999), and linguistic organization (Landauer &
To account for natural variability in cognitive processing, it is
standard practice to optimize the parameters of a model to            Dumais, 1997). In effect, the assumption acknowledges that
account for behavioral data. However, variability reflecting the      human beings are embedded in a structured physical
information to which one has been exposed is usually ignored.         environmental that informs learning and that constrains
Nevertheless, most language theories assign a large role to an        behavior. As Simon (1969; p. 53) makes clear, “The apparent
individual’s experience with language. We present a new way to        complexity of our behavior over time is largely a reflection
fit language-based behavioral data that combines simple learning      of the complexity of the environment in which we find
and processing mechanisms using optimization of language
materials. We demonstrate that benchmark fits on multiple
                                                                      ourselves”.
linguistic tasks can be achieved using this method and will argue       Although we acknowledge natural variation in processing
that one must account not only for the internal parameters of a       mechanisms, we explore in this paper the other source of
model but also the external experience that people receive when       variation—the environmental information to which people
theorizing about human behavior.                                      have been exposed—on lexical tasks. Subjects differ in what
Keywords: Cognitive modeling; Model                optimization;      they know, and the differences should cause a corresponding
Language processing; Corpus-based models.                             change in behavior. Of course, the effect of variable
                                                                      knowledge depends on the specific task. For example,
                          Introduction                                accumulated linguistic knowledge likely affects a lexical
Models of cognition often have to deal with troublesome               decision experiment more than a non-verbal perceptual
sources of variance that other fields (e.g., physical systems)        identification task, so including linguistic knowledge when
do not. For example, no two individuals process a stimulus in         modeling lexical decision makes a good deal of sense.
the same way, and the same individual rarely processes the              One way to build linguistic information into a model is to
same stimulus identically at multiple times. In addition to           use a representation of word meaning constructed from a
individual differences and temporal stability, there is true          standard corpus, such as the TASA corpus (introduced by
random and measurement variance. While many of the                    Landauer & Dumais, 1997). The TASA corpus includes
sources of variance can be represented by free parameters,            paragraphs from textbooks, from grades 1 to 12 and has been
much of what may be systematic variance ends up being                 used as the gold standard in tests of co-occurrence models
encapsulated by an overall noise parameter, often thought to          (e.g., Landauer & Dumais, 1997; Jones & Mewhort, 2007); it
reflect the inherent stochastic nature of the response process        has frequently been integrated into processing models in
(Shiffrin, Lee, Kim, & Wagenmakers, 2008).                            cognate areas (e.g., Johns, Jones, & Mewhort, 2012).
   Almost every cognitive model contains free parameters,             Although the TASA corpus is likely representative of the
coefficients that are initially unknown, but are estimated from       linguistic experiences that subjects have experienced, it is not
the observable data. The exact values for free parameters do          intended to map exactly onto the experiences of specific
not change the model’s architecture—the theory that the               individuals. Indeed different groups of subjects may have
model formalizes should be independent of its parameter               had exposure to wildly different linguistic sources, depending
values—but the settings do change a model’s behavior.                 on culture, geography, educational system, and so forth.
Hence, researchers use estimation methods to find the set of          Hence, for any group of subjects, there is a natural variation
parameters that maximize a model’s fit to data, and those             in their knowledge, variation that should impact the behavior
parameter estimates are often allowed to vary across different        of those subjects on specific laboratory tasks.
data sets to which the model is applied.                                Paradoxically, most theorists recognize that knowledge is
   A tacit assumption in cognitive models is that behavioral          central to performance in standard laboratory tasks but rely
differences across individuals or tasks can be explained by           on a single corpus to model cognition. By relying on a single
differences in process parameters. But an alternative source          corpus, they ignore an important source of variability, namely
of variance, often ignored, comes from differences in the             the different knowledge that individuals bring to the task. If
subject’s individual learning history or variance in memory           one could estimate a group of subjects’ average linguistic
representations selected for a task, independent of changes in        experience, it should be possible to account for behavioral
the process parameters.                                               data at a more refined level.
   Theories of cognition commonly assume that aspects of the            The present article describes a new method for taking into
external world are stored internally. The storage assumption          account both the internal parameters of a model as well as the
                                                                      environmental information that defines a subject’s unique
                                                                  2291

experience. Unfortunately, one cannot track a subject’s               model to a set of data. To do so, we used a hill-climbing
linguistic history. As a proxy of that history, however, we           algorithm iteratively to select the sections that maximize the
collected a very large diverse set of language materials and          model’s likelihood of generating various behavioral datasets.
combined it with a simple mechanism to uncover the most                 A hill-climbing algorithm is an iterative local search
informative texts for a specific set of data.                         algorithm, where a model is fit by incrementally improving
  To demonstrate the generality of our method, it was applied         its fit to a set of data. Once an increase in fit is no longer
to both lexical organization and lexical semantic data.               possible, the algorithm terminates. For experiential fitting,
Modelling each of these tasks requires information to be              the first iteration selects the section that provides the best fit.
accumulated into the mental lexicon, after which it will be           Subsequent iterations add additional sets on top of the
acted upon to accomplish a specific task. The aim is to show          previously selected sections, to construct an overall training
that, by combining experiential fitting with realistic cognitive      corpus. Once a section has been selected, it can no longer be
models across multiple areas, benchmark accounts of                   used (sampling without replacement). Hence, the training
language-based behaviors can be attained.                             materials increase their resolution continuously, in
                                                                      correspondence with the structure of the set of data
      Corpora and Data Fitting Methodology                            attempting to be modeled. Fitting ends when the addition of
To estimate the type of linguistic information used in a              a further section into the overall corpus does not increase the
certain behavioral task, we use a wide variety of large               fit of the model to the data. To avoid getting stuck on local
language sources. These sources are then split into smaller           maxima, 10 unique starting points were made, in a rank order
sections, and it is iteratively determined which sections             of the best fitting sections. The best fit will be displayed in
maximized the fit of a model to a set of data. At the end of          the below simulations.
the iterative process, the algorithm will have determined the         Discussion
most informative set of texts needed to explain performance
on the task. Here, we describe the training materials and             To explore the power of experiential fitting, a large amount
specifics of the method used to fit the models.                       of text was assembled across a number of different sources.
                                                                      To determine the optimal set of linguistic data to explain a set
Training Materials                                                    of data, the texts were split into smaller pieces, and a hill-
The texts come from five different sources: (a) Wikipedia             climbing algorithm was used iteratively to find the selection
(Shaoul & Westbury, 2010), (b) Amazon product                         of text that maximally increased the fit of a model to a set of
descriptions (attained from McAuley & Leskovec, 2013), (c)            data. One could think of the process as a kind of parameter
1,000 fiction books, (d) 1,050 non-fiction books, and (e)             fitting (see Shiffrin, et al., 2008), but instead of optimizing
1,500 young-adult books. All of the books were attained from          the internal parameters to explain a set of behavioral data, the
e-books, and the vast majority were written in the last 50            procedure optimized the structure of the external world (i.e.
years by popular authors. The set of sources—from an online           linguistic information).           Optimizing the linguistic
encyclopedia to books targeted at young adults, to marketing          information allows us to determine the power gained by
materials for a large range of products—was designed to               accounting for the variance in linguistic experience to an
represent a broad set of possible experiences that an                 explanation of human behavior. That is, if linguistic behavior
individual might have with written language. It is impossible,        is related to the structure of linguistic experience,
of course, to span the entire range of possible linguistic            determining the optimal set of language materials with which
information, but the present materials represent a substantial        to train a model should provide a substantial increase in the
range of texts, one that should give experiential fitting a fair      fit of the model.
test. To equate each source’s contribution, each was trimmed
to six million sentences, for a total of 30 million sentences                               Lexical Semantics
across all texts (approximately 400 million words).                   Models of semantic memory, particularly Latent Semantic
  The data-fitting method will determine which set of texts is        Analysis (Landauer & Dumais, 1997), have strongly
the most informative for fitting a particular experiment, just        influenced studies of the effect of linguistic experience. LSA
as statistical methods are used to estimate the optimal free          showed that a simple averaging mechanism, when combined
parameters of a model. The corpora were split into small              with sufficient amounts of language information (derived
sections of 50,000 sentences yielding 120 sections for each           from a large text corpus), can construct a representation of
corpus, for a total of 600 different sections across the corpora.     the meaning of words that is closely matches how people use
Each section is large enough to allow for a measure of how            language.
much linguistic information the section contains, but is small          The model used here is derived from BEAGLE (Jones &
enough that the different sections can still be combined to           Mewhort, 2007), a random vector accumulation model. The
determine an optimal set of language.                                 BEAGLE model is based on using sentential information in
Data Fitting Methodology                                              the learning process. In this model, words are represented by
                                                                      two vector types: a static environmental vector, that
The goal of the data-fitting algorithm is to determine the            represents the perceptual (visual/auditory) aspects of a word,
combination of the sources that gives the best fit for a specific     and dynamic context/order vectors, which mark both co-
                                                                  2292

occurrence and simple syntactic usages of a word. Each time          adding new material failed to increase the quality of the fit.
a word is seen in a corpus of text, the dynamic vectors are          To minimize the chance of falling into a local minimum
updated. For context information, updating is done by                during the fit, 10 random starts were used. To form a
summing the environmental vectors of the other words that            comparison, 50 resamples of the full corpus (of 30 million
occurred in the sentence with it (with high frequency function       sentences), and the average performance increase across each
words removed). Accordingly, the context representation              50,000 section of this corpus was recorded. This will provide
accumulates pure co-occurrence information. Order vectors,           a measure of how successful the model is independent of
by contrast, accumulate rudimentary syntactic information,           experiential optimization.
by recording the position of words that surround the usage of
a word. The original BEAGLE model used circular                      Results
convolution to form an n-gram representations of a sentence.         The top panel of Figure 1 shows accuracy on the TOEFL test
Here, we will use a simplified form of the model (see                as a function of the number of sentences included in the fit; it
Recchia, Sahlgren, Kanerva, & Jones, 2015), because the              shows results for the three kinds of information (item, order,
simplified form is less computationally expensive. We refer          and complete) along with a control condition in which the
the reader to Recchia, et al. (2015) for a complete description      sections of text were assembled randomly. For the random
of the simplified form of BEAGLE. In the following                   corpora, the results were concatenated at 10 million sentences
simulations, order, context, and the complete (the sum of the        in order to aid in visualization. The complete model achieved
order and context vectors) representations were used.                the best performance at 97% accurate at only 1.1 million
  The model was tested using two different data types: (a)           sentences. The context representation maximized at 92%
synonym tests, and (b) item-level semantic priming.                  accurate at 3 million sentences, while the order representation
Following Landauer and Dumais (1997), synonym tests have             maximized at 82% accurate at 2.1 million sentences. For the
become a standard test for models of semantic representation.        random corpora, the average maximum performance was
In the synonym test, subjects are required to pick the word          57%, consistent with the past results (Jones & Mewhort,
from a set of four that is most similar in meaning to a target       2007). The complete representation is essentially performing
word. A real-world example is the Test of English as a               at the same level as a native English speaker, an impressive
Foreign Language (TOEFL). Landauer & Dumais used 80                  level of performance for a rather simple model.
questions from the TOEFL and reported that LSA achieved
an accuracy of 55% on this test.
  Semantic priming will also be analyzed, a type of data that
semantic space models have had success in accounting for
(Jones, Kintsch, & Mewhort, 2006). In behavioral
experiments, subjects are asked to perform simple tasks, such
as lexical decision, but the target word is preceded by a prime.
The prime can be semantically related or not, and the benefit
provided by the prime is measured in terms of the speedup
seen when the target is preceded by a semantically related
item versus a semantically unrelated word.
  Hutchison, Balota, Cortese, & Watson (2008) have shown
that models of semantic representation may succeed at the
mean level across items but fail at the individual word level.
They examined priming in lexical decision for 300 different
items and found that semantic variables offered minimal fits
to the data, with forward association strength having the best
correlation to overall levels of priming at r = 0.164, p < 0.01,      Figure 1. Results of BEAGLE and experiential optimization.
while LSA had a non-significant correlation of r = 0.053.
Clearly semantic priming data are difficulty to account for at         We also examined Hutchison, et al.’s (2008) item-level
an item level; hence, the data provide an excellent test for the     semantic priming results. Recall that it has been challenging
power of experiential fitting.                                       for semantic-space models to account for item-level results.
                                                                     Figure 3 shows the fitted correlation as a function of the
Data Fitting Methodology                                             number of sentences for item, order, complete (combined)
As noted earlier, all corpora were split into sections of 50,000     and random controls. As is shown in the bottom panel of
sentences and vector sets were generated for all the three           Figure 1, all three representation types (item, order, and
types of information created by BEAGLE (context, order,              combined) provided a good fit to the item-level data in
and complete). The hill-climbing algorithm selected semantic         semantic priming. There was not a great deal of difference
vectors to maximize the model’s performance on the TOEFL             among the non-random representations, the complete model
test, and rank correlation in semantic priming. That is, the         did offer the best fit at r = 0.412, p < 0.001. Note that the
necessary semantic information was refined iteratively to            complete model provided a better fit than all the semantic
maximize the fit to the data. Iterative refinement halted when
                                                                 2293

variables tested by Hutchison, et al. (2008). Indeed, it             with a change in context. On the basis of these results, and a
approached the fit of their 18-variable regression (r = 0.5).        corpus analysis, Jones, et al. (2012) proposed a new model
                                                                     that builds a more accurate measure of a word’s strength in
Discussion                                                           memory, entitled the semantic distinctiveness memory
Semantic space models have been fundamental in exploring             (SDM) model.
the influence of the linguistic environment on human                   The SDM builds a word’s strength in memory by weighting
behavior. This section explored the power that comes from            each new context by how much unique information that
combining a simplified form of a popular semantic space              context provides about the meaning of the word. Across
model (Jones & Mewhort, 2007) with experiential fitting,             various corpora, this model was able to account for a larger
with the result being benchmark fits for every dataset               amount of variance to a mega dataset of lexical decision and
analyzed. What this suggests is that the representations that        naming times over word frequency and a document count.
people form in semantic memory are heavily influenced by             Additionally, Johns, et al. (2012) demonstrated that the
the content of experience, and by constructing corpora that          advantage for a semantic diversity count extends to spoken
reflects this experience, better representations can be              word recognition performance. Johns, Dye, and Jones (2016)
constructed.                                                         have extended the results of the artificial language
  However, one question about this method that needs to be           experiment of Jones, et al. (2012) with natural language
determined is what source of variance is exploited in these          materials and found similar results.
simulations. It needs to be shown that the method is sensitive         The simulations in this section will compare WF, CD, and
to group characteristics, where groups of subjects who have          SDM magnitudes, in combination with experiential fitting.
likely had different linguistic experiences, are found to have       The main source of data is 40,000 lexical decision times
different corpora statistics by the experiential fitting method.     attained from the English Lexical Project (ELP; Balota, Yap,
That is, the method does not just exploit noise in the different     Cortese, Hutchison, Kessler, Loftis, Neely, Nelson, Simpson,
datasets, but is actually approximates the type of experiences       & Treiman, 2007). This is a standard dataset that has been
a group of subjects may have had.                                    used to differentiate different lexical information sources
                                                                     (Brysbaert & New, 2009; Jones, et al., 2012). Additionally, a
                   Lexical Organization                              set of 2,900 lexical decisions times for young and old adults
A prominent area in the study of word recognition has                attained from Balota, Cortese, and Pilotti (1999) was used to
focused on examining the influence of environmental                  test the sensitivity of the experiential fitting method to
variables on the retrieval of words from the mental lexicon.         different subject groups.
Classically, word frequency has been the most important              Data Fitting Methodology
lexical variable used to examine lexical retrieval, based on
findings that higher frequency words are processed more              Because the SDM uses paragraphs, the fitting method split
efficiently. This has led to word frequency to be considered a       each corpora into 3,000 paragraphs/documents (roughly
central information type to models of lexical retrieval.             equivalent to 50,000 sentences). For the Wikipedia corpus,
  The exact nature of frequency effects has recently been            this was a single document in the encyclopedia. For the
questioned on several grounds. In one line of research,              Amazon product descriptions, one product description was
Adelman, Brown, and Quesada (2006) demonstrated that a               considered a separate document. For the books, due to how
measure that builds a word’s strength in memory by counting          they are formatted, there was no simple method to split them
the number of contexts that a word occurs in (operationalized        into paragraphs. Instead a moving window, with a size of 15
as the number of document occurrences across a corpus)               sentences, was used to assemble paragraph-like units.
provides a superior fit to retrieval times than word frequency;        Typically, the SDM is trained on a whole corpus, as the
this finding has been replicated across different corpora and        model is dynamic: previously experienced information is
datasets (Adelman, et al., 2006; Brysbaert & New, 2009).             used to determine what should be stored for any new context.
This measure is commonly referred to as a word’s contextual          However, the model is quite computationally complex, so
diversity (CD).                                                      magnitudes were derived separately for each section. Overall
  However, Adelman et al.’s (2006) document count measure            magnitudes were then the sum of the different selected
ignores the semantic diversity of the contexts that a word           sections, which was also done for the WF and CD variables.
occurs in. To examine this possibility more closely, Jones,          These variables were transformed with a natural logarithm
Johns, and Recchia (2012) conducted an artificial language           before assessing the correlation to the data.
learning experiment that manipulated word frequency and              Results
contextual diversity, such that certain words occurred with
                                                                     The results of the experiential fitting method on the z-
different sets of words (high semantic diversity), while others
                                                                     transformed ELP lexical decision time data are displayed in
repeatedly occurred with the same set (low semantic
                                                                     the top panel of Figure 2. Only the results of the SDM are
diversity). Although there was no effect of diversity for low-
                                                                     displayed in this figure, because all three measures produce
frequency words, high frequency words were retrieved more
                                                                     similar results (explored further below). This result is
quickly when they had been learned across multiple diverse
                                                                     contrasted with the fit that CD values from the SUBTLEX
contexts, indicating that processing savings occurred only
                                                                     corpus (Brysbaert & New, 2009) provides for this data set, as
                                                                 2294

it provides the best fits to this data currently available. The        highly significant difference for the young adult sections
figure demonstrates that the use of experiential fitting allows        [F(1,39)=203.51, p<0.001] and the fiction sections
for a large increase in fit for retrieval latencies, even when         [F(1,39)=219.45, p<0.001]. These differences emerge
compared against a very well-constructed corpus, as it                 because the young subject group had a higher proportion of
outperformed SUBTLEX by a large margin. Additionally,                  young adult sections, while older adults were better described
the randomized corpora also achieved a correlation that                by the fiction sections. Given the composition of the different
equaled the SUBTLEX corpus (Brysbaert & New, 2009),                    corpora, this suggests that the retrieval time data of these
demonstrating that the source materials that the experiential          different groups are sensitive to the statistics of different
fitting method was using was of very high quality.                     linguistic sources that the subjects have experienced: young
                                                                       adults are better described by simpler examples of language
                                                                       as encoded in young adult books, but older adults are better
                                                                       accounted for by more linguistically diverse fiction and
                                                                       literature books. At least anecdotally, this is consistent with
                                                                       the type of linguistic experiences these subjects likely had.
Figure 2. Results of SDM with experiential optimization.
  As has been pretty previously shown, magnitudes from the
SD model had the highest correlation, with an r =-0.708,
p<0.001, compared with an r =-0.702, p<0.001 for contextual
diversity, and r =-0.701, p<0.001 for word frequency. As a
comparison, the correlation for CD values from SUBTLEX
is an r=-0.666, p<0.001. The SDM model providing the                   Figure 3. Proportion of different sections selected for young
superior fit is consistent with past results (Jones, et al., 2012;     and old subjects.
Johns, et al., 2012), but the interesting aspect of this
simulation is the power that experiential fitting provided to          Discussion
all three variables.                                                   This section demonstrates that the use of experiential fitting
  As noted previously, there is still a question of what the           can be expanded easily to examine lexical retrieval. There is
source of variance that the method is capitalizing on, as it is        a rich history of using environment variables (i.e. word
possible that it is not capitalizing on group or individual            frequency) to examine word retrieval patterns, with recent
characteristics, but instead random noise within the different         research pointing to the importance of contextual and
datasets. As a test of this, 2,900 lexical decision times were         semantic variables in the construction of a word’s strength in
attained from Balota, et al. (1999) for younger and older              the mental lexicon (Adelman, et al., 2006; Jones, et al., 2012).
adults. The bottom panel of Figure 2 displays the fits to              It was found that the SDM model, previously shown to
Balota et al.’s data for the SD model, and demonstrates that a         provide a superior fit to large scale lexical decision data than
high level of fit was attained for both subject groups, but with       word frequency or a document count, when combined with
a higher fit to younger than older subjects, a standard finding.       experiential fitting, provides a better accounting than
However, a more interesting analysis is to examine the                 previously published norms. Additionally, in an examination
composition of the resulting corpora for the two subject               of young and older adult lexical decision data (Balota, et al.,
groups. To do this, the proportion of the different sources that       1999), it was found that the method was sensitive to group
was selected was recorded across 20 runs of the hill-climbing          characteristics, suggesting that the method is fitting to the
algorithm. These runs were done by removing the previously             experiences that a group of subjects may have had with
selected first section for the current run, so that each run           language.
begins differently. The results of this analysis are contained
in Figure 3.                                                                               General Discussion
  There was no difference in proportions selected for the non-         The current article describes a new method for optimizing
fiction, Wikipedia, and Amazon sections, but there was a               cognitive models through experiential fitting, where the
                                                                   2295

information that a model “knows” is manipulated to provide            computational resources, but neither of these factors are
the best fit to a set of data. The manipulation was done by           limitations anymore. It is readily possible to examine the
assembling very large sets of texts spanning multiples areas,         impact of linguistic information on human behavior, and by
including an online encyclopedia, product descriptions from           optimizing the linguistic information to which a model is
Amazon, and sets of fiction, non-fiction, and young adult             exposed, it provides a powerful test of a model’s ability to
books. These corpora were split into small sections, and a            account for behavioral data.
hill-climbing algorithm was used to determine the best
combination of these materials for a specific model and set of                                       References
data. It was demonstrated that this method, combined with             Adelman, J. S., Brown, G. D. A., & Quesada, J. F. (2006). Contextual
experience-based cognitive models, provided benchmark fits                diversity, not word frequency, determines word-naming and lexical
to multiple types of lexical information.                                 decision time. Psychological Science, 17, 814-823.
                                                                      Anderson, J. R., & Schooler, L. J. (1991). Reflections of the environment in
  The underlying philosophy of our method is similar to                   memory. Psychological Science, 2, 396–408.
standard parameter fitting methods (Shiffrin, et al., 2008),          Balota, D.A., Cortese, M.J., & Pilotti, M. (1999). Item-level analyses of
which assume that there is natural variability in the                     lexical decision performance: Results from a mega-study. In Abstracts
parameters that define the cognitive processes that underlie              of the 40th Annual Meeting of the Psychonomics Society (p. 44). Los
                                                                          Angeles, CA: Psychonomic Society.
behavior. Similarly, experiential fitting is designed around          Balota, D. A., Yap, M J., Cortese, M. J., Hutchison, K. A., Kessler, B., Loftis,
the idea that there is natural variability in the knowledge               B., Neely, J. H., Nelson, D. L., Simpson, G. B., & Treiman, R. (2007).
bases that different subjects groups have (and also in                    The English lexicon project. Behavior Research Methods, 39, 445-459.
individual subjects) that leads to variability in behavior.           Barsalou, L. W. (1999). Perceptual symbol systems. Behavioral and Brain
                                                                          Sciences, 22, 577-660.
  One of the exciting aspects of this technique is that it            Brysbaert, M., & New, B. (2009). Moving beyond Kucèra and Francis: A
provides a mechanism by which to discriminate the varying                 critical evaluation of current word frequency norms and the introduction
contributions of internal cognitive mechanisms and external               of a new and improved word frequency measure for American English.
information, an old goal in the cognitive sciences (Anderson              Behavior Research Methods, 41, 977-990.
                                                                      Christiansen, M., & Chater, N. (2008). Language as shaped by the brain.
& Schooler, 1991; Simon, 1969). If one accepts that language              Behavioral and Brain Sciences, 31, 489-558.
is dictated by a complex interaction of biological and cultural       Hutchison, K. A., Balota, D. A., Cortese, M. J., & Watson, J. M. (2008).
evolution (Christiansen & Chater, 2008), then it is necessary             Predicting semantic priming at the item level. The Quarterly Journal of
to determine how much of the complexity in human behavior                 Experimental Psychology, 61, 1036-1066.
                                                                      Johns, B. T., Jones, M. N., & Mewhort, D. J. K. (2012). A synchronization
is derived from evolved mechanisms in the brain and how                   account of false recognition. Cognitive Psychology, 65, 486-518.
much is provided by the heavily structured environment in             Johns, B. T., Gruenenfelder, T. M., Pisoni, D. B., & Jones, M. N. (2012).
which humans are embedded. The simulations reported here                  Effects of word frequency, contextual diversity, and semantic
provide substantial evidence that the information used to train           distinctiveness on spoken word recognition. Journal of the Acoustical
                                                                          Society of America, 132, EL74-EL80.
a model is very important to a model’s behavior, just as              Johns, B. T., Dye, M., Jones, M. N. (2016). The influence of contextual
human behavior is sensitive to the knowledge that a person                variability on word learning. Psychonomic Bulletin and Review.
has gained. The simulation reported in Figure 3, where the            Jones, M. N., Kintsch, W., & Mewhort, D. J. K. (2006). High-dimensional
experiential fitting method found different corpus                        semantic space accounts of priming. Journal of Memory and Language,
                                                                          55, 534-552.
constructions to explain younger and older adult’s lexical            Jones, M. N., & Mewhort, D. J. K. (2007). Representing word meaning and
decision data is a promising first step that group-level                  order information in a composite holographic lexicon. Psychological
experiences can be estimated with this method.                            Review, 114, 1-37.
  More generally, this work points to the usefulness of               Jones, M. N., Johns, B. T., & Recchia, G. (2012). The role of semantic
                                                                          diversity in lexical organization. Canadian Journal of Experimental
building cognitive models around a learning mechanism that                Psychology, 66, 115-124.
is capable of extracting information from large text-bases, an        Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s problem: The
issue that has been explored in greater detail elsewhere (e.g.            latent semantic analysis theory of the acquisition, induction, and
Johns, Jones, & Mewhort, 2012). By basing a model’s                       representation of knowledge. Psychological Review, 104, 211-240.
                                                                      McAuley, J., and Leskovec, J. (2013). Hidden factors and hidden topics:
performance in the learning of large-scale environmental                  understanding rating dimensions with review text. In Proceedings of the
information, it provides a stronger case for the plausibility of          7th ACM Conference on Recommender Systems (RecSys), 165–172.
a model, as it is capable of scaling to levels of data input that     Recchia, G. L., Sahlgren, M., Kanerva, P., & Jones, M. N. (2015). Encoding
a typical person may receive.                                             sequential information in vector space models of semantics: Comparing
                                                                          holographic reduced representation and random permutation.
  As Simon (1969) described, in order to provide a complete               Computational             Intelligence          &          Neuroscience.
account of behavior, it is necessary to understand both the               doi.org/10.1155/2015/986574
internal mechanisms and the environmental information that            Shaoul, C., & Westbury, C. (2010). Exploring lexical co-occurrence space
people use to behave. This is especially important in the                 using HiDEx. Behavior Research Methods, 42, 393–413.
                                                                      Shiffrin, R. M., Lee, M. D., Kim, W., & Wagenmakers, E. J. (2008). A
study of language, as the vast majority of psycholinguistic               survey of model evaluation approaches with a tutorial on hierarchical
theories have focused on the internal mechanisms that are                 Bayesian methods. Cognitive Science, 32, 1248-1284.
responsible for linguistic behavior, while the influence of           Simon, H. A. (1969). The Sciences of the Artificial. Cambridge, MA: MIT
environmental information has been downplayed.                            Press.
  Downplaying environment information was necessary in
early work because we lacked both large amount of texts and
                                                                  2296

