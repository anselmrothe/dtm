                               Improving Visual Memory with Auditory Input
                                   Scott R. Schroeder (schroeder@u.northwestern.edu)
                            Department of Communication Sciences and Disorders, 2240 Campus Drive
                                                        Evanston, IL 60208 USA
                                       Viorica Marian (v-marian@northwestern.edu)
                            Department of Communication Sciences and Disorders, 2240 Campus Drive
                                                        Evanston, IL 60208 USA
                             Abstract                                 cognitive domains, there is, to our knowledge, no persuasive
                                                                      behavioral evidence for cross-modal interactions in episodic
   Can input in one sensory modality strengthen memory in a           memory. In several previous studies, hearing congruent
   different sensory modality? To address this question, we           auditory input during the encoding of a visual image (for
   asked participants to encode images presented in various           example, hearing a dog’s bark while encoding a picture of a
   locations (e.g., a depicted dog in the top left corner of the
   screen) while they heard spatially uninformative sounds.
                                                                      dog) was found to aid later recognition of the previously-
   Some of these sounds matched the image (e.g., the word             viewed visual image (the picture of the dog) (Lehmann &
   “dog” or a barking sound) while others did not. In a               Murray, 2005). However, the finding that congruent
   subsequent memory test, participants were better at                auditory input improves visual item memory (also known as
   remembering the locations of images that were encoded with         visual what memory) might not reflect auditory input
   a matching sound, even though these sounds were spatially          changing visual memory. When participants were presented
   uninformative – an effect that was mediated by whether the         with a visual image (a picture of a dog) on the recognition
   sounds were verbal or non-verbal. Because the sounds did not
   provide any relevant location information, better spatial          memory test, they may have remembered hearing the
   memory cannot be attributed to auditory memory; rather, it is      image’s congruent sound (the barking sound of a dog),
   attributed to visual memory being strengthened by the              which could be used to correctly indicate that they had
   matching auditory input. These findings provide the first          previously seen the image, even if the participant forgot the
   behavioral evidence for cross-modal interactions in memory.        image. Thus, the use of the helpful auditory memory trace
   Keywords: Audio-Visual Integration; Memory; Multisensory           may have led to better visual memory performance, even if
   Processing; Visual Spatial Memory                                  hearing the auditory input did not actually strengthen visual
                                                                      memory.
                         Introduction                                    To determine if auditory input can truly strengthen visual
We live in a multi-sensory world, where auditory, visual,             memory, we created a multi-sensory audio-visual memory
and other sensory inputs merge together to form our                   task in which memory for the auditory input itself could not
experiences, many of which we later try to remember. For              help participants perform the visual memory test.
                                                                      Specifically, we presented to-be-encoded visual objects in
example, consider the experience of parking your car in an
                                                                      various spatial locations on the screen (for example, an
unfamiliar place and then later trying to find it. After
parking your car, you will likely try to commit the location          image of a dog placed in the top left corner of the screen),
of the car to memory. At the same time as you are                     along with spatially uninformative auditory cues (for
memorizing the car’s location and preparing to walk away,             example, a barking sound played to both ears and thus not
you might lock the car’s doors, and the car might make a              linked in any way to the location of the image). Then, in a
sound to indicate that the doors have been successfully               later memory test, we assessed visual spatial memory (also
locked. Does hearing this auditory input (the sound of the            known as visual where memory) for the previously-seen
                                                                      images. If where memory performance for a visual image is
car) help you encode and then later remember the important
                                                                      improved when the image is encoded with a spatially
visual input (the car’s location in space)? More generally,
                                                                      uninformative but congruent auditory cue (relative to a
can input in one sensory modality strengthen episodic
                                                                      control condition), it would have to reflect better visual
memory in a different sensory modality?
                                                                      memory per se. It could not reflect the use of auditory
   The answer to this question has basic-science implications
for our understanding of how memory works in real-world,              memory to help participants perform the where memory test
ecologically-valid contexts, as our everyday experiences are          because the auditory memory trace does not contain any
largely multi-sensory in nature. The answer may also have             relevant location information and therefore would not help
                                                                      spatial memory performance. Thus, better visual where
applied-science implications for educational programs,
                                                                      memory performance in this task would provide evidence
cognitive therapies, and human factors designs, as multi-
                                                                      that input in one modality can strengthen episodic memory
sensory input might improve memory performance.
                                                                      in a different modality.
   Despite its relevance to basic- and applied-science and
                                                                         A cross-modal effect of auditory input on visual memory
despite evidence for cross-modal interactions in other
                                                                      might depend on the type of auditory input. Verbal sounds
                                                                  348

(i.e., spoken words like “dog”) and non-verbal sounds (i.e.,       20 participants completed the spoken words experiment
environmental sounds like a dog barking) are known to              first. None of the auditory or visual stimuli that appeared in
affect visual processing differently (Chen & Spence, 2011;         the first experiment also appeared in the second experiment.
Edmiston & Lupyan, 2015). Environmental sounds and
spoken words differ in the location information they provide       Environmental Sounds and Visual Memory Experiment
during visual spatial processing. Environmental sounds
(such as a barking dog) provide helpful information about          Encoding. Participants viewed 60 pictures during the
the location of the relevant object (the dog). In contrast,        encoding task. In the set of 60 pictures, 20 were presented
spoken labels (such as “dog”) provide location information         with their congruent environmental sound (i.e., the sound
about the speaker but not about the relevant object (i.e., the     associated with that object), 20 with an incongruent
dog), as the word “dog” can be uttered irrespective of the         environmental sound (i.e., the sound associated with a
specific location (or even presence) of the dog (a feature of      different object), and 20 with a neutral control sound (i.e.,
language known as displacement). Because environmental             one of twenty tonal beep sounds). A semantically neutral
sounds and spoken words differ in their spatial                    sound (a tonal beep) was used for the control condition
informativeness for relevant objects, they may have                instead of no sound. The reason for using a semantically
different effects on visual where memory.                          neutral sound as the control rather than no sound is that, in
   In the current study, we examined the effects of                no-sound trials there would be a matching context between
environmental sounds and spoken words on visual where              encoding and retrieval (i.e., both contexts would be silent),
memory (as well as visual what memory). In an                      whereas in the congruent and incongruent trials, there would
environmental sounds experiment and a spoken words                 be a mismatching context between encoding and retrieval
experiment, participants encoded a series of visual objects        (i.e., there would sound at encoding but not at retrieval).
(for example, a dog) located in one of the four corners of the     The degree of match between encoding and retrieval context
screen while hearing task-irrelevant, spatially uninformative      affects memory performance (Smith & Vela, 2001). By
auditory cues. The auditory cues were either congruent with        using a neutral sound (a tonal beep), the change in context
respect to the visual object (the sound of a dog barking           from encoding to retrieval (a sound at encoding and no
while seeing a dog), incongruent with respect to the object        sound at retrieval) is the same for all trials, be they
(the sound of a motorcycle’s exhaust while seeing a                congruent, neutral, or incongruent.
trumpet), or neutral with respect to the object (a                    To ensure that potential differences in the memorability of
semantically-meaningless beep sound while seeing a                 the pictures across congruent, neutral, and incongruent
helicopter). Participants then performed an item (what) and        conditions did not affect the results, we created the stimuli
spatial (where) memory task to test memory for what                in the following way. Four lists of 20 picture-sound pairs
pictures they saw and where they saw them. If hearing a            were compiled. Each of the four lists served in one of four
congruent auditory cue were shown to help what memory              positions – (1) as the 20 picture-sound pairs in the congruent
performance relative to the neutral control condition, this        trials, (2) as the 20 pictures in the neutral trials, which were
finding would replicate previous research and demonstrate          paired with a tonal beep sound, (3) as the 20 pictures in the
the benefits of having two sensory memory traces (or dual-         incongruent trials, or (4) as the 20 sounds in the incongruent
codes) for memory performance; however, it would not               trials. The four lists rotated, serving in all four positions an
provide evidence for cross-modal effects. The crucial test         equal number of times across participants. The benefit of
for cross-modal effects is where memory performance. A             creating four lists and having them rotate positions is that if
finding of better where memory for the congruent condition         one list of pictures is easier or harder to remember than
relative to the neutral control condition would provide            others, the results will not be affected because each list
evidence for cross-modal effects in memory.                        appears in each condition an equal number of times across
                                                                   participants and thus each condition is equally influenced by
                          Methods                                  any discrepancies between lists. Nevertheless, care was
                                                                   taken to ensure that the lists were equivalent. The words
Participants                                                       associated with the picture-sound pairs (e.g., the word “cat”
Forty English-speaking young adults (median age = 21.5             for a picture of a cat and the cat’s meow sound) were
years; 32 females, 8 males) were included in the study.            matched across all four lists on English frequency,
Participants received monetary compensation or course              concreteness, familiarity, and imageability (MRC
credit for their participation. The experiment was approved        Psycholinguistic Database).
by the Northwestern University Institutional Review Board.            All pictures were selected to be similar in saturation and
                                                                   line thickness. Each sound was edited to be 1000
Materials and Procedure                                            milliseconds in duration, so that congruent, neutral, and
                                                                   incongruent auditory cues were matched in duration. The 20
Participants completed the environmental sounds
                                                                   tonal beeps were sine waveforms ranging from 300 Hz to
experiment and the spoken words experiment in a
                                                                   2200 Hz with each tone being 100 Hz different from its
counterbalanced order, such that 20 participants completed
                                                                   nearest two tones. All sounds were peak-amplitude
the environmental sounds experiment first while the other
                                                               349

normalized using Audacity. The sounds were presented
using monophonic sound reproduction and played to both              Spoken Words and Visual Memory Experiment
ears through headphones, so as to ensure that they did not
provide any relevant spatial information.                           The spoken words experiment had the same methodology as
   Of the 60 pictures, 15 were presented in the upper left          the environmental sounds experiment unless noted below.
corner, 15 were presented in the upper right corner, 15 were           Encoding. In the spoken words experiment, participants
presented in the lower left corner, and 15 were presented in        were shown 64 pictures, of which 16 were presented with a
the lower right corner.                                             congruent auditory cue (the English word for the visual
   Each trial started with a 200-millisecond fixation cross in      object), 16 with an incongruent auditory cue (the English
the center of the screen. Following the fixation cross, a           word for a different object), 16 with a neutral non-linguistic
picture was displayed for 1000 milliseconds. Simultaneous           control auditory cue (a tonal beep sound), and 16 with a
with the onset of the picture, a sound was played for 1000          neutral linguistic control auditory cue (a pseudoword). The
milliseconds. Figure 1 provides a visual representation of          neutral non-linguistic control was included as in the
the encoding task.                                                  environmental sounds experiment. In addition to the neutral
   In the instructions of the encoding task, participants were      non-linguistic control, a neutral linguistic control was also
asked to try to remember the pictures for a later memory test       included for the purpose of having a linguistic control
but not to be concerned with remembering the sounds. After          against which to compare the congruent linguistic condition.
the encoding task, participants completed a five-minute             (The spoken words experiment had more pictures than the
filler task in which they performed a simple math test              environmental sounds experiment in order to accommodate
(participants determined which of two values is larger). The        the additional condition in the spoken words experiment;
purpose of the filler task was to prevent recency effects           note, however, that a pilot study that included an equal
(Cohen, 1989), which might lead participants to remember            number of pictures and conditions in both experiments
predominantly the last sequence of pictures, regardless of          yielded the same results.)
condition.                                                             Five lists of 16 picture-word pairs were compiled. The
   Retrieval. In the retrieval task, participants viewed 120        five lists served in one of the five positions – (1) as the 16
pictures: the 60 pictures they had seen in the encoding task        picture-word pairs in the congruent condition, (2) as the 16
(‘old’ pictures), plus 60 foil pictures that they had not seen      pictures in the neutral non-linguistic condition, which were
before (‘new’ pictures). The retrieval task had two                 paired with a tonal beep sound, (3) as the 16 pictures in the
components – an item or what memory component and a                 neutral linguistic condition, which were paired with a
spatial or where memory component. In each trial, a picture         pseudoword, (4) as the 16 pictures in the incongruent
was displayed and participants had to click ‘new’                   condition, or (5) as the 16 words in the incongruent
(indicating that they did not recognize the picture from the        condition. The pairings in the incongruent condition were
encoding task) or ‘old’ (indicating that they did recognize         created by matching a picture from one list (e.g., a trumpet)
the picture from the encoding task). If participants clicked        with a word from another list (e.g., the word “dog”). The
‘old’ (indicating that they had seen the picture before), they      five lists rotated, serving in every position an equal number
then made a judgment about its spatial location. They did so        of times across participants.
by clicking in one of four boxes located in the four corners           The words in the picture-word pairs were matched across
of the screen. A visual representation of the retrieval task is     all five lists on English frequency, English phonological
shown in Figure 1.                                                  neighborhood size, English biphone frequencies, number of
                                                                    English phonemes, concreteness, familiarity, and
                                                                    imageability        (MRC        Psycholinguistic    Database;
                                                                    CLEARPOND; Marian, Bartolotti, Chabal, & Shook, 2012).
                                                                       The pseudowords came from Colbertian, an artificial
                                                                    language (Bartolotti & Marian, 2012). Colbertian
                                                                    pseudowords were designed to conform to phonotactic rules
                                                                    of English and did not differ from the five lists of picture-
                                                                    word pairs in number of phonemes or in English biphone
                                                                    frequencies (CLEARPOND)
                                                                       The spoken word stimuli were recorded at 44100 Hz by a
                                                                    female native English speaker. All words and pseudowords
                                                                    were equal to or shorter than 1000 milliseconds in duration.
                                                                    The tonal beep sounds were 1000 milliseconds in duration
                                                                    and ranged from 250 Hz to 1750 Hz, with each tone being
      Figure 1: Top row depicts a congruent, neutral, and           100 Hz different from its nearest two tones. None of the
         incongruent trial in the encoding phase of the             tones had the same frequency as the tones in the
    Environmental Sounds and Visual Memory experiment.              environmental sounds experiment.
    Bottom row depicts the what and where retrieval trials.
                                                                350

   After the encoding task, participants completed the five-        across individuals, we computed the number of participants
minute filler math test, as in the environmental sounds             who remembered more locations in the congruent condition
experiment, but with different numbers.                             than in the neutral condition (and vice versa). Twenty-eight
   Retrieval. In the retrieval task, participants viewed 128        participants remembered more locations in the congruent
pictures: the 64 pictures they had seen in the encoding task        condition, whereas only 12 remembered more locations in
(‘old’ pictures), plus 64 foil pictures that they had not seen      the neutral condition.
before (‘new’ pictures). As in the environmental sounds
experiment, the retrieval task had two components – an item
or what memory component and a spatial or where memory
component. A visual representation of the encoding and
retrieval phases is presented in Figure 2.
                                                                     Figure 3: Memory accuracy on the spatial memory trials in
                                                                     the Environmental Sounds and Visual Memory experiment.
                                                                    Spoken Words. To analyze the effects of spoken words on
                                                                    visual spatial or where memory, a repeated-measures
      Figure 2: Top row depicts a congruent, neutral non-           ANOVA was conducted with condition (Congruent,
linguistic control, neutral linguistic control, and incongruent     Incongruent, Neutral Non-Linguistic, Neutral Linguistic) as
  trial in the Spoken Words and Visual Memory experiment.           the independent variable and spatial memory accuracy as
    Bottom row depicts the what and where retrieval trials.         the dependent variable. Accuracy by condition is presented
                                                                    in Figure 4. The ANOVA revealed no significant main
                           Results                                  effect of condition, F (3, 111) = 0.24, p > .1, ηp2 = .01.
                                                                    These results indicated that visual spatial (where) memory
Where Memory                                                        was not improved by hearing a congruent spoken word.
                                                                    Consistent with these group-level results, individual-level
Environmental Sounds. The effects of environmental sounds           results indicated that 18 participants remembered more
on visual spatial or where memory were analyzed using a             locations in the congruent condition than in the neutral non-
repeated-measures ANOVA with condition (Congruent,                  linguistic condition and 20 participants remembered more
Incongruent, Neutral) as the independent variable and               locations in the neutral non-linguistic condition than in the
accuracy on the spatial memory task as the dependent                congruent condition.
variable. Accuracy rates by condition are displayed in
Figure 3. The ANOVA yielded a significant main effect of
condition, F (2, 78) = 11.72, p < .001, ηp2 = .23. The
significant main effect was followed up with contrasts
between the experimental conditions (congruent and
incongruent) and control condition (neutral). The contrasts
indicated that the locations of pictures in the congruent
condition were remembered significantly better than the
locations of pictures in the neutral control condition (68.7%
versus 56.9%), F (1, 39) = 14.74, p < .001, ηp2 = .27.
Conversely, the locations of pictures in the incongruent
condition were not remembered significantly differently
than the locations of pictures in the neutral control condition
(57.8% versus 56.9%), F (1, 39) = 0.11, p > .1, ηp2 = .003.
These results suggest that visual spatial (where) memory
was improved by hearing a congruent environmental sound.             Figure 4: Memory accuracy on the spatial memory trials in
To determine whether this group-level effect was consistent              the Spoken Words and Visual Memory experiment.
                                                                351

What Memory                                                          better and incongruent pictures were remembered
                                                                     significantly better than pictures in the neutral linguistic
Environmental Sounds. To analyze the effects of                      control condition (73.8%) (F (1, 37) = 3.00, p = .09, ηp2 =
environmental sounds on visual item or what memory, a                .08 and F (1, 39) = 7.37, p < .05, ηp2 = .17, respectively).
repeated-measures ANOVA was conducted with condition                 These results provide evidence that hearing a congruent (or
(Congruent, Incongruent, Neutral) as the independent                 incongruent) spoken word improved visual item (what)
variable and accuracy on item memory trials as the                   memory performance. These group-level results were also
dependent variable. The accuracy rates for the three                 reflected in the individual data, with 19 participants
conditions are displayed in Figure 5. The ANOVA yielded a            remembering more congruent pictures than neutral non-
significant main effect of condition, F (2, 78) = 4.21, p <          linguistic pictures, 13 participants remembering more
.05, ηp2 = .10. Follow-up contrasts revealed that pictures in        neutral non-linguistic pictures than congruent pictures, and
the congruent condition were recognized at a significantly           6 participants remembering the same number of both.
higher rate than pictures in the neutral control condition
(79.3% versus 73%), F (1, 39) = 5.47, p < .05, ηp2 = .12.
Similarly, pictures in the incongruent condition were
recognized with significantly higher accuracy than pictures
in the neutral control condition (77.9% versus 73%), F (1,
39) = 5.32, p < .05, ηp2 = .12. These results indicate that
hearing a congruent (or incongruent) environmental sound
helped visual item (what) memory performance. These
results at the group level were consistent with the results at
the individual level, as 24 participants remembered more
congruent pictures than neutral pictures, 13 participants
remembered more neutral pictures than congruent pictures,
and 3 participants remembered the same number of
congruent and neutral pictures.
                                                                     Figure 6: Memory accuracy on the item memory trials in the
                                                                            Spoken Words and Visual Memory experiment.
                                                                                             Discussion
                                                                     We examined whether auditory input can impact visual
                                                                     memory. The results showed that hearing a congruent yet
                                                                     spatially uninformative environmental sound (for example, a
                                                                     barking sound played to both ears) improved memory for
                                                                     where a visual object was located (for example, a dog
                                                                     located in the top left corner of the screen). Because
                                                                     improved spatial memory in this case cannot be attributed to
                                                                     better auditory memory (memory for the environmental
Figure 5: Memory accuracy on the item memory trials in the           sound had no valid location information and therefore
   Environmental Sounds and Visual Memory experiment.                would not lead to a correct answer on the visual spatial
                                                                     memory test), the results are attributable to visual memory
Spoken Words. The effects of spoken words on item or what            being improved by the environmental sound. To our
memory were analyzed using a repeated measures ANOVA                 knowledge, these results provide the first behavioral
with condition (Congruent, Incongruent, Neutral Non-                 evidence for a cross-modal interaction in memory.
Linguistic, Neutral Linguistic) as the independent variable             The effects of auditory input on where memory depended
and accuracy on item memory trials as the dependent                  on the type of sound. While environmental sounds
variable. Accuracy rates by condition are presented in               strengthened visual spatial memory, spoken words did not.
Figure 6. The ANOVA yielded a significant main effect of             These differences can be explained by theories positing that
condition, F (3, 108) = 4.56, p < .01, ηp2 = .11. Follow-up          people unconsciously generate expectations on the basis of
contrasts revealed that pictures in both the congruent               learned regularities (for example, predictive coding and
(77.5%) and incongruent (78%) conditions were recognized             schema theory). According to these theories, our cognitive
significantly better than pictures in the neutral non-linguistic     system would expect an environmental sound, such as a
control condition (71.2%) (F (1, 37) = 5.13, p < .05, ηp2 =          dog’s bark, to carry helpful location information because of
.12 and F (1, 37) = 10.31, p < .01, ηp2 = .22, respectively).        the learned regularity that environmental sounds nearly
Moreover, congruent pictures were remembered marginally              always correlate with the location of the visual object (that
                                                                 352

is, the dog’s bark comes from the location of the dog).            multi-sensory research on perception and attention by
Because of this expectation, we may become more                    showing that audio-visual interactions are not just short-
responsive to the relevant visual spatial information upon         lived perceptual and attentional effects; they have longer-
hearing an environmental sound. In contrast, our cognitive         term consequences that persist in memory. Because most of
system would be unlikely to expect a spoken label, such as         our experiences are multi-sensory, these results capture how
the word “dog”, to carry helpful location information about        memory works in everyday situations. These findings may
the dog because spoken labels rarely correlate with the            also carry practical implications, as cross-modal
location of the visual object to which they refer. With a          enhancements may be applied to increase memory in
weaker expectation for valid spatial information about the         educational programs, cognitive therapies, and human
referent, we might not be especially responsive to the             factors designs.
relevant visual spatial information upon hearing a spoken
label.                                                                                 Acknowledgments
   A second possible explanation for why congruent                 This research was funded by grant NICHD R01 HD059858
environmental sounds (but not congruent spoken words)              to Viorica Marian.
enhanced where memory is that congruent environmental
sounds may have elicited deeper processing, heightened
arousal, or increased attention to the stimuli (relative to
                                                                                            References
congruent spoken words). However, if congruent                     Bartolotti, J., & Marian, V. (2012). Language learning and
environmental sounds prompted deeper processing,                      control in monolinguals and bilinguals. Cognitive
heightened arousal, or increased attention to the stimuli,            Science, 36(6), 1129-1147.
then congruent environmental sounds should have also               Chen, Y. C., & Spence, C. (2011). Crossmodal semantic
enhanced what memory to a larger degree than congruent                priming by naturalistic sounds and spoken words
spoken words because deeper processing, heightened                    enhances visual sensitivity. Journal of Experimental
arousal, and increased attention to the stimuli are all known         Psychology: Human Perception and Performance, 37(5),
to produce stronger what memory (Craik & Tulving, 1975;               1554-1568.
Dolcos, LaBar, & Cabeza, 2004; Naveh-Benjamin, Guez, &             Cohen, R. L. (1989). The effects of interference tasks on
Marom, 2003). Yet, relative to a neutral control sound,               recency in the free recall of action events. Psychological
congruent       environmental        sounds       did      not        Research, 51(4), 176-180.
enhance what memory more than congruent spoken words.              Craik, F. I., & Tulving, E. (1975). Depth of processing and
   Still another explanation for why congruent                        the retention of words in episodic memory. Journal of
environmental sounds (but not congruent spoken words)                 Experimental Psychology: General, 104(3), 268-294.
enhanced where memory relates to the ventriloquism effect.         Dolcos, F., LaBar, K. S., & Cabeza, R. (2004). Dissociable
According to the ventriloquism effect (Slutzky &                      effects of arousal and valence on prefrontal activity
Recanzone, 2004), simultaneously hearing a sound and                  indexing emotional evaluation and subsequent memory:
seeing an image can sometimes lead to the illusion that the           An event-related fMRI study. Neuroimage, 23(1), 64-74.
sound is coming from the image. If the ventriloquism effect        Edmiston, P., & Lupyan, G. (2015). What makes words
occurred for congruent environmental sounds (but not for              special? Words as unmotivated cues. Cognition, 143, 93-
congruent spoken words), it may have yielded helpful                  100.
spatial encoding of the congruent environmental sounds.            Lehmann, S., & Murray, M. M. (2005). The role of
However, this explanation assumes that the ventriloquism              multisensory      memories        in   unisensory      object
effect depends on semantic congruence and on type of                  discrimination. Cognitive Brain Research, 24(2), 326-
auditory cue, is precise enough to distinguish spatial                334.
locations within centimeters, and is reliable with                 Marian, V., Bartolotti, J., Chabal, S., & Shook, A. (2012).
headphones. At present, none of these assumptions has                 CLEARPOND: Cross-linguistic easy-access resource for
strong empirical support.                                             phonological       and      orthographic       neighborhood
   The finding that congruent environmental sounds and                densities. PloS one, 7(8), e43230.
spoken words increased what memory replicates several              Naveh-Benjamin, M., Guez, J., & Marom, M. (2003). The
previous studies (e.g., Lehmann & Murray, 2005). It is                effects of divided attention at encoding on item and
possible that these results are due in part to cross-modal            associative memory. Memory & Cognition, 31(7), 1021-
interactions in memory, but they can also be attributed to            1035.
dual-coding, where a memory is encoded in both the                 Slutsky, D. A., & Recanzone, G. H. (2001). Temporal and
auditory modality and visual modality, and later visual               spatial      dependency        of     the      ventriloquism
memory performance is helped by remembering the                       effect. Neuroreport, 12(1), 7-10.
encoded auditory cue.                                              Smith, S. M., & Vela, E. (2001). Environmental context-
   In conclusion, we found that auditory input can strengthen         dependent memory: A review and meta-
visual episodic memory. The current results provide                   analysis. Psychonomic Bulletin & Review, 8(2), 203-220.
evidence for cross-modal interactions in memory and extend
                                                               353

