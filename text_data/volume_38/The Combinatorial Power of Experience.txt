                                       The Combinatorial Power of Experience
    Brendan T. Johns,1 Randall K. Jamieson,2 Matthew J. C. Crump,3 Michael N. Jones, 4 & Douglas J. K. Mewhort5
        1
          btjohns@buffalo.edu, Department of Communicative Disorders and Sciences, University at Buffalo, Buffalo, NY
                                 2
                                  Department of Psychology, University of Manitoba, Winnipeg, MB
                                     3
                                      Department of Psychology, Brooklyn College, Brooklyn, NY
                              4
                               Department of Psychological and Brain Sciences, Indiana University, IN
                                    5
                                      Department of Psychology, Queen‚Äôs University, Kingston, ON
                             Abstract                                 this explanation into question. Jamieson and Mewhort
                                                                      demonstrated, across a wide variety of tasks and
   Recent research in the artificial grammar literature has found
   that a simple exemplar model of memory can account for a           manipulations that a simple explicit memory model (Minerva
   wide variety of artificial grammar results (Jamieson &             2; Hintzman, 1986) can account for most of the relevant
   Mewhort, 2009, 2010, 2011). This classic type of model has         findings in the artificial grammar literature.
   also been extended to account for natural language sentence           The reason why a simple recording of the environment
   processing effects (Johns & Jones, 2015). The current article      could explain artificial language results is clear: the stimuli
   extends this work to account for sentence production, and          contained in an artificial grammar task provide a lot of
   demonstrates that the structure of language itself provides
   sufficient power to generate syntactically correct sentences,      information about the structure of the underlying grammar.
   even with no higher-level information about language provided      That is, like natural language, the artificial languages that are
   to the model.                                                      constructed from artificial grammars are regular. This
                                                                      regularity causes the underlying structure of a grammar to
Keywords: Language production; Computational models of
language; Corpus-based models.
                                                                      emerge across the exemplars that are displayed in an artificial
                                                                      grammar experiment, even with no higher-level abstraction
                         Introduction                                 taking place during learning.
                                                                         To formalize this, Jamieson & Mewhort (2011) have
Human languages are both productive and regular. They are             implemented the latter position of a string using a model of
productive in that an infinite number of utterances are               memory that combines the storage and retrieval operations
possible for any language, and regular in that the utterances         from Hintzman‚Äôs (1986) MINERVA 2 model of episodic
produced by speakers are systematic. In order to explain              memory with the holographic reduced representations from
these aspects of language, it has been proposed that it is            Jones and Mewhort‚Äôs (2007) BEAGLE model of word
necessary to have a formal grammar underlying language                meaning. According to the model, each studied grammatical
performance (Chomsky, 1988). A grammar of sufficient                  sentence is stored to memory. When a probe is presented at
complexity can construct utterances of any length while               test, it retrieves all of the stored sentences in parallel. If the
maintaining consistency in utterance construction.                    information retrieved from memory is consistent with the
   The need and evidence for formal grammars in explaining            probe, the probe is judged to be grammatical; else, it is judged
language performance has been argued in many different                to be ungrammatical.
places (e.g. Christiansen & Chater, 2008; Evans & Levinson,              Despite the model‚Äôs simplicity, it predicts a surprising
2009). The goal of this article is not a rehashing of this debate     number of results in the artificial-grammar task including (a)
but instead to examine the interaction between regularity and         the linear relationship between mean judgement accuracy and
productivity, cornerstones of any theory of language, with the        the redundancy of the generative grammar, (b) judgements of
ultimate goal being to understand the constraints that the            grammaticality for individual test items, (c) grammatical
structure of language itself provides. Specifically, given that       string completion, and (d) variation in peoples‚Äô judgements
languages are highly regular and consistent, the question that        depending on how they represent strings in memory (Chubala
will be examined here is what power this regularity provides          & Jamieson, 2013; Jamieson & Mewhort, 2009, 2010, 2011).
to the production of grammatically correct utterances.                But, why?
   Historically, to make the study of language more                      The power of this model comes from the natural correlation
manageable, researchers have studied how people learn the             between the form and amount of structure in an exemplar
grammar of small artificial languages. Classic accounts of            produced with a grammar and the grammar that was used to
artificial grammar results propose that as subjects are               produce it (Jamieson & Mewhort, 2005). It follows, then, that
exposed to strings randomly generated from a pre-defined              each studied grammatical exemplar provides information
grammar, they are capable of using this experience to                 about the underlying grammar. It also follows that a
internally generate a representation of this grammar, and in          collection of grammatical exemplars will almost always
turn use this abstraction to discriminate between grammatical         provide a sum of information greater than that provided by
and ungrammatical strings (Reber, 1967). However, people              one exemplar alone. That is, it is the combination of
are unable to verbally describe the rules they used to                exemplars that gives the model its power. The question, then,
accomplish this discrimination (i.e., it is implicit knowledge).      is not how information about the grammar can be stored in
Recent results by Jamieson and Mewhort (2009, 2010) call              memory, but how is that information harnessed at retrieval?
                                                                  1325

In artificial grammar experiments, parallel retrieval of             where o is the order vector, n is the number of words in the
grammatical exemplars is sufficient (Jamieson & Mewhort,             sentence, wi is the word in serial position i, wi-1 is the word in
2011). But, can that class of explanation scale from handling        serial position i ‚Äì 1, wi-2 is the word in serial position i ‚Äì 2, li
small artificial grammars to handling how people learn               is a vector that represents serial position i, and ‚äõ denotes
grammar in their natural language?                                   directional circular convolution (see Jones & Mewhort, 2007;
   The goal of this article is to examine the power that the         Plate, 1995). As shown, the order vector sums information
productive and regular nature of language provides in                about (a) what word appears in each serial position in the
allowing an exemplar model of natural language to produce            sentence (i.e., serial position information), (b) which pairs of
grammatically correct utterances. Exemplar models are                words follow one another from left to right in the sentence
attractive models to examine this question because the               (i.e., bigram information), and which triplets of words follow
complexity of the model lies in the complexity of the                one another from left to right in the sentence (i.e., trigram
information that is being encoded. By storing natural                information). Given the inclusion of trigram information, the
language sentences within an exemplar memory, and                    formula cannot be applied to a sentence with fewer than three
determining how effective this stored information is at              words.
generating grammatically correct utterances, an examination             Finally, a sentence‚Äôs vector representation, s, is a 2N
of the power of the structure of language is provided.               dimensional vector formed by concatenating the N
                                                                     dimensional context vector and the N dimensional order
        The Exemplar Production Model (EPM)                          vector such that dimensions 1‚Ä¶N in s store the context vector
Our model combines the storage and retrieval scheme from             and dimensions N+1‚Ä¶2N in s store the sentence‚Äôs order
Hintzman‚Äôs (1986) MINERVA 2 model of episodic memory                 vector. Thus a sentence is represented as a vector s that is
with the reduced holographic representation scheme from              equal to c // o, where // represents concatenation.
Jones and Mewhort‚Äôs (2007) BEAGLE model of semantics,
and is similar to an exemplar approach to language                   Storage of language experience
comprehension recently explored by Johns & Jones (2015).             To represent experience with language, we store m sentences
Representation                                                       to a m ÔÇ¥ 2N matrix, where rows represent sentences and
                                                                     columns represent features that encode the information in the
In the model, each word is represented by its own unique
                                                                     sentence. Thus, memory for 1000 sentences is represented in
random vector, w, of dimensionality N, where each
                                                                     a 1000 ÔÇ¥ 2048 matrix whereas memory for 125,000 sentences
dimension takes a randomly sampled value from a normal
                                                                     is represented by a 125,000 ÔÇ¥ 2048 matrix.
distribution with mean zero and standard deviation 1/‚àöùëÅ. In
the simulations that follow, N = 1024.                               Retrieval
   Any given sentence is represented by two vectors, both of         Retrieval in the model is probe-specific, similarity- driven,
which are constructed from the word representations. The             and parallel. When a probe is presented to memory, it
first vector, c, is called the sentence‚Äôs context vector and is      interacts with the information in the stored traces to construct
computed as,                                                         the memory of a previously experienced event. Decision
                                  ùëõ
                                                                     follows from the construction. Since retrieval is similarity-
                                                                     driven, a probe retrieves traces that are similar to it. Because
                            ùêú = ‚àë ùê∞ùíä                                 a probe retrieves whole traces from memory and these whole
                                 ùëñ=1                                 traces record both context and order information in a
                                                                     sentence, a probe that includes just the context information
where c is the context vector, n is the number of words in the       will also retrieve the order information that it has co-occurred
sentence, and wi is the vector that represents the word in serial    with in the past. This is how the model simulates cued-recall,
position i of the sentence. As shown, the context vector sums        and it is the mechanism that the model uses to retrieve a
the information from all of the words that appear in the             sentence (i.e., word order) given a context vector (i.e., an
sentence, but it does not include any information about the          unordered list of words).
order in which the words occurred. For example, the context             The echo, e, is computed as,
vector that encodes ‚Äúeat your dinner‚Äù is equal to the context
vector that encodes ‚Äúdinner your eat‚Äù.                                                                                9
   The second vector, o, is called the sentence‚Äôs order vector                         ùëö
                                                                                                ‚àëùëÅ ùëó=1 ùëùùëó √ó ùëÄùëñùëó
and is computed as,                                                              ùêû=‚àë                                    √ó ùëÄùëñ
                                                                                      ùëñ=1   ‚àö ‚àëùëÅùëó=1 ùëùùëó
                                                                                                       2
                                                                                                         ‚àö‚àëùëÅ     2
                                                                                                            ùëó=1 ùëÄùëñùëó
                ùëõ               ùëõ                                                         (                         )
         ùê® = ‚àë ùê∞ùëñ ‚äõ ùê•ùëñ + ‚àë ùê∞ùëñ‚àí1 ‚äõ ùê∞ùëñ
               ùëñ=1             ùëñ=2                                   where p is the context vector that encodes an unordered list
                                 ùëõ
                                                                     of words (i.e., includes information in serial positions 1‚Ä¶N
                             + ‚àë ùê∞ùëñ‚àí2 ‚äõ ùê∞ùëñ‚àí1 ‚äõ ùê∞ùëñ                    with serial positions N+1‚Ä¶2N set to zero), M is the memory
                                ùëñ=3                                  matrix that stores the model‚Äôs sentence knowledge, e is the
                                                                     echo, and m is the number of sentences stored in memory. As
                                                                 1326

with a sentence representation, features 1‚Ä¶N in e represent         (Abbot-Smith & Tomasello, 2006; Jamieson & Mewhort,
the context vector retrieved from memory and features               2010, 2011; Johns & Jones, 2015).
N+1‚Ä¶2N in e represents the order vector retrieved from
memory. Although it is not explicitly stated in the formula,        Sentences
if the similarity between a probe and memory trace is less            Given our goal is to conduct an analysis of natural
than zero, the similarity is rewritten as equal to 0 (i.e.,         language, it was critical that we get a fair sample of natural
equivalent to excluding the trace from the calculation of the       language use. To meet that demand and to model a wide
echo). This same procedure was used in Johns and Jones              range of language experience, we assembled a pool of
(2015) and Kwantes (2005), and is useful because it allows          6,000,000 sentences from a number of sources including
for the amount of noise in the echo to be reduced.                  Wikipedia articles, Amazon product descriptions (attained
                                                                    from McAuley & Leskovec, 2013), 1000 fiction books, 1050
Decision                                                            non-fiction books, and 1500 young adult books. Once
Our goal is to measure the model‚Äôs ability to produce a             collated, we edited the list to exclude repetitions and, then,
syntactic sentence composed of words presented in an                we organized the total list into sub-lists of sentences
unordered words list. For example, given the words eat,             composed of 3, 4, 5, 6, and 7 words. Finally, we used the
dinner, and your, we would like the model to produce ‚Äúeat           sentences in the final pool to construct a list of 200 three word
your dinner‚Äù rather than ‚Äúdinner eat your‚Äù.                         test sentences, 200 four word test sentences, 200 five word
  To accomplish the transformation from unordered word list         test sentences, 200 six word test sentences, and 200 seven
to syntactic production, the model compares the order vector        word test sentences. All sentences simple in construction,
in the echo to each of the n! order vectors corresponding to        and use mostly high frequency word, but given the
the n! ways or ordering the words in the unordered list. For        complexity of the task provide a useful assessment of the
example, given the list eat, your, and dinner the model             model‚Äôs performance (all test sentences can be found at
retrieves an order vector based on the context vector, c = weat     http://btjohns.com/experience_sents.zip).            No     general
+ wyour + wdinner, and then compares the retrieved order vector     syntactic construction was used, but the majority consist of
against all 3! = 6 sentences that can be constructed from the       single phrase structures. Examples of sentences used for each
three words: ‚Äúeat your dinner‚Äù, ‚Äúeat dinner your‚Äù, ‚Äúyour eat        sentence size are:
dinner‚Äù, ‚Äúyour dinner eat‚Äù, ‚Äúdinner eat your‚Äù, and ‚Äúdinner            3 - Eat your dinner.
your eat‚Äù. The order vector that is most similar to the               4 - The children were flourishing.
information in the echo is selected as the best alternative.          5 - He walked into the bedroom.
Because all other orders bear some similarity to the order            6 - She held something in one hand.
information in the echo, the operation can also be used to rank
order the model‚Äôs preference over all possible n! sentences         Simulation parameters
from first (i.e., most similar) to last (i.e., least similar).      We conducted simulations as a function of two key
                                                                    parameters: sentence length (i.e., n) and language experience
Summary                                                             (i.e., m). Analysis of sentence length was accomplished by
In summary, the model builds and stores a representation of         conducting separate simulations for sentences of length n =
each sentence in a text corpus. When presented with an              3, 4, 5, 6, and 7. Analysis of language experience was
unordered word list, the model retrieves a corresponding            accomplished by conducting separate simulations given m =
order vector and produces a word order that corresponds to          1000, 2000, 3000 ‚Ä¶ 125,000 sentences stored in memory.
the order vector that is most similar to the order vector           To ensure that results were not conditional on a particular
retrieved from memory.                                              record of language experience, each simulation was
                                                                    conducted using a different random sample of m words.
                         Simulations                                Crossing both factors produces a 5 (sentence length) x 125
The simulations that follow apply the model to a sentence           (language experience) factorial design.
production task. Each simulation involved two major steps.          Two measurements of performance
First, we constructed a record of language experience by
                                                                    We measured sentence completion performance two ways.
storing m sentences of length n to memory; the sentences
                                                                    The first method tallied the percentage of tests in which the
were sampled randomly from a corpus. Second, we computed
                                                                    model most preferred the word order that corresponded to the
the model‚Äôs ability to translate each of 200 unordered word
                                                                    original sentence.
lists of length n into ordered sentences of length n.
                                                                      The second method ranked the model‚Äôs decisions for all
  We expect the model will re-write unordered word lists as
                                                                    possible word orders from first (i.e., most similar to the order
syntactic sentences. If true, our simulations would
                                                                    vector in the echo) to last (i.e., least similar to the order vector
demonstrate that parallel retrieval from a record of language
                                                                    in the echo) and, then, recording the rank at which the original
is sufficient to produce at least one hallmark of syntactic
                                                                    input sentence appeared. For example, if the model was given
behavior. This would reinforce the power of exemplar
                                                                    ‚Äúeat your dinner‚Äù it would produce a rank order of all six
models of language, and would add to the growing literature
                                                                    possible sentences composed of the three input words. If ‚Äúeat
on the importance of individual experience with language
                                                                1327

your dinner‚Äù was the third preferred word order, the trial          Also shown, the model‚Äôs performance improves as a
would be scored as a rank 3 decision.                             function of language experience, with the improvement being
  In summary, the first percent correct measure was an            fastest early on in the addition of sentences to memory and
absolute index of model performance, as if the model (like an     slowing considerably after memory includes a record of
experimental subject) provided a single response for each test    approximately 50,000 sentences.
sentence. The second rank based measure offers a more               The bottom panel in Figure 1 presents a more nuanced
nuanced assessment that measures how close the model was          picture of results. As shown, when language experience is
to making the right decision whether its first choice matched     modest, m = 1000, the model commits far more misses (i.e.,
or did not match the exact word order in the test sentence.       large mean rank scores). But as language experience
                                                                  increases, mean rank scores drop considerably to nearly 1 for
                                                                  most sentence lengths, and under 10 for all. Combined, these
                                                                  figures show that even though the model does not always
                                                                  choose the particular word order in the input sentence, it
                                                                  nevertheless has a strong preference for that specific word
                                                                  order. So, what does a near miss mean?
                                                                    Although we have scored near misses as wrong, they may
                                                                  occasionally be correct in the broader sense. For example,
                                                                  consider that the model preferred ‚Äúthey quietly went down
                                                                  the stairs‚Äù when tested on ‚Äúthey went quietly down the stairs‚Äù.
                                                                  Although the model failed to produce the input sentence, it
                                                                  nevertheless generated a syntactic alternative.
Figure 1. Syntactic completion rates. Top panel shows the
percentage of items that the model reproduced the exact
word order in the test sentence. Bottom panel shows the
mean rank order of the model‚Äôs preference for the exact word
order in the test sentence where 1 = best.
Results
Figure 1 shows results over the complete 5 ÔÇ¥ 125 factorial
design. The top panel in Figure 1 shows the percent correct
production rate (e.g., the model returned ‚Äúeat your dinner‚Äù
when presented with ‚Äúdinner your eat‚Äù). The bottom panel in
Figure 1 shows the mean rank of the model‚Äôs decisions. In         Figure 2. Performance from Figure 1 where m = 125,000 and
both cases, performance for 3, 4, 5, 6, and 7 words test          results from a new simulation were m = 500,000.
sentences are presented as different lines with language
experience measured in number of sentences ordered along            Figure 2 shows results when m = 125,000 sentences
the abscissa.                                                     (hatched bars; previous data from Figure 1) and when m =
  As shown in the top panel of Figure 1, the model‚Äôs ability      500,000 sentences (closed bars). The top panel shows results
to reproduce word order in the presented test sentence varies     with the percent correct measure; the bottom panel shows
as a monotone function of sentence length, being best for         results with the average rank measure. Although the results
short sentences and worse for long sentences. However, one        in the hatched bars in Figure 2 are already presented in Figure
must exercise caution in making that comparison. Chance           1, the re-presentation shows differences that cannot be seen
performance changes dramatically as a function of sentence        in in the ranking data in Figure 1.
length. At the extremes, when n = 3 the probability of              As shown, the mean rank was less than 2 for the three, four,
guessing the correct word order is equal to 1 in 6 whereas        and five word sentences, was less than 4 for the six word
chance is equal to 1 in 5040 when n = 7. Thus, performance        sentences, and was less than 10 for the seven word sentences.
at m = 1000, the final score of 90% correct in the three word     Given the average expected ranks for guessing with 3, 4, 5,
condition might be considered as or even less impressive than     6, and 7 word sentences are equal to 3, 12, 60, 360, 2520, and
the corresponding but lower score of 52% correct in the seven     5040 respectively, the results are very impressive indeed.
word condition.                                                   Plus, as we have already mentioned, whereas a rank greater
                                                              1328

than one indicates that the model failed to reproduce the exact     between abstract categories. The approach taken here with
order of words from the input sentence, the model still might       the Exemplar Production Model is the opposite of this: use a
have chosen an alternative syntactic completion‚Äîa                   model that only knows the structure of past utterances, and
possibility that increases with the number of words in the          use that experience to construct a future utterance. That is, it
sentence. We plan to assess this possibility in the future.         is a bottom-up approach, where past experience controls
   Finally, the solid bars in Figure 2 show results for             future behavior. The EPM was designed to exploit the
simulations where m was increased from 125,000 to                   productivity and regularity of natural language, in order to
500,000‚Äîa larger sample of language experience that is              determine the power of experience in producing grammatical
more in line with work in other corpus based model analyses         utterances.
(e.g., Landauer & Dumais, 1997).                                      The EPM proposes that it is not an analysis or encoding of
   As shown, increasing the number of sentences in memory           a single utterance that provides the model power. Instead it is
from 125,000 to 500,000 produced a modest 2-3% overall              the overlap in the usage of language: even though no two
improvement of performance but with most of that                    utterances may be identical (productivity; this was ensured in
improvement in judgments about longer sentences. This               the above simulations by removing repeat sentences), the
result reinforces our previous conclusion that after an initial     structure of a language emerges as a function of recorded
50,000 sentences are stored in memory (see Figure 1), each          exemplars. That is, it is the combinatorial statistics that
additional sentence has a diminished impact on the model‚Äôs          emerges from the parallel retrieval of a relatively large
performance.                                                        number of sentence exemplars that provides the model the
   In summary, Figures 1 and 2 represent a very high level of       ability to construct grammatically correct sentences. What
performance, even at large sentence sizes. For smaller              this suggests is that the consistent, but not identical, structure
sentence sizes of 3, 4, and 5, the model operated at greater        of studied utterances allows for the development of grammar-
than 75% correct, and was greater than 50% correct even at          like behavior, albeit without an actual grammar.
seven word sentences. Interestingly, much of the                      The EPM is a simple model that encodes pure location and
improvement in the model‚Äôs performance was made with a              linear n-gram information to encode an exemplar of a
relatively small number of sentences, with small                    sentence. A classic retrieval operation, based off of
improvements after 50,000 sentences. The reduction in               MINERVA 2, is used to construct the likely ordering of a
performance across sentence sizes is linear, suggesting that        sentence. Every possible ordering of a sentence is tested, with
as more sentence types are possible, the introduction of noise      the ordering that is most similar to the expected structure
reduces model performance by a constant. In fact, the final         being produced. There is no higher-level processing
ranking across the different sentence sizes was almost              integrated into the model, and so the behavior of the model is
entirely due to the number of alternatives in the construction      entirely experience-dependent. In that sense, the theory is
process, with a Pearson correlation equal to 0.99 between           perfectly continuous with our previous efforts to build an
final ranking and number of permutations. As already                exemplar-based model of language learning and
discussed, this may also be a function of there being more          comprehension using the same mechanisms and ideas (see
possible syntactic constructions for words being greater with       Johns & Jones, 2015). However, there are some differences
a higher number of words.                                           in the details of the current and previous models that need to
   In conclusion, the model demonstrates a simple point: a raw      be resolved before a complete integration of the two is
record of language experience combined with parallel                realized. We take the problem of that integration as a
retrieval can provide a powerful mechanism to account for           challenge that would move toward the kind of model needed
how people order words in sentences. The analysis also              to generate a complete picture of how an exemplar-based
suggests that the body of linguistic experience need not even       model of memory can serve as a valuable competitor in the
be overwhelmingly large and that a relative few (i.e., 50,000       discipline‚Äôs pursuit of a theory of language and language use.
exemplars) can go a long way to helping a person produce              The model was shown to be able to construct the correct
syntactic word orders in their natural language. Finally, the       ordering of simple sentences of sizes 3 to 7 to a high degree,
analysis also demonstrates that an appreciation of syntax can       with a linear drop in performance as sentence size increases.
emerge from the application of a simple parallel retrieval          This demonstrates that past experience with language
mechanism applied to a realistically scaled representation of       provides a large amount of power in producing
language experience.                                                grammatically correct utterances.
                                                                      However, the really interesting part of the model‚Äôs behavior
                    General Discussion                              is the performance of the EPM as a function of the number of
Natural languages are defined by productivity and regularity.       exemplars it has studied. Performance rapidly increases with
They are capable of producing an infinite number of different       the first 20-25,000 sentences studied, with small
utterances, with all the utterances having a consistent             improvements subsequently (the learning function most
structure. To account for these differing aspects of language       resembles a power law). Even when the total number of
it has been proposed that a formal grammar is necessary.            exemplars studied is quadrupled from 125k to 500k
   A formal grammar is a top-down approach, which seeks to          sentences, only a small improvement in performance is seen.
understand language processing from the connections                 However, it does provide a look into what the regular nature
                                                                1329

of language provides productivity: even with a small amount              implicit learning: An analysis using information theory.
of linguistic experience, the correct structure of language              Journal of Experimental Psychology: Learning, Memory,
emerges, due to the highly structured nature of natural                  and Cognition, 31, 9‚Äì23.
language. Language is far from random and this consistency            Jamieson, R. K., & Mewhort, D. J. K. (2009). Applying an
provides a simple model the ability to construct                         exemplar model to the artificial-grammar task: Inferring
grammatically correct sentences, without any higher-level                grammaticality from similarity. Quarterly Journal of
processing. As more exemplars are stored, the overlap in                 Experimental Psychology, 62, 550-575.
structure of the sentences emerges (due to the productivity of        Jamieson, R. K., & Mewhort, D. J. K. (2010). Applying an
language), which allows for the model to exploit the                     exemplar model to the artificial-grammar task: String-
combinatorial nature of language usage.                                  completion and performance for individual items.
  This is not to say that this approach does not have any                Quarterly Journal of Experimental Psychology, 63, 1014-
challenges. The main one being that the model potentially                1039.
operates at the wrong level of analysis ‚Äì phrases may be the          Jamieson, R. K., & Mewhort, D. J. K. (2011).
right unit of exemplars rather than whole sentences, as is the           Grammaticality is inferred from global similarity: A reply
typical case in generative linguistics. Sentences then can be            to Kinder (2010). Quarterly Journal of Experimental
constructed by determining the correct order of phrases,                 Psychology, 64, 209-216.
integrating higher-level information into the exemplar                Johns, B. T., & Jones, M. N. (2015). Generating structure
construction process. This would also allow for the model to             from experience: A retrieval-based model of language
operate with lower number of words, which would be                       processing. Canadian Journal of Experimental
advantageous due to the model becoming computationally                   Psychology, 69, 233-251.
burdensome at a high number of words. It would also allow             Jones, M. N., & Mewhort, D. J. K. (2007). Representing word
for the model to be tested on longer sentence sizes.                     meaning and order information in a composite
  Another issue with the model concerns its encoding                     holographic lexicon. Psychological Review, 114, 1-37.
scheme: if it is generating the structure of a sentence of size       Kwantes, P. J. (2005). Using context to build semantics.
n, it studies only exemplars of the same size. More research             Psychonomic Bulletin & Review, 12, 703-710.
is required to determine the best mechanism to encode                 Landauer, T. K., & Dumais, S. T. (1997). A solution to
location in a relative fashion, where exemplars of differing             Plato‚Äôs problem: The latent semantic analysis theory of
length can be included in the same retrieval process.                    the acquisition, induction, and representation of
  However, these problems arise because of the simplicity of             knowledge. Psychological Review, 104, 211-240.
the approach, which is also its most promising feature. There         McAuley, J., and Leskovec, J. (2013). Hidden factors and
is very little built into the machinery of the model and it still        hidden topics: understanding rating dimensions with
operates at a high level of performance. It provides a                   review text. In Proceedings of the 7th ACM Conference
promising framework to examine language production and                   on Recommender Systems (RecSys), 165‚Äì172.
comprehension from a bottom-up point of view and allows               Plate, T. A. (1995). Holographic reduced representations.
for an examination into the power of experience in explaining            IEEE Transactions on Neural Networks, 6, 623‚Äì641.
linguistic behavior.                                                  Reber, A. S. (1967). Implicit learning of synthetic languages:
                                                                         The role of instructional set. Journal of Experimental
                           References                                    Psychology: Learning, Memory and Cognition, 2, 88-94.
Abbot-Smith, K., & Tomasello, M. (2006). Exemplar-
    learning and schematization in a usage-based account of
    syntactic acquisition. The Linguistic Review, 23, 275-290.
Chomsky, N. (1988). Language and the problems of
    knowledge. The Managua Lectures. Cambridge, MA:
    MIT Press.
Christiansen, M., & Chater, N. (2008). Language as shaped
    by the brain. Behavioral and Brain Sciences, 31, 489-558.
Chubala, C. M., & Jamieson, R. K. (2013). Recoding and
    representation in artificial grammar learning. Behavior
    Research Methods, 45, 470-479.
Evans, N., & Levinson, S. C. (2009). The myth of language
    universals: Language diversity and its importance for
    cognitive science. Behavioral and Brain Sciences, 32,
    429-492.
Hintzman, D. L. (1986). ‚ÄúSchema abstraction‚Äù in a multiple-
    trace memory model. Psychological Review, 93, 411-428.
Jamieson, R. K., & Mewhort, D. J. K. (2005). The influence
    of grammatical, local, and organizational redundancy on
                                                                  1330

