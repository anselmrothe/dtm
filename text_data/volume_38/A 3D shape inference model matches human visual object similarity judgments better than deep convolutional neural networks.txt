   A 3D shape inference model matches human visual object similarity judgments
                              better than deep convolutional neural networks
                                        Goker Erdogan (gerdogan@bcs.rochester.edu)
                                         Robert A. Jacobs (robbie@bcs.rochester.edu)
                                              Department of Brain and Cognitive Sciences
                                                         University of Rochester
                                                            Rochester, NY USA
                             Abstract                                   Due to their huge impact on computer vision research, these
                                                                        models have now started to attract attention in cognitive sci-
   In the past few years, deep convolutional neural networks            ence and neuroscience where they are actively investigated as
   (CNNs) trained on large image data sets have shown impres-
   sive visual object recognition performances. Consequently,           models of biological visual perception.
   these models have attracted the attention of the cognitive sci-
   ence community. Recent studies comparing CNNs with neural               A recent study by Khaligh-Razavi and Kriegeskorte (2014)
   data from cortical area IT suggest that CNNs may—in addi-            compared a large set of models from computer vision and
   tion to providing good engineering solutions—provide good            neuroscience to human fMRI and monkey neural data from
   models of biological visual systems. Here, we report evidence
   that CNNs are, in fact, not good models of human visual per-         IT. Similarity matrices calculated from each model were cor-
   ception. We show that a 3D shape inference model explains            related with similarity matrices from human and monkey neu-
   human performance on an object shape similarity task better          ral data. They found that AlexNet (Krizhevsky, Sutskever, &
   than CNNs. We argue that deep neural networks trained on
   large amounts of image data to maximize object recognition           Hinton, 2012), a deep CNN trained on 1.2 million images,
   performance do not provide adequate models of human vision.          had the highest correlation with IT data. An ensemble model
   Keywords: shape perception; object recognition; neural net-          combining the outputs of each layer of AlexNet with scores
   works; 3D shape; deep learning;                                      from multiple categorization models trained on the features
                                                                        learned by AlexNet was able to capture the entire variance
                         Introduction                                   in IT data. The scores from animate/inanimate, face/nonface,
Despite decades of research, we know little about the neural            and body/nonbody categorization models were needed to em-
representations underlying visual perception (Peissig & Tarr,           phasize the differences between these categories, since it
2007; Kourtzi & Connor, 2011). This is especially true of               seems that IT gives more weight to these categorical distinc-
high-level representations involved in visual object identifi-          tions than AlexNet did. These authors also showed that these
cation and recognition. Although we understand little about             results are not purely driven by the category structure in IT.
how our brains accomplish visual perception, we are able to             AlexNet on its own did, in fact, capture some of the within-
build engineering solutions that approach, and in some cases            category structure. It is remarkable that a model trained to
match, human performance on some visual tasks. Recently,                maximize object recognition accuracy is able to provide a
multi-layered artificial neural networks known as convolu-              good model of biological visual systems. This raises the in-
tional neural networks (CNNs) have shown impressive object              teresting possibility that biological visual systems might be
recognition performances when trained on large image data               optimized primarily for object recognition. If so, a high-
sets. Importantly for the cognitive science community, there            performing model of visual object categorization may also
seems to be evidence suggesting that these computer vision              be a good model of biological visual systems.
models may also be good models of biological visual systems                Yamins et al. (2014) recently offered evidence for this
(Kriegeskorte, 2015). Several studies have shown that CNNs              claim. They showed that models that are better at catego-
provide good accounts of neural data from both monkey and               rization explain neural responses better. Instead of using a
human inferotemporal (IT) cortex, explaining almost all of              fixed set of models, they defined a model space using param-
the variance in some cases (Baldassi et al., 2013; Cadieu et            eters that control various features of CNNs such as number
al., 2014; Khaligh-Razavi & Kriegeskorte, 2014; Yamins et               of layers, filter sizes, and activation thresholds. Examining
al., 2014). Here, we present evidence suggesting that CNNs              a large number of models in this space, their results showed
are, in fact, not good models of human visual perception.               that categorization performance was highly correlated with
We show that CNNs fail to capture people’s responses on an              IT response predictivity. However, they also showed that an
object shape similarity task. Moreover, we show that a 3D               “ideal” categorization model was not highly correlated with
shape inference model outperforms CNNs, suggesting that                 IT responses. This suggests that solely aiming for good cat-
3D structure is an important feature of people’s visual object          egorization does not, by itself, result in models predictive of
representations that CNNs fail to capture.                              IT responses. The authors claimed that it is the combination
   CNNs implement a sequence of convolution and subsam-                 of a hierarchical architecture and high categorization perfor-
pling operations to extract useful visual representations when          mance that accounts for why CNNs provide good models of
trained in a supervised manner on large image data sets.                IT responses.
                                                                   2543

   In spite of these impressive results, there is reason to ques-                        P     →    P | PP | PPP | ε
tion whether CNNs provide good models of human visual
systems. CNNs are trained only to maximize object recog-              Figure 1: Production rules of the shape grammar used in gen-
nition performance. However, human visual systems solve               erating the experimental stimuli and representing shape in our
not only object recognition but a myriad of visual tasks from         3D shape inference model. P is the only non-terminal sym-
segmentation to extraction of 3D shape. Indeed, because 3D            bol, and ε is the Null symbol.
shape and semantic category labels are highly correlated, it
is unclear whether IT representations are best thought of as
shape-based or semantic (Kourtzi & Connor, 2011). Even                our base objects to have a depth of four, which produces ob-
though CNNs often account for more variance in IT responses           jects with three levels of parts (see Figures 2a and 2b for an
than other models, it is possible that the driving factor be-         example base object and its parse tree).
hind these results is shape similarity rather than semantic fea-         Each base object was then used to create 8 additional ob-
tures. Baldassi et al. (2013) provided evidence that shape            jects, called variations, by applying 1 of 4 possible manipula-
similarity, rather than semantic information, accounts for the        tions, referred to as change part size, add part, remove part,
structure of IT representations. They demonstrated that most          and change connecting face of part. Each of these four ma-
of the semantic category structure in IT is explained by vi-          nipulations was applied at two different levels (second and
sual shape similarities within semantic categories. This re-          third levels) of the parse trees (see Figure 2 for examples of
sult raises the question of whether the representations learned       each manipulation). When using the change part size ma-
by CNNs—which receive supervised training based solely on             nipulation, we picked one of the parts at the desired level in
semantic category labels—adequately characterize represen-            the parse tree and resampled its size. When using the add
tations used by human visual systems. A striking demonstra-           part manipulation, a new part was added to the desired level,
tion suggesting these representations are, in fact, inadequate        picking its size and connecting face (i.e., the face of its par-
was provided by Szegedy et al. (2013). They showed that it is         ent to which it is connected) randomly. For the remove part
possible to create pairs of images that are indistinguishable to      manipulation, we again randomly picked one part at the de-
the human eye, but nonetheless are classified by CNNs into            sired level and removed it and all of its children parts. Lastly,
different classes. For example, it is possible to impercepti-         for the change connecting face manipulation, we randomly
bly perturb an image that a CNN classifies as a bus such that         picked a part and chose a new connecting face for it from the
the CNN classifies the perturbed image as an ostrich. This            empty faces of its parent. This manipulation moved the part
finding suggests that CNNs might be solving the problem of            and all of its children.
object recognition in a way that is rather different from that           Experimental stimuli consisted of images of the 10 base
of human visual systems.                                              objects and 80 variations (90 images in total).1 We used
   Here, we report behavioral evidence from an object shape           Blender (http://www.blender.org), a 3D computer graph-
similarity task suggesting that CNNs are not good models of           ics and animation software package, to render each object
human vision. Moreover, we show that a 3D shape inference             from a random viewpoint by rotating the camera around the
model provides a better account for human behavior. We ar-            vertical axis keeping its distance to the origin fixed. Con-
gue that models trained solely to maximize object recognition         sequently, there was significant pose variation in our experi-
performance cannot capture the nature of human visual rep-            mental stimuli.
resentations. A crucial feature of these representations not             The goal of the experiment was to collect people’s object
captured by these models is the 3D structure of objects.              shape similarity judgments. On each trial, a subject was pre-
                                                                      sented with one target and two comparison objects, and was
                          Experiment                                  asked to pick the comparison object that he or she thought
                                                                      was more similar in shape to the target object. The target
We created a set of 10 base objects using a “shape grammar”           object was always one of the ten base objects, and the two
(Figures 1 and 2) where each object consisted of multiple             comparisons were two randomly picked variations of the tar-
rectangular blocks (referred to as “parts” and denoted by P           get object. (For instance, a trial may show Figure 2a as the
in the grammar). A base object was generated as follows. To           target object, and Figures 2c and 2d as the comparison ob-
start, a root part was assigned 0-3 neighboring parts, also re-       jects.) On “catch” trials, one of the comparison objects was
ferred to as child parts, using the production rules of the shape     the same as the target object. Each subject participated in 100
grammar. A child part connected to the root part at one of its        trials, 16 of which were catch trials. The experiment was per-
six faces. This face was chosen at random. Similarly, the             formed on the world wide web by 41 subjects via Amazon
width, height, and depth of a child part were randomly cho-           Mechanical Turk. Five subjects were discarded because they
sen from the range [0, 1]. A child part could also be assigned        failed to reach 85% correct performance on catch trials.
neighboring (or grandchild) parts using the same production
rules and random selections. Note that, in this framework, an             1 The   entire set of experimental stimuli can be
object can be characterized using a “parse tree” due to our           seen       online     at     http://gokererdogan.github.io/
use of a shape grammar. We constrained the parse trees for            CogSci16SupplementaryMaterials/
                                                                  2544

               (a)                                                     layer (five convolutional, three fully connected layers) CNN
                                                (b)                    by Krizhevsky et al. (2012), referred to as AlexNet, trained
                                               P(1)                    on 1.2 million images in the ImageNet dataset. AlexNet
                                                                       achieved the best performance on the 2012 ImageNet Large
                                       P(2)            P(3)            Scale Visual Recognition Challenge. We treat each of its lay-
                                                                       ers as a separate mini-model. There are, in total, 14 layers
                                   P(4)    P(5)     P(6)   P(7)        (making the three max-pooling and two normalization layers
                                                                       explicit). Using the standard terminology in the deep neu-
                                     ε      ε        ε       ε         ral network literature, these layers are: conv1, pool1, norm1,
                                                                       conv2, pool2, norm2, conv3, conv4, conv5, pool5, fc6, fc7,
               (c)                              (d)
                                                                       fc8, and prob. The last layer, prob, is a 1000-dimensional
                                                                       vector encoding the probability of belonging to each of 1000
                                                                       object categories in ImageNet. The second deep CNN that we
                                                                       test is by Szegedy et al. (2014), named GoogLeNet, which
                                                                       set the state-of-the-art performance on the 2014 ImageNet
                                                                       Large Scale Visual Recognition Challenge. GoogLeNet has
                                                                       22 layers (with an additional five pooling layers). Our sim-
                                                                       ulations used 16 layers: pool1, conv2, inception3a-b, pool3,
                                                                       inception4a-e, pool5, inception5a-b, pool5, loss3 and prob.
                                                                       To make predictions from AlexNet and GoogLeNet, we input
                                                                       each image to a CNN and perform a bottom-up pass to cal-
               (e)                              (f)                    culate each layer’s responses. The dissimilarity between two
                                                                       objects is computed as the Euclidean distance between these
                                                                       responses. When presented with a trial from our experiment,
                                                                       a mini-model chooses the comparison object that is closest in
                                                                       its response space to the target object.
                                                                          Our 3D shape inference model: We developed a shape
                                                                       perception model that aims to infer 3D shape from 2D input
                                                                       images. Similar to our previously published 3D shape infer-
                                                                       ence models (Yildirim & Jacobs, 2013; Erdogan, Yildirim,
                                                                       & Jacobs, 2015), this model combines a representational lan-
                                                                       guage characterizing 3D shape with forward models mapping
Figure 2: (a) An example base object. The numbers on parts             from shape representations to 2D images. Using Bayesian in-
refer to the part numbers in its parse tree. (b) Parse tree rep-       ference, we invert this forward 3D-to-2D mapping and extract
resenting the object in (a). (c)-(f) Examples of change part           3D shape from 2D images. Formally, a shape representation
size (c), add part (d), change connecting face of part (e), re-        H consists of a string T from our shape grammar (Figure 1)
move part (f) manipulations, respectively. The parts affected          and a spatial model S that associates a size vector (s ∈ R3 )
by each manipulation are Part 2 in (c), Part 4 in (e), and Part        and a connecting face ( f ∈ {1, 2, 3, 4, 5, 6}) with each P node
6 in (f).                                                              in T . The probability of H is
                                                                                               p(H) = p(S|T )p(T )                        (1)
                  Computational Models
                                                                       where p(T ) is the probability of producing parse tree T from
We compare five models on how well they account for our                the shape grammar. We assume production probabilities to be
experimental data.                                                     uniform3 which gives the following expression for p(T )
   Pixel-Based model: The first one, called the Pixel-Based
model, works directly on image pixel values. The dissimi-                                                      1
larity between two objects is calculated as the Euclidean dis-                                      p(T ) =        .                      (2)
                                                                                                             4|P |
tance between their images in pixel space. The predictions
of the Pixel-Based model are determined by calculating the             The probability for spatial model S consists of the probabil-
distances between each comparison object and the target, and           ities of picking part sizes and connecting faces. Since we
choosing the comparison that is closest to the target.                 assumed part sizes to be uniform over the interval [0, 1], we
   CNN models: Our main aim is to compare the perfor-                  only need to focus on the probabilities for connecting faces.
mances of deep CNNs and a 3D shape inference model. For                (Jia et al., 2014).
this purpose, we use two CNNs.2 The first one is the eight-                3 Production probabilities can also be integrated out, which leads
                                                                       to a slightly different prior distribution. Note that our results here
    2 We use the pretrained models provided by the Caffe framework     are significantly robust to choice of prior distribution.
                                                                   2545

For   a part with k available faces and c children, there are          These proposal strategies are: add/remove part, change part
  k
  c  possible  combinations of face assignments to its children.       size, change connecting face of part, and change viewpoint.
Since we have six empty faces for the root P node and five             The add/remove part either adds a new P node to a ran-
for the remaining P nodes (because one face is occupied by             dom location in the tree, or removes randomly one of the P
the parent), the probability of spatial model S is                     nodes with no child parts. Note that this move jumps be-
                                                                       tween spaces of different dimensions; hence, we need to use
                                          1
              p(S|T ) =                                         (3)    a reversible-jump MCMC method. For the change part size
                             6                       5    
                           |Oroot |  ∏n∈{P \root} (|On |−1)            move, we resample the size of a randomly picked P node.
                                                                       Similarly, the change connecting face of part move picks one
where Oi refers to the set of occupied faces of node i. To             P node randomly and assigns it a new random connecting
map these shape representations to 2D images, we use a for-            face from the available faces of its parent P node. Finally, the
ward model that takes in the 3D representation and renders it          change viewpoint move rotates the viewpoint around the ver-
as a 2D image. Because the shape representation H does not             tical axis a random amount, which is drawn from a Gaussian
specify the viewpoint, forward model F takes in viewpoint θ            distribution. Due to space limitations, we cannot go into the
along with the shape representation H and produces a 2D im-            implementation details here.6
age I (i.e., F : {H, θ} → I). We used the Visualization Toolkit           In our simulations, we ran one chain for each image used
(VTK; http://www.vtk.org), a software package for 3D                   in the experiment. To speed convergence, we constrained the
computer graphics, image processing, and visualization, to             depth of parse trees to be at most six. Each chain was run
implement the forward model. To define the likelihood func-            for 200,000 iterations, and sample collection started after the
tion L (H, θ; I), we assume Gaussian noise on I:                       first 50,000 iterations. Hence, we had 15 samples per image
                                         1                             (see Figure 3 for two typical samples [i.e., two illustrations of
           L (H, θ; I) = p(I|H, θ) ∝        ||I − F(H, θ)||2F . (4)    the model’s inferred 3D shape given an image]). To calculate
                                         σ2
                                                                       the similarity between two images, we used Eqn.6, approxi-
Here σ2 denotes the variance of the noise (this is the only free       mating the integral by a sum over samples from the posterior
parameter of the model—it was set to a value that achieves             p(H|I).
acceptance rates around 20%) on I, and || · ||F is the Frobe-             Ideal 3D observer model: As our last model, we use an
nius norm. Combining the prior on shape representations and            “ideal” 3D observer that can perfectly extract the true 3D
the likelihood function, we use Bayes’ rule to infer likely 3D         shape of an object from its image. Although not realistic, this
shape representations given a 2D image:                                model provides a useful benchmark because it defines opti-
                                                                       mal performance for our model. Two objects are compared
                   p(H, θ|I) ∝ p(I|H, θ)p(H)p(θ).               (5)    by an alignment mechanism that rotates one object and finds
                                                                       the viewpoint that matches the image of the other object best
We assume p(θ) is a uniform distribution. Object similarity is
                                                                       (as in Eqn. 6). If we assume that shape matching is done on
computed by calculating how likely the model is to observe
                                                                       the basis of only the MAP sample, this ideal observer model
the image for one object given the image of the other. De-
                                                                       sets the performance upper bound for our 3D shape inference
noting the images by I1 and I2 , we calculate three similarity
                                                                       model.
measures: p(I2 |I1 ), p(I1 |I2 ), and their average. We calculate
p(I2 |I1 ) as follows (and similarly for p(I1 |I2 )):                                     Results and Discussion
                         Z
                                                                       We calculated the predictions of each model as described in
            p(I2 |I1 ) =   p(I2 |H, θ)p(H|I1 )p(θ)dHdθ.         (6)
                                                                       the previous section. For the experimental data, we gathered
                                                                       the data from all subjects and, for each trial, chose the ma-
The expression inside the integral in Eqn. 6 is equivalent to
                                                                       jority response. We measured the accuracy of each model by
inferring the 3D shape representation for I1 , picking a random
                                                                       calculating the percentage of correctly predicted trials (where
viewpoint, and calculating the sum of squared error between
                                                                       a trial is correctly predicted if a model’s response matches the
the observed I2 and the rendered image on the basis of in-
                                                                       subjects’ majority response).
ferred H and chosen viewpoint.
                                                                          The results are shown in Figure 4. The Pixel-Based
    To sample from the posterior distribution p(H, θ|I)4 we use
                                                                       model has the lowest accuracy with 58%. CNNs achieve
an MCMC procedure. We devised multiple proposal strate-
                                                                       accuracies of 62% (AlexNet, using responses of the out-
gies to move in the hypothesis space, and used a Metropolis-
                                                                       put layer) and 64% (GoogLeNet, using responses of the
Hastings (MH) algorithm to sample from the posterior.5
                                                                       layer inception5a). Our 3D shape inference model achieves
    4 To calculate Eqn. 6, we need samples from p(H|I). However,       72% accuracy using the similarities calculated by averaging
p(H|I) ≈ p(H, θMAP ) because there is only a single viewpoint from     p(Comparison|Target) and p(Target|Comparison). This per-
which an object H looks close to its image I. The results reported
here do not change if we integrate out θ instead of using the MAP      formance is significantly better than both AlexNet’s (bino-
sample.                                                                mial test, p < 0.001) and GoogLeNet’s performance (p =
    5 The code for our shape inference model is available at
https://github.com/gokererdogan/Infer3DShape                               6 Readers interested in these details can contact the first author.
                                                                   2546

               (a)                              (b)                 improved slightly (2%-4% increase) but not significantly, and
                                                                    our model still significantly outperforms both CNNs.
                                                                        Taken together, these results show that a 3D shape infer-
                                                                    ence model captures human performance better than deep
                                                                    CNNs on an object shape similarity task. This suggests that
                                                                    CNNs are, in fact, not good models of human vision. Al-
                                                                    though CNNs perform significantly better than chance, we
                                                                    believe this is due largely to the correlations between seman-
                                                                    tic object categories and shape features (Baldassi et al., 2013).
                                                                    Our study casts doubt on the claim that biological visual sys-
               (c)                              (d)
                                                                    tems are optimized chiefly for object categorization, and that
                                                                    a system trained solely for object categorization will learn
                                                                    representations that are similar to ours. To the contrary, the
                                                                    low performances of the intermediate layers of the CNNs in
                                                                    our study suggests the opposite. Why does our 3D shape in-
                                                                    ference model perform better than CNNs? We believe this
                                                                    is due to the 3D nature of our model’s shape representations.
                                                                    In contrast, a CNN trained to maximize object categoriza-
                                                                    tion performance learns to extract 3D features only to the ex-
                                                                    tent that 3D information helps discriminate object categories.
                                                                    Therefore, it is unclear whether shape representations learned
                                                                    by CNNs carry 3D shape information. Since there is substan-
Figure 3: Sample runs of our 3D shape inference model. (a)
                                                                    tial evidence showing that human and monkey IT are selective
An example input image. (b) One sample from our model for
                                                                    for 3D shape (Orban, 2011), it becomes doubtful that CNNs
the image in (a). (c)-(d) Another example input image and a
                                                                    offer good models of biological visual systems.
sample from our model.
                                                                        Lastly, we believe that our model is better suited than
                                                                    CNNs to understand visual perception in its totality because
0.004). The 3D ideal observer reaches an accuracy of 76%.           it is not intended simply as a model of object categorization.
However, the performances of the 3D ideal observer model            Biological visual systems solve a myriad of tasks from seg-
and of our model are not significantly different (p = 0.21).        mentation to scene perception, and our model can be read-
   Because subjects did not show a strong preference for ei-        ily extended to handle these diverse set of tasks. Moreover,
ther of the comparisons in some trials, we also measured per-       vision is just one aspect of perception. We believe that our
formance on only “high confidence” trials in which at least         model—with its combination of a rich, modality-independent
80% of the subjects picked the same response. There were in         representational language, a forward model, and Bayesian
total 120 (out of 280) high-confidence trials. Performances         inference—provides a promising theoretical framework for
were as follows (due to space constraints, we omit the graph        understanding not only visual, but also multisensory percep-
of these results). The Pixel-Based model’s accuracy is 62%.         tion (Yildirim & Jacobs, 2013; Erdogan et al., 2015).
Using the outputs of layer prob, AlexNet performs at 73%.
                                                                                        Acknowledgments
GoogLeNet achieves an accuracy of 68% with the outputs
of layer inception5b. Our 3D shape inference model per-             This work was supported by AFOSR (FA9550-12-1-0303)
forms the best, matching the accuracy of the 3D ideal ob-           and NSF (BCS-1400784) research grants.
server model with 87% accuracy using the average similar-
                                                                                             References
ity measure. This performance is significantly better than
both GoogLeNet’s performance (binomial test, p < 0.001)             Baldassi, C., Alemi-Neissi, A., Pagan, M., DiCarlo, J. J.,
and AlexNet’s performance (p < 0.001).                                 Zecchina, R., & Zoccolan, D. (2013). Shape Similarity,
   Our comparison here might seem unfair because our model             Better than Semantic Membership, Accounts for the Struc-
knows that stimuli are built out of blocks while we used pre-          ture of Visual Object Representations in a Population of
trained CNNs that have never seen similar objects. However,            Monkey Inferotemporal Neurons. PLoS Comput Biol, 9(8),
subjects in our experiment have also never seen objects like           e1003167.
our stimuli. In addition, previous studies presenting CNNs as       Cadieu, C. F., Hong, H., Yamins, D. L. K., Pinto, N., Ardila,
good models of our visual systems used pre-trained networks.           D., Solomon, E. A., . . . DiCarlo, J. J. (2014). Deep Neural
However, in order to alleviate further concerns, we have fitted        Networks Rival the Representation of Primate IT Cortex
the representations learned by CNNs to subjects’ data using            for Core Visual Object Recognition. PLoS Comput Biol,
a metric-learning (Kulis, 2013) approach.7 Accuracies have             10(12), e1003963.
                                                                    Erdogan, G., Yildirim, I., & Jacobs, R. A. (2015). From
    7 See Supplementary Materials for further information.             Sensory Signals to Modality-Independent Conceptual Rep-
                                                                2547

Figure 4: Prediction accuracies of each model using all experimental trials. The error bars denote SEMs and are calculated by
bootstrapping with 1000 replications. P(C|T): similarity calculated from p(Comparison|Target). P(T|C): similarity calculated
from p(Target|Comparison). Avg: similarity calculated from the average of these two probabilities. See the text for the layer
labels for AlexNet and GoogLeNet. Note that the y-axis starts from 0.4.
   resentations: A Probabilistic Language of Thought Ap-             Orban, G. A. (2011). The Extraction of 3d Shape in the
   proach. PLoS Comput Biol, 11(11), e1004610.                         Visual System of Human and Nonhuman Primates. Annual
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Gir-       Review of Neuroscience, 34(1), 361–388.
   shick, R., . . . Darrell, T. (2014). Caffe: Convolutional Ar-     Peissig, J. J., & Tarr, M. J. (2007). Visual Object Recogni-
   chitecture for Fast Feature Embedding. arXiv:1408.5093.             tion: Do We Know More Now Than We Did 20 Years Ago?
Khaligh-Razavi, S.-M., & Kriegeskorte, N. (2014). Deep                 Annual Review of Psychology, 58, 75–96.
   Supervised, but Not Unsupervised, Models May Explain              Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
   IT Cortical Representation. PLoS Comput Biol, 10(11),               Anguelov, D., . . . Rabinovich, A. (2014). Going Deeper
   e1003915.                                                           with Convolutions. arXiv:1409.4842.
                                                                     Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D.,
Kourtzi, Z., & Connor, C. E. (2011). Neural Representations
                                                                       Goodfellow, I., & Fergus, R. (2013). Intriguing Properties
   for Object Perception: Structure, Category, and Adaptive
                                                                       of Neural Networks. arXiv: 1312.6199.
   Coding. Annual Review of Neuroscience, 34(1), 45–67.
                                                                     Yamins, D. L. K., Hong, H., Cadieu, C. F., Solomon, E. A.,
Kriegeskorte, N. (2015). Deep Neural Networks: A New                   Seibert, D., & DiCarlo, J. J. (2014). Performance-
   Framework for Modelling Biological Vision and Brain In-             optimized Hierarchical Models Predict Neural Responses
   formation Processing. Annual Reviews of Vision Science,             in Higher Visual Cortex. Proceedings of the National
   1, 417–446.                                                         Academy of Sciences, 111(23), 8619–8624.
Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). Imagenet         Yildirim, I., & Jacobs, R. (2013). Transfer of Object Category
   Classification with Deep Convolutional Neural Networks.             Knowledge Across Visual and Haptic Modalities: Experi-
   In Advances in Neural Information Processing Systems 25             mental and Computational Studies. Cognition, 126, 135–
   (pp. 1097–1105).                                                    148.
Kulis, B. (2013). Metric Learning: A Survey. Foundations
   and Trends R in Machine Learning, 5(4), 287–364.
                                                                 2548

