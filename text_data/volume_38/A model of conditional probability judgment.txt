                                  A model of conditional probability judgment
                                                 Fintan J. Costello (fintan.costello@ucd.ie)
                                                        School of Computer Science,
                                                University College Dublin, Dublin 4, Ireland
                                                    Paul Watts (watts@thphys.nuim.ie)
                                                    Department of Mathematical Physics,
                                       National University of Ireland Maynooth, Maynooth, Ireland
                               Abstract                                  value of these estimates is equal to P(A), the probability of A;
                                                                         individual estimates vary with an approximately normal dis-
   A standard view in cognitive psychology is that people esti-
   mate probabilities using heuristics that do not follow proba-         tribution around this value. We assume that people estimate
   bility theory. We describe a model of probability estimation          the probability of A in exactly this way: randomly sampling
   where people do follow probability theory in estimation, but          episodes from memory, counting the number that are A, and
   are subject to random error or noise. This model predicts that
   people’s conditional probability estimates will agree closely         dividing by the sample size. We assume a long-term mem-
   with probability theory for certain noise-cancelling expres-          ory from which a random sample of episodes or traces can
   sions, but deviate from probability theory for other expres-          be drawn. For some event A we assume that each episode i
   sions. We describe an experiment which strongly confirms
   these predictions, suggesting that people estimate conditional        holds a flag set to 1 if i contains event A and set to 0 otherwise.
   probabilities in a way that follows standard probability theory,      An estimate for the probability of A is obtained by randomly
   but is subject to the biasing effects of random noise.                sampling episodes from memory, counting the number where
                                                                         the flag for A is set to 1, and dividing by the sample size.
                           Introduction                                     If this counting process was error-free, people’s estimates
Probability theory provides a calculus of chance describing              would have an expected value of P(A). Human memory is
how to make optimal predictions under uncertainty. Up to                 subject to various forms of random error, however. To re-
the 1960s the standard view in psychology was that people’s              flect this we assume some small probability d < 0.5 that when
probabilistic reasoning essentially followed probability the-            some flag is read, the value obtained is not the correct value
ory. However, various systematic biases in people’s proba-               for that flag. We assume that this noise is symmetric, so that
bility judgements (many identified in the 1970s and 1980s by             the probability of 1 being read as 0 is the same as that of 0
Tversky, Kahneman and colleagues) led researchers to con-                being read as 1. We also assume a minimal representation
clude that, in fact, people do not follow probability theory but         where every type of event, be it a simple event A, a conjunc-
instead estimate probabilities using various heuristics. While           tive event A ∧ B, a disjunctive event A ∨ B, or any other more
these heuristics often yield reasonable judgments, they can              complex form, is represented by such a flag, all with same
also produce strong biases in people’s probabilistic reasoning           probability d of being read incorrectly.
in certain cases (Tversky and Kahneman, 1973).                              A randomly sampled event will be counted as A if the event
   In this paper we return to the view that people follow prob-          truly is A and its flag is read correctly (this occurs with prob-
ability theory when reasoning about uncertainty. We present              ability (1 − d)P(A), since P(A) events are truly A and flags
a simple model of conditional probability judgment (judg-                have a 1 − d chance of being read correctly), or if the event is
ment of probabilities P(A|B): the probability of A given that            truly ¬A (not A) and its flag is read incorrectly as A (this oc-
B has occurred) where people estimate probabilities accord-              curs with probability (1 − P(A))d, since 1 − P(A) events are
ing to probablity theory, but are subject to random error or             truly ¬A, and flags have a d chance of being read incorrectly).
noise in recall from memory. Importantly, this model pre-                The expected value or average for a noisy estimate of P(A) is
dicts that bias will be ‘cancelled’ for certain combinations of          the sum of these two terms:
conditional probability estimates, and so those combinations
should agree closely with probability theory. In an exper-
iment testing this prediction, we find close agreement with                                hPE (A)i = (1 − 2d)P(A) + d                   (1)
probability theory for noise-cancelling expressions, alongside
systematic deviation from probability (in just the direction             with individual estimates varying around this value. This av-
predicted by the model) for other expressions.                           erage is systematically biased away from the ‘true’ probabil-
                                                                         ity P(A), such that estimates will tend to be greater than P(A)
                  Estimating probabilities                               when P(A) < 0.5, and less than P(A) when P(A) > 0.5. This
In standard probability theory, the probability of some event A          model explains observed patterns of bias in probability esti-
is estimated by drawing a random sample of events, counting              mates such as conservatism, underconfidence, subadditivity,
the number of those events that are instances of A, and di-              binary complementarity and the conjunction and disjunction
viding by the sample size. In probability theory the expected            fallacies (see Costello and Watts, 2014, 2016).
                                                                    2453

Table 1: 16 identities that must have a value of 0 in standard probability theory. Our model predicts if these identities are
computed from people’s individual probability estimates for any pair of events A, B, values obtained for identities 1 to 8 will
have a mean of 0, the value required by probability theory. Our model predicts that identities 9 to 16 will have values that are
positive and significantly different from 0, with identities 13 to 16 having values approximately half those of identities 9 to 12.
              Label                                   Identity                                          Predicted value
                1                       P(A) + P(B) − P(A ∧ B) − P(A ∨ B)                          =           0
                2                      P(A) + P(B ∧ ¬A) − P(B) − P(A ∧ ¬B)                         =           0
                3                           P(A|B)P(B) − P(B|A)P(A)                                =           0
                4                 P(A|B)P(B) + P(A|¬B) − P(A|¬B)P(B) − P(A)                        =           0
                5                 P(B|A)P(A) + P(B|¬A) − P(B|¬A)P(A) − P(B)                        =           0
                6                 P(B|A)P(A) + P(A|¬B) − P(A|¬B)P(B) − P(A)                        =           0
                7                 P(A|B)P(B) + P(B|¬A) − P(B|¬A)P(A) − P(B)                        =           0
                8       P(A|¬B) + P(B) + P(B|¬A)P(A) − P(B|¬A) − P(A) − P(A|¬B)P(B)                =           0
                9                          P(A) + P(B ∧ ¬A) − P(A ∨ B)                             =           d
               10                          P(B) + P(A ∧ ¬B) − P(A ∨ B)                             =           d
               11                          P(A ∧ ¬B) + P(A ∧ B) − P(A)                             =           d
               12                          P(B ∧ ¬A) + P(A ∧ B) − P(B)                             =           d
               13                             P(A ∧ B) − P(A|B)P(B)                                =          d/2
               14                             P(A ∧ B) − P(B|A)P(A)                                =          d/2
               15                  P(A ∧ B) + P(A|¬B)P(B) − P(A) − P(A|¬B)                         =          d/2
               16                  P(A ∧ B) + P(B|¬A) − P(B) − P(B|¬A)P(A)                         =          d/2
Predictions                                                          will violate the requirements of probability theory for identi-
Consider the identities given in Table 1. Probability theory         ties 9 to 12 in Table 1, with the same degree of violation for
requires that when the terms in the first identity (identity 1)      each identity. Probability theory requires that these identi-
are summed, the resulting value must be 0 for all events A           ties must also sum to 0 for all events A and B. Substituting
and B. Our model also predicts that, on average, people’s            our model’s expression for the expected value for estimates
estimates for this identity will also sum to 0. For example          of each term gives an overall positive expected value of d, vi-
suppose we ask people to estimate P(A), P(B), P(A ∧ B) and           olating the requirement of probability theory. For example,
P(A ∨ B) and combine each person’s estimates in the form of          the estimated value of the expression in Identity 9 is
identity 1. Since the expected value of a sum is equal to the
sum of expected values of its terms, the expected value for              hPE (A)i + hPE (B ∧ ¬A)i − hPE (A ∨ B)i =
this combination is, using Equation 1,                                     (1 − 2d) [P(A) + P(B ∧ ¬A) − P(A ∨ B)] + 2d − d = d
   hPE (A)i + hPE (B)i − hPE (A ∧ B)i − hPE (A ∨ B)i =               Again, a number of experiments have shown that these iden-
(1−2d) [P(A) + P(B) − P(A ∧ B) − P(A ∨ B)]+2d −2d = 0                tities are indeed violated in people’s probability estimates, in
                                                                     just the way predicted by the model. These results, however,
and so we expect that the average value for this identity will       apply to only to unconditional or direct probabilities. In the
be 0 just as required in probability theory. Since individ-          next section we describe our more general model of condi-
ual values for this sum are perturbed by random noise, we            tional probabilities, and derive a similar set of results.
expect these individual values to be distributed symmetri-
cally around that mean of 0. The same prediction holds for                    Estimating conditional probabilities
identity 2. A number of experiments have shown that these            Just as above, we assume that people estimate P(A|B) by
identities do in fact hold in people’s probability judgments:        drawing a random sample of instances of B, counting the
when we ask people to estimate probabilities for the terms in        number that are also A, and dividing by the sample size. As
these identities for a range of events, and then combine each        before, we assume some chance of random error d in this
person’s estimates according to the identity, the values ob-         counting process. Given this random error there are two mu-
tained are distributed approximately symmetrically around a          tually exclusive ways in an item can be read as an instance of
mean of 0, as required by probability theory and predicted by        event B: (i) when the item truly is an instance of B and is read
our model (Costello and Watts, 2014, Costello and Mathison,          correctly (this occurs with probability (1 − d)P(B)); and (ii)
2014, Fisher and Wolfe, 2014).                                       when the item is actually ¬B but is read incorrectly as B (this
   This model also predicts that people’s probability estimates      occurs with probability d(1 − P(B))).
                                                                2454

    We first take case (i). Given that a randomly sampled item       Predictions
is read as B, the probability that the item is truly an instance     From probability theory we have a number of identities
of B is                                                              whose value must be 0 for all events A and B. One such iden-
              (1 − d)P(B)                     (1 − d)P(B)            tity is Bayes’ Rule (Identity 3 in Table 1). Our model pre-
                                      =                              dicts that this identity should also hold in people’s probabil-
      (1 − d)P(B) + d(1 − P(B))            (1 − 2d)P(B) + d
                                                                     ity judgments, on average. To see this, suppose we ask people
with the denominator here representing the probability that          to estimate P(A), P(B), P(A|B) and P(B|A) and for each per-
an item will be read as B, and the numerator the probability         son we take the products P(A|B)P(A) and P(B|A)P(A). Since
that such an item was read correctly.                                estimates vary independently, the expected value of the prod-
    Given that we truly have an instance of B, there are two         ucts is equal to the product of the expected values of their
mutually exclusive ways in which that item can be read as A:         constituents, giving
when the item is indeed an instance of A and is read correctly,
or when the item is actually ¬A and is read incorrectly as               hPE (A|B)PE (B)i = hPE (A|B)i hPE (B)i
A. Since P(A|B) is the probability of an item being truly an
instance of A given that it is truly an instance of B, the first            = (1 − 2d)2 P(A ∧ B) + d(1 − 2d)[P(A) + P(B)] + d 2
possibility occurs with probability (1 − d)P(A|B); since 1 −
                                                                     and similarly
P(A|B) is the probability of an item being truly ¬A given that
it is truly an instance of B, the second possibility occurs with
                                                                         hPE (B|A)PE (A)i = hPE (B|A)i hPE (A)i
probability d(1−P(A|B)). The sum of these two probabilities
is (1 − 2d)P(A|B) + d, and so the overall probability of an                 = (1 − 2d)2 P(A ∧ B) + d(1 − 2d)[P(A) + P(B)] + d 2
instance being read as A given that it was read as B (and is
truly B) is                                                          and so
                   (1 − d)P(B) [(1 − 2d)P(A|B) + d]                              hPE (A|B)PE (B)i − hPE (B|A)PE (A)i = 0
                                                             (2)
                            (1 − 2d)P(B) + d
                                                                     Thus our model predicts that the average value of this identity,
    Taking case (ii) and reasoning in just the same we we get        computed from people’s individual probability judgments,
that the overall probability of an instance being read as A          should equal 0 as required by probability theory. Since de-
given that it was read as B (but is truly ¬B) is                     viations from this expected average in individual estimates
                  d(1 − P(B)) [(1 − 2d)P(A|¬B) + d]                  are due to random error, we also expect that individual val-
                                                             (3)     ues for these identities will be approximately symmetrically
                            (1 − 2d)P(B) + d
                                                                     distributed around 0.
    Since (i) and (ii) are mutually exclusive and cover all pos-         Similar expansion and rearrangement gives the same result
sibilities, the sum of Equations 2 and 3 gives our predicted         for Identities 4 through 8 in Table 1. Our model therefore pre-
value for hPE (A|B)i, the average estimate for the conditional       dicts that these identities should all have an average value of
probability P(A|B). Adding and using the identities                  0 in people’s estimates (matching the requirements of prob-
                                                                     ability theory), and that individual values for these identities
            P(B)P(A|B) = P(A ∧ B)
                                                                     will be approximately symmetrically distributed around 0.
   (1 − P(B))P(A|¬B) = P(A ∧ ¬B) = P(A) − P(A ∧ B)                       While this model predicts agreement with probability the-
we get                                                               ory for the identities given above, it also predicts that Iden-
                                                                     tities 13 through 16 in Table 1 should have a positive value
                (1 − 2d)2 P(A ∧ B) + d(1 − 2d) [P(A) + P(B)] + d 2   in people’s estimates, violating probability theory. For exam-
hPE (A|B)i =
                                  (1 − 2d)P(B) + d                   ple, using the same substitutions as above we get an expected
                                                             (4)     value for Identity 13 of
Just as with Equation 1, this average is systematically biased
away from the ‘true’ probability P(A|B).                                 hPE (A ∧ B) − PE (A|B)PE (B)i
    A direct probability P(A) is, in probability theory, equiva-
lent to a conditional probability P(A|B) where the condition-                 = (1 − 2d)P(A ∧ B) + d − (1 − 2d)2 P(A ∧ B)
ing event B has a probability of 1. Rearrangement shows that                          − d(1 − 2d)[P(A) + P(B)] − d 2
when P(B) = 1, Equation 4 reduces to Equation 1, our ex-                      = d(1 − d) − d(1 − 2d) [P(A) + P(B) − 2P(A ∧ B)]
pression for direct probability estimation. Equation 4 thus
completely describes all probability estimates, both direct and      Similar substitutions gives exactly the same expected value
conditional, in this model. While Equation 4 appears compli-         for identities 14, 15 and 16. Probability theory requires that
cated, it follows directly from two simple assumptions: that         0 ≤ P(A) + P(B) − 2P(A ∧ B) ≤ 1 for all A and B, and since
probabilities are estimated by counting event occurrence (in         d < 0.5 by assumption, we see that values for this expres-
accordance with probability theory) and that this counting           sion are distributed between d 2 and d(1 − d) and the expected
process is subject to random noise.                                  value for Identities 13 through 16 will is at the centerpoint of
                                                                 2455

   Table 2: A, B weather event pairs used in the experiment.         Table 3: Average value (SD) for Identities in the Experiment,
                                                                     computed from participants’ probability estimates in Blocks 1
    pair   A, B pairs in Block 1   pair   A, B pairs in Block 2      and 2. Values for identities 1 to 8 are close to 0, while values
     1          cold, rainy          6        cloudy, rainy          9 to 16 are significantly different from 0 in one-sample t-tests,
     2          cloudy, icy          7          cold, icy            as predicted by our model. Values for identities 13 to 16 were
     3        cold, thundery         8      cloudy, thundery         approximately half of those for identities 9 to 12, again as
     4         cloudy, warm          9        sunny, warm            predicted by our model.
     5         sunny, snowy         10         icy, snowy
                                                                                                Block
                                                                          Identity        1               2           predicted
this range, which is d/2. Our prediction, therefore, is that                  1      0.00 (0.31)    -0.03 (0.26)          0
Identities 13 through 16 should have, on average, a value of                  2     -0.01 (0.26)    -0.08 (0.30)          0
d/2; half the value of Identities 9 through 12.                               3     -0.01 (0.12)     0.00 (0.16)          0
                                                                              4     -0.02 (0.20)     0.02 (0.20)          0
                         An Experiment                                        5      0.01 (0.19)     0.02 (0.19)          0
We now describe an experiment testing the predictions of our                  6     -0.01 (0.21)     0.02 (0.20)          0
                                                                              7     -0.01 (0.16)     0.02 (0.11)          0
model; in particular, those concerning the identities shown in
                                                                              8     -0.02 (0.16)     0.00 (0.19)          0
Table 1. To test these predictions we gathered 62 participants’
estimates for the 10 different constituent probability terms in               9      0.24 (0.29)∗    0.17 (0.26)∗         d
the identities in the Table (i.e. P(A), P(A ∧ B), P(A|B) and                 10      0.25 (0.31)∗    0.25 (0.31)∗         d
so on) for five different pairs of events. We combined each                  11      0.25 (0.31)∗    0.28 (0.32)∗         d
participant’s individual estimates for each pair according to                12      0.24 (0.29)∗    0.20 (0.28)∗         d
the given identities. Participants in Block 1 saw one set of                 13      0.14 (0.18)∗    0.10 (0.18)∗        d/2
five pairs of events and those in Block 2 saw a different set:               14      0.13 (0.20)∗    0.10 (0.20)∗        d/2
we expected the predictions to hold for both blocks.                         15      0.12 (0.26)∗    0.12 (0.27)∗        d/2
                                                                             16      0.13 (0.22)∗    0.12 (0.22)∗        d/2
Materials                                                                  ** p < 0.0005, with Bonferroni correction for multiple
We constructed two sets of pairs of weather events, each con-              comparisons
taining five pairs; participants in Block 1 gave estimates for
one set of pairs and those in Block 2 gave estimates for the
second set. The two sets are shown in Table 2; the pairs were        where W and X were the two single component events of the
selected so that each set contained events of high, medium           conditional P(X|W ). Participants gave their estimates on a
and low probabilities, and with varying conditional probabil-        100-point scale, with the 0 point labelled ‘will never hap-
ity relationships between events.                                    pen’ and the 100 point labelled ‘certain to happen’. Questions
                                                                     were presented in random order on a web browser. The task
Method                                                               took around half an hour to complete. Participants’ responses
Participants were 62 undergraduate students at the School of         these were divided by 100 prior to analysis.
Computer Science and Informatics, UCD, who volunteered
to take part in exchange for partial course credit. Participants     Results
were asked to estimate the ten probabilities                         Two participants were excluded because they gave the same
                                                                     response for all questions, leaving 60 participants (31 in
   P(A), P(B), P(A ∧ B), P(A ∧ ¬B), P(¬A ∧ B), P(A ∨ B),             Block 1 and 29 in Block 2). As a consistency check we
               P(A|B), P(B|A), P(A|¬B), P(B|¬A)                      split participants in each block into two random groups and
                                                                     calculated the average probability estimate in each group for
for each of the five pairs of weather events, giving 50 esti-        each one of the 50 presented probability terms. If participants
mation questions for each participant. For single events, con-       were responding consistently we would expect there to be a
junctions, and disjunctions participants were asked                  reliable correlations between these split-half averages. Both
• What is the probability that the weather will be W on a            blocks showed a high correlation between split-half averages
   randomly-selected day in Ireland?                                 (r = 0.96, p < 0.0001 and r = 0.97, p < 0.0001), indicating
                                                                     consistent responses.
where the weather event W could be, for example, ‘cloudy’,
                                                                     Deviations from probability theory As predicted by our
‘cold’, ‘cloudy and cold’, ‘cloudy and not cold’ and so on.
                                                                     model, average values for identities 9 to 16 were positive for
For conditionals, participants were asked
                                                                     every A, B pair in both blocks, representing significant devia-
• If the weather in Ireland is W on a given randomly selected        tion from probability theory’s requirement that these identites
   day, what is the probability that the weather will also be X      have a value of 0 (see Table 3). Average values for every one
   on that same day?                                                 of these identities were significantly different from probabil-
                                                                 2456

ity theory’s value of 0 in one-sample t-tests across individual
values in both blocks ( p < 0.0005 in all cases, with Bonfer-                       200                                                identity 1
roni correction for multiple comparisons), just as predicted.
                                                                                                                                       identity 2
   Recall that our model predicts that Identities 9 through 12                                                                         identity 3
should all have the same average value, equal to d (the rate                        150
                                                                                                                                       identity 4
of random error for a given participant), and that Identities 13                                                                       identity 5
                                                                        Frequency
through 16 should all have the same average value, equal to                         100                                                identity 6
d/2. The average values in Table 3 support this prediction:                                                                            identity 7
values for Identities 9 through 12 were all close to their over-                                                                       identity 8
                                                                                     50
all mean of 0.235 and values for Identities 13 through 16 were
all around half that value (close to their overall mean of 0.12).
   We would expect this chance of random error, d, to vary                            0
across participants, but to be relatively constant within a given                         -1      -0.5          0            0.5   1
                                                                                                         Value of identity
participant. To test this prediction, for each participant we
calculated the average value of Identities 9 through 12 from
that participant’s estimates and measured the correlation be-        Figure 1: Frequency of occurrence of different values for
tween participants’ values for pairs of identities. All correla-     Identities 1 through 8 in Experiment 1 across all A, B pairs in
tions were positive (average pairwise correlation of r = 0.57);      the experiment, grouped into ‘bins’ from v − 0.05 . . . v + 0.05
of the six pairs, five showed a significant correlation at the       for v from −1 to +1 in steps of 0.1. For example, since there
p < 0.001 level (with Bonferroni correction for multiple com-        were 60 participants in the experiment and each participant
parisons), while correlation for the remaining pair was not          saw five pairs of events, the value of Identity 7 was calcu-
significant (p = 0.17). Similarly, for each participant we cal-      lated 5 × 60 = 300 times in total. Grouping these values into
culated the average value of Identities 13 through 16 from that      bins, we find that more than 60% of these calculations gave a
participant’s estimates and measured the correlation between         value that fell in the −0.05 . . . + 0.05 bin. Probability theory
participants’ values for pairs of identities. Again, all correla-    predicts these values will be symmetric around 0.
tions were positive (average pairwise correlation of r = 0.71);
all pairs showed a significant correlation at the p < 0.001
level (with Bonferroni correction for multiple comparisons).         the null hypothesis (that the value for the identity is 0) in all
                                                                     but one identity (and the overall value for that identity was
Agreement with probability theory                                    still relatively close to 0). These results suggest that values
We expected reliable agreement with probability theory for           for all these identities are distributed around 0, just as pre-
the identities 1 to 8. This expectation was also confirmed.          dicted by our model. Of course, the fact that some values for
For all identities 1 to 8, participants’ responses had an aver-      these identities were different from 0 means that there may
age value very close to probability theory’s required value          be some other factor in play that is not accounted for in our
of 0 in both Blocks 1 and 2. Averaging across all these              model. However, even values that were significantly different
identities gave a grand mean of M = −0.006(95%CI =                   from 0 were nevertheless still close to 0; this suggests that,
[−0.002, +0.015], SD = 0.22). Figure 1 graphs the frequency          even if there is some other such factor, the influence it has is
of occurrence of values for these identities. It is clear from the   small.
graph that values for these identities are symmetrically dis-
tributed around 0, the value predicted by our model. G1 sam-                                   Discussion and Conclusions
ple skewness for values for each identity in this graph were         We can summarise the main point of our work as follows:
close to zero (all fell in the range ±0.15), and the overall sam-    when deviations due to noise are cancelled out in people’s
ple skewness across all identities was G1 = −0.01, indicating        probability judgments (as in Identities 1 through 8), those
symmetric distributions (Bulmer, 2012).                              judgements are, on average, just as required by probabil-
   This pattern of close agreement with probability theory for       ity theory with no systematic bias. This pattern of agree-
identities 1 to 8 also held for each individual event pair A, B.     ment with probability theory holds for all the different event
There are 80 different averages for these identities in Table        pairs A, B in our experiment. This agreement with probabil-
3 (10 event pairs by 8 identities); of these 74 (92.5%) fell         ity theory cannot be dismissed by suggesting that our partici-
in the range −0.1 . . . + 0.1, and 48 (60%) fell in the range        pants happened to be particularly good at probability estima-
−0.05 . . . + 0.05. We analysed the distribution of these val-       tion, because this agreement occurs alongside significant bias
ues for individual event pairs by carrying out 80 separate one-      away from the requirements of probability theory for identi-
sample t-tests. Of these 80 tests,none were significantly dif-       ties which do not cancel out the effects of random noise (Iden-
ferent from 0 at the p < 0.01 level; with Bonferroni correc-         tities 9 through 16). For these identities the average degree of
tion for multiple comparisons, just one t-test was marginally        bias follows the predictions of our model (an approximately
significant (p = 0.04). JZS Bayes Factor tests on overall val-       constant degree of bias d for Identities 9 through 12, and an
ues for identities (across all pairs) gave evidence in favour of     approximately constant degree of bias d/2 for Identities 13
                                                                 2457

through 16). Taken together, the most natural explanation for        ily follow the rules of probability theory, and our reasoning
these results seems to be that people estimate probabilities us-     processes are necessarily subject to noise.
ing a mechanism that is fundamentally rational (in line with            Our results have broader implications for research on pat-
frequentist probability theory), but is subject to the biasing       terns of bias in aspects of people’s decision-making. A com-
effects of random noise.                                             mon pattern in such research is to identify a systematic bias in
   While our results demonstrate that people’s probability es-       people’s responses, and to then take that bias as evidence that
timates follow probability theory (when bias due to noise is         people are reasoning via some heuristic shortcut rather than
cancelled) we do not think people are consciously aware of           the correct reasoning process. Our results, however, show that
the equations of probability theory when estimating proba-           this inference from observed bias to inferred heuristic can be
bilities. Indeed we doubt whether the participants in our ex-        premature: random noise in reasoning can cause systematic
periment were aware of the probablity theory’s requirement           biases in people’s responses even when people are using nor-
that thse identities should equal 0 or would be able to ap-          matively correct reasoning processes. To demonstrate con-
ply that requirement to their estimations. Instead we propose        clusively that people are using heuristics, researchers must
that people’s probability judgments are derived from a ‘black        show that observed biases cannot be explained as the result
box’ module of cognition that estimates the probability of an        of systematic effects caused by random noise.
event A by retrieving (some analogue of) a count of instances
of A from memory. Such a mechanism is necessarily subject
                                                                                               References
to the requirements of set theory and therefore embodies the         Bulmer, M. G. (2012). Principles of statistics. Courier Cor-
equations of probability theory.                                        poration.
   We expect this probability module to be based on observed         Costello, F. and Watts, P. (2014). Surprisingly rational: Prob-
event frequencies, and to be unconscious, automatic, rapid,             ability theory plus noise explains biases in judgment. Psy-
relatively undemanding of cognitive capacity and evolution-             chological Review, 121(3):463–480.
arily ‘old’. Support for this view comes from that fact that         Costello, F. and Watts, P. (2016). Explaining high conjunc-
people make probability judgments rapidly and typically do              tion fallacy rates: the probability theory plus noise account.
not have access to the reasons behind their estimations, from           Journal of Behavioral Decision Making. In press, available
evidence that event frequencies are stored in memory by an              at http://dx.doi.org/10.1002/bdm.1936.
automatic and unconscious encoding process (Hasher and Za-           Costello, F. J. and Mathison, T. (2014). On fallacies and nor-
cks, 1984), and from results showing that animals effectively           mative reasoning: when people’s judgements follow prob-
judge probabilities (for instance, of obtaining food from a             ability theory. In Proceedings of the 36th annual meeting
given source) and that their probabilities are typically close          of the Cognitive Science Society, pages 361–366.
to optimal (Kheifets and Gallistel, 2012).                           Fisher, C. R. and Wolfe, C. R. (2014). Are people naı̈ve prob-
   Our results have implications for current approaches to              ability theorists? A further examination of the probability
the psychology of people’s probabilistic reasoning. In par-             theory + variation model. Journal of Behavioral Decision
ticular, our results are problematic for the view that people           Making, 27(5):433–443.
estimate probabilities via heuristics such as ‘representative-       Hasher, L. and Zacks, R. (1984). Automatic processing of
ness’ (Tversky and Kahneman, 1983) or ‘denominator ne-                  fundamental information: the case of frequency of occur-
glect’ (Reyna and Brainerd, 2008) that do ‘do not appear to             rence. The American Psychologist, 39(12):1372–1388.
follow the calculus of chance or the statistical theory of pre-      Kahneman, D. and Tversky, A. (1973). On the psychology of
diction’ (Kahneman and Tversky, 1973, p. 237). It seems                 prediction. Psychological Review, 80(4):237.
to us that such heuristic accounts are motivated by the as-          Kheifets, A. and Gallistel, C. R. (2012). Mice take calculated
sumption that the observed biases and errors seen in people’s           risks. Proceedings of the National Academy of Sciences, in
probability judgments cannot be explained by probability the-           press.
ory. This motivation arises because probability theory is the
                                                                     Reyna, V. F. and Brainerd, C. J. (2008). Numeracy, ratio bias,
normative model against which these biases and errors are
                                                                        and denominator neglect in judgments of risk and proba-
assessed. If researchers had not taken those biases and errors
                                                                        bility. Learning and Individual Differences, 18(1):89–107.
as evidence that people don’t reason using probability theory,
they would have had no reason to propose those alternative           Tversky, A. and Kahneman, D. (1973). Availability: A
accounts. However, our model suggests that these biases do              heuristic for judging frequency and probability. Cognitive
not, in fact, count as evidence that people don’t reason using          Psychology, 5:207–232.
probability theory. Those alternative models thus lose their         Tversky, A. and Kahneman, D. (1983). Extensional versus
fundamental motivation: there is no reason for moving from              intuitive reasoning: The conjunction fallacy in probability
probability theory to those alternative accounts in an attempt          judgment. Psychological Review, 90(4):293–315.
to explain human probabilistic reasoning. There is, in con-
trast, an underlying motivation for the probability theory plus
noise model: the probability of events in the world necessar-
                                                                 2458

