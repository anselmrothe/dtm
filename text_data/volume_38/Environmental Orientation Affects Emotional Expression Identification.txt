Environmental Orientation Affects Emotional Expression Identification
Stephen J. Flusberg (stephen.flusberg@purchase.edu)
Derek Shapiro (dshapiro723@gmail.com)
Kevin B. Collister (kevin.b.collister@gmail.com)
SUNY Purchase College, Department of Psychology
735 Anderson Hill Road, Purchase, NY 10577, USA

Paul H. Thibodeau (pthibode@oberlin.edu)
Oberlin College, Department of Psychology
101 N. Professor St., Oberlin, OH 44074, USA
Abstract
Spatial metaphors for affective valence are common in
English, where up in space=happy/positive and down in
space=sad/negative. Past research suggests that these
metaphors have some measure of psychological reality:
people are faster to respond to valenced words and faces
when they are presented in metaphor-congruent regions of
space. Here we explore whether the orientation of a stimulus
– rather than its position – is sufficient to elicit such spatialvalence congruency effects, and, if so, which spatial reference
frame(s) people use to represent this orientation. In
Experiment 1, participants viewed images of happy and sad
profile faces in different orientations and had to identify the
emotion depicted in each face. In Experiment 2, participants
completed this task while lying down on their sides, thereby
disassociating environmental and egocentric reference frames.
Experiment 1 revealed a metaphor-congruent interaction
between emotion and orientation, while Experiment 2
revealed that this spatial-valence congruency effect was only
reliable in the environmental frame of reference.
Keywords: spatial metaphor; valence, emotional expression
identification; spatial reference frames

Introduction
We often talk about abstract domains like time, emotion,
consciousness, health, and social status using spatial
metaphors (Lakoff & Johnson, 1980). For example, we
organize the concept of emotional valence around a vertical
spatial dimension, where a higher spatial position connotes
happiness and positivity while a lower spatial position
connotes sadness and negativity. Thus we say things like
“She sank into a deep depression and is feeling quite low,
very down in the dumps; we need to give her a lift, raise her
spirits, and boost her self-esteem until she’s flying high.”
This particular mapping between space and valence may
have its origins in everyday embodied experiences: sad
feelings are associated with a drooping posture and
drowsiness, while happy feelings are associated with a more
alert and erect bearing (though it’s rare to see people
literally jumping with joy).
Interestingly, research has found that this association
between space and valence goes beyond language: people
seem to automatically activate metaphor-congruent spatial
representations in the course of processing valenced stimuli
(and vice versa. e.g., Brookshire, Ivry, & Casasanto, 2010;

Casasanto & Dijkstra, 2010; Lynott & Coventry, 2013;
Meier & Robinson, 2004).
In one study, participants were faster and more likely to
retrieve positive memories while making concurrent motor
movements upwards, and faster and more likely to retrieve
negative memories while making concurrent motor
movements downwards (Casasanto & Dijkstra, 2010). In
another experiment, Meier and Robinson (2004) found that
participants were faster to identify positive words like hero
when they appeared at the top of the screen than when they
appeared at the bottom of the screen, while the reverse was
true for negative words like liar. A follow-up study revealed
that simply attending to higher or lower regions of space
facilitated the subsequent processing of valenced words
presented centrally in a metaphor-congruent fashion. More
recently, Lynott and Coventry (2013) extended these
findings by using non-linguistic stimuli: in their study,
participants were faster to respond to happy faces that
appeared at the top of the screen compared to sad faces that
appeared at the top of the screen and happy faces that
appeared at the bottom of the screen. Taken together, these
findings provide compelling evidence for the cognitive
reality of the spatial-valence metaphorical mapping.
The present work builds on these findings by testing
whether people are also sensitive to the orientation of
valenced stimuli. That is, one way to test for spatial-valence
congruency effects is to manipulate the position of stimuli
in a display (or the direction of movement towards a
particular position) – the method used by other researchers
tackling this phenomenon. Another way of testing for
spatial-valence congruency effects is to manipulate the
orientation of a stimulus. In addition to presenting a face at
the top of a computer screen (position), “up” can be cued by
presenting an upward gazing face (orientation).
Furthermore, spatial relations like upright and orientation
must be defined with respect to a particular frame of
reference; objects that are upright with respect to a computer
screen (environmental frame of reference) would appear
upside-down to a person standing on their head (egocentric
frame of reference). In other words, spatial relationships are
multifaceted; a fuller consideration of this nuance can help
us understand how people use space to represent valence.
Across two experiments, we investigated whether the
orientation of a stimulus – rather than its position in space –

2315

using the same ovular template to highlight the face and
keep each picture the same size.
On every trial, a black fixation cross appeared at the
center of the screen, which had a light gray background.
After 500 milliseconds, one of the face images appeared at
the center of the display in one of five possible orientations
(0º = upright; -90º, -45º = looking downwards; 45º, 90º =
looking upwards; see Figure 1). All 40 face images
appeared in each of the 5 orientations, for a total of 200
trials. Half of the faces were presented facing to the left, and
half were presented facing to the right (counterbalanced);
the order of trials was fully randomized.
Participants were instructed to respond as quickly and as
accurately as possible as soon as the face image appeared
during a trial, pressing one button on the keyboard if the
face was “happy” and another button if the face was “sad.”
Participants used the “f” and “j” keys to respond
(counterbalanced across participants).
Participants were also randomly assigned to one of two
stimulus duration conditions. In the Unmasked condition
(N=40), the face image remained on the screen until
participants pressed a response key. In the Masked condition
(N=41), the face image remained on the screen for 100
milliseconds and was then replaced by a scrambled version
of one of the images, created in Photoshop. This
manipulation was included to test whether the effects of
metaphorical spatial congruence on emotional expression
identification emerge early in visual processing (i.e., within
the first 100 milliseconds).
Before the main experimental task, participants completed
8 practice trials consisting of upright, front view, cartoon
faces (two sad faces and two happy faces, each presented
twice) to acclimate them to the task.

Unmasked
Condition

would be sufficient to elicit such spatial-valence congruency
effects, and, if so, which spatial reference frame people use
to represent this orientation. In Experiment 1, participants
viewed images of happy and sad profile faces in different
orientations and had to identify the emotion depicted in each
face. We expected a metaphor-congruency effect: that
people would be faster (and more accurate) to respond to
upward gazing happy faces and downward gazing sad faces,
and slower (and less accurate) to respond to upward gazing
sad faces and downward gazing happy faces. This finding
would provide additional evidence that representations of
valence are grounded in conceptions of space, and it would
demonstrate that spatial-valence congruency effects are not
limited to spatial located, but extend also to spatial
orientation.
In Experiment 2, participants completed this task while
lying on their right sides, thereby disassociating
environmental and egocentric reference frames. In a
majority of everyday experiences, environmental and
egocentric reference frames are highly correlated – most of
the time we see faces that are upright in the world
(environmental reference frame) while we sit or stand in an
upright position (egocentric reference frame). This study
design allowed us to investigate whether the representation
of valence is more strongly tied to the reference frame of the
world (environmental) or the individual (egocentric).
Given the importance of egocentric reference frames in
face perception (e.g., Rossion, 2008; Troje, 2003), one
possibility is that the spatial-valence mapping will be
defined with respect to the orientation of the participant. On
the other hand, our experience with faces in the world,
which are normally upright (even if we are tilted or on our
side), may tie the spatial-valence mapping to an
environmental frame of reference. Indeed, some metaphors
in English seem to reference the environmental frame
specifically, as when we say, “things are looking up.”
Identifying the reference frame(s) in which the spatialvalence mapping is defined can help us understand how
these representations are learned and when they influence
our behavior (Davidenko & Flusberg, 2012).

+
500 msec

Experiment 1

Happy or sad?

Methods

Materials & Procedure The experiment was created using
PsychoPy software (Pierce, 2007) and was administered on
a 21.5” iMac desktop computer. Face stimuli were drawn
from the Karolinska Directed Emotional Faces Database
(KDEF; Lundqvist, Flykt, & Öhman, 1998). We selected 40
profile faces from the database for use in the study: 10 male
faces and 10 female faces each expressing both happiness
and sadness. The images were cropped in Adobe Photoshop

Masked
Condition

Participants We recruited 81 participants (59 female) from
the Introduction to Psychology Participant Pool at SUNY
Purchase College. The average age was 19 (SD=1.2), and
participants received course credit for their participation.

+
500 msec
100 msec
Happy or sad?

Figure 1. Schematic diagram of trial structure for both
stimulus duration conditions in Experiment 1.

2316

Results
Response times faster than 200 milliseconds and slower
than five standard deviations above the overall mean RT
were removed from analysis (<1% of all trials). Accuracy
was very good overall (M=93.7%, SD=4.3).
Our initial analysis included only trials where participants
correctly identified the emotional facial expression. Using
reaction time as our dependent variable, we ran a 2
(emotion: happy vs. sad) X 5 (orientation: -90º, -45, 0º, 45º,
90º) repeated measures ANOVA with stimulus duration
condition included as a between-subjects factor. Of
particular relevance to our theoretical question was a
predicted metaphor-congruent interaction between the
emotion and orientation of the face, which was statistically
significant, F(4, 316)=5.74, p<0.001, η2=.0681 (see Figure
2). Planned contrasts revealed that participants were faster
to recognize happy faces oriented at 45º, t[80]= 2.28,
p=.025, and 90º, t[80]=3.07, p=.003. There were no
differences in recognition time by emotional expression at
other orientations (-90º, -45º, 0º), ts < 1, ps > .3.
In addition to the predicted interaction, the model
revealed a main effect of orientation, F(4, 316)=36.88,
p<0.001, η2 = .317, consistent with prior work on the effects
of orientation on face perception (e.g., Davidenko &
Flusberg, 2012): participants were fastest to respond to
upright faces (0º) and were progressively slower to respond
as the faces were rotated away from upright. No other main
effects or interactions were statistically significant, ps > .1.
Happy Faces
Sad Faces

0.82
0.80

Masked Condition

0.78

4.0

0.76

3.5

Unmasked Condition
Happy Faces

Mean Number of Errors

Reaction Time (seconds)

0.84

conducted another 2 (emotion) X 5 (orientation) repeatedmeasures ANOVA with condition as a between-subjects
factor. Consistent with the analysis of RTs, we found that
people made more errors on trials that presented downward
gazing happy faces than trials that presented downward
gazing sad faces, and vice versa for upward gazing faces,
F(4, 316)=4.84, p<0.001, η2=.056. Planned contrasts
revealed that participants made more errors in recognizing
happy faces oriented at -90º, t[80]= 5.23, p<.001, -45º,
t[80]=3.28, p=.002, 0º, t[80]=4.35, p<.001, and 45º,
t[80]=2.94, p=.004; there were no differences in error rates
by emotional expression 90º, t[80]=0.05, p=.958.
The model also revealed differences between the Masked
and Unmasked conditions. Not surprisingly, participants in
the Unmasked condition (mean accuracy=96.4%, SD=2.39)
made fewer errors than those in the Masked condition (mean
accuracy=91%, SD=4.13), F(1, 79)=47.76, p<0.001,
η2=.379. Since the performance of participants in the
Unmasked condition was close to ceiling, the interaction
between emotion and orientation was only present for
participants in the Masked condition (i.e., the 2-way
interaction between emotion and orientation was qualified
by a 3-way interaction between emotion, orientation, and
condition, F(4, 316)=3.89, p<0.005, η2=.0452).
In addition, this model revealed that people made more
errors for happy face trials (M=92.4% accuracy, SD=5.64)
than for sad face trials (M=95.2% accuracy, SD=4.59)3, F(1,
79)=20.1, p<0.001, η2=.201, and more errors as the faces
were rotated away from upright, F(4, 316)=13.6, p<0.001,
η2=.136. As shown in Figure 3, the effect of orientation was
only apparent for those in the Masked condition, F(4,
316)=10.66, p<0.001, η2=.104. See Figure 3.

0.74
0.72
0.70

-90º

-45º

-0º

45º

90º

Sad Faces

3.0
2.5
2.0
1.5
1.0
0.5
0.0

-90º

Orientation

Figure 2. Mean reaction times for happy and sad faces for
each stimulus orientation in Experiment 1, collapsed across
stimulus duration condition. Error bars represent 95% CIs
An analysis of error trials revealed a similar pattern.
Using error frequency as the dependent variable, we
1

Though Mauchly’s test indicated that the assumption of
sphericity was violated in this and several of the following
analyses, the F and p-values remain nearly identical under both
Greenhouse-Geisser and Huynh-Feldt corrections in all cases.

-45º

-0º

45º

90º -90º

-45º

-0º

45º

90º

Orientation

Figure 3. Mean number of errors for happy and sad faces
for each stimulus orientation in each condition in
Experiment 1. Error bars represent 95% CIs

2

Results of planned contrasts do not change when focusing
exclusively on data from the Masked condition: ts > 2.5, ps < .016
for orientations < 90º; t = .62, p = .539 at 90º.
3
This may reflect a slight response bias in our sample to
perceive sadness in others (or negativity more generally), or it may
signal that our stimuli were not equally discriminable based on
emotion.

2317

Discussion
In Experiment 1, we asked whether the orientation of a face
(i.e. where the face is looking, as opposed to its position in
space) would be sufficient to elicit spatial-valence
congruency effects on performance in an emotional
expression identification task. The answer was a clear yes:
participants were faster and more accurate to identify
emotional expressions when the faces were oriented towards
metaphor-congruent regions of space. This was true whether
the stimuli were masked after 100 milliseconds or remained
visible until response, suggesting that metaphorical spatial
representations of valence are activated quickly and
automatically when people view emotional stimuli (and vice
versa; cf., Brookshire, Ivry, & Casasanto, 2010).
Interestingly, these effects seemed to be driven largely by
a decrease in performance for sad faces facing upwards:
while RTs and error rates (in the Masked condition) for
happy faces increased symmetrically as the images were
rotated upwards and downwards away from upright, RTs
and error rates for sad faces dramatically increased on
upward rotations (i.e., metaphor-incongruent orientations).
This is somewhat surprising, as the only other published
work on spatial-valence congruency effects that used happy
and sad face stimuli found a response time advantage for
happy faces positioned in metaphor-congruent regions of
space (i.e., the top of the display), rather than a metaphorincongruent decrease in performance for sad faces (Lynott
& Coventry, 2013). These researchers interpreted these
findings as evidence for a “polarity” account of spatialvalence congruency effects, which is an issue we return to in
the general discussion (cf., Dolscheid & Casasanto, 2015).
The results of Experiment 1 cannot address one key
question: which way is up? Spatial relations like up, down,
and orientation must be defined with respect to a particular
frame of reference. When participants are seated at a
computer in a typical lab study like Experiment 1, several
spatial reference frames are conflated: faces that are
oriented upwards with respect to the computer screen, the
room itself, and the directional pull of gravity
(environmental frames) are also orientated upwards with
respect to the participant (egocentric reference frames). This
makes it impossible to determine which reference frame(s)
participants are using to represent the orientation of the
faces (and thus which reference frame is driving the
observed spatial-valence congruency effects).
Fortunately, there is a simple method for disassociating
environmental and egocentric reference frames: tilt your
head 90º to one side. Now faces that appeared to be gazing
upwards in the environment will appear to be upright or
upside-down in your egocentric frame of reference
(depending on which way you tilt your head). Interestingly,
prior research has shown that people process faces
independently in both the environmental and egocentric
reference frames: Davidenko & Flusberg (2012) found that
people were better at classifying and remembering images
of faces that were egocentrically upright (as compared to
egocentrically inverted) as well as environmentally upright

(as compared to environmentally inverted), though effects in
the environmental reference frame were reliably smaller. In
Experiment 2, participants completed the same task as in
Experiment 1 while lying down on one side.

Experiment 2
Methods
Participants We recruited 85 participants (59 female) from
the Introduction to Psychology Participant Pool at SUNY
Purchase. The average age was 19.2 (SD=2.63) and
participants received course credit for their participation.
Materials & Procedure The experiment was similar in
design to Experiment 1, with a few key differences:
Instead of sitting on a stool at a computer workstation,
participants began the experiment by sitting upright on a
futon positioned at the back of the lab room. The computer
running the experimental software was positioned on a low
table in front of the futon. Participants first completed the
same 8 practice trials featuring front-view cartoon faces that
participants completed in Experiment 1. The only difference
was that they used only their left hand to make the speeded
response, using the “1” and “2” keys on the keyboard
(counterbalanced across participants). After the practice
trials, participants were instructed to lay down on their right
side with their head resting horizontally on a flat pillow
facing the computer screen.
For Experiment 2 we used eight out of the ten male and
eight out of the ten female profile faces that we had used in
Experiment 1, each one again appearing with both a happy
and sad expression4. On any given trial, one of the 32
individual profile images (8 males, 8 females, 2 expressions
each) appeared in one of 8 possible orientations (see Table 1
and Figure 4). Participants saw each of the 32 faces in each
of the 8 possible orientations, for a total of 256 trials, the
order of which was randomized across participants.
Table 1. Stimulus orientations in Experiment 2

1
2
3
4
5
6
7
8
4

Egocentric
Orientation
upright
upside-down
upright
upside-down
90º
90º
-90º
-90º

Environmental
Orientation
90º
90º
-90º
-90º
upright
upside-down
upright
upside-down

This was to keep the experiment short enough to complete in a
reasonable time frame, since each face appeared 8 times in
Experiment 2 compared to 5 times in Experiment 1. The 4 faces
we eliminated for Experiment 2 were chosen based on pilot subject
ratings (1-10 scale) of how happy and sad the expressions looked.
We selected the two male and two female faces that scored lowest
on these ratings.

2318

The data from four participants were removed from analysis
because the computer crashed mid-session (N=1), the
participant was under 18 and could not give legal consent to
participate (N=26), or the participant’s error rate was
extremely high, representing a clear outlier (32% errors;
N=1). Accuracy for the remaining 81 participants was quite
good (M=96%, SD=3.14). Response times less than 200
milliseconds and greater than five standard deviations above
the overall mean RT across all participants and trials were
removed from analysis (<1% of all trials).
Past research suggests that there are independent effects
of spatial orientation on face perception in the
environmental and egocentric reference frames (Davidenko
& Flusberg, 2012). Therefore, we analyzed the trial data
separately for each frame, including only those trials where
participants correctly identified the emotional facial
expression.
Environmental Frame We first conducted a 2 (emotion:
happy vs. sad) X 2 (orientation: -90º in the environment vs.
90º in the environment) repeated measures ANOVA with
mean RT as the dependent variable. There was no main
effect of emotion, as participants had similar reaction times
to happy and sad faces, F(1, 80)=0.75, p=0.39. There was a
marginal effect of orientation, as participants were slightly
slower to respond to faces looking upwards in the
environment (90º) compared to faces looking downwards in
the environment (-90º), F(1, 80)=3.29, p=0.074. Crucially,
there was a significant metaphor-congruent interaction
between emotion and orientation, F(1,80)=4.48, p=0.038,
η2=.053: participants were marginally slower to respond to
sad faces gazing upwards compared to happy faces gazing
upwards, t[80]=1.82, p=.073, and to sad faces facing
downwards, t[80]=2.94, p=.004, in the environment; there
was no difference in recognition time for downward facing
faces by emotional expression, t[80]=.14, p=.887;
participants responded similarly fast to happy faces facing
5

As it turns out, when people tilt their head to one side, their
eyes rotate several degrees in the opposite direction, a phenomenon
known as Ocular Counter-Roll (OCR). At 90º rotation, this effect
is very small (roughly 4º), and it does not appear to explain the
effects of environmental orientation on face processing (see
Davidenko & Flusberg, 2012). Because we observe an interaction
between emotion and orientation in our data, OCR cannot account
for our findings, since it should equally affect all faces.
6
A valuable legal and ethical lesson for undergraduate research
assistants!

Environmental FoR
0.94

Egocentric FoR
0.94

Happy Faces

0.84

Sad Faces

0.92

0.92

0.90

0.90

0.88

0.88

0.86

0.86

0.84

0.84

-90º

90º

Reaction Time (seconds)

Results & Discussion

upwards and downwards in this environment, t[80]=.36,
p=.719 (see Figure 4).
Egocentric Frame We repeated this analysis for trials
where the faces were oriented upwards or downwards in the
egocentric frame of reference. There were no main effects
of emotion or orientation, nor was there an interaction
between the two (all F’s < 0.2, all p’s > 0.7; see Figure 4).
In both the environmental, F(1, 80)=27.93, p<.001,
η2=.259, and egocentric, F(1, 80)=6.53, p=.013, η2=.075,
frames of reference, analyses of error rates revealed a main
effect of emotional expression: in both frames of reference
people were more accurate recognizing happy faces.

Reaction Time (seconds)

Note that when participants lay on their right side to view
these images, faces that were oriented upwards or
downwards in one frame of reference (i.e., rotated 90º or 90º in that frame) were always either perfectly upright or
perfectly upside-down in the other frame of reference5. This
decoupling of the environmental and egocentric reference
frames allowed us to investigate independent spatial-valence
congruency effects in both frames of reference.

0.82

-90º

0.70

Happy Faces
Sad Faces

0.80
0.78
0.76
0.74
0.72

90º
-90º

-45º

-0º

45º

Orientation

Figure 4. Mean RTs for happy and sad faces for each
stimulus orientation and each frame of reference
(environmental on the left) in Experiment 2. Error bars
represent 95% CIs.

General Discussion
Spatial metaphors for affective valence are common in
English, where up in space connotes happy or positive
feelings (“things are looking up!”) and down in space
connotes sad or negative feelings (“I’m down in the
dumps”). Past research suggests that this association is not
merely a matter of language; rather, it offers a window into
how people (metaphorically) represent the concept of
emotional valence. For example, people are faster to
respond to positive and negative words and faces when they
are presented in metaphor-congruent regions of space
(Lynott & Coventry, 2013; Meier & Robinson, 2004). In the
present study, we explored whether the orientation of a
stimulus – rather than its position in space – is sufficient to
elicit such spatial-valence congruency effects, and, if so,
which spatial reference frame people use to represent this
orientation.
In Experiment 1, participants viewed images of happy and
sad profile faces in different orientations and had to identify
the emotion depicted in each face. Results revealed a
significant
spatial-valence
congruency
effect
on
performance: participants were faster and more accurate to
respond when faces were oriented towards metaphor-

2319

90º

congruent regions of space. In Experiment 2, participants
completed the same task while lying down on their side,
thereby disassociating environmental and egocentric
reference frames. Results indicated that this spatial-valence
congruency effect was only reliable in the environmental
frame of reference, suggesting that (metaphorical)
representations of the spatial dimension of emotional
valence are constructed with respect to the environment.
Of course, multiple environmental reference frames were
conflated in the present study design (e.g., faces that were
environmentally upright were upright with respect to the
computer display, the lab room, and the directional pull of
gravity), so future work is required to tease apart which
one(s) people are using to structure affective valence.
Nonetheless, the present findings are notable in part because
other research has found that effects of spatial orientation on
face perception and memory are typically larger in the
egocentric frame (e.g., Davidenko & Flusberg, 2012; Troje,
2003).
Also of note, in both experiments the spatial-valence
congruency effects appeared to be driven by a decrease in
performance for sad faces looking upwards in the
environment. As mentioned in the discussion of Experiment
1, this result is somewhat surprising, as other researchers
have observed similar congruency effects driven by a
relative increase in performance for happy faces and
positively valenced words that appear in higher regions of
space (Lakens, 2012; Lynott & Coventry, 2013). One
possibility is that differences in stimuli may account for
these disparate findings: It may be that people simply
respond to profile faces differently than they do to front
view faces and common English words, perhaps due to the
fact that judging emotions based on profile views is not a
common activity.
No matter the explanation, these data may pose a
challenge to some theories that have been put forth to
explain spatial-valence congruency effects. In particular, the
“polarity-based” perspective suggests that stimulus
dimensions (including space and valence) are always
anchored at a default endpoint (+pole) that is typically more
frequent and unmarked linguistically (Lakens, 2012; Lynott
& Coventry, 2013). In the case of valence, for example,
“happy” is the default +pole (you can negate the unmarked
term happy – unhappy – but not the term sad; unsad is not
an English word). The polarity account attributes spatialvalence congruency effects to a generic processing
advantage for +polar items, and since up is another example
of a +polar endpoint, this means that people should be
fastest to respond to happy stimuli in higher regions of
space (but see Dolscheid & Casasanto, 2015, for evidence
against a universal polarity correspondence account of
metaphor-congruency effects). In the present study,
however, we do not observe this sort of processing
advantage, but rather a processing cost for sad faces
oriented towards the metaphor-incongruent upper-regions of
the environment.

That being said, there is one way to interpret the present
findings that would actually support the polarity account: if
people are generally much worse at perceiving upward than
downward gazing faces, then in fact it would be the case
that we are seeing a processing advantage for happy faces
looking upwards. However, based on other research on
orientation effects in face processing and other
(unpublished) findings from our lab, we do not think this is
the most parsimonious explanation of the current findings.
Still, more work may be required to fully rule out this
possibility, and to fully explain why different performance
asymmetries emerge in studies of spatial-valence
congruency.

References
Brookshire, G., Ivry, R., & Casasanto, D. (2010).
Modulation of motor-meaning congruity effects for
valenced words. In S. Ohlsson & R. Catrambone (Eds.),
Proceedings of the 32nd Annual Conference of the
Cognitive Science Society (pp. 1940-1945). Austin, TX:
Cognitive Science Society.
Casasanto, D., Dijkstra, K. (2010). Motor action and
emotional memory. Cognition, 115, 179 – 185.
Davidenko, N. & Flusberg, S. J. (2012). Environmental
inversion effects in face perception. Cognition, 123(3),
442-447.
Dolscheid, S., & Casasanto, D. (2015). Spatial congruity
effects reveal metaphorical thinking, not polarity
correspondence. Frontiers in Psychology, 6(1836), 1-11.
doi: 10.3389/fpsyg.2015.01836
Lakens, D. (2012). Polarity correspondence in metaphor
congruency effects: Structural overlap predicts
categorization times for bipolar concepts presented in
vertical space. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 38, 726–736
Lakoff, G., & Johnson, M. (1980). Metaphors We Live By.
Chicago and London: University of Chicago Press.
Lundqvist, D., Flykt, A., & Öhman, A. (1998). The
Karolinska Directed Emotional Faces - KDEF, CD ROM
from Department of Clinical Neuroscience, Psychology
section, Karolinska Institutet, ISBN 91-630-7164-9.
Lynott, D., & Coventry, K. (2013). On the ups and downs of
emotion: testing between conceptual-metaphor and
polarity accounts of emotional valence–spatial location
interactions. Psychonomic Bulletin & Review, 21; 218226.
Meier, B.P., Robinson, M.D. (2004). Why the sunny side is
up, associations between affect and vertical position.
Psychological Science, 15, 243 – 247.
Peirce, J.W. (2007) PsychoPy - Psychophysics software in
Python. Journal of Neuroscience Methods, 162(1-2):8-13.
Rossion, B. (2008). Picture-plane inversion leads to
qualitative changes of face perception. Acta Psychologia,
128(2), 274–289.
Troje, N. F. (2003). Reference frames for orientation
anisotropies in face recognition and biological-motion
perception. Perception, 32(2), 201–210.

2320

