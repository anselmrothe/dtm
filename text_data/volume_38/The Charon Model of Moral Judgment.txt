The Charon Model of Moral Judgment
Deirdre Kelly (deirdrekrkelly@gmail.com)
Jim Davies (jim@jimdavies.org)
Institute of Cognitive Science, Carleton University
Ottawa, Ontario, K1S 5B6 Canada
Abstract
We present a model of moral judgment, Charon, which adds
to previous models several factors that have been shown to
influence moral judgment: 1) a more sophisticated account of
prior mental state, 2) imagination, 3) empathy, 4) the
feedback process between emotion and reason, 5) selfinterest, and 6) self-control. We discuss previous classes of
models and demonstrate Charon’s extended explanatory
power with a focus on psychopathy and autism.
Keywords: morals, morality, ethics, ethical reasoning, moral
reasoning, moral judgment, philosophy, deontological,
utilitarian, dual-process, modeling, psychopathy, autism,
reasoning, emotion, emotions, self-interest, empathy,
compassion, willpower, self-control, imagination.

Introduction
Traditional psychological models of moral judgment
mainly focus on the types of reasons that people have to do
the right thing (e.g., Rest, 1986; Gaudine & Thorne, 2001;
Thorne & Saunders, 2002). More recently Haidt (2001)
introduced a social-intuitionist model of moral judgment in
which judgments are emotional, quick, and intuitive. The
role of reason in Haidt's account is entirely post-hoc and
does not play a role in generating judgments.
The role of emotion in moral judgment has also been
investigated in dual-process models. Greene et al. (2001)
distinguish between two types of moral processes –
deontological and utilitarian. They argue that the former are
an intuitive, emotional process and that the latter is reasonbased.
This paper examines the state of psychological moral
judgment modelling. We present a new model that better
accommodates a breadth of empirical moral judgment
evidence and, further, can explain the differences in moral
judgment between abnormal populations, such as
psychopaths and people with autism.

Reason-based Models
Reason-based models, such as Rest’s (1986), involve
recognition of moral issues, and then application of moral
reasons, rules, or frameworks (such as utilitarianism) to
arrive at a moral judgement. Others have added to this basic
model to account for the influence of situational factors
(e.g., Trevino, 1986). Models of moral judgment have
similarly almost exclusively focused on reason-based
decision making for the past thirty years.

One major problem with reason-based models is that they
have trouble accounting for moral judgements that seem to
have nothing to with good moral reasoning, such as when
somebody thinks it’s immoral to burn a flag. Much of moral
judgment happens quickly, with no deliberation, suggesting
the importance of emotion in moral judgement.
According to Kohlberg (1973), as people become more
proficient at reasoning and have a more fully developed
concept of themselves as connected to the largest sphere of
relations, they will develop an abstract, universalized form
of morality. At its highest stages, these moralities take the
form of utilitarianism, and better still Kantianism.
Kohlberg’s focus was on reasoning alone. The way people
are moral is by becoming ever more proficient reasoners. As
people improve their understanding of the world, they are
better able to make moral decisions.

Haidt’s Social Intuitionist model
Haidt (2001) introduced a social intuitionist model, later
called Moral Foundations Theory (2012), that holds that
moral judgements are based on application of several moral
foundations (five in earlier versions, six in its 2012 version)
that work through emotion. For Haidt, the role of moral
reasoning is post hoc. Once we have already made a snap
judgment, we use moral reasoning to justify or confabulate
the decision we have already made.
Haidt’s empirical support for the post-hoc role of
reasoning in moral judgment comes from his moral
dumbfounding experiments, in which he presented subjects
with various moral scenarios and found that most would
have an emotional judgment of things and hold that
judgment even after their reasons failed to support it (Haidt,
2001; Haidt et al., 2000).
Research has highlighted the important role that disgust
plays in moral judgment (Haidt, 2012; Pizarro et al., 2011).
Haidt (2012, 2001) argues that disgust reactions, such as
that which is felt when faced with cases of incest, or the idea
of your neighbour eating his dead dog, are in themselves
moral judgments of the wrongness of the acts. Pizarro takes
this a step further in his research. He and his colleagues
argue that even the presence of disgust is enough for
someone to frame the situation they later view as being
moral. In other words, disgust can trigger a moral mindset.
We argue that Moral Foundations Theory underestimates
the use of reason in moral judgement. Examination of

1415

Haidt’s results indicates that depending on the scenario
people sometime change their minds from their original
intuition based on their their post-hoc reasoning. For
example, when prompted to consider various ways in which
the incest case is not actually “morally wrong”; i.e. no one
is getting hurt, there will not be an infant conceived, etc.,
people will sometimes change their minds and arrive at a
new moral judgment. 10-23% of people changed their
judgments when presented with counter-evidence to their
original intuitions (Haidt 2001; Haidt et al., 2000). Though
not a majority, this is a sizable percentage and a model of
moral judgement must accommodate it.

Dual-Process Models
Dual-process models of moral judgment are a relatively
new area of investigation across disciplines. As a result,
there are few existent models that take this approach. While
it has been gaining traction in psychology, neuroscience,
and other more applied disciplines, very little to date has
been done from a philosophical standpoint.
Dual-process models of moral decision-making claim that
emotion/intuition and reasoning are both integral to human
morality.1 Joshua Greene and his colleagues in the early
2000s were the first team to apply this framework, and other
dual-process accounts resemble theirs.
Greene and colleagues used Philippa Foot's (1978)
trolley/footbridge cases to examine how people make moral
judgments. Trolley cases involve a person having to make a
decision to act (such as by pulling a switch) where their
choice to act will save a greater number of lives. For
example, the classic trolley case involves a train coming
down the tracks, if you choose to do nothing then the train
will hit and kill 5 people, but if you pull a switch then the
train will be diverted and will only kill 1 person. Greene et
al. (2001) found that the large majority of people are willing
to pull the switch in the trolley case. They argue that this is
because when choosing whether or not to pull the switch
people do some kind of utilitarian calculus. In this study,
people will often explain their actions in the following way:
“It's better to save five people, and to let one person die,
because five lives saved is better than one.” This reflects a
utilitarian type of reasoning. Greene et al. were interested in
explaining why it is that despite willingness to pull the
switch in the trolley case, people will not act to save the five
in the footbridge case where it is required that you
physically push another person onto the tracks to save the
five people’s lives.
To explain this finding, Greene et al. invoke a dual1
This approach aligns moral decision-making much closer to
current theories of decision making more broadly. Daniel
Kahneman’s (2013) work on decision-making has shown how both
reason and emotion play separate and integral roles in decisionmaking. They can cause separate judgments. They influence each
other. They are both necessary for decision-making to work well.

process model of moral judgment. They claim that people
are more averse to doing things that involve personal harm
and that footbridge cases involve an entirely different type
of process than in the trolley case. They explain that when
faced with footbridge cases, people invoke (what Greene
considers) a deontological approach to morality. Greene and
colleagues explain that deontological approaches are
emotional and that the idea of personally harming another
harming another person causes an emotional interference
that impedes doing a utilitarian calculus.
Greene sees complex moral decisions as being the result
of an internal struggle between reason-based and emotionbased approaches happening in the brain.
Messervey, Nelson, and Peach (2016) and Messervey
(2013) also present a dual-process model equipped for the
fact that depending on situation and context people can
appeal to two different processes for moral judgment. Their
focus is on stress’ influence on ethical judgment. They
explain that in situations where you are under time
constraints, are uncertain, or stressed, deliberative
processing is difficult. This predicts that when a person is
stressed they are more likely to rely on intuitive moral
judgments.

Criticism of Previous Models
We have presented three broad classes of models that
attempt to account for the range of moral judgments’
influences. However there are empirical findings that have
not yet been accommodated by any existing model.
First, they do not account for the way in which
imagination affects moral decision-making. Visual imagery
has been shown to have an effect on the ways in which
people reason. For example, being asked to visualize a
situation before making a judgment about it tends to make
people less utilitarian in their rule-application. They tend to
reject the concept that the ends justify the means compared
to those who do not visualize the situation (Amit & Greene,
2012).
The second is that they do not account for weakness of
will and self-interest. This is surprising, considering that
many consider morality as the opposite of being selfish. But
in the classes of models we have looked at, there is no piece
that explicitly accounts for differences in how selfish people
are (either in general or in the moment).
Similarly, someone with a weak willpower (or with a
willpower weakened by something like ego depletion),
might make different moral judgments.
Third, these models do not make explicit the role of
empathy, which has been shown to affect moral judgment
(Batson, et al., 1997).
In response to these ignored issues, we present our own
model of moral reasoning.

The Charon Model
The Charon modal is a dual-process model of moral
judgment built on previous models, such as those of

1416

Messervey (2013) and Reynolds (2006).
Important ways in which our model (see Fig. 1) deviates
from previous models is its introduction of 1) a more
sophisticated account of prior mental state, 2) imagination,
3) empathy, 4) the feedback process from the emotion to
reason and reason to emotion, 5) self-interest, and 6) selfcontrol.
The structure of the model, with arrows indicating
influence, can be seen in Fig. 1.

Top-Down Mental States
There are many ways in which top-down processing can
affect decision-making. Long-held beliefs and desires, for
example, will have an impact on the way in which we
perceive incoming information as well as constrain our
imaginative processing. Our phenomenological experiences
are determined by the interaction of mental states with
either sensory input or imagination.
The types of mental states that can influence moral
judgment are broad. However, we want to focus on three
ways in which mental states affect moral decision-making.
The first, introduced by Messervey, Nelson, and Peach
(2016), is the role of stress. Being in a stressed mental state
has consequences from the onset of decision-making. The
major consequence of stress is that it makes it difficult to
reason effectively. In other words, if someone is already
stressed then deliberative processing will be impaired. As
predicted by dual-process models, those under cognitive
load suffer impairments in utilitarian but not deontological
reasoning, and removing time pressure increases utilitarian
judgment (Greene, 2012, 127).

Figure 1. The Charon model of moral judgment. Arrows
indicate influence. Dotted lines are mediating.
General affect can alter moral behaviour. Isen and Levin
(1972) ran a study where they found that getting an extra
dime out of a pay phone increased the likelihood of that
person helping another by 22 times. This supports the
hypothesis that a positive emotional state will result in more

ethical behaviour. We assume that moral judgment was also
affected, but we know of no study that differentiates moral
action and judgment for the “dime effect.”
Haidt’s (2012) moral foundations are also stored in the
“mental state” box in our model. Given the different
reactions that people have to Kohlberg’s (1976) moral
dilemmas, leaving a broad category for rule application
helps to explain the differences. Additionally, by building in
the moral foundations as part of the mental states, it will
also help in predicting how they will reason. For example,
someone who is high on the fairness consideration may lean
more towards not stealing.

Imagination
Amit and Greene (2012) examined the role visual
imagination plays in moral judgments. They found that
those with more visual cognitive styles compared to those
with verbal styles were significantly more likely to prefer
deontological judgments. They tended to prefer the rights of
the individual over the collective good. Further, impairing
visualization ability by overloading visual capacity made all
participants less deontological in their judgments.
This lends empirical support to the introduction of the
role of imagination in moral judgments. Presumably,
imagination generates more visceral reactions than abstract
thought, preferentially activating the emotional system,
which, as we have seen, results in deontological judgments.

Empathy
Empathy, sometimes characterized as that which creates
an emotional understanding of the other (Lamm, Batson &
Decety, 2007) has long been linked to morality, dating as far
back as Hume. More recently, there are those, such as Slote
(2007), who have argued that morality is founded on
empathy. While we hold that empathy is neither sufficient
nor necessary for moral judgment, the ability to empathize
with another person can change the ways in which people
reason as well as their emotion judgments. As such, it is
important that it be included as part of a moral judgment
model. There are those who see empathy as that which
creates an emotional understanding of the other. Our model
characterizes empathy as involving having two distinctive
processes. These are cognitive and affective empathy.2
Cognitive empathy includes the perspective taking
necessary to identify the emotional state of another person
and the imagination required to put oneself in that other
person’s shoes. In other words, it allows a person to imagine
how she would feel if she were in similar circumstances to
that other person. Batson et al. (1997) has shown that asking
participants to take on others’ perspectives makes us more
likely to offer them help.
Affective empathy can be understood as actually feeling,
2

While there are possibilities of further subdividing empathy, it
would unnecessarily over complicate our model at this point.

1417

as opposed to simply perceiving, the emotional state that
another is in. It is a state of emotion matching to the other
person. Affective empathy is believed to be very important
for altruistic behaviour and personal relationships, as has
been shown to increase the likelihood of helpful behaviours
(Mullins-Nelson, Salekin, & Leistico, 2006).
Cognitive empathy appears to have the ability to change
the way in which we reason when we have a better
understanding of what the other person is going through.
However, cognitive empathy without affective empathy
may result in increases of unethical behaviour. For example,
psychopaths are noted for their manipulative behaviours and
are known to exploit the information that they have on
others (Hare, 2003). Their cognitive empathy is intact, but
affective empathy is not. Perspective taking without
appropriate emotional response may result in a higher
probability of unethical behaviour.

Rule Application
Our model includes a box for rule application to account
for the influence of explicitly represented moral principles
(such as “I don’t eat meat”), as well as the use of rote
retrieval of previous cases (or prototypes) of moral
judgments (Reynolds, 2006).
This box also contains “scripts” that can be used in moral
judgment. To demonstrate how scripts work, we will briefly
discuss moral reasoning in those with autism, who do not
use emotional cues (Brewer et al., 2015). People with
autism are solely reliant on deliberative, reason-based
approaches for making moral decision. One of the most
effective strategies for teaching children with autism is the
use of social scripts. These scripts include everything from
everyday tasks such as proper hygiene and setting the dinner
table to more complex, such as how to interact with
someone in distress. Kelly and Maibom (2012) argued that
autistic morality is founded in these social scripts. This may
offer insight into how, despite having emotional deficits
(Blair, 2005) that impair emotional processing, they are able
to make ethical judgments and act accordingly. We assume
that this process occurs in those without autism as well.
As described earlier in the case of moral dumbfounding,
people change their minds in 10-23% of the cases of
trolley/footbridge problems, based on subsequent reasoning.
Charon allows for this because, like other dual-process
models, reason and emotion run in parallel, both
contributing “opinions” that are weighed to eventuate in a
final judgment.

Self-Interest
People’s self-interest clearly affects their moral action.
Sometimes people will do something they know is wrong
because it will help them. But self-interest can also affect
moral judgment. In a magazine survey, 85% of people
agreed that “If someone sues you and you win the case,
should he pay your legal costs?” But only 44% agreed with

“If you sue someone and lose the case, should you pay his
costs?” (cited in Greene, 2012, 83).
Thus, in Charon, self-control affects moral judgment as
well as moral intention (Fig. 1).

Self-Control / Akrasia
A final distinction of our model, the inclusion of selfcontrol, will be made by appealing to psychopathy. As
asserted earlier, a person can enter a situation with depleted
self-control; however self-control can also fail throughout a
morally-charged event. A place in where it seems to fail in
psychopaths is at the level of intention.
Lack of self-control is part of the psychological construct
for psychopathy. For example, M. Sib Ansari and
colleagues (2010) found that psychopaths are hypersensitive
to rewards, such as money and drugs. As was earlier
established, psychopaths can avail themselves of utilitarian
reasoning, but even when they make a judgment that
something is wrong they seem unable to act accordingly
(Cima et al., 2010). These findings suggest that this might
be a failure of self-control: they might know right from
wrong, but when they have something to gain, that goes out
the window.3
Even for non-psychopaths, people with impulsivity issues
might fail to act ethically because of a failure of selfcontrol, even after producing the right moral judgment.
Another study found that training in self-control resulted
in decreased anger (and retaliation) when facing aggression
from another person (Denson, Capper, Oaten, Friese, &
Schofield, 2011). The fact that anger was decreased is
suggestive that moral judgment was affected, because anger
has been shown to result in moral judgments (Haidt, 2012).
But future studies should tease the effects of action and
judgment out more carefully.

Applying Charon to Successful Psychopathy
Hare describes the psychopath in the following way: “A
social predator who charms, manipulates and ruthlessly
plows their way through life...completely lacking in feelings
for others, they selfishly take what they want and do as they
please, violating social norms and expectations without the
slightest sense of guilt or regret (Hare, 2003, xi).” Though
this combination of traits most often results in criminal
behaviour, psychopaths make up between 1-2% of the
general population (Hare, 2003)—far more than are
imprisoned.
Those who meet the diagnostic criteria for psychopathy,
but maintain successful and productive lifestyles are known
as “successful” psychopaths. The main features which
distinguish a successful psychopath is their ability to abstain

3

Another interpretation is that even when psychopaths
know right from wrong, and have self-control, they are
simply not sufficiently motivated to care.

1418

from criminal behaviour and that they succeed in their
professional domain (Lykken, 1995).
Surveys sent to to people in three professions – attorneys,
psychologists, and professors—asked people to report on
psychopaths they worked with. It was found that successful
psychopaths were described as dishonest, exploitative, low
in remorse, minimizing of self-blame, arrogant, and shallow
(Mullins-Sweatt, Glover, Derefinko, Miller, & Widiger,
2010).
Unsuccessful psychopaths tend to have lower scores in
the facets dutifulness, self-discipline and deliberation
(Lynam & Widiger, 2007). An important difference
between successful and unsuccessful psychopaths is that the
former is higher in conscientiousness (Mullins-Sweatt et al.,
2010).
An examination of the successful psychopath suggests
four ways in which the underling cognitive features of their
moral decision-making system contribute to their success.
The first process is their cognitive empathy. As Babiak
and Hare (2006) note, being able to read people can easily
contribute to being able to manipulate them, understand
their weaknesses, and use these to one’s advantage. While
this doesn’t necessarily lead to a “moral” course of action, it
can and does lead to success in many professional areas.
A third way in which the successful psychopath fits well
within the Charon model of moral decision-making is that
they have higher conscientiousness (Lynam & Widiger,
2007; Mullins-Sweatt et al.). In trying to explain why it is
that they remain unincarcerated, one needs to refer back to
the self-control/akrasia box of the model. This is the
sequentially last cognitive control that we have over
morality prior to action. At the very end, it determines
whether we act or refrain from action both in the case of
good and bad action. The successful psychopath has better
inhibition than does the regular psychopath. Even if they
were to arrive at a bad choice, they have the ability to
refrain from action in the case of having made an immoral
judgment because they have more self-control.
Finally, while there has been very little empirical work
done investigating the specific moral deficits of successful
psychopaths, there is no evidence to suggest that they are
different from other psychopaths when it comes to their
ability to use utilitarian reasoning to arrive at judgments.
They would have the capacity to follow through on these
judgments because of their higher self-control. While the
use of utilitarian reasoning unchecked by emotional
processes can lead to a cold morality, there are times when
this is the necessary course of action. For example, for
politicians who need to make decisions regarding who and
when to send people to war when it means that they may die
requires some utilitarian calculations. It is potentially the
case that those with only utilitarian reasoning to use for
these purposes may be well-suited to making these kinds of
decisions as they are less likely to allow emotions to impede
their decision-making process.

To summarize, while the Charon model can differentiate
between the successful and unsuccessful psychopath and
explain why it is that the former are more successful, it
cannot conclude that the successful psychopath are
necessarily more morally successful. They are better at not
doing illegal things because they have higher self-control,
but many of the ways in which they move ahead
professionally are of dubious moral methods. They are
manipulative, cold, hurtful, and will leave a metaphorical
line of bodies behind them to get to where they want to be.
(Babiak & Hare, 2007). The severity of their moral lapses
may not be as severe as those of the unsuccessful
psychopath, but it is a difference in kind. It would not be
fair to conclude that one is moral and the other is not. Both
make bad moral decisions in general, just one is more
inhibited about it and stays on the right side of the law.

Conclusions
The Charon model builds on past research in ethical
decision-making to produce a unique approach that accounts
for a broader range of evidence than previous models. Its
use of empirical evidence from psychopathy and autism
results in a more robust concept of emotion judgment. It is
intended to account not only for quick moral judgments, but
also judgments that are arrived at after considerable
reflection.
Where previous models contribute valuable concepts,
such as reason, emotion, moral foundations, and dualprocessing, we have shown evidence of the influence of
many more factors in the complex process of moral
judgment, including a more sophisticated account of prior
mental state, imagination, empathy, the feedback process
between emotion and reason, self-interest, and self-control.
We expect that future research will show that even more
aspects of mind affect moral judgment. We encourage
modelers to incorporate all of these findings into their
models.

References
Amit, E., Greene, J. (2012). You see the ends don’t justify
the means: Visual imagery and moral judgment.
Psychological Science, 23(8), 861—868.
Ansari, M. Sib, et al. (2010). Mesolimbic dopamine reward
system hypersensitivity in individuals with psychopathic
traits. Nature Neuroscience, 13, 419-423.
Babiak, P., & Hare, R. (2006). Snakes in suits: When
psychopaths go to work. US: Harper Business.
Batson, C. D. et al. (1997). Empathy and attitudes: Can
feeling for a member of a stigmatized group improve
feelings toward the group?. Journal of Personality and
Social Psychology, 72, 105-118.
Blair, R. J. (2005). Responding to the emotions of others:
Disassociating forms of empathy through the study of
typical and psychiatric populations. Consciousness and
Cognition, 14, 698-718.

1419

Brewer. R., Biotti, F., Catmur, C., Press, C.,Happé, F.,
Cook, R., & Bird, G. (2015). Can neurotypicals read
autistic facial expressions? Atypical production of
emotional facial expressions in Autism Spectrum
Disorder. Autism Research, 00, 1-10.
Cima, M., Tonnaer, F., & Hauser, M. D. (2010).
Psychopaths know right from wrong but don’t care.
Social Cognitive and Affective Neuroscience, 5, 59–67.
Denson, T. F., Capper, M. M., Oaten, M., Friese, M., &
Schofield, T. P. (2011). Self-control training decreases
aggression in response to provocation in aggressive
individuals. Journal of Research in Personality, 42, 252–
256.
Foot, P. (1978). Virtues and vices and other essays in moral
philosophy. CA: University of California Press.
Gaudine, A., & Thorne, L. (2001). Emotion and ethical
decision-making, Journal of Business Ethics, 31, 175-187.
Greene, J., Sommerville, R., Nystrom, L., Darley, J., &
Cohen, J. (2001). Emotional engagement in moral
judgment. Science, 293, 2105-2108.
Haidt, J. (2012). The righteous mind: Why good people are
divided by politics and religion. New York: Pantheon
Books.
Haidt, J. (2001). The emotional dog and its rational tail: A
social intuitionist approach to moral judgment.
Psychological Review, 108, 814-834
Haidt, Bjorkland, & Murphy. (2000). Moral dumbfounding:
When intuition finds no reason. Lund psychological
reports.
Hare, R. D. (2003). Manual for the hare psychopathy
checklist-revised (2nd ed.). Toronto: Multi-Health
Systems.
Isen, A., & Levin, P. (1972). Effects of feeling good on
helping: Cookies and kindness. Journal of Personality and
Social Psychology, 21, 384-388.
Kahneman, D. (2013). Thinking, fast and slow. Canada:
Anchor Canada.
Kelly, D. K., & Maibom, H. (2012). “But that’s your role”:
Social models and autistic reasoning. International
Conference on Thinking, London, England. Moral
education. Toronto: University of Toronto Press.
Kelly, D., Messervey, D. L., & Nelson. E. (In press).
Exploration of the personality types and situational
factors in a qualitative analysis of Israeli soldiers during
the First Intifada. (Director General Military Personnel
Research and Analysis Scientific Report ). Ottawa, ON:
Defence Research and Development Canada.
Kohlberg, L. (1976). Moral stages and moralization: : The
cognitive development developmental approach. In C.
Beck and E. Sullivan (Eds.), Moral Education. Toronto:
University of Toronto Press.
Lamm, C., Batson, C. D., & Decety, J. (2007). The neural
stubstrate of human empathy: Effects of perspectivetaking and cognitive appraisal. Journal of Cognitive
Neuroscience, 19, 42-58.
Lykken, D. T. (1995). The antisocial personalities.
Mahwah, NJ: Erlbaum.

Lynam, D., & Widiger, T. (2007) Using a general model of
personality to understand sex differences in the
personality disorders. Journal of Personality Disorders,
21, 583-602.
Messervey, D. L. (2013). What drives moral attitudes and
behaviour? Director General Military Personnel Research
and Analysis Technical Report 2013-003. Ottawa, ON:
Defence Research and Development Canada.
Messervey, D. L., Dean, W. H., Nelson, E., & Peach, J.
(2016). The Defence Ethical Decision-Making Model.
Manuscript in preparation.
Mullins-Nelson, J., Salekin, R., Leistico, A-M. (2006).
Psychopathy, empathy, and perspective-taking in a
community sample: Implications for the successful
psychopathy concept. International Journal of Forensic
Mental Health, 5, 133-149.
Mullins-Sweatt, S. N., Glover, N. G., Derefinko, K. J.,
Miller, J. D., & Widiger, T. A. (2010). The search for the
successful psychopath. Journal of Research in
Personality, 44, 554-558.
Pizarro, D.A., Inbar, Y., & Helion, C. (2011). On disgust
and moral judgment. Emotion Review, 3, 267-268.
Rest, J. (1986). Moral development: Advances in research
and theory. New York: Praeger.
Reynolds, S. J. (2006). A neurocognitive model of the
ethical decision-making process: Implications for study
and practice. Journal of Applied Psychology, 91, 737-748.
Slote, M. (2007). Ethics of Care and Empathy. London:
Routledge.
Thorne, L., & Saunders, S. (2002). The socio-cultural
embeddedness of individuals' ethical reasoning in
organizations (cross-cultural ethics). Journal of Business
Ethics, 35, 1-14.
Trevino, L. (1986). Ethical decision making in
organizations: A person-situation interactionist model.
The Academy of Management Review, 11, 601-617.

1420

