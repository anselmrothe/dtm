                              A rational speech-act model of projective content
                                         Ciyang Qing, Noah D. Goodman, Daniel Lassiter
                                             {qciyang, ngoodman, danlassiter}@stanford.edu
                                                             Stanford University
                              Abstract                                   and discussed in the literature. The problem of explaining how
   Certain content of a linguistic construction can project when
                                                                         certain inferences can survive entailment-canceling operators
   the construction is embedded in entailment-canceling environ-         is called the projection problem.
   ments. For example, the conclusion that John smoked in the                There are two main approaches to the projection problem.
   past from the utterance John stopped smoking still holds for          According to the semantic approach, projective contents are
   John didn’t stop smoking, in which the original utterance is
   embedded under negation. There are two main approaches to             conventional properties of lexical items (e.g., Frege, 1948;
   account for projection phenomena. The semantic approach adds          Heim & Kratzer, 1998). According to the pragmatic approach,
   restrictions of the common ground to the conventional meaning.        projection can be derived from general conversational princi-
   The pragmatic approach tries to derive projection from general
   conversational principles. In this paper we build a probabilistic     ples (e.g., Stalnaker, 1974; Simons, 2001, 2006). How could
   model of language understanding in which the listener jointly         we capture projection patterns using general conversational
   infers the world state and what common ground the speaker             principles? To illustrate the reasoning, let us expand the ex-
   has assumed. We take change-of-state verbs as an example
   and model its projective content under negation. Under certain        ample scenario: Alice and Bob are talking about John, an old
   assumptions, the model predicts the projective behavior and its       friend of Alice’s who is visiting her. Bob has never met John
   interaction with the question under discussion (QUD), without         before so he knows nothing about him. Bob asks Alice, “Does
   any special semantic treatment of projective content.
                                                                         John smoke?” Alice replies, “John did not stop smoking.”1
   Keywords: Presupposition; projection; Bayesian pragmatics
                                                                         Taken literally, Alice’s utterance seems under-informative: it
                          Introduction                                   can be literally true, regardless of whether or not John smokes.
                                                                         If Alice knows whether John smokes and is cooperative, she
  “How am I to get in?” asked Alice again, in a louder tone.
                                                                         would not have said something under-informative. So perhaps
  “Are you to get in at all?” said the Footman. “That’s the
                                                                         her answer is informative after all, but how? Maybe she has
   first question, you know.”          — Carroll (1866), ch. VI
                                                                         taken some additional information for granted, assuming that
   Courtroom drama, political misinformation, and ordinary               it is in the common ground with Bob. Indeed, if Alice took
misunderstandings often revolve around presupposition, a                 for granted that John smoked in the past, then, together with
backgrounded aspect of meaning with a complex logic and                 “John did not stop smoking,” this information would mean that
communicative function. Famously, presuppositions can be                 John still smokes, which fully answers the question of whether
used manipulatively, as in the classic loaded question “Have             John smokes now. In other words, assuming that Alice took
you stopped beating your wife?”, or the sly reporter’s “When             for granted that John smoked in the past best explains Alice’s
did you become aware that this policy was a failure?”. At the            utterance. If Bob further assumes that if Alice took it for
same time, presuppositions serve to streamline conversation              granted then it must be true, then he will arrive at the projected
by allowing interlocutors to convey multiple pieces of infor-            content: John used to smoke.
mation simultaneously. Alice’s question “How am I to get in?”                There are different types of projective content (Tonhauser,
efficiently indicates both Alice’s assumption that she will get          Beaver, Roberts, & Simons, 2013). It is possible that they
in, and her wish to know how to enter.                                   project for different reasons. For change-of-state verbs, there
   In addition to their important communicative role, presup-            are several reasons why one might prefer a pragmatic approach
positions are interesting because they seem to flout some of the         to projection to a semantic one. First, they systematically
most basic rules of logic. For example, from John danced we              show projective behavior. Therefore, a generalization would
can infer John moved, but we cannot infer this from John didn’t          be missing if their projective contents are lexically-encoded
dance—the inference that John moved is canceled by negation.             properties that could vary arbitrarily from verb to verb. In
In contrast, from both John stopped smoking and John didn’t              contrast, a pragmatic approach could in principle explain why
stop smoking we are likely to infer that John used to smoke—             a class of verbs with a similar basic meaning would also have
this inference is said to project over negation. In natural              the same projection behavior. Second, projection interacts
language semantics and pragmatics, an inference that survives            with the contextual question under discussion, as can be seen
an operator that usually cancels inferences is called projective         from the following example (Geurts, 1995). Imagine that
content of the sentence under that operator. Change-of-state             Bob asked Alice: “I notice that John keeps chewing on his
verbs can have information about the past as projective content          pencil. Did he recently stop smoking?” In this context Bob
under negation: John used to smoke is projective content under           is not interested in whether John is currently a smoker, but
negation of “John stopped smoking.” There are many other                     1 Alice’s answer is indirect and complex, and hence would be infe-
types of projective contents (e.g., the complement of know)              licitous without additional contextual justification. This is predicted
under different operators (e.g., questions, modals) identified           by our model.
                                                                     1110

in whether there was a change from smoking to non-smoking                                     u                        JuK
which could explain John’s odd behavior. As a result, if Alice                         “John smokes”             {(T, T ), (F, T )}
were to answer “no (it’s just an nervous habit)”, Bob would                            “John smoked”             {(T, T ), (T, F)}
not infer that John used to smoke. Similarly, if a customer                     “John has always smoked”            {(T, T )}
asks whether an item on sale has been used by anyone before,                     “John stopped smoking”             {(T, F)}
a reply “it is not refurbished” would imply that it is brand                      “John started smoking”            {(F, T )}
new. Third, projection is sensitive to prosodic focus: “John                     “John has never smoked”            {(F, F)}
did not stop smoking” seems to suggest he never smoked. Yet                   Table 1: Positive utterances and their denotations
a major obstacle to adopting the pragmatic approach has been
the difficulty in formalizing the reasoning and showing that it           A Question Under Discussion (QUD) is a function Q that
emerges naturally from conversational principles.                      takes a possible world as its argument and returns the answer
   In this paper, we build on and formalize previous ideas of          to the question in this world. For example, QUDnow is the
the pragmatic approach. We do so within the Rational Speech-           question “Does John smoke now?” It takes a world and returns
Acts (RSA) framework (Frank & Goodman, 2012; Goodman                   its second element, which answers whether John smokes now.
& Stuhlmüller, 2013): a Bayesian approach to language un-             Another example is the maximal QUDmax , which is the identity
derstanding. We extend the previous models by allowing the             function. Intuitively, QUDmax is asking which is the actual
listener to reason about the facts that the speaker took to be in      world. It is maximal in the sense that knowing its answer
common ground (along similar lines to Degen, Tessler, and              means knowing the answer to any QUD.
Goodman (2015)). We find that to account for the example                  To account for projective behavior, we propose additional
scenario we must also make certain assumptions about which             components and assumptions to the standard RSA model in the
facts are plausible common ground and which question is un-            literature (Frank & Goodman, 2012; Goodman & Stuhlmüller,
der discussion. With these assumptions and extensions, the             2013; Goodman & Lassiter, 2015). To better illustrate why
model accounts for projection phenomena of change-of-state             each of them is necessary and how they contribute to the
verbs under negation. It further predicts an interaction between       model’s prediction, we will present the model incrementally.
projective behavior and the question under discussion (QUD)            We will start with the standard RSA model, point out its
(Roberts, 2012), suggesting further experimental research to           problems, motivate a modification, explain the problem it
the growing body of literature (e.g., Cummins, Amaral, &               addresses, review the remaining issues, motivate another mod-
Katsos, 2013; Schwarz, 2015)                                           ification, and so on, until we reach the final model.
        A Rational Speech-Act (RSA) model                              Standard RSA model
                                                                          In the standard RSA model (augmented with QUD as in
In this section we introduce an extension to Rational Speech-          Goodman and Lassiter (2015)), the literal listener, given an
Act (RSA) model (Frank & Goodman, 2012; Goodman &                      utterance and a QUD, randomly samples a world that is con-
Stuhlmüller, 2013) to account for the projection phenomenon           sistent with the utterance, and returns the value of the QUD in
of change-of-state verbs under negation, by formalizing the            that world, as in (2). In this paper we always assume that all
ideas introduced in the previous section. We will continue to          worlds are equally likely a priori, i.e. Pr(w) = 1/4 for each w.
use our working example of the conversation between Alice
and Bob regarding John’s smoking habit.                                           L0 (Q(w) | u, Q) ∝   ∑     δQ(w)=Q(w0 ) · Pr(w0 )   (2)
   We consider the following relevant utterances: “John                                              w0 ∈JuK
smokes,” “John smoked,” “John has always smoked,” “John                   Here δ subscripted with a statement is defined to be 1 if
stopped smoking,” “John started smoking,” “John has never              the statement is true and 0 otherwise. For example, if Q
smoked,” and their negations. In addition, we introduce the            is QUDmax , which asks for a complete specification of the
null utterance “” (say nothing). The prior probability of an           state of the world, after hearing “John didn’t stop smoking,”
utterance depends on the number of content words (i.e., nega-          the literal listener will rule out world (T, F), and return the
tion and auxiliaries are excluded) that it has. The shorter an         remaining 3 worlds with equal probability.
utterance, the higher its prior probability is, as defined in (1).        Given the actual world and the QUD, the probability of the
                    Pr(u) ∝ 2−#content-words(u)                (1)     speaker’s utterance u depends on two factors: the utterance
                                                                       prior and the probability that the utterance will make the literal
   The meaning/denotation of an utterance is standardly de-            listener return the correct answer to the QUD, as in (3).
fined as the set of worlds where the utterance is true. We define
a world w as a pair. Its first element is whether John smoked                         S(u|w, Q) ∝ Pr(u) · L0 (Q(w) | u, Q)α           (3)
in the past and its second element is whether John smokes                 Here α is a rationality parameter controlling the extent to
now. This gives us a set of four possible worlds (the universe         which the speaker optimizes her utterance to induce the correct
U = {(T, T ), (T, F), (F, T ), (F, F)}). All positive utterances       answer from the literal listener. When α → ∞, the speaker will
and their denotations are listed in Table 1. In addition, we de-       always choose utterances that strictly maximize the probability
fine that saying nothing is always true, and that the denotation       of inducing the right answer. In this paper we set α = 6, but
of the negation of an utterance u is U − JuK.                          the qualitative predictions do not hinge on this specific value.
                                                                   1111

                                Standard (no CG + QUDmax )                                                    Uniform CS + QUDmax CG prior + QUDmax CG prior + QUDnow
   literal              L0 (Q(w) | u, Q) ∝ ∑w0 ∈JuK δQ(w)=Q(w0 ) · Pr(w0 )                                             L0 (Q(w) | u,C, Q) ∝ ∑w0 ∈C∩JuK δQ(w)=Q(w0 ) · Pr(w0 )
  speaker                    S(u|w, Q) ∝ Pr(u) · L0 (Q(w) | u, Q)α                                                          S(u|w,C, Q) ∝ Pr(u) · L0 (Q(w) | u,C, Q)α
  listener                     L(w | u, Q) ∝ Pr(w) · S(u | w, Q)                                                           L(w,C | u, Q) ∝ Pr(w) · Pr(C) · S(u | w,C, Q)
  CG prior                                     –                                                                    Pr(C) ∝ 1                Pr(C) ∝ ∑CG⊆Obs P(CG) · δC=∩CG
   QUD                                 QUDmax (w) = w                                                           QUDmax (w) = w            QUDmax (w) = w          QUDnow ((x, y)) = y
                 Table 2: Specifications of four RSA models , with Pr(w) ∝ 1 and Pr(u) ∝ 2−#content-words(u) for all four models
        (T, T)                                                 (T, T)                                                 (T, T)                                                     (T, T)
        (T, F)                                                 (T, F)                                                 (T, F)                                                     (T, F)
    world                                                  world                                                  world                                                      world
        (F, T)                                                 (F, T)                                                 (F, T)                                                     (F, T)
        (F, F)                                                 (F, F)                                                 (F, F)                                                     (F, F)
                 0.00   0.25      0.50       0.75   1.00                0.00   0.25      0.50       0.75   1.00                0.00     0.25      0.50       0.75   1.00                  0.00   0.25      0.50       0.75   1.00
                               probability                                            probability                                              probability                                              probability
            (a) No CG + QUDmax                                (b) Uniform CS + QUDmax                                     (c) CG prior + QUDmax                                      (d) CG prior + QUDnow
                        Figure 1: Pragmatic listener after hearing “John did not stop smoking” for each model, with α = 6
  The pragmatic listener, given the QUD, infers the actual                                                           empty subset of the universe (Stalnaker, 1974). Since we have
world given the speaker’s utterance, using Bayes’ rule (4).                                                          4 possible worlds, there are 24 − 1 = 15 different context sets.
                                                                                                                     These context sets are intuitively named. For example, +past
               L(w | u, Q) ∝ Pr(w) · S(u | w, Q)                 (4)
                                                                                                                     is the context set that contains (T, T ) and (T, F), +past+now
   The standard RSA model is summarized in the first column                                                          contains only (T, T ), ∼+past+now contains all the worlds ex-
of Table 2, and the predicted pragmatic listener’s distribution                                                      cept (T, T ) (∼ A is defined to be U − A, i.e., A’s complement),
over worlds is shown in Figure 1(a). As we can see, the                                                              and change is the context set that contains (T, F) and (F, T ).
standard RSA model predicts a uniform distribution over the                                                             A literal listener, given an utterance, the current context set
three worlds that are consistent with the literal meaning of                                                         and QUD, randomly samples a world that is consistent with
“John did not stop smoking”. It therefore fails to capture the                                                       both the utterance and the context set, and returns the value of
projective content — the inference that John used to smoke.                                                          the QUD in that world, as in (5).
   The reason for this failure is that “John did not stop smok-
ing” is equally under-informative in any of the three worlds                                                                          L0 (Q(w) | u,C, Q) ∝                     ∑            δQ(w)=Q(w0 ) · Pr(w0 )                  (5)
compatible with its literal meaning. For example, suppose the                                                                                                              w0 ∈C∩JuK
actual world is (T, T ). Since the literal listener will return this
                                                                                                                        For example, given context set +past and QUDmax , after
world with probability only 1/3 after hearing “John did not
                                                                                                                     hearing “John did not stop smoking,” the literal listener will
stop smoking,” the speaker is unlikely to choose this utterance.
                                                                                                                     rule out (T, F) because of the utterance’s literal meaning, and
She is more likely to say “John has always smoked” instead,
                                                                                                                     (F, T ) and (F, F) because they are incompatible with the con-
which will always induce the correct answer. The same holds
                                                                                                                     text set. Therefore he will always return (T, T ). We can see
for the other two worlds (F, T ) and (F, F) and therefore the
                                                                                                                     from this example that an utterance that is under-informative
pragmatic listener in the standard RSA model will infer that
                                                                                                                     when the entire universe is considered can be informative in
the three worlds are equally likely.
                                                                                                                     some other context sets.
RSA with common ground                                                                                                  The new speaker model is almost the same as (3), except
   We have seen that one important reason that the standard                                                          that it is relativized to the current context set, as in (6).
RSA model fails to capture the projective content of “John
                                                                                                                                         S(u|w,C, Q) ∝ Pr(u) · L0 (Q(w) | u,C, Q)α                                                  (6)
did not stop smoking” is that its literal meaning is under-
informative when considered in the entire universe U. How-                                                              Finally, given an utterance and the QUD, the pragmatic
ever, as discussed before, there can be information taken for                                                        listener now jointly infers the real world and the context set
granted by the speaker and the listener, i.e., the common                                                            the speaker assumes when she produces the utterance.
ground, and an utterance that is under-informative when con-
                                                                                                                                        L(w,C | u, Q) ∝ Pr(w) · Pr(C) · S(u | w,C, Q)                                               (7)
sidered in the entire universe U may nevertheless be informa-
tive when evaluated in the common ground. To formalize this                                                             We need to specify a prior distribution Pr(C) over context
observation, we now add common ground to the RSA model.                                                              sets in (7). We consider two possibilities. First, we consider
   We first define a related notion. A context set C is a non-                                                       a uniform distribution over all context sets, i.e., Pr(C) ∝ 1.
                                                                                                             1112

                                 (T, T), U                                                                              (T, T), U
                     (T, T), +past+now                                                                      (T, T), +past+now
                   (T, T), ~+past−now                                                                     (T, T), ~+past−now
                   (T, T), ~−past+now                                                                     (T, T), ~−past+now
                   (T, T), ~−past−now                                                                     (T, T), ~−past−now
                            (T, T), +past                                                                          (T, T), +past
                            (T, T), +now                                                                           (T, T), +now
   world, context set                                                                     world, context set
                        (T, T), ~change                                                                        (T, T), ~change
                                 (F, T), U                                                                              (F, T), U
                     (F, T), −past+now                                                                      (F, T), −past+now
                   (F, T), ~+past+now                                                                     (F, T), ~+past+now
                   (F, T), ~+past−now                                                                     (F, T), ~+past−now
                   (F, T), ~−past−now                                                                     (F, T), ~−past−now
                            (F, T), −past                                                                          (F, T), −past
                            (F, T), +now                                                                           (F, T), +now
                          (F, T), change                                                                         (F, T), change
                                 (F, F), U                                                                              (F, F), U
                     (F, F), −past−now                                                                      (F, F), −past−now
                   (F, F), ~+past+now                                                                     (F, F), ~+past+now
                   (F, F), ~+past−now                                                                     (F, F), ~+past−now
                   (F, F), ~−past+now                                                                     (F, F), ~−past+now
                            (F, F), −past                                                                          (F, F), −past
                            (F, F), −now                                                                           (F, F), −now
                        (F, F), ~change                                                                        (F, F), ~change
                                             0.00    0.25   0.50     0.75   1.00                                                    0.00   0.25   0.50   0.75   1.00
                                                        probability                                                                           probability
                                   (a) Uniform CS + QUDmax                                                                  (b) CG prior + QUDmax
                                       Figure 2: Pragmatic listener after hearing “John did not stop smoking,” with α = 6
Assuming the maximal QUD, the model is summarized in                                  model assigns low prior probability to those context sets that
the second column of Table 2 and the pragmatic listener’s                             cannot be built up via conjunctions of natural observations.
marginal distribution over worlds is shown in Figure 1(b). We                         One example of such a context set is change, the rather com-
can see that this model predicts that (F, T ) is slightly less likely                 plex assumption that John has either switched from smoking
than (T, T ) and (F, F), and (T, T ) has the same probability as                      to not, or switched from not smoking to smoking — but we do
(F, F). This does not capture projection.                                             not know which. In contrast, context sets such as +past (i.e.,
   The second possibility makes use of the notion of a com-                           taking for granted that John used to smoke) and -past-now
mon ground (CG) in the pragmatic approach to derive a prior                           (i.e., John did not smoke and does not smoke now) receive
over context sets (Stalnaker, 1974). Intuitively, a common                            higher probabilities because they correspond to observations
ground represents everything that is taken for granted for con-                       that the speaker could plausibly have made.
versational purposes. Formally it is a set of propositions (a                            Using the CG prior (the model is summarized in the third
proposition is a set of worlds), all of which are taken for                           column of Table 2), the pragmatic listener’s marginal distribu-
granted. The context set C, as defined above, can be thought                          tion over worlds is shown in Figure 1(c). We can see that this
of as the conjunction ofTall of the propositions that are being                       model predicts that world (F, T ) is very unlikely, and world
taken for granted: C = CG.                                                            (T, T ) has the same probability as world (F, F). Although it
   In our example scenario, Alice (the speaker) could reason-                         still does not capture projection because (T, T ) is predicted to
ably take for granted certain propositions representing plausi-                       be as likely as (F, F), the model correctly predicts that (F, T )
ble observations about John’s smoking habits — whether he                             is unlikely. Therefore we have made some progress.
smoked in the past, and whether he does now. Therefore, as-
                                                                                         To better understand how the CG prior improves the model
suming that the propositions in the common ground come from
                                                                                      and what the remaining problem is, we plot the pragmatic
observations about John’s past and present smoking habits,
                                                                                      listener’s joint distribution of world and context set in Figure 2.
we can use (8) to naturally define a prior over context sets
(henceforth the CG prior).                                                               In Figure 2(a), with a uniform prior over context sets, the
                                                                                      pragmatic listener has 3 most likely outcomes: world (T, T )
                              Pr(C) ∝         ∑     P(CG) · δC=∩CG           (8)      with context set +past, world (F, F) with context set -now,
                                        CG⊆Obs
                                                                                      and world (F, T ) with context set change (this last outcome is
   Concretely, we assume that each of the observations enters                         slightly less likely than the first two). This is because in these
the common ground independently, with probability .4 (mean-                           outcomes, “John did not stop smoking” can fully identify the
ing that the speaker does not tend to take things for granted).                       world given the context set, and hence these outcomes best
In addition, we add a small amount (5%) of noise to (8), so                           explain the speaker’s utterance. As a result, the marginal
that every non-empty C has a nonzero prior probability. This                          distribution over worlds is almost uniform over the 3 worlds.
                                                                                   1113

                               (T, T), U                                         roughly corresponds to QUDs in our model, which may be
                   (T, T), +past+now                                             non-maximal.
                 (T, T), ~+past−now
                 (T, T), ~−past+now                                                 In our running example, Bob explicitly asked about whether
                 (T, T), ~−past−now
                          (T, T), +past                                          John smokes, which means that the QUD is presumably
                          (T, T), +now                                           QUDnow (i.e., “Does John smoke?”). When we use the previ-
 world, context set
                      (T, T), ~change
                               (F, T), U                                         ous RSA model with the common ground prior, but replace
                   (F, T), −past+now
                 (F, T), ~+past+now                                              QUDmax with QUDnow (summarized in the last column of
                 (F, T), ~+past−now                                              Table 2), the pragmatic listener’s marginal distribution over
                 (F, T), ~−past−now
                          (F, T), −past                                          worlds is shown in Figure 1(d) and the joint distribution of
                          (F, T), +now                                           world and context set is in Figure 3. We can see from Figure 3
                        (F, T), change
                               (F, F), U                                         that (T, T ) with context set +past is the only most likely out-
                   (F, F), −past−now
                 (F, F), ~+past+now                                              come, and the world (T, T ) is the only most likely world (and
                 (F, F), ~+past−now                                              its probability increases with a higher α). This is exactly the
                 (F, F), ~−past+now
                          (F, F), −past                                          projection pattern we aim to capture.
                          (F, F), −now
                      (F, F), ~change                                               To understand why we obtain this result, we note that when
                                           0.00   0.25   0.50   0.75   1.00      the QUD is QUDnow , (F, F) with context set -now is dispre-
                                                     probability                 ferred because the context set -now already entails the answer
                                                                                 to the QUD. That is, it is already known from the context
Figure 3: Pragmatic listener after hearing “John did not stop
                                                                                 set -now that John does not smoke now. This means that
smoking,” with α = 6, CG prior + QUDnow
                                                                                 the speaker would be maximally informative even if he says
   In contrast, in Figure 2(b), with the CG prior, world (F, T )                 nothing. As a result, the speaker would be unlikely to say
with context set change is no longer a likely outcome, be-                       “John did not stop smoking” when the context set is -now, and
cause as noted earlier, the context set change is assigned a                     the pragmatic listener could therefore infer that the context
very low prior. This is why world (F, T ) is correctly predicted                 set -now is unlikely, which means that (T, T ) with context set
to be unlikely. Although the CG prior we introduce above is                      +past is the only winner.
probably over-simplified, the crucial assumption we need is                         Hence the current model predicts the projective behavior for
just that not all context sets are equally likely a priori, and in               “John did not stop smoking” in the example scenario, where
particular change is a fairly unusual context set and should be                  the QUD is explicitly given by Bob’s question. Assuming
assigned a low prior probability, which seems intuitively plau-                  that people generally care about information about now rather
sible. As long as this assumption is satisfied, there could be                   than the past, i.e., the default or most salient QUD is QUDnow ,
alternative ways to motivate a more realistic prior over context                 the model predicts that the preferred projective content of
sets without affecting the model’s qualitative prediction.                       “John did not stop smoking” without explicit QUD is that John
   Nevertheless, we can see that world (F, F) with common                        smoked in the past. Note that our speaker model predicts
ground -now is still one of the most likely outcomes in Fig-                     that “John did not stop smoking” is very unlikely to be used
ure 2(b), and hence the marginal probability of (F, F) is the                    to answer QUDnow , as there exist simpler utterances “John
same as (T, T ) in Figure 1(c). This is not desirable, but is                    smokes/does not smoke.” This explains the perceived weird-
totally expected from the model: the prior for context set -now                  ness of Alice’s indirect answer to Bob’s explicit question.
is the same as for context set +past. Therefore, to fully cap-                   Other QUDs We have introduced a RSA model with
ture projective behavior, we need to further explain why (F, F)                  common ground and shown its predictions for QUDnow and
with context set -now is dispreferred.                                           QUDmax . The prediction is sensitive to the QUD—in Figure 4
Non-maximal QUDs So far, we have been assuming that                              we show predictions for eight different QUDs. In general, it
the QUD is maximal, i.e., the utterance “John did not stop                       seems that the model is making plausible predictions. The
smoking” is chosen to address the question of whether John                       utterance “John did not stop smoking” implies that John has al-
smoked in the past and whether John smokes now. For this                         ways smoked if the QUD is whether John has always smoked
QUD, the RSA model with common ground prior predicts                             (Figure 4(e)).2 It implies that John has never smoked when the
a tie between (T, T ) with context set +past and (F, F) with                     QUD is whether John has never smoked (Figure 4(h)). When
context set -now.                                                                the QUD is about whether John stopped smoking (Figure 4(f))
                                                                                 or there is a change (Figure 4(d)), the listener will believe that
   The maximal QUD is often assumed in applications of RSA
                                                                                 John smoking in the past is about 50% likely (recall Geurts’
models (though see Kao, Bergen, and Goodman (2014)), but
                                                                                 example described earlier). These results are compatible with
in this case there are good reasons to consider non-maximal
                                                                                 Simons et al.’s generalization that only non-at-issue content
QUDs. Empirically, as noted in the beginning, projection
                                                                                 projects. But further experimental data will be needed to
is sensitive to the QUD. Theoretically, there has been a lot
                                                                                 assess whether they are borne out.
of discussion in the previous literature on the relation be-
tween at-issueness and projection (Beaver, 2010; Simons, Ton-                       2 Note that QUDs are just partitions and not presuppositional. We
hauser, Beaver, & Roberts, 2010). Their notion of at-issueness                   use, e.g., always to describe the QUDs only as a mnemonic device.
                                                                              1114

            (T, T)                                                      (T, T)                                                      (T, T)                                                       (T, T)
            (T, F)                                                      (T, F)                                                      (T, F)                                                       (T, F)
     world                                                       world                                                       world                                                        world
            (F, T)                                                      (F, T)                                                      (F, T)                                                       (F, T)
            (F, F)                                                      (F, F)                                                      (F, F)                                                       (F, F)
                     0.00    0.25       0.50      0.75   1.00                    0.00    0.25       0.50      0.75   1.00                    0.00     0.25      0.50       0.75   1.00                    0.00     0.25       0.50      0.75   1.00
                                    probability                                                 probability                                                  probability                                                  probability
                            (a) QUDmax                                                  (b) QUDnow                                                  (c) QUDpast                                              (d) QUDchange
        (T, T)                                                      (T, T)                                                      (T, T)                                                       (T, T)
        (T, F)                                                      (T, F)                                                      (T, F)                                                       (T, F)
    world                                                       world                                                       world                                                        world
        (F, T)                                                      (F, T)                                                      (F, T)                                                       (F, T)
        (F, F)                                                      (F, F)                                                      (F, F)                                                       (F, F)
                 0.00        0.25      0.50       0.75   1.00                0.00        0.25      0.50       0.75   1.00                0.00        0.25       0.50      0.75    1.00                0.00         0.25      0.50       0.75   1.00
                                    probability                                                 probability                                                 probability                                                   probability
                        (e) QUDalways                                                   (f) QUDstop                                                 (g) QUDstart                                                 (h) QUDnever
       Figure 4: Pragmatic listener after hearing “John did not stop smoking” for different QUDs, with α = 6 and CG prior
                      Conclusion and future directions                                                                         Frank, M. C., & Goodman, N. D. (2012). Predicting pragmatic
                                                                                                                                  reasoning in language games. Science, 336(6084), 998. doi:
In this paper, we introduced a probabilistic model in the RSA                                                                     10.1126/science.1218633
framework, which analyzes the projective content of change-                                                                    Frege, G. (1948). Sense and reference. Philosophical Review, 57,
of-state verbs under negation as the result of the listener using                                                                 209–230. (Originally published in 1892)
                                                                                                                               Geurts, B. (1995). Presupposing. Doctoral dissertation, University
general conversational principles to jointly infer the actual                                                                     of Stuttgart.
world and the context set that the speaker assumes The model                                                                   Goodman, N. D., & Lassiter, D. (2015). Probabilistic semantics and
predicts an interaction between projection and the question                                                                       pragmatics: Uncertainty in language and thought. In S. Lappin
                                                                                                                                  & C. Fox (Eds.), The handbook of contemporary semantic theory,
under discussion, formalizing insights of previous pragmatic                                                                      2nd edition. Wiley-Blackwell.
approach to projection and providing concrete quantitative                                                                     Goodman, N. D., & Stuhlmüller, A. (2013). Knowledge and implica-
                                                                                                                                  ture: Modeling language understanding as social cognition. Topics
predictions that we plan to test experimentally.                                                                                  in Cognitive Science, 5(1), 173–184. doi: 10.1111/tops.12007
   Our model is a first step towards a full integration of prag-                                                               Heim, I., & Kratzer, A. (1998). Semantics in generative grammar.
matic approach to the projection problem into a general proba-                                                                    Blackwell Oxford.
                                                                                                                               Kao, J. T., Bergen, L., & Goodman, N. D. (2014). Formalizing
bilistic model of language understanding. We plan to further                                                                      the pragmatics of metaphor understanding. In Proceedings of the
explore the model and see to what extent it can be general-                                                                       thirty-sixth annual conference of the Cognitive Science Society.
ized to other types of projective content and other entailment-                                                                Roberts, C. (2012). Information structure in discourse: Towards an
                                                                                                                                  integrated formal theory of pragmatics. Semantics and Pragmatics,
canceling operators, which will help us further understand                                                                        5(6), 1–69. doi: 10.3765/sp.5.6
the division of labor between semantics and pragmatics in the                                                                  Schwarz, F. (Ed.). (2015). Experimental perspectives on presupposi-
projection phenomena.                                                                                                             tions. Switzerland: Springer International Publishing.
                                                                                                                               Simons, M. (2001). On the conversational basis of some presup-
                                                                                                                                  positions. In R. Hastings, B. Jackson, & Z. Zvolenszky (Eds.),
                                               References                                                                         Semantics and linguistic theory (SALT) 11 (pp. 431–448). Ithaca,
Beaver, D. (2010). Have you noticed that your belly button lint colour                                                            NY: Cornell University.
  is related to the colour of your clothing. In Presuppositions and                                                            Simons, M. (2006). Foundational issues in presupposition. Philoso-
  discourse: Essays offered to Hans Kamp (pp. 65–99). Philadelphia,                                                               phy Compass, 1(4), 357–372.
  PA: Elsevier.                                                                                                                Simons, M., Tonhauser, J., Beaver, D., & Roberts, C. (2010). What
Carroll, L. (1866). Alice’s Adventures in Wonderland. Macmillan.                                                                  projects and why. In N. Li & D. Lutz (Eds.), Semantics and
Cummins, C., Amaral, P., & Katsos, N. (2013). Backgrounding and                                                                   linguistic theory (SALT) 20 (pp. 309–327). Ithaca, NY: Cornell
  accommodation of presuppositions: an experimental approach. In                                                                  University.
  E. Chemla, V. Homer, & G. Winterstein (Eds.), Proceedings of                                                                 Stalnaker, R. C. (1974). Pragmatic presuppositions. In M. K. Munitz
  Sinn and Bedeutung 17 (pp. 201–218).                                                                                            & P. K. Unger (Eds.), Semantics and philosophy (pp. 197–214).
                                                                                                                                  New York: New York University Press.
Degen, J., Tessler, M. H., & Goodman, N. D. (2015). Wonky                                                                      Tonhauser, J., Beaver, D., Roberts, C., & Simons, M. (2013). Toward
  worlds: Listeners revise world knowledge when utterances are                                                                    a taxonomy of projective content. Language, 89(1), 66–109. doi:
  odd. In Proceedings of the thirty-seventh annual conference of the                                                              10.1353/lan.2013.0001
  Cognitive Science Society.
                                                                                                                       1115

