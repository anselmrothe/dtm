                                         Controlled vs. Automatic Processing:
     A Graph-Theoretic Approach to the Analysis of Serial vs. Parallel Processing
                                             in Neural Network Architectures
                                     Sebastian Musslick1,∗ , Biswadip Dey2,∗ , Kayhan Özcimder1,2,∗ ,
                               Md. Mostofa Ali Patwary3 , Theodore L. Willke3 , and Jonathan D. Cohen1
                         1 Princeton Neuroscience Institute, Princeton University, Princeton, NJ 08544, USA.
            2 Department of Mechanical and Aerospace Engineering, Princeton University, Princeton, NJ 08544, USA.
                               3 Parallel Computing Lab, Intel Corporation, Santa Clara, CA 95054, USA.
                                  ∗ Equal Contribution, Corresponding Author: musslick@princeton.edu
                               Abstract                                   that this constraint reflects an intrinsic, structural property of
                                                                          the control system itself (e.g., limited capacity of working
   The limited ability to simultaneously perform multiple tasks
   is one of the most salient features of human performance and           memory). However, alternative accounts have suggested that
   a defining characteristic of controlled processing. Based on           limitations in multitasking capacity reflect local properties of
   the assumption that multitasking constraints arise from shared         the mechanisms used for task execution, rather than an intrin-
   representations between individual tasks, we describe a graph-
   theoretic approach to analyze these constraints. Our results           sic property of the control system itself. According to such
   are consistent with previous numerical work (Feng, Schwem-             accounts, constraints on multitasking arise when two tasks
   mer, Gershman, & Cohen, 2014), showing that even modest                call upon the same local resources (e.g., representations spe-
   amounts of shared representation induce dramatic constraints
   on the parallel processing capability of a network architecture.       cific to the tasks) for different purposes (Allport, 1980; Meyer
   We further illustrate how this analysis method can be applied          & Kieras, 1997; Navon & Gopher, 1979; Salvucci & Taatgen,
   to specific neural networks to efficiently characterize the full       2008) and thus cannot be performed at the same time.1
   profile of their parallel processing capabilities. We present
   simulation results that validate theoretical predictions, and dis-        Building on this idea, it has been proposed that a funda-
   cuss how these methods can be applied to empirical studies             mental purpose of control mechanisms is to prevent cross-
   of controlled vs. and automatic processing and multitasking            talk, by limiting the engagement of representations used by
   performance in humans.
                                                                          multiple processes (”mulituse representations”) to a single
Keywords: multitasking; cognitive control; capacity con-                  purpose (e.g., task) at any given time (e.g. Cohen, Dunbar,
straint                                                                   & McClelland, 1990; Botvinick, Braver, Barch, Carter, &
                                                                          Cohen, 2001). From this perspective, constraints on multi-
                           Introduction                                   tasking of control-demanding processes reflect the purpose
The human ability to carry out multiple tasks concurrently – a            of control, rather than an intrinsic limit in control mecha-
longstanding focus of cognitive research – presents an inter-             nisms. To the extent that the processing pathways required
esting puzzle. In some domains, humans can fluidly execute a              to perform different tasks rely on shared (i.e., multiuse) rep-
large number of behaviors concurrently (e.g., locomote, navi-             resentations, not only do they become increasingly reliant on
gate, talk, and bimanually gesticulate). However, in other do-            control (to specify the current intended use, and avoid cross-
mains, this capacity is strikingly limited (e.g., conduct mental          talk from competing uses), but the multitasking capacity of
arithmetic while constructing a grocery list). In addition to             the network becomes limited (i.e., driven toward serial pro-
their obvious practical importance, constraints on multitask-             cessing). In other words, control mechanisms are guilty by
ing are also of theoretical significance. Any general theory              association, rather than themselves the source of constraints
of cognition must address how choices are made among the                  on multitasking.
limited set of behaviors that can be carried out at a given time             One question that might be asked is: how does the con-
(Anderson, 2013; Lieder & Griffiths, 2015; Kurzban, Duck-                 straint on multitasking imposed by pathway overlap scale
worth, Kable, & Myers, 2013; Shenhav, Botvinick, & Cohen,                 with network size? A naive assumption might be that, in
2013), and thus the sources of such limitations occupy a cen-             large networks (such as the brain), the constraint is relatively
tral role in cognitive theory.                                            weak, and thus is inadequate to explain the prohibitive con-
   Whether a set of tasks can or cannot be carried out concur-            straints apparent in human control-dependent processing. We
rently has often been attributed to a fundamental distinction             have addressed this question in previous work, by examining
between automatic and controlled processing, with the former              the effects of pathway overlap (multiuse of representations) in
relying on parallel processing mechanisms and the latter on a                 1 Multitasking (and apparent parallelism) can, in some situations,
limited capacity, serial processing (Posner & Snyder, 1975;               be achieved by rapid switching between serial processes (as is com-
Shiffrin & Schneider, 1977). However, this begs a fundamen-               mon in computers). Here, we focus on forms of multitasking that
tal question: why are control-dependent processes capacity                reflect truly concurrent processing, sometimes referred to as per-
                                                                          fect timesharing or pure parallelism. In the General Discussion, we
limited? Early theories, as well as some of the most success-             consider how our findings concerning the conditions for such paral-
ful unified theories of cognition (e.g., ACT-R) have assumed              lelism relate to the capability for rapid serial processing.
                                                                      1547

two types of networks of varying size (Feng et al., 2014). We        for output representations (in-degree of output nodes), and
found that even modest degrees of pathway overlap produced           together these indicate the degree of pathway overlap in the
a strikingly strong constraint on parallel processing that was       network. We assume that such overlap produces interference,
nearly scale invariant. This supported the idea that constraints     prohibiting performance of the tasks involved. We formalize
in human multitasking may reflect representational multiuse,         three types of interference, as shown in Fig 1. Convergent
rather than a limitation intrinsic to control mechanisms them-       interference (shown in green) occurs when two sources of in-
selves. However, while this work was suggestive, it relied on        put compete to determine a common output. In addition, we
numerical simulations that were restricted to a limited range        consider divergence (shown in red) as a form of interference
of parameters. It also failed to provide a clear path from these     in our analysis. Although this does not pose an impediment
theoretical ideas to empirical validation.                           to performance (i.e., it is possible to generate two distinct re-
   Here, we conduct an exhaustive analysis of the relationship       sponses to the same input), it represents a restriction on the
between pathway overlap and parallel processing in single-           number of independent sources of input (and therefore num-
layered, feed-forward, non-linear networks. Our findings val-        ber of tasks) that the system can process at once, and thus can
idate and extend those of Feng et al. (2014), identifying addi-      be treated formally as a type of interference in our analysis of
tional factors that influence the relationship between pathway       multitasking capability. Finally, we consider a third, indirect
overlap and parallel processing capability. We also show how         form of interference that supervenes on the first two (shown
these analysis methods can be used to fully specify the mul-         in blue). In this case, the two tasks in question do not directly
titasking capabilities of a network, and validate derived the-       interfere with one another. However, their simultaneous en-
oretical predictions in simulated neural networks. Critically,       gagement would necessarily engage a third task (shown in
we suggest how this method could also be applied to empir-           purple) that would produce interference; accordingly the two
ical data to determine the multitasking capabilities of natural      tasks shown in blue can not be performed simultaneously. It
agents in realistically large task spaces. Finally, we discuss       is important to note that not only the amount of interference
related results using these methods to examine the interaction       (of the forms just described), but also how it is distributed
between pathway overlap, learning and generalization.                over the network impacts multitasking performance. Here,
                                                                     for simplicity, we assume a uniform distribution of pathways
     Graph-Theoretic Approach to Parallel vs.                        among the input and output components2 , which means the
              Serial Processing Capability                           pathway overlap P is equal to the in-degree and out-degree of
                                                                     each component in the network.
Following Feng et al. (2014), we consider single-layered,
feedforward networks with N input and N output layer com-
                                                                                    A           B            C           D
ponents. Each component represents an input or output di-
mension (vector subspace), and the connection from an input
to an output component constitutes the processing pathway
for a given task (defined as a unique mapping from all pos-
sible vectors in input subspace to corresponding vectors in
the output subspace, that is independent of the mappings for                        1            2           3           4
all other combinations of input and output components in the
network). The network can be represented as a directed bi-           Figure 1: Illustration of the three types of interference con-
partite graph GB = (V , E ), in which the node set V can be          sidered in our analysis (see text).
partitioned into two disjoint sets of nodes Vin and Vout , rep-
resenting the input and output layer components respectively.           To quantify multitasking capability, we begin by construct-
Moreover, an edge (i, j) ∈ E ⊆ Vin × Vout represents a di-           ing an interference graph GI associated with the original bi-
rected pathway from the input layer to the output layer in the       partite graph GB . By assigning each edge of the original graph
network (i.e., a task). We introduce the matrix A = [ai j ] ∈        GB to a node in GI , each node in the GI is used to repre-
{0, 1}N×N to represent the network structure and define its el-      sent a task. Interference between tasks is then represented
ements such that ai j = 1 when, (i, j) ∈ E , i ∈ Vin , j ∈ Vout      by assigning edges to pairs of nodes in GI if the tasks repre-
and ai j = 0 otherwise.                                              sented by those nodes are subject to any of the three forms of
                                                                     interference defined above (formally, this corresponds to the
Pathway Overlap and Interference
                                                                     square of the line graph of GB ). The adjacency relationships
The matrix A, extracted from the adjacency matrix of the bi-         between nodes in GI thus describe which tasks in the original
partite graph, captures the overall network structure, since by      network can be executed concurrently (i.e., in parallel). This,
definition the graph is directed and has no self-loops. In par-      in turn, can be used to identify the maximum multitasking
ticular, it represents the degree to which pathways overlap          capability of the network, as discussed in the next section.
(i.e., share representations): the sum of each row of matrix
A reflects the multiuse of input representations (out-degree             2 Such a uniform distribution is also reflective of a relatively
of input nodes), the sum of each column reflects the same            broad range of distributions in constraining multitasking.
                                                                 1548

                              A             B         C                       3C          3B                     pirical data (e.g. neuroimaging analyses). We will describe
                                                                                                                 how neural representations of tasks can be used to generate
                                                                         1C                     2B               predictions about how many and which combinations of tasks
                                                                                                                 a network (or person) can perform in parallel (a space of pos-
                                                                                                                 sibilities that grows combinatorially with the number of tasks,
                                   1         2        3                       1A          2A                     and thus quickly becomes intractable to direct empirical in-
    (a)Bipartite Graph - GB                                    (b)Interference Graph - GI                        quiry), based on measurements of single task performance
                                                                                                                 (that grows only linearly in the number of tasks). These anal-
Figure 2: The first subfigure (2a) illustrates a network of size                                                 yses may provide useful diagnostic tools for exhaustively as-
N = 3 and pathway overlap P = 2 (in-degree and out-degree                                                        sessing multitasking capabilities based on amounts of data
of each component is 2). The second subfigure (2b) shows                                                         that are practical to acquire.
the associated interference graph with 6 nodes, and each of
these nodes has 4 neighbors due to interferences.                                                                Network Architecture and Processing
Maximum Independent Set (MIS) as a Measure of                                                                    We focus on a network architecture that has been used to
Multitasking Capability                                                                                          simulate a wide array of empirical findings concerning hu-
Identifying the multitasking (maximum parallel processing)                                                       man performance (e.g. Cohen et al., 1990; Cohen, Servan-
capability in the original network can be cast as the prob-                                                      Schreiber, & McClelland, 1992; Botvinick et al., 2001). Such
lem of finding the largest set of nodes in the interference                                                      networks typically consist of four layers (see Figure 4): an
graph wherein no two nodes are adjacent. This is formally                                                        input layer with two partitions, one of which represents the
known as the maximum independent set (MIS). Finding the                                                          current stimulus and projects to an associative layer, and an-
MIS of a graph is an NP-hard problem, that has been stud-                                                        other that encodes the current task and projects to both the
ied extensively in the graph theory literature (Tarjan & Tro-                                                    associative and output layers; an associative (hidden) layer
janowski, 1977). Figure 3 summarizes the effect of pathway                                                       that projects to the output layer; and an output layer that rep-
overlap and network size on the MIS for networks with uni-                                                       resents the network’s response. Input units are clamped to
form pathway distribution (comparable to Feng et al., 2014),                                                     either 0 or 1 to represent the current stimulus and task. These
confirming that parallel processing capability is severely con-                                                  values are multiplied by the matrix of connection weights
strained by pathway overlap in a manner that is virtually                                                        from the input layers to the associative layer, and then passed
scale invariant for network size (source code available at                                                       through a logistic function to determine the pattern of activ-
github.com/musslick/CogSci-2016). In the sections that                                                           ity over the units in the associative layer. This pattern is then
follow, we show how these analysis tools can be used to in-                                                      used (together with projections from the task units in the in-
fer the particular parallel processing capabilities of specific                                                  put layer) to determine the pattern of activity over the output
networks, validate predictions made based on extracted in-                                                       layer. The latter provides a response pattern that is evaluated
terference graphs in simulations of network multitasking per-                                                    by computing its mean squared error (MSE) with respect to
formance, and describe how these tools could be used to infer                                                    the correct (task-determined) output pattern.
similar information regarding human performance from neu-                                                                          output
roimaging data.                                                                                                           
                                                                                                                          yo
     Application to Neural Network Models                                                                                                                       1
                                                                                                                     neto = ∑ woh yh +∑ wot xt + θ o yo =
In the previous section we introduced graph-theoretic analy-                                                                   h            t
                                                                                                                                                             1+ e−neto
ses to investigate factors affecting the parallel processing ca-                                                                                                               wot
                                                                                                                               hidden                  woh
pability in simplified network structures. Here, we examine                                                              
                                                                                                                         yh                                    …
the extent to which these analyses can be applied to more
complex models (such as artificial neural networks) and em-                                                                                                      1
                                                                                                                     neth = ∑ whs xs +∑ wht xt + θ h   yh =
                                                                                                                               s            t                 1+ e−neth    wht
                                                                                                                                                       whs
      Parallel Processing Capability
                                                                                                                               stimulus                                        task
                                       50                                                                                                                                
                                                                                                                         xs                                               xt          …
                                       40
                                       30
                                                                                                                 Figure 4: Feedforward neural network used in simulations.
                                                                                                                 The input layer is composed of stimulus vector →    −
                                       20
                                                                                                                                                                     xs and task
                                       10                                                                                →
                                                                                                                         −
                                                                                                                 vector xt . The activity of each element in the associative
                                                                                                         50
                                                                                                                 layer yh ∈ →
                                                                                                                            −
                                                                                                    40
                                        0                                                      30
                                            10   20       30                         10
                                                                                          20                                yh is determined by all elements xs and xt and their
                                                                    40        50
                                                 Path Overlap - P                  Network Size - N              respective weights whs and wht to yh . Similarly, the activity
Figure 3: Network parallel processing capability as a function                                                   of each output unit yo ∈ →−
                                                                                                                                           yo is determined by all elements yh
of pathway overlap (P) and network size (N) for networks                                                         and xt and their respective weights woh and wot to yo . A bias
with uniform pathway distributions.                                                                              of θ = −2 is added to the net input of all units yh and yo .
                                                                                                              1549

   Stimulus input units are structured according to dimensions             Accordingly, a bipartite graph (of the type shown in Figure 3)
(subvectors of the stimulus pattern), each of which is com-                can be constructed by measuring the patterns of activity ob-
prised of a set of feature units with only one feature unit ac-            served in the network while it performs each individual task.
tivated per dimension. Similarly, output units are organized               This can then be analyzed, using the graph-theoretic meth-
into response dimensions, with only one response unit per-                 ods described above, to examine the full multitasking pro-
mitted to be active per dimension. Each task is represented                file of the network – that is, both the maximum concurrency
by a single task input unit that is associated with a set of               (parallel processing) capability of the network and, perhaps
unique, one-to-one mappings between the input units in one                 more interestingly, the exact profile of which combinations of
stimulus dimension and the output units in one response di-                tasks can and cannot be performed concurrently (see Figure
mension, and that is independent of the mappings for all other             6). This procedure is substantially more efficient, and scales
tasks. Here, we focus on networks (N = 6) in which there are               more gracefully (linearly with size of the network) than de-
six input dimensions comprised of two features each, and six               termining the multitasking profile by simulating and examin-
output dimensions comprised of two responses each. Such                    ing performance of the network for all combinations of tasks
networks support a total of 6 ∗ 6 = 36 possible tasks; and,                (which scales factorially).
since each stimulus input dimension consists of two features,                                                Task Environment
26 = 64 possible input patterns per task (including both task-                                    
relevant and task-irrelevant features). The number of hidden                             Response yo
layer units in each network is set to 200 to avoid constrain-                                                 2                         8       10    12
ing the network to low-dimensional mappings of the input                                                                                         11
                                                                                                         1        3           6
                                                                                                                          5
space. Networks are initialized with a set of small random                                                           4           7         9
                                                                                          Stimulus xs
weights and then trained using the backpropagation algorithm
(David E. Rumelhart & Williams, 1986) to produce the task-                                                        Training
specified response for all stimuli in each task. That is, the net-              Thresholded Task Similarity Matrices From Trained Neural Network
work is trained to generate the response for the corresponding                             Associative Layer                          Output Layer         r
stimulus in the task-relevant dimension, while suppressing re-
sponses in all other response dimensions.
Extracting Directed Bipartite Graph from Task
                                                                                 Tasks
Representations
Our analysis focuses on the representations (patterns of ac-
tivity) over the associative and output units, insofar as these
reflect the computations carried out by the network required                                    Tasks                                    Tasks
to perform each task. In particular, we are interested in the
characteristics of these representations for each task, how                                      Extracted Directed Bipartite Graph
they compare across tasks, and how these factors correspond
to multitasking performance. The representations associated                                              2                            10 12
with each task can be characterized by calculating, for each                                                              7            11
                                                                                                     1       3 45 6           8
unit in the associative and output layers, the mean of its activ-                                                                 9
ity over all of the stimuli for a given task; this mean pattern
of activity can then be used as a representation of the task3 .            Figure 5: A task environment consists of 12 possible tasks
Correlating these patterns of activity across tasks yields a task          represented as input-output mappings. The bipartite graph
similarity matrix that can be examined separately for the as-              can be extracted from thresholded task similarity matrices
sociative and output layers of the network. This can then be               that are obtained from task activity correlations at the asso-
used to assess the extent to which different tasks rely on sim-            ciative layer and output layer of a trained network.
ilar or different representation within each layer of the net-
work. Figure 5 provides an example of such similarity ma-                  Simulation Experiment
trices (thresholded for similarity correlations above r > 0.8).            To validate the methods described above, we compared simu-
Tasks that have similar representations over the associative               lated network performance with analysis predictions for 100
layer can be inferred to rely on the same input dimension –                networks of size N = 6, each trained on a different sub-
that is, they share an input component in the bipartite graph              set of 12 randomly sampled tasks (source code available at
representation of the network – and tasks that are similar at              github.com/musslick/CogSci-2016). Tasks were chosen
the output layer can be inferred to share an output component.             subject to the constraint that each stimulus dimension was
                                                                           associated with two tasks. For each network we extracted a
   3A   formally equivalent analysis could be carried out using the        bipartite graph from the task similarity matrices as outlined
weight matrix of the network. Here we focus on patterns of activ-
ity, as these may serve as useful predictors for patterns of activity      above. Figure 5 shows the results for an example network,
observed in empirical data, such as fMRI and/or unit recordings.           from which a bipartite graph was generated that recovered
                                                                        1550

the exact task structure imposed during training. That is, the                                  General Discussion and Conclusion
network learned to use similar associative layer representa-
                                                                                        We have introduced a graph-theoretic approach to com-
tions for tasks that involved the same stimulus dimension (e.g.
                                                                                        pute the multitasking (parallel processing) capability of feed-
tasks 1 & 2 in Figure 5), and learned similar output represen-
                                                                                        forward, single-layer non-linear networks. This was achieved
tations for tasks involving the same response dimensions (e.g.
                                                                                        by generating an interference graph from the directed bipar-
tasks 2, 3, 5 & 7 in Figure 5).
                                                                                        tite graph representation of the network, which provides a
                                                                                        compact representation of its multitasking capabilities. Iden-
                                                               Multitasking Score!
                                                                                        tifying the MIS in the interference graph reveals the maxi-
                                                                                        mum number of concurrent tasks that can be executed with-
              Tasks!                                                                    out performance loss. The interference graph can also be used
                                                                                        to identify all combinations of tasks that can be performed in
                                                                                        parallel. We have shown that, consistent with previous work
                                                                                        (Feng et al., 2014), introducing even modest amounts of path-
                                     Tasks!                                             way overlap induces dramatic constraints on multitasking ca-
Figure 6: Extracted adjacency matrix of interference graph.                             pability. We then illustrated how the graph-theoretic analysis
Off-diagonal colored entries indicate all tasks that can be                             can be applied to specific networks, using the patterns of ac-
paired with a given task. Colors of off-diagonal elements                               tivity associated with individual tasks to characterize the full
indicate the number of all possible multitasking conditions                             profile of the network’s multitasking capabilities.
in which the corresponding task-pairing can occur. Colors                                  At a practical level, these methods suggest possibilities for
of diagonal elements indicate the number of all multitasking                            empirical research. For example, if patterns of neural activity
combinations in which the corresponding task can occur.                                 (measured using direct neuronal recordings and/or fMRI) can
   For each network, a bipartite graph can be constructed and                           be identified for a set of individual tasks, then the analyses
used to extract a corresponding interference graph. The adja-                           described above can be used to predict multitasking perfor-
cency matrix of the latter indicates which tasks can be paired.                         mance for all possible combinations of tasks in the set. The
This can be used, in turn, to identify all combinations of tasks                        measurements required to carry out this analysis grow lin-
that can be performed concurrently (see example in Figure                               early with the number of tasks in the set, whereas the num-
6), as well as the MIS which specifies the greatest number                              ber of measurements required to characterize the interactions
of tasks that can be performed concurrently. For each of the                            among them from behavior would grow factorially with set
100 trained networks, we extracted its interference graph and                           size. That is, these analyses may be particularly useful in sit-
computed the multitasking performance (MSE) for all combi-                              uations in which exhaustively assessing the entire space of
nations of tasks that belonged to an independent set. We com-                           task combinations is empirically impractical.
pared this to a random sample (of identical size) composed of                              Theoretically, the approach provides a formal framework
combinations of tasks in which two or more of the tasks did                             for studying the relationship between learned task representa-
not belong to an independent set. Figure 7 shows that these                             tions and controlled (serial) vs. automatic (parallel) process-
analyses yield accurate predictions about which tasks can be                            ing. Specifically, it permits quantitative analysis of how task
performed concurrently and which cannot. As the network                                 environment and learning impact the tradeoff between com-
was never trained on multitasking, concurrent performance                               pactness of representation (associated with serial, control-
of tasks from independent sets can still produce some error.                            dependent processing) and multitasking capability (associ-
However, the performance for those tasks is markedly and re-                            ated with parallel, automatic processing). In related work
liably better than multitasking performance for combinations                            others (e.g. Caruana, 1997; Bengio, Courville, & Vincent,
of tasks in which not all belong to the same independent set,                           2013; Saxe, McClelland, & Ganguli, 2013) have shown that
t(98) = 232.34, p < .001 (2 tasks); t(98) = 132.03, p < .001                            compact, ”multiuse” representations not only make more ef-
(3 tasks); t(79) = 29.64, p < .001 (4 tasks).                                           ficient use of network resources (e.g. fewer associative units)
                       0.4                                                              but also are likely to arise most quickly (especially in hier-
                                Independent
                                Non−Independent
                                                                                        archically structured environments), and support greater gen-
                       0.3
                                                                                        eralization during learning. However, the present work il-
                MSE    0.2                                                              lustrates the costs this incurs with regard to parallelism of
                       0.1
                                                                                        processing: as pathway overlap (mulituse of representations)
                                                                                        increases, processing in the network is rapidly driven to be
                        0
                                   2       3         4                                  serial, and becomes reliant on control mechanisms to avoid
                             Total Number Of Tasks Performed
                                                                                        cross-talk. We suggest that this tension underlies the trade-
Figure 7: Multitasking performance for sets of identified in-                           off between controlled and automatic processing observed in
dependent and non-independent tasks. Error bars indicate the                            human performance, and that constraints on the capacity for
standard error of the mean for multitasking conditions of net-                          human multitasking reflect this tension.
works trained in different task environments.                                              While we have focused on forms of multitasking arising
                                                                                     1551

from concurrent parallelism, our findings are likely to have             cessing account of the stroop effect. Psychological review,
implications even when multitasking is achieved by rapidly               97(3), 332.
switching between tasks. One of the most robust findings in            Cohen, J. D., Servan-Schreiber, D., & McClelland, J. L.
the cognitive literature is the cost associated with switching           (1992). A parallel distributed processing approach to au-
between tasks, reflecting at least in part carry-over effects that       tomaticity. The American journal of psychology, 239–269.
the representations of one task have on the next (Yeung, Nys-          David E. Rumelhart, G. E. H., & Williams, R. J. (1986).
trom, Aronson, & Cohen, 2006, for a review see Kiesel et al.,            Learning representations by back-propagating errors. Na-
2010). To the extent that such carry-over effects reflect inter-         ture, 323, 533–536.
ference from shared representations, then this may determine           Feng, S. F., Schwemmer, M., Gershman, S. J., & Cohen, J. D.
the speed and/or accuracy with which sequential switching                (2014). Multitasking vs. Multiplexing: Toward a norma-
can be achieved in a manner similar to its impact on pure                tive account of limitations in the simultaneous execution of
parallelism. That is, multiuse of representations may define             control-demanding behaviors. Cognitive, Affective, & Be-
a continuum from pure sequential processing, though rapid                havioral Neuroscience, 14(1), 129-146.
task switching, to pure parallelism, and the methods we de-            Kiesel, A., Steinhauser, M., Wendt, M., Falkenstein, M., Jost,
scribe may provide a way of analyzing networks to determine              K., Philipp, A. M., & Koch, I. (2010). Control and inter-
where they lie along this continuum.                                     ference in task switchinga review. Psychological bulletin,
   There are a variety of network parameters that can im-                136(5), 849.
pact the extent to which multiuse representations arise during         Kurzban, R., Duckworth, A., Kable, J. W., & Myers, J.
training (including weight initialization, regularization con-           (2013). An opportunity cost model of subjective effort
straints, number of hidden units, etc.). Here, we have fo-               and task performance. The Behavioral and brain sciences,
cused on a simple network parameterization (two-layered,                 36(6), 661–679.
feedforward, with random weight initialization and no reg-             Lieder, F., & Griffiths, T. L. (2015). When to use which
ularization) as a first assessment of the usefulness of the an-          heuristic: A rational solution to the strategy selection prob-
alytic approach. Another simplification in our treatment was             lem. In 37th cognitive science society conference, tx.
the construction of binary bipartite and interference graphs,          Meyer, D. E., & Kieras, D. E. (1997). A computational theory
by thresholding the real-valued correlation matrix of network            of executive cognitive processes and multiple-task perfor-
representations. Additional simulation results (not reported)            mance: Part i. basic mechanisms. Psychological review,
suggest that the analysis methods we report are robust across            104(1), 3.
a wide range of thresholds and learned task representations.           Navon, D., & Gopher, D. (1979). On the economy of the
However, a generalization of the method to address graded                human-processing system. Psychological review, 86(3),
interference effects (e.g., using weighted graphs) is an im-             214.
portant avenue for future research. More generally, it will            Posner, M., & Snyder, C. (1975). attention and cognitive con-
be important to explore the extent to which the methods and              trol,. In Information processing and cognition: The loyola
analyses we describe can be extended to networks with more               symposium (pp. 55–85).
complex and realistic architectures (e.g., multi-layered and/or        Salvucci, D. D., & Taatgen, N. A. (2008). Threaded cogni-
recurrent, with varying pathway distributions and graded de-             tion: an integrated theory of concurrent multitasking. Psy-
grees of interference). We hope that the work described here             chological review, 115(1), 101.
will encourage a proliferation of efforts along these lines.           Saxe, A. M., McClelland, J. L., & Ganguli, S. (2013). Learn-
                                                                         ing hierarchical category structure in deep neural networks.
                          References                                     In Proceedings of the 35th annual meeting of the cognitive
                                                                         science society (pp. 1271–1276).
Allport, D. A. (1980). Attention and performance. Cognitive            Shenhav, A., Botvinick, M. M., & Cohen, J. D. (2013). The
   psychology: New directions, 1, 12–153.                                expected value of control: an integrative theory of anterior
Anderson, J. R. (2013). The architecture of cognition. Psy-              cingulate cortex function. Neuron, 79(2), 217–240.
   chology Press.                                                      Shiffrin, R. M., & Schneider, W. (1977). Controlled and auto-
Bengio, Y., Courville, A., & Vincent, P. (2013). Represen-               matic human information processing: Ii. perceptual learn-
   tation learning: A review and new perspectives. Pattern               ing, automatic attending and a general theory. Psychologi-
   Analysis and Machine Intelligence, IEEE Transactions on,              cal review, 84(2), 127.
   35(8), 1798–1828.                                                   Tarjan, R. E., & Trojanowski, A. E. (1977). Finding a Maxi-
Botvinick, M. M., Braver, T. S., Barch, D. M., Carter, C. S.,            mum Independent Set. SIAM Journal on Computing, 6(3),
   & Cohen, J. D. (2001). Conflict monitoring and cognitive              537-546. doi: 10.1137/0206038
   control. Psychological review, 108(3), 624.                         Yeung, N., Nystrom, L. E., Aronson, J. A., & Cohen, J. D.
Caruana, R. (1997). Multitask learning. Machine learning,                (2006). Between-task competition and cognitive control in
   28(1), 41–75.                                                         task switching. The Journal of Neuroscience, 26(5), 1429–
Cohen, J. D., Dunbar, K., & McClelland, J. L. (1990). On the             1438.
   control of automatic processes: a parallel distributed pro-
                                                                   1552

