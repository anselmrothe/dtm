                  Active Viewing in Toddlers Facilitates Visual Object Learning:
                                            An Egocentric Vision Approach
                               Sven Bambach, David J. Crandall, Linda B. Smith†, Chen Yu†
                                           {sbambach, djcran, smith4, chenyu}@indiana.edu
                                       School of Informatics and Computing, Indiana University
                                 †Department of Psychological and Brain Sciences, Indiana University
                                                        Bloomington, IN, 47405 USA
                              Abstract
   Early visual object recognition in a world full of cluttered vi-
   sual information is a complicated task at which toddlers are
   incredibly efficient. In their everyday lives, toddlers con-
   stantly create learning experiences by actively manipulating
   objects and thus self-selecting object views for visual learn-
   ing. The work in this paper is based on the hypothesis that ac-
   tive viewing and exploration of toddlers actually creates high-
   quality training data for object recognition. We tested this
   idea by collecting egocentric video data of free toy play be-
   tween toddler-parent dyads, and used it to train state-of-the-art
   machine learning models (Convolutional Neural Networks, or
   CNNs). Our results show that the data collected by parents
   and toddlers have different visual properties and that CNNs
   can take advantage of these differences to learn toddler-based
   object models that outperform their parent counterparts in a
   series of controlled simulations.
   Keywords: vision, visual object learning, convolutional neural
   networks, head-mounted cameras
                                                                         Figure 1: All instances of a toy as seen by cameras mounted
                          Introduction                                   on heads of toddlers (left) and parents (right) during joint play
Visual object recognition is of fundamental importance to hu-            between 10 toddler-parent dyads, showing greater diversity in
mans and most animals, whose everyday lives rely on identi-              toddler views. Instances are shown to scale and colored boxes
fying a large variety of visual objects. Because of its impor-           depict the field of view size.
tance, even human infants possess sophisticated perceptual
and learning processes to form categorical representations of            stored in memory, and how they are activated to recognize
visual stimuli (Quinn & Eimas, 1996). Even as toddlers, they             new instances. However, we also know that these experimen-
already seem to be able to easily recognize everyday objects.            tal paradigms are very different from young children’s ev-
A vexing question for cognitive scientists is how young learn-           eryday learning experiences: active toddlers do not just pas-
ers achieve this ability in a visually noisy and dynamic world           sively perceive visual information but instead generate man-
where objects are often encountered under seemingly sub-                 ual actions to objects, thereby creating self-selection of object
optimal conditions, including in unusual orientations, varying           views (Yu et al., 2009). Indeed, recent work shows that in-
lighting conditions, or partial occlusions (Johnson & Aslin,             fants who have more experience in manual object exploration
1995; Casasola, Cohen, & Chiarello, 2003). Despite recent                have more robust expectations about unseen views of novel
progress in object recognition in the computer vision com-               objects (Soska, Adolph, & Johnson, 2010). Another study
munity (Krizhevsky, Sutskever, & Hinton, 2012), even the                 using head-mounted cameras to record toddlers fields of view
most powerful computational algorithms trained with large                found a preference towards planar views of objects: toddlers
amounts of data are arguably not yet able to learn as effi-              dwelled longer on these views while manually exploring held
ciently as toddlers do.                                                  3-d objects than would be expected if the objects were rotated
   Many previous studies on early visual object recogni-                 randomly. This bias substantially increased between the ages
tion focus on examining exactly what visual information is               of 12-36 months (Pereira et al., 2010).
extracted from the retinal image to construct invariant de-                 Visual object recognition depends on the specific views
scriptors of objects. For this purpose, many experimental                of objects experienced by the learner. In everyday contexts
paradigms have been invented that repeatedly expose young                such as toy play, toddlers actively create many different views
visual learners to stimuli displayed on a computer screen (fa-           of the same object. In light of this, the overall hypothesis
miliarization phase), and then measure looking times towards             in the present study is that active viewing may create high-
familiar and novel stimuli (test phase). These paradigms                 quality training data for visual object recognition. To test this
are powerful, allowing us to examine, in a rigorously con-               hypothesis, we used head-mounted cameras to collect first-
trolled way, which visual features are extracted, how they are           person video data from a naturalistic environment in which
                                                                     1631

parents and children were asked to jointly play with a set of
toy objects. Figure 1 shows examples of different views of
the same toy car, collected from toddlers’ view (left) and par-
ents’ view (right) during the same play sessions. Clearly, tod-
dlers created more diverse views in terms of relative object
size, orientation, and occlusion compared to their parents. In
the present study, we first quantify the differences in visual
properties of objects between toddler and parent views, find-
ing a higher variation among visual instances for the child.
A learning system could take advantage of such variation by
building more generalizable representations for recognizing
unseen instances, thus better facilitating visual object recog-
nition.                                                                         (a) Parent view                 (b) Child view
   To test this idea, the main focus of the study was to train        Figure 2: First-person video examples that were captured dur-
machine learning models based on Convolutional Neural Net-            ing joint child-parent play in our toy room, contrasting parent
works (CNNs), which are currently considered the most pow-            view (a) and child view (b). Each row shows one dyad. Also
erful visual learning models in the computer vision commu-            shown are bounding boxes and toy sizes (as % of FOV).
nity (Krizhevsky et al., 2012), with data from the two differ-
ent views, and to examine the extent to which these models
take advantage of visual information created and perceived
by toddlers. The results show that the CNNs perform better
on object recognition in multiple simulation conditions when
trained with the toddlers’ data than with the parents’ data. To
the best of our knowledge, this is the first study to collect and
use egocentric video in everyday contexts and demonstrate
a working learning system taking advantage of object view
self-selection by active toddlers for visual object recognition.      Figure 3: The 24 toys that were used in all of our experiments.
                       Data Collection                                in the center of the floor and encouraged the dyad to play to-
To test our hypotheses and models, we collected two types of          gether as they pleased. Once they were engaged with the toys,
image data, one for training our CNN models and one for test-         we left the room and did not give further instructions. Most
ing them. For the training data, we used head-mounted cam-            parents sat on the floor, while toddlers switched between sit-
eras to capture first-person video of toddlers and parents as         ting on the floor and walking or crawling around to pick up
they jointly played with a set of toys in a naturalistic, uncon-      new toys. Two toddlers briefly sat in the small chair.
strained setting. For the test data, we collected a controlled           For each child-parent dyad, we extracted the greater of 10
dataset in which we photographed the same set of objects,             minutes of video or the longest period of continuous toy play
but against a clean background and from a systematic set of           (uninterrupted by the child taking off the camera or losing in-
canonical viewpoints. We now describe each dataset in detail.         terest), yielding at least 3 minutes 35 seconds and an average
                                                                      of 7 minutes 58 seconds of video per dyad. All videos were
Training Data                                                         captured with a resolution of 720×1280 pixels at 30 frames
The training data was collected in a small (~15m2 ) room with         per second, and each video pair between toddler and parent
a soft carpet to facilitate sitting on the floor. This “toy room”     was synchronized.
had an adult-sized chair and a toddler-sized chair, but oth-             The location of each of the 24 toy objects within the cap-
erwise no large objects or other distractions. Figure 2 gives         tured first-person video data was manually annotated. To
an impression of the setting. Our data was collected from             do this, we subsampled the video stream at one frame every
10 child-parent dyads (9 mothers and 1 father; 6 girls and 4          five seconds, and then manually drew bounding boxes around
boys, mean child age 22.6 months and SD = 2.1 months). Be-            each toy in each frame. Figure 2 shows four annotated exam-
fore entering the toy room, both the parent and toddler were          ple frames. Since toys were often occluded by other objects
equipped with a head-mounted camera. Both cameras were                or truncated at the frame boundaries, we used the following
small (4.8cm×4.8cm×1.5cm), lightweight (22g) Looxcie 3                guideline: if only part of a toy was visible, we drew a box
cameras with a 100° diagonal field of view. Video data was            around the part if it was visually identifiable as the right toy;
recorded directly onto a microSD card. Cameras were at-               if multiple parts of an identifiable toy were visible, we drew
tached (with velcro) to an adjustable headband to ensure a            a box that included all visible parts of the toy.
tight but comfortable fit on the center of each participant’s            Overall, we captured 9,646 toy instances from the toddler
forehead. Next, we randomly arranged 24 toys (Figure 3)               views and 11,313 from the parent views, for an average of 401
                                                                  1632

instances per class across all toddlers and 471 across parents.      two (cyan box in Figure 4). In total, our test data consisted of
There were no large outliers for any of the toys in terms of         8 × 8 × 2 = 128 images for each toy and 3,072 images total.
appearance frequency; the least frequent toy appeared 307
and 341 times for toddlers and parents, respectively, while            Study 1: Quantifying and Comparing Object
the most frequent appeared 559 and 600 times.                                    Properties in Egocentric Views
Testing Data                                                         During joint play, toddlers and parents generate many in-
                                                                     stances of visual objects within their self-selected fields of
We also created a separate test set of the same 24 toy objects.      view. Our first study quantified and compared properties of
The goal of this test data was to have a large variety of clean,     object appearance across the two views.
systematically-collected, unobstructed third-person views for
each toy, to serve as a view-independent and therefore objec-        Object Appearance in the Field of View
tive way to evaluate the performance of visual object recog-
                                                                     We begin by studying how many toys are present within the
nition. We again used the Looxcie 3 camera but this time
                                                                     field of view, as well as the perceptual size of those toys.
captured static photos (at the same resolution as the video).
The toy room floor was covered in a black cloth to obscure           Number of Visual Objects Figure 5(a) presents histograms
background clutter, and the camera was mounted onto a tri-           showing the number of visual objects that appear simultane-
pod, pointing towards the floor at a 45° angle. Each toy was         ously in the field of view. Toddlers have a larger fraction
put on the floor at a distance of 50cm from the tripod cen-          of frames (16.3%) with only 1-4 objects compared to parents
ter. The height of the camera was 45cm, creating a distance          (11.3%). Conversely, parents are more likely to have most ob-
from lens to toy center of around 67cm, which approximately          jects in view at once, with 24.0% of parent frames containing
centered each toy in the camera frame.                               more than 17 objects versus only 15.4% of toddler frames.
   We captured 8 photos from each toy, one from each 45° an-         Visual Object Sizes Next, we investigate the size of visual
gle rotation around its vertical axis. Sample images from ev-        objects within the fields of view. We approximate the ac-
ery toy are shown inside the red box in Figure 4, while the          tual size of an object with the area of its bounding box, and
green box shows one of the toys from all 8 viewpoints. To            measure the fraction of the field of view that is occupied by
create even more diversity, we additionally rotated each im-         this box. Figure 5(b) shows that 42.3% of object instances
age around the optical center of the camera in 45° increments        occupy ≤ 2% of parents’ field of view, while only 3.5% of
(blue box in Figure 4). Images were then cropped to a bound-         all objects appear dominantly in view (> 8% of FOV). On
ing box around the object. To add scale variation, we padded         the other hand, toddlers exhibit a more spread-out distribu-
and rescaled images to simulate zooming out by a factor of           tion: only 28.3% of objects appear small (≤ 2% FOV) while
                                                                     11.9% of objects occupy more than 10% of the view. For per-
                                                                     spective, the white car (red bounding box) in the bottom row
                                                                     of Figure 2 occupies 5% of the parent view (a) and 13% in
                                                                     the child view (b). These results are consistent with findings
                                                                     from previous head-camera studies (Yu et al., 2009).
                                                                     Variation in Visual Object Appearance
                                                                     Finally, we aim to quantify the visual diversity across the
                                                                     views. We resize each toy image to a canonical size (10×10
                                                                     pixels), and, for each subject, compute the pixelwise mean
                                                                     squared error (MSE) between all instances of the same ob-
                                                                     ject. In other words, we take each 10×10×3 color image, rep-
                                                                     resent it as a 300-dimensional vector, and then compute the
                    (a) Controlled toy images
                                                                     mean MSE distance between all possible pairs of instances
                                                                     for each object and subject. This score should be low for a
                                                                     subject who sees many visually similar instances of an ob-
                                                                     ject, and high for one that sees much variation. We compute
               (b) Toy image with added occlusion                    scores on a per subject basis to control for inter-subject dif-
                                                                     ferences in object appearance.
Figure 4: (a) Samples from the controlled test data. Each               Figure 6 compares the visual diversity between views gen-
of the 24 toys (red) was photographed from 8 viewpoints              erated by toddlers and parents. For each object, we subtract
(green), and each resulting image was further rotated 8 times        the average MSE score across all toddlers from the corre-
(blue). To add scale variation, each image was also cropped          sponding score across parents, so that positive values indicate
at a lower zoom level (cyan). (b) Sample test images with            higher diversity in toddler views while negative values indi-
synthetic occlusion.                                                 cate higher diversity in parent views. Using this metric, we
                                                                 1633

                           0.25
                                                                                          Child
                                                                                                          Study 2: Visual Object Recognition Based on
                                                                                          Parent
                                     0.2                                                                             Deep Learning Models
 Prop. of Frames
                           0.15
                                                                                                      Deep learning using Convolutional Neural Networks (CNNs)
                                     0.1                                                              has recently shown impressive success in computer vision,
                           0.05                                                                       improving the state-of-the-art for visual recognition by a large
                                                                                                      margin (Krizhevsky et al., 2012). We investigate how well a
                                      0
                                            1-4      5-8     9-12       13-16    17-20   21-24        CNN trained with real-world toy instances (as captured dur-
                                                           # Toys per Frame                           ing our joint play experiments) recognizes the same 24 visual
                                            (a) Number of toy objects visible per frame               objects in a separate, controlled testing environment. We do
                                     0.5                                                              not claim that a CNN constitutes the perfect model to emulate
                                                                                          Child
            Prop. of Toy Instances
                                     0.4                                                  Parent      visual object learning in toddlers (or humans in general). In-
                                                                                                      stead, we are interested in CNNs as ideal learners. We assume
                                     0.3
                                                                                                      that the network will learn to use whichever visual features
                                     0.2                                                              are sufficient to distinguish the 24 objects. Given the differ-
                                     0.1
                                                                                                      ences in captured visual object views of parents and toddlers,
                                                                                                      two separate networks, one trained with toddler data and the
                                      0
                                           <=2%   (2%, 4%] (4%, 6%] (6%, 8%] (8%, 10%]    >10%        other trained with parent data, might learn different (better)
                                                           Toy Size [% of FOV]                        strategies. More directly, we hypothesize that the toddler data
                                           (b) Size of toy objects within the field of view
                                                                                                      captures a richer representation of each object, leading to bet-
                                                                                                      ter classification performance on the controlled test data.
Figure 5: Comparison of how objects appear in the fields of                                              We first describe CNN implementation details and verify
view of toddlers and parents, in terms of (a) number of objects                                       that the networks can learn visual object appearance based
appearing simultaneously, and (b) size of objects in view.                                            on first-person data. We then test the networks in a series of
                                                                                                      experiments based on our controlled views of each object.
                                     600
      child MSEt - parent MSEt
                                     400
                                                                                                      Convolutional Neural Networks
                                                                                                      CNNs are a special type of multi-layer, feed-forward neu-
                                     200
                                                                                                      ral networks, consisting of multiple convolutional layers fol-
                                       0                                                              lowed by multiple fully-connected layers. Neurons between
                                                                                                      the convolutional layers are connected sparsely and with
                                 -200
                                                                                                      shared weights, effectively implementing a set of filters. Fil-
                                                                    Toy t                             ter responses are passed to a non-linear activation function as
Figure 6: Difference between toddlers and parents in the vi-                                          well as a local pooling function before serving as input to the
sual diversity for each of the toys. Positive values indicate                                         next layer. Intuitively, the convolutional layers learn filters
higher diversity for toddlers. See text for details.                                                  (from low-level in early layers to high-level in deep layers)
                                                                                                      that extract image features, while the fully-connected layers
                                                                                                      act as a classifier. Please see (Krizhevsky et al., 2012) for
find that toddlers on average generated more diverse views                                            more details on CNNs.
for 20 out of the 24 toys. We also experimented with other                                            Implementation For all our experiments, we used the well
image representations such as grayscale image vectors and                                             established AlexNet CNN architecture (Krizhevsky et al.,
GIST features (Oliva & Torralba, 2001) (which capture shape                                           2012), consisting of five convolutional layers and three fully-
and texture information), and found similar tendencies, with                                          connected layers. The input layer of the network has a fixed
21 and 15 toys being considered as more diverse respectively.                                         size of 224×224×3 neurons, which means the network ex-
                                                                                                      pects input images to be resized to 224×224 pixels. Instead
Discussion                                                                                            of training the network from scratch, we follow the common
                                                                                                      protocol of beginning with a network pre-trained on the Ima-
Our study showed large differences in the views of objects                                            geNet dataset (Deng et al., 2009), which consists of millions
that toddlers and adults interact with, even when jointly inter-                                      of images. We adjust the final layer to have 24 neurons to
acting with them at the same time. While parents are more                                             accommodate our 24-way object classification task, and then
likely to have “overview” views including many objects, tod-                                          use the parameters learned from ImageNet as initialization
dlers are more likely to pick out and inspect single objects                                          for training on our data. Each network is trained via back
up close, resulting in fewer, larger objects in view. Addition-                                       propagation with a softmax loss function, using batch-wise
ally, this object selection process seems to create more diverse                                      stochastic gradient descent with a learning rate of 0.001, mo-
viewpoints for the children than for the parents.                                                     mentum of 0.9, and batch size of 256 images.
                                                                                                   1634

Simulation 1: CNNs Learn from the Training Data                                0.8
                                                                                                                     Child
                                                                                                                     Parent              0.8
                                                                                                                                                                            Child
                                                                                                                                                                            Parent
                                                                               0.7                                                       0.7
Before we experiment with controlled test images, we need
                                                                               0.6                                                       0.6
to ensure that CNNs are indeed able to learn visual object
                                                                    Accuracy                                                  Accuracy
                                                                               0.5                                                       0.5
models from our first-person data. As detailed in Data Col-                    0.4                                                       0.4
lection, our training data includes 24 different visual objects                0.3                                                       0.3
with 11,313 parent views and 9,646 toddler views. Figure 1                     0.2                                                       0.2
illustrates how some of these images look. We now consider                     0.1                                                       0.1
these images as two different datasets (toddler data and par-                    0
                                                                                     Controlled Data     Accuracy per Toy
                                                                                                                                          0
                                                                                                                                               Occluded Data    Accuracy per Toy
ent data), and perform a 6-fold cross validation split on both.
                                                                                                   (a)                                                      (b)
This means that for each dataset we train six different CNN
networks that each use a randomly-selected one-sixth of the                    0.8
                                                                                                                     Child
                                                                                                                     Parent              0.8
                                                                                                                                                                            Child
                                                                                                                                                                            Parent
data for testing and the remaining five-sixths for training.                   0.7                                                       0.7
   For both parent and toddler data, we found that the back-                   0.6                                                       0.6
                                                                    Accuracy                                                  Accuracy
propagation converged after about 10 epochs, i.e. after ob-                    0.5                                                       0.5
                                                                               0.4                                                       0.4
serving the training data around 10 times. The average test
                                                                               0.3                                                       0.3
accuracy across splits was 89.9% for the toddler views and
                                                                               0.2                                                       0.2
93.1% for the parent views. To put this into perspective, ran-                 0.1                                                       0.1
dom guessing achieves 1/24 ≈ 4.2%, while guessing the ma-                       0                                                         0
                                                                                     Grayscale Data    Accuracy per Toy                        Occ. Grayscale   Accuracy per Toy
jority class achieves 5.8% for toddlers and 5.3% for parents.
   We investigated failure cases by inspecting the ten images                                      (c)                                                      (d)
that each network was least confident about (i.e., having the       Figure 7: Classification accuracies of CNNs trained with first-
lowest predicted probability of the true class). Both parent        person image data from toddlers (blue) and parents (orange),
and toddler networks showed similar patterns, where most            when tested on controlled image data of the same objects.
mistakes were caused by either two or more objects overlap-         Bars show standard errors across 10 trained networks.
ping each other, strong motion blur, or a combination of both.
From this we conclude that the failures are reasonable and
that CNNs are indeed able to learn from the first person data.      To test this, we systematically added occlusion to each test-
                                                                    ing image, by splitting the image into quadrants and then oc-
Simulation 2: Using Testing Data from a                             cluding each possible combination of one to three quadrants
Third-Person View                                                   with gray boxes. This resulted in 14 occlusions per image,
                                                                    as shown in Figure 4(b). The occluded testing data thus con-
Now we investigate how well learned concepts from the first-        sisted of 14×3,072 = 43,008 images.
person training data transfer to the clean testing data. We
                                                                       Figure 7(b) presents results of testing the same 2×10 net-
trained a CNN on the first-person toddler training data, and a
                                                                    works from Simulation 2 on the occluded data. Toddler net-
separate CNN on the first-person adult training data, and then
                                                                    works retain better overall mean accuracy compared to the
tested both with the same controlled testing data described
                                                                    parent networks (56.1% vs. 51.3%). The relative perfor-
above (3,072 images). To avoid learning frequency biases,
                                                                    mances when compared to the non-occluded data drop by
since some objects have more training instances than oth-
                                                                    ~30% for both parent and toddler networks, which suggests
ers, we uniformly sampled the training data from each class.
                                                                    that both are affected similarly by occlusion.
Given that CNN training is non-deterministic due to random
training subset sampling and parameter initialization, we re-       Simulation 4: The Effect of Color Information
peated the full training and test procedure over 10 indepen-
dent trials. We stopped training each network after conver-         The differences in performance on the controlled object im-
gence (around 12 epochs).                                           ages might be because one set of networks relies more on
   As shown in Figure 7(a), the networks trained on tod-            color information to learn object models based on the first-
dler data achieve higher recognition accuracy by 6.3 per-           person training data. To examine this idea, we repeat all ex-
centage points compared to the networks trained on parent           periments with grayscale images.
data, reaching 79.6% as opposed to 73.3%. Figure 7(a) also          Cross-validation First, we investigate if the absence of
compares the distribution of mean accuracies for each object.       color information increases the difficulty to learn from the
Overall, the child networks achieve the same or better results      two first-person datasets. We repeat the same 6-fold cross
for 16 out of the 24 toys, indicating that the differences in       validation experiments as described above in Simulation 1,
overall accuracy are not caused by some minority of classes.        but this time train the networks with grayscale images. The
                                                                    average test accuracy across splits decreased to 76.9% (from
Simulation 3: Recognizing Occluded Objects                          89.9%) for the toddler networks and to 83.1% (from 93.1%)
Another interesting question is how well the toddler and par-       for the parent networks. Thus, the absence of color accounts
ent views allow the trained networks to deal with occlusion.        for a rather small drop in learnability of the data.
                                                                 1635

Controlled Testing We repeat our Simulation 2 and 3 ex-              future direction within our framework is to further examine
periments and train two sets of 10 networks, one with the            how CNNs take advantage of visual instances captured from
grayscale toddler images and the other with grayscale par-           diverse viewpoints. Our results here seem to support view-
ent images, and test them on grayscale versions of the testing       based theories of visual object recognition, stating that hu-
dataset images. Figure 7(c-d) summarizes the results. The            man learners store viewpoint-dependent surface and/or shape
toddler networks again significantly outperform parent net-          information (Tarr & Vuong, 2002), and recognize objects by
works in terms of overall mean accuracy, both for the non-           their similarity to stored views (Bülthoff & Edelman, 1992).
occluded and the occluded testing data, with mean accura-            By diagnosing the network directly and visualizing learned
cies of 52.3% over 43.5%, and 30.3% over 26.4%, respec-              high-level filters and the image regions they are likely to fire
tively. The relative performance drops compared to the color         on (Zeiler & Fergus, 2014), we may be able to provide more
experiments are in favor of the toddler networks on the non-         direct evidence on the mechanisms of object recognition.
occluded data (-34% vs. -40%), but slightly in favor of the
parent networks on the occluded data (-51% vs. -54%).                                      Acknowledgments
                                                                     This work was supported by the National Science Foundation (CA-
Discussion                                                           REER IIS-1253549, CNS-0521433, BCS-15233982), the National
Our results suggest that naturalistic first-person images of un-     Institutes of Health (R01 HD074601, R21 EY017843), and the IU
constrained toy play can be used to train CNN-based object           OVPR through the IUCRG and FRSP programs. It used compute fa-
models that generalize to recognizing the same objects in a          cilities provided by NVidia, the Lilly Endowment through its support
different context. It appears that toddlers generate high qual-      of the IU Pervasive Technology Institute, and the Indiana METACyt
                                                                     Initiative. SB was supported by a Paul Purdom Fellowship. We
ity object views that facilitate learning, as networks trained
                                                                     thank Sam Dong, Steven Elmlinger, Seth Foster, and Charlene Tay
on toddler data consistently outperformed parent networks.           for helping with the data collection.
          Summary and General Discussion                                                        References
In the present paper, we collected egocentric video data of          Bülthoff, H. H., & Edelman, S. (1992). Psychophysical sup-
free toy play between toddler-parent dyads, and used it to              port for a two-dimensional view interpolation theory of ob-
train state-of-the-art machine learning models (CNNs). Our              ject recognition. PNAS, 89(1), 60–64.
results showed that (1) CNNs were indeed able to learn ob-           Casasola, M., Cohen, L. B., & Chiarello, E. (2003). Six-
ject models of the toys in this first-person data and (2) that          month-old infants’ categorization of containment spatial
these models could generalize and recognize the same toys               relations. Child development, 74(3), 679–693.
in a different context with different viewpoints. Finally, we        Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei,
showed that (3) the visual data collected by toddlers seems to          L. (2009). Imagenet: A large-scale hierarchical image
be of particularly high quality as models trained with toddler          database. In Proc. CVPR.
data consistently outperformed those trained with parent data        Johnson, S. P., & Aslin, R. N. (1995). Perception of object
in multiple simulation conditions.                                      unity in 2-month-old infants. Dev Psychol, 31(5), 739.
   In the real world, toddlers spend hours every day playing         Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). Imagenet
with toys, actively manipulating objects and, as a result, cre-         classification with deep convolutional neural networks. In
ate learning experiences by self-selecting object views for vi-         Proc. NIPS (pp. 1097–1105).
sual learning. It may not sound surprising that better data          Oliva, A., & Torralba, A. (2001). Modeling the shape of
leads to better learning. Nonetheless, if active viewing by tod-        the scene: A holistic representation of the spatial envelope.
dlers creates high-quality training data for object recognition,        IJCV, 42(3), 145–175.
as evident in the present study, then the potential long-term        Pereira, A., James, K., Jones, S., & Smith, L. (2010). Early
impact of day-in and day-out object play that repeatedly and            biases and developmental changes in self-generated object
incrementally provides such data may be the key to why tod-             views. Journal of vision, 10(11), 22.
dlers are incredibly efficient in visual object learning. If so,     Quinn, P. C., & Eimas, P. D. (1996). Perceptual cues that per-
future research should focus not only on studying particular            mit categorical differentiation of animal species by infants.
learning mechanisms in experimental tasks but also on how               J Exp Child Psychology, 63(1), 189–211.
high-quality data is created by learners themselves. A bet-          Soska, K. C., Adolph, K. E., & Johnson, S. P. (2010). Systems
ter understanding of human learning systems would require a             in development: motor skill acquisition facilitates three-
better understanding of both learning algorithms and the data           dimensional object completion. Dev Psychol, 46(1), 129.
fed into them. This paper represents a first step towards this       Tarr, M. J., & Vuong, Q. C. (2002). Visual object recognition.
direction by linking high-density video data collected in nat-          In Stevens’ handbook of experimental psychology. Wiley.
uralistic contexts with state-of-the-art machine learning.           Yu, C., Smith, L., Shen, H., Pereira, A., & Smith, T. (2009).
   Our future work will focus on further understanding the              Active information selection: Visual attention through the
factors that may account for the observed performance differ-           hands. IEEE Trans Auton Ment Dev, 1(2), 141–151.
ences (e.g. different spatial scales and resolutions of object       Zeiler, M., & Fergus, R. (2014). Visualizing and understand-
images versus innate differences in the viewpoints). Another            ing convolutional networks. In Proc. ECCV (pp. 818–833).
                                                                 1636

