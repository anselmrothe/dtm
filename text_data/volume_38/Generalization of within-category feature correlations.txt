                           Generalization of within-category feature correlations
                                                 Nolan Conaway & Kenneth J. Kurtz
                                            Department of Psychology, Binghamton University
                                                         Binghamton, NY 13905 USA
                                Abstract                                 another (either globally or within a class). By consequence,
                                                                         reference point models are only sensitive to correlations be-
   Theoretical and empirical work in the field of classification
   learning is centered on a ‘reference point’ view, where learn-        tween features insofar as those correlations are reflected in
   ers are thought to represent categories in terms of stored points     the distances between stored reference points.
   in psychological space (e.g., prototypes, exemplars, clusters).          Although there is evidence that people make use of fea-
   Reference point representations fully specify how regions of
   psychological space are associated with class labels, but they        ture correlations in natural concepts (Malt & Smith, 1984),
   do not contain information about how features relate to one           research on correlation learning in a traditional artificial clas-
   another (within- class or otherwise). We present a novel exper-       sification learning (TACL) setting has been mixed. Whereas
   iment suggesting human learners acquire knowledge of within-
   class feature correlations and use this knowledge during gen-         Medin, Altom, Edelson, and Freko (1982) reported evidence
   eralization. Our methods conform strictly to the traditional ar-      that feature correlations influence classification of artificial
   tificial classification learning paradigm, and our results can-       categories, Murphy and Wisniewski (1989) later expanded
   not be explained by any prominent reference point model (i.e.,
   GCM, ALCOVE). An alternative to the reference point frame-            upon that study and found little evidence of correlation learn-
   work (DIVA) provides a strong account of the observed perfor-         ing, unless features are expected to be correlated. Finally,
   mance. We additionally describe preliminary work on a novel           Anderson and Fincham (1996) reported that participants used
   discriminative clustering model that also explains our results.
                                                                         correlations to infer values of missing features; though note
   Keywords: categorization; generalization; formal modeling             that traditional reference point models are unable to simulate
                                                                         feature inference (Lee & Navarro, 2002).
                            Introduction                                    In this paper, we report a classification learning experiment
Research on human classification learning is fundamentally               demonstrating that people represent correlations between fea-
interested in questions of representation: How do people                 tures, beyond what can be explained in terms of stored refer-
represent categories? How does a category’s internal struc-              ence points. In addition to providing evidence that learners
ture influence its subjective difficulty? How do people gen-             do acquire knowledge about correlations between features,
eralize their knowledge about categories? Current research               the classification performance we report demonstrates a sys-
addressing these questions is centered around a ‘reference-              tematic failure in the reference point framework. We bolster
point’ framework, whereby people are thought to acquire cat-             our empirical results with simulations using the Generalized
egory knowledge associating stored perceptual referents (e.g.,           Context Model (GCM; Nosofsky, 1984), an exemplar model
prototypes, exemplars) with individual categories. The suc-              that embodies the central tenets of the reference point view.
cess of reference point models (e.g., Kruschke, 1992; Love,                 We also report simulations using the DIVergent
Medin, & Gureckis, 2004; Nosofsky, 1984; J. D. Smith &                   Autoencoder model (DIVA; Kurtz, 2007, 2015), a au-
Minda, 2000) is unparalleled within the field, and as a result           toassociative network model that stands as a similarity-based
these models are widely considered to be definitive accounts             alternative to the reference point framework. The DIVA
of how categories are learned and represented (for reviews,              model is fully instantiated as a connectionist network: as
see Murphy, 2002; Pothos & Wills, 2011; see also Kurtz,                  in traditional multilayer perceptron (MLP) architectures,
2015).                                                                   DIVA is initialized with a input units encoding feature
   Although reference point models differ from one another               values, as well as a collection of hidden units enabling the
in a variety of ways, these models are comparable in that                learning of an internal representation (Rumelhart, Hinton, &
they assume that categories are represented by one or more               Williams, 1986). DIVA’s primary point of departure from
points in a psychological space. On the extremes, prototype              these models lies in its learning objective: instead of learning
models represent categories with a central tendency (i.e., the           representations to predict class responses, the DIVA model
average across known members), whereas exemplar models                   learns auto-associatively to predict feature values along
use specific observations. Many successful reference point               divergent, category-specific output channels. Thus, DIVA’s
models employ a selective attentional mechanism, enabling                category representations are acquired for the purposes of
them to weight the importance of each stimulus dimension                 making feature predictions rather than class predictions.
(Kruschke, 1992; Medin & Schaffer, 1978).                                Classification decisions are made using a secondary response
   Importantly, however, reference point representations do              rule: the probability of any given classification depends on
not incorporate all aspects of class structure. While points             the relative amount of feature prediction error (reconstruction
of reference can be used to encode information about how                 error) across all categories, with better reconstructions
regions of space are associated with known categories, they              leading to increased probability.
do not contain information about how features relate to one                 DIVA’s design principles offer a unique account of human
                                                                     2375

category learning: rather than assuming people learn to pre-            formance is concisely explained by its design principles: be-
dict class responses through association to stored points of            cause DIVA is trained on feature prediction (rather than class
reference, DIVA proposes that people learn representations of           prediction), the model learns that the features can be used to
the observed regularities within each class. Accordingly, with          predict one another. After training, DIVA reconstructs novel
regard to learning feature correlations, DIVA’s predictions             items following its knowledge of each category’s feature cor-
sharply contrast from those made by reference point models.             relations, affording an extension of the diagonal boundary.
Specifically, because DIVA is trained on feature prediction                We found that the GCM could not mimic DIVA’s perfor-
(rather than class prediction), it strongly relies on within-class      mance. Instead of extending the diagonal boundary outward,
feature correlations to aid learning. Thus, whereas reference           the GCM classifies each critical item with equal probabil-
point models do not encode any information about feature                ity. A close examination of the Diagonal classification ex-
correlations, DIVA relies on those correlations in the service          plains the GCM’s performance: each of the critical items is
of minimizing feature prediction error.                                 equally distant (under a city-block metric) to known members
   Finally, we conclude our report with preliminary work on             of both categories. While a Euclidean metric would enable
a novel discriminative clustering model that explains our re-           the GCM to extend each category to its critical items, the use
sults without acquiring knowledge of how features are cor-              of a city-block metric reflects the separable stimulus dimen-
related. Development of this model is ongoing, though our               sions used in the behavioral experiment reported below (see
simulations results indicate that it may succeed as an account          Garner, 1974). City-block distance was also supported by the
of human classification more generally.                                 results of an independent pairwise similarity-rating study –
                                                                        stimuli X & Y were not rated as more similar to items on their
                                                                        own side of the diagonal boundary.
                                     Y   Y    Y
                                                                           With evenly distributed selective attention, the GCM clas-
                                     Y   Y         X                    sifies each item with a probability of 0.5. Unequal allocation
                                3
                                     Y        X    X                    of attention results in uniform changes to the classification
                               D=
                                                                        gradient, but not generalization of each category along the
                           B             X    X    X                    diagonal. For example, greater allocation to the vertical di-
                      B                                                 mension results in increased Category B probability for the
                                             D=3                        entire collection of critical items. Finally, other types of ref-
                 B                  A                                   erence points (i.e., prototypes, clusters) also lead to the same
            B                  A                                        performance. The critical items area are equally close to the
                                                                        category prototypes, as well as a variety of cluster configu-
                           A                                            rations. The GCM’s results therefore characterize a set of
                      A                                                 predictions made by reference point models more generally.
                                                                           Examining these predictions more methodically, we con-
                                                                        ducted a ‘grid-search’ to evaluate DIVA and GCM perfor-
Figure 1: The Diagonal classification. X and Y indicate crit-           mance under a range of parameter settings. At each point in
ical generalization items. Annotations illustrate equal city-           the search, the models were tested on classification of novel
block distance between the critical items and known category            items X & Y. To quantify each parameterization’s degree of
members.                                                                diagonal extension, we calculated the difference score in the
                                                                        Category A classification probability for the critical items on
                                                                        either side of the boundary (X −Y ). Positive difference scores
                     The Current Study                                  indicate systematic generalization of each class, and neutral
In a Diagonal classification, the two categories (‘A’ and ‘B’)          (≈ 0) scores indicate uniform generalization. Plotting these
are organized along a diagonal boundary (see Figure 1).                 scores as a density curve across points in the search (Figure 2)
This classification is notable for its likeness to Information-         reveals strongly systematic behavior: whereas DIVA nearly
Integration (Ashby & Maddox, 1990) and Condensation cat-                always generalized each class outward, the GCM produced
egories (Gottwald & Garner, 1972; Kruschke, 1993). The                  identical responses to X & Y under every parameter setting.
features are perfectly correlated within each category, but the            In what follows, we report a behavioral study testing the
training exemplars are isolated in one region of the stimulus           predictions made by DIVA and the GCM on generalization of
space, allowing for generalization items that follow the diag-          the Diagonal classification. If human learners represent cate-
onal boundary (labeled X & Y in Figure 1).                              gories solely in terms of reference points, then generalization
   Our DIVA simulations revealed that the model typically ex-           should be uniform: participants should be no more likely to
tends the diagonal boundary to these items – exemplars on the           produce a Category A response to items on the A-side of the
Category A side of the extended boundary are more likely to             diagonal than on the opposite side (i.e., X ≈ Y ). However, if
be classified as members of Category A than exemplars on the            participants acquire knowledge about how the features relate
other side, and vice-versa (i.e., A → X, B → Y ). DIVA’s per-           to one another within each category (as predicted by DIVA),
                                                                     2376

                1                                                      included (intermixed). Feedback was not provided during the
                                                   DIVA                generalization phase. Participants were informed that there
               0.9
                                                   GCM                 would be test trials prior to beginning the experiment.
               0.8
                                                                       Results. By the end of the training phase, most participants
               0.7                                                     had successfully mastered the categories. On average, partici-
               0.6                                                     pants were 89.2% (SE = 2.5) accurate during the last training
     Density
                                                                       block. Only two of 30 participants failed to reach greater than
               0.5                                                     6/8 correct during the final training block. Aggregate training
               0.4                                                     data is depicted in the left panel of Figure 4.
                              Behavioral                                  As a test of whether learners extended the diagonal bound-
               0.3
                                                                       ary, we compared the average number of ‘A’ classification
               0.2                                                     responses made to critical items X & Y (Figure 4). Learners
               0.1                                                     were more likely to produce an ‘A’ response to X than to Y,
                                                                       t(29) = 5.02, p < 0.001, d = 0.56. We then compared the
                0                                                      difference score we obtained behaviorally (X −Y ) to our ear-
                −0.2      0      0.2    0.4    0.6        0.8
                                                                       lier results with DIVA and the GCM: as shown in Figure 2,
                          Difference Score (X −Y )                     the difference score we observed cannot be produced by the
                                                                       GCM, but is fully explained by DIVA. Aggregate generaliza-
Figure 2: ‘Grid search’ simulations with DIVA and the GCM.             tion data is depicted in Figure 5.
Densities in this plot reflect predictions about the classifica-
                                                                       Summary
tion of critical items, across many different parameter set-
tings. Positive scores indicate that each category was gener-          We provided classification training to human learners on a
alized to the items on its side of the boundary, A → X, B → Y .        Diagonal classification (Figure 1). Learners systematically
                                                                       extended each category to novel items that are equidistant
                                                                       to known exemplars from both categories. The generaliza-
then we should observe systematic generalization of the diag-          tion we observed can be considered evidence that learners ac-
onal boundary outward (i.e., A → X, B → Y ).                           quire knowledge about feature correlations, and they apply
Participants and Materials. 30 undergraduates from                     that knowledge during generalization. Our results are also
Binghamton University participated in fulfillment of a course          inconsistent with the notion that category knowledge solely
requirement. Stimuli were squares varying in shading and               consists of exemplar, prototype, or cluster reference points.
size (see Figure 3). The separability of these features justifies         Our results are concisely explained by the DIVA model.
a city-block metric (Garner, 1974). An independent scaling             Because DIVA’s is principally autoassociative, the model re-
study verified that the dimensions are nearly equal in percep-         lies on within-class internal regularities (such as feature cor-
tual salience. The assignment between perceptual and con-              relations) to support reconstruction learning. In our above
ceptual dimensions was counterbalanced across participants.            simulations, we found that the model frequently generalized
                                                                       the diagonal boundary outward, just was we observed in the
                                                                       behavioral study. However, it is customary to assess models
                                                                       using a post-hoc parameter fitting process. In the next section,
                                                                       we describe a more formal examination of the performance
                                                                       by DIVA and the GCM.
                                                                                               Simulations
                                                                       The overall goal of the following simulations is to formally
                                                                       evaluate the performance of DIVA and the GCM in terms of
                       Figure 3: Sample stimuli.
                                                                       quantitative fit to our observed generalization behavior. Be-
                                                                       fore proceeding, it is worth noting that DIVA and the GCM
Procedure. Participants completed 96 training trials (12               are not fully comparable: unlike the GCM, DIVA’s category
blocks consisting of the 8 training examples). On each trial,          representations are constrained via back-propagation learn-
a stimulus was presented on a computer screen and learners             ing (Rumelhart et al., 1986), and its performance is stochas-
were prompted to make a classification decision by clicking            tic. However, model performance on generalization testing
one of two buttons (labeled ‘Alpha’ and ‘Beta’). After select-         can still be compared to assess whether our results can be ex-
ing a class label, learners were given feedback on their re-           plained under a reference point scheme.
sponse. Following the training phase, participants completed              We used parameter optimization techniques to find each
81 generalization trials consisting of items sampled at 9 po-          model’s best fit to the observed generalization data using of
sitions on each dimension. All of the training examples were           a mean-squared error (MSE) metric. We used a hill climb-
                                                                    2377

                  1                                                                                      1
                 0.9                                                                                    0.9
                                                                          Probability of ‘A’ Response
                 0.8                                                                                    0.8
                 0.7                                                                                    0.7
                 0.6                                                                                    0.6
      Accuracy
                 0.5                                                                                    0.5
                 0.4                                                                                    0.4
                 0.3                                                                                    0.3
                 0.2                                                                                    0.2
                 0.1                                                                                    0.1
                  0                                                                                      0
                       2   4        6     8     10    12                                                      X            Y
                               Training Block
  Figure 4: Left: Behavioral training accuracy. Right: Behavioral responses to critical items X & Y. Error bars reflect ±1 SE.
ing procedure to fit the GCM over four parameters: exemplar         exemplars, the category’s central tendency, or the central ten-
specificity (c), response determinism (γ; Nosofsky & Zaki,          dency of select clusters of exemplars. Instead, the location
2002), and attention strengths for features 1 and 2 (W1 and         of reference points may discriminatively reflect the proxim-
W2 ). DIVA’s behavior is stochastic, however, which precludes       ity of opposite-category members. In doing so, this model
the use of hill-climbing. As a result, we search for DIVA’s         may provide a account of the effects of contrast categories on
best fit using a ‘grid-search’ technique to generate predictions    conceptual representation (Davis & Love, 2010; Levering &
along a range of settings for its four parameters: number of        Kurtz, 2006; Palmeri & Nosofsky, 2001).
hidden units, learning rate, initial weight range, and a focus-        Overall, the design of the discriminative clustering model
ing parameter, β (Kurtz, 2015). At each point, DIVA was ini-        is similar to SUSTAIN (Love et al., 2004): the model’s refer-
tialized 2000 times with random small-valued weights and a          ence point representation consists of a collection of clusters,
random presentation sequence. The model used logistic hid-          and classification is based on each category’s association the
den units and linear outputs.                                       clusters. The model begins training with no internal repre-
   Overall, DIVA was able to provide a stronger fit to the full     sentation, and recruits clusters in when it makes a poor clas-
behavioral gradient than the GCM. DIVA’s best fit (MSE =            sification decision. The primary departure from SUSTAIN
0.006) was achieved with 3 hidden units, learning rate =            concerns the localization of the clusters: on each trial, clus-
0.55, weight range ±0.5, and β = 3. The GCM’s best fit              ters belonging to the correct category are moved toward the
(MSE = 0.0075) was achieved with c = 5.304, γ = 1.055,              presented exemplar, and clusters belonging to incorrect cate-
W1 = 0.507, W2 = 0.493. Model performance was further               gories are move away from the presented exemplar. By allow-
differentiated the responses to the critical items (rather than     ing the clusters to move discriminatively from members of the
the entire gradient): DIVA achieved MSE = 0.0037, and the           opposite category, they become more similar to critical items
GCM achieved MSE = 0.02. Beyond quantitative fitting,               on the category’s side of the diagonal boundary, producing
however, it is important to acknowledge that DIVA’s predic-         the observed pattern of generalization. Sample performance
tions match the qualitative patterns of observed performance        from this model is depicted in Figure 6.
– the model extends each category to novel items along the
diagonal boundary. Conversely, the GCM’s generalization is                                                    Discussion
completely neutral for these items. Each model’s best perfor-       The reference point framework has dominated research and
mance is depicted in Figure 5.                                      theory in category learning for the past 30 years. Reference
                                                                    point representations are useful in that they specify how re-
A discriminative clustering account.                                gions of space are associated with class labels, but they do
Although traditional reference point models are unable to ex-       not encode information about global or within-class regulari-
plain our results, we have recently implemented a novel dis-        ties, such as feature correlations.
criminative clustering account that successfully captures the          We reported an experiment suggesting that human learn-
observed generalization performance. The advance made by            ers acquire knowledge of within-class feature correlations.
this account lies in the realization that the reference points      Specifically, after training on a Diagonal classification, par-
associated with each category need not be localized as the          ticipants systematically generalized in accord with each cat-
                                                                 2378

                   Behavioral                                  DIVA                                       GCM
            Figure 5: Left: Behavioral generalization. Center & Right: Best-fit predictions from DIVA and the GCM.
egory’s internal structure to exemplars that are equally sim-         tion (Cohen, Nosofsky, & Zaki, 2001; E. E. Smith & Sloman,
ilar to members of both categories. The observed perfor-              1994). Finally, our recent work (Conaway & Kurtz, 2015) has
mance is inconsistent with prominent reference point models           uncovered unique generalization behavior that cannot be ex-
(e.g., Kruschke, 1992; Love et al., 2004; Nosofsky, 1984;             plained via similarity to reference points. Taken as a whole,
J. D. Smith & Minda, 2000), and indicates that learners               these findings raise a substantive challenge to theories of hu-
may acquire knowledge about categories that cannot be rep-            man category learning based on similarity to reference points.
resented under a reference point scheme. The Divergent
Autoencoder model (DIVA; Kurtz, 2007, 2015) succinctly ex-                                 Acknowledgments
plains the observed performance: because DIVA is chiefly an           We thank Sarah Laszlo and Gregory Murphy for helpful com-
autoassociator, the model depends on within-class regulari-           ments on this work. We additionally thank Sarah Laszlo for
ties (such as feature correlations) to aid in feature prediction.     access to her server. We also thank the members of the Learn-
Accordingly, DIVA’s generalization shows strong sensitivity           ing and Representation in Cognition (LaRC) lab at Bingham-
to the within-category feature correlations.                          ton University, as well as the BU Modeling meeting.
   Although existing reference point models fail to match
our results, we introduced a novel discriminative clustering                                    References
model capable of producing the observed generalization. De-           Anderson, J. R., & Fincham, J. M. (1996). Categorization
velopment of this model is ongoing, though its results here              and sensitivity to correlation. Journal of experimental psy-
show promise. Unlike existing cluster-based approaches (i.e.,            chology: Learning, Memory, and Cognition, 22(2), 259.
Love et al., 2004), the cluster locations in our model are op-        Ashby, F. G., & Maddox, W. T. (1990). Integrating informa-
timized both for similarity to same-category exemplars, and              tion from separable psychological dimensions. Journal of
for dissimilarity to opposite-category exemplars. Thus, the              Experimental Psychology: Human Perception and Perfor-
model’s clusters are gradually moved outward in the stimulus             mance, 16, 598–612.
space, taking on the value of a category ideal. From this loca-       Cohen, A. L., Nosofsky, R. M., & Zaki, S. R. (2001). Cate-
tion, clusters are more similar to novel items on their side of          gory variability, exemplar similarity, and perceptual classi-
the diagonal boundary, affording generalization of each class.           fication. Memory & Cognition, 29(8), 1165–1175.
Future work will attempt to dissociate the predictions made           Conaway, N. B., & Kurtz, K. J. (2015). A dissociation
by DIVA and the discriminative clustering model.                         between categorization and similarity to exemplars. In
   This report adds to an accumulating body of evidence                  D. C. Noelle & R. Dale (Eds.), Proceedings of the 37th an-
against the idea that category learners solely acquire knowl-            nual conference of the cognitive science society (pp. 435–
edge in the form of reference points. Research in function               440). Austin, TX: Cognitive Science Society.
learning (DeLosh, Busemeyer, & McDaniel, 1997) has, for               Davis, T., & Love, B. C. (2010). Memory for category in-
example, demonstrated key flaws in the account of extrap-                formation is idealized through contrast with competing op-
olation put forward by reference point similarity – studies              tions. Psychological Science, 21(2), 234–242.
on on rule-based generalization have revealed similar flaws           DeLosh, E. L., Busemeyer, J. R., & McDaniel, M. A. (1997).
in a category learning context (e.g., Erickson & Kruschke,               Extrapolation: the sine qua non for abstraction in function
2002). Traditional reference point approaches are also unable            learning. Journal of Experimental Psychology: Learning,
to explain the effects of category variability on generaliza-            Memory and Cognition, 23, 968–986.
                                                                  2379

                                    Trial 8                                             Post-training
Figure 6: Sample performance from the discriminative clustering model. Learned clusters are labeled using large, black and
white ‘A’ and ‘B’ characters. The model is capable of learning multiple clusters, but a single-cluster solution is typical for this
classification. Left: generalization at the beginning of Block #2. Right: generalization at the end of the training phase.
Erickson, M. A., & Kruschke, J. K. (2002). Rule-based ex-           Medin, D. L., Altom, M. W., Edelson, S. M., & Freko, D.
  trapolation in perceptual categorization. Psychonomic Bul-          (1982). Correlated symptoms and simulated medical clas-
  letin & Review, 9(1), 160–168.                                      sification. Journal of Experimental Psychology: Learning,
Garner, W. R. (1974). The processing of information and               Memory, and Cognition, 8(1), 37.
  structure. Potomac, MD: Erlbaum.                                  Medin, D. L., & Schaffer, M. M. (1978). A context theory
Gottwald, R. L., & Garner, W. R. (1972). Effects of focusing          of classification learning. Psychological Review, 85, 207–
  strategy on speeded classification with grouping, filtering         238.
  and condensation tasks. Perception and Psychophysics, 11,         Murphy, G. L. (2002). The big book of concepts. Cambridge:
  179-182.                                                            MIT press.
Kruschke, J. K. (1992). ALCOVE: An exemplar-based con-              Murphy, G. L., & Wisniewski, E. J. (1989). Feature correla-
  nectionist model of category learning. Psychological Re-            tions in conceptual representations. Advances in cognitive
  view, 99(1), 22–44.                                                 science, 2, 23–45.
Kruschke, J. K. (1993). Human category learning: Impli-             Nosofsky, R. M. (1984). Choice, similarity, and the context
  cations for backpropagation models. Connection Science,             theory of classification. Journal of Experimental Psychol-
  5(1), 3–36.                                                         ogy: Learning, Memory and Cognition, 10(1), 104–114.
Kurtz, K. J. (2007). The divergent autoencoder (DIVA) model         Nosofsky, R. M., & Zaki, S. R. (2002). Exemplar and proto-
  of category learning. Psychonomic Bulletin and Review,              type models revisited: response strategies, selective atten-
  14, 560–576.                                                        tion, and stimulus generalization. Journal of Experimental
Kurtz, K. J. (2015). Human category learning: Toward a                Psychology: Learning, memory, and cognition, 28(5), 924.
  broader explanatory account. Psychology of Learning and           Palmeri, T. J., & Nosofsky, R. M. (2001). Central tendencies,
  Motivation, 63, 77–114.                                             extreme points, and prototype enhancement effects in ill-
Lee, M. D., & Navarro, D. J. (2002). Extending the ALCOVE             defined perceptual categorization. The Quarterly Journal
  model of category learning to featural stimulus domains.            of Experimental Psychology: Section A, 54(1), 197–235.
  Psychonomic Bulletin and Review, 9, 43–58.                        Pothos, E. M., & Wills, A. J. (2011). Formal approaches in
Levering, K., & Kurtz, K. J. (2006). The influence of learn-          categorization. Cambridge University Press.
  ing to distinguish categories on graded structure. In R. Sun      Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986).
  & N. Miyake (Eds.), Proceedings of the 28th annual con-             Learning representations by back-propagating errors. Na-
  ference of the cognitive science society (pp. 1681–1686).           ture, 323, 533–536.
  Mahwah, NJ: Erlbaum.                                              Smith, E. E., & Sloman, S. A. (1994). Similarity-versus rule-
Love, B. C., Medin, D. L., & Gureckis, T. M. (2004). SUS-             based categorization. Memory & Cognition, 22(4), 377–
  TAIN: A network model of category learning. Psychologi-             386.
  cal Review, 111, 309–332.                                         Smith, J. D., & Minda, J. P. (2000). Thirty categorization
Malt, B. C., & Smith, E. E. (1984). Correlated properties in          results in search of a model. Journal of Experimental Psy-
  natural categories. Journal of verbal learning and verbal           chology: Learning, Memory, and Cognition, 26(1), 3.
  behavior, 23(2), 250–269.
                                                               2380

