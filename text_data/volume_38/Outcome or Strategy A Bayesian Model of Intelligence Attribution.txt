              Outcome or Strategy? A Bayesian Model of Intelligence Attribution
                Marta Kryven (mkryven@uwaterloo.ca)                              Tomer Ullman (tomeru@mit.edu)
          Department of Computer Science, University of Waterloo             Department of Brain and Cognitive Sciences
                                                                                 Massachusetts Institute of Technology
            William Cowan (wmcowan@cgl.uwaterloo.ca)                          Joshua B. Tenenbaum (jbt@mit.edu)
                        Department of Computer Science                       Department of Brain and Cognitive Sciences
                               University of Waterloo                            Massachusetts Institute of Technology
                                Abstract                                  and ‘Is this the best way to achieve that goal?’. The partic-
    People have a common-sense notion of intelligence and use it          ular end-goal might even be negative, but the behavior could
    to evaluate decisions and decision-makers. One can attribute          still be conceded as intelligent and thus preferable. Children
    intelligence by evaluating the strategy or the outcome of a           as young as two understand and value competence, preferring
    goal-directed agent. We propose a model of intelligence at-
    tribution, based on inverse planning in Partially Observable          agents who can perform an action on the first attempt, even
    Markov Decision Processes (POMDPs) in a probabilistic envi-           if the agent is mean (Jara-Ettinger, Tenenbaum, & Schulz,
    ronment, inferring the most likely planning parameters given          2015).
    observed actions. The model explains the agent’s decisions
    by a combination of probabilistic planning, a softmax decision           However, fully evaluating an agent’s planning procedure
    noise, prior knowledge about the world and forgetting, estimat-       can be computationally hard, and observing an agent’s out-
    ing the agent’s intelligence by a proxy measure of efficiently        come can provide a shortcut to evaluating its intelligence.
    optimising costs and rewards. Behavioural evidence from two
    experiments shows that people cluster into those who attribute        In effect, the observer might think ‘If they achieved their
    intelligence to the strategy and those who attribute intelligence     goal, they must be smart’, even though the goal was achieved
    to the outcome of the observed actions. People in the strat-          by sheer luck or prior knowledge. Adults under conditions
    egy cluster attribute more intelligence to decisions that min-
    imise the agent’s overall cost, even if the outcome is unlucky.       of cognitive load (Olson et al., 2013) and 7-year old chil-
    People in the outcome cluster attribute intelligence to the out-      dren perceive lucky persons as more likable (Olson, Banaji,
    come, judging low-cost outcomes as a sign of intelligence even        Dweck, & Spelke, 2006; Olson, Dunham, Dweck, Spelke, &
    if the outcome is accidental and make neutral judgements be-
    fore they observe the result. Our model explains human in-            Banaji, 2008), which suggests that lucky others may also be
    telligence judgements better than perceptual cues such as the         seen as more intelligent.
    number of revisits or moves.                                             In this paper we develop a formal model of intelligence at-
    Keywords: Theory of mind; Intelligence attribution; Social            tribution based on Bayesian inverse planning (Baker, Saxe, &
    cognition; Bayesian inference; Partially Observable Markov
    Decision Processes; Inverse planning                                  Tenenbaum, 2011). We describe two behavioral experiments
                                                                          that validate the model, considering the role of rational ex-
                            Introduction                                  pectations and mental short-cuts. The next section gives an
In everyday life people make fast, intuitive judgements of in-            informal sketch of the model, followed by formal modelling
telligence. For example, it is more intelligent to rush to catch          and behavioral experiments to evaluate the model.
an infrequent bus than to run towards a subway that departs
every 3 minutes, and solving a puzzle from scratch is more                              Computational Framework
intelligent than looking up the answer in a key. Existing qual-           To describe intelligent behaviour formally, consider a simple
itative accounts of intelligence explain in general terms why             context in which it can be observed: a 2-D animation of ge-
people might describe actions as intelligent or unintelligent.            ometric shapes. Most adults describe the actions of agents
According to Dennett’s rationality principle, people expect               in such displays by constructing a narrative about the actors’
intentional agents to act efficiently in order to achieve their           mental states (Heider & Simmel, 1944). Such 2-D anima-
desires, given their beliefs about the world (Dennett, 1989).             tions were previously used to inform computational models
People may describe behaviour as intelligent if it agrees with            of goal attribution (Baker et al., 2011), preference attribu-
their expectations of what the agent should do, and as stupid             tion (Jara-Ettinger, Baker, & Tenenbaum, 2012) and social
if the behavior can be explained by a failure of attention, over-         behaviour (Ullman et al., 2009). Building on these studies
confidence or loss of control (Aczel, Palfi, & Kekecs, 2015).             we propose a model of intelligence attribution in the context
    One way to attribute intelligence is to judge the agent’s             of a rational agent looking for an object in a 2-D world.
efficiency in achieving a goal, echoing the rationality princi-              Consider an agent (for concreteness, a mouse) looking for a
ple. Efficiency means achieving a rewarding goal at a min-                goal (a treat) in a maze. The mouse is familiar with the layout
imal cost. So, an observer attributing intelligence needs to              of the maze: it knows which rooms are big and which are
understand the costs and rewards of a situation and how to                small, it knows which rooms are close and easily accessible
plan under uncertainty to maximise the likelihood of success.             and which are far away. The mouse knows there is a treat in
Evaluating the specific action of an agent seeking a goal re-             the maze, but does not know where it is. The treat is equally
quires the observer to ask ‘Is this the best goal for that agent?’        likely to be anywhere. What should an intelligent mouse do
                                                                      1649

to find the treat? It should plan efficiently. And when seeing                          Table 1: Agent optimality rank
the mouse running through the maze, how does one know if
it is intelligent? By comparing its behavior to the behaviour          Rank      Description
expected of an agent that plans efficiently.                           6         Optimal
    We formalise a rational agent’s decision-making process as         5         Suboptimal with decision noise or
probabilistic planning in a Partially Observable Markov De-                      Suboptimal or with prior knowledge
cision Processes (POMDP). POMDP assumes that the agent                 4         Suboptimal with decision noise and prior knowledge
acts sequentially to maximise the reward and minimise the              3         Suboptimal with decision noise and forgetting
cost of each action, given its beliefs about the world (Fig-           2         Suboptimal with prior knowledge,
ure 1). After each action the agent updates its beliefs based                    forgetting, and decision noise
on observations caused by the previously chosen action.                1         Suboptimal and seemingly random
                                                                     The Gridworld and POMDP in Detail
                                                                     Formally, a model world (a maze) is described by dis-
                                                                     crete time, 0 ≤ t ≤ T , and a grid of cells, W = {w(i, j)},
                                                                     w(i, j) ∈ {wall, empty, goal}. A single cell contains a goal:
                                                                     ∃(ig , jg ) → w(ig , jg ) = goal. The agent acts based on its belief
                                                                     about the world, which is a set of probabilities Xt = {P(xs )t }.
                                                                     Here X = {xs } is the set of all possible world states such that
                                                                     xs represents a world with the goal in cell s. X0 encodes the
Figure 1: (a) POMDP is a sequential process described by             set of the agent’s initial beliefs.
beliefs Xt , observations Ot , rewards Rt and actions At at time        The agent knows its own location at time t, Lt and sees
step t. Arrows indicate a causal relationship.                       1800 of visual field. The results are observation probabilities
                                                                     Ot = P(W |Xt , Lt ). A cell s is visible if the four rays cast from
    A viewer observes decisions made by an agent and judges          each of the corners of Lt to the corresponding corners of cell
the agent’s intelligence by inferring the parameters guiding         s do not intersect walls. If s is visible then Ot (s) = 1, other-
the agent’s planning. The observer estimates the agent’s in-         wise Ot (s) = 0. Unseen cells are portrayed as dark, and cells
telligence by a measure of agent efficiency, so that the more        previously seen are patterned (Figure 2). The agent moves
optimal agents are judged as more intelligent. We define the         one grid cell at a time, choosing among available determinis-
agent optimality rank (Table 1) based on how well the cor-           tic actions A(Lt ) = {ai } ∈ {N, S,W, E}. An action is available
responding planing strategies optimise the long-term costs           if it does not lead into a wall. The agent updates its beliefs
and rewards over randomly-generated mazes with a randomly            contingent on observations using standard Bayesian updating:
placed goal. Thus, this optimality rank is defined for the           Xt+1 = P(W |At , Xt ) ∝ P(Ot |At , Xt )P(At |Xt )P(W )
model problem and not as a universal cost-minimising strat-
                                                                     Calculating Rewards
egy. Prior knowledge indicates a non-uniformly distributed
prior belief about the goal’s location. Since the agent sup-         At time t, the agent calculates the value of each action
posedly does not know where the goal is, we use such prior           Q(a, Lt , Xt ) and the reward function R : Xt × At 7→ R and
knowledge (with correct information) to represent luck. De-          chooses an action At with a likelihood proportional to its re-
cision noise is captured by a so f tmax parameter, set such that     ward. Strictly optimal planning in POMDPs is computation-
actions are chosen with a probability proportional to their re-      ally intractable, and in our example the solution is approxi-
ward. To model the observer, we use Bayesian inverse infer-          mated by one step lookahead, or by direct design, on an as-
ence on the POMDP by integrating the likelihood of the ob-           sumption that the space of beliefs remains unchanged after
served actions with the prior over a set of possible POMDP           taking one action (Hauskrecht, 2000):
parameters (Baker et al., 2011).
    In principle, this model can describe an evaluation of any                Q(a, Lt , Xt ) = ∑ P(xs )t ρ(xs , Lt + a)
                                                                                                xs                                    (1)
behaviour encoded in discrete time and space and can admit
                                                                                                +γ     max {Q(ai , Lt + a, Xt )},
a variety of cost functions, reward functions and discount rate                                    ai ∈A(Lt +a)
models. For concreteness, we limit the planning part of the
model to three possible reasons for deviating from an opti-          where γ is a discount factor, and ρ is proportional to the
mal search strategy: prior knowledge about the world, deci-          square of the distance traveled to reach each cell:
                                                                                                1
sion noise, and forgetting. While this model is not meant to         ρ(xs , Lt + a) ∝ ||(L +a)−(i, j)||2
                                                                                                         .
                                                                                            t
fully capture the complexities of human intelligence attribu-           Thus, ρ(xs , Lt + a) represents the value of being in a loca-
tion, it provides a computational ideal-observer benchmark to        tion Lt + a in the world xs and ∑xs P(xs )t ρ(xs , Lt + a) is the
test against experimental data and perceptual-based metrics.         value of an action a given current beliefs. The second term
                                                                 1650

in equation (1) describes the discounted value of subsequent          times and the control condition occurred twice. The full set
actions, assuming actions are chosen optimally.                       of stimuli used in this and the following experiment can be
   Finally, the reward function is defined as:                        downloaded from http://cgl.uwaterloo.ca/˜mkryven/
                                  exp(Qt (ai )/τ)
                   R(Qt (ai )) =                     ,        (2)
                                 ∑ j exp(Qt (a j )/τ)
where τ is a so f tmax decision noise. As τ → 0 the agent
deterministically chooses the action with the highest value,
and as τ → ∞ the agent acts at random. For our experiments
we either set τ = 0 (optimal) or used a low level of τ = 0.01
such that actions were chosen with a probability proportional
to Q(ai , Lt , Xt ) but with most actions optimal (decision noise
conditions). A forgetting parameter 0 ≤ f ≤ 1 regresses the
agent’s beliefs toward the mean after each step so that the
agent gradually forgets whether a previously observed cell is
empty and must re-check already visited locations. Further-
more, prior knowledge about the world is used to to simulate
luck: an agent with correct prior knowledge finds the goal
sooner.                                                               Figure 2: An example of an optimal-lucky condition. The
                                                                      agent makes an (optimal) decision to go into the room on
                           Experiments                                the left (reader’s perspective). Incidentally, it finds the goal
Experiment 1                                                          quickly. Dark squares indicate that the agent has not yet seen
The first experiment tests whether people attribute intelli-          the area.
gence differently to optimal and to suboptimal actions.
Participants. 12 participants recruited from the Univer-              Method. Each participant read the instructions on the com-
sity of Waterloo, 4 females and 8 males, median age 27.5.             puter screen and viewed four familiarisation examples fol-
Both experiments received ethics clearance from a University          lowed by the 30 stimuli in two blocks. After viewing each
of Waterloo Research Ethics Committee and from an MIT                 movie the participant recorded his or her rating into a pro-
Ethics Review Board.                                                  vided Likert answer sheet on a scale from 1 (less intelligent)
                                                                      to 5 (more intelligent).
Stimuli. 30 animated movies of a mouse looking for food in
a maze were shown in two blocks on a computer screen using            Results. There was no main effect of block. A two fac-
Psychtoolbox (Brainard, 1997). The movies were computer-              tor ANOVA of rating for participant × condition shows a
generated by solving a POMDP on one of 9 mazes with two               main effect of participant (mean rating 3.44, standard devi-
levels of forgetting (on, off), prior knowledge (a correct prior,     ation 0.46, p < .0001) and of condition (p < .0001, Table
or a uniformly distributed prior belief) and decision noise (on,      2). There was no significant difference between the optimal-
off). We varied the appearance of the mouse, the layout of the        lucky and optimal-unlucky conditions (p = 0.99) or between
maze, the textures of the maze and the POMDP parameters               the optimal-lucky, optimal-unlucky and decision noise condi-
on each trial. In every movie the mouse found the treat. The          tions (p = .08) indicating that participants did not penalise oc-
location of the food was counterbalanced so that in one half of       casional inefficient moves. However, the difference between
the mazes the mouse could find it equally quickly by optimal          the optimal-lucky, optimal-unlucky and the prior knowledge
planning or by luck. In another half of the mazes, the goal was       conditions (p = .001, difference = 0.8) indicates that partici-
placed so that an optimal agent had to search exhaustively,           pants attributed intelligence to the agent’s strategy more than
while a lucky agent with prior knowledge could get finish in          to the agent’s outcome.
fewer steps.                                                             According to a Scheffe Post-Hoc test, suboptimal condi-
   Each movie was labeled according to the most likely                tions were rated differently depending on the cause inferred
POMDP settings estimated by the inverse-planning infer-               by the model. Thus, the decision noise condition was rated
ence. A movie was labelled optimal-lucky if the opti-                 higher than noise-forgetting (p < .0001, difference = 1) or
mal planner and the prior knowledge planner were equally              noise-forgetting-prior (p < .0001, difference = 2), and prior
likely and optimal-unlucky if the optimal planner was most            knowledge condition was rated higher than noise-forgetting
likely. Labels prior knowledge, decision noise, noise-                (p = .0002, difference = 0.9) or noise-forgetting-prior (p <
forgetting, noise-forgetting-prior and noise-prior accordingly        .0001, difference = 1.8). The suboptimal-control was rated
represented combinations of parameters. A control condi-              lower than all other conditions (p < .0001). Pearson corre-
tion suboptimal-control showed an inefficient, highly forget-         lations between the agent optimality rank and human rating
ful and random agent. Each of the conditions occurred four            is .73 and regression of optimality and rating (r2 = 53.7%),
                                                                  1651

showing a good fit of our model to the data.                        time? We used the same mouse in a maze scenario as in Ex-
                                                                    periment 1, except that on half of the trials the movie stopped
Table 2: Mean ratings and Std. err. of the mean by condition        after the mouse chose one of the rooms but before the treat
                                                                    was found. In the other half of the trials the movie played
            Condition                 Rating   SEM                  until the mouse found the treat.
            optimal-lucky               4.7    0.11                 Participants. 32 participants were recruited via Amazon
            optimal-unlucky             4.6    0.11                 Mechanical Turk, 2 were discarded for failing to answer ques-
            decision noise              4.1    0.11                 tions. The analysis thus included 30 participants (11 females,
            prior knowledge             3.9    0.11                 median age 34).
            noise-prior                 3.5    0.11
            noise-forgetting             3     0.11                 Stimuli. 40 animated movies were generated by solving
            noise-forgetting-prior      2.1    0.11                 POMDP on 8 different mazes. We varied the appearance of
            suboptimal-control          1.6    0.15                 the mouse, the layout, textures and orientation of the maze,
                                                                    the location of the goal, and the POMDP settings on each
                                                                    trial. There were 19 complete trials, with each condition
   The agent’s strategy can thus help explain human attribu-
                                                                    (optimal-unlucky, prior knowledge, decision noise and de-
tions of intelligence. But can perceptual cues, the path length
                                                                    cision noise with prior knowledge) occurring 4 times and
or the number of cells revisits, explain it equally well? As-
                                                                    optimal-lucky occurring three times. Another 19 trials were
suming that a move is scored either for moving from one
                                                                    incomplete, 12 showing a suboptimal decision (suboptimal-
cell to another or turning, ANOVA for participant × moves ×
                                                                    incomplete condition ) and 7 showing an optimal decision
revisits, shows significant effects of participant (p < .0001),
                                                                    (optimal-incomplete condition). Two control trials showed
moves (p = .001) and revisits (p < .0001). Table 3 shows
                                                                    highly forgetful, inefficient mice.
mean ratings for different averaged levels of moves and of
revisits, where move levels were obtained by splitting the tri-     Method. Participants read the instructions on a computer
als into four bins of equal size and calculating the average        screen in a web browser and viewed 4 familiarisation ex-
number of moves per bin. Pearson correlations between the           amples followed by the 40 animated movies shown in two
number of moves and ratings, −.42, number of revisits and           blocks. After viewing each movie, the participant selected
ratings, −.49, and multiple regression of moves, revisits and       a rating from a Likert scale between 1 (less intelligent) to 5
rating (r2 = 31.5%), calculated over individual trials, show        (more intelligent). At the end of the Web survey participants
that our model provides a closer fit to the data than the per-      were asked: How did you make your decision?
ceptual metrics alone.
   In summary, participants did not attribute more intelligence     Table 4: Mean ratings and Std. err. of the mean by condition
to lucky agents, and judged agents with an efficient decision-
making strategy to be more intelligent. The participants at-                   Condition                  Rating     SEM
tributed the highest intelligence to the approximately rational                optimal-lucky                4.5      0.08
agents, which either chose optimally or deviated from the op-                  prior knowledge              4.3      0.07
timal decision within a margin explainable by a softmax de-                    optimal-unlucky              4.1      0.07
cision noise. Moreover, the inferred cause of sub-optimality                   noise-prior                  3.9      0.07
matters: random agents are judged least intelligent of all, and                decision noise               3.8      0.07
forgetful agents as less intelligent than agents with decision                 optimal-incomplete           3.5      0.06
noise.                                                                         suboptimal-incomplete        3.3      0.05
                                                                               control                      1.2      0.1
Table 3: Mean ratings and Std. err. of the mean for number
of moves and revisits
                                                                    Results. There was no main effect of block. A two-factor
   Moves     Rating    SEM         Revisits   Rating    SEM         ANOVA of rating for participant × condition shows a main
   14        4         0.76        0          3.9       0.55        effect of participant (p < .0001, mean 3.63, standard devia-
   19        3.2       0.14        2          4.2       0.27        tion 0.45) and a main effect of condition (p < .0001, Table 4).
   23        3.1       0.38        4          3.6       0.18        Main effects of age (p = .02) and gender (p = .0006) indicate
   31        3.7       0.37        9          2.2       0.25        that older participants (difference between groups split by the
                                                                    median age 0.24; median ages 26 and 43) and females (differ-
                                                                    ence 0.3) were more generous. ANOVA of rating over incom-
Experiment 2                                                        plete conditions for participant × condition shows a small
In the second experiment we investigated how do people form         difference between intelligence attributed to optimal and sub-
a judgement of intelligence. May people decide after observ-        optimal decisions (difference = 0.17, p = .009).
ing just one decision, or do they accumulate evidence over             In agreement with Experiment 1, the control condition was
                                                                1652

judged as least intelligent (p < .0001). However, there was          Table 6: Correlations of ratings with perceptual features and
no difference between the optimal-lucky (p = .73), optimal-          with optimality over invividual trials
unlucky (p = .19) and the prior knowledge conditions. More-
over, optimal-lucky and optimal-unlucky agents were rated as           Metric                           Strategy Cluster  Outcome Cluster
different (p = .0.004, difference 0.47) indicating that online         Pearson, number of revisits            -.50        -.73
participants judged lucky and efficient agents equally, and op-        Pearson, number of moves               -.12        -.62
timal agents as more intelligent when they were lucky. Pear-           Pearson, Optimality                     .65        .40
son correlation between the agent optimality rank and ratings          r2 , revisits, moves                   35%         55%
is .55, regression of optimality and rating r2 = 34.6%.                r2 , optimality                       44.3%        21.8%
   The relationship between ratings and perceptual cues was
analysed only on full trials, as incomplete trials are always
shorter. ANOVA of rating for participant × moves × revisits
shows significant main effects of participants (p < .0001),
moves (p = .009) and revisits (p < .0001). The mean rat-
ings for number of moves and revisits are shown in Table
5. Pearson correlations between the number of moves and
ratings, −.31, and between number of revisits and ratings,
−.59. Multiple regression of moves, revisits and rating over
individual trials (r2 = 36.4%), suggests that the ratings in Ex-
periment 2 can be explained by the perceptual cues as well as
by the agent’s efficiency.
Table 5: Mean ratings and Std. err. of the mean for number
of moves and revisits
                                    Revisits   Rating    SEM         Figure 3: Comparing ratings between the two clusters OL-
   Moves     Rating    SEM                                           optimal-lucky, OU-optimal-unlucky, N-decision noise, P-
                                    0          4.1       0.13
   10        4.4       0.11                                          prior knowledge, NP-noise-prior, OP-optimal-incomplete,
                                    1.5        4.2       0.1
   20        3.6       0.11                                          SP-suboptimal-incomplete, C-control.
                                    3.5        3.8       0.07
   27        3.5       0.11
                                    7          2.1       0.09
                                                                     Table 7). Indeed, the participants’ answers to the verbal ques-
   Did the online participants in Experiment 2 prefer lucky          tion support this conclusion. We divided the participant an-
agents? To test the hypothesis that people may use more than         swers to ‘How did you make your decision?’ into two groups:
one way of attributing intelligence we used a k-means (Lloyd,        outcome and strategy. For example, we coded a response as
1982) and a Gaussian Mixture Model with a Bayesian Infor-            strategy if it said: ‘Based on if the mouse checked every nook
mation Criterion over a 3-dimensional space of correlations          and cranny.’ and as outcome if they said ‘Based on how long
of individual ratings with moves, revisits and optimality rank.      it took for the rat to find the treat’. Two independent raters
Two clusters were preferred over three or one. People in             agreed on 27 out of 30 participants, coding 8 of them as out-
the bigger cluster (19 participants) based their rating on the       come and 19 as strategy. The remaining 3 were randomly
agent’s strategy, and people in the smaller cluster relied on        assigned to either group. Importantly, the 8 participants inde-
perceptual cues.                                                     pendently agreed on as outcome by both raters (using verbal
   ANOVA of rating for participant × condition over each             measures) were also the 8 participants identified as belonging
cluster shows that our model fits the judgements of people           to the outcome cluster using K-Means clustering.
in the strategy cluster, but not in the outcome cluster (Table
6). Participants in the Outcome cluster rated optimal-lucky                                    Discussion
agents higher than to the optimal-unlucky ones (p < .0001,
difference = 1.54) and were indifferent between optimal and          Our model proposes a formal account of intelligence attribu-
suboptimal decisions on incomplete trials (p = .5). In con-          tion and an apparatus for generating quantitative predictions.
trast, people in the Strategy cluster were indifferent between       The model predicts that people perceive some suboptimal de-
optimal-lucky and optimal-unlucky agents (p = .92) and on            cisions as more intelligent than others, depending on the in-
incomplete trials rated optimal-incomplete agents higher than        ferred causes of suboptimal planning, in agreement with be-
the suboptimal-incomplete(p = .00002, difference 0.32).              havioural data.
   The discrepancy between model and data reveals two dif-              Participants were clustered into two groups: those in the
ferent styles of attributing intelligence: attributing intelli-      Strategy group attribute intelligence based on partial results,
gence to efficient strategies, or to shorter paths (Figure 3 and     while those in the Outcome group do not decide until they
                                                                 1653

Table 7: Mean ratings of conditions and Std. err. of the mean         mans understand as intelligent. Thus, to make better AI ap-
of people in the Strategy (S) and the Outcome (O) clusters            plications we need to measure intelligence in technical terms,
                                                                      and our model takes a step in that direction.
      Condition                    S    SEM     O     SEM
                                                                      Acknowledgements
      optimal-unlucky            4.5    0.08    3.3    0.11
      optimal-lucky              4.3    0.07    4.9    0.12           We thank the reviewers for valuable comments and feedback,
      decision noise               4    0.07    3.3    0.11           which helped us make this manuscript better. This work was
      prior knowledge              4    0.07    4.9    0.11           supported by the Center for Minds, Brains and Machines
      noise-prior                  4    0.07    4      0.11           (CBMM), under NSF STC award CCF-1231216.
      optimal-incomplete         3.6    0.04    3.2   0.006
                                                                                               References
      suboptimal-incomplete      3.3    0.04    3.3   0.006
      control                    1.1    0.1     1.3    0.14           Aczel, B., Palfi, B., & Kekecs, Z. (2015). What is stupid?
                                                                        people’s conception of unintelligent behavior. Intelligence,
                                                                        53, 51–58.
                                                                      Baker, C. L., Saxe, R. R., & Tenenbaum, J. B. (2011).
have seen the consequences1 . Both groups, however, see
                                                                        Bayesian theory of mind: Modeling joint belief-desire at-
random actions as less intelligent than those that can be ex-
                                                                        tribution. In Proceedings of the thirty-second annual con-
plained by causal inference.
                                                                        ference of the cognitive science society (pp. 2469–2474).
   Why did people differ in intelligence attribution? Our             Brainard, D. H. (1997). The psychophysics toolbox. Spatial
experiments used simple mazes that most adults can easily               vision, 10, 433–436.
solve. While people in the Strategy cluster expected the agent        Dennett, D. C. (1989). The intentional stance. MIT press.
to do what a human would do, people in the Outcome cluster            Gardner, H. (2011). Frames of mind: The theory of multiple
did not. According to Gardner’s theory of multiple intelli-             intelligences. Basic books.
gences (Gardner, 2011) as skills in different domains people          Hauskrecht, M. (2000). Value-function approximations for
in the Outcome cluster may attribute the agent’s luck to an             partially observable markov decision processes. Journal of
invisible skill (a sense of smell). Alternatively, people may           Artificial Intelligence Research, 33–94.
assume that lucky agents must be intelligent based on a belief        Heider, F., & Simmel, M. (1944). An experimental study of
in a just world (Lerner, 1980). The former implies that peo-            apparent behavior. The American Journal of Psychology,
ple themselves should act rationally when solving a maze,               243–259.
and the latter that people should decide randomly. We plan to         Jara-Ettinger, J., Baker, C. L., & Tenenbaum, J. B. (2012).
address this question in future work.                                   Learning what is where from social observations. In Pro-
   Another interesting avenue for future work is to investigate         ceedings of the thirty-fourth annual conference of the cog-
how children attribute intelligence. Although children recog-           nitive science society (pp. 515–520).
nise and value competence (Jara-Ettinger et al., 2015), chil-         Jara-Ettinger, J., Tenenbaum, J. B., & Schulz, L. E. (2015).
dren also prefer lucky agents (Olson et al., 2006, 2008). To            Not so innocent. toddlers inferences about costs and culpa-
our knowledge there are no studies evaluating children’s attri-         bility. Psychological Science, 26(5), 633–640.
bution of intelligence, and at what age the abilities to reason       Lerner, M. (1980). The belief in a just world; a fundamental
about outcome vs. strategy emerge.                                      delusion. New york: Plenum Press.
   One possible criticism of our specific implementation is           Lloyd, S. P. (1982). Least squares quantization in pcm. In-
that it encodes space as a grid of squares and makes a new              formation Theory, IEEE Transactions on, 28(2), 129–137.
decision each time the agent moves into a new square, which           Olson, K. R., Banaji, M. R., Dweck, C. S., & Spelke, E. S.
may not be an accurate representation of how people navi-               (2006). Children’s biased evaluations of lucky versus un-
gate. An alternative – encoding the maze as a weighted graph            lucky people and their social groups. Psychological Sci-
where each room is represented by a vertex – may be a better            ence, 17(10), 845–846.
approximation of how people represent space. In addition,             Olson, K. R., Dunham, Y., Dweck, C. S., Spelke, E. S., &
more fine-grained rationalistic explanations in terms of vari-          Banaji, M. R. (2008). Judgments of the lucky across de-
able costs and reward functions may be a better causal model            velopment and culture. Journal of Personality and Social
for actions currently attributed to decision noise.                     Psychology, 94(5), 757.
   The goal of current research in Artificial Intelligence (AI)       Olson, K. R., Heberlein, A. S., Kensinger, E., Burrows, C.,
is to create intelligent computer applications, such as self-           Dweck, C. S., Spelke, E. S., & Banaji, M. R. (2013). The
driving cars, automatic trading and intelligent energy-saving           role of forgetting in undermining good intentions. PLoS
appliances designed to take over routine human decision-                ONE.
making. Such applications must be not only be algorithmi-             Ullman, T., Baker, C., Macindoe, O., Evans, O., Goodman,
cally correct, but also must interact with people in a way hu-          N., & Tenenbaum, J. B. (2009). Help or hinder: Bayesian
    1 As Solon advised to Croesus, ‘Count no man happy until he is      models of social goal inference. In Advances in neural in-
dead’.                                                                  formation processing systems (pp. 1874–1882).
                                                                  1654

