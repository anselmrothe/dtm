UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Integrating Syntactic Knowledge into a Model of Cross-situational Word Learning
Permalink
https://escholarship.org/uc/item/8cx990jc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Alishahi, Afra
Fazly, Afsaneh
Publication Date
2010-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

     Integrating Syntactic Knowledge into a Model of Cross-situational
                                                  Word Learning
                              Afra Alishahi                                      Afsaneh Fazly
               Computational Linguistics and Phonetics               Computer Sciences and Engineering
                     Saarland University, Germany                            Shiraz University, Iran
                        afra@coli.uni-saarland.de                            fazly@cse.shirazu.ac.ir
                           Abstract                            Therefore, it is likely that they draw on their knowledge
                                                               of the structural regularities of language (and of lexical
   It has been suggested that children learn the meanings
   of words by observing the regularities across different     categories in particular) to facilitate word learning, es-
   situations in which a word is used. However, experi-        pecially in cases where cross-situational evidence is not
   mental studies show that children are also sensitive to     reliable. However, a coherent account of word learning
   the syntactic properties of words and their context at
   a young age, and can use this information to find the       that explains the interaction between these two informa-
   correct referent for novel words. We present a unified      tion sources is lacking. Also, despite the extensive body
   computational model of word learning which integrates       of experimental research on the role of syntactic knowl-
   cross-situational evidence with the accumulated seman-
   tic properties of the lexical categories of words. Our      edge in semantics acquisition, few computational models
   experimental results show that using lexical categories     have been developed to explore the usefulness of lexical
   can improve performance in learning, particularly for       categories in learning word meanings (but see Yu, 2006).
   novel or low-frequency words in ambiguous situations.
                                                                  We present a probabilistic model of word learn-
                                                               ing which integrates cross-situational evidence and the
        Learning the Meaning of Words                          knowledge of lexical categories into a single learning
In the course of learning a language, children need            mechanism. We use an existing computational model of
to learn mappings between words and their meanings,            cross-situational learning proposed by Fazly et al. (2008),
mostly from noisy and ambiguous contexts. It has been          and augment it with the syntactic categories of words.
suggested that children learn the meanings of words by         Our computational simulations show that such informa-
observing the regularities across different situations in      tion can improve the model’s performance in learning
which a word is used, or the cross-situational evidence        words. Especially, the results suggest that the syntactic
(Quine, 1960; Pinker, 1989). Experimental studies on           category of a word and the context the word appears in
children and adult learners have shown that both groups        provide complementary information for the acquisition
are sensitive to cross-situational evidence, and can effi-     of word–meaning mappings.
ciently use it to deduce the correct meanings of novel
words in ambiguous situations (Smith & Yu, 2007; Mon-          Related Computational Models
aghan & Mattock, 2009). Moreover, many computa-                A number of computational word learning models have
tional models have demonstrated that cross-situational         used cross-situational learning as their core mechanism
learning is a powerful and efficient mechanism for learn-      for mapping words to meanings. The rule-based model
ing the correct mappings between words and meanings,           of Siskind (1996) and the probabilistic models of Yu
and can explain several behavioural patterns observed in       (2005) and Fazly et al. (2008) all rely on the regularities
children (Siskind, 1996; Yu, 2005; Fazly et al., 2008).        of the co-occurrences of words and meaning elements,
   Another valuable source of information for mapping          successfully learning word meanings from noisy and am-
words to meanings is the syntactic structure of the sen-       biguous data. Moreover, these models simulate several
tence that a word appears in. There is substantial evi-        behavioural patterns observed in children, such as vo-
dence that children are sensitive to the structural regu-      cabulary spurt, fast mapping, and learning synonymy
larities of language from a very young age, and that they      and homonymy. However, all these models ignore the
use these structural cues to find the referent of a novel      syntactic properties of the utterances and treat them as
word (e.g. Naigles & Hoff-Ginsberg, 1995; Gertner et al.,      unstructured bags of words.
2006), a hypothesis known as syntactic bootstrapping              There are only a few existing computational models
(Gleitman, 1990). The syntactic bootstrapping account          that explore the role of syntax in word learning. Mau-
is in accordance with children’s early sensitivity to dis-     rits et al. (2009) has investigated the joint acquisition of
tributional properties of language: one-year-old infants       word meaning and word order using a batch model. This
can recognize sentences from an artificial grammar af-         model is tested on an artificial language with a simple
ter a short period of exposure (Gomez & Gerken, 1999),         relational structure of word meaning, and limited built-
and 2 to 3-year-olds demonstrate robust knowledge of           in possibilities for word order. The Bayesian model of
some of the abstract lexical categories such as nouns and      Niyogi (2002) simulates the bootstrapping effects of syn-
verbs (e.g., Gelman & Taylor, 1984; Kemp et al., 2005).        tactic and semantic knowledge in verb learning, i.e., the
                                                           2452

use of syntax to aid in inducing the semantics of a verb,          a variety of tasks (see Fazly et al., n.d., for a full set of
and the use of semantics to narrow down possible syntac-           experiments on this model).
tic forms in which a verb can be expressed. However, this             In order to augment the base model with category
model relies on extensive prior knowledge about the as-            knowledge, we assume that an independent categoriza-
sociations between syntactic and semantic features, and            tion module can process each sentence and determine
is tested on a toy language with very limited vocabulary           the lexical category for each word, e.g., based on its sur-
and a constrained syntax. Yu (2006) integrates informa-            rounding context. That is, we make the simplifying as-
tion about syntactic categories of words into his model of         sumption that prior to the onset of word learning, the
cross-situational word learning, showing that this type of         categorization module has already formed a relatively ro-
information can improve the model’s performance. Yu’s              bust set of lexical categories from an earlier set of child-
model also processes input utterances in a batch mode,             directed data. This assumption is on the basis of pre-
and its evaluation is limited to situations in which only          vious empirical findings that young children gradually
a coarse distinction between referring words (words that           form a knowledge of abstract categories, such as verbs
could potentially refer to objects in a scene, e.g., concrete      and nouns (e.g., Gelman & Taylor, 1984). In addition,
nouns) and non-referring words (words that cannot pos-             several computational models have been proposed for in-
sibly refer to objects, e.g., function words) is sufficient. It    ducing reliable categories of words by drawing on distri-
is thus not clear whether information about finer-grained          butional properties of their context (see, e.g. Parisien et
categories (e.g., verbs and nouns) can indeed help word            al., 2008). However, children’s acquisition of categories
learning in a more naturalistic incremental setting.               is most probably interleaved with the acquisition of word
                                                                   meaning, and these two processes must be studied simul-
   An Overview of Our Integrated Model                             taneously. As a first step, we investigate whether the
                                                                   word learning process can benefit from the knowledge of
Consider a young language learner hearing the sentence             lexical categories, assuming that such knowledge exists.
the kittie is playing with the yarn, and trying to find               In the next sections we sketch the base model of cross-
out the meaning of yarn. Usually there are many pos-               situational learning, and explain how we extend it to
sible interpretations for yarn based on the surrounding            integrate lexical categories as an alternative source of
scene, and the child has to narrow them down using some            guidance. During the course of learning in both models,
learning strategy. One such method is to register the po-          we use the feedback from the categorization model to de-
tential meanings in the current scene, and compare them            tect different senses of the same word. That is, the same
to those inferred from the previous usages of the same             word types which belong to different categories are rep-
word (i.e., cross-situational learning). Another way to            resented as separate lexical items. For example, the verb
make an informed guess about the meaning of yarn is to             sense and the noun sense of the word cry are mapped to
pay attention to its syntactic properties. For example,            two independent meaning representations.
if the child has already heard some familiar words in a
similar syntactic context (e.g., daddy is playing with the                     Cross-situational Learning
ball, the kittie is sniffing the slipper), she can conclude
                                                                   This section explains the details of the cross-situational
that a group of words which can appear in the context
                                                                   word learning model of Fazly et al. (2008), which we use
“is Xing the –” usually refer to physical objects. There-
                                                                   as our base model.
fore yarn must refer to one of the objects present in the
scene, and not for example to an action or a property.             Representation of Input
   We present a computational model that combines                  The input to our word learning model consists of a set of
these two complementary approaches into a single mech-             utterance–scene pairs that link an observed scene (what
anism of word learning. Our goal is to examine whether             the child perceives) to the utterance that describes it
using the knowledge of word categories in addition to              (what the child hears). We represent each utterance as
cross-situational observations can improve the perfor-             a sequence of words, and the corresponding scene as a
mance in word learning. We use the computational                   set of semantic features, for example:
model of Fazly et al. (2008) as the base model of cross-
situational learning: the model learns word meanings as               He hit the rabbit { animate, male person, act, mo-
probabilistic associations between words and semantic                 tion, contact, force, animal, mammal, rabbit }
elements, using an incremental and probabilistic learning          In the Evaluation section, we explain how the utterances
mechanism, and drawing only on the word–meaning co-                and the corresponding semantic features are selected.
occurrence statistics gradually collected from naturally-             Given a corpus of such utterance–scene pairs, our
occurring child-directed input. This model has been                model learns the meaning of each word w as a probability
shown to accurately learn the meaning of a large set               distribution p(.|w) over the semantic features appearing
of words from noisy and ambiguous input data, and to               in the corpus. In this representation, p(f |w) is the prob-
exhibit patterns similar to those observed in children in          ability of feature f being part of the meaning of word w.
                                                               2453

In the absence of any prior knowledge, all features can             where F is the set of all features seen so far. We use a
potentially be part of the meaning of all words. Hence,             smoothed version of this formula to accommodate noisy
prior to receiving any usages of a given word, the model            or rare input, as explained in Fazly et al. (n.d.).
assumes a uniform distribution over semantic features as
its meaning.                                                        Word Acquisition Score
The Learning Algorithm                                              To evaluate our model, we need to verify how accurately
The model proposes a probabilistic interpretation of                the model learns the meaning of words. We thus define
cross-situational learning (Quine, 1960) through an in-             the acquisition score of a word w at time t as an esti-
teraction between two types of probabilistic knowledge              mation of how closely the meaning probability p(t) (.|w)
acquired and refined over time. Given an utterance–                 resembles the correct meaning of w, or Tw . The correct
scene pair (U(t) , S(t) ) received at time t, the model first       meaning of a word is a set of semantic features accord-
calculates an alignment probability a for each w ∈ U(t)             ing to an input-generation lexicon.1 Ideally, a word is
and each f ∈ S(t) , using the meaning p(.|w ) of all the            accurately learned when its relevant semantic features
words in the utterance prior to this time. It then revises          (those in Tw ) are ranked at the very top of the distribu-
the meaning of the words in U(t) by incorporating the               tion p(t) (.|w). We use average precision2 to measure how
alignment probabilities for the current input pair. This            well p(t) (.|w) separates the relevant features of w from
process is repeated for all the input pairs, one at a time.         irrelevant ones.
Step 1: Calculating the alignment probabilities.                     Adding Lexical Categories to the Model
For a feature f ∈ S(t) and a word w ∈ U(t) , the higher the
probability of f being part of the meaning of w (accord-            As mentioned before, we assume that prior to the on-
ing to p(f |w)), the more likely it is that f is aligned with       set of word learning, the child has formed a number of
w in the current input. In other words, a(w |f , U(t) , S(t) )      lexical categories, each containing a set of word forms.
is proportional to p (t−1) (f |w ). In addition, if there is        More formally, we assume that the word learning model
strong evidence that f is part of the meaning of an-                has access to a categorization function cat(w, U(t) ) which
other word in U(t) —i.e., if p(t−1) (f |wk ) is high for some       at any time t during the course of learning can deter-
wk ∈ U(t) other than w—the likelihood of aligning f to              mine the category of a word w in utterance U(t) . We
w should decrease. Combining these two requirements:                do not make any assumptions about the details of the
                                                                    categorization process, except that it does not rely on
                                    p (t−1) (f |w )                 the meaning of words in order to find their appropriate
      a(w |f , U(t) , S(t) ) =    X                          (1)
                                         p (t−1) (f |wk )           category.
                                wk ∈U(t)                               As the model learns meanings for words, the cate-
   Note that a feature can have a non-zero alignment                gories that these words belong to are implicitly assigned
with more than one word in an utterance. For example,               a meaning as well. Once the word learning process be-
if two concrete nouns occur in a sentence, they both need           gins, we assign a meaning distribution to each category
to be aligned with the single feature artifact.                     on the basis of the meanings learned for its members.
Step 2: Updating the word meanings. We need                         Formally, we define the meaning of a category C as the
to update the probabilities p(.|w) for all words w ∈ U(t) ,         average of the meaning distributions of its members, as
based on the evidence from the current input pair re-               in:                             1 X (t)
flected in the alignment probabilities. We thus add                                  p(t) (f |C) =           p (f |w)            (3)
                                                                                                   |C|
the current alignment probabilities for w and the fea-                                                 w∈C
tures f ∈ S(t) to the accumulated evidence from prior               where |C| is the number of word forms in category C,
co-occurrences of w and f . We summarize this cross-                and p(t) (f |w) is the meaning probability of word w for
situational evidence in the form of an association score,           feature f at time t. Prior to observing any instances of
which is updated incrementally:                                     the members of a category in the cross-situational input,
                                                                    we assume a uniform distribution over all the possible
assoc(t) (w, f ) = assoc(t−1) (w, f ) + a(w|f, U(t) , S(t) )        meaning elements for each category.
                                                                        1
where assoc(t−1) (w, f ) is zero if w and f have not co-                  The model does not have access to this lexicon for learn-
                                                                    ing; it is used only for input generation and evaluation.
occurred before. The model then uses these association                  2
                                                                          Precision is calculated as the proportion of the number of
scores to update the meaning of the words in the current            features from Tw to the total number of features at each cut-
input, as in:                                                       off point in the ranked list p(t) (.|w). The acquisition score is
                                                                    the average over the precisions for all the cut-off points up
                               assoc(t) (f, w)                      to the point where all the features in Tw are included in the
           p(t) (f |w) = X                                   (2)    ranked list. Note that this score is 1 when the probabilities
                                 assoc(t) (fj , w)                  assigned to all of the relevant features of w are higher than
                           fj ∈F                                    those assigned to the irrelevant features.
                                                                2454

Using Categories in Alignment                                           ball
                                                                             → game equipment#1
                                                                               → equipment#1
Knowledge of word categories is integrated into the base                          → instrumentality#3, instrumentation#1
                                                                                    → artifact#1, artefact#1
model in the alignment phase (i.e. Step 1 of the learning                              → ...
algorithm), where we decide which semantic feature in                   ball: { game equipment#1,equipment#1,instrumentality#3,artifact#1, ...
an observed scene must be aligned with which word(s)
in the accompanying utterance. Given a new utterance–                    Figure 1: Semantic features for ball from WordNet.
scene pair, we can align words in the utterance with the
semantic features in the observed scene based on the                  (MacWhinney, 1995), which contains transcripts of con-
cross-situational evidence that we have accumulated so                versations with children between the ages of 1;8 and 3;0.
far. Alternatively, we can find the category for each word            We use the mother’s speech from transcripts of 6 chil-
and use the meaning associated with the word category                 dren, remove punctuation and lemmatize the words, and
as a guide to align it with the best matching semantic                concatenate the corresponding sessions as our test data.
features from the scene. We can merge these two pieces                We automatically construct a scene representation for
of information into an extended version of Eqn. (1):                  each utterance based on the semantic features of the
                                                                      words in that utterance. For nouns, we extract the se-
a(w|f, U(t) , S(t) )      = weight(w) · aw (w|f, U(t) , S(t) )        mantic features from WordNet4 as follows: We take all
                  +      (1 − weight(w)) · ac (w|f, U(t) , S(t) )     the hypernyms (ancestors) for the first sense of the word,
                                                                      and add the first word in the synset of each hypernym to
The word-based alignment score aw (w|f, U(t) , S(t) ) is              the set of the semantic features of the target word (see
calculated as in Eqn. (1). The category-based alignment               Figure 1 for an example). For verbs, we extract features
score ac (w|f, U(t) , S(t) ) is calculated in a similar fashion,      from WordNet as well as from a verb-specific resource,
except it relies on the meaning of the word category:                 VerbNet.5 For adverbs, adjectives and closed class words
                               p(t−1) (f |cat(w, U(t) ))              we use the features of Harm (2002). Words not found in
ac (w|f, U(t) , S(t) ) =     X                                        these three resources are removed from the utterance.
                                    p(t−1) (f |cat(wk , U(t) ))
                                                                         To form the initial lexical categories, we use a non-
                           wk ∈U(t)
                                                                      overlapping portion of the part-of-speech tagged version
where the meaning probability p(t−1) (f |cat(wk , U(t) )) is          of the Manchester corpus. The original corpus has 60
calculated as in Eqn. (3).                                            fine-grained tags, which we map to 11 coarser-grained
   The relative contribution of the word-based versus the             categories, such as Noun, Verb, and Preposition.6
category-based alignment is determined by the function
weight(w). It has been shown that cross-situational ev-
                                                                      Learning Curves
idence is a reliable cue for frequent words: Fazly et al.
(n.d.) show that once their model receives a few instances
of a word form, it can reliably align it with proper se-              To understand whether category information improves
mantic features. On the other hand, the category-based                learning of word–meaning mappings, we compare the
score is most informative when the model encounters a                 pattern of word learning over time for two models: the
low-frequency word. Therefore, we define weight(w) as                 base model which only uses cross-situational evidence,
a function of the frequency of w:                                     and the extended model which incorporates lexical cat-
                                                                      egories into learning. For each model we measure the
                                    freq(w)                           average acquisition score (defined on page 3) of all words
                 weight(w) =                                          that the model has encountered up to each point in time.
                                 freq(w) + 1
   Once the overall alignment score is calculated for the                Figure 2 shows the learning curve for each model over
new input pair, the meaning probabilities of words are                5000 time units (or processed input pairs). The curves
updated through Step 2 of the original learning algo-                 show that the extended model consistently outperforms
rithm, and the meaning of their corresponding categories              the base model. The improvement is more pronounced
are updated accordingly.3                                             as the model receives more input, since by learning more
                                                                      about the meanings of words the model also forms a more
                           Evaluation                                 reliable knowledge about the meanings of categories and
The training data for our model consists of a sequence                can use them more efficiently in aligning the novel words
of utterances, each paired with a set of semantic fea-                with their referents.
tures. We extract utterances from the Manchester cor-
pus (Theakston et al., 2001) in the CHILDES database                     4
                                                                           http://wordnet.princeton.edu/
                                                                         5
    3
                                                                           http://verbs.colorado.edu/~mpalmer/projects/
      For each word w in U (t) , the meaning distribution of          verbnet.html
the corresponding category C is incrementally updated as                 6
                                                                           We thank Chris Parisien for providing us with the coarse-
p(t) (f |C) = p(t−1) (f |C) + |C|
                               1
                                  (p(t) (f |w) − p(t−1) (f |w)).      grained tagging of the corpus.
                                                                  2455

                                                                                                                           0.9
                              0.95                                                                                                                  Without Categories
                                                                                                                           0.8                      With Categories
                              0.90                                                                                         0.7
           Acquistion Score                                                                            Acquisition Score
                                                                                                                           0.6
                              0.85
                                                                                                                           0.5
                                                                                                                           0.4
                              0.80
                                                                    With categories                                        0.3
                              0.75
                                                                    Without categories
                                                                                                                           0.2
                                     0   1000   2000         3000   4000         5000                                            High CF   Low CF      Multi−Novel
                                                   Input Items
Figure 2: Avg. acquisition score for all words over time,                                   Figure 3: Avg. acquisition score for words in contexts
with and without using lexical categories.                                                  with different degrees of familiarity.
Categories and Context Familiarity                                                          that in learning the meaning of words, the context of a
The learning curves presented above show an overall im-                                     word and its lexical category can be seen as complemen-
provement when lexical categories are incorporated in                                       tary sources of knowledge.
word learning. However, we expect the gain from in-                                         Comparing Different Categories
cluding categories to vary across different situations. For
                                                                                            To better understand the impact of lexical categories
example, experimental and computational studies have
                                                                                            on word learning, we examine the pattern of improve-
shown that cross-situational learning can account for ac-
                                                                                            ment for words with different parts of speech. Lexical
curate mapping of a novel word to a novel object in a
                                                                                            categories differ in their frequency of occurrence and in
familiar context (see Fazly et al. (n.d.) for a discussion
                                                                                            their semantic properties. For example, open-class cate-
on this phenomenon). The same pattern is expected in
                                                                                            gories such as Verb and Noun tend to have lower token
our base model, where the alignment between a word
                                                                                            frequency, higher type frequency, and more within-class
and a semantic feature is in part determined by what
                                                                                            meaning variability compared to closed-class categories
the model has learned about the possible meanings of
                                                                                            such as Determiner and Preposition.
the co-occurring context words (see Eqn.1). Therefore,
                                                                                               Recall that words in our test corpus are tagged with
it can learn a lot about a novel word from a single expo-
                                                                                            one of 11 coarse-grained parts of speech. Three of these
sure if that word appears in a familiar context.
                                                                                            categories (Auxiliary, Infinitive and Negation) each con-
   We hypothesize that categories can be particularly
                                                                                            tain only a single word type, and one (Other) is not a
helpful in cases where a novel word first appears in an
                                                                                            coherent and meaningful category. The average acqui-
unfamiliar context (where not all words in the utterance
                                                                                            sition score in both models for the remaining categories
are accurately learned), or when an utterance contains
                                                                                            are shown in Figure 4. Out of these seven categories, four
more than one novel word. To investigate this hypothe-
                                                                                            are open-class: Noun (599 word types), Verb (261), Ad-
sis, we introduce a context familiarity measure CF as the
                                                                                            jective (60), and Adverb (25), and three are closed-class:
mean familiarity of all words that co-occur with a target
                                                                                            Determiner (23), Preposition (17), and Conjunction (8).
word, where the familiarity of a word is determined by
                                                                                               Interestingly, we observe that category information
its frequency range. The mappings between familiarity
                                                                                            helps more with the acquisition of open-class words, in
values and frequency ranges are as follows: 0 (0), 1 (1),
                                                                                            particular Noun (p < 10−16 ) and Verb (p < 0.0001).
2 (2–4), 3 (5–9), 4 (10–29), and 5 (≥ 30), where the
                                                                                            We believe this difference is due to the high token fre-
numbers in parentheses specify the frequency range.
                                                                                            quency of closed-class words which makes them very easy
   Figure 3 shows the average acquisition score of words
                                                                                            to learn, even for the base model that does not take into
with high and low context familiarity (CF ≤ 3 vs. CF
                                                                                            account the information about their categories. More-
> 3), and for novel words which appear in the company
                                                                                            over, using categories does not significantly improve the
of other novel words (this last condition is marked as
                                                                                            acquisition of adjectives and adverbs. We suspect that
Multi-Novel in Figure 3). The average scores are calcu-
                                                                                            this is a result of the small number and the inconsis-
lated by both models after the first occurrence of each
                                                                                            tent meaning representations of these categories in the
word. As can be seen, the inclusion of categories leads
                                                                                            resource of Harm (2002). In general, we predict that
to a statistically significant improvement for words in all
                                                                                            using better resources for extracting semantic features
three conditions (p < 0.05).7 However, the improvement
                                                                                            will boost the contribution of lexical categories in word
is much more pronounced for words with low context
                                                                                            learning.
familiarity, and particularly when an utterance includes
more than one novel word (i.e. a highly unfamiliar con-                                         Conclusions and Future Directions
text). These results support our hypothesis, and suggest
                                                                                            Our computational model of word learning demonstrates
   7
     The p-values are measured according to a two-sided sign                                the advantage of integrating lexical categories into a
test for a confidence interval of 95%.                                                      cross-situational model of word learning. Drawing on
                                                                                         2456

                                                                                                 Gertner, Y., Fisher, C., & Eisengart, J. (2006). Learning
                                                                         Without Categories
                                                                                                   words and rules: Abstract knowledge of word order in
                               0.98                                      With Categories
                                                                                                   early sentence comprehension. Psychological Science,
           Acquisition Score
                               0.94                                                                17 (8), 684–691.
                                                                                                 Gleitman, L. (1990). The structural sources of verb
                               0.90
                                                                                                   meanings. Language Acquisition, 1 (1), 3–55.
                               0.86                                                              Gomez, R., & Gerken, L. (1999). Artificial grammar
                                      CON   PRP   ADV     DET        V    ADJ         N
                                                                                                   learning by 1-year-olds leads to specific and abstract
                                                   Part Of Speech Tags
                                                                                                   knowledge. Cognition, 70 (2), 109–135.
Figure 4: Avg. acquisition score for different categories.                                       Harm, M. W. (2002). Building large scale distributed
                                                                                                   semantic feature sets with WordNet (Tech. Rep. No.
the meaning probabilities of individual words, the model                                           PDP.CNS.02.1). Carnegie Mellon University.
gradually associates each lexical category with a mean-                                          Kemp, N., Lieven, E., & Tomasello, M. (2005). Young
ing representation, which in turn can boost learning of                                            Children’s knowledge of the ”determiner” and ” ad-
novel words. Our simulations of the model over the                                                 jective” categories. Journal of Speech, Language and
course of acquisition show that using lexical categories                                           Hearing Research, 48 (3), 592–609.
consistently improves learning over a base model which
                                                                                                 MacWhinney, B. (1995). The CHILDES project:
only relies on cross-situational evidence. Moreover, our
                                                                                                   Tools for analyzing talk (second ed.). Hillsdale, NJ:
analyses of the results suggest that lexical categories can
                                                                                                   Lawrence Erlbaum Associates.
have a significant impact on the acquisition of open-class
                                                                                                 Maurits, L., Perfors, A. F., & Navarro, D. J. (2009).
words which appear in less familiar context.
                                                                                                   Joint acquisition of word order and word reference. In
   The model in its current form makes simplifying as-
                                                                                                   Proc. of CogSci’09.
sumptions that must be addressed in future work. It is
assumed that lexical categories are formed prior to the                                          Monaghan, P., & Mattock, K. (2009). Cross-situational
onset of the word learning process, and that the category                                          language learning: The effects of grammatical cate-
of each word can be precisely determined upon its first                                            gories as constraints on referential labeling. In Proc.
appearance in the input data. In the future, we intend                                             of CogSci’09.
to use an incremental model of category induction to si-                                         Naigles, L., & Hoff-Ginsberg, E. (1995). Input to verb
multanously learn lexical categories and word meanings.                                            learning: Evidence for the plausibility of syntactic
In fact, using a finer-grained set of categories induced                                           bootstrapping. Dev. Psychology, 31 (5), 827–37.
by such a model might be more suitable for our purpose,                                          Niyogi, S. (2002). Bayesian learning at the syntax-
since they can represent more specialized meanings (e.g.,                                          semantics interface. In Proc. of CogSci’02.
fruits and animals instead of nouns). Moreover, the cat-                                         Parisien, C., Fazly, A., & Stevenson, S. (2008). An in-
egorization process can benefit from the integration of                                            cremental Bayesian model for learning syntactic cate-
word meanings in addition to the distributional context.                                           gories. In Proc. of CoNLL’08.
This extension will allow us to study how the early stages                                       Pinker, S. (1989). Learnability and cognition: The ac-
of word learning and category formation interact.                                                  quisition of argument structure. Cambridge, MA: MIT
                                                                                                   Press.
                                      Acknowledgment                                             Quine, W. (1960). Word and object. Cambridge, MA:
We would like to thank Grzegorz Chrupala for his invalu-                                           MIT Press.
able help, and the anonymous reviewers for their insight-                                        Siskind, J. M. (1996). A computational study of cross-
ful comments on our paper. Afra Alishahi was funded by                                             situational techniques for learning word-to-meaning
IRTG 715 Language Technology and Cognitive Systems                                                 mappings. Cognition, 61 , 39–91.
provided by the German Research Foundation (DFG).                                                Smith, L., & Yu, C. (2007). Infants rapidly learn words
                                                                                                   from noisy data via cross-situational statistics. In
                                             References                                            Proc. of CogSci’07.
Fazly, A., Alishahi, A., & Stevenson, S. (n.d.). A proba-                                        Theakston, A. L., Lieven, E. V., Pine, J. M., & Rowland,
  bilistic computational model of cross-situational word                                           C. F. (2001). The role of performance limitations in
  learning. Cognitive Science. (To appear)                                                         the acquisition of verb-argument structure: An alter-
Fazly, A., Alishahi, A., & Stevenson, S. (2008). A proba-                                          native account. J. of Child Language, 28 , 127–152.
  bilistic incremental model of word learning in the pres-                                       Yu, C. (2005). The emergence of links between lexical ac-
  ence of referential uncertainty. In Proc. of CogSci’08.                                          quisition and object categorization: A computational
Gelman, S., & Taylor, M. (1984). How two-year-old                                                  study. Connection Science, 17 (3–4), 381–397.
  children interpret proper and common names for un-                                             Yu, C. (2006). Learning syntax–semantics mappings to
  familiar objects. Child Development, 1535–1540.                                                  bootstrap word learning. In Proc. of CogSci’06.
                                                                                              2457

