UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Beyond Transitional Probabilities: Human Learners Impose a Parsimony Bias in Statistical
Word Segmentation
Permalink
https://escholarship.org/uc/item/4w73d4nc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Frank, Michael
Tily, Harry
Arnon, Inbal
et al.
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                           Beyond Transitional Probabilities:
      Human Learners Impose a Parsimony Bias in Statistical Word Segmentation
                              Michael C. Frank                                               Harry Tily
                               mcfrank@mit.edu                                           hjt@stanford.edu
                 Department of Brain and Cognitive Sciences                          Department of Linguistics
                    Massachusetts Institute of Technology                                Stanford University
                                  Inbal Arnon                                           Sharon Goldwater
                      inbal.arnon@manchester.ac.uk                                      sgwater@inf.ac.uk
                            Department of Linguistics                                   School of Informatics
                            University of Manchester                                  University of Edinburgh
                               Abstract                                 of the speech stream, however. Lexicon-based learners like
   Human infants and adults are able to segment coherent se-            PARSER (Perruchet & Vinter, 1998) and Bayesian lexical
   quences from unsegmented strings of auditory stimuli after           models (Brent, 1999; Goldwater, Griffiths, & Johnson, 2009)
   only a short exposure, an ability thought to be linked to early      have also been proposed as possible models of segmentation.
   language acquisition. Although some research has hypothe-
   sized that learners succeed in these tasks by computing tran-        Though these models differ on several dimensions, all assume
   sitional probabilities between syllables, current experimen-         that learners attempt to learn a consistent lexicon—a set of
   tal results do not differentiate between a range of models of        word forms that is combined to form the training sequence—
   different computations that learners could perform. We cre-
   ated a set of stimuli that was consistent with two different         and they do this by preferring small lexicons composed of
   lexicons—one consisting of two-syllable words and one of             frequent, short words.
   three-syllable words—but where transition probabilities would
   not lead learners to segment sentences consistently according           Two previous studies have examined whether this kind of
   to either lexicon. Participants’ responses formed a distribution     model could provide a good fit to human learning perfor-
   over possible segmentations that included consistent segmen-         mance. The first contrasted recognition of sub-parts of the
   tations into both two- and three-syllable words, suggesting that
   learners do not use pure transitional probabilities to segment       words from a speech stream and found that PARSER, like hu-
   but instead impose a bias towards parsimony on the lexicons          man learners, failed to discriminate sub-parts of words after
   they learn.                                                          training (Giroux & Rey, 2009). The second study found that
   Keywords: Word segmentation; statistical learning; computa-          a parsimony-biased chunk-finding model better accounted for
   tional modeling.
                                                                        human performance across a range of experiments in the vi-
                           Introduction                                 sual domain than a purely associative model (Orbán, Fiser,
                                                                        Aslin, & Lengyel, 2008). Thus, both of these studies sug-
Human adults, infants, and even members of other species
                                                                        gest that human learners do not simply represent association
have the ability to identify statistically coherent sequences in
                                                                        probabilities in statistical learning.
unsegmented streams of stimuli after only a very short ex-
posure (Saffran, Aslin, & Newport, 1996; Saffran, Newport,                 Our current study asks what kinds of learning biases op-
& Aslin, 1996; Hauser, Newport, & Aslin, 2001). This seg-               erate in statistical learning. Our study makes use of a novel
mentation ability is extremely robust, operates across a wide           language whose transition statistics support not just one but a
range of modalities (Conway & Christiansen, 2005), and has              range of possible coherent segmentations: training data could
been hypothesized to play an important role in early language           be interpreted as a sequence of sentences of six words from
acquisition (Kuhl, 2004). Nevertheless, relatively little is            a lexicon of two-syllable words or a sequence of sentences
known about the computations underlying statistical segmen-             of four words from a lexicon of three-syllable words (where
tation.                                                                 all words appeared with approximately the same frequency).
   In one influential study, Saffran, Newport, and Aslin                TPs for a single sentence in this language are shown in Fig-
(1996) exposed participants to a simple artificial language             ure 1. A learner using pure TPs to segment the language
which consisted of six trisyllabic words concatenated to-               would not recover either lexicon but would instead either
gether to form a continuous speech steam. After only a few              segment the language into sets of six-syllable words or else
minutes of exposure, participants were able to distinguish              segment inconsistently into a mix of two- and three-syllable
words in this language from strings that did not occur with             words. Thus, our language was designed to test whether hu-
the same frequency. They speculated that participants could             man learners would learn more parsimonious lexicons than
succeed by computing syllable-to-syllable transitional proba-           those implied by pure transition statistics.
bilities (TPs) and segmenting the speech stream at local min-              Experiment 1 validates two methodological innovations:
ima in TP.                                                              a web-based interface for data collection and a dependent
   There are many possible computations by which learn-                 measure which directly evaluates participants’ word segmen-
ers could extract coherent units from the statistical structure         tation judgments. Experiment 2 uses these methods to test
                                                                    760

                                                                                                                         100
                               0.50
    transitional probability
                                                                                                                         75
                                                                                                       Percent correct
                               0.25
                                                                                                                         50
                               0.00                                                                                      25
                                      1   2   3    4   5    6    7    8    9   10 11 12
                                                                                                                         0
                                                       syllable position
                                                                                                                               Lab               Turk
Figure 1: Average transitional probabilities between syllables                                                                       Subject group
in an ambiguous language from Experiment 2.
                                                                                                Figure 2: Average percent correct is plotted by subject for
participants’ segmentation judgments in the ambiguous lan-                                      in-lab participants from Frank et al. (under review) and Me-
guage discussed above. We compare the distribution of par-                                      chanical Turk participants from the 2AFC condition of Ex-
ticipants’ segmentations to the performance of two compu-                                       periment 1. Each point is an individual participant, bars show
tational models—a standard TP model and a Bayesian model                                        the mean, and the dashed line represents chance.
that looks for a parsimonious lexicon—and conclude that par-
ticipants’ judgments reflect the operation of a parsimony bias.
                                                                                                another word; this created distractors which appeared in the
                                                  Experiment 1                                  training corpus with lower frequency than the words. For the
                                                                                                segmentation condition, we generated 10 extra sentences ac-
The first condition of Experiment 1 compares web-based
                                                                                                cording to the same uniform frequency distribution and lexi-
data on a segmentation task to previously-collected lab data
                                                                                                con as the training corpus.
(Frank, Goldwater, Griffiths, & Tenenbaum, under review) on
a standard 2 alternative forced choice (2AFC) test trial. The                                   Procedures After selecting our HIT, our Adobe Flash in-
second condition evaluates a new measure of segmentation:                                       terface tested that participants’ sound was on and that they
explicit segmentation decisions. We developed a graphical                                       were able to understand our instructions by asking them to
paradigm in which participants heard a sentence, saw it tran-                                   listen to a simple English word and enter it correctly. Par-
scribed on the screen, and were asked to click between syl-                                     ticipants were then instructed that they would listen to a set
lables to indicate where they thought the boundaries between                                    of sentences from a made-up language and then be tested on
words were.                                                                                     what they had learned. In order to hear each sentence during
                                                                                                training, participants clicked a button marked “next.”
Methods                                                                                            In the test phase of the 2AFC condition, participants heard
Participants Forty eight separate HITs (opportunities for a                                     24 pairs consisting of a word and a length-matched part-word
participant to work) were posted on Amazon’s Mechanical                                         and clicked a button for each to indicate which one sounded
Turk web-based crowd-sourcing platform. We received 40                                          more like the language they just heard. In the segmentation
HITS from distinct individuals. Participants were paid $1 for                                   condition, participants were asked to click on the breaks be-
participating.                                                                                  tween words in a graphic display of a sentence. They per-
                                                                                                formed one practice trial on an English sentence presented
Stimuli For each condition, we constructed 16 distinct lan-
                                                                                                in this way (“In di an go ril las ne ver eat ba na nas”) and
guages to be heard by different participants (to avoid item
                                                                                                prevented from continuing until they segmented it correctly.
effects caused by phonological similarity of words). These
                                                                                                They then segmented 10 test sentences. Sentences were pre-
languages each had a lexicon of six words (2 x two syllables,
                                                                                                sented with each syllable separate. Each sentence was played
2 x three syllables, 2 x four syllables). Words were created
                                                                                                once at the beginning of a trial, and below the sentence was a
by randomly concatenating the syllables ba, bi, da, du, ti, tu,
                                                                                                button that offered the option of hearing the sentence again.
ka, ki, la, lu, gi, gu, pa, pi, va, vu, zi, and zu. Stimuli were
synthesized using MBROLA (Dutoit, Pagel, Pierret, Bataille,                                     Results and Discussion
& Vrecken, 1996) at a constant pitch of 100Hz with 25ms
consonants and 225ms vowels. Sentences were generated by                                        In the 2AFC condition (N=24), we found that participants
randomly concatenating words into strings of four words with                                    were above chance in their mean accuracy, taken as a group
no repetitions. All words had frequencies of 300 in the result-                                 (t(23) = 5.92, p < .0001). Results are plotted together with
ing corpus of 75 sentences.                                                                     data from an identical condition of Frank et al. (under re-
   For the 2AFC condition, part-word test stimuli (Saffran,                                     view) (Experiment 2, 300 words exposure), collected from
Newport, & Aslin, 1996) were created by concatenating the                                       a group of participants in the lab (Figure 2). Mean perfor-
first syllable of each word with the remaining syllables of                                     mance was slightly lower for the Internet-based Turk par-
                                                                                          761

                                                                         t(15) = 3.63, p = .002; recall, t(15) = 2.71, p < .01; F-score,
               1.00
                                         ●
                                                                         t(15) = 3.41, p < .004), though boundary performance was
               0.75
                                                                         better than token performance. Participants were able to un-
                           ●●            ●          ●
                           ●
                                         ●
                                         ●          ●
                                                                         derstand the segmentation task and link the regularities they
       Score   0.50
                           ●
                           ●●            ●          ●●●                  extracted from the exposure corpus to the response format.
                           ●             ●●         ●●
                       −   ●
                           ●         −   ●
                                         ●●   −     ●●
                                                    ●
               0.25
                           ●●●
                           ●             ●          ●●
                                                                                               Experiment 2
                           ●             ●          ●
                           ●             ●
                                         ●
                                                    ●
                                                    ●
                                                                         We made use of the two methodological innovations from
               0.00
                           ●             ●          ●
                                         ●                               Experiment 1—Internet data collection and explicit segmen-
                                                                         tation judgments—to ask about participants’ responses to a
                      precision      recall   f score
                                                                         language where TP did not reveal the possible lexicons of
                               Evaluation measure                        two- or three-syllable words. Instead, pure TPs predicted that
                                                                         participants would often segment the language into words of
                                                                         six-syllables and would rarely segment into words of two or
Figure 3: Token precision, recall, and F-score are plotted for
                                                                         three syllables. Our next experiment tests these predictions.
individual participants in the segmentation response condi-
tion of Experiment 1. Points represent individual participants           Methods
for each measure. Bars show means and dashed lines show
                                                                         Participants Two-hundred and three separate experimental
permutation baselines.
                                                                         HITs were posted on Amazon Mechanical Turk. We received
                                                                         119 HITs from distinct individuals who made segmentation
ticipants (M=66% compared with M=71%) but not signifi-                   decisions on every trial. Participants were paid $0.50 for par-
cantly so (Welch two-sample t-test for unequal sample sizes,             ticipating. An addition 145 HITs in the test-only control con-
t(21.21) = −.92, p = .37). Participants completing the learn-            dition were posted at $0.25 each; we received 102 HITs from
ing task on their own computer via the Internet were able to             distinct individuals who made segmentation decisions.
perform at levels comparable to participants in an isolated
room in a psychology laboratory.                                         Stimuli Languages were generated using two parallel vo-
   In the segmentation condition (N=16), we could not ana-               cabularies, one of eight two-syllable words and one of six
lyze participants’ percent correct judgments as in the 2AFC              three-syllable words. These vocabularies were designed to
condition. Instead, we evaluated two aspects of performance.             allow overlapping segmentations where the presence of a cer-
First, we asked about the correctness of the boundaries par-             tain word from one vocabulary did not always indicate the
ticipants placed: whether these decisions corresponded to                presence of the same set of words from the other. For ex-
the correct segmentation (boundary performance). Second,                 ample, if the three-syllable vocabulary contained ABC, the
we asked about whether each word in the sentence was seg-                two-syllable vocabulary would contain at least either AB and
mented correctly at its boundaries (token performance).                  two words beginning C, or BC and two words ending A. Sen-
   We computed hits (correctly placed boundaries or correctly            tences of 12 syllables were generated by choosing syllables
segmented tokens), misses (missed boundaries or tokens that              one at a time from the set that made the sentence to the current
were not segmented appropriately), and false-alarms (extra               point compatible with both vocabularies. At each point, syl-
boundaries or incorrect tokens that were segmented). Pre-                lables were chosen from a distribution over this set, weighted
cision captures the proportion of boundaries that were placed            inversely to the frequency with which they had been chosen
correctly and is computed as hits / (hits + false-alarms), while         to follow the previous syllable in all sentences so far. The
recall captures the total proportion of correct boundaries that          resulting sentences displayed probabilistic word-to-word de-
were identified and is computed as hits / (hits + misses). We            pendencies, much as one would expect in natural language
combined these into an F-score, a commonly used metric that              due to the syntactic relationships between words, but in no
is the harmonic mean of precision and recall (Goldwater et               languages were there pairs of words from either vocabulary
al., 2009).                                                              which always appeared together. We generated 30 distinct
   Figure 3 shows token precision, recall, and F-score for par-          languages and synthesized them as in Experiment 1. Each
ticipants in the segmentation condition. We calculated an em-            language contained 25 sentences for training and 10 test sen-
pirical baseline for each measure via permutation: we repeat-            tences, sampled from the same distribution. Sentence presen-
edly shuffled each participant’s boundary decisions within               tation order was random.
each sentence at random and computed the same measures                   Procedures Procedures were identical to the segmentation
over it, then took the mean for each. We then used these                 condition of Experiment 1. Participants in the test-only con-
empirical baselines to test whether participants were above              trol condition received no training sentences.
chance in this condition and found that they were for both
measures (boundary performance: one sample t-test for pre-               Results and Discussion
cision, t(15) = 5.23, p = .0001; recall, t(15) = 6.79, p <               Participants produced a wide range of segmentations, from
.0001; F-score, t(15) = 8.75, p < .0001, token performance:              those which segmented every three syllables to those which
                                                                   762

         1:11            1:11            1:11            1:11            1:11               1:11            1:11             1:11
         1:11            1:11            1:11            1:11            1:11               1:11            1:11             1:11
Figure 4: Twenty four participants in Experiment 2, uniformly sampled along the dimension of 2-segmentation F-score. Plots
show average probability of placing a boundary at each location in a sentence. Top left shows three-segmenters (three peaks sep-
arating four three-syllable plateaus), while bottom right shows two-segmenters (five peaks separating six two-syllable plateaus).
segmented every two syllables. Sample responses are shown            duced by these models on the same criteria that we used for
in Figure 4. While there was an overall trend towards 2-             the human participants.
consistent segmentations, a wide variety of segmentations
were observed. Contrary to the predictions of the TP account,        Transitional probability model
there were almost no segmentations into words of six sylla-          For each language, we calculated TP for each pair of sylla-
bles and there were a considerable number of segmentations           bles that appeared in the training portion of the corpus. We
into words of two and three syllables.                               computed TP as P(s2 |s1 ) = C(s1 , s2 )/ ∑s0 ∈S C(s1 , s0 ) where
   We evaluated participants’ performance on the same mea-           C(s1 , s2 ) refers to the count of instances of the string s1 s2 .
sures used in Experiment 1: precision, recall, and F-score for          Earlier proposals for TP models called for segmenting at
both boundaries and tokens. Rather than using a single cor-          local minima in TP (Saffran, Newport, & Aslin, 1996). How-
rect segmentation, we calculated these measures for both the         ever, this method produces only a single possible segmen-
2-syllable lexicon and the 3-syllable lexicon (Figure 5), show-      tation for a given sentence and provides no plausible expla-
ing the distribution of responses on the continuum between a         nation for how participants could have given such different
perfect 2-segmentation and a perfect 3-segmentation.                 responses for such similar languages. Thus, we chose to con-
   One possible alternative explanation of our finding could         vert the TPs for test sentences into decision boundaries via
be that learners have a bias towards segmenting consistently         a simple threshold operation: we inserted a boundary in a
(e.g., because of the trochaic, bisyllabic structure of English)     test sentence every time TP was below a threshold value in
even without taking into account the structure of the lan-           that sentence. Rather than picking a single threshold value,
guages they heard. However, results from the first trial of          we assumed that participants might have a range of threshold
the test-only condition had a very different distribution than       values and that this range might explain the variation between
those who underwent training (Figure 5). Without training,           participants we observed. Therefore we created a separate
performance was similar to a randomized baseline in which            segmentation for each language for each threshold value from
participants’ judgments for each sentence were shuffled ran-         zero to one at an interval of .1.
domly. Although there was some learning during test for
participants in the test-only condition, there was very little       Lexical model
change in the distribution of responses during test for those        We also ran the unigram Bayesian Lexical model described in
participants who underwent training.                                 Goldwater et al. (2009). This model is a probabilistic model
   Our results are inconsistent with the hypothesis that partic-     which uses Bayesian inference to search the space of segmen-
ipants segmented on the basis of TPs. Instead, the distribu-         tations of the training corpus, evaluating each segmentation
tion of participants’ responses shows a bias towards segmen-         on the parsimony of the lexicon that would have created it.
tations that were consistent with a more parsimonious lexicon        The structure of the model makes a segmentation more prob-
than that produced by segmenting at low transition probabili-        able when it results in fewer, shorter lexical items (though
ties.                                                                also when the segmentation itself contains fewer word tokens,
                                                                     which leads to a trade-off).
                            Models                                      As in the TP model, it was important to investigate the
To formalize the intuitions motivating Experiment 2, we eval-        range of segmentations that were available under this model.
uated a TP model and a lexicon-finding model on the exper-           When we ran a standard Markov-chain monte carlo algorithm
imental stimuli. We then evaluated the segmentations pro-            using the parameter set from previous simulations, we found
                                                                 763

                                                                Human data                                                                                                                Test−only condition                                                                                                                Random baseline
         F−score for three−segmentation                                                                                                           F−score for three−segmentation                                                                                                F−score for three−segmentation
                                          1.0    ●
                                                  ●                                                                                                                                1.0                                                                                                                           1.0
                                                      ●
                                                 ●      ●
                                                      ●
                                                      ●  ●      ●
                                                                ●
                                                       ●
                                                       ● ●
                                                 ●
                                          0.5                                                                                                                                      0.5                                                                                                                           0.5
                                                        ●
                                                 ●          ●
                                                           ●
                                                          ●         ● ●
                                                                ● ●                                                                                                                                 ●
                                                                                                                                                                                                    ●
                                                            ●          ●
                                                                ●                ●                                                                                                              ●       ●
                                                                           ●
                                                                           ●                                                                                                                                                                                                                                                       ●
                                                                               ●●●    ●                                                                                                            ● ● ●●                                                                                                                   ●
                                                            ●             ●                                                                                                                           ● ●● ●● ●
                                                                                ●●                                                                                                                  ●       ● ●                                                                                                            ●●●● ●●
                                                      ● ●                    ● ● ●                                                                                                              ● ●● ●● ●●                                                                                                                 ● ●●
                                                                                                                                                                                                                                                                                                                           ●
                                                         ● ●●         ●       ●● ● ●      ●●                                                                                                       ● ●●● ● ● ●●●●●● ●                                                                                                     ●●
                                                                                                                                                                                                                                                                                                                           ●●●
                                                                                                                                                                                                                                                                                                                           ●
                                                                                                                                                                                                                                                                                                                           ●  ●
                                                                                                                                                                                                                                                                                                                             ●●●
                                                                                                                                                                                                                                                                                                                               ●●●
                                                                                                                                                                                                                                                                                                                                 ●
                                                                                                                                                                                                                                                                                                                                 ●
                                                         ●            ●● ●   ● ● ●● ●                                                                                                                        ●● ●       ●                                                                                                 ●     ●●●●●
                                                         ●          ●  ● ● ●             ● ●                                                                                                        ●                                                                                                                     ●●●
                                                                                                                                                                                                                                                                                                                            ●●●●●
                                                                                                                                                                                                                                                                                                                                ●
                                                                                                                                                                                                                                                                                                                                ●●
                                                                                                                                                                                                                                                                                                                                 ● ●● ●●
                                                                    ●                                                                                                                                                                                                                                                     ● ●  ●
                                                           ●              ● ●● ●●
                                                                          ●      ● ●●                                                                                                                         ●●                                                                                                           ●●●
                                                                                                                                                                                                                                                                                                                             ●●●
                                                                                                                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                                                                                                                              ●●●
                                                                                                                                                                                                                                                                                                                                ●
                                                                                                                                                                                                                                                                                                                                ●●
                                                                                                                                                                                                                                                                                                                                 ●●●●
                                                                                                                                                                                                                                                                                                                                    ●●●
                                                                                                                                                                                                                                                                                                                                      ●●●●
                                          0.0                                                                                                                                      0.0                                                                                                                           0.0
                                                                                                                                                                                                                 ●    ●                                                                                                      ●●
                                                                                                                                                                                                                                                                                                                              ●●●●●
                                                                                                                                                                                                                                                                                                                                  ●●
                                                                                                                                                                                                                                                                                                                                   ●●●
                                                      ●                      ●        ●●    ●●  ●                                                                                                                       ● ●                                                                                                      ●
                                                                                                                                                                                                                                                                                                                                ●●●● ●
                                                          ●         ●   ●● ● ●●         ●● ●● ●●●                              ●●
                                                                                                                              ●● ●
                                                                                                                                 ●●                                                                                     ●                                                                                                ● ● ●         ●
                                                0.0                                  0.5                                          1.0                                                    0.0                      0.5                                         1.0                                                      0.0                   0.5   1.0
                                                F−score for two−segmentation                                                                                                             F−score for two−segmentation                                                                                                  F−score for two−segmentation
                                                                                                                        Bayesian Lexical Model                                                                                                       Transition Probability
                                                                                 F−score for three−segmentation                                                                                               F−score for three−segmentation
                                                                                                                  1.0                                                                                                                          1.0
                                                                                                                         ●
                                                                                                                         ●
                                                                                                                         ●
                                                                                                                         ●
                                                                                                                         ●                                                                                                                                        ●
                                                                                                                  0.5                                                                                                                          0.5
                                                                                                                         ●
                                                                                                                              ●                                                                                                                               ●        ●
                                                                                                                                                                                                                                                                      ●●
                                                                                                                                                                                                                                                                       ●
                                                                                                                               ●                                                                                                                      ●     ●
                                                                                                                                                                                                                                                           ●          ●
                                                                                                                         ●
                                                                                                                         ●●                                                                                                                                            ●
                                                                                                                                  ●                                                                                                                   ●    ●           ●●●●
                                                                                                                         ●                  ●                                                                                                                 ● ●●
                                                                                                                                                                                                                                                              ●          ●
                                                                                                                                                                                                                                                                        ●●
                                                                                                                                                                                                                                                      ●    ● ● ●● ●
                                                                                                                           ●            ●                                                                                                                         ● ● ● ● ●
                                                                                                                                                                                                                                                                  ● ●
                                                                                                                                                                                                                                                                    ●●● ● ●● ●●
                                                                                                                         ● ●                                                                                                                                        ●
                                                                                                                                                                                                                                                                    ● ●● ●
                                                                                                                                                                                                                                                                     ●●   ●●●● ●
                                                                                                                           ●                                                                                                                                    ●● ●       ●●●
                                                                                                                                                                                                                                                                             ●
                                                                                                                                        ●                                                                                                                        ●●    ●●● ●
                                                                                                                                    ●                                                                                                                                       ●
                                                                                                                  0.0                                                                                                                          0.0
                                                                                                                           ●      ●
                                                                                                                         ●● ●●● ●●●●
                                                                                                                                   ● ●●
                                                                                                                                      ● ● ●● ●●●                                    ●                                                                     ●   ●
                                                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                                                              ●●●
                                                                                                                                                                                                                                                               ●●●
                                                                                                                                                                                                                                                                 ●
                                                                                                                                                                                                                                                                 ●
                                                                                                                                                                                                                                                                 ●●●
                                                                                                                                                                                                                                                                   ●
                                                                                                                                                                                                                                                                   ●●●
                                                                                                                                                                                                                                                                    ●●●
                                                                                                                                                                                                                                                                      ●
                                                                                                                                                                                                                                                                      ●●
                                                                                                                                                                                                                                                                       ●
                                                                                                                                                                                                                                                                       ●●
                                                                                                                                                                                                                                                                        ●
                                                                                                                                                                                                                                                                        ●●
                                                                                                                                                                                                                                                                         ●●●
                                                                                                                                                                                                                                                                           ●
                                                                                                                                                                                                                                                                           ●●
                                                                                                                                                                                                                                                                            ●●
                                                                                                                                                                                                                                                                            ● ●●●
                                                                                                                                                                                                                                                                                ●●● ●
                                                                                                                        0.0                           0.5                                      1.0                                                   0.0                            0.5                                      1.0
                                                                                                                        F−score for two−segmentation                                                                                                 F−score for two−segmentation
Figure 5: Participant and model token F-scores for Experiment 2. Three-syllable token F-scores are plotted by their two-syllable
token F-scores. Each dot represents a single participant or a single model run.
Table 1: Kullback-Leibler divergence between the distribu-                                                                                                                                                                                Table 2: Log probability of consistent segmentations under
tion of human experimental data and other data.                                                                                                                                                                                           the Lexical model.
     Model                                                                                                        Token F                       Boundary F                                                                                            Syllables per word Log probability
     Test-only condition                                                                                           4.01                            3.45                                                                                               6                        -594.28
     Random baseline                                                                                               7.26                            9.45                                                                                               4                        -932.92
     Lexical model                                                                                                 2.07                            3.16                                                                                               3                        -530.62
     Transitional probability                                                                                      4.62                            3.72                                                                                               2                        -697.07
                                                                                                                                                                                                                                                      1                       -1127.20
                                                                                                                                                                                                                                                      unsegmented             -1907.20
that it converged to a segmentation that preferred a lexicon of
three-syllable words. In order to investigate a broader range
of segmentations, we manipulated the temperature of infer-                                                                                                                                                                                human results: nearly all segmentations it found were compa-
ence in the model by exponentiating posterior probabilities                                                                                                                                                                               rable in F-score for 2- and 3-segmentation, and no segmenta-
at a range of values. (This manipulation is a standard tech-                                                                                                                                                                              tion was over an F-score of .5 on either measure. The Lexical
nique for allowing sampling algorithms to explore a hypoth-                                                                                                                                                                               model came closer to capturing the distribution of responses,
esis space more broadly, rather than converging to the single                                                                                                                                                                             though it was not as effective at finding 2-segmentations
highest-probability answer.) With slightly higher tempera-                                                                                                                                                                                as the human participants, suggesting a possible role for a
tures, our sampler explored a broad range of possible seg-                                                                                                                                                                                trochaic bias. Unlike the TP model, however, its probabil-
mentations. We report results for temperature = 2 although                                                                                                                                                                                ity landscape was truly multi-modal, finding relatively high
results for a temperature of 3 were comparable.                                                                                                                                                                                           probability segmentations with 2, 3, and 6 syllables per word
                                                                                                                                                                                                                                          (Table 2).
Results and Discussion
                                                                                                                                                                                                                                             We measured the differences between the distributions
Results for both models are shown in Figure 5, bottom. The                                                                                                                                                                                of responses across human participants and models using
transitional probability model failed to capture the spread of                                                                                                                                                                            Kullback-Leibler divergence—an information-theoretic mea-
                                                                                                                                                                                                        764

sure of the difference between a true distribution and an ap-            Results in the statistical learning literature have rightly
proximation of that distribution—to quantify the number of            been interpreted as showing that human learners are sen-
bits between distributions (MacKay, 2003). In order to con-           sitive to associative and transitional statistics in their envi-
vert sets of observations into smooth distributions, we con-          ronment. But these interpretations should not be confused
volved them with a Gaussian kernel with a constant kernel             with the conclusion that learners compute these particular—
width. This manipulation produced a smooth density which              or any—transition statistics. Instead, future research on sta-
could be effectively compared using KL divergence.1 Results           tistical learning should attempt to characterize both human
are shown in Table 1. The Lexical model showed the lowest             learning biases and the computations that give rise to them.
divergence from the human response distribution, while the
TP model was closer to the empirical baseline in its diver-                                Acknowledgments
gence from the human distribution.                                    Thanks to Richard Aslin, Noah Goodman, Elissa Newport,
                                                                      Josh Tenenbaum, and Ed Vul for valuable discussion. MCF
                     General Discussion                               was supported by a Jacob Javits Graduate Fellowship and
We presented two studies of statistical word segmentation.            NSF DDRIG #0746251.
The first study introduced two methodological innovations,                                     References
web-based data collection and explicit segmentation judg-
                                                                      Brent, M. (1999). An efficient, probabilistically sound al-
ments. We used these new methods in the second study
                                                                         gorithm for segmentation and word discovery. Machine
to test whether human learners faithfully learned the transi-
                                                                         Learning, 34, 71–105.
tional probabilities of an ambiguous language or whether they
                                                                      Conway, C. M., & Christiansen, M. H. (2005). Modality-
gave a segmentation that was more consistent with one of the
                                                                         constrained statistical learning of tactile, visual, and au-
two possible lexicons that generated the training corpus. We
                                                                         ditory sequences. Journal of Experimental Psychology:
found that the distribution of participants’ responses was not
                                                                         Learning, Memory, and Cognition, 31, 24–39.
consistent with the distribution of segmentations produced by
                                                                      Dutoit, T., Pagel, V., Pierret, N., Bataille, F., & Vrecken, O.
segmenting according to a TP model. Thus, our results pro-
                                                                         (1996). The MBROLA project: Towards a set of high qual-
vide evidence that human learners do not simply encode tran-
                                                                         ity speech synthesizers free of use for non commercial pur-
sitional or associative statistics but instead impose some kind
                                                                         poses. In Fourth International Conference on Spoken Lan-
of bias on what they learn.
                                                                         guage Processing.
   This bias could be either a bias for consistent word lengths
                                                                      Frank, M., Goldwater, S., Griffiths, T., & Tenenbaum, J. (un-
or for a parsimonious lexicon. A model which searched for
                                                                         der review). Modeling human performance in statistical
lexicons with small lexicons consisting of highly frequent,
                                                                         word segmentation.
short words produced a distribution similar to that produced
                                                                      Giroux, I., & Rey, A. (2009). Lexical and sub-lexical units in
by the human learners. Nonetheless, the Lexical model pre-
                                                                         speech perception. Cognitive Science, 33, 260–272.
ferred a lexicon with three-syllable words, unlike human
                                                                      Goldwater, S., Griffiths, T., & Johnson, M. (2009). A
learners who preferred to segment into two-syllable words;
                                                                         bayesian framework for word segmentation: Exploring the
and the Lexical model assigned a high probability to a seg-
                                                                         effects of context. Cognition, 112, 21–54.
mentation into two words of six syllables each, while partic-
                                                                      Hauser, M., Newport, E., & Aslin, R. (2001). Segmentation
ipants rarely produced this segmentation. Frank et al. (under
                                                                         of the speech stream in a non-human primate: Statistical
review) found that models with memory limitations provided
                                                                         learning in cotton-top tamarins. Cognition, 78, 53–64.
a better fit to human performance, suggesting that one possi-
                                                                      Kuhl, P. (2004). Early language acquisition: cracking the
ble explanation for these differences is the increased difficulty
                                                                         speech code. Nature Reviews Neuroscience, 5, 831–843.
for human learners of remembering longer words.
                                                                      MacKay, D. (2003). Information Theory, Inference, and
   The language used in Experiment 2 has a number of limita-             Learning Algorithms. Cambridge, UK: Cambridge Uni-
tions. First, unlike recent studies (Frank et al., under review;         versity Press.
Giroux & Rey, 2009), the competing lexicons we used in this           Orbán, G., Fiser, J., Aslin, R. N., & Lengyel, M. (2008).
study were composed of words of homogenous length, lead-                 Bayesian learning of visual chunks by human observers.
ing to stimuli that could be perceived as isochronous. Second,           Proceedings of the National Academy of Sciences, 105,
the size of the lexicons was relatively small and the restric-           2745-2750.
tions on sentences were tight, leading to a small number of           Perruchet, P., & Vinter, A. (1998). Parser: A model for word
possible sentences. Our ongoing work attempts to address                 segmentation. Journal of Memory and Language, 39, 246–
both of these issues.                                                    263.
    1 Because both the TP model and the Lexical model produced        Saffran, J., Aslin, R., & Newport, E. (1996). Statistical learn-
a significant number of segmentations that failed to place any           ing by 8-month-old infants. Science, 274, 1926.
boundaries—for the TP model this was due to extreme threshold         Saffran, J., Newport, E., & Aslin, R. (1996). Word segmen-
values, and for the Lexical model this was due to convergence is-
sues in the online sampler we used—we excluded all model runs            tation: The role of distributional cues. Journal of memory
that failed to make any segmentation decisions.                          and language, 35, 606–621.
                                                                  765

