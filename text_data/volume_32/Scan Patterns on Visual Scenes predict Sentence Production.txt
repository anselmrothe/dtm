UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Scan Patterns on Visual Scenes predict Sentence Production
Permalink
https://escholarship.org/uc/item/19k5r28h
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Coco, Moreno I.
Keller, Frank
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                     Scan Patterns on Visual Scenes predict Sentence Production
                                            Moreno I. Coco (M.I.Coco@sms.ed.ac.uk) and
                                                     Frank Keller (keller@inf.ed.ac.uk)
                                              School of Informatics, University of Edinburgh
                                               10 Crichton Street, Edinburgh EH8 9AB, UK
                               Abstract                                 responses are driven by exogenous visual mechanisms while
                                                                        minimizing the need for cross-modal coordination.
   A range of cognitive modalities are involved in everyday tasks,
   which raises the question to which extend these modalities are          Different patterns of visual attention emerge in other vi-
   coordinated. In this paper, we focus on two particular aspects       sual tasks, such as memorization or imagery (Humphrey and
   of this coordination: linguistic structure and visual attention      Underwood, 2008), where participants are asked to memo-
   during sentence production, based on the hypothesis that sim-
   ilar scan patterns are associated with similar sentences. We         rize images in preparation for a recall phase. In the recall
   tested this hypothesis using a dataset from an eye-tracking ex-      phase, despite the absence of the original stimuli (preventing
   periment in which participants had to describe photo-realistic       bottom-up effects), scan patterns on a blank screen were more
   scenes. We paired each sentence produced with the corre-
   sponding scan pattern, and computed a range of similarity            similar across participants within the same scene than across
   measures for both modalities. Correlation and mixed model            different ones (Humphrey and Underwood, 2008). In this
   analyses confirmed that trials involving similar scan patterns       case, the task requires an endogenous control of visual atten-
   also involve similar sentences productions. This was true for
   all pairs of linguistic and scan pattern similarity measures we      tion through top-down knowledge, i.e., scene layout, contex-
   investigated. The result holds both before and during sentence       tual information, and even semantic relations between objects
   production, and for within-scene and between-scene analyses.         (Hwang et al., 2009). Thus, exogenous bottom-up effects are
   Keywords: scan patterns; sentence production; eye-tracking;          overridden by endogenous contextual guidance effects.
   sentence similarity.                                                    These results, consistent with similar findings from visual
                                                                        search studies (Yang and Zelinsky, 2009), suggest that in
                           Introduction                                 tasks requiring endogenous control, categorical and seman-
Everyday tasks demand the coordination of a range of cogni-             tic information is activated. It seems plausible that this en-
tive modalities. If the task is to make tea, for example, then          dogenous access to categorical information is activated dur-
motor actions (e.g., arm-hand movement) and visual atten-               ing daily actions (Land, 2006); e.g., categorical knowledge
tion (e.g., looking at the pot) have to be coordinated (Land,           about the tea pot (i.e., it has a handle to grasp) is necessary to
2006). This implies that if two different persons perform the           allow cross-modal coordination between visual attention and
same task, they will do so in a similar way. It follows that            motor action.
the sequence of scan patterns, i.e., eye fixations across spa-             It is important to notice that this information has a direct
tial locations in temporal order (Noton and Stark, 1971) as             link with language processing. Categorical information, in
well as the sequence of motor actions, will be similar across           fact, is typically expressed verbally, and drives linguistic tasks
participants (Land, 2006).                                              such as scene description. It seems likely that the same mech-
   In this paper, we investigate whether a similar evidence of          anism, based on categorical information, which allows coor-
cross-modal coordination can be found when vision and lan-              dination between motor-action and visual attention might also
guage have to be coordinated. In particular, we focus on the            underlie the coordination between language processing and
similarity between scan patterns and linguistic structures in a         visual attention.
language generation task.                                                  Previous research has looked at the interaction between
   In the visual cognition literature, similarity of scan patterns      vision and language principally using the visual world
has not received much attention, mainly because of the high             paradigm (VWP, Tanenhaus et al. 1995), an eye-tracking
variability across participants (Henderson, 2003). There are            paradigm which has demonstrated clear links between the
some results, however, that point toward a range of visual fac-         processing of certain linguistic constructions and the access
tors that can trigger similarity. Often, these factors are related      to visual contextual information (Knoeferle and Crocker,
to the task (Castelhano et al., 2009) and to the degree of cross-       2006). Research in this field suggests a tightly coupled re-
modal interactivity required to perform it.                             lation between vision and language, but previous works
   In tasks with a low level of interactivity, i.e. free viewing,       has largely focused on specific psycholinguistic phenom-
visual attention is mainly guided by exogenous factors like             ena (e.g., attachment ambiguity), rather than uncovering the
saliency (Itti and Koch, 2000): a measure of visual promi-              shared mechanisms by which this interaction takes place. We
nence based on low-level features (color, intensity and ori-            explain this coupled relation assuming a categorical interface
entation). A free viewing task does not require visual atten-           which coordinates the cross-modal, visual and linguistic, in-
tion to interact with knowledge-based (top-down) informa-               teraction.
tion. The low interactivity of free viewing means the visual               In this paper, we investigate the extent to which visual and
                                                                    1934

                                                                         the scan patterns produced both in preparation for produc-
                                                                         tion (min = 800 ms; max = 10205 ms) and during production
                                                                         (min = 2052 ms; max = 18361 ms). Both types of variabil-
                                                                         ity have to be taken into account when developing metrics for
                                                                         sentence and scan pattern similarity.
                                                                         Similarity Measures
                                                                         Before quantifying the association between scan patterns
                                                                         and sentence productions, we measure similarity within each
                                                                         modality. We defined two similarity (or equivalent, dissimi-
                                                                         larity) measures for both modalities. Applying more than one
                                                                         measure makes it less likely that our results will be an artifact
                                                                         of the type of measure used.
                                                                         Sentence Measures We define two similarity measures on
                                                                         sentences: Feature Dissimilarity (FD) and semantic similarity
                                                                         computed using Latent Semantic Analysis (LSA). We pre-
                                                                         process the sentences produced by the participants using an
Figure 1: Example of scene and cues used as stimuli for the descrip-     automatic part of speech (POS) tagger (Toutanova and Man-
tion task                                                                ning, 2000), whose reported accuracy is 96.8% on the Penn
                                                                         Treebank. The POS tags make it easy to extract relevant in-
                                                                         formation from a sentence.
language processing are synchronized when participants per-                 For FD measure, we represent each sentence as a vector,
form a task viz., scene description in a visual context, which           each element of which represents a feature of the sentence.
requires endogenous interaction between linguistic and visual            We include semantic and syntactic features, as well as contex-
processing. Our main hypothesis is that scan patterns and sen-           tual information derived from the scenario a scene belongs to.
tences are correlated, i.e., if two trials involve similar scan          (In the result section, we also report correlation coefficients
patterns, then the sentences produced in these two trials will           obtained when excluding the contextual features.)
also be similar.                                                            Syntactic features include (1) the length of the utterance,
                                                                         which is a general indicator of complexity while also reflect-
                  Experimental Setting                                   ing the total number of visual referents, and (2) the presence
In this section, we discuss how the data was collected and pro-          of coordination, which reflects disambiguation strategies. As
cessed, and explain how we computed the measures of scan                 semantic features we include (1) the verb frame and (2) se-
pattern and linguistic similarity.                                       mantic similarity between verbs. The verb frame encodes the
                                                                         arguments the verb can take, obtained from WordNet (e.g.,
Data Collection and Pre-processing                                       transitive or intransitive); semantic similarity is computed us-
In an eye-tracking language production experiment (Coco                  ing Jiang and Conrath’s (JC) synset path-distance (Budanit-
and Keller, 2010), we asked participants to describe photo-              sky and Hirst, 2006). This distance measure is based on the
realistic indoor scenes after being prompted with cue words              number of nodes from one verb to another in the WordNet
which referred to visual objects in the scenes. The cue words            database. We calculate pairwise JC distance on all pairs of
were either animate or inanimate (e.g., man or suitcase)                 unique verbs in our corpus of sentence productions; we then
and were ambiguous with respect with the scene (see Fig-                 apply hierarchical clustering to group together similar verbs.
ure 1 for an example trial). Participants’ eye-movements were            Cluster membership is the feature value included in the FD
recorded using an Eyelink II eye-tracker with at sampling rate           vector.
of 500 Hz on a 21” screen (1024 x 768 pixel resolution),                    The contextual features include (1) the animacy of the
while the speech of the participants was recorded with a lapel           cue word, useful to discriminate between different descriptive
microphone. We collected a total of 576 sentences produced               routines and, (2) the scenario in which the sentence was pro-
for 24 scenes which were drawn from six different scenarios              duced (e.g., bathroom, entrance). Notice that the contextual
(e.g., bedroom, entrance). The sentences were manually tran-             features are not scene specific; each scenario is represented
scribed and paired with the scan patterns that participants fol-         by four different scenes.
lowed when generating the sentences. We removed two pairs                   After converting each sentence into a vector of features,
because the sentences were missing.                                      we calculate FD between all pairs of sentences by applying
   The data varies across participants and scenes both in terms          Gower distance (Gower, 1971), which can be used when the
of the complexity of the sentences (i.e., one man waits for              data is both numerical and categorical.
another man to fill out the registration form for a hotel vs.               LSA measures the similarity between words based on the
the man is checking in for Figure 1) and in the length of                co-occurrence of content words within a collection of docu-
                                                                     1935

                                                                        SPold , we obtain the number of time-points corresponding to
                                                                        a time unit of SPnew by simply dividing the length of SPold
                                                                        with the length of SPnew . Over the SPold time-points, we look
                                                                        for the object which has received the highest number of looks
                                                                        and map it into the corresponding time-unit of SPnew . The fi-
                                                                        nal result is a normalized scan-pattern of fixed length contain-
                                                                        ing the objects most likely to be fixated.
                                                                           The second method used to compare scan patterns is Or-
                                                                        dered Sequence Similarity (note that despite its name, OSS
                                                                        is in fact a dissimilarity measure). Its main advantage is that
                                                                        it can be used with sequences of different lengths, and has
                                                                        shown to be more effective than established measures such
                                                                        as edit distance (Gomez and Valls, 2009). OSS is based on
                                                                        two aspects of sequential data: the elements the sequence is
                                                                        composed of, and their positions. When comparing two se-
                                                                        quences, it takes into account the number of common ele-
                                                                        ments and their relative order. The first step is to find target
                                                                        objects that occur in both scan patterns. For example in Fig-
                                                                        ure 2, four objects are shared by the two scan patterns (man,
                                                                        plant, statue, suitcase). For each common element, we calcu-
Figure 2: Example of how scan patterns are represented and normal-
ized (for VR only); and how measures of scan pattern similarity are     late the distance between the two sequences, e.g., statue of
computed                                                                scan pattern 1 is two units distant from statue in scan pat-
                                                                        tern 2. Distances are then summed and normalized on the ba-
                                                                        sis of sequence lengths (for details refer to Gomez and Valls
ments (in our case the British National Corpus). It indicates           2009).
how likely two words are to occur in the same document. Dif-               All four measures of similarity are computed pairwise, i.e.,
ferent from Hwang et al. (2009) where LSA is calculated be-             every trial (sentence and scan pattern) is paired with every
tween individual words, we implemented a version of LSA                 other trial. This resulted in a total of 164,164 pairs for each
generalized to compute the similarity of sentences (Mitchell            region of analysis, i.e., Before and During production.
and Lapata, 2009). We compute an LSA vector for each con-
tent word in the sentence (context window of size five; low                                          Analysis
frequency words are removed) and then combine these vec-
                                                                        To analyze the correspondence between sentences and scan
tors using addition to obtain a sentence vector (an alternative
                                                                        patterns, we divide the data into two regions: Before speech
discussed by Mitchell and Lapata 2009 would be vector mul-
                                                                        onset, and During production. The Before region provides ev-
tiplication). Similarity between sentence vectors is measured
                                                                        idence about the process of utterance planning and visual in-
using cosine distance.
                                                                        formation retrieval, whereas During is informative about lin-
Scan Pattern Measures We use two measures to compute                    guistic encoding and the utilization of visual information dur-
the similarity between scan patterns: Visual Recurrence (VR)            ing this process. We perform two types of analysis: descrip-
and Ordered Sequence Similarity (OSS, Gomez and Valls                   tive and inferential.
2009).                                                                     In the descriptive analysis, we investigate the data at two
   We consider scan patterns as temporally ordered sequences            levels: (1) globally, i.e., by performing comparisons between
of fixated target objects. Each trial is therefore encoded as a         all pairs of trials in the full data set, and (2) locally, i.e., by
sequence of discrete time points, each annotated with the ob-           comparing only the trials that pertain to a given scene (24
ject fixated at that time, encoded numerically (see Figure 2).          in total). These two forms of analysis make it possible to
VR is a percentage measure of scan pattern similarity that              test whether the correspondence between sentences and scan
counts the frequency of looks to the same objects during the            patterns is scene specific. For comparison, we also report a
same time points between two scan patterns relative to its to-          baseline correlation (Humphrey and Underwood, 2008) that
tal length. For example, in Figure 2, we have two matches on            is obtained by pairing sentences and scan patterns randomly
a total of seven time points, i.e., 25.87% agreement between            (rather than pairing the scan patterns with the sentences they
the scan patterns.                                                      belong to).
   VR can only compare scan patterns equal in length. We                   We quantify the strength of the correspondence between
therefore normalize each scan pattern (SPold ) by mapping               similarity measures by computing Spearman’s ρ for all pairs
it onto a normalized time course of fixed length (SPnew ).              of measures. We do not report coefficients for the baselines,
The length of SPnew is set on the basis of the shortest eye-            as they are not significant across all combined measures:
movement sequence found across all participants. For each               ρ ≈ 0.002; p > 0.1. For the correlation analysis, we also con-
                                                                    1936

sider a variant of the Feature Dissimilarity measure for which
we remove the contextual features (FD-C). This makes it pos-
sible to investigate the contribution of scenario and animacy
of the cue word to the correspondence between scan pattern
and sentence similarity.
   The distinction we made between global and local similar-
ity has implications for the nature of correspondence. A cor-
relation found globally (across all scenes) would imply that
scan patterns are partially independent from the precise lay-
out of the scene, i.e., position of the objects, etc., as these fac-
tors varied across scenes, but rather dependent on the categor-
ical structure shared, i.e. the visual referents common across
scenes. A correlation found at the local level would be con-
sistent with well-known scene-based effects, both bottom-up
and top-down, which guide visual attention (Itti and Koch,
2000; Humphrey and Underwood, 2008).
   In the inferential analysis, we apply linear mixed effects
modeling (LME) (Baayen et al., 2008) using the R-package
lme4. We use scan pattern similarity as the dependent vari-
able (fitting a separate model for OSS and VR) and sentence
similarity (FD and LSA) as predictors. The region of analy-
sis (before or after) is also included as a factor. As random
effects, we included participants and trials.1 All fixed factors         Figure 3: Correlation between LSA similarity, Feature dissimilarity
                                                                         (FD) and Ordered Sequence Similarity (OSS)
were centered to reduce collinearity. The models are built fol-
lowing a forward step-wise procedure. We start with an empty
model, then we add the random effects. Once all random ef-               ities during sentence planning, compared to sentence encod-
fects have been evaluated, we proceed by adding the predic-              ing. During planning, visual attention integrates the categor-
tors. The parameters are added one at time, and ordered by               ical information of the scene with the linguistic referents se-
their log-likelihood improvement of model fit: the best pa-              lected as arguments of the sentence. When production starts,
rameter goes first. Every time we add a new parameter to                 detailed information is sourced from the visual processor to
the model (fixed or random), we compare its log-likelihood               drive encoding, thus triggering more specialized routines of
against the previous model. We retain the additional predic-             visual information retrieval.
tor if log-likelihood fit improves significantly (p < 0.05). The
                                                                            Figure 4 plots local similarity values, i.e., values computed
final model is therefore the one that maximizes fit with the
                                                                         separately for each scene (OSS against LSA)3 . Generally, the
minimal number of predictors.
                                                                             3 Again, for space limitation, we can show only one pair of com-
                   Results and Discussions                               bined measures, OSS/LSA. However, we observe a similar trend for
                                                                         all the other pairs.
Figure 3 plots the linguistic similarity measures LSA and FD
against the scan pattern similarity measure OSS2 , computed
globally, i.e. across all scenes. We bin the data on the x-axis
and include 95% confidence intervals. The plots also include
the random baseline.
   For both linguistic measures, we observe a clear trend be-
tween sentence and scan pattern: when LSA similarity in-
creases, scan pattern dissimilarity decreases; when feature
dissimilarity (FD) increases, OSS also increases. This effect
is observed both Before and During region, but not in the ran-
dom baseline.
   We also observe a difference in the intercept between the
Before and During region. In the Before region, there is less
dissimilarity between scan-patterns overall. This could indi-
cate a higher degree of coordination between the two modal-
    1 Similarity is calculated pairwise. Thus, we need to include as
random variables two participants and two trials for each pair.          Figure 4: Scan pattern dissimilarity (OSS) as a function of the Lin-
    2 For reason of space, VR is shown only in the LME results.          guistic Similarity (LSA) across all 24 scenes
                                                                     1937

Table 1: Correlations (Spearman ρ) between the different similarity
measures. Before and During aggregated
       Measures VR              OSS        FD         LSA
       OSS         −0.63 ∗∗∗
       FD          −0.07∗∗∗ 0.15∗∗∗
       LSA           0.15∗∗∗ −0.10∗∗∗ −0.06∗∗∗
       FD-C        −0.02∗∗ 0.01∗           0.86∗∗∗ −0.10∗∗∗
Table 2: Minimum and Maximum correlations (Spearman ρ) across
different scenes between the different similarity measures
           Measures              VR       OSS       FD
                        min   −0.10                                     Figure 5: Predicted values of the linear mixed effects model: linguis-
           OSS                                                          tic similarity predicted by scan pattern similarity
                        max   −0.56
                        min   −0.01 −0.02                               Table 3: LME coefficients. The dependent measures are: OSS and
           FD                                                           VR. The predictors are: Region (contrast coding: Before = −0.5;
                        max   −0.55       0.44                          During = 0.5) and the Linguistic Measures (LM) FD or LSA. Each
                        min      0.01 −0.001 −0.52                      column shows which linguistic/scan pattern similarity measure is
           LSA                                                          compared
                        max      0.33 −0.30 −0.79
                                                                          Predictor        FD/OSS FD /VR LSA/OSS LSA/VR
                                                                          Intercept         0.0879∗∗∗ 18.95∗∗∗        0.639∗∗∗ 18.97∗∗∗
trend previously observed at the global level is confirmed,                                       ∗∗∗          ∗∗∗
                                                                          Region            0.062 −0.907              0.062∗∗∗ −0.906∗∗∗
both for the Before and the During region, though there is
some variation in the degree of association between scan pat-             LM                0.087∗∗∗ −6.151∗∗∗ −0.104∗∗∗           5.953∗∗∗
                                                                          LM:Region −0.083        ∗∗∗   4.866  ∗∗∗    n.sig.     −2.687∗∗∗
terns and linguistic similarity across scenes.
   Table 1 lists the correlation coefficients for all pairs of sim-
ilarity measure. There are weak but significant correlations
across all measures. In particular, both VR and OSS are sig-               Turning now to the inferential analysis, Figure 5 plots LME
nificantly correlated with both FD and LSA in the direction             predicted values calculated globally for all pairs of measures.
expected, i.e., positively in case of dissimilarity and nega-           The models closely follow the empirical patterns in Figure 3.
tively in the case of similarity. Between the two scan pat-             Table 3 lists the coefficients of the mixed models; we find a
tern measures (OSS and VR), we observe a strong correlation,            significant main effect of scan pattern similarity for both FD
whereas the association between the two linguistic measures             and LSA, for both the OSS and the VR model. Moreover, we
(FD and LSA) is weak. We also observe that FD-C, the mea-               observe a main effect of region across all combined measure:
sure obtained by removing contextual information from FD is             for the Before region, sentence similarity is more strongly re-
highly correlated with FD, but the removal of contextual in-            lated to scan pattern similarity, compared to the During re-
formation weakens the correlation with the scan pattern mea-            gion.
sures. On the other hand, FD-C is somewhat more strongly                   Furthermore, we observe an interaction of region of anal-
correlated with LSA than FD is. It seems that the contextual            ysis and linguistic similarity: for Before region, the similar-
information, even if at the level of the scenario, prominently          ity between sentence and scan pattern has a steeper change,
contribute to the prediction of scan pattern similarity.                compared to During. In linguistically driven visual planning,
   In Table 2, we show the minimum and maximum values of                we retrieve the referents going to be encoded. Thus, if two
the correlation coefficients for similarity measures observed           sentences are going to be very different, the set of referents
locally, i.e. computed trials aggregated by scene. As expected          chosen during visual planning is also going to be very dif-
from the plots in Figure 4, correlation coefficients vary across        ferent. During encoding instead, the visual system is already
scenes for all pairs of measures. The context of the individ-           sourcing detailed information in a sentence specific way, thus
ual scenes modulates the correspondence between scan pat-               the magnitude of change is smaller compared to planning.
terns and linguistic productions. Compared to the global co-
efficients, the most noticeable difference is a strengthening of                              General Discussion
the correlation between the two linguistic measures FD and              A range of cognitive modalities are involved in everyday
LSA. It seems that in a scene context, the semantic informa-            tasks, which raises the questions to which extend these
tion expressed by LSA more directly matches the information             modalities are coordinated. In this paper, we focused on two
in FD, which also includes verb semantics and scenario infor-           particular aspects of this coordination: linguistic structure and
mation.                                                                 visual attention during sentence production. Our main hy-
                                                                    1938

pothesis was that similarity of scan patterns predict the simi-          based measures of semantic distance. Computational Lin-
larity of sentences.                                                     guistics, 32(1):13–47.
   We tested this hypothesis using a dataset from an eye-             Castelhano, M., Mack, M., and Henderson, J. (2009). View-
tracking experiment in which participants had to describe                ing task influences eye movement control during active
photo-realistic scenes. We paired each sentence produced                 scene perception. Journal of Vision, 9(3)(6):1–15.
with the corresponding scan pattern, and computed similar-            Coco, M., I. and Keller, F. (2010). Sentence production in
ity measures for both modalities. We used Visual Recurrence              naturalistic scene with referential ambiguity. In R. Catram-
and Ordered Sequence Similarity to compare scan patterns,                bone & S. Ohlsson (Eds.), Proceedings of the 32th Annual
while for sentences we used a semantic similarity measure                Conference of the Cognitive Science Society, Portland.
based on LSA and a feature dissimilarity measure that com-            Gomez, C. and Valls, A. (2009). A similarity measure for
bines syntactic, semantics, and contextual information.                  sequences of categorical data based on the ordering of
   Both descriptive and inferential analysis confirmed our hy-           common elements. Lecture Notes in Computer Science,
pothesis: if two trials involve similar scan patterns, then the          5285/2009:134–145.
sentences produced in these two trials are also similar. This         Gower, J. (1971). A general coefficient of similarity and some
was true for all pairs of linguistic and scan pattern similar-           of its properties. Biometrics, 27:623–637.
ity measures. Furthermore, we subjected the data to a global
                                                                      Henderson, J., M. (2003). Human gaze control during real-
analysis (i.e., we computed similarity across different scenes)
                                                                         world scene perception. Trends in Cognitive Sciences,
and a local analysis (i.e., we only compared scan patterns
                                                                         7:498–504.
and sentences within the same scene). Significant correla-
tions were found in both cases, which suggests that the cor-          Humphrey, K. and Underwood, G. (2008). Fixation se-
respondence between sentences and scan patterns cannot be                quences in imagery and in recognition during the process-
explained as a simple mapping between individual scene con-              ing of pictures of real-world scenes. Journal of Eye Move-
tent and the objects mentioned in the corresponding sentence.            ment Research, 2(2):1–15.
This conclusion is confirmed at the level of individual scenes,       Hwang, A., Wang, H., and Pomplun, M. (2009). Semantic
where the variability observed suggests the presence of dif-             guidance of eye movements during real-world scene in-
ferent visual and linguistic factors modulating the strength of          spection. In N.A. Taatgen & H. van Rijn (Eds.), Proceed-
the correspondence.                                                      ings of the 31th Annual Conference of the Cognitive Sci-
   An important point emerged during our analysis regard-                ence Society, Amsterdam.
ing the role of contextual information in predicting similarity.      Itti, L. and Koch, C. (2000). A saliency-based search mecha-
When contextual features were removed from the linguistic                nism for overt and covert shifts of visual attention. Vision
measure, the strength of the correlation was reduced (but was            Research, 40(1):1489–1506.
still significant). Even though our contextual features were          Knoeferle, P. and Crocker, M. (2006). The coordinated inter-
not scene specific, but rather pertained to more general sce-            play of scene, utterance and world knowledge. Cognitive
narios, they were still helpful in predicting scan patterns.             Science, 30:481–529.
   Within the broader context of cognition, our results indi-         Land, M. (2006). Eye movements and the control of actions
cate that in tasks demanding the interaction of vision and lan-          in everyday life. Progress in Retinal and Eye Research,
guage, where endogenous control plays an essential role, they            25:296–324.
synchronize processing through coordination over a shared             Mitchell, J. and Lapata, M. (2009). Language models based
categorical interface.                                                   on semantic composition. Proceedings of the 2009 Confer-
   Ongoing work is currently investigating the sequential and            ence on Empirical Methods in Natural Language Process-
temporal aspects of the correspondence using alignment tech-             ing, pages 430– 439.
niques borrowed from bio-informatics. Preliminary results             Noton, D. and Stark, L. (1971). Eye movements and visual
show that the inclusion of temporal information together with            perception. Scientific American, 224(1):34–43.
a more stringent analysis of sequential data increase correla-        Tanenhaus, M., Spivey-Knowlton, M., Eberhard, K., and Se-
tion between sentences and scan patterns.                                divy, J. (1995). Integration of visual and linguistic in-
   Finally, in future work we plan to investigate a range of lin-        formation in spoken language comprehension. Science,
guistic features separately, thus enabling us to establish which         (268):632–634.
aspects of scan patterns predict syntactic, semantic, or con-
                                                                      Toutanova, K. and Manning, C. (2000). Enriching the knowl-
textual aspects of sentence production.
                                                                         edge sources used in a maximum entropy part-of-speech
                                                                         tagger. In Proceedings of the Joint SIGDAT Conference on
                         References                                      Empirical Methods in Natural Language Processing and
Baayen, R., Davidson, D., and Bates, D. (2008). Mixed-                   Very Large Corpora (EMNLP/VLC-2000).
   effects modeling with crossed random effects for subjects          Yang, H. and Zelinsky, G. (2009). Visual search is guided to
   and items. Journal of Memory and Language, 59:390–412.                categorically-defined targets. Vision Research, 49:2095–
Budanitsky, A. and Hirst, G. (2006). Evaluating wordnet-                 2103.
                                                                  1939

