UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Likability-Based Genres: Analysis and Evaluation of the Netflix Dataset
Permalink
https://escholarship.org/uc/item/20t3q21z
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Author
Olney, Andrew
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

         Likability-Based Genres: Analysis and Evaluation of the Netflix Dataset
                                            Andrew M. Olney (aolney@memphis.edu)
                                         Institute for Intelligent Systems, 365 Innovation Drive
                                                         Memphis, TN 38152 USA
                              Abstract                                  such normative characterizations to consider genre in terms
   This paper describes a new approach to defining genre. A             of sociocultural effects between film, audience, and author.
   model is presented that defines genre based on likability rat-       However, even in a more elaborated perspective, there are
   ings rather than features of the content itself. By collecting       a number of outstanding issues in genre theory, which can
   hundreds of thousands of likability ratings, and incorporating
   these into a topic model, one can create genre categories that       loosely be divided into problems of definition and problems
   are interesting and intuitively plausible. Moreover, we give         of analysis.
   evidence that likability-based features can be used to predict          Problems of definition in genre theory include circularity
   human annotated genre labels more successfully than content-
   based features for the same data. Implications for outstanding       and the monolithic assumption (Stam, 2000). The problem of
   questions in genre theory are discussed.                             circularity arises when one tries to define a genre in terms of
   Keywords: Genre; topic model; Netflix; likability;                   features like those given in Table 1.
                          Introduction
                                                                           Table 1: Genre Features (Adapted from Chandler (1997)
Many web sites, e.g. Amazon, allow users to rate items
along several dimensions, the most common being likabil-                            Feature              Example
ity or overall satisfaction. These ratings allow other users                        Time                 Films of the 1930s
to roughly estimate their own probable satisfaction with the                        Author               Stephan King
item, leading to better item selection and better satisfaction                      Age of audience      Kid movie
with the web site itself. Moreover, the same rating informa-                        Technology           Animated
tion can be exploited by a website to make personalized rec-                        Star                 Sylvester Stallone
ommendations for the user producing the ratings. In theory,                         Director             Quentin Tarantino
highly accurate recommendations might influence the user to                         Structure            Narrative
purchase additional products, again leading to greater prof-                        Ideology             Christian
itability for the web site in question.                                             Culture of origin    Bollywood
   This process of tracking ratings and using ratings to make                       Subject matter       Disaster movie
personal recommendations often falls under the classification                       Location             Western
of “recommender system” or “collaborative filtering,” and is a
widely studied problem in the data mining/machine learning
field (Resnick & Varian, 1997). To assist the development of               A feature based analysis requires first assembling all the
new and better algorithms, some companies like Netflix have             films representative of that genre and then analyzing their
released large datasets containing hundreds of thousands of             features. However, gathering the films requires knowing their
ratings by hundreds of thousands of users (The Netflix Prize            genre in the first place, otherwise how would one know which
Rules, 2010). These datasets can be analyzed in multiple                films to assemble? A second problem of definition is the
ways, and an interesting perspective is to view them as a kind          monolithic assumption, in which a film is assumed to belong
of graph or social network. By viewing users as nodes and               to one and only one genre. While the monolithic assump-
items as edges, we can study how users are related to each              tion in some ways makes the task of genre definition simpler,
other through item connectivity. Conversely, we can study               it nevertheless ignores genres that are part of our public dis-
how items are related to each other through users who have              course, e.g. “romantic comedy.”
rated them. Another way of looking at this second scenario                 Genre theory is also plagued by problems of analysis.
is as “mass criticism” wherein each user is afforded the same           Some questions with regard to genre analysis of film are as
status as a critic, and the mass action of all critics determines       follows (Stam, 2000). First, are genres real or imagined? In
not only the overall value of the item (through ratings) but            other words, are they merely analytic constructs, or do they
also the association of an item with other items (through con-          have some status in the world. Second, are the number of
nectivity).                                                             genre categories finite or infinite? Third, are genres time-
   In film theory, criticism and genre theory are likewise inter-       less or are they culture-driven and therefore trendy? Finally,
twined (Stam, 2000), creating relationships between the value           are genres universal, or are they culturebound? As questions
of film and its taxonomic place. Intuitively, a film might be           about genre, these four questions are inherently tied back to
called, “a good comedy” or “a poor horror,” in the sense that           the definition of what genre is. Therefore to answer them, we
the genre defines a kind of rubric or context by which the              must first define genre.
film is evaluated. Genre theorists often attempt to go beyond              In this paper, we analyze the information implicit in user
                                                                     37

ratings to build a model of genre. Our study focuses on the           one first probabilistically samples a from the distribution of
ratings from the Netflix dataset, which we incorporate into a         topics, yielding a particular topic. One then probabilistically
probabilistic topic model (Griffiths, Steyvers, & Tenenbaum,          samples from the distribution of words associated with that
2007). Moreover, we show how the extracted genres can be              particular topic, yielding a word. This process can be re-
used to predict human annotated genres with better perfor-            peated to generate more words and more documents. Thus a
mance than typical features used by genre critics. That a             topic model specifies how to generate the observed data; how-
content-free analysis, based purely on likability ratings, can        ever a model may be fitted to existing data using probabilistic
predict genres is surprising and provocative. We argue that           inference. Briefly, this is accomplished by randomly initial-
the ability of a likability-based analysis to predict genre with      izing the model and then using Gibbs sampling to reestimate
more success than a traditional feature-based approach sug-           the model’s parameters, iteratively, until the model converges.
gests that likability ratings not only represent a new way of         For more details see Griffiths, Kemp, and Tenenbaum (2008).
considering genre, but they also represent a significant force           Though topic models have primarily been applied to text in
in shaping genre categories, a force that is possibly more sig-       the cognitive science community, the model itself is agnostic
nificant than the content itself.                                     to the underlying data it represents, so long as that data has a
                                                                      form consistent with the assumptions of the model. One gen-
                     Study 1: Modeling                                eralization of these assumptions would be as follows: data
Method                                                                consists of a set of samples, each sample has a distribution of
                                                                      topics, and each item in the sample is generated from one of
The data used in this study consisted of the Netflix dataset,         these topics. It doesn’t matter whether the samples are doc-
which is freely available online (The Netflix Prize Rules,            uments or whether the items are words. Using this intuition,
2010). The dataset has a collection of information applicable         it is fairly straightforward to map the Netflix dataset into a
to both training a model as well as evaluating the model using        form consistent with the topic model. Indeed there are alter-
the Netflix API (The Netflix Prize Rules, 2010). In this study        nate mappings (Rubin & Steyvers, 2009), but in what follows
and succeeding studies, only the training data was used. The          we will only consider one.
training data consists of two logical components. The first is a         Our mapping is as follows. Each customer is a mixture
master file which lists for each movie a unique id, along with        of genres, and each genre is a distribution over movies. To
the title and release year for the movie. The second compo-           transform the existing Netflix dataset using this mapping, we
nent is a folder which contains, for each movie id, the set of        collect all of the movies seen by a customer. The number of
ratings given to that id by various users. Each rating is a triple    stars given that movie is represented by the number of times
consisting of user id, rating, and date of rating. Each rating        that movies label appears. For example, if a customer had
is an integral number from 1 to 5. There are 17,770 movies            only rated the movie “Whale Rider” and gave it three stars,
in the dataset, 480,189 users, and 100,480,507 ratings. The           then the customer would be represented as (Whale Rider,
dataset is sparse, meaning that not every user has rated every        Whale Rider, Whale Rider), analogous to a document con-
movie.                                                                taining the same word three times. Under the assumptions of
   Topic models (Griffiths & Steyvers, 2002; Griffiths et al.,        this mapping and the underlying topic model, each star in a
2007), also known in other communities as Latent Dirichlet            customer’s rating can generated by a different genre. For ex-
Allocation (Blei, Ng, & Jordan, 2003), are a class of genera-         ample two stars of “Whale Rider” might be generated by the
tive statistical models typically applied to text. Topic models       drama genre, and one star might be generated by the foreign
use “bag of words” assumption, making them somewhat sim-              film genre.
ilar to methods such as latent semantic analysis (Landauer,              The inference algorithm to fit our model to the Netflix data
Foltz, & Laham, 1998; Landauer, McNamara, Dennis, &                   is identical to that used in typical topic models. However,
Kintsch, 2007), however there are significant differences.            given the large size of the dataset and the widespread avail-
Rather than reduce the dimensionality of the data according           ability of multi-core processors, we have created and make
an optimal least-squares approximation, topic models use a            publicly available our code for fast parallel topic models in
probabilistic model that assumes the data was generated by            the C# language 1 . Inference parameters were as follows.
an underlying process involving hidden variables. Thus while          The number of topics was 50, the prior for topics appearing
LSA expresses the data along latent dimensions, i.e. singu-           in a document (α) was 1, and the prior for words appearing in
lar vectors, which have no clear semantic interpretation, topic       a topic (β) was 0.01. The α and β smoothing parameters are
models express the data according to the topics that gener-           typical (Steyvers & Griffiths, 2007). The model was run for
ated the data, and these topics are expressed as a collection         200 iterations.
of semantically related words, i.e. the words that are most
probable given a topic.                                               Results
   More specifically, the standard topic model makes the fol-         An initial inspection of the genres found by the model reveals
lowing assumptions. For each document, there is an associ-            intuitive categories, as displayed in Table 2. The intuitive
ated distribution of topics. Each of these topics has an asso-
ciated distribution of words. Thus to generate a document,                1 http://andrewmolney.name
                                                                   38

                                                      Table 2: Selected Genres.
     Genre 1                     Genre 2                     Genre 3                               Genre 4
     Bowling for Columbine       The Mummy Returns           Spirit: Stallion of the Cimarron      My Big Fat Greek Wedding
     Fahrenheit 9/11             Bad Boys II                 Brother Bear                          Sweet Home Alabama
     Whale Rider                 Face/Off                    Treasure Planet                       How to Lose a Guy in 10 Days
     Super Size Me               Behind Enemy Lines          The Lion King 1 1/2                   Pretty Woman
     Hotel Rwanda                Tomb Raider                 Stuart Little 2                       Legally Blonde
     Maria Full of Grace         The Fast and the Furious    Garfield: The Movie                   Two Weeks Notice
     City of God                 Rush Hour 2                 Spy Kids 2                            When Harry Met Sally
     The Motorcycle Diaries      Gone in 60 Seconds          Home on the Range                     Bridget Jones’s Diary
     Spellbound                  XXX: Special Edition        Scooby-Doo 2                          13 Going on 30
     Rabbit-Proof Fence          The Mummy                   SpongeBob SquarePants                 The Wedding Planner
appeal of these genres is consistent with word-based topics
                                                                                           Table 3: IMDB Genres.
presented in the topic model literature (Steyvers & Griffiths,
2007). Each genre list is rank ordered by probabilistic mem-            Documentary        Animation     Family         Sport
bership. Therefore the first ranked film in each genre is the           Crime              Drama         Mystery        Action
most probable film given that genre, and so on. This ranking            Sci-Fi             Comedy        Short          Game-Show
is derived from the φ matrix of the topic model (Steyvers &             Romance            Fantasy       Adventure      Music
Griffiths, 2007).                                                       Thriller           Biography     History        Musical
    Consistencies in Table 2 are evident. For example, Genre            Horror             Adult         War            Film-Noir
1 could be considered documentaries or biographically in-               Reality-TV         Western       Talk-Show      News
spired independent films. Genre 2 consists of action films
that veer towards the fantastic. Genre 3 is made up of ani-
mated films directed at children. And Genre 4 lists romantic         and Music. How these genre labels were generated for IMDB
comedies. However, inconsistencies are also apparent. For            is not clear, and interrater reliability for these genres is not
example is “Bad Boys II” really as fantastic as a film about         available. The task of correspondence is then to match up ev-
mummies? Or are Michael Moore films really that much like            ery film in the Netflix dataset (which contains all the likability
“Whale Rider”? Under this critical view, what can be gleaned         ratings) with the genres in the IMDB dataset. Unfortunately,
from Table 2 is somewhat mixed. On the one hand, it is clear         this is less straightforward than it might first appear. The Net-
that some sense of genre can be driven by likability ratings         flix dataset is intentionally sparse, including only title, year,
alone. On the other, it is unclear to what extent these ratings-     and ratings for each film.
driven genres correspond to typical film genres. Without a
                                                                         IMDbPy is the Python-based software library used for ma-
correspondence-based evaluation, it is unclear whether the
                                                                     nipulating the IMDB data (IMDbPy, 2010). IMDbPy pro-
genres in Table 2 represent strong coherent categories or an
                                                                     vides a search capability for querying a particular title. This
observer bias towards any category that might make them co-
                                                                     search capability purposely returns more than single title in
herent.
                                                                     order to accommodate alternate title forms. Using IMDbPy,
    Study 2: Correspondence-based Evaluation                         a correspondence requiring an exact match of both year and
                                                                     title yields only 8,283 exact matches out of a possible 17,770.
Method                                                               Relaxing the exact match requirement so that years match
To carry out a correspondence-based evaluation of our model,         and titles match up to the colon yields an additional 1,082
it is necessary to find a large existing dataset with human an-      matches.
notated genres for each movie. Fortunately such a dataset                Inspection of the data reveals that failures to match have a
exists and is freely available: the Internet Movie Database          variety of reasons. First, typographic conventions differ be-
(IMDB). IMDB contains an enormous amount of informa-                 tween datasets, such that a foreign film may have its original
tion for a given film, ranging from the director and year of         title spelling in one dataset and an Anglicized title in another,
release to less commonly known information such as the art           e.g. “Character” and “Charackter.” In addition, year informa-
department. Including amongst the hundreds of pieces of in-          tion may be off by one between the two databases. Sequels
formation associated with each movie is a set of 28 genres,          and series are a particular problem, such that one database
listed in Table 3.                                                   may precede the name of an episode with the name of the
    Each film in IMDB is associated with one or more of the          series, whereas the other does not. Some errors also exist in
genres in Table 3. For example, the biopic, “Ray,” based on          the matched films. It is possible, though rare, for two films
the story of Ray Charles, is labeled with Biography, Drama,          to be released in the same year with the same name. For
                                                                  39

example, “Ray,” the biopic of Ray Charles, appeared in the
                                                                                       Table 5: IMDB Features.
same year as a genre short of the same name. Finally, be-
cause to the inconsistencies with series naming conventions                             Feature     Type
and the partial match strategy described above, some within-                            Plot        NUMERIC
genre mismatches can occur, e.g. “Star Trek: Insurrection”                              Title       NUMERIC
and “Star Trek: First Contact.” However, the distribution of                            Actor1      NOMINAL
genres is very similar in both the matched set and the original                         Actor2      NOMINAL
set, as shown in Table 4. Additionally, the correlation be-                             Director    NOMINAL
tween the proportional distributions for original and matched                           Year        NUMERIC
sets is .978.                                                                           MPAA        NOMINAL
                                                                                        Genre       NOMINAL
                 Table 4: Proportion of Genres.
            Genre               Matched    Original
                                                                      A few features of Table 5 warrant brief remarks. Plot is a
            Action              0.14       0.12                    plot synopsis of the film. The two actor features are the first
            Adult               0          0.02                    and second named actors on the billing, i.e. the stars of the
            Adventure           0.04       0.04                    film. MPAA is the rating of the film, e.g. PG-13. The other
            Animation           0.04       0.05                    features are self-explanatory.
            Biography           0.03       0.02
                                                                      Some of these features are nominal, such as actor and di-
            Comedy              0.24       0.2
                                                                   rector names, meaning that they are associated with a fixed
            Crime               0.06       0.05
                                                                   set of labels as is genre in Table 3. However, the IMDB plot
            Documentary         0.08       0.1
                                                                   synopsis is an arbitrary string of considerable length, e.g. 500
            Drama               0.21       0.19
                                                                   words, and the title is a shorter but equally arbitrary string.
            Family              0.02       0.02
                                                                   In order to be usable features that two films could have in
            Fantasy             0.01       0.01
                                                                   common, both plot and title were transformed using term fre-
            Film-Noir           0          0
                                                                   quency/inverse document frequency such that each word in
            Game-Show           0          0
                                                                   the string became its own feature. This large set of features
            History             0          0
                                                                   was considerably pruned using stop words and stemming, so
            Horror              0.05       0.04
                                                                   that only 1,420 features remained. The WEKA command line
            Music               0.02       0.02
                                                                   used to convert plot and title to these numeric features was
            Musical             0.01       0.01
                                                                   “StringToWordVector -R1,2 -W100 -prune-rate-1.0 -C -T -I
            Mystery             0.01       0.01
                                                                   -N0 -L -S -SnowballStemmer -M1 -WordTokenizer”.
            News                0          0
                                                                      In both the first and second sets, the genre class to be pre-
            None (missing)      0          0.05
                                                                   dicted is the first genre listed by IMDB. This restriction is
            Reality-TV          0          0
                                                                   due to WEKA’s inability to perform multi-class classifica-
            Romance             0.01       0.01
                                                                   tions, and implies that overall performance of the models is
            Sci-Fi              0.01       0.01
                                                                   significantly lower than would be the case if any genre label
            Short               0.01       0.03
                                                                   associated with a movie was permitted as a correct answer.
            Sport               0          0
            Talk-Show           0          0                          The two differing data formats is what separates the first
            Thriller            0.02       0.01                    and second sets of models. Within each set, the same machine
            War                 0          0                       learning algorithms were used to predict genre. These include
            Western             0.01       0.01                    the following five models. First, ZeroR, which predicts the
                                                                   most prevalent class, e.g. Comedy. Secondly, NaiveBayes,
                                                                   which assumes features are independent and uses Bayes Rule
   Once the 9,249 films were paired, the WEKA toolkit (Hall        to construct a classifier. Thirdly, AdaBoostM1 uses an en-
et al., 2009) was used to build two sets of predictive models.     semble of weak learners, in this case a decision stump, using
The first set uses as features only the distribution of topics     the boosting approach (Schapire, 2003). Fourthly, J48 is a
associated with each movie, a row vector. For example, posi-       decision tree whose internal branching on attribute values is
tion 1 would be the probability that a movie belongs in genre      constructed to maximally discriminate amongst the training
1, position 2 to probability a movie belongs in genre 2, and so    data. And finally, Ibk is an instance/prototype based clas-
on for all 50 genres. The second set of models uses as features    sifier, i.e. k nearest neighbors where k has been set to 10
a collection of information from IMDB, chosen to best match        neighbors. These five algorithms were selected because they
the features sometimes used by film critics to determine the       represent a cross section of the most widespread and effective
genre of a film, as described in Table 1. These features are       machine learning techniques (Wu et al., 2007).
listed in Table 5.                                                    Each model was trained using 10 fold cross validation in
                                                                40

which the dataset is divided into ten bins, and the model                The topic model we use makes very few assumptions, and
trained 10 times, using a different bin as test data each time.       yet the assumptions it does make are quite strong. The basic
Significant differences were measured using a paired samples          premise of the model is that people are a mixture of genres.
t-test, p = .05, corrected for the variability introduced by cross    These genres, in turn, generate the ratings observed. To claim
validation (Nadeau & Bengio, 2003).                                   that people are a mixture of genres, when genres are typically
                                                                      considered to be a property of artifacts, is a strong and radical
Results                                                               claim. The results of the two studies presented above not
The results of the predictive models are displayed in Table 6.        only support this claim but also suggest that it should be taken
Numbers shown indicate percent correct, aggregated across             seriously as a new approach to genre.
all genre categories. All significant differences are relative to        Suppose that likability-based genres are taken seriously.
the ZeroR model for each set.                                         Are they useful, particularly in regard to existing genre stud-
                                                                      ies? The current focus on film suggests that they are. Recall
                                                                      the complementary problems of genre definition and analysis
               Table 6: Results in Percent Correct.                   discussed in the introduction. Using likability-based genres
  Model                    Likability Based      Content Based        as a framework, these can be addressed straightforwardly.
  rules.ZeroR              23.51                 23.51                   As before, the problems of definition include circularity
  bayes.NaiveBayes         9.94                  27.12                and the monolithic assumption (Stam, 2000). The basic prob-
  meta.AdaBoostM1          23.96                 23.51                lem of circularity lies in a supervised approach in which a
  trees.J48                37.30                 29.21                critic tries to align film features with a given genre cate-
  lazy.IBk                 41.22                 27.50                gory. A likability-based model, as an unsupervised model,
                                                                      avoids this problem entirely because their is no initial as-
                                                                      sumption of genre used to define the features of genre. In-
                                                                      stead, genre emerges from genre-agnostic likability ratings.
   Interestingly there is a fair distribution of performance          The second problem of definition, the monolithic assumption,
across all models for the first set (likability-based genres).        is addressed by the structure of the topic model. Under this
The worst performer is NaiveBayes, worse than the ZeroR               model, every movie has some probability of membership in
model, while the best performer is IBk-10, at 41%. All dif-           every genre. Study 2 above illustrates that it is not necessary
ferences in this first set are significant.                           to pigeonhole a movie into a genre in order to create meaning-
   Performance on the second set of models is worse than the          ful genres: even using a probabilistic definition of genre, one
performance on the first set. There is very little deviation          can still approximate the monolithic assumption to 41% ac-
away from ZeroR. All differences are significant, except Ad-          curacy. Pluralistic genres, like “romantic comedy,” are not a
aBoostM1, which is not significantly different from ZeroR.            special case but are represented in the same way as any other
The best model of the second set, J48, has only 29% accuracy          genre.
compared to 41% for IBk in the first set. This performance is            Using the likability-based definition of genre, we can also
particularly poor considering the base rate (ZeroR) is 23%.           clarify problems of analysis that have been raised (Stam,
   Two important points are clear from this data. The first           2000). First, are genres real or imagined? According to our
is that the likability-based genres are indeed strong and co-         approach, genres are only manifested through people’s pref-
herent, predicting the correct human annotated label in 41%           erences. Therefore they do not have any status in the world
of cases. The second is that the likability-based features are        except as a consensus of preferences across large groups of
more successful at predicting the human annotated label than          people. On whether the number of genre categories finite
are the content-based features.                                       or infinite, the structure of the topic model suggests that the
                                                                      number of genres is completely arbitrary, and is controllable
                           Discussion                                 using the parameter T , the number of topics. This suggests
Perhaps the most significant finding of both studies is that          that likability-based genres are potentially infinite. Third, on
genres can be extracted from just ratings. Although the per-          whether genres are timeless or are trendy, the likability-based
cent accuracy using just ratings is 41%, that is still a large        model suggests that they are trendy. Any new ratings that are
figure given two observations. The first is that the 41% perfor-      assimilated into the model can change the resulting genres.
mance is based on a single genre classification, when IMDB            As long as the people making the new ratings represent a new
allows multiple classifications. So 41% performance repre-            mixture of genres, the genres will shift towards the trendy. Fi-
sents the lowest, most conservative figure. The second obser-         nally, as to whether the genres are universal or culturebound,
vation is that the likability-based performance is considerably       one can speculate that they are culturebound to the extent that
higher than the content-based performance at 29%. This dif-           one culture may rate movies consistently differently from an-
ference suggests that likability-based genre classification is a      other culture. This is intuitively plausible, e.g. Bollywood
more accurate model of how humans classify film genres than           movies rated in India vs. the United States, and may be ac-
is content-based classification.                                      counted for in the same way as the timeless or trendy prob-
                                                                   41

lem.                                                                Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007).
   Likability-based genres also extend beyond the traditional         Topics in semantic representation. Psychological review,
conceptualization of genre and correspond to the notion of            114(2), 211–244.
intertextuality. In film, intertextuality has be described as       Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann,
having several properties (Stam, 2000). The first overarch-           P., & Witten, I. H. (2009). The weka data mining software:
ing property is that every film is necessarily related to ev-         an update. SIGKDD Explor. Newsl., 11(1), 10–18.
ery other film. Second, intertextuality is an active process,       Imdbpy.          (2010, February).          Available from
so rather than “belonging” to a genre, a film dynamically re-         http://imdbpy.sourceforge.net/index.php?page=main
lates to other films. Finally, intertextuality involves not only
all other films, but potentially other arts and media. Clearly      Landauer, T. K., Foltz, P. W., & Laham, D. (1998). Intro-
the likability-based model corresponds to each of these three         duction to latent semantic analysis. Discourse Processes,
properties, by being based on the connectivity amongst all            25(2&3), 259-284.
movies via ratings, using an active data-driven model, and          Landauer, T. K., McNamara, D. S., Dennis, S., & Kintsch, W.
using the abstract notion of rating, which can be applied to          (2007). Handbook of latent semantic analysis. Lawrence
heterogeneous items like film, music, and books simultane-            Erlbaum.
ously. Thus the likability-based model can apply to modern          Nadeau, C., & Bengio, Y. (2003). Inference for the general-
intertextual theories of media in addition to traditional no-         ization error. Mach. Learn., 52(3), 239–281.
tions of genre.                                                     The netflix prize rules. (2010, February). Available from
   In summary, likability-based genres offer a novel and use-         http://www.netflixprize.com/rules
ful way of considering genre: people are a mixture of genres.       Resnick, P., & Varian, H. R. (1997). Recommender systems.
Likability-based genres can predict a significant percentage          Commun. ACM, 40(3), 56–58.
of genres in the Netflix dataset. Moreover, likability-based        Rubin, T., & Steyvers, M. (2009). A topic model for movie
genres can also be used to address fundamental problems of            choices and ratings. In Proceedings of the ninth interna-
definition and analysis in film theory. Likability-based gen-         tional conference on cognitive modeling. Manchester, UK.
res can also be extended to broader frameworks than genre,          Schapire, R. E. (2003). The boosting approach to machine
such as intertextuality. However, likability-based genres as          learning: An overview. In D. D. Denison, M. H. Hansen,
described in this paper do not represent a complete theory. In        C. C. Holmes, B. Mallick, & B. Yu (Eds.), Nonlinear es-
order to understand this phenomenon fully, it is necessary to         timation and classification (Vol. 171, pp. 149–172). New
understand how the ratings themselves are generated as well           York: Springer Verlag.
as how likability-based genres manifest in other contexts.          Stam, R. (2000). Film theory: an introduction. Malden,
                                                                      Mass.: Wiley-Blackwell.
                    Acknowledgments                                 Steyvers, M., & Griffiths, T. L. (2007). Probabilistic topic
The research reported here was supported by the Institute of          models. In T. Landauer, D. McNamara, S. Dennis, &
Education Sciences, U.S. Department of Education,through              W. Kintsch (Eds.), Handbook of latent semantic analysis
Grant R305A080594 and by the National Science Founda-                 (pp. 424–440). Lawrence Erlbaum.
tion, through Grant BCS-0826825, to the University of Mem-          Wu, X., Kumar, V., Ross Quinlan, J., Ghosh, J., Yang, Q.,
phis. The opinions expressed are those of the authors and do          Motoda, H., et al. (2007). Top 10 algorithms in data min-
not represent views of the Institute, the U.S. Department of          ing. Knowl. Inf. Syst., 14(1), 1–37.
Education, or the National Science Foundation.
                         References
Blei, D., Ng, A., & Jordan, M. (2003). Latent dirichlet al-
   location. The Journal of Machine Learning Research, 3,
   993–1022.
Chandler, D. (1997). An introduction to genre the-
   ory. Available from http://www.aber.ac.uk/media/
   Documents/intgenre /chandler genre theory.pdf
Griffiths, T. L., Kemp, C., & Tenenbaum, J. B. (2008).
   Bayesian models of cognition. In R. Sun (Ed.), The Cam-
   bridge handbook of computational psychology (pp. 59–
   100). New York: Cambridge University Press.
Griffiths, T. L., & Steyvers, M. (2002). A probabilis-
   tic approach to semantic representation. In W. D. Gray
   & C. D. Schunn (Eds.), Proceedings of the 24th annual
   conference of the cognitive science society (pp. 381–386).
   Lawrence Erlbaum Associates.
                                                                 42

