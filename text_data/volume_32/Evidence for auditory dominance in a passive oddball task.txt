UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Evidence for auditory dominance in a passive oddball task

Permalink
https://escholarship.org/uc/item/9gg173d6

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Robinson, Chris
Ahmar, Nayef
Sloutsky, Vladimir

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Evidence for auditory dominance in a passive oddball task
Christopher W. Robinson (robinson.777@osu.edu)
Center for Cognitive Science
The Ohio State University
208F Ohio Stadium East, 1961 Tuttle Park Place
Columbus, OH 43210, USA

Nayef Ahmar (ahmar.1@osu.edu)
Center for Cognitive Science
The Ohio State University
208F Ohio Stadium East, 1961 Tuttle Park Place
Columbus, OH 43210, USA

Vladimir M. Sloutsky (sloutsky.1@osu.edu)
Center for Cognitive Science
The Ohio State University
208C Ohio Stadium East, 1961 Tuttle Park Place
Columbus, OH 43210, USA

information to one sensory modality interferes with
learning in a second modality. These modality
dominance effects can occur on detection tasks and
on more complex discrimination tasks, with auditory
input often attenuating visual processing in young
children (Sloutsky & Napolitano, 2003; Robinson &
Sloutsky, 2004) and visual input often attenuating
auditory processing in adults (Colavita, 1974;
Colavita & Weisberg, 1979).
Support for visual dominance in adults comes from
a long history of research examining how multimodal
stimuli affect the detection of auditory and visual
input (Colavita, 1974; Colavita & Weisberg, 1979;
Klein, 1977; Posner, Nissen, & Klein, 1976; see also
Sinnett, Spence, & Soto-Faraco, 2007; Spence,
Shore, & Klein, 2001, for reviews). For example, in a
classic study Colavita (1974) presented adults with a
tone, a light, or the tone and light paired together.
Participants had to press one button when they heard
the tone and a different button when they saw the
light. While participants were accurate when the tone
and light were presented unimodally, they often
responded to the visual stimulus when the stimuli
were paired together, with many adults failing to
detect the auditory stimulus. This finding has been
replicated using a variety of stimuli and procedures,
with little evidence demonstrating that auditory input
attenuates visual processing in adults (see Sinnett,
Spence, & Soto-Faraco, 2007 for a review).
There appears to be an attentional component
underlying visual dominance (Posner, Nissen, &
Klein, 1976). In particular, the underlying idea is that
the auditory and visual modalities share the same
pool of attentional resources. While auditory stimuli

Abstract
Simultaneous presentation of auditory and visual input can
often lead to visual dominance. Most studies supporting
visual dominance often require participants to make an
explicit response, therefore, it is unclear if visual input disrupt
encoding/discrimination of auditory input or results in a
response bias. The current study begins to address this issue
by examining how multimodal presentation affects
discrimination of auditory and visual stimuli, while using a
passive oddball task that does not require an explicit response.
Participants in the current study ably discriminated auditory
and visual stimuli in all unimodal and multimodal conditions.
Furthermore, there was no evidence that visual stimuli
attenuated auditory processing. Rather, multimodal
presentation sped up auditory processing (shorter latency of
P300) and slowed down visual processing (longer latency of
P300). These findings are consistent with research examining
modality dominance in young children and suggest that visual
dominance effects may be restricted to tasks that require an
explicit response.
Keywords:
Attention,
Cross-modal
Processing,
Electroencephalograph (EEG), Neurophysiology, Psychology.

Introduction
Most of our experiences are multimodal in nature. The
objects and events that we encounter in the environment
can be seen, touched, heard, and smelled. The fact that
the brain can integrate this knowledge into a coherent
experience is amazing given that each modality
simultaneously receives different types of input, and this
information is processed, at least in the early stages of
processing, by dedicated sensory systems.
While multimodal presentation can sometimes facilitate
learning, there are many occasions when presenting

2644

automatically engage attention, visual stimuli often have
poor alerting abilities. To compensate for the poor
alerting ability of visual input, adults endogenously direct
attention to visual stimuli. This increased attention to the
visual modality comes with a cost – attenuated auditory
processing.
While there is much support for visual dominance, it is
important to note that this support primarily comes from
studies examining response latencies and response
accuracies. Therefore, it is possible that visual input have
no effect on encoding or discrimination of auditory
stimuli. Rather, these effects may stem solely from visual
input dominating the response. The current study begins
to address this issue by examining processing of auditory,
visual, and multimodal stimuli in a task that does not
require an explicit response.
Participants in the current study were presented with a
passive oddball task where they were presented with
auditory, visual, or multimodal stimuli. Event Related
Potentials (ERPs) were recorded as adults passively
attended to frequent stimuli (standard) and infrequent
stimuli (oddballs). The signature pattern of discrimination
is a P300. P300 is a positive component with a peak
latency occurring between 300-800 ms after stimulus
onset and is strongest over the temporal, parietal, and
fronto-central regions (see Polich & Criado, 2006 for a
review). The amplitude of P300 is larger for novel or
infrequent stimuli (Sutton, Braren, Zubin, & John, 1965),
and the latency of P300 can be used as a measure of
processing time (Kutas, McCarthy, & Donchin, 1977). In
particular, experimental manipulations that affect the
processing leading up to classification and responding
should affect the latency of P300. The same underlying
idea is guiding the current research: multimodal
facilitation and interference should manifest themselves
by affecting the latency (and possibly amplitude) of P300.
Previous studies have used oddball tasks to examine
unimodal and multimodal processing and to examine
effects of response on ERP components. However, these
procedures differed from the ones reported here in several
important ways. First, ERP studies that have directly
compared unimodal and multimodal conditions either
focused on early ERP components associated with
stimulus detection or they required participants to make a
response to oddballs (e.g., Brown, Clarke, & Barry, 2007;
Fort, Delpuech, Pernier, & Giard, 2002; Giard &
Peronnet, 1999; Vidal, Giard, Roux, Barthelemy, &
Bruneau, 2008). In contrast, the current study focused
exclusively on discrimination of standards and oddballs
(P300), and participants did not make an explicit response
to these stimuli. Second, the studies that have examined
the effects of explicit response on oddball tasks were not
interested in modality dominance, thus, they did not
examine discrimination of the same auditory and visual
stimuli when presented unimodally and multimodally

(Mertens & Polich, 1997; Wronka, Kaiser, &
Coenen, 2008).
Thus, to the best of our knowledge this is the first
study to use a passive oddball task to examine how
multimodal presentation affects auditory and visual
processing. If visual stimuli interfere with the
encoding and/or discrimination of auditory stimuli,
then the latency of P300 should occur later in the
multimodal condition than in the unimodal condition.
However, if visual stimuli only affect the response,
then no effects should be found or auditory input may
attenuate visual processing (auditory dominance).

Methods
Participants
Thirty-nine undergraduate students from The Ohio
State University (23 men and 16 women, M = 19.5
years, SD = 3.9 years) participated in this experiment
for course credit. Prior to the experiment all
participants gave informed consent and provided
basic personal information (handedness, age, medical
history). All participants had normal hearing and
normal (or corrected to normal) vision.

Stimuli
The stimuli and cover story were designed for young
children. The visual stimuli consisted of six novel
creatures that were created in PowerPoint and
exported as 400 x 400 pixel jpeg images (see Figure
1 for examples).
**

**

AUD2

*

*
AUD1

AUD1

Time
AUD1

Unimodal
Visual

Unimodal
Auditory

AUD2

AUD1

AUD1

AUD1

Multimodal

Figure 1: Example of stimuli and overview of the
visual, auditory, and multimodal conditions. Note:
“*” denotes visual oddball and “**” denotes auditory
oddball.
Visual stimuli were presented centrally on a Dell
17” LCD monitor for 480 ms. The interstimulus
interval (ISI) randomly varied from 1000 ms - 1520
ms. Auditory stimuli were also 480 ms in duration,
with a 1000 ms - 1520 ms ISI. The auditory stimuli

2645

were dynamic sounds that changed in pitch and amplitude
across time. The sounds were created in CoolEdit 2000 by
using preset functions (e.g., DTMF signal, out of control,
etc.). Stimuli in the multimodal condition were
constructed by pairing the auditory and visual stimuli
together (see Figure 1 and Table 1). Thus, the same
stimuli were used in the unimodal and multimodal
conditions, therefore, any differences found between these
conditions cannot be accounted for by properties of the
unimodal stimuli.

the other creatures. In the auditory condition they
were told that they would hear the sounds of
creatures eating vegetables and cookies, and in the
multimodal condition they were told that they would
see creatures and hear the sounds that they make
while eating vegetables and cookies.

Standard
Oddballs (A)

Procedure
Three different oddball tasks were used (see Figure 1 and
Table 1), and task order was pseudo-randomized for each
participant. Approximately half of the participants were
presented with the unimodal oddball tasks (order of
auditory and visual was randomized for each participant),
and then they participated in the multimodal task. The
remaining participants were presented with the
multimodal task, followed by the two unimodal tasks
(order randomized for each participant). The multimodal
task took approximately 40 minutes, and each unimodal
task took approximately 20 minutes.
As can be seen in Table 1, each task consisted of one
standard (presented approximately 80% of the time), four
oddballs (each presented approximately 4% of the time),
and one novel (presented approximately 4% of the time).
Participants were instructed to press a button every time
they saw/heard the novel, and to not respond to the
standards and oddballs (see cover story). The novel trials
were presented to keep participants engaged, and ERPs
from these trials were discarded. Four oddballs were used
to keep the task interesting for participants and to
maintain a low probability of oddballs (each oddball was
only presented 4% of the time). In each of the unimodal
conditions there were four oddballs, and in the
multimodal condition, there were eight oddballs (four
auditory and four visual). To examine how multimodal
stimuli affect auditory processing, we compared auditory
oddballs in the silent condition (e.g., A2, A3, etc.) with
the same auditory oddballs in the multimodal condition
(e.g., A2V1, A3V1, etc.). To examine how multimodal
stimuli affect visual processing, we compared visual
oddballs in the silent condition (e.g., V2, V3, etc.) with
the same visual oddballs in the multimodal condition
(e.g., A1V2, A1V3, etc.).
Prior to each task participants were told a short cover
story. For example, in the unimodal visual task,
participants were told: You are going to see creatures
from a far away place. Most of the creatures that you will
see eat vegetables. However sometimes you will see this
creature (novel was presented). This creature eats
cookies. In this game you have to press a button every
time you see this creature that eats cookies (novel was
presented). Do not press any buttons when you see any of

Unimodal
Auditory
A1 (280)
A2 (16)
A3 (16)
A4 (16)
A5 (16)

Oddballs (V)

Novel

Unimodal
Visual
V1 (280)

A6 (16)

Multimodal
A1V1 (560)
A2V1 (16)
A3V1 (16)
A4V1 (16)
A5V1 (16)

V2 (16)
V3 (16)
V4 (16)
V5 (16)

A1V2 (16)
A1V3 (16)
A1V4 (16)
A1V5 (16)

V6 (16)

A6V6 (16)

Table 1. Overview of stimuli and tasks (frequency of
each stimulus).
Participants were presented with a warm up task
where they were given 10 standards and 3 novels.
ERPs from the warm up task were not included in the
final data. Feedback was provided throughout the
entire experiment. Feedback was provided if
participants: (a) responded to a standard or oddball or
(b) failed to respond to a novel. All data were
recorded with eyes open and participants in the
unimodal auditory condition were asked to fixate on
a square taped to the top of the LCD monitor.

Recording Conditions and Data Acquisition
Experiments were conducted in a sound-attenuated,
illuminated, and well-ventilated presentation chamber
which housed a Dell 17” monitor, two Polk
PLKRC65I wall mount speakers, and a response pad.
In the experimenter room, a Dell Optiplex 755
computer with E-prime software v.2.0.8.22 was used
to present stimuli to participants, and a Harman
Kardon AVR-154 receiver was used to amplify the
sounds. Timing tests were conducted to ensure that
auditory and visual stimuli were presented
simultaneously. Offsets between trigger registration
and stimuli presentation were measured for unimodal
and multimodal conditions and were adjusted during
analysis. A PowerPC G5 Mac with Netstation
software was used to record and store ERP data.
ElectroEncephalography (EEG) brain activity was
recorded using a 128-channel HydroCel Geodesic
Sensor Net (Electrical Geodesics, Inc., Eugene, OR).
Scalp-electrode impedances were kept below 50
kOhms. All channels were referenced to Cz during

2646

acquisition. EEG was recorded using a 0.1 to 100 Hz
band-pass filter (3 dB attenuation), amplified at a gain of
1000, sampled at a rate of 250 Hz, and digitized with a
24-bit A/D converter.

(standards vs. oddball) as a repeated measure. The
same analyses were conducted in the four conditions
(i.e., Unimodal Auditory, Unimodal Visual,
Multimodal Auditory, and Multimodal Visual). All
ANOVAs were significant, Fs > 20, ps < .0001.

Data analysis
Unimodal Auditory

Participants ably discriminated novels in all of the
conditions (proportion of hits to novels – proportion of
false alarms to standards + oddballs > .99). Because
auditory and visual components both changed on novel
trials and participants made a response, it is unclear if
ERP waveforms reflect auditory discrimination, visual
discrimination, or the response. Therefore, data from
novel trials were not included in any of the analyses.
ERPs to standards and oddballs were processed using
Netstation waveform tool. EEGs were band-passed
between 0.1 Hz and 30 Hz and segmented between 100
ms pre-stimulus onset and 1000ms post stimulus onset.
ERPs were referenced with respect to the average of all
channels after correcting for bad trials using neighboring
channels. Trials were then baseline corrected with respect
to the 100 ms pre-stimulus and then exported to Matlab.
Initially, we looked at 8 different scalp regions, each
comprising of 6 or 7 channels from the 10/20 system
representing: F3, F4, P3, P4, T3, T4, Pz, and Oz.
However, in the current study we focused exclusively on
Pz; the region that provided the best measure of
discrimination in all conditions (see Figure 2). In each of
the unimodal conditions, participants provided two ERP
waveforms (one for the standard and one for the oddball).
To equate the number of standards and oddballs, we
randomly picked and averaged 64 of the 280 standards
and we averaged across the four different oddballs. In the
multimodal condition, adults provided a waveform for the
standard, a waveform for auditory oddballs, and a
waveform for visual oddballs (see Table 1).

Multimodal Auditory

Standard
Oddball
Difference

Multimodal Visual

5
0

t

Microvolt

Unimodal Visual

-5
0

200 400 600
Time(ms)

800

Figure 2. ERP waveforms for Standards and Oddballs
across conditions. Solid line represents difference
waves (Oddball – Standard).
To examine the effects of visual input on auditory
discrimination, we compared the auditory difference
waveform in the multimodal condition to the auditory
difference waveform in the unimodal condition (see
Figure 3a). A one-way AVOVA revealed that mean
amplitude between 250-650 ms did not differ
between the unimodal and multimodal conditions.
We also examined how the presence of auditory input
affected visual discrimination by comparing the
visual difference waveform in the multimodal
condition to the visual difference waveform in the
unimodal condition (see Figure 3b). As in the
auditory conditions, mean amplitude did not differ
between the unimodal and multimodal conditions.
To statistically find and quantify any significant
displacement of P300 between unimodal and
multimodal conditions, we computed the fractional
area latency. In particular, for a predefined window,
we measured the area under the curve, and then we
found the latency that divided that area into two equal
parts (see Hansen & Hillyard, 1980). Using this
measure for a window between 250 ms and 650 ms,
we found that multimodal presentation sped up
auditory discrimination by 26ms and slowed down
visual discrimination by 12 ms.
However, as can be seen in Figure 3a, there are
multiple peaks in both auditory conditions that could
be the result of multiple underlying components.
Therefore, we ran sliding windows of 200 ms, 300
ms, and 400 ms for each participant’s data covering
the whole time range of interest (250ms to 650ms).
That is, for each window length, centered at a time
sample, we computed the 50% area latency for both

Results and Discussion
A reliable P300 was found at Pz, P3, and P4, however, as
mentioned above, discrimination was most pronounced at
Pz. Thus, analyses reported below focus on Pz between
250-650 ms after stimulus onset. Waveforms for the
unimodal conditions are presented on the left side of
Figure 2 and waveforms for the multimodal conditions are
presented on the right side of Figure 2. As can be seen in
Figure 2, participants ably discriminated auditory and
visual stimuli when presented unimodally and
multimodally. Mean averages were computed for each
participant. For example, to assess discrimination of the
auditory stimuli in the unimodal auditory condition, we
computed a mean average for the standard (between 250650 ms) and a mean average for the oddball (between
250-650 ms) for each participant. These means were then
submitted to a one-way ANOVA with trial type

2647

the multimodal difference waveform and for the unimodal
difference waveform. We then calculated a difference
wave (Difference Multimodal – Difference Unimodal) to
denote the displacement. We kept doing this while sliding
the window at 4ms increments. Figure 4a – 4c plot the
displacement waveforms for the 200 ms, 300 ms, and 400
ms windows, respectively. Values greater than zero
denote that multimodal presentation increased the latency
of P300 and values less than zero denote that multimodal
presentation shortened the latency of P300.

latency of the visual P300 and shortening the latency
of the auditory P300.
a.
Pz - 200 ms window 50% peak
Auditory

10

Visual

Displacement (ms)

5

a.
Pz Auditory

0

-5

8
-10

6
-15
250

300

350

2

400
450
500
Time Window (ms)

550

600

650

b.

0

Pz - 300 ms window 50% peak

t

Microvolt

4

-2
10

-4

-8

UA
MA
0

200

400
Time(ms)

600

5

Displacement (ms)

-6

800

b.
Pz Visual
8

0
-5
-10
-15

6

Auditory

250

2

300

400
450
500
Time Window (ms)

550

600

650

Pz - 400 ms window 50% peak

-2
-4

10

UV
MV
0

200

400
Time(ms)

600

5

800

Displacement (ms)

-6
-8

350

c.

0
t

Microvolt

Visual

-20

4

Figure 3. (a) Difference waves for Unimodal Auditory
(UA) and Multimodal Auditory (MA), (b) Difference
waves for Unimodal Visual (UV) and Multimodal Visual
(MV). All data are averaged across participants.

0
-5
-10
-15
-20

Auditory
Visual

-25

As can be seen in Figures 4a-4c, across all windows,
multimodal presentation sped up auditory processing and
slowed down visual processing.
ANOVAs were
conducted for each window at every 4 ms increment.
Using a window size of 200 ms, auditory and visual
displacement waves differed from 370 ms to 514 ms, ps <
.05. Using a window size of 300 ms, auditory and visual
displacement waves differed from 362 ms to 534 ms, ps <
.05. Finally, using a window size of 400 ms, auditory and
visual displacement waves differed from 370 ms to 554
ms, ps < .05. These findings suggest the multimodal
presentation had different effects on auditory and visual
processing, with multimodal presentation increasing the

250

300

350

400
450
500
Time Window (ms)

550

600

650

Figure 4. Displacement for (a) 200 ms window, (b)
300 ms window, (c) 400 ms window.

General Discussion
The current study used a passive oddball task to
examine the time course of auditory and visual
processing when stimuli were presented unimodally
and multimodally. As can be seen in Figures 2, 3a,
and 4a-4c, there was no evidence that visual input
attenuated discrimination of auditory stimuli. Rather,

2648

multimodal presentation appeared to speed up auditory
processing and slow down visual processing. These
findings have important implications for understanding
the underlying mechanisms and time course of modality
dominance. In particular, the current findings suggest that
some of the effects of visual dominance may stem from
visual input dominating the response. However, future
research will need to make direct comparisons on tasks
that do and do not require explicit responses before any
strong conclusions can be drawn.
The novelty of the current research is that we examined
the time course of auditory and visual processing on a
task that did not require an explicit response. The results
replicate auditory dominance effects found in young
children, with multimodal presentation attenuating visual
processing, and having no effect or facilitating auditory
processing (see Robinson & Sloutsky, 2010 for a review).
This interaction suggests that effects cannot solely stem
from increased tasks demands, otherwise processing in
both modalities would have been delayed. Rather, we
believe this interaction stems from the dynamics of crossmodal processing. According to this account (Robinson &
Sloutsky, 2010), auditory stimuli quickly engage attention
and processing of the details of a visual stimulus does not
begin until the auditory modality releases attention. While
this account has received some support in young children,
the finding that auditory input can also slow down visual
processing in adults is novel.

Hansen, J.C., & Hillyard, S. A. (1980). Endogenous
brain potentials associated with selective auditory
attention. Electroencephalography and Clinical
Neurophysiology, 49, 277-290
Klein, R. M. (1977). Attention and visual dominance:
A chronometric analysis. Journal of Experimental
Psychology: Human Perception & Performance, 3,
365-378.
Kutas, M., McCarthy, G., Donchin, E. (1977).
Augmenting mental chronometry: the P300 as a
measure of stimulus evaluation. Science, 197, 792–
795.
Mertens, R., & Polich, J. (1997). P300 from a singlestimulus paradigm: Passive versus active tasks and
stimulus modality. Electroencephalography &
Clinical Neurophysiology, 104(6), 488-497.
Polich, J., & Criado, R. (2006). Neuropsychology
and neuropharmacology of P3a and P3b.
International Journal of Psychophysiology, 60(2),
172-185.
Posner, M. I., Nissen, M. J., & Klein, R. M. (1976).
Visual dominance: An information-processing
account of its origins and significance.
Psychological Review, 83, 157-171.
Robinson, C. W., & Sloutsky, V. M. (2004).
Auditory dominance and its change in the course of
development. Child Development, 75, 1387-1401.
Robinson, C. W., & Sloutsky, V. M. (2010).
Development of Cross-modal Processing. Wiley
Interdisciplinary Reviews: Cognitive Science, 1,
135-141.
Sloutsky, V. M., & Napolitano, A. (2003). Is a
picture worth a thousand words? Preference for
auditory modality in young children. Child
Development, 74, 822-833.
Sinnett, S., Spence, C., & Soto-Faraco, S. (2007).
Visual dominance and attention: Revisiting the
Colavita effect. Perception & Psychophysics, 69,
673–686.
Spence, C., Shore, D. I., & Klein, R. M. (2001).
Multisensory prior entry. Journal of Experimental
Psychology: General, 130, 799-832.
Sutton, S., Braren, M., Zubin, J., John, E. (1965).
Evoked potential correlates of stimulus uncertainty.
Science, 150, 1187–1188.
Vidal, J., Giard, M-H., Roux, S., Barthelemy, C., &
Bruneau, N. (2008). Cross-modal processing of
auditory-visual stimuli in a no-task paradigm: A
topographic event-related potential study. Clinical
Neurophysiology, 119(4), 763-771.
Wronka, E., Kaiser, J., & Coenen, A. M. L. (2008).
The auditory P3 from passive and active threestimulus oddball paradigm. Acta Neurobiologiae
Experimentalis, 68(3), 362-372.

Acknowledgments
This research has been supported by grants from the NSF
(BCS-0720135), NIH (R01HD056105) and from the US
Department
of
Education
(R305H050125
and
R305B070407) to Vladimir Sloutsky and from NIH
(RO3HD055527) to Chris Robinson.

References
Brown, C.R., Clarke, A.R., & Barry, R.J. (2007).
Auditory processing in an inter-modal oddball task:
Effects of a combined auditory/visual standard on
auditory target ERPs. International Journal of
Psychophysiology, 65, 122-131.
Colavita, F. B. (1974). Human sensory dominance.
Perception & Psychophysics, 16, 409-412.
Colavita, F. B., & Weisberg, D. (1979). A further
investigation of visual dominance. Perception &
Psychophysics, 25, 345–347.
Fort, A., Delpuech, C., Pernier, J., Giard, M.H., (2002).
Early auditory–visual interactions in human cortex
during nonredundant target identification. Cogn. Brain
Res, 14, 20–30.
Giard, M.H., Peronnet, F., 1999. Auditory–visual
integration during multimodal object recognition in
humans: A behavioural and electrophysiological study.
J. Cogn. Neurosci. 11 (5), 473–494.

2649

