UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Pavlovian conditioning from a foraging perspective
Permalink
https://escholarship.org/uc/item/815548rx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Anderson, James
Bracis, Chloe
Goodwin, Andrew
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                              Pavlovian conditioning from a foraging perspective
                                                James J. Anderson (jjand@uw.edu)
                                   School of Aquatic and Fishery Sciences, University of Washington
                                                         Seattle, WA 98195 USA
                                                   Chloe Bracis (cbracis@uw.edu)
                              Quantitative Ecology and Resource Management, University of Washington
                                                         Seattle, WA 98195 USA
                                      R. Andrew Goodwin (Andy.Goodwin@us.army.mil)
                          Environmental Laboratory, U.S. Army Engineer Research & Development Center,
                                                         Portland, OR 97208 USA
                               Abstract                                unrewarded trials during partial reinforcement schedules
   Principles in foraging and standard associative learning
                                                                       create a different context where the unrewarded trials signal
   theories motivate a model for Pavlovian conditioning. The           rewarded trials. We develop a model that readily explains
   model tracks distal and proximal scales of expected reward          PC phenomenon in what we believe is a robust manner. Our
   probabilities plus the strength of signal-reward association. A     model builds on PC and foraging theories and the
   combined reward probability is developed by combining the           neuroscience of memory and decision-making.
   distal and proximal estimates through their uncertainties.             Animals in natural and laboratory environments meld
   Possible neural structure equivalents to the model variables        their past and current experiences in making decisions; it is
   are discussed. Model flexibility is demonstrated with data on
   the partial reinforcement extinction effect, a phenomenon           often assumed that the laws and processes in both
   difficult to explain with learning models.                          environments are similar if not identical. In foraging,
                                                                       animals choose between staying in the current patch and
   Keywords: Mathematical model; Pavlovian conditioning;               moving to another. Deciding when to give up on a patch
   associative learning; foraging theory; partial reinforcement
   extinction effect, neural structures.                               depends on the expected reward rate of the current patch
                                                                       relative to the expected reward rate on another patch
          Foraging and Associative Learning                            discounted for the interpatch travel time (Charnov, 1976). In
                                                                       PC the response rate reflects expectations within the single
Pavlovian conditioning (PC), or associative learning, is one           environment as dependent on learning and unlearning of
of the most well studied psychological processes and has an            signal-reward associations (Bouton, 1993). Notably, in both
array of associated phenomena. The two main processes,                 frameworks, responses are based on comparisons of distal
acquisition of a behavior by pairing a signal and reward in            and proximal information. Distal information in PC is the
trials and extinction of the behavior by removing the reward           memory of the previous signal-reward patterns, while in
during the trials, can be explained by a number of models,             foraging distal information is the memory of the average
the most common being the delta model where learning is                habitat reward rate. Proximal information in PC reflects the
guided by the error, i.e. delta, between the expected and              most recent reward outcomes, while in foraging it reflects
received reward per trial (Rescorla & Wagner, 1972).                   the expected foraging rate in the current patch. In both
However, the basic delta model cannot explain other widely             frameworks, expected reward for the next trial is computed
known phenomena, including spontaneous recovery of                     by a delta rule, which is an exponentially weighted moving
behavior when signaling trials are conducted after                     average (EWMA) that adds a percentage of the most recent
extinguishing the behavior and the partial reinforcement               outcome to a percentage of the previous expectation.
extinction effect (PREE) where intermittent rewards during                The two frameworks diverge in how non-reward events
acquisition trials increase the resistance to extinguishing            and extinction are treated. PC models commonly consider
behavior and strengthen the response during spontaneous                acquisition-extinction patterns in terms of distinct learning
recovery trials. In particular, the PREE challenges                    streams. A stream developed during the acquisition phase of
associative models of PC, since lower reward expectations              the experiment characterizes signal-reward expectations,
for partial compared to continuous reinforcement would                 and a second stream developed during the extinction phase
appear in principle to produce faster extinction. One theory           characterizes a signal-no reward expectation. Extinction
for PREE involve the rate of reinforcement, such the ratio of          learning inhibits the original acquisition learning (Bouton,
the current cumulative signal duration since the last reward           1993). However, when animals are retested after some
to the average signal duration between rewards (Gallistel &            interval of time the extinction learning is forgotten and
Gibbon, 2000). However, this result is not supported                   spontaneous recovery of the original learning appears
experimentally (Haselgrove et al., 2004). An alternative               (Sissons & Miller, 2009). In foraging models, the learning
verbal model proposed by Pearce et al. (1997) assumed that             streams do not inhibit each other nor are they forgotten.
                                                                   1276

Expected probabilities of rewards are tracked for both the        vulnerable to interference from other mental activity and
current patch, i.e. the proximate patch, and the habitat, i.e.    Wixted (2005) suggested that forgetting is largely a function
the distal patch. With parallel information streams, the          of nonspecific retroactive interference acting on memory
animal does not need to distinguish whether information           traces that have not yet consolidated in the neocortex. Wang
belongs in the acquisition or the extinction learning stream,     & Morris (2010) hypothesized that extinction trials involve
an issue in PREE experiments where signals without                reactivation of the acquisition-trial memories in the absence
rewards occur during the acquisition phase. Rather, the           of further reinforcement. However, such interactions can be
animal is constantly adapting to an always changing               complex and two memories may mutually exclude each
environment.                                                      other or coexist depending on the timing of the signal during
   Patch foraging models naturally involve multiple               extinction (Perez-Cuesta & Maldonado, 2009).
temporal scales because information on the proximal patch            Decision making is treated differently in PC and foraging
is always more recent than information on distal patches. To      models. In foraging models, the decision to leave a patch is
capture these temporal differences, models have expressed         depends on which patch has the higher reward probability
distal and proximal learning with slow and fast learning          (maximizing) or is selected probabilistically (matching)
rates respectively (Anderson, 2002; Moorter et al., 2009).        (Kacelnik, Krebs , & Ens, 1987). PC models do not have
Mixed learning rates are also used in the primary value           choice-based decision rules and express the response rate as
learned value (PVLV) model (O’Reilly et al., 2007), that          a monotonic function of the reward expectation. However, if
seeks a mapping to dopaminergic neuron dynamics during            PC and foraging have the same basis, then PC models
reinforcement learning.                                           contain a hidden decision rule in which the animal chooses
   Retaining reward probabilities across different temporal       between proximal and distal information. However, decision
and spatial scales requires memory systems, and here              rules in both PC and foraging models are incomplete
neuroscience provides information on their nature.                because psychology, ecology, neuroscience, and machine
McClelland et al. (1995) postulated memories are created          learning research show that uncertainty in the reward
and stored in a two-stage process involving short- and long-      assessment is an important factor in decision-making (Daw
term processes. First, events are stored via synaptic changes     et al., 2005; Platt & Huettel, 2008).
in the hippocampal system, a short-term memory (STM)
which then supports reinstatement of recent memories into                                       The Model
long-term memories (LTM) in the neocortex. The                    We now develop a model for PC that has application to
neocortical synapses change by a small amount on each             foraging models, draws on concepts from both modeling
reinstatement, which assures that learning, as a stochastic       frameworks, and has some analogy to the neurology of
process, converges to the mean value of the statistical           decision-making. We model reward probability estimates
association of ensembles of experiences. The hippocampal          for distal and proximal information streams, which
system permits rapid learning of new items without                correspond to the immediate patch and the surrounding
disrupting the neocortex structure, and interleaves and           habit in foraging models and to the short- and long-term
integrates them into the neocortical system. In essence, the      estimates of rewards in PC models. We then combine the
LTM is built-up incrementally from activation of STM.             estimates with weightings based on their respective
Furthermore, since extinction involves new learning,              uncertainties. We also account separately for the process of
evidence suggests multiple memory systems may be                  learning that a signal can indicate a reward. Finally, we use
applicable to the neural basis of extinction (Gabriele &          the weighted expectation to model the animal’s response
Packard, 2006). We suggest the distal and proximal                rate in a trial.
information streams which are contained in both PC and
foraging models represent the STM and LTM system                  Distal and Proximal Reward Estimates
identified by neurological studies.
   Forgetting is the other side of remembering and is             For each trial we define the distal and proximal expected
important in PC models to explain spontaneous recovery.           reward estimate with a modified delta model,
The idea being that the information stream acquired in the                              xˆ j ,i  m j yi j ,i 1  xˆ j ,i 1 (1)
extinction phase is forgotten over time, which then removes       where j = 1, 2 indicates distal and proximal information
the inhibition of the information streams acquired in the         streams, i designates a PC trial, mj is the learning rate for
acquisition phase. This process is offered as an explanation      stream j. For each stream the error between the expected
for the stronger spontaneous recovery response that is            reward probability and realized reward is
observed with greater time between extinction and recovery                                       j ,i  xi  xˆ j ,i          (2)
tests and thus supports the view that learning in the             where xi is a reward/no-reward outcome (0,1) for trial i. The
extinction phase dissipates more rapidly than learning in the
acquisition phase (Brooks & Bouton, 1993; Rescorla, 2004;         term, yi, is a measure of the strength of the association of the
Sissons & Miller, 2009).                                          signal-reward and is independent of reward probabilities.
   Studies on forgetting provide valuable insight into its        For convenience, we consider the distal and proximal
significance in associative learning. Recent memories are         information streams unconscious reward estimators because
                                                                  individually they are sub-process that must be combined to
                                                              1277

                                                                                                                           
affect the animal’s response. We designate the combined                                                      i  xi  xi .                  (7)
estimator the conscious reward estimate.                                        Second, because errors are by nature random and one
                                                                                correct prediction,  i  0 , is insufficient to develop an
Combined Estimate
                                                                                association, we compute an average error with a EWMA:
                                                                                                                                   
                                                                                                    i2  n  i21   i21    i21 .
The distal and proximal estimates of reward probability are
combined into a single conscious reward estimate that the                                                                                     (8)
animal uses in making decisions:                                                where n is again the uncertainty learning rate. Third, to
                              
                             xi  w1,i xˆ1,i  w2,i xˆ2,i               (3)     capture the repetitive and asymptotic nature of appetitive
where the estimates are combined according to their                             learning, we incrementally accumulate the uncertainties
respective weighting factors that depend on their associated                    with a standard saturation function
uncertainties  j2,i . As we develop next, the uncertainties are
                                                                                                      i 1                    i 1     
                                                                                                yi                           
                                                                                                           1  k2  g  1  k2 
                                                                                                                                          
                                                                                                                                              (9)
in fact EWMAs of the variance in the distal and proximal                                              k 1                    k  1       
estimators and so the estimates can be combined using a                         where g is the halfway point in the learning process.
standard statistical weighting formula in which the weight
for estimate j on trial i is                                                    Response Rate
                  w  1  2
                      j ,i        j ,i   1  2  1  2 .
                                                    1, i         2,i   (4)     We relate the conscious reward expectation to the response
It is noteworthy that this weighting scheme is not found in                     rate with a matching function that asymptotically increases a
either PC or foraging models.                                                   response from a background level to a maximum and is
                                                                                defined with scale and shape parameters rmax and r as
                                                                                                                                   
Temporal Discounting Uncertainty                                                                  R  rmax xi  xi  r (1  xi )  .         (10)
The uncertainties used in weighting,  j2,i , are developed from
                                                                                Parameter Summary
the mean-squared errors of the distal and proximal reward
estimates. Of relevance, the uncertainties depend on the                        The complete model combines elements of classical
time between trials as follows. First, compute unadjusted                       associative leaning and patch foraging. While several
uncertainty estimates as EWMAs from errors defined by eq.                       models contain multiple memory streams that track
(2):                                                                            information over different time scales, the model presented
                                                                                here tracks the uncertainties in the estimates as information
                    ˆ 2j ,i  n( j2,i 1   j2,i 1 )   j2,i1 , (5)
                                                                                streams as well. The model contains 7 parameters (Table 1).
where n is the uncertainty learning rate. Next, adjust the
uncertainties for the time interval t  ti  ti 1 between trials:                  Table 1: Model parameters and values fitted to data.
                                  2  ˆ 2 h t
                                    j ,i      j ,i   j                  (6)
                                                                                  Parameter    Fitted      Meaning
where hj is a decay parameter that controls the rate at which                                  value
the uncertainty in information stream j decays as time
                                                                                      m1       0.055       Distal learning rate
between trials increases. In this model, as the inter-trial time
increases, we want to put more confidence on the distal                               m2       0.248       Proximal learning rate
(long-term) estimate and less on the proximal (short-term)                            n        0.075       Uncertainty learning rate
estimate. The idea being that in a sequence of trials with                            h1       0.126       Distal uncertainty decay rate
uncertain outcomes, as time passes since the last trial we                            g        971         Association half-way constant
should trust the long-term estimate of reward probability                            rmax      6.88        Response function scale parameter
more than the short-term estimate based only on the last few                          r        0.13        Response function shape parameter
rewards. To insure this shift in confidence to the distal
estimate, we decay the distal uncertainty but not the
proximal uncertainty as time passes between trials:                                           Comparison to Experiment
Mathematically this is achieved with 0  h1  1 and h2  1 .
                                                                                To demonstrate the flexibility and perspective the model
                                                                                provides, we fit it to a study of partial reinforcement
Signal-Reward Association                                                       extinction conducted by Haselgrove, et al. (2004). We
The term yi in eq. (1) tracks the strength of the signal-reward                 selected this experiment because PREE is difficult for PC
association, which we assume is distinct from probability                       models to explain. In addition, the study covers an
learning but also depends on the error of predictions.                          acquisition phase and two extinction phases, which
Learning requires repetition and reduction of errors in                         demonstrate spontaneous recovery. Several models produce
prediction, and we model these properties with a three step                     these basic patterns but not when one of the groups is
process. First, we track conscious error based on the                           trained with partial rewards.
difference between the trial outcome and the conscious
expectation from eq. (3) giving
                                                                            1278

  In the experiment, rats divided into partial and continuous    Continuous Reinforcement Group In Fig. 2a the signal-
reinforcement groups received the same signal and number         reward association strength, y, rises over the acquisition
of rewards during an acquisition phase in which the              phase to its full value and remains constant thereafter,
reinforcement schedules differed. In the partial group, half     implying that the animal has fully learned the association.
of the trials were reinforced with two rewards, while in the     The conscious reward probability also reaches its full value
continuous group one reward was given on every trial.            in the acquisition phase and then exponentially declines in
Following the acquisition sessions, the rats received two        the extinction phases. At the beginning of the second
sessions with unreinforced signals. In Figures 1-3, each         extinction phase, the expectation is higher than at the end of
point designates an entire session in the acquisition phase,     the first extinction phase, then the expectation again decays
while each point represents a block of two trials in the two     since the animal receives no rewards. This somewhat
extinction sessions following.                                   complex pattern of responses is generated by a unique
  We fit the model to the data from both groups with a           weighting of relatively simple patterns in the distal and
single set of parameters (Table 1) using the “mco” package       proximal estimators. The proximal estimator (Figure 2b),
in the R statistical programming language. This is a multi-      which is generated by a faster learning coefficient, rises
criteria optimization algorithm based on a genetic algorithm     quickly in the acquisition phase and then decays quickly in
(cran.r-project.org/web/packages/mco/mco.pdf).                   the first extinction phase and remains at zero throughout the
  The model fit the response patterns for the continuous and     second extinction phase. The distal estimator, being the
partial groups reasonably well. The mean responses in the        slow learner, rises slowly in the acquisition phase and then
acquisition phase developed in a similar manner for both         decays slowly over the next two phases. The pattern in the
groups, while in the extinction phase the continuous group       weights (Figure 2c) that mix the two estimators produces
response decayed more rapidly than the response in the           the spontaneous recovery. Beginning in the acquisition
partial group. Both groups exhibited spontaneous recovery        phase, the weightings are equal. Because rewards are
in the final extinction session with the continuous group        consistently received, the proximal estimator quickly adjusts
response again decaying faster than the partial group            and has less uncertainty than the distal estimator, giving the
response (Figure 1).                                             distal estimator the greatest weight in forming the conscious
                                                                 estimator in eq. (4). In the period between the acquisition
                                                                 and extinction phases, eq. (6) decays the distal uncertainty
                                                                 (trust the long-term estimate when information is old), so
                                                                 the two weights are nearly equal beginning the extinction
                                                                 phase. However, as signals are consistently unrewarded, the
                                                                 proximal estimator better represents the environment and its
 R                                                               weight rises over the trials. The distal uncertainty decays
                                                                 again after the first extinction phase, and the pattern is
                                                                 repeated in the second extinction phase. At the beginning of
                                                                 the second extinction the proximal estimator, which predicts
                                                                 a reward, has a higher weight than the distal estimator,
                                                                 which predicts no reward, so the animal exhibits
                                                                 spontaneous recovery.
              Acquisition      Extinction 1   Extinction 2
                                                                 Partial Reinforcement Group In the acquisition phase, the
       Figure 1: Haselgrove et al. (2004) data and model         patterns of conscious expectation and the signal-reward
       fit for partial and continuous reinforcement              association (Figure 3a) are similar to the patterns in the
       groups using parameter values in Table 1.                 continuous reinforcement group (Figure 2a), although the
                                                                 strengths are half the continuous reinforcement values. The
                                                                 patterns in the distal and proximal estimators are similar
                          Discussion                             also (Figure 3b), and again the strengths are about half
The patterns of the underlying streams producing the fit to      showing the accurate estimation of the 50% reward
the Haselgrove et al. (2004) data for the continuous             probability during acquisition. However, the experiments
reinforcement group (Figure 2) and the partial reinforcement     differ significantly in the weighing function patterns. These
group (Figure 3) illustrate how a framework of multiple-         are reversed in the partial reinforcement group (Figure 3c)
scale estimators and uncertainties can account for seemingly     compared to that in the continuous reinforcement group
complex patterns in PC studies. As in Figure 1, the first        (Figure 2c). This difference drives the differences in the
section consists of session averages for the acquisition         response patterns (Figure 1). Again, at the beginning of the
sessions, and the next two sections each represent an            experiment, the distal and proximal uncertainties are equal,
extinction session in blocks of two trials.                      making for equal weights. However, both estimators
                                                             1279

        a                                                                   a
                                                                    
 x                                                                  x
  y                                                                  y
        b                                                                    b
 xˆ j                                                               xˆ j
        c                                                                   c
 wj                                                                 wj
             Acquisition      Extinction 1    Extinction 2                      Acquisition          Extinction 1  Extinction 2
          Figure 2: Changes in model variables                                    Figure 3: Changes in model variables
          for the continuous reinforcement group.                                 for the partial reinforcement group.
have higher uncertainty with partial reinforcement, but the        sense, we suggest that the distal and proximal information
proximal estimator, which is strongly influenced by the            streams xˆ1,i , xˆ2,i represent parallel memory systems that
previous trial, has higher uncertainty than the distal             characterize reward probabilities estimated on different
estimator, which integrates the reward expectation over            temporal scales. These terms might be candidates for STM-
multiple trials. The result is lower uncertainty for the distal    LTM systems involving the hippocampus and neocortex.
estimator and thus greater weight in forming the conscious         However, the two streams are competitive and so they might
estimator. Between the acquisition and extinction phases,          be representative of competitive memory systems such as
the distal uncertainty declines while the proximal                 the hippocampus and basal ganglia (White & McDonald,
uncertainty is fixed, so the distal estimator is dominant at       2002; Poldrack & Packard, 2003). In our model the signal-
the beginning of the first extinction phase. Over the phase        reward association yi represents a separate memory stream
the distal uncertainty increases while the proximal                that builds in a cumulative manner by summing the inverse
uncertainty decreases until they are equal at the end of the       of trial-by-trial uncertainties. This incremental building of
extinction. Therefore, at the end of the extinction phase, the     memories is also a feature of the STM-LTM interaction of
animal has a higher response rate than in the continuous           the hippocampus and neocortex (McClelland, McNaughton,
case, which is dominated by the proximal estimator.                & O'Reilly, 1995).
Between the first and second extinction phases, the distal            Uncertainty is specifically formulated in our model, and
estimator uncertainty again decays giving it more weight in        neural structures are clearly involved with uncertainty in
the second extinction phase, resulting in a higher response        decision-making. For example, Doya (2008) noted
and slower decline in response for the partial acquisition         uncertainty has two flavors: one resulting from the
group.                                                             environmental stochasticity (risk) and one from the limited
                                                                   knowledge of the decision-maker (ambiguity). Studies
Neurological Analogies                                             suggest that risk is represented in the striatum and
As our ultimate goal is to model the brain, not just observed      precuneus while ambiguity is represented in the lateral
behavior, we seek to identify possible equivalences between        obitofrontal cortex and amygdala (Platt & Huettel, 2008).
the model’s elements and neural structures as has been             Our model also has two flavors of uncertainty. The
encouraged by Rangel et al. (2008) and others. In a broad
                                                               1280

uncertainty in the distal and proximal reward estimators ˆ j2,i     Kacelnik, A., Krebs , J. R., & Ens, B. (1987). Foraging in a
                                                                        changing environment: an experiment with starlings
tracks variability in the environment that we suggest is akin           (Sturnus vulgaris). In A. K. a. S. J. S. M.L. Commons
to risk uncertainty. The uncertainty in signal-reward
                                                                       (Ed.), Quantitative Analysis of Behavior Foraging (Vol.
association  i2 is a candidate for the decision-maker’s                6, pp. 63–87). Hillsdale: L. Erlbaum.
ambiguity.                                                           McClelland, J. L., McNaughton, B. L., & O'Reilly, R. C.
                                                                        (1995). Why there are complementary learning systems
Final Thoughts                                                          in the hippocampus and neocortex: insights from the
Under the assumption that animals in laboratory studies use             successes and failures of connectionist models of
behavioral strategies and neurological processes that                   learning and memory. Psychol Rev, 102(3), 419-457.
evolved through natural selection, we reconsider Pavlovian           Moorter, B. V., Visscher, D., Benhamou, S., Börger, L.,
conditioning in the context of animal foraging. From this               Boyce, M. S., & Gaillard, J.-M. (2009). Memory keeps
perspective, animal behavior in a constrained environment               you at home: a mechanistic model for home range
has a hidden spatial component that leads us to consider the            emergence. Oikos, 118(5), 641-652.
behavior in terms of distal (habitat) and proximal (local            O'Reilly, R. C., Frank, M. J., Hazy, T. E., & Watz, B.
patch) information streams. In this framework the animal                (2007). PVLV: The Primary Value and Learned Value
does not track distinct memory streams for acquisition and              Pavlovian      Learning        Algorithm.     Behavioral
extinction phases, which we suggest is the experimenter’s               Neuroscience, 121(1), 31-49.
perspective. Instead of having to know when the                      Pearce, J. M., Redhead, E. S., & Aydin, A. (1997). Partial
experimenter ends one phase and starts another, the animal              reinforcement in appetitive Pavlovian conditioning with
can view the environment as continuous yet random and                   rats. Quarterly Journal of Experimental Psychology,
simply track information measuring over two different time              50B, 273-294.
scales and weighting the estimates according to the trial-by-        Platt, M. L., & Huettel, S. A. (2008). Risky business: the
trial changes in their uncertainties.                                   neuroeconomics of decision making under uncertainty.
                                                                        Nat Neurosci, 11(4), 398-403.
                    Acknowledgments                                  Poldrack, R. A., & Packard, M. G. (2003). Competition
                                                                        among multiple memory systems: converging evidence
This work was supported by the U.S. Army Engineer                       from      animal     and      human     brain    studies.
Research and Development Center. Permission was granted                 Neuropsychologia, 41(3), 245-251.
by the Chief of Engineers to publish this information.               Rangel, A., Camerer, C., & Montague, P. R. (2008). A
                                                                        framework for studying the neurobiology of value-based
                         References                                     decision making. Nat Rev Neurosci, 9(7), 545-556.
Anderson, J. J. (2002). An agent-based event drive foraging          Rescorla, R. A. (2004). Spontaneous recovery. Learn Mem,
    model. Natural Resource Modeling, 15(1), 55-82.                     11(5), 501-509.
Bouton, M. E. (1993). Context, time, and memory retrieval            Rescorla, R. A., & Wagner, A. R. (1972). A theory of
    in the interference paradigms of Pavlovian learning.                Pavlovian conditioning: Variations in the effectiveness
    Psychol Bull, 114(1), 80-99.                                        of reinforcement and nonreinforcement. In A. H. Black
Brooks, D. C., & Bouton, M. E. (1993). A retrieval cue for              & W. F. Prokasy (Eds.), Classical conditioning II (pp.
    extinction attenuates spontaneous recovery. J Exp                   64-99). New York: Appleton-Century-Crofts.
    Psychol Anim Behav Process, 19(1), 77-89.                        Sissons, H. T., & Miller, R. R. (2009). Spontaneous
Charnov, E. L. (1976). Optimal foraging, the marginal value             recovery of excitation and inhibition. J Exp Psychol
    theorem. Theor Popul Biol, 9(2), 129-136.                           Anim Behav Process, 35(3), 419-426.
Daw, N. D., Niv, Y., & Dayan, P. (2005). Uncertainty-based           Wang, S.-H., & Morris, R. G. M. (2010). Hippocampal-
    competition between prefrontal and dorsolateral striatal            Neocortical Interactions in Memory Formation,
    systems for behavioral control. Nat Neurosci, 8(12),                Consolidation, and Reconsolidation. Annual Review of
    1704-1711.                                                          Psychology, 61(1), 49-79.
Doya, K. (2008). Modulators of decision making. Nat                  White, N. M., & McDonald, R. J. (2002). Multiple Parallel
    Neurosci, 11(4), 410-416.                                           Memory Systems in the Brain of the Rat. Neurobiology
Gabriele, A., & Packard, M. G. (2006). Evidence of a role               of Learning and Memory, 77(2), 125-184.
    for multiple memory systems in behavioral extinction.            Wixted, J. T. (2005). A Theory About Why We Forget
    Neurobiology of Learning and Memory, 85(3), 289-299.                What We Once Knew. Current Directions in
Gallistel, C. R., & Gibbon, J. (2000). Time, rate, and                  Psychological Science, 14(1), 6-9.
    conditioning. Psychological Review, 107, 289-344.
Haselgrove, M., Aydin, A., & Pearce, J. M. (2004). A
    partial reinforcement extinction effect despite equal rates
    of reinforcement during Pavlovian conditioning. J Exp
    Psychol Anim Behav Process, 30(3), 240-250.
                                                                 1281

