UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Pavlovian conditioning from a foraging perspective

Permalink
https://escholarship.org/uc/item/815548rx

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Anderson, James
Bracis, Chloe
Goodwin, Andrew

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Pavlovian conditioning from a foraging perspective
James J. Anderson (jjand@uw.edu)
School of Aquatic and Fishery Sciences, University of Washington
Seattle, WA 98195 USA

Chloe Bracis (cbracis@uw.edu)
Quantitative Ecology and Resource Management, University of Washington
Seattle, WA 98195 USA

R. Andrew Goodwin (Andy.Goodwin@us.army.mil)
Environmental Laboratory, U.S. Army Engineer Research & Development Center,
Portland, OR 97208 USA

Abstract
Principles in foraging and standard associative learning
theories motivate a model for Pavlovian conditioning. The
model tracks distal and proximal scales of expected reward
probabilities plus the strength of signal-reward association. A
combined reward probability is developed by combining the
distal and proximal estimates through their uncertainties.
Possible neural structure equivalents to the model variables
are discussed. Model flexibility is demonstrated with data on
the partial reinforcement extinction effect, a phenomenon
difficult to explain with learning models.
Keywords: Mathematical model; Pavlovian conditioning;
associative learning; foraging theory; partial reinforcement
extinction effect, neural structures.

Foraging and Associative Learning
Pavlovian conditioning (PC), or associative learning, is one
of the most well studied psychological processes and has an
array of associated phenomena. The two main processes,
acquisition of a behavior by pairing a signal and reward in
trials and extinction of the behavior by removing the reward
during the trials, can be explained by a number of models,
the most common being the delta model where learning is
guided by the error, i.e. delta, between the expected and
received reward per trial (Rescorla & Wagner, 1972).
However, the basic delta model cannot explain other widely
known phenomena, including spontaneous recovery of
behavior when signaling trials are conducted after
extinguishing the behavior and the partial reinforcement
extinction effect (PREE) where intermittent rewards during
acquisition trials increase the resistance to extinguishing
behavior and strengthen the response during spontaneous
recovery trials. In particular, the PREE challenges
associative models of PC, since lower reward expectations
for partial compared to continuous reinforcement would
appear in principle to produce faster extinction. One theory
for PREE involve the rate of reinforcement, such the ratio of
the current cumulative signal duration since the last reward
to the average signal duration between rewards (Gallistel &
Gibbon, 2000). However, this result is not supported
experimentally (Haselgrove et al., 2004). An alternative
verbal model proposed by Pearce et al. (1997) assumed that

unrewarded trials during partial reinforcement schedules
create a different context where the unrewarded trials signal
rewarded trials. We develop a model that readily explains
PC phenomenon in what we believe is a robust manner. Our
model builds on PC and foraging theories and the
neuroscience of memory and decision-making.
Animals in natural and laboratory environments meld
their past and current experiences in making decisions; it is
often assumed that the laws and processes in both
environments are similar if not identical. In foraging,
animals choose between staying in the current patch and
moving to another. Deciding when to give up on a patch
depends on the expected reward rate of the current patch
relative to the expected reward rate on another patch
discounted for the interpatch travel time (Charnov, 1976). In
PC the response rate reflects expectations within the single
environment as dependent on learning and unlearning of
signal-reward associations (Bouton, 1993). Notably, in both
frameworks, responses are based on comparisons of distal
and proximal information. Distal information in PC is the
memory of the previous signal-reward patterns, while in
foraging distal information is the memory of the average
habitat reward rate. Proximal information in PC reflects the
most recent reward outcomes, while in foraging it reflects
the expected foraging rate in the current patch. In both
frameworks, expected reward for the next trial is computed
by a delta rule, which is an exponentially weighted moving
average (EWMA) that adds a percentage of the most recent
outcome to a percentage of the previous expectation.
The two frameworks diverge in how non-reward events
and extinction are treated. PC models commonly consider
acquisition-extinction patterns in terms of distinct learning
streams. A stream developed during the acquisition phase of
the experiment characterizes signal-reward expectations,
and a second stream developed during the extinction phase
characterizes a signal-no reward expectation. Extinction
learning inhibits the original acquisition learning (Bouton,
1993). However, when animals are retested after some
interval of time the extinction learning is forgotten and
spontaneous recovery of the original learning appears
(Sissons & Miller, 2009). In foraging models, the learning
streams do not inhibit each other nor are they forgotten.

1276

Expected probabilities of rewards are tracked for both the
current patch, i.e. the proximate patch, and the habitat, i.e.
the distal patch. With parallel information streams, the
animal does not need to distinguish whether information
belongs in the acquisition or the extinction learning stream,
an issue in PREE experiments where signals without
rewards occur during the acquisition phase. Rather, the
animal is constantly adapting to an always changing
environment.
Patch foraging models naturally involve multiple
temporal scales because information on the proximal patch
is always more recent than information on distal patches. To
capture these temporal differences, models have expressed
distal and proximal learning with slow and fast learning
rates respectively (Anderson, 2002; Moorter et al., 2009).
Mixed learning rates are also used in the primary value
learned value (PVLV) model (O’Reilly et al., 2007), that
seeks a mapping to dopaminergic neuron dynamics during
reinforcement learning.
Retaining reward probabilities across different temporal
and spatial scales requires memory systems, and here
neuroscience provides information on their nature.
McClelland et al. (1995) postulated memories are created
and stored in a two-stage process involving short- and longterm processes. First, events are stored via synaptic changes
in the hippocampal system, a short-term memory (STM)
which then supports reinstatement of recent memories into
long-term memories (LTM) in the neocortex. The
neocortical synapses change by a small amount on each
reinstatement, which assures that learning, as a stochastic
process, converges to the mean value of the statistical
association of ensembles of experiences. The hippocampal
system permits rapid learning of new items without
disrupting the neocortex structure, and interleaves and
integrates them into the neocortical system. In essence, the
LTM is built-up incrementally from activation of STM.
Furthermore, since extinction involves new learning,
evidence suggests multiple memory systems may be
applicable to the neural basis of extinction (Gabriele &
Packard, 2006). We suggest the distal and proximal
information streams which are contained in both PC and
foraging models represent the STM and LTM system
identified by neurological studies.
Forgetting is the other side of remembering and is
important in PC models to explain spontaneous recovery.
The idea being that the information stream acquired in the
extinction phase is forgotten over time, which then removes
the inhibition of the information streams acquired in the
acquisition phase. This process is offered as an explanation
for the stronger spontaneous recovery response that is
observed with greater time between extinction and recovery
tests and thus supports the view that learning in the
extinction phase dissipates more rapidly than learning in the
acquisition phase (Brooks & Bouton, 1993; Rescorla, 2004;
Sissons & Miller, 2009).
Studies on forgetting provide valuable insight into its
significance in associative learning. Recent memories are

vulnerable to interference from other mental activity and
Wixted (2005) suggested that forgetting is largely a function
of nonspecific retroactive interference acting on memory
traces that have not yet consolidated in the neocortex. Wang
& Morris (2010) hypothesized that extinction trials involve
reactivation of the acquisition-trial memories in the absence
of further reinforcement. However, such interactions can be
complex and two memories may mutually exclude each
other or coexist depending on the timing of the signal during
extinction (Perez-Cuesta & Maldonado, 2009).
Decision making is treated differently in PC and foraging
models. In foraging models, the decision to leave a patch is
depends on which patch has the higher reward probability
(maximizing) or is selected probabilistically (matching)
(Kacelnik, Krebs , & Ens, 1987). PC models do not have
choice-based decision rules and express the response rate as
a monotonic function of the reward expectation. However, if
PC and foraging have the same basis, then PC models
contain a hidden decision rule in which the animal chooses
between proximal and distal information. However, decision
rules in both PC and foraging models are incomplete
because psychology, ecology, neuroscience, and machine
learning research show that uncertainty in the reward
assessment is an important factor in decision-making (Daw
et al., 2005; Platt & Huettel, 2008).

The Model
We now develop a model for PC that has application to
foraging models, draws on concepts from both modeling
frameworks, and has some analogy to the neurology of
decision-making. We model reward probability estimates
for distal and proximal information streams, which
correspond to the immediate patch and the surrounding
habit in foraging models and to the short- and long-term
estimates of rewards in PC models. We then combine the
estimates with weightings based on their respective
uncertainties. We also account separately for the process of
learning that a signal can indicate a reward. Finally, we use
the weighted expectation to model the animal’s response
rate in a trial.

Distal and Proximal Reward Estimates
For each trial we define the distal and proximal expected
reward estimate with a modified delta model,
(1)
xˆ j ,i  m j yi j ,i 1  xˆ j ,i 1
where j = 1, 2 indicates distal and proximal information
streams, i designates a PC trial, mj is the learning rate for
stream j. For each stream the error between the expected
reward probability and realized reward is
 j ,i  xi  xˆ j ,i
(2)

where xi is a reward/no-reward outcome (0,1) for trial i. The
term, yi, is a measure of the strength of the association of the
signal-reward and is independent of reward probabilities.
For convenience, we consider the distal and proximal
information streams unconscious reward estimators because
individually they are sub-process that must be combined to

1277



affect the animal’s response. We designate the combined
estimator the conscious reward estimate.

Combined Estimate
The distal and proximal estimates of reward probability are
combined into a single conscious reward estimate that the
animal uses in making decisions:

xi  w1,i xˆ1,i  w2,i xˆ2,i
(3)
where the estimates are combined according to their
respective weighting factors that depend on their associated
uncertainties  j2,i . As we develop next, the uncertainties are
in fact EWMAs of the variance in the distal and proximal
estimators and so the estimates can be combined using a
standard statistical weighting formula in which the weight
for estimate j on trial i is
w  1  2
1  2  1  2 .
(4)
j ,i



j ,i



1, i

2,i



It is noteworthy that this weighting scheme is not found in
either PC or foraging models.

Temporal Discounting Uncertainty
The uncertainties used in weighting,  j2,i , are developed from
the mean-squared errors of the distal and proximal reward
estimates. Of relevance, the uncertainties depend on the
time between trials as follows. First, compute unadjusted
uncertainty estimates as EWMAs from errors defined by eq.
(2):
ˆ 2j ,i  n( j2,i 1   j2,i 1 )   j2,i1 ,
(5)

 i  xi  xi .
(7)
Second, because errors are by nature random and one
correct prediction,  i  0 , is insufficient to develop an
association, we compute an average error with a EWMA:



(8)
 i2  n  i21   i21    i21 .
where n is again the uncertainty learning rate. Third, to
capture the repetitive and asymptotic nature of appetitive
learning, we incrementally accumulate the uncertainties
with a standard saturation function
i 1
i 1


 
yi 
1  k2  g  1  k2 
(9)


k 1
k

1


where g is the halfway point in the learning process.



Response Rate
We relate the conscious reward expectation to the response
rate with a matching function that asymptotically increases a
response from a background level to a maximum and is
defined with scale and shape parameters rmax and r as
 

R  rmax xi  xi  r (1  xi )  .
(10)

Parameter Summary
The complete model combines elements of classical
associative leaning and patch foraging. While several
models contain multiple memory streams that track
information over different time scales, the model presented
here tracks the uncertainties in the estimates as information
streams as well. The model contains 7 parameters (Table 1).

where n is the uncertainty learning rate. Next, adjust the
uncertainties for the time interval t  ti  ti 1 between trials:
 2  ˆ 2 h t
(6)
j ,i

j ,i

Table 1: Model parameters and values fitted to data.

j

where hj is a decay parameter that controls the rate at which
the uncertainty in information stream j decays as time
between trials increases. In this model, as the inter-trial time
increases, we want to put more confidence on the distal
(long-term) estimate and less on the proximal (short-term)
estimate. The idea being that in a sequence of trials with
uncertain outcomes, as time passes since the last trial we
should trust the long-term estimate of reward probability
more than the short-term estimate based only on the last few
rewards. To insure this shift in confidence to the distal
estimate, we decay the distal uncertainty but not the
proximal uncertainty as time passes between trials:
Mathematically this is achieved with 0  h1  1 and h2  1 .

Signal-Reward Association
The term yi in eq. (1) tracks the strength of the signal-reward
association, which we assume is distinct from probability
learning but also depends on the error of predictions.
Learning requires repetition and reduction of errors in
prediction, and we model these properties with a three step
process. First, we track conscious error based on the
difference between the trial outcome and the conscious
expectation from eq. (3) giving



Parameter
m1
m2
n
h1
g
rmax
r

Fitted
value
0.055
0.248
0.075
0.126
971
6.88
0.13

Meaning
Distal learning rate
Proximal learning rate
Uncertainty learning rate
Distal uncertainty decay rate
Association half-way constant
Response function scale parameter
Response function shape parameter

Comparison to Experiment
To demonstrate the flexibility and perspective the model
provides, we fit it to a study of partial reinforcement
extinction conducted by Haselgrove, et al. (2004). We
selected this experiment because PREE is difficult for PC
models to explain. In addition, the study covers an
acquisition phase and two extinction phases, which
demonstrate spontaneous recovery. Several models produce
these basic patterns but not when one of the groups is
trained with partial rewards.

1278

In the experiment, rats divided into partial and continuous
reinforcement groups received the same signal and number
of rewards during an acquisition phase in which the
reinforcement schedules differed. In the partial group, half
of the trials were reinforced with two rewards, while in the
continuous group one reward was given on every trial.
Following the acquisition sessions, the rats received two
sessions with unreinforced signals. In Figures 1-3, each
point designates an entire session in the acquisition phase,
while each point represents a block of two trials in the two
extinction sessions following.
We fit the model to the data from both groups with a
single set of parameters (Table 1) using the “mco” package
in the R statistical programming language. This is a multicriteria optimization algorithm based on a genetic algorithm
(cran.r-project.org/web/packages/mco/mco.pdf).
The model fit the response patterns for the continuous and
partial groups reasonably well. The mean responses in the
acquisition phase developed in a similar manner for both
groups, while in the extinction phase the continuous group
response decayed more rapidly than the response in the
partial group. Both groups exhibited spontaneous recovery
in the final extinction session with the continuous group
response again decaying faster than the partial group
response (Figure 1).

R

Acquisition

Extinction 1

Extinction 2

Figure 1: Haselgrove et al. (2004) data and model
fit for partial and continuous reinforcement
groups using parameter values in Table 1.

Discussion
The patterns of the underlying streams producing the fit to
the Haselgrove et al. (2004) data for the continuous
reinforcement group (Figure 2) and the partial reinforcement
group (Figure 3) illustrate how a framework of multiplescale estimators and uncertainties can account for seemingly
complex patterns in PC studies. As in Figure 1, the first
section consists of session averages for the acquisition
sessions, and the next two sections each represent an
extinction session in blocks of two trials.

Continuous Reinforcement Group In Fig. 2a the signalreward association strength, y, rises over the acquisition
phase to its full value and remains constant thereafter,
implying that the animal has fully learned the association.
The conscious reward probability also reaches its full value
in the acquisition phase and then exponentially declines in
the extinction phases. At the beginning of the second
extinction phase, the expectation is higher than at the end of
the first extinction phase, then the expectation again decays
since the animal receives no rewards. This somewhat
complex pattern of responses is generated by a unique
weighting of relatively simple patterns in the distal and
proximal estimators. The proximal estimator (Figure 2b),
which is generated by a faster learning coefficient, rises
quickly in the acquisition phase and then decays quickly in
the first extinction phase and remains at zero throughout the
second extinction phase. The distal estimator, being the
slow learner, rises slowly in the acquisition phase and then
decays slowly over the next two phases. The pattern in the
weights (Figure 2c) that mix the two estimators produces
the spontaneous recovery. Beginning in the acquisition
phase, the weightings are equal. Because rewards are
consistently received, the proximal estimator quickly adjusts
and has less uncertainty than the distal estimator, giving the
distal estimator the greatest weight in forming the conscious
estimator in eq. (4). In the period between the acquisition
and extinction phases, eq. (6) decays the distal uncertainty
(trust the long-term estimate when information is old), so
the two weights are nearly equal beginning the extinction
phase. However, as signals are consistently unrewarded, the
proximal estimator better represents the environment and its
weight rises over the trials. The distal uncertainty decays
again after the first extinction phase, and the pattern is
repeated in the second extinction phase. At the beginning of
the second extinction the proximal estimator, which predicts
a reward, has a higher weight than the distal estimator,
which predicts no reward, so the animal exhibits
spontaneous recovery.
Partial Reinforcement Group In the acquisition phase, the
patterns of conscious expectation and the signal-reward
association (Figure 3a) are similar to the patterns in the
continuous reinforcement group (Figure 2a), although the
strengths are half the continuous reinforcement values. The
patterns in the distal and proximal estimators are similar
also (Figure 3b), and again the strengths are about half
showing the accurate estimation of the 50% reward
probability during acquisition. However, the experiments
differ significantly in the weighing function patterns. These
are reversed in the partial reinforcement group (Figure 3c)
compared to that in the continuous reinforcement group
(Figure 2c). This difference drives the differences in the
response patterns (Figure 1). Again, at the beginning of the
experiment, the distal and proximal uncertainties are equal,
making for equal weights. However, both estimators

1279

a

a


x
y


x
y

b

b

xˆ j

xˆ j

c

c

wj

wj

Acquisition

Extinction 1

Extinction 2

Acquisition

Figure 2: Changes in model variables
for the continuous reinforcement group.

Extinction 1

Extinction 2

Figure 3: Changes in model variables
for the partial reinforcement group.

have higher uncertainty with partial reinforcement, but the
proximal estimator, which is strongly influenced by the
previous trial, has higher uncertainty than the distal
estimator, which integrates the reward expectation over
multiple trials. The result is lower uncertainty for the distal
estimator and thus greater weight in forming the conscious
estimator. Between the acquisition and extinction phases,
the distal uncertainty declines while the proximal
uncertainty is fixed, so the distal estimator is dominant at
the beginning of the first extinction phase. Over the phase
the distal uncertainty increases while the proximal
uncertainty decreases until they are equal at the end of the
extinction. Therefore, at the end of the extinction phase, the
animal has a higher response rate than in the continuous
case, which is dominated by the proximal estimator.
Between the first and second extinction phases, the distal
estimator uncertainty again decays giving it more weight in
the second extinction phase, resulting in a higher response
and slower decline in response for the partial acquisition
group.

Neurological Analogies
As our ultimate goal is to model the brain, not just observed
behavior, we seek to identify possible equivalences between
the model’s elements and neural structures as has been
encouraged by Rangel et al. (2008) and others. In a broad

sense, we suggest that the distal and proximal information
streams xˆ1,i , xˆ2,i represent parallel memory systems that
characterize reward probabilities estimated on different
temporal scales. These terms might be candidates for STMLTM systems involving the hippocampus and neocortex.
However, the two streams are competitive and so they might
be representative of competitive memory systems such as
the hippocampus and basal ganglia (White & McDonald,
2002; Poldrack & Packard, 2003). In our model the signalreward association yi represents a separate memory stream
that builds in a cumulative manner by summing the inverse
of trial-by-trial uncertainties. This incremental building of
memories is also a feature of the STM-LTM interaction of
the hippocampus and neocortex (McClelland, McNaughton,
& O'Reilly, 1995).
Uncertainty is specifically formulated in our model, and
neural structures are clearly involved with uncertainty in
decision-making. For example, Doya (2008) noted
uncertainty has two flavors: one resulting from the
environmental stochasticity (risk) and one from the limited
knowledge of the decision-maker (ambiguity). Studies
suggest that risk is represented in the striatum and
precuneus while ambiguity is represented in the lateral
obitofrontal cortex and amygdala (Platt & Huettel, 2008).
Our model also has two flavors of uncertainty. The

1280

uncertainty in the distal and proximal reward estimators ˆ j2,i
tracks variability in the environment that we suggest is akin
to risk uncertainty. The uncertainty in signal-reward

association  i2 is a candidate for the decision-maker’s
ambiguity.

Final Thoughts
Under the assumption that animals in laboratory studies use
behavioral strategies and neurological processes that
evolved through natural selection, we reconsider Pavlovian
conditioning in the context of animal foraging. From this
perspective, animal behavior in a constrained environment
has a hidden spatial component that leads us to consider the
behavior in terms of distal (habitat) and proximal (local
patch) information streams. In this framework the animal
does not track distinct memory streams for acquisition and
extinction phases, which we suggest is the experimenter’s
perspective. Instead of having to know when the
experimenter ends one phase and starts another, the animal
can view the environment as continuous yet random and
simply track information measuring over two different time
scales and weighting the estimates according to the trial-bytrial changes in their uncertainties.

Acknowledgments
This work was supported by the U.S. Army Engineer
Research and Development Center. Permission was granted
by the Chief of Engineers to publish this information.

References
Anderson, J. J. (2002). An agent-based event drive foraging
model. Natural Resource Modeling, 15(1), 55-82.
Bouton, M. E. (1993). Context, time, and memory retrieval
in the interference paradigms of Pavlovian learning.
Psychol Bull, 114(1), 80-99.
Brooks, D. C., & Bouton, M. E. (1993). A retrieval cue for
extinction attenuates spontaneous recovery. J Exp
Psychol Anim Behav Process, 19(1), 77-89.
Charnov, E. L. (1976). Optimal foraging, the marginal value
theorem. Theor Popul Biol, 9(2), 129-136.
Daw, N. D., Niv, Y., & Dayan, P. (2005). Uncertainty-based
competition between prefrontal and dorsolateral striatal
systems for behavioral control. Nat Neurosci, 8(12),
1704-1711.
Doya, K. (2008). Modulators of decision making. Nat
Neurosci, 11(4), 410-416.
Gabriele, A., & Packard, M. G. (2006). Evidence of a role
for multiple memory systems in behavioral extinction.
Neurobiology of Learning and Memory, 85(3), 289-299.
Gallistel, C. R., & Gibbon, J. (2000). Time, rate, and
conditioning. Psychological Review, 107, 289-344.
Haselgrove, M., Aydin, A., & Pearce, J. M. (2004). A
partial reinforcement extinction effect despite equal rates
of reinforcement during Pavlovian conditioning. J Exp
Psychol Anim Behav Process, 30(3), 240-250.

Kacelnik, A., Krebs , J. R., & Ens, B. (1987). Foraging in a
changing environment: an experiment with starlings
(Sturnus vulgaris). In A. K. a. S. J. S. M.L. Commons
(Ed.), Quantitative Analysis of Behavior Foraging (Vol.
6, pp. 63–87). Hillsdale: L. Erlbaum.
McClelland, J. L., McNaughton, B. L., & O'Reilly, R. C.
(1995). Why there are complementary learning systems
in the hippocampus and neocortex: insights from the
successes and failures of connectionist models of
learning and memory. Psychol Rev, 102(3), 419-457.
Moorter, B. V., Visscher, D., Benhamou, S., Börger, L.,
Boyce, M. S., & Gaillard, J.-M. (2009). Memory keeps
you at home: a mechanistic model for home range
emergence. Oikos, 118(5), 641-652.
O'Reilly, R. C., Frank, M. J., Hazy, T. E., & Watz, B.
(2007). PVLV: The Primary Value and Learned Value
Pavlovian
Learning
Algorithm.
Behavioral
Neuroscience, 121(1), 31-49.
Pearce, J. M., Redhead, E. S., & Aydin, A. (1997). Partial
reinforcement in appetitive Pavlovian conditioning with
rats. Quarterly Journal of Experimental Psychology,
50B, 273-294.
Platt, M. L., & Huettel, S. A. (2008). Risky business: the
neuroeconomics of decision making under uncertainty.
Nat Neurosci, 11(4), 398-403.
Poldrack, R. A., & Packard, M. G. (2003). Competition
among multiple memory systems: converging evidence
from
animal
and
human
brain
studies.
Neuropsychologia, 41(3), 245-251.
Rangel, A., Camerer, C., & Montague, P. R. (2008). A
framework for studying the neurobiology of value-based
decision making. Nat Rev Neurosci, 9(7), 545-556.
Rescorla, R. A. (2004). Spontaneous recovery. Learn Mem,
11(5), 501-509.
Rescorla, R. A., & Wagner, A. R. (1972). A theory of
Pavlovian conditioning: Variations in the effectiveness
of reinforcement and nonreinforcement. In A. H. Black
& W. F. Prokasy (Eds.), Classical conditioning II (pp.
64-99). New York: Appleton-Century-Crofts.
Sissons, H. T., & Miller, R. R. (2009). Spontaneous
recovery of excitation and inhibition. J Exp Psychol
Anim Behav Process, 35(3), 419-426.
Wang, S.-H., & Morris, R. G. M. (2010). HippocampalNeocortical Interactions in Memory Formation,
Consolidation, and Reconsolidation. Annual Review of
Psychology, 61(1), 49-79.
White, N. M., & McDonald, R. J. (2002). Multiple Parallel
Memory Systems in the Brain of the Rat. Neurobiology
of Learning and Memory, 77(2), 125-184.
Wixted, J. T. (2005). A Theory About Why We Forget
What We Once Knew. Current Directions in
Psychological Science, 14(1), 6-9.

1281

