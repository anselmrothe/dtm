UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Perceptually Grounded Word Meaning Acquisition: A Computational Model
Permalink
https://escholarship.org/uc/item/4dp0g5w2
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Glaser, Claudius
Joublin, Frank
Publication Date
2010-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                            Perceptually Grounded Word Meaning Acquisition:
                                                      A Computational Model
                                         Claudius Gläser (claudius.glaeser@honda-ri.de)
                                                        Honda Research Institute Europe
                                            Carl-Legien-Strasse 30, 63073 Offenbach, Germany
                                             Frank Joublin (frank.joublin@honda-ri.de)
                                                        Honda Research Institute Europe
                                            Carl-Legien-Strasse 30, 63073 Offenbach, Germany
                               Abstract                                   tions of words from few training samples. The thus acquired
   We present a computational model for the incremental acqui-            knowledge can be used to generalize to previously unseen
   sition of word meanings. Inspired by Complementary Learn-              scenes. Moreover, the framework is endowed with a learn-
   ing Systems theory the model comprises different compo-                ing mechanism that extracts features which are relevant to
   nents which are specifically tailored to satisfy the contradictory
   needs of (1) rapid memorization of word-scene associations             the core meaning of a word. This is done by exploiting the
   and (2) statistical feature extraction to reveal word meanings.        statistical evidence which resides from a word’s use in dif-
   Both components are recurrently coupled to achieve a memory            ferent contexts. Our model tightly couples the rapid memo-
   consolidation. This process reflects itself in a gradual transfer
   of the knowledge about a word’s meaning into the extracted             rization of word-scene associations with the statistical feature
   features. Thereby, the internal representation of a word be-           extraction. This results in learning dynamics which resemble
   comes more efficient and robust. We present simulation results         a gradual knowledge transfer and consolidation.
   for a visual scene description task in which words describing
   the relations between objects have been trained. This includes            We will present experimental results which validate the
   relations in size, color, and position. The results demonstrate        model. Therefore, the model has been applied in a simulated
   our model’s capability to acquire word meanings from few               visual scene description task where words for the relations
   training exemplars. We further show that the model correctly
   extracts word meaning-relevant features and therefore percep-          between pairs of geometric objects have been trained. This
   tually grounds the words.                                              includes relations in position, color, and size. The results
   Keywords: Word Learning; Computational Model; Comple-                  from this experiment illustrate that our model rapidly acquires
   mentary Learning Systems; Categorization                               word meanings from few training exemplars and further ex-
                                                                          tracts word meaning-relevant features.
                           Introduction
                                                                             The remainder of this paper is organized as follows. Next,
When hearing a novel word, a language learner has to as-                  we will review existing approaches for word meaning acqui-
sociate the word with its meaning. Establishing such word-                sition and relate our model to them. Afterwards, we will state
meaning mappings is an inherently difficult task as the learner           contradictory needs that computational models have to sat-
initially cannot know to what the word refers to. Quine (1960)            isfy. We proceed with the presentation of our computational
illustrated this problem with the example of a stranger who               model and subsequently show experimental results for it. Fi-
hears a native saying ”gavagai” after seeing a rabbit. How                nally, we give a summary and outline our future work.
can the stranger determine the meaning of ”gavagai”? It may
refer to the rabbit, a part of the rabbit, its color, any fast mov-                              Related Work
ing animal, or even that a rabbit is tasty. This problem, usu-
ally referred to as referential uncertainty, cannot be solved             Existing computational models address different levels of ref-
from a single word-scene pairing. Rather the use of the word              erential uncertainty. Firstly, there are approaches which con-
in different contexts enables the learner to extract its mean-            sider the problem of how a learner establishes a mapping be-
ing. Nevertheless, children learn the meaning of words from               tween words and a set of pre-defined meanings (e.g. Siskind,
few exposures to them. They rapidly construct hypotheses                  1996; K. Smith, Smith, Blythe, & Vogt, 2006; Fontanari,
about word meanings, which may initially be linked to spe-                Tikhanoff, Cangelosi, Ilin, & Perlovsky, 2009). In these
cific contexts in which the words occurred. Over time, how-               models the first occurrence of a word typically induces mul-
ever, children generalize among different observations, even              tiple hypotheses about its meaning. These hypotheses be-
though this may result in an overextension of a word’s use                come subsequently pruned either by incorporating learning
(MacWhinney, 1998). This remarkable ability of children                   constraints (Markman, 1990) or via cross-situational learn-
has been subject to many studies and resulted in numerous                 ing (L. Smith & Yu, 2008) - a technique making use of the sta-
theories on early word learning.                                          tistical evidence across many individually ambiguous word-
   In this paper we present a computational model for the in-             scene pairings. However, these models disregard the fact
cremental acquisition of word meanings which is inspired by               that learners can seldom rely on a set of pre-established con-
the learning capabilities of children. More precisely, the sys-           cepts. Word meanings rather become flexibly constructed and
tem has been designed to rapidly build internal representa-               shaped through language use (Boroditsky, 2001).
                                                                      1744

   Therefore, a second group of models further asks how lan-            Complementary Learning Systems Theory
guage use yields sensori-motor concepts to which words be-          The way how children acquire the meaning of new words
come associated (e.g. Steels & Kaplan, 2002; Skocaj et al.,         is fascinating in multiple respects. When they hear a word
2007; Kirstein, Wersing, Gross, & Körner, 2008; Wellens,           for the first time they already get a glimpse on what it may
Loetzsch, & Steels, 2008). In these models the learner ob-          mean. This ability may be facilitated by learning constraints
serves the world through multiple (analog or discretized) in-       or biases (Markman, 1990). It is anyway non-disputable that
put channels. The words finally serve as labels for cate-           even the exposure to just a few uses of the word enables
gories, which become incrementally constructed on the multi-        the child to generalize and apply the word in novel contexts.
dimensional input space and gradually refined by concentrat-        Even though generalization may occasionally result in errors
ing on the most important input dimensions.                         (MacWhinney, 1998), over time children robustly identify the
   Lastly, there are models which aim at the acquisition of         core meaning of a word.
both phonological form and semantic form. Such models ei-
                                                                    A Computational Learning Dilemma
ther build perceptual clusters in the acoustic space and the
semantic space and subsequently associate them with each            Modeling word meaning acquisition computationally, how-
other (Yu & Ballard, 2003; Goerick et al., 2009) or cluster-        ever, is difficult as contradictory needs have to be simulta-
ing is directly carried out in the joint acoustic-semantic space    neously satisfied. McClelland, McNaughton, and O’Reilly
(Roy & Pentland, 2002).                                             (1995) illustrated this fact on the example of artificial neu-
                                                                    ral network models: On the one hand, the learning from few
   The model we present in this paper falls into the second
                                                                    training samples requires a rapid or even one-shot memoriza-
group of methods, i.e. based on the observation of multiple
                                                                    tion of the items which can be achieved by using high learning
word-scene pairs it acquires perceptual categories by which
                                                                    rates. This implies that localized representations, which keep
the words become grounded. To achieve realistic word mean-
                                                                    the memory items separated from each other, have to be used.
ing acquisition we further place several requirements on our
                                                                    Otherwise, a neural network would suffer from catastrophic
model: (1) It should be capable of learning during online op-
                                                                    forgetting - the problem that the incorporation of new knowl-
eration. Consequently, the model has to apply incremental
                                                                    edge overwrites previously memorized items. On the other
learning techniques as training exemplars sequentially arise
                                                                    hand, the extraction of the core structure underlying a word
during a learner’s interaction with its environment. (2) The
                                                                    meaning necessitates a statistical learning approach as knowl-
model should further rapidly learn from few examples and af-
                                                                    edge has to be accumulated over many training exemplars.
terwards apply the acquired knowledge to generalize to novel
                                                                    Such a learning can be achieved using low learning rates and
scenes. (3) However, to be efficient and robust the internally
                                                                    overlapping representations. Artificial systems, which learn
built categories should reflect the core structure underlying
                                                                    from few examples while they simultaneously extract statis-
the word meanings. Thereby, we use the term core struc-
                                                                    tical evidence, are consequently difficult to achieve.
ture to refer to the essential aspects which define the mean-
ing of a word. (4) Lastly, for systems with minimum pre-            A Solution to the Problem
defined knowledge this core structure is usually hidden and         Obviously, humans (and particularly children) successfully
thus cannot be directly accessed by concentrating on input          solve this learning task. Endowing artificial systems with
dimensions which carry the meaning. The model rather has            mechanisms inspired by human learning may consequently
to extract word meaning-relevant feature dimensions in terms        lead a way to overcome the dilemma. Complementary Learn-
of a transformation from the input space.                           ing Systems (CLS) Theory (McClelland et al., 1995) suggests
   The combination of these requirements is what distin-            that the human brain makes use of separate but tightly cou-
guishes our model from existing approaches. Particularly            pled learning and memory devices which are specifically tai-
the combination of rapid incremental learning with word             lored to satisfy the contradictory needs. More precisely, it is
meaning-relevant feature extraction has (to our best knowl-         proposed that new memories are first stored in the hippocam-
edge) not been realized previously. In (Skocaj et al., 2007;        pal system which is known to perform rapid learning while
Wellens et al., 2008) and most notably (Kirstein et al., 2008)      utilizing localized representations. The hippocampal system
feature selection is applied, i.e. the learning focuses on the      further allows the reactivation of recent memories during rest
input dimensions which are considered to be relevant for rep-       or sleep. This reactivation in turn enables neocortical areas to
resenting the word meanings. By doing so the approaches in-         extract the core structure underlying different memories via
herently rely on the assumption that words can be grounded          interleaved learning - a technique where new items become
in a subset of the input dimensions. This in turn means that        gradually learned while learning is interleaved with the mem-
significant knowledge about the words to learn has to be put        orization of other items. Consequently, a gradual memory
into the system by the designer. In contrast, our system gen-       consolidation and transfer from the hippocampal system to
erates new word meaning-relevant feature dimensions out of          neocortical sites can be observed. Furthermore, there is be-
a set of basic input dimensions. We consider this ability to be     havioral and neuroscientific evidence which is in accordance
crucial for life-long incremental learning systems for which        with a CLS theory for the lexical and semantic acquisition of
the extent of words to be learned is unknown at design time.        novel words (Davis & Gaskell, 2009).
                                                                1745

    input                       feature                 category                                        category membership
                Feature                                                                            -1 / +1
      x                             y    Categorization     c
              Extraction
                                                                              Σ
                                  (a)
  category                                                                                                              experts
      c                                                                                                        ...
    input                       feature                                                                                   feature space
                Feature
      x                            y     Categorization
               Extraction
                               samples
                                 (y',c')                                        Figure 2: The architecture of an NGnet.
                                  (b)
Figure 1: Architecture of the computational model: (a) Input         category labels becomes maximized. By relying on Renyi’s
samples x become transformed into feature patterns y which           quadratic entropy H2 (Y ) and its estimation using Parzen win-
are subsequently categorized. (b) During learning the system         dows (Hild et al., 2006) the criterion to be maximized is
components are recurrently coupled (see text for details).
                                                                                        1 K
                                                                      I(Y ;C) = − log     ∑ G(y(k) − y(k − 1), 2σ2 )
                                                                                       K k=1
                   Computational Model                                                            Kj
                                                                                                                                       ! (1)
                                                                                      Kj      1                                      2
In what follows we treat word learning as category learning.          +     ∑         K
                                                                                         log
                                                                                             Kj  ∑ G(y j (k) − y j (k − 1), 2σ )        .
                                                                         j∈{−1,+1}               k=1
This is reasonable as a word refers to collections of enti-
ties which belong to the same category. Word meanings are                                            T
consequently the conditions underlying category membership           Here, G(z, σ2 I) = exp(− 12 z2σz)2 ) is a Gaussian kernel, y+1 (k)
(Bloom, 2000). We restrict our description to the learning of        and y−1 (k) denote the k-th exemplars of feature patterns be-
one word. Multiple words can be learned straightforwardly            longing to a category or not, K+1 and K−1 are the numbers of
by creating multiple instances of the system. As shown in            such patterns, and K = K+1 + K−1 . Since y(k) = R · x(k), we
Fig. 1, the framework consist of a feature extraction layer          can estimate R via stochastic gradient ascent on I(Y ;C).
and a categorization layer which are recurrently coupled. The           To de-correlate the feature dimensions and to perform di-
feature extraction transforms an input pattern x into a feature      mensionality reduction we additionally apply Principal Com-
pattern y for which a category membership c is subsequently          ponent Analysis (PCA) on the extracted features. By assum-
calculated. Here, c is a binary variable which signals whether       ing the inputs x to be white with zero mean and unit variance,
the category’s word label is appropriate for the description of      the principal feature dimensions can be obtained via eigende-
the input pattern (c = +1) or not (c = −1).                          composition of R · RT . Let Ψ be the matrix of eigenvectors
    Our model is largely inspired by CLS theory. Nevertheless,       whose cumulative energy content exceeds a threshold. Then
the model is not meant to provide a 1:1 mapping to certain           we calculate feature patterns y according to
brain areas. It rather resembles CLS theory from a functional                              y = Ω · x = ΨT · R · x.                        (2)
perspective. For this reason, we will highlight functional cor-
respondences of our model with different brain areas.                Categorization
                                                                     To incrementally learn a category we use an adaptive Normal-
Feature Extraction
                                                                     ized Gaussian Network (NGnet) which we recently proposed
In the feature extraction layer word meaning-relevant fea-           (Gläser & Joublin, 2010). As shown in Fig. 2, the NGnet is
tures, which facilitate the subsequent categorization of a pat-      composed of multiple locally operating experts, each of them
tern, should become extracted. The learning consequently has         being responsible for features stemming from its associated
to exploit the statistical evidence stemming from the observa-       input region. The category membership c ∈ {−1, 1} of a fea-
tion of multiple word-scene pairings. Such a statistical fea-        ture pattern y is calculated according to
ture extraction is obviously part of neocortical learning.                                   "                                 #
                                                                                                                 M
    In (Hild, Erdogmus, Torkkola, & Principe, 2006) a learn-                                           1
                                                                                c(y) = sign                    · ∑ αi · φi (y) .          (3)
ing technique called Maximizing Renyi’s Mutual Information                                      ∑Mj=1 φ j (y) i=1
(MRMI) has been proposed. MRMI tries to maximize the in-
formation that the feature patterns carry about category mem-        Here, M denotes the number of experts and αi the weight
berships. Hence it is ideally suited to accomplish the learn-        of expert i to the output neuron. Furthermore, φi (y) is the
ing task. We restrict learning to a linear feature extraction of     response of the i-th expert to feature y which is described by
form y = R · x. We consequently aim at the identification of         a multivariate Gaussian of form
a transformation matrix R such that the mutual information
                                                                                                                                
                                                                                                 1
I(Y ;C) = H(Y ) − H(Y |C) between the feature patterns and                     φi (y) = exp − · (y − µi )T Σ−1     i (y  −  µi )   ,      (4)
                                                                                                 2
                                                                 1746

where µi and Σi denote the center and covariance matrix of the                                                            "obj1 is larger than obj2"
                                                                                            J
Gaussian. The decision whether a feature pattern belongs to                                                                           obj2
                                                                     A                               E                    A B C D E F GH I J K LM
a category is finally obtained by application of the sign func-           M                                           A
                                                                                                 H
tion to the continuously valued output. The network parame-                                                           B
                                                                                                         F            C
ters are determined during online operation via Expectation-         G                  B
                                                                                                                      D
                                                                                                                      E
Maximization (EM) training as proposed in (Xu, 1998).                                                          obj1   F
   Since the NGnet statistically learns an internal category                  I                                       G
                                                                                             K                        H
representation which associates inputs from different modal-                                                          I
                                                                                                                      J
ities, our categorization layer functionally resembles multi-                     D                      L            K
                                                                      C                                               L
modal associative cortices, e.g. the perirhinal cortex. How-                                                          M
ever, our adaptive NGnet is additionally endowed with mech-
                                                                                      (a)                                          (b)
anisms which allow a demand-driven allocation and removal
of experts (Gläser & Joublin, 2010). This enables the network    Figure 3: In (a) an example scene used in the visual descrip-
to perform a one-shot memorization of word-scene associa-         tion task is depicted. In (b) the output of the model after learn-
tions. Our categorization layer consequently also models the      ing the meaning of is larger than is shown. Black circles cor-
rapid initial learning as it is carried out by the hippocampus.   respond to category members, white circles to non-members,
   More precisely, our model accomplishes network growth          and dotted circles denote errors made by the system.
and pruning as follows: (1) New word-scene associations be-
come memorized based on the novelty or surprise of an input
sample. Similarly, already memorized associations become              Afterwards, the generated samples are used to train the fea-
(2) pruned if they became redundant, (3) split if the inter-      ture extraction. In other words, the feature extraction searches
nal representation has to be refined, or (4) merged if they are   for commonalities among the reactivated patterns and tries
sufficiently similar. For a detailed description of these mech-   to extract the condition which discriminates between mem-
anisms we refer to (Gläser & Joublin, 2010).                     bers and non-members of the category. This learning process
                                                                  changes the feature space the categorization layer is operat-
Coupling of the Components                                        ing on. For this reason, we finally adapt the NGnet to the
Inspired by CLS theory we finally couple the slow statistical     changed feature space in an analytic way. Since we use a lin-
feature extraction and the rapid category learning. As shown      ear feature extraction, the change in the feature space can be
by the pseudo-code in Alg. 1 the incremental learning mech-       expressed in terms of an affine transformation ỹ = A · y with
anism consists of four steps which are carried out every time     A = Ω̃ · Ω−1 . Here, Ω and Ω̃ denote the feature extraction ma-
a new training exemplar is obtained.                              trices before and after the learning. We consequently adjust
                                                                  a local expert’s receptive field by calculating its new center
Algorithm 1 Incremental Learning                                  µ̃i according to µ̃i = A · µi as well as its associated covariance
                                                                  matrix Σ̃i according to Σ̃i = A · Σi · AT .
  Initialize the feature extraction to R = I, Ψ = I                   Since these learning steps are carried out iteratively, knowl-
  Initialize an empty NGnet                                       edge about a category becomes consolidated as more training
  for all training samples (x, c) do                              samples are processed. The knowledge, which has been first
     Update the NGnet with (y, c)                                 acquired in the categorization layer (via the memorization of
     Generate a set of samples (y′ , c′ ) using the NGnet         word-scene associations), becomes gradually transfered into
     Train the feature extraction on the generated samples        the extracted features. Due to the fact that the extracted fea-
     Adapt the NGnet to the changed feature space                 tures facilitate the categorization task, this knowledge transfer
  end for                                                         leads to a more robust categorization as well as a less complex
                                                                  NGnet needed to represent the category.
   After updating the NGnet with a training sample, the inter-
nal representation of a category is used to reactivate memo-                                Experimental Results
rized associations. This step resembles hippocampal dream-        We evaluated our computational model in a visual scene de-
ing. We consequently produce a set of samples (y′ , c′ ) com-     scription task in which the meaning of words for the relations
posed of feature patterns y′ and associated category member-      between objects has to be acquired. Thereby, a learner and a
ships c′ . To do so, we first determine whether a local expert    tutor observe a scene composed of geometric objects as the
i represents category members (c′ = +1) or non-members            one shown in Fig. 3(a). The tutor selects two out of the ob-
(c′ = −1) and next randomly draw feature patterns y′ from         jects and describes the relation between them, e.g. by saying
its Gaussian-shaped receptive field. Since the receptive field
is described by its mean µi and covariance matrix Σi , a fea-                                    ”K is larger than D.”
ture pattern y′ can be generated by y′ = µi + B · z, where        Based on such exemplars of word use the learner has to incre-
z ∼ N (0, I) is a random vector and B is obtained from the        mentally build-up internal concepts which correspond to the
Cholesky decomposition B · BT = Σi .                              words’ meanings. The training of the model is illustrated in
                                                              1747

                       "K is larger than D"                                                100
                                                                        correct cat. (%)
  input vector                                         word labels
                                                                                            80                                            is to the left of
                                                          left                                                                            is larger than
                     Feature Extr.   Categorization                                                                                       is brighter than
                                                          right                             60
                     Feature Extr.   Categorization
   object K
                                                         above                                   0         50            100           150           200
                     Feature Extr.   Categorization                                                                      # samples
                                                         below                                                               (a)
                     Feature Extr.   Categorization
                                                                                            50
                                                         larger
                                                                        # experts
                     Feature Extr.   Categorization                                         40
                                                         smaller                            30
                                                                                            20
   object D
                     Feature Extr.   Categorization
                                                                                            10
                                                        brighter                             0
                     Feature Extr.   Categorization                                              0         50            100           150           200
                                                         darker                                                          # samples
                     Feature Extr.   Categorization                                                                          (b)
Figure 4: The training of the model in the visual scene de-             Figure 5: The evolution of (a) the correct categorization rate
scription task is illustrated (see text for details).                   and (b) the complexity of the NGnet is shown for the learning
                                                                        of different words.
Fig. 4. For the present experiment we consider the learner to
have sufficient syntactical knowledge to identify the objects           words resulted in qualitatively similar curves.
of interest (e.g. K and D) as well as the word to be learned               From the plots we see that the system performance rapidly
(e.g. is larger than). For computational purposes we further            increases during the presentation of the first training exem-
did not carry out the experiment in direct interaction with the         plars and afterwards converges towards a near optimal level.
system, but rather used simulated scenarios which provide a             In contrast, the complexities of the classifiers also increase
ground truth for performance evaluation.                                at the beginning, but subsequently decrease and maintain a
   Each of the objects in a scene is represented by its abso-           low level afterwards. The observed behavior of the model
lute position, its width and height, as well as its RGB color           is in-line with CLS theory, insofar as it can be explained
value. Consequently, tuples composed of a 14-dimensional                by two complementary learning processes which run at dif-
perceptual vector (7 dimensions per object) as well as a word           ferent timescales: (1) Initially, new knowledge is rapidly
label served as training inputs to the system. Words for object         memorized. In our model this is accomplished by the on-
relations concerning position (is to the left of, is to the right       demand allocation of local experts within the classifier. After
of, is above, is below), size (is larger than, is smaller than),        a while, the experts adequately represent upcoming training
and color (is brighter than, is darker than) has been trained.          samples such that they do not have to be memorized addi-
However, it is important to note that the system did not have           tionally. Consequently, the classifier complexity as well as
prior knowledge about the relevance of input dimensions with            the system performance increase at the beginning. (2) After-
respect to the meaning of the words. In contrast, important             wards, knowledge is gradually transferred. In our model the
dimensions (e.g. the relative object positions) are even not            knowledge shifts into the iteratively extracted word meaning-
present and have to be extracted by the system. For each of             relevant features. These features facilitate the classification
the words to learn we applied an adaptive NGnet as a binary             task such that a less complex classifier can be applied. At
categorization module and further extracted word meaning                the same time, however, the internal representation of a word
relevant features. To cope with missing negative training data          meaning becomes more robust and, thus, further increases the
we implemented the mutual exclusivity bias between words                system performance.
related to object positions, sizes, and colors, respectively. In           After training, an analysis of the extracted features revealed
other words, a positive training exemplar for is larger than            that the built categories solely rely on the meaning of the cor-
has been additionally used as negative training sample for is           responding words. For example, for representing the meaning
smaller than (Regier, 1996).                                            of is larger than the feature
   The results of this experiment are shown in Fig. 5. In (a)                                (widthob j1 + heightob j1 ) − (widthob j2 + heightob j2 )
we plot the system performance for the learning of individ-             has been extracted which is an adequate linear approximation
ual words. The performance (correct categorization rate) has            of the real decision criteria
been determined on a set of scenes not included in the training
data. In (b) we further plot the complexity of the individual                               (widthob j1 · heightob j1 ) − (widthob j2 · heightob j2 ) > 0.
classifiers for which the number of local experts comprising            Similarly, the relative horizontal and vertical positions have
the NGnet is an indicator. To keep the plots readable, here we          been extracted for the description of spatial relations. This
restrict ourselves to curves for the learning of is to the left of,     shows that our framework is able to acquire the meaning of
is larger than, and is brighter than. The learning of the other         words and consequently grounds them.
                                                                     1748

   Finally, the output of the classifiers can be used to describe      Perlovsky, L. (2009). Cross-situational learning of object-
a visual scene. For the scenario depicted in Fig. 3(a), we             word mapping using Neural Modeling Fields. Neural Net-
show the output of our framework concerning the judgment               works, 22(5-6), 579–585.
whether an object is larger than another object in Fig. 3(b).        Gläser, C., & Joublin, F. (2010). An adaptive normalized
As can be seen, objects pairs are correctly categorized except         gaussian network and its application to online category
for rare cases in which the object sizes are very similar.             learning. In Proc. IJCNN.
                                                                     Goerick, C., Schmuedderich, J., Bolder, B., Janssen, H.,
               Summary & Future Work                                   Gienger, M., Bendig, A., et al. (2009). Interactive online
In this paper we presented a computational model for the               multimodal association for internal concept building in hu-
incremental acquisition of word meanings. The novelty of               manoids. In Proc. IEEE-RAS Int. Conf. on Humanoids.
the framework stems from its combined ability to (1) rapidly         Hild, K., Erdogmus, D., Torkkola, K., & Principe, J. (2006).
build categories which correspond to the learned words while           Feature extraction using information-theoretic learning.
(2) it simultaneously extracts features which underly the              IEEE Trans. on Pattern Anal. and Machine Intell., 29(9),
meaning of the words. We consider these abilities to be fun-           1385–1392.
damental for life-long incremental learning systems which            Kirstein, S., Wersing, H., Gross, H. M., & Körner, E. (2008).
have to cope with minimal predefined task knowledge. To                An integrated system for incremental learning of multiple
satisfy the contradictory needs of rapid learning from few ex-         visual categories. In Proc. ICONIP.
amples as well as statistical feature extraction we modeled          MacWhinney, B. (1998). Models of the emergence of lan-
learning mechanisms which are known to be beneficial for               guage. Annu Rev Psychol, 49, 199–227.
humans. More precisely, our framework resembles CLS the-             Markman, E. M. (1990). Constraints children place on word
ory insofar as it uses separate but tightly coupled components         meanings. Cognitive Science, 14(1), 57–77.
which are specifically tailored to meet these criteria.              McClelland, J., McNaughton, B., & O’Reilly, R. (1995).
   We evaluated our model in a visual scene description task,          Why there are complementary learning systems in the hip-
where words for the relations between objects have been                pocampus and neocortex: insights from the successes and
taught. Our results demonstrate that the system acquires word          failures of connectionist models of learning and memory.
meanings based on the observation of just a few word-scene             Psychol Rev, 102(3), 419–457.
pairings. It subsequently uses its knowledge to generalize to        Quine, W. V. O. (1960). Word and object. MIT Press.
novel scenes. The results further showed that the system im-         Regier, T. (1996). The human semantic potential: Spatial
plements a memory consolidation process in which knowl-                language and constrained connectionism. MIT Press.
edge about a word’s meaning gradually shifts from the rapidly        Roy, D., & Pentland, A. (2002). Learning words from sights
learned category representation into the slowly extracted fea-         and sounds: A computational model. Cognitive Science,
tures. This consolidation process is beneficial as it abstracts        26(1), 113-146.
the core meaning of a word and, hence, lets the internal rep-        Siskind, J. M. (1996). A computational study of cross-
resentation of a word become more robust and efficient.                situational techniques for learning word-to-meaning map-
   Part of our future work will be the extension of the model          pings. Cognition, 61(1-2), 39–91.
to incorporate a non-linear feature extraction. This would al-       Skocaj, D., Berginc, G., Ridge, B., Vanek, O., Hutter, M., &
low the system to extract more complex dependencies which              Hawes, N. (2007). A system for continuous learning of
may underly a word’s meaning. Secondly, we will endow the              visual concepts. In Proc. ICVS.
model with a mechanism which detects the mutual exclusivity          Smith, K., Smith, A. D. M., Blythe, R. A., & Vogt, P. (2006).
between words. This learning bias is currently pre-defined,            Cross-situational learning: A mathematical approach. In
but has to be autonomously applied by the system to enable a           Symbol grounding and beyond. Springer.
learning of an arbitrary set of words. Lastly, we will extend        Smith, L., & Yu, C. (2008). Infants rapidly learn word-
our teaching scenario to include social learning. Social learn-        referent mappings via cross-situational statistics. Cogni-
ing enables an active learning by the system which is useful           tion, 106(3), 1558–1568.
for testing hypotheses about a word’s meaning.                       Steels, L., & Kaplan, F. (2002). Aibos first words: The social
                                                                       learning of language and meaning. Evolution of Communi-
                         References                                    cation, 4(1), 3–32.
Bloom, P. (2000). How children learn the meaning of words.           Wellens, P., Loetzsch, M., & Steels, L. (2008). Flexible word
  MIT Press.                                                           meaning in embodied agents. Connection Science, 20(2–
Boroditsky, L. (2001). Does language shape thought? Man-               3), 173–191.
  darin and English speakers’ conceptions of time. Cognitive         Xu, L. (1998). RBF nets, mixture experts, and Bayesian
  Psychology, 43(1), 1–22.                                             Ying-Yang learning. Neurocomputing, 19, 223–257.
Davis, M., & Gaskell, M. (2009). A complementary systems             Yu, C., & Ballard, D. H. (2003). A computational model
  account of word learning: neural and behavioural evidence.           of embodied language learning (Tech. Rep. No. TR791).
  Phil. Trans. R. Soc. B, 364(1536), 3773–3800.                        University of Rochester.
Fontanari, J., Tikhanoff, V., Cangelosi, A., Ilin, R., &
                                                                 1749

