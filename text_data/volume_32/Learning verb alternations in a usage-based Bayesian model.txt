UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning verb alternations in a usage-based Bayesian model
Permalink
https://escholarship.org/uc/item/186313ch
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Parisien, Christopher
Stevenson, Suzanne
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                    Learning verb alternations in a usage-based Bayesian model
                                           Christopher Parisien and Suzanne Stevenson
                                        Department of Computer Science, University of Toronto
                                                           Toronto, ON, Canada
                                                     {chris, suzanne}@cs.toronto.edu
                              Abstract                                    One way to bring these opposing positions together is to
                                                                       demonstrate, using naturalistic data, how to connect a usage-
   One of the key debates in language acquisition involves the
   degree to which children’s early linguistic knowledge employs       based representation of language with abstract syntactic gen-
   abstract representations. While usage-based accounts that fo-       eralizations. We argue that alternation structure can be ac-
   cus on input-driven learning have gained prominence, it re-         quired and generalized from usage patterns in the input, with-
   mains an open question how such an approach can explain the
   evidence for children’s apparent use of abstract syntactic gen-     out a priori expectations of which alternations may or may
   eralizations. We develop a novel hierarchical Bayesian model        not be acceptable in the language. We support this claim us-
   that demonstrates how abstract knowledge can be generalized         ing a hierarchical Bayesian model (HBM) which is capable of
   from usage-based input. We demonstrate the model on the
   learning of verb alternations, showing that such a usage-based      making inferences about verb argument structure at multiple
   model must allow for the inference of verb class structure, not     levels of abstraction simultaneously. We show that the in-
   simply the inference of individual constructions, in order to       formation relevant to verb alternations can be acquired from
   account for the acquisition of alternations.
                                                                       observations of how verbs occur with individual arguments
   Keywords: Verb learning; language acquisition; Bayesian
   modelling; computational modelling.                                 in the input. In this sense, we present a competency model
                                                                       showing what can be acquired, but we do not make claims
                          Introduction                                 regarding the specific processing mechanisms involved.
An important debate in language acquisition concerns the na-              From a corpus of child-directed speech, our model acquires
ture of children’s early syntax. On one side of the debate lies        a wide variety of argument structure constructions over hun-
a claim that children develop their syntactic knowledge in an          dreds of verbs. Moreover, by forming classes of verbs with
item-based manner. This claim of usage-based learning ar-              similar usage patterns, the model can generalize knowledge
gues that very young children associate verb argument struc-           of alternation patterns to novel verbs. This stands in contrast
ture with specific lexical items, only gradually abstracting           to earlier models which have focused on either the acquisition
syntactic knowledge after four years of age (e.g., Tomasello,          of the constructions themselves, or the formation of classes
2003). An alternative claim suggests that young children do            over given constructions. The integration in our model of
indeed possess abstract syntactic representations—i.e., gen-           these two important aspects of verb learning has implications
eralizations about the structure of their language that are not        for current theories of language acquisition, by showing how
necessarily tied to lexical items (e.g., Fisher, 2002).                abstract syntactic knowledge can be acquired and generalized
   Syntactic alternation structure is often considered to be a         from usage-level input.
central phenomenon in this debate. Consider the following
                                                                                              Related work
example of the English dative alternation:
                                                                       Previous computational approaches to language acquisition
(1) I gave a toy to my dog.
                                                                       have used HBMs to represent the abstract structure of verb
(2) I gave my dog a toy.                                               use. Alishahi and Stevenson (2008) used an incremental
   These sentences mean roughly the same thing, but are ex-            Bayesian model to cluster individual verb usages (or tokens),
pressed in different ways. The first, a prepositional dative,          simulating the acquisition of verb argument structure con-
expresses the theme (a toy) as an object and the recipient (my         structions. Using naturalistic input, the authors showed how a
dog) in a prepositional phrase. The second, a double-object            probabilistic representation of constructions can explain chil-
dative, expresses both the theme and recipient as objects and          dren’s recovery from overgeneralization errors. In another
reverses their order.                                                  Bayesian model of verb learning, Perfors et al. (2010) clus-
   Verbs that allow similar alternations often have similar se-        ter verb types by comparing the variability of constructions
mantics (Levin, 1993), which suggests that alternations re-            for each of the verbs. The model can distinguish alternating
flect much of our cognitive representations of verbs. Fur-             from non-alternating dative verbs and can make appropriate
thermore, these regularities appear to influence our language          generalizations when learning novel verbs.
use. In word learning experiments, children as young as three             Both of the above models show realistic patterns of gen-
years of age appear to use abstract representations of the da-         eralization, but they operate at complementary levels of ab-
tive alternation (Conwell & Demuth, 2007). While this is ev-           straction. The model of Alishahi and Stevenson does not cap-
idence of abstract syntax at a very young age, it does not nec-        ture the alternation patterns of verbs, while Perfors et al. as-
essarily invalidate the usage-based hypothesis, since the ab-          sume that the individual constructions participating in the al-
stractions may originate from item-specific representations.           ternation have already been learned. Furthermore, Perfors et
                                                                   2674

al. limit their model to only consider two possible construc-                 Features                       Description
tions (the prepositional and double-object dative), and only                  OBJ, OBJ2                      Objects
the verbs that participate in those constructions.                            COMP, XCOMP                    Clausal complements
   In this work, we address both levels of abstraction of the                 PRED, CPRED, XPRED             Predicate complements
above models. We cluster individual verb usages to learn ar-                  LOC                            Locatives
gument structure constuctions and their patterns of use across                JCT, CJCT, XJCT                Adjuncts
many verbs, and we also cluster verb types to learn alternation               PP                             Prepositional phrases
behaviour, generalizing that behaviour to novel verbs. More-                  PREP                           Preposition (nominal value)
over, we use representative corpora of child-directed speech                  NSLOTS                         Number of slots used
to model the acquisition of verb alternation behaviour in the
context of many constructions, verbs, and alternations.                                           Table 1: Slot features.
   Vlachos et al. (2009) used a Dirichlet Process mixture
model to cluster verb types by their subcategorization pref-              Model 1: Argument structure constructions
erences, but did not address learning the argument structures
themselves. Other work has modelled different aspects of the              Like other topic models, the HDP (Teh et al., 2006) is es-
dative alternation, such as how discourse features affect the             sentially a model of category learning: the model clusters
expression of dative constructions (de Marneffe et al., submit-           similar items in the input to discover structure. Adopting a
ted), yet did not consider how these preferences are learned.             usage-based approach to language (e.g., Goldberg, 2006), we
                                                                          view the acquisition of verb argument structure as a category-
                     Model description                                    learning problem. In this view, structured verb knowledge
We discuss the feature representation of a verb usage and de-             translates well to the hierarchical nature of the model.
velop two contrasting models to show how alternation classes                  Model 1 is a straightforward adaptation of the HDP to verb
contribute to generalization in verb learning. Model 1 is                 argument structure, which we will use as a point of compari-
an adaptation of an existing probabilistic topic model, the               son for an extended model. Figure 1(a) provides an intuitive
Hierarchical Dirichlet Process (HDP; Teh et al., 2006), to                description of the hierarchical levels of inference in Model 1.
the problem of learning verb argument structure. Model 2,                 At level 1, the lowest level of abstraction, individual verb us-
a novel extension to the HDP, addresses the limitations of                ages yi are represented by sets of features as described above.
Model 1 by learning verb alternation classes, allowing reg-                   At level 2, the model clusters similar usages together to
ularities in construction use to be transferred to novel verbs.           form argument structure constructions, where a construction
                                                                          is represented by a set of multinomial distributions, one for
Verb features                                                             each feature. Since the clustering mechanism is nonparamet-
Following from existing approaches (as in Joanis, Stevenson,              ric, we need not specify the total number of constructions to
and James (2008)), we use syntactic “slot” features to en-                learn. Each of these constructions, denoted by its multino-
code basic argument information about a verb usage. Table                 mial parameters θ, probabilistically represents a pattern such
1 presents the 14 features used in our representation. The                as a simple transitive or a prepositional dative. While a con-
first 12 (up through “PP”) are binary features denoting the               struction here encodes only syntactic information, with no se-
presence or absence of the stated syntactic slot, such as an              mantic elements, the model can be generalized to a combined
object (OBJ) or a prepositional phrase (PP); the slots are in-            syntactic/semantic input representation.
dicated by labels used by the CHILDES dependency parser                       At level 3, a multinomial distribution for each verb (π) rep-
(Sagae et al., 2007).1 When a PP is present, the nominal                  resents the range of constructions that tend to occur with the
feature PREP denotes the preposition used. Such syntactic                 verb. For example, in Figure 1(a), give (π2 ) would have a high
slot features are easier to extract than full subcategorization           probability for the double-object dative and prepositional da-
frames. We make the assumption that children at this devel-               tive constructions (θ2 and θ3 , respectively), but a low proba-
opmental stage can distinguish various syntactic arguments in             bility for the transitive construction, θ1 . Let yi j denote feature
the input, but may not yet recognize recurring patterns such               j of usage i. Levels 1 through 3 are given by the following:
as transitive and double-object constructions. The following
examples show this representation used with a double-object                                  πv     ∼     Dirichlet(α · β)
dative and a prepositional dative, respectively:
                                                                                               zi   ∼     Multinomial(πv )
(3) I sent my mother a letter.
                                                                                            θ jzi   ∼     Dirichlet(1)
       h OBJ, OBJ2, PREP = null, NSLOTS = 2 i
                                                                                             yi j   ∼     Multinomial(θ jzi )
(4) I sent a letter to my mother.
       h OBJ, PP, PREP = to, NSLOTS = 2 i                                 The indicator variable zi selects a cluster (i.e., a construction,
    1 We consider only the slots internal to the verb phrase, for now     one of the θ) for usage i. Given a verb v, this is drawn from
ignoring syntactic subjects. We also do not attempt to distinguish        a multinomial distribution which includes a small probability
true arguments from adjuncts, a very difficult distinction to make.       of creating a new construction.
                                                                      2675

Figure 1: (a) Model 1, a Hierarchical Dirichlet Process applied to learning verb argument structure constructions. (b) Model 2,
an extension of Model 1 to learn verb alternation classes.
   The verb-specific distributions πv depend on hyperparame-          are less likely to occur as simple transitives. By recognizing
ters which encode expectations about constructions in gen-            the similarity of π2 and π3 , we can create a cluster contain-
eral, across all verbs. They represent acquired knowledge             ing give, show, and other similar verbs. Figure 1(b) presents
about the likely total number of constructions, which con-            this intuition in Model 2. We extend Model 1 by introducing
structions are more likely to occur overall, and so on:               a fourth level of abstraction, where we represent clusters of
                                                                      similar verbs. For each verb cluster c, we use φc to represent
                    γ    ∼   Exponential(1)                           the range of constructions that tend to occur with any of the
                    α    ∼   Exponential(1)                           verbs in that cluster. By serving as a prior on the verb-level
                    β    ∼   Stick(γ)                                 parameters πv , φc directly influences each verb in the cluster.
                                                                         The lower levels of this model are the same as in Model
As with lower-level parameters, these are influenced by ob-           1. In addition, the verb representations, πv , depend on the
served structure in the input. β, drawn from a stick-breaking         alternation classes in level 4:
process (Stick), encodes how many constructions will be used
                                                                                        φcv    ∼   Dirichlet(α0 · β0 )
and which constructions are more likely overall. α affects
the variability of πv . Large values of α push πv closer to β,                           πv    ∼   Dirichlet(α1 · φcv )
the global distribution over constructions, while smaller val-                             zi  ∼   Multinomial(πv )
ues encourage more variation among verbs. γ affects the to-                             θ jzi  ∼   Dirichlet(1)
tal number of constructions; small values of γ correspond to
                                                                                         yi j  ∼   Multinomial(θ jzi )
fewer constructions. By drawing α and γ from an exponen-
tial distribution, we give a weak preference for verb-specific        Each verb v belongs to a cluster of verbs, denoted cv . Now, πv
behaviour and for solutions with fewer constructions. These           depends on φcv , which gives a distribution over constructions
preferences are effectively designed into the model; they may         for all the verbs in the same cluster.
be informed by general human category-learning behaviour.                As before, these parameters themselves depend on top-
For further details of this model, see Teh et al. (2006).             level hyperparameters:
Model 2: Alternation classes                                                                γ0  ∼     Exponential(1)
Model 1 acquires argument structure constructions from in-                               α0,1   ∼     Exponential(1)
dividual verb usages, and learns how those constructions are                                β0  ∼     Stick(γ0 )
used by individual verbs, but it is unable to recognize that
certain kinds of verbs behave differently than others. Compe-         These hyperparameters serve similar roles to those in Model
tent language speakers regularly use this kind of information.        1. β0 gives a global distribution over all the constructions in
For example, if a verb occurs in a double-object dative con-          use. γ0 affects the total number of constructions overall. α1
struction, then we should infer that it is also likely to occur in    affects the variability of a verb compared with its class, and
a prepositional dative. We develop a novel extension of the           α0 affects the variability of verb classes.
above model to capture this phenomenon by learning clusters              To group verbs into alternation classes, we use a mecha-
of similar verbs.                                                     nism similar to the way we group individual verb usages into
   Recall that we represent a verb by a probability distribu-         constructions. Recall that cv acts as an indicator variable, se-
tion over the constructions in which it may occur. In the ex-         lecting a class for verb v from the available classes in level
ample shown in Figure 1(a), give and show both tend to oc-            4. This is drawn from a multinomial distribution σ which
cur with a double-object dative and a prepositional dative, but       includes a small probability of creating a new verb class:
                                                                  2676

                    γ1  ∼     Exponential(1)                        the proportions of various usages are identical for these verbs
                     σ  ∼     Stick(γ1 )                            across the development and evaluation sets.
                                                                       We implement both learning models using an adaptation of
                    cv  ∼     Multinomial(σ)
                                                                    the NPBayes package (Release 1).2 For each of the 12 chil-
As with earlier uses of the stick-breaking construction, γ1 af-     dren in the input, we run 10 randomly initialized simulations.
fects the expected total number of verb classes. This method        The parameters appear to converge within 3,000 iterations,
of clustering verb types is similar to Wallach (2008).              so we run each simulation for 5,800 iterations, discarding the
                                                                    first 3,300 as burn-in. We record a sample of the model pa-
Parameter estimation                                                rameters on every 25th iteration after the burn-in, giving 100
Models 1 and 2, as written, each specify a prior distribution       samples per simulation, 1,000 per child. By averaging over
over the complete set of possible parameters to the models          these samples, we can examine the models’ behaviour.
(i.e., all possible values for θ, z, φ, and so on). We update
these distributions using the observed verb usage data, thus                                 Experiments
obtaining posterior distributions over parameters.                  We compare the ability of our two models to acquire knowl-
   We estimate the posterior distributions using Gibbs sam-         edge about the usage patterns of verbs in the input and gener-
pling, a Markov Chain Monte Carlo (MCMC) method (Teh                alize that knowledge to new verbs. Firstly, we examine con-
et al., 2006). Model parameters are initially set randomly,         struction preferences in two related classes of verbs. Sec-
then iteratively adjusted according to the observed data. We        ondly, we test whether the models use an abstract representa-
randomly set each zi to one of 10 initial constructions, and        tion of the dative alternation to help learn new verbs.
each cv to one of 10 verb classes (if applicable). We set the
remaining parameters to random values drawn from the dis-           Verb argument preferences
tributions specified in the model descriptions. We then itera-      We examine how our models acquire the usage patterns of
tively update each model parameter individually by drawing          verbs in the input by looking at verbs that participate in two
it from a posterior distribution conditioned on the data and        different alternation patterns. Earlier, we demonstrated the
all the other parameters in the model. As we iterate through        dative alternation in examples (3) and (4). The benefactive
the parameters many times, we collect samples of their val-         alternation is a related pattern, in which verbs alternate be-
ues. Over time, this set of samples converges on the posterior      tween a double-object form and a prepositional benefactive
distribution—i.e., the model parameters given the observed          form, as in the following examples:
data. In the experiments, we average over this set of samples       (5) John made his friend a sandwich.
to estimate what each model has learned about the input.                    h OBJ, OBJ2, PREP = null, NSLOTS = 2 i
                                                                    (6) John made a sandwich for his friend.
                    Experimental set-up
                                                                            h OBJ, PP, PREP = for, NSLOTS = 2 i
We use child-directed speech from the Manchester corpus
                                                                    We consider all verbs involved in the dative and benefactive
(Theakston et al., 2001), part of the CHILDES database
                                                                    alternations, as listed by Levin (1993, Sections 2.1 and 2.2).
(MacWhinney, 2000). The corpus covers 12 British English-
                                                                    We test three constructions: the prepositional dative (PD); the
speaking children between the ages of approximately 2 and 3
                                                                    double-object construction (DO), whether dative or benefac-
years. Using CLAN, we extract all child-directed utterances
                                                                    tive; and the prepositional benefactive (PB). Using the sam-
containing at least one verb. We parse the utterances with the
                                                                    ples of the model parameters, we estimate the posterior pre-
MEGRASP dependency parser (Sagae et al., 2007), then re-
                                                                    dictive likelihood of each of these frames for each of the verbs
serve every second usage for an evaluation dataset, using the
                                                                    in the given classes. For a given test frame y0 , using verb v,
remainder for development. As described above, we extract
                                                                    and the observed data Y,
14 slot features for each verb usage. The datasets correspond-
ing to each child contain between 4,400 and 10,700 usages                          P(y0 |Y) = ∑ P(yo |k, Y)P(k|v, Y)
and between 239 and 479 verb types. All reported results are                                     k
obtained using the evaluation data.                                                          = ∑ ∏ P(y0 j |θ jk )P(k|πv )           (1)
   Due to flaws in the automatic part-of-speech tagging and                                      k  j
parsing, the data contains many errors, particularly in ditran-
                                                                    This likelihood is averaged over all 1,000 samples per child.
sitive constructions. We manually correct the portion of the
                                                                       Figure 2 shows the behaviour of both models. We average
input related to the dative alternation. For each verb in the
                                                                    the likelihoods over all 12 children, and over all verbs in the
development set that occurs with at least one prepositional or
                                                                    following cases: (a) verbs listed as dative but not benefactive,
double-object dative (as given by the automatic parsing), we
                                                                    (b) verbs listed as benefactive but not dative, and (c) verbs
draw a sample of up to 50 usages. We repair any cases of in-
                                                                    in both classes. In both models, both dative and benefactive
correctly parsed dative constructions, then duplicate the cor-
                                                                    verbs show a high likelihood for the DO frame, and a some-
rected samples as necessary. Since manual annotation is so
                                                                    what higher likelihood for the appropriate prepositional frame
labour-intensive, we use this same sample to correct the data
for corresponding verbs in the evaluation set. We assume that           2 http://www.gatsby.ucl.ac.uk/˜ywteh/research/software.html
                                                                2677

Figure 2: Argument preferences for known dative and bene-             Figure 3: Generalization of novel dative verbs in Models 1
factive verbs in Models 1 and 2. Shorter bars indicate higher         and 2, under various training conditions. Shorter bars indicate
likelihood. The two models show similar behaviour.                    higher likelihood.
(PD and PB, respectively) than for the inappropriate one (PB          the appropriate samples of the verb-level distribution πv . For
and PD, respectively). Verbs that occur in both classes show          each of the 1,000 parameter samples per child we obtained
closer likelihoods for all three frames.                              from the original simulations, we re-initialize the model with
   These results suggest that both models can acquire the ar-         the parameters from the sample, add in the novel data for case
gument structure preferences of verbs in the input. In this           (a), (b), or (c), then do a further 350 iterations, recording 10
case, the ability of Model 2 to acquire verb alternation classes      new samples of the model parameters. This gives 10,000 new
is not necessary. Both models are able to cluster verb usages         samples per test case, per child. Using equation (1) and the
into a range of constructions and acquire appropriate usage           new samples, we estimate the posterior predictive likelihood
patterns over a range of verbs. Both models acquire approx-           of each of the three constructions. This gives an estimate
imately 20 different constructions. Model 2 acquires 35-40            of the relative preferences for a verb’s usage and is a direct
verb classes, depending on the child.                                 measure of the acquired lexicon. Translating this estimate to
                                                                      production, as seen by Conwell and Demuth (2007), would
Novel verb generalization                                             require a model of how discourse and other factors influence
Children as young as three years of age have been shown to            dative production (e.g., de Marneffe et al., submitted). This is
use abstract representations of the dative alternation (Conwell       beyond the scope of this paper.
& Demuth, 2007). When young children hear a sentence like                Figure 3 shows how the ability to acquire verb classes aids
I gorped Charlie the duck, they appear to know that the same          generalization. In Model 1, without verb classes, only the
meaning can be expressed by saying I gorped the duck to               frames already seen with the novel verb are highly likely.
Charlie. We test this generalization in our models by pre-            This means that Model 1 is unable to generalize beyond ob-
senting a novel verb in one form of the dative and measuring          served data. In contrast, Model 2 shows appropriate gener-
the likelihood of the alternating form.                               alization for the dative alternation. When the novel verb is
   We test each model by independently presenting it with             trained with the prepositional dative, the double-object dative
a novel verb in three different situations: (a) two instances         shows a much higher likelihood than the unrelated SC frame.
of the prepositional dative, (b) two instances of the double-         A similar effect occurs with DO-only training: the PD frame
object dative, or (c) one instance of each. Only in case (c) is       is now more likely than the SC frame, although only slightly.
the verb explicitly seen to be alternating. We test the ability to    Compared with Model 1, both dative frames obtain a higher
generalize alternation behaviour by comparing the likelihood          likelihood across all three training cases, while the SC likeli-
of the unseen alternating form with an unseen form unrelated          hood remains low. The ability to acquire alternation classes
to the alternation. The non-alternating frame is the sentential       improves the ability to learn both alternating constructions.
complement (SC) frame, which occurs in 1-1.5% of the input,              One aspect of our results differs from the behaviour ob-
approximately the same overall frequency as either of the two         served in children. Our verb-clustering model is more likely
dative frames. For example, if we train the novel verb using          to generalize to the double-object form when trained only on
only the PD, yet the DO frame shows a higher likelihood than          a prepositional form, than the other way around (i.e., gener-
the unrelated SC frame, then we can say that the model has            alizing from a DO to a PD). However, three-year-old chil-
generalized the dative alternation.                                   dren seem to be biased to the prepositional form, the opposite
   Since the novel verbs are not in the observed data, we must        effect (Conwell & Demuth, 2007). We suggest that this is
further iterate the Gibbs sampler, using the new data, to obtain      a result of our small corpora. High-frequency dative verbs
                                                                  2678

tend to be biased toward the double-object form (Campbell &                             Acknowledgments
Tomasello, 2001). However, Gries and Stefanowitsch (2004)           We thank Yee Whye Teh for valuable discussions, and
show that out of 40 alternating verbs in the larger ICE-GB          NSERC and the University of Toronto for financial support.
corpus, 19 are prepositional-biased. This strongly suggests
that more low-frequency verbs are prepositional-biased than                                  References
otherwise. A small corpus will likely over-represent a double-      Alishahi, A., & Stevenson, S. (2008). A probabilistic model
object bias because of undersampling of low-frequency verbs.          of early argument structure acquisition. Cognitive Science,
By applying Model 2 to larger corpora of child-directed               32(5), 789-834.
speech in future work, we hope to correct this issue.               Campbell, A. L., & Tomasello, M. (2001). The acquisition
                                                                      of English dative constructions. Applied Psycholinguistics,
                         Conclusions                                  22(02), 253–267.
In this paper, we show how verb alternation classes contribute      Conwell, E., & Demuth, K. (2007). Early syntactic pro-
to generalization in verb learning. We develop a hierarchical         ductivity: Evidence from dative shift. Cognition, 103(2),
Bayesian model, Model 2, that is capable of acquiring knowl-          163–179.
                                                                    de Marneffe, M. C., Grimm, S., Arnon, I., Kirby, S., & Bres-
edge of verb argument structure at multiple levels of inference
                                                                      nan, J. (submitted). A statistical model of the grammatical
simultaneously. We demonstrate this using the wide range of
                                                                      choices in child production of dative sentences.
verbs and constructions contained in a corpus of naturalistic
                                                                    Fisher, C. (2002). The role of abstract syntactic knowledge
child-directed speech.
                                                                      in language acquisition: a reply to Tomasello (2000). Cog-
   By clustering individual verb usages, both of our mod-             nition, 82(3), 259-278.
els acquire a variety of argument structure constructions and       Goldberg, A. E. (2006). Constructions at work: the nature of
learn their patterns of use over hundreds of verbs. Further-          generalization in language. Oxford University Press.
more, Model 2 learns groups of verbs that occur with similar        Gries, S. T., & Stefanowitsch, A. (2004). Extending col-
usage patterns. Using the dative alternation as a key example,        lostructional analysis: A corpus-based perspective on al-
we demonstrate how this knowledge of alternation classes can          ternations. Intl. J. Corpus Linguistics, 9, 97–129.
be generalized to novel verbs, as observed in the behaviour of      Joanis, E., Stevenson, S., & James, D. (2008). A general fea-
children and adults. This verb class model can acquire and            ture space for automatic verb classification. Natural Lan-
apply this knowledge without any prior expectation of which           guage Engineering, 14(3), 337-367.
constructions and alternations may or may not be relevant.          Levin, B. (1993). English verb classes and alternations: A
   In contrast to previous analyses of the dative alternation         preliminary investigation. University of Chicago Press.
(Perfors et al., 2010; de Marneffe et al., submitted), we           MacWhinney, B. (2000). The CHILDES Project: Tools for
demonstrate its acquisition in the context of many other con-         analyzing talk (3rd ed., Vol. 2: The Database). Erlbaum.
structions, verbs, and alternations. Despite the low frequency      Perfors, A., Tenenbaum, J. B., & Wonnacott, E. (2010). Vari-
of the participating constructions, our model successfully ac-        ability, negative evidence, and the acquisition of verb argu-
quires the dative alternation. This is a strong endorsement of        ment constructions. J. Child Language, 37(3), 607–642.
hierarchical Bayesian models of language acquisition.               Sagae, K., Davis, E., Lavie, A., MacWhinney, B., & Wint-
   This approach offers a potential bridge between differing          ner, S. (2007). High-accuracy annotation and parsing of
theoretical positions in language acquisition. By simultane-          CHILDES transcripts. In Proc. ACL-2007 Wkshp on Cog-
ously learning at multiple levels of abstraction, our model           nitive Aspects of Computational Language Acquisition.
                                                                    Scott, R., & Fisher, C. (2009). Two-year-olds use distribu-
connects a usage-based representation of language, as pro-
                                                                      tional cues to interpret transitivity-alternating verbs. Lan-
posed by Tomasello (2003), with weak abstract representa-
                                                                      guage and Cognitive Processes, 24, 777–803.
tions similar to those championed by Fisher (2002). Other
                                                                    Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2006).
usage-based Bayesian models, such as that of Alishahi and
                                                                      Hierarchical dirichlet processes. Journal of the American
Stevenson (2008), offer a similar opportunity, although our
                                                                      Statistical Association, 101(476), 1566-1581.
model develops higher-level abstractions regarding the struc-       Theakston, A. L., Lieven, E. V. M., Pine, J. M., & Rowland,
tured knowledge of verbs.                                             C. F. (2001). The role of performance limitations in the
   One of the key features of usage-based constructions is that       acquisition of verb-argument structure: an alternative ac-
they couple form to meaning (Goldberg, 2006). Moreover,               count. J. Child Language, 28, 127-152.
Fisher argues that abstract syntactic representations influence     Tomasello, M. (2003). Constructing a language: A Usage-
semantics in verb learning, and vice-versa. By augmenting             Based theory of language acquisition. Harvard U. Press.
our model’s input with semantic properties, we will exam-           Vlachos, A., Korhonen, A., & Ghahramani, Z. (2009). Unsu-
ine the interaction of syntax and semantics in verb alterna-          pervised and constrained Dirichlet process mixture models
tions. We will investigate how an argument alternation may            for verb clustering. In Proceedings of the EACL Workshop
convey semantic information, as in Scott & Fisher’s (2009)            on Geometrical Models of Natural Language Semantics.
demonstration of 28-month-old children inferring causation          Wallach, H. M. (2008). Structured topic models for language.
in transitivity-alternating verbs.                                    Unpublished doctoral dissertation, Univ. of Cambridge.
                                                                2679

