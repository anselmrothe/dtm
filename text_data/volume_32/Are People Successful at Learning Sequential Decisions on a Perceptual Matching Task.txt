UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Are People Successful at Learning Sequential Decisions on a Perceptual Matching Task?
Permalink
https://escholarship.org/uc/item/2n57p3dr
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Yakushijin, Reiko
Jacobs, Robert A
Publication Date
2010-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                                  Are People Successful at Learning Sequential
                                     Decisions on a Perceptual Matching Task?
                                              Reiko Yakushijin (yaku@cl.aoyama.ac.jp)
                      Department of Psychology, Aoyama Gakuin University, Shibuya, Tokyo, 150-8366, Japan
                                           Robert A. Jacobs (robbie@bcs.rochester.edu)
                  Department of Brain & Cognitive Sciences, University of Rochester, Rochester, NY 14627, USA
                               Abstract                                 learner’s performance can be compared to the optimal perfor-
                                                                        mance for that task. If a learner achieves near-optimal perfor-
   Sequential decision-making tasks are commonplace in our ev-          mance at the end of training, then it can be claimed that the
   eryday lives. We report the results of an experiment in which
   human subjects were trained to perform a perceptual matching         learner has been successful.
   task, an instance of a sequential decision-making task. We use
   two benchmarks to evaluate the quality of subjects’ learning.           A second way of evaluating a learner is to compare the
   One benchmark is based on optimal performance as defined             learner’s performances with those of an adaptive computa-
   by a dynamic programming procedure. The other is based               tional agent that is trained to perform the same task. We con-
   on an adaptive computational agent that uses a reinforcement
   learning method known as Q-learning to learn to perform the          sider here an agent that learns via “reinforcement learning”
   task. Our analyses suggest that subjects learned to perform          methods developed by researchers interested in artificial in-
   the perceptual matching task in a near-optimal manner at the         telligence (Sutton & Barto, 1998). Cognitive scientists have
   end of training. Subjects were able to achieve near-optimal
   performance because they learned, at least partially, the causal     begun to use reinforcement learning methods to develop new
   structure underlying the task. Subjects’ learning curves were        theories of biological learning (e.g., Busemeyer & Pleskac,
   broadly consistent with those of model-based reinforcement-          2009; Daw & Touretzky, 2002; Schultz, Dayan, & Montague,
   learning agents that built and used internal models of how their
   actions influenced the external environment. We hypothesize          1997; Fu & Anderson, 2006). To date, however, there are
   that, in general, people will achieve near-optimal performances      few comparisons of the learning curves of people and agents
   on sequential decision-making tasks when they can detect the         based on reinforcement learning methods. Because reinforce-
   effects of their actions on the environment, and when they can
   represent and reason about these effects using an internal men-      ment learning is regarded as effective and well-understood
   tal model.                                                           from an engineering perspective, and as plausible from psy-
   Keywords: sequential decision making; optimal performance;           chological and neurophysiological perspectives, the perfor-
   dynamic programming; reinforcement learning                          mances of agents based on this form of learning can provide
                                                                        useful benchmarks for evaluating a person’s learning. If a
                           Introduction                                 person’s performance during training improves at the same
Tasks requiring people to make a sequence of decisions to               rate as that of a reinforcement-learning agent, then it can
reach a goal are commonplace in our lives. When playing                 be argued that the person is a successful learner. If a per-
chess, a person must choose a sequence of chess moves to                son’s performance improves at a slower rate, then the person
capture an opponent’s king. When driving to work, a per-                is not learning as much from experience as he or she could
son must choose a sequence of left and right turns to arrive            learn. Experimentation is often required to identify the cogni-
at work in a timely manner. And when pursuing financial                 tive “bottlenecks” preventing the person from learning faster.
                                                                        Lastly, if a person’s performance improves at a faster rate,
goals, a person must choose a sequence of saving and spend-
ing options to achieve a financial target. Interest in sequen-          then this suggests that the person is using information sources
tial decision-making tasks among cognitive scientists has in-           or information processing operations that are not available to
creased dramatically in recent years (e.g., Busemeyer, 2002;            the agent. A new, more complex agent should be considered
Chhabra & Jacobs, 2006; Fu & Anderson, 2006; Gibson,                    in this case.
Fichman, & Plaut, 1997; Gureckis & Love, 2009; Lee, 2006;                  We report the results of an experiment in which human sub-
Sutton & Barto, 1998; Shanks, Tunney, & McCarthy, 2002).                jects were trained to perform a perceptual matching task. This
   Here, we are interested in whether people are successful at          task was designed to contain a number of desirable features.
learning to perform sequential decision-making tasks. There             Importantly, the perceptual matching task is an instance of a
are at least two ways in which the quality of learning can be           sequential decision-making task. Subjects made a sequence
evaluated. These ways differ in terms of the benchmark to               of decisions (or, equivalently, took a sequence of actions) to
which the performances of a learner are compared. One way               modify an environmental state to a goal state. In addition, ef-
uses a benchmark of optimal performance on a task. Anal-                ficient performance on the perceptual matching task required
yses based on optimal performance are referred to as ideal              knowledge of how different properties of an environment in-
observer analyses, ideal actor analyses, or rational analyses           teracted with each other. In many everyday tasks, people are
in the literatures on perception, motor control, and cognition,         required to understand the interactions, or “causal relations”,
respectively. At each moment during training with a task, a             among multiple components (Busemeyer, 2002; Gopnik &
                                                                    156

Shulz, 2007). For example, when reaching for a coffee mug,              subjects could directly and easily control their values through
a person must understand that forces exerted at the shoulder            the use of the buttons. The values of the action variables de-
also influence the positions and velocities of the elbow, wrist,        termined the values of the shape parameters, denoted X, Y ,
and fingers. To make an efficient movement, a person must               and Z. Note that there are causal relations among the shape
use this knowledge of the causal interactions among motor               parameters. According to the network in Figure 1, if the value
components to design an effective motor plan.                           of X is changed, then this leads to a modification of Y which,
   Subjects’ performances on the perceptual matching task               in turn, leads to a modification of Z. The shape parameters de-
were evaluated via two benchmarks. Using an optimization                termine the shape of the comparison object, whose perceptual
technique known as dynamic programming, optimal perfor-                 features are denoted f1 , f2 , f3 , f4 , f5 , and f6 . The perceptual
mance on this task was calculated. In addition, computer                features used by a subject to assess the similarity of target and
simulations of an adaptive agent were conducted in which the            comparison object shapes may only be implicitly known by a
agent was trained to perform the perceptual matching task               subject, and may differ between subjects.
using a reinforcement learning method known as Q-learning                  Importantly, to efficiently convert the comparison object’s
(Sutton & Barto, 1998; Watkins, 1989). Comparisons of sub-              shape to the target object’s shape (i.e., with the fewest num-
jects’ performances during training with optimal performance            ber of button presses) often requires an understanding of the
and with those of the adaptive agent suggest that: (i) subjects         causal relations among the shape parameters. For instance,
learned to perform the perceptual matching task in a near-              if the values of parameters X, Y , and Z all need to be mod-
optimal manner at the end of training; (ii) subjects learned,           ified, a person who does not understand the causal relations
at least partially, the causal structure underlying the task; (iii)     among shape parameters may decide to change the value of
subjects’ learning curves were consistent with those of model-          action variable C (thereby changing shape parameter Z), then
based reinforcement-learning agents; and (iv) subjects may              the value of action variable B (thereby changing Y and Z),
have learned by building and using mental models of how                 and finally the value of action variable A (thereby changing
their actions influenced the external environment. Additional           X, Y , and Z). In many cases, this will be an inefficient strat-
details and results are reported in Yakushijin & Jacobs (2010).         egy. A person with good knowledge of the causal relations
                                                                        among the shape parameters knows that he or she can change
                          Experiment                                    the values of X, Y , and Z with a single button press that de-
                                                                        creases or increases the value of action variable A. Thus, a
Methods: Twenty-four undergraduate students at the Uni-
                                                                        good understanding of the causal relations among the shape
versity of Rochester participated in the experiment. Subjects
                                                                        parameters will lead to efficient task performance, whereas a
were paid $10 for their participation. All subjects had nor-
                                                                        poor understanding of the causal relations will lead to many
mal or corrected-to-normal vision. Subjects were randomly
                                                                        more button presses than necessary.
assigned to one of six experimental conditions. Each condi-
tion included both training and test trials. Only the results of           The six experimental conditions differed in the causal rela-
training trials are discussed here due to space limitations.            tions among the latent shape parameters X, Y , and Z. Two of
   On a training trial, subjects performed a perceptual match-          the causal relations were “linear” structures (one parameter
ing task which used visual objects from a class of parame-              had a direct causal influence on a second parameter which,
terized objects known as “supershapes” (highly realistic but            in turn, had a direct causal influence on a third parameter;
unfamiliar shapes; see Gielis, 2003). The parameters were               e.g., X → Y → Z or Y → X → Z), two of the relations were
latent (hidden) variables whose values determined the shapes            “common cause” structures (one parameter had direct causal
of the objects. On each trial, subjects viewed a target object,         influences on the two remaining parameters; e.g., Y ← X → Z
a comparison object, and a set of six buttons (see left panel of        or X ← Y → Z), and two of the relations were “common ef-
Figure 1). Buttons were organized into three pairs, and each            fect” structures (two parameters had direct causal influences
pair could be used to decrease or increase the value of an ac-          on a third parameter; e.g., X → Y ← Z or Y → X ← Z).
tion variable. By pressing the buttons, subjects could change              An experimental session consisted of 7 blocks of trials where
the values of the action variables which, in turn, changed the          a block contained a set of training trials followed by a set of
values of the parameters underlying the comparison object’s             test trials. (Test trials evaluated subjects’ one-step look-ahead
shape which, in turn, changed the shape of the comparison               knowledge; on a test trial, a subject decided if a comparison
object. Subjects’ task was to press one or more buttons (i.e.,          object could be converted to a target object using a single
to change the values of the action variables) to modify the             button press, and the subject did not receive feedback. Again,
shape of the comparison object until it matched the shape of            test trials are not discussed here.) Each set contained 26 tri-
the target object using as few button presses as possible.              als, one trial for each possible perturbation of a target object
   An experimental condition was characterized by a specific            shape to form an initial comparison object shape.
set of causal relations among the latent shape parameters. For          Results: Task Performances: As a benchmark for evalu-
example, one such set is schematically illustrated in the right         ating subjects’ performances on training trials, we computed
panel of Figure 1. Here, the three action variables are denoted         optimal performances on these trials using an optimization
A, B, and C. These variables are observable in the sense that           method known as dynamic programming (Bellman, 1957).
                                                                    157

                                                                                                                     Action variables
                                                                                    A         B        C
                                                                                                                       (observable)
                                                                                                                    Shape parameters
                                                                                    X         Y        Z                  (latent)
                                                                                                                   Perceptual features
                                                                            f1     f2     f3      f4    f5     f6
                                                                                                                      (observable)
Figure 1: Left: Example of an experimental display. Right: Bayesian network representing the causal relations (in one of the
experimental conditions) among the action variables, shape parameters, and perceptual features. For simplicity, the network
does not represent the fact that subjects’ button presses determined the values of the action variables.
In brief, dynamic programming is a technique for computing               of the experiment and, thus, their performances were highly
optimal solutions to multi-stage decision tasks. That is, dy-            sub-optimal during this time period. However, every subject
namic programming finds the shortest sequences of actions                learned during the course of the experiment. Importantly, ev-
that move a system from an initial state to a goal state when            ery subject achieved near-optimal performance at the end of
all states are fully observable. In the context of a training trial,     training: The average difference between a subject’s perfor-
the initial state corresponds to the initial values of the shape         mance and the optimal performance at the end of training is
parameters X, Y , and Z for the comparison object, and the               less than 1/2 of a step (mean = 0.434; standard deviation =
goal state corresponds to the values of the shape parameters             0.324).
for the target object. The dynamic programming algorithm                 Results: Causal Learning: The data from the training tri-
is provided with full state information. This means that the             als show that subjects achieved near-optimal performances.
algorithm knows the values of the comparison object’s shape              These results are consistent with the idea that subjects learned
parameters at every time step. It also knows the state tran-             about the causal relations among the latent shape parameters.
sition dynamics, meaning that it knows the causal relations              Additional analyses of training and test trials, not described
among the shape parameters and, thus, knows how any button               here due to space limitations, confirm that subjects did indeed
press will change the values of the shape parameters. Rela-              learn (at least partially) about these causal relations, and that
tive to our subjects, the dynamic programming algorithm is               this knowledge played a role in their task performances. De-
at an advantage. At the start of the experiment, our subjects            tails can be found in Yakushijin & Jacobs (2010).
did not know the values of the shape parameters or the causal
relations among the parameters. Consequently, it would be
                                                                                    Reinforcement Learning Agents
impressive if subjects learned to perform the task as well as
the dynamic programming algorithm.                                       Above, our analysis of subjects’ data used a benchmark of
   We determined the optimal performances in the six exper-              optimal performance based on dynamic programming. Al-
imental conditions via dynamic programming. Our analysis                 though very useful, this analysis does not allow us to evalu-
revealed that the range (1-5 steps or button presses) and the            ate the quality of subjects’ rates of learning. To do so, we
average length (2.54 steps) of the optimal action sequences              use a different benchmark based on an adaptive computa-
were identical for all conditions. Thus, the conditions were             tional agent that uses a reinforcement learning method known
well balanced in terms of their intrinsic difficulties.                  as Q-learning to learn to perform the perceptual matching
   Figure 2 shows subjects’ learning curves on training trials           task (Sutton & Barto, 1998; Watkins, 1989). Without go-
in the two experimental conditions with linear causal struc-             ing into the mathematical details, the reader should note that
tures among shape parameters. Due to space limitations, we               Q-learning is an approximate dynamic programming method
do not show results for conditions with common-cause and                 (Si et al., 2004). It is easy to show that, under mild con-
common-effect structures, though subjects in these conditions            ditions, the sequence of decisions found by an agent using
showed very similar results to subjects in linear structure con-         Q-learning is guaranteed to converge to an optimal sequence
ditions (Yakushijin & Jacobs, 2010). Eight subjects partici-             found by dynamic programming (Watkins & Dayan, 1992).
pated in linear structure conditions and, thus, the figure con-          Hence, the benchmarks based on dynamic programming and
tains eight graphs. The horizontal axis of each graph gives the          on Q-learning are related.
block number, and the vertical axis gives the average differ-               In a reinforcement learning framework, it is assumed that
ence between the number of steps (i.e., button presses) used             an agent attempts to choose actions so as to receive the most
by a subject during a trial and the optimal number of steps              reward possible. The agent explores its environment by as-
for that trial as computed by the dynamic programming pro-               sessing its current state and choosing an action. After execut-
cedure. These graphs show a number of interesting features.              ing this action, the agent will be in a new state, and will re-
Many subjects found the task to be difficult toward the start            ceive a reward (possibly zero) associated with this new state.
                                                                     158

                                                               kes                       jx                          ky                      sg
                                                           6                         6                           6                       6
                    Average actual steps − optimal steps
                                                           4                         4                           4                       4
                                                           2                         2                           2                       2
                                                           0                         0                           0                       0
                                                                     2   4   6                2   4   6                   2   4   6               2   4   6
                                                               ks                        lw                          ef                      ak
                                                           6                         6                           6                       6
                                                           4                         4                           4                       4
                                                           2                         2                           2                       2
                                                           0                         0                           0                       0
                                                                     2   4   6                2   4   6                   2   4   6               2   4   6
                                                                                                          Block number
Figure 2: Subjects’ learning performances on training trials in the two experimental conditions with linear causal structures
among shape parameters (top row: X → Y → Z; bottom row: Y → X → Z).
The agent adapts its behavior in a trial-by-trial manner by                                                      action with probability ε. The value of ε was initialized to
noticing which actions tend to be followed by future rewards                                                     one, and then it was slowly decreased during the course of
and which actions are not. To choose good actions, the agent                                                     a simulation. As a result, the agent tended to “explore” a
needs to estimate the long-term reward values of selecting                                                       wide range of actions toward the beginning of a simulation,
possible actions from possible states. Ideally, the value of se-                                                 and tended to “exploit” its current estimates of the best ac-
lecting action at in state st at time t, denoted Q(st , at ), should                                             tion to take toward the middle and end of a simulation. If the
equal the sum of rewards that the agent can expect to re-                                                        agent chose an action that caused the comparison object to
ceive in the future if it takes action at in state st : Q(st , at ) =                                            have the same shape as the target object, the agent received
E[∑∞ k=0 γk rt+k+1 ] where t is the current time step, k is an in-                                               a reward of 100. Otherwise, it received a reward of -1. The
dex over future time steps, rt+k+1 is the reward received at                                                     agent performed the training trials of the experiment in the
time t + k + 1, and γ (0 < γ ≤ 1) is a term that serves to dis-                                                  same manner as our human subjects—it performed 7 blocks
count rewards that occur in the far future more than rewards                                                     of training trials with 26 trials per block. To accurately esti-
that occur in the near future. An agent can learn accurate es-                                                   mate the agent’s performances during training, the agent was
timates of these ideal values on the basis of experience if it                                                   simulated 1000 times.
updates its estimates at each time step using the equation:                                                         The results for experimental conditions using linear causal
Q(st , at ) ← Q(st , at ) + α[rt+1 + γ max Q(st+1 , a) − Q(st , at )]                                            structures are shown in the left graph of Figure 3 (results
                                                                                 a                               for other conditions were similar). The horizontal axis plots
where the agent makes action at in state st and receives re-                                                     the block number, and the vertical axis plots the average dif-
ward rt+1 , and α is a step size or learning rate parameter                                                      ference between the number of steps (i.e., actions or button
(Sutton & Barto, 1998; Watkins, 1989).                                                                           presses) used by the agent or by human subjects during a trial
   In our first set of simulations in which a reinforcement-                                                     and the optimal number of steps for that trial as computed by
learning agent was trained to perform the perceptual match-                                                      the dynamic programming procedure (as in Figure 2; the error
ing task, all “Q-values” were initialized to zero, the discount                                                  bars in Figure 3 indicate the standard deviations). The solid
rate γ was set to 0.7, and the learning rate α was set to 0.45. In                                               line shows the data for the simulated agent, and the dotted
preliminary simulations, these values were found to be best                                                      line shows the data for our human subjects. Interestingly, the
in the sense that they led to performances that most closely                                                     learning curves of the simulated agent and of the human sub-
matched human performances. At each time step, the state                                                         jects have similar shapes, though subjects learned faster than
of the agent represented the difference in shape between the                                                     the agent at nearly all stages of training in all experimental
comparison and target objects. It was a three-dimensional                                                        conditions. Modifications of the agent by either using differ-
vector whose elements were set to the values of the shape pa-                                                    ent values for the agent’s parameters or by adding “eligibility
rameters for the comparison object minus the values of these                                                     traces” did not significantly alter this basic finding.
parameters for the target object. Six possible actions were                                                         Why did subjects show better learning performances than
available to the agent corresponding to the six buttons that a                                                   the simulated agent? In the machine learning literature, a
subject could press to modify the action variables. The agent                                                    distinction is made between model-free versus model-based
chose an action using an ε-greedy strategy, meaning that the                                                     reinforcement learning agents. The agent described above
agent chose the action a that maximized Q(st , a) with proba-                                                    is an instance of a model-free agent. Although model-free
bility 1 − ε (ties were broken at random), and chose a random                                                    agents are more common in the literature, we hypothesized
                                                                                                           159

Figure 3: Left: Learning curves for the simulated agent trained via Q-learning (solid line) and for the human subjects (dotted
line) in experimental conditions using linear causal structures (error bars plot standard deviations). Right: Identical to the left
graph except that the simulated agent learned a model of how actions influenced the environment, and used this model to reason
about good actions to take at each time step.
that a model-based reinforcement learning agent may pro-             queue were prioritized by the absolute amount that their Q-
vide a better account of our subjects’ performances. Model-          values would be modified. For example, suppose that at some
based agents typically learn faster than model-free agents, al-      moment in time, state-action pair (s∗ , a∗ ) had the highest pri-
beit with greater computational expense. Based on real-world         ority. Then Q(s∗ , a∗ ) would be updated. If performing this
experiences, a model-based agent learns an internal model of         update on the basis of simulated experience, the agent used
how its actions influence the environment. The agent updates         the model to predict the resulting new state. In addition, the
its Q-values from both real-world experiences with the envi-         agent also used the model to examine changes to the Q-values
ronment and from simulated experiences with the model (see           for all state-action pairs predicted to lead to state s∗ , known as
Sutton and Barto, 1998, for details).                                predecessor state-action pairs. These predecessor state-action
   In our simulations, the model was an artificial neural net-       pairs were added to the queue, along with their corresponding
work. Its six input units corresponded to the six possible ac-       priorities.
tions or key presses (an action variable could either increase
                                                                        The simulations with the model-based agent were identi-
or decrease in value, and there were three action variables).
                                                                     cal to those with the model-free agent. However, the model-
Its nine output units corresponded to the nine possible influ-
                                                                     based agent used different parameter values. Its discount rate
ences on the comparison objects’ shape parameters (a shape
                                                                     γ was set to 0.3, its learning rate α was set to 0.05, and N, the
parameter could either increase in value, decrease in value, or
                                                                     number of Q-value updates based on simulated experiences
maintain the same value, and there were three shape parame-
                                                                     for each update based on a real experience, was set to 5. In
ters). The network did not contain any hidden units.
                                                                     preliminary simulations, these values were found to be best
   When updating its Q-values, the model-based agent used
                                                                     in the sense that they led to performances that most closely
‘prioritized sweeping’ (Moore & Atkeson, 1993). This is an
                                                                     matched human performances.
efficient method for focusing Q-value updates to state-action
pairs associated with large changes in expected reward. Large           The combined results for the experimental conditions us-
changes occur, for example, when the current state is a non-         ing linear causal structures are shown in the right graph of
goal state and the agent discovers a previously unfamiliar ac-       Figure 3 (once again, results for the other experimental con-
tion that leads to a goal state. Large changes also occur when       ditions were similar). The learning curves of the model-based
the current state is a non-goal state, and the agent discovers a     agent are more similar to those of human subjects than the
new action that leads to a new non-goal state known to lie on        curves of the model-free agent. Indeed, the curves of the
a path toward a goal state.                                          model-based agent and of the human subjects are nearly iden-
   In brief, our simulations used prioritized sweeping as fol-       tical. Our findings suggest (but do not prove) that subjects
lows. At each moment in time, the model-based agent main-            may have achieved near-optimal performances on the percep-
tained a queue of state-action pairs whose Q-values would            tual matching task by building internal models of how their
change based on either real or simulated experiences. For            actions influenced the external environment. By using these
each update based on a real experience, there were up to N           models to reason about possible action sequences, subjects
updates based on simulated experiences. The items on the             quickly learned to perform the task.
                                                                 160

                         Conclusions                                   Chhabra, M. & Jacobs, R. A. (2006). Near-optimal human
                                                                          adaptive control across different noise environments. The
Sequential decision-making tasks are commonplace in our
                                                                          Journal of Neuroscience, 26, 10883-10887.
everyday lives. Here, we studied whether people were suc-
                                                                       Daw, N. D. & Touretzky, D. S. (2002). Long-term reward
cessful at learning to perform a perceptual matching task, an
                                                                          prediction in TD models of the dopamine system. Neural
instance of a sequential decision-making task. We used two
                                                                          Computation, 14, 2567-2583.
benchmarks to evaluate the quality of subjects’ learning. One
                                                                       Fu, W.-T. & Anderson, J. R. (2006). From recurrent choice
benchmark was based on optimal performance as defined by a
                                                                          to skill learning: A reinforcement-learning model. Journal
dynamic programming procedure. The other was based on an
                                                                          of Experimental Psychology: General, 135, 184-206.
adaptive computational agent that used Q-learning to learn to
                                                                       Gibson, F. P., Fichman, M., & Plaut, D. C. (1997). Learn-
perform the task. Overall, our analyses suggest that subjects
                                                                          ing in dynamic decision tasks: Computational model and
learned to perform the perceptual matching task in a near-
                                                                          empirical evidence. Organizational Behavior and Human
optimal manner. When doing so, subjects learned, at least
                                                                          Decision Processes, 71, 1-35.
partially, the causal structure underlying the task. In addition,
                                                                       Gielis, J. (2003). A generic geometric transformation that
subjects’ learning curves were broadly consistent with those
                                                                          unifies a wide range of natural and abstract shapes. Ameri-
of model-based reinforcement-learning agents that built and
                                                                          can Journal of Botany, 90, 333-338.
used internal models of how their actions influenced the ex-
                                                                       Gopnik, A. & Shulz, L. (2007). Causal Learning: Psychol-
ternal environment.
                                                                          ogy, Philosophy, and Computation. New York: Oxford
   The cognitive science literature now contains several stud-            University Press.
ies of human performance on sequential decision-making tasks.          Gureckis, T. M. & Love, B. C. (2009). Short-term gains,
Some studies have suggested that human performance is op-                 long-term pains: How cues about state aid learning in dy-
timal, whereas other studies have suggested the opposite. To              namic environments. Cognition, 113, 293-313.
date, our field does not have a good understanding of the              Lee, M. D. (2006). A hierarchical Bayesian model of human
factors influencing whether people will achieve optimal per-              decision-making on an optimal stopping problem. Cogni-
formance on a task. Future research will need to focus on                 tive Science, 30, 1-26.
this critical issue. Previous articles in the literature suggested     Moore, A. & Atkeson, C. (1993). Prioritized sweeping: Re-
that perceptual aliasing (Stankiewicz et al., 2006) or the ex-            inforcement learning with less data and less real time. Ma-
istence of actions leading to large rewards in the short-term             chine Learning, 13, 103-130.
but not the long-term (Neth, Sims, & Gray, 2006; Gureckis              Neth, H., Sims, C. R., & Gray, W. D. (2006). Melioration
& Love, 2009) seem to be factors leading to sub-optimal per-              dominates maximization: Stable suboptimal performance
formance. Here, we propose a new understanding of when                    despite global feedback. Proceedings of the 28th Annual
people will (or will not) achieve optimal performance. We hy-             Meeting of the Cognitive Science Society.
pothesize that people will achieve near-optimal performance            Schultz, W., Dayan, P., & Montague, P. R. (1997). A neural
on sequential-decision making tasks when they can detect the              substrate of prediction and reward. Science, 275, 1593-
effects of their actions on the environment, and when they                1598.
can represent and reason about these effects using an internal         Shanks, D. R., Tunney, R. J., & McCarthy, J. D. (2002). A
mental model.                                                             re-examination of melioration and rational choice. Journal
                                                                          of Behavioral Decision Making, 15, 233-250.
                     Acknowledgments                                   Si, J., Barto, A. G., Powell, W. B., & Wunsch, D. (2004).
This work was supported by a Grant-in-Aid for Scientific Re-              Handbook of Learning and Approximate Dynamic Program-
search (#20730480) from the Japan Society for the Promotion               ming. Piscataway, NJ: Wiley-IEEE.
of Science, by a research grant from the Air Force Office of           Stankiewicz, B. J., Legge, G. E., Mansfield, J. S., & Schlicht,
Scientific Research (FA9550-06-1-0492), and by a research                 E. J. (2006). Lost in virtual space: Studies in human and
grant from the National Science Foundation (DRL-0817250).                 ideal spatial navigation. Journal of Experimental Psychol-
                                                                          ogy: Human Perception and Performance, 32, 688-704.
                          References                                   Sutton, R. S. & Barto, A. G. (1998). Reinforcement Learning:
                                                                          An Introduction. Cambridge, MA: MIT Press.
Bellman, R. (1957). Dynamic Programming. Princeton, NJ:
                                                                       Watkins, C. J. C. H. (1989). Learning From Delayed Re-
   Princeton University Press.                                            wards. Unpublished doctoral dissertation. Cambridge, UK:
Busemeyer, J. R. (2002). Dynamic decision making. In N.                   Cambridge, University.
   J. Smelser & P. B. Baltes (Eds.), International Encyclope-          Watkins, C. J. C. H. & Dayan, P. (1992). Q-learning. Machine
   dia of the Social and Behavioral Sciences. Oxford, UK:                 Learning, 8, 279-292.
   Elsevier Press.                                                     Yakushijin, R. & Jacobs, R. A. (2010). Are people successful
Busemeyer, J. R. & Pleskac, T. J. (2009). Theoretical tools for           at learning sequential decisions on a perceptual matching
   understanding and aiding dynamic decision making. Jour-                task? Manuscript submitted for journal publication.
   nal of Mathematical Psychology, 53, 126-138.
                                                                   161

