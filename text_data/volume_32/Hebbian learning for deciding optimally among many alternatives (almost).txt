UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Hebbian learning for deciding optimally among many alternatives (almost)
Permalink
https://escholarship.org/uc/item/5q6497p6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Simen, Patrick
McMillen, Tyler
Behseta, Sam
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

        Hebbian learning for deciding optimally among many alternatives (almost)
                                           Patrick Simen (psimen@math.princeton.edu)
             Princeton Neuroscience Institute, Princeton University, Green Hall, Washington Rd., Princeton, NJ 08544
                                            Tyler McMillen (tmcmillen@fullerton.edu)
                      Department of Mathematics, California State University at Fullerton, Fullerton, CA 92834
                                               Sam Behseta (sbehseta@fullerton.edu)
                      Department of Mathematics, California State University at Fullerton, Fullerton, CA 92834
                              Abstract                                  trials, and the average signal-to-noise ratio of each stimulus
                                                                        is fixed. The duration, rather than the number of trials, is also
   Reward-maximizing performance and neurally plausible
   mechanisms for achieving it have been completely character-          held fixed, and the distribution of response-to-stimulus inter-
   ized for a general class of two-alternative decision making          vals (RSIs) that delay the onset of the next stimulus after a
   tasks, and data suggest that humans can implement the optimal        response is stationary. In this case, maximizing the rate of
   procedure. A greater number of alternatives complicates the
   analysis, but here too, analytical approximations to optimal-        reward also maximizes the total reward.
   ity that are physically and psychologically plausible have been         Maximizing gains in this and a variety of similar tasks
   analyzed. All of these analyses, however, leave critical open        requires probabilistic inference. While the importance of
   questions, two of which are the following: 1) How are near-
   optimal model parameterizations learned from experience? 2)          a principled inference process is widely understood in psy-
   How can sensory neurons’ broad tuning curves be incorporated         chology and neuroscience, the complexity of optimal deci-
   into the aforementioned optimal performance theory, which as-        sion policies in tasks with response-terminated stimuli (also
   sumes decisions are based only on the most informative neu-
   rons? We present a possible answer to all of these questions in      known as ‘free response’ or ‘response time’ tasks) and N > 2
   the form of an extremely simple, reward-modulated Hebbian            choices appears to be less well appreciated.
   learning rule for weight updates in a neural network that learns
   to approximate the multi-hypothesis sequential probability ra-          For 2-choice tasks of the type just described, reward-
   tio test.                                                            maximizing performance has been completely characterized
   Keywords: Hebbian learning; diffusion model; neural net-             (Bogacz et al., 2006): a sequential probability ratio test
   work; multi-hypothesis sequential test; sequential probability       (SPRT) should be carried out in which the current likelihood
   ratio test; speed-accuracy tradeoff; response time                   ratio of the two hypotheses is multiplied by the probability
                                                                        of a given data sample under one hypothesis and divided by
                           Introduction                                 the probability of that data sample under the other hypothe-
We examine the problem of maximizing earnings from a se-                sis (equivalently, the logs of these probabilities can be added
quence of N-alternative decisions about the identity of noisy           and subtracted, respectively — from now on, we will cast our
stimuli, with N > 2. Our goal is to parameterize a simple               discussion in terms of log-likelihoods). A response should
neural circuit model whose behavior approximates optimal                be made when the resulting log-likelihood exceeds a fixed
performance in such tasks, while simultaneously accounting              threshold (Wald & Wolfowitz, 1948). There exists an optimal
for the fundamental role of tuning curves in the neural repre-          starting point of the log-likelihood ratio (e.g., 0, for equally
sentation of sensory stimuli. Throughout, we take ‘optimal’             likely stimuli) and an optimal separation between the two re-
to mean reward maximizing, and we assume that correct de-               sponse thresholds (one greater and one less than zero) that de-
cisions earn rewards for the decider.                                   pends on the signal-to-noise ratio (SNR) and the RSI (Bogacz
   As we show, simple principles of neural computation are              et al., 2006). Gold and Shadlen (2001) have demonstrated
sufficient to approximate this form of optimality quite closely         that for systems consisting of a neuron/anti-neuron pair, each
in a class of N-choice tasks involving response-terminated              of which is tuned for one of the two stimulus types in a 2-
stimuli: that is, stimuli that provide information continuously         choice task, the log-likelihood ratio is approximately propor-
until the time (the response time) at which participants decide         tional simply to the difference between the activations of the
for themselves when to stop observing and make a response.              two neurons, suggesting an extremely simple neural imple-
This is somewhat surprising, given that a general decision              mentation of the SPRT.
policy that guarantees truly optimal performance cannot even               In contrast, if the number of choices is greater than 2, the
be explicitly formulated for such tasks, as we discuss below.           optimal policy for deciding based on accumulated informa-
                                                                        tion is nontrivial. In particular, a natural (but definitely sub-
N-choice, response-terminated decision tasks                            optimal) approach to N-choice decision making is to com-
We assume that participants earn rewards for correct re-                pute the posterior probability of each of the N hypotheses,
sponses, and earn less for errors (for simplicity, we assume            and then select whichever one first exceeds a fixed threshold.
errors earn nothing). In the simple tasks we consider, each             In fact, the best decision is made when the entire set of pos-
stimulus type has a fixed prior probability within a block of           terior probabilities meets conditions that are nontrivial func-
                                                                    1816

tions of the posterior values. A thought experiment may help          labels reflect the fact that our model exhibits known proper-
make clear why this is true. Consider the case of a 3-choice          ties of neurons in the monkey middle temporal area (MT),
task in which one choice has attained an 80% posterior prob-          the lateral intraparietal area (LIP) and the superior collicu-
ability of being correct, while the other posteriors are 10%          lus (SC) in decision making tasks requiring eye movements
and 10%. A fixed 80% threshold will therefore not distin-             in response to visual motion stimuli (i.e., random dot kine-
guish this case from a case in which the posteriors are 80%,          matograms; Shadlen & Newsome, 2001). The architecture of
19% and 1%. These two cases are quite different, however,             this circuitry is expected to apply without major modification
and dealing optimally with them requires taking account of            to other stimulus and response types, however.
all posterior probabilities in a more sophisticated way. Be-
cause of this, and because of the inherent difficulty in defin-
                                                                          Table 1: Three layer model with weight learning rule.
ing truly optimal decision policies to apply to the posteriors,
multi-hypothesis sequential probability ratio tests (MSPRTs)                        Si , i = 1, . . . , n               input signals (MT)
were designed to approximate optimality with a decision pol-                                                !
icy consisting of fixed thresholds applied to posteriors or like-
lihood ratios (Dragalin, Tartakovsky, & Veeravalli, 1999).              dxi =    −k xi − m ∑ x j + Si dt + . . .
                                                                                             j6=i
                                                                                                               c dBi    accumulators (LIP)
Tuning curves                                                                                              n
                                                                           zi = H(yi − Θ),        yi =    ∑ wi j x j    decision units (SC)
Tuning curves are ubiquitous in neural responses to stimuli                                               j=1
(Butts & Goldman, 2006). The relationship between tuning
curve shape and decision making performance has intrigued                   wnew
                                                                              ij   = (1 − α)wold  i j + α ∆wi j         LIP to SC weight-
researchers for several years (e.g., Pouget, Deneve, Ducom,
                                                                                      ∆wi j = rzi x j                   learning rule
& Latham, 1999). Naively, one may suppose that task par-
ticipants improve their performance by sharpening the tun-
ing curves of the neurons involved. However, wider tuning
curves are in some cases more efficient in conveying infor-
mation, and the most informative tuning curve shape depends                       SC                 z1         z2        z3
strongly on the noise and correlations (Zhang & Sejnowski,
1999; Seriés, Latham, & Pouget, 2004). Moreover, in several
tasks, participants may improve performance without signifi-                                                                    W
cantly altering the shapes of the tuning curves in the neurons
involved. For instance, in an angle discrimination task, mon-
keys are able to learn to discriminate between finer angles                       LIP      x1              x2        x3       x4
over time, while the tuning curves of primary sensory neu-
rons are altered very little (Ghose, Yang, & Maunsell, 2002;
Law & Gold, 2008). This suggests that improvements in per-
                                                                                  MT       S1              S2        S3        S4
formance take place in a learning process downstream from
the receptor neurons.
   In this paper we explore the ways in which a subject may
improve performance in decision tasks, given tuning curve             Figure 1: Neural network model with 4 accumulators and 3
shapes in receptor neurons. We do not consider the alteration         alternatives. The weight matrix W denotes the weights of the
of receptor units’ tuning curves, but rather how the informa-         connections between the xi ’s and z j ’s. Arrows represent exci-
tion in tuning curves can be utilized more efficiently over the       tatory connections; circles represent inhibitory connections.
course of many trials.
                                                                         We suppose that MT neurons have tuning curves that are
   The leaky competing accumulator model for                          preferentially sensitive to a single, given direction of visual
                                                                      motion, and that another layer is stimulated by the activ-
                      decision making                                 ity in this input layer. By virtue of their excitatory connec-
We propose a three layer neural model for decision making             tions to LIP, model MT units’ tuning curves and their feed-
(defined in Table 1, and depicted in Fig. 1). The first layer         forward connections to LIP in turn define tuning curves for
acts simply as a sensory amplifier; the next layer integrates         LIP units. Questions of major importance in computational
the information from the first layer, but also exhibits com-          neuroscience are: Through what sort of learning process do
petitive dynamics that gradually build a commitment to one            these tuning curves arise? Can we define an optimal con-
course of action over the alternatives; the last layer triggers a     nection scheme that maximizes some function, such as the
discrete motor response when commitment to one response is            rate of reward earned by the model? We attempt to make
sufficiently strong. For convenience, we refer to these three         progress on these questions while making the simplifying as-
layers, respectively, as the MT, LIP and SC layers. These             sumption that the brain circuits in question are approximately
                                                                  1817

linear systems (at least over a limited range of inputs), and
that they employ simple learning schemes (such as Hebbian
learning, or error-updating rules such as the Widrow-Hoff,
Rescorla-Wagner or delta rule). Recent work (e.g. McMillen
& Holmes, 2006; Bogacz & Gurney, 2007) that avoids discus-                                                            90  180 270   360
sion of tuning curves and learning shows that these assump-
tions allow simple neural network models to map precisely
onto one or another form of MSPRT. We now demonstrate
that a similar model that learns connections strengths and ac-                                                        90  180 270   360
counts for tuning curves does remarkably well at approaching
optimal (reward maximizing) performance in decision mak-
ing tasks with multiple alternatives. The model’s layers are
represented mathematically by S, x, and z.
   Upon presentation of a stimulus, the model’s MT layer                                                              90  180 270   360
presents a vector of signals to accumulators in the LIP layer.
The signal presented to the ith unit in the LIP layer is referred
to as Si , representing the total weighted sum of MT signals to          Figure 2: Possible directions of coordinated movement (left
the ith accumulator. Each stimulus corresponds to a unique               panels) and corresponding signal vectors (right panel).
signal, so that the set of signals to the LIP layer may be rep-
resented as a vector indexed by µ:
                                                                         where k is the decay rate, m is the mutual inhibition, and Bi
                               µ µ
                      Sµ = S1 , S2 , . . . , Snµ .
                                                
                                                                         is a Wiener process (integrated white noise) representing the
                                                                         noise in the signal and from other sources. The signal-to-
The task is to determine which of N possible signal vectors              noise ratio is the ratio of the magnitude of the largest signal
this represents. Notice that the number of vectors can be dif-           to the variance of the noise, i.e. a/c. We can thus model
ferent from the number of signals, i.e. in general n > N.                changes in the direction coherence by changing this ratio. The
   Although it is not required, we will generally take the Sµ            effect of decay and inhibition is to concentrate the values of
signals to be Gaussian:                                                  the accumulators onto the signal vectors. Thus, moderate val-
              µ
                        
                            (i − dirµ )2
                                                                        ues of w and k tend to increase the accuracy. Best results are
             Si = a exp −                    , i = 1, . . . , n . (1)    achieved when decay and inhibition are balanced, i.e. w = k
                                2φ2
                                                                         (McMillen & Holmes, 2006). For simplicity, and to be con-
Here dirµ is the peak of the signal, a is the height of the peak         crete, throughout the rest of this paper we will present results
and φ is the width of the curve. As in McMillen and Behseta              for k = w = 0.5, a = 2 and c = 1. Results are qualitatively
(2010), we interpret the Sµ in terms of approximately Gaus-              insensitive to these choices.
sian MT tuning curves and weights from MT to LIP that pre-                  The output from accumulator j feeds into the ith unit of
serve this Gaussian tuning in the LIP units. Notice that if              SC with weight wi j . SC units apply step functions H with
φ = 0, then                                                              thresholds Θ to their inputs. A response is made when SC
                             µ
                           Si = a δi,dirµ ,                              unit j transitions from 0 to 1 (i.e., when y j = ∑nj=1 wi j x j > Θ).
where δi, j is the Kronecker delta, so that the signal is concen-           The results in this paper are generally applicable, but to be
trated in the channel dirµ . But, if φ > 0, the signal will have         precise we consider a motion direction task with 36 accumu-
a spread around the peak. For MT units associated with the               lators and interpret these as representing increments of 10◦ .
dot-motion task, tuning curves have been measured to have                If the direction j · 10◦ is presented, the signal vector takes the
                                                                                  µ
a width of about 40◦ (Law & Gold, 2008). The situation is                shape Si as in (1), with dirµ = j. For concreteness we con-
illustrated in Fig. 2. Angles far apart have very little overlap         sider four possible directions of motion: 30◦ , 60◦ , 140◦ , 220◦ .
in the signals, but when the angles are close the overlap is             Thus, if say, the direction of coordinated movement is 60◦ ,
substantial. For a two-alternative task in which dots travel on          the signal vector has a peak at the sixth accumulator. The
average either up or down, the signals have very little overlap.         four possibilities are represented by the four possible signal
Signals for alternatives corresponding to more similar motion            vectors with peaks at accumulators 3, 6, 14 and 22. In this
directions have more overlap.                                            paper we only consider the case when all the possibilities are
   We model the LIP layer as a set of n leaky competing                  equally likely, in which case the appropriate initial condition
accumulators. The linearized model for their evolution is a              for the accumulators is xi (0) = 0.
stochastic differential equation (Usher & McClelland, 2001;                 McMillen and Behseta (2010) showed that the optimal
Bogacz et al., 2006; McMillen & Holmes, 2006):                           weights wi j in the above are achieved when the weights
                                 !                                       mimic the shape of the possible incoming signal vectors.
   dxi =     −kxi − m ∑ x j + Si dt + c dBi , i = 1, · · · , n , (2)     That is to say, a threshold crossing test best approximates
                                                                                                       µ
                      j6=i                                               an MSPRT when wi j = S j i . The magnitude of the weights
                                                                     1818

are not important in terms of optimality, as the magnitude                                      !W = !                                  !=4
may be incorporated into the thresholds. The performance                        1.4                                       1.4
of the threshold crossing tests is illustrated in Fig. 3. Here                  1.2                                       1.2
we consider a test with 36 accumulators and the four alterna-                    1                                         1
tives as described above. In Fig. 3 we plot the mean response             MRT                                       MRT
                                                                                0.8                                       0.8
time (MRT) for a fixed value of the error proportion (ER).
                                                                                0.6                                       0.6
For each value of the spread we compute the threshold such
                                                                                0.4                                       0.4
that ER = 0.1, and find the corresponding MRT. Each panel
                                                                                0.2                                       0.2
demonstrates an important fact, as we elucidate below.                                0   2       4       6     8               0   2
                                                                                                                                        !
                                                                                                                                         4    6   8
                                                                                                  !                                      W
   In the left panel of Fig. 3 we take the signal vectors to be as
(1), and allow φ to vary. Thus, φ = 0 corresponds to the case
when the signal is concentrated in a single channel. Positive
                                                                      Figure 3: Effects of signal spread and weight shape. Left
values of φ correspond to signals that are spread about a peak.
                                                                      panel: Simulated MRT vs. spread in the signal vectors, where
In these computations, the weights are as desired for MSPRT
                              µ                                       the weights have the same shape. Right panel: MRT vs.
approximation, i.e. wi j ∝ S j i . This panel thus shows the min-
                                                                      spread in shape of weights with signal vector fixed with φ = 4.
imal MRT that can be achieved by a threshold crossing test
                                                                      In all cases the threshold is such that ER = 0.1.
for an ER of 0.1. We see that there is an advantage to a mod-
erate spread in the signals if this information can be utilized
by the decision mechanism. In fact, the optimal spread is near        any point, its value is an approximately exponentially de-
φ = 3. It is interesting to note that this corresponds to a width     caying, weighted average of past input values. High fre-
in the shape of the signal vectors of about 30◦ , while the width     quency changes in this signal (representing noise) are filtered
of tuning curves in MT associated with the direction task as          out by the algorithm, producing little change in the updated
measured in Law and Gold (2008) are approximately 40◦ .               weight. In contrast, low frequency signal changes (represent-
   In the right panel we fix the spread in the signal vectors at      ing, hopefully, the uncorrupted input signal) produce signifi-
φ = 4, and compute MRT for various spreads in the weights.            cant changes in the weight. If the signal is constant and noise
In order to get an idea of how the spread in the shape of the         is absent, the weight will converge approximately exponen-
weights affects performance when the signal shape is fixed,           tially on the value of the signal. If what is being tracked is a
in these simulations we suppose that the weights also have a          signal that depends on the product of activations in a sending
Gaussian shape:                                                       unit and a receiving unit, then this rule is simply a Hebbian
                                                                      update rule with a decay term for forgetting old co-activation
                           (i − j)2
                                   
                                                                      levels — a useful feature in a noisy neural system.
           wi j = w0 exp −            , j = 1, . . . , n,
                               2
                             2φW                                         After each trial the subject responds with a choice among
                                                                      alternatives, say i. At this time the weights to the output unit
where w0 is a normalizing factor chosen so that ∑nj=1 w2i j = 1       zi corresponding to the choice made are updated, according
(this normalization step is not in fact required). The spread         to whether a reward is received or not. Then, if the choice
φW controls how the values of the accumulators are weighted           corresponding to zi is chosen, the weights are updated by the
before making a decision. In the case φW = 0, we have yi =            rule
xi , so that the accumulator values are not weighted. When
φW = ∞, each yi is the same, i.e. the sum of all accumulators.                            wnew
                                                                                           ij            = (1 − α)wold
                                                                                                                   i j + α ∆wi j ,                    (3)
The right panel of Fig. 3 shows that MRT is minimized when                                    ∆wi j      = rzi x j ,                                  (4)
φW = φ. That is, the optimal weights occur when the width of
the weight shape is the same as that in the signal vector.            where r is the magnitude of the reward, and α is the learning
    To reiterate, a moderate spread in the signals is a significant   rate. Notice that only the weights to the unit corresponding to
advantage, but only if the LIP-to-SC weights can be tuned to          the choice made are updated, and this is the sense in which the
take on the same shape as the possible signal vectors defined         rule is Hebbian. For simplicity, we assume here that a reward
by MT activity. In the following section we consider how the          is either earned or not, so that r is either 1 or 0 depending on
weights may be modified over the course of trials.                    whether a correct decision is made.
                                                                         Thus, after each trial, if a correct decision is made the
     An algorithm for learning the LIP to SC                          weights to the correct output unit are increased in propor-
                                                                      tion to the values of the accumulators x. There is no need
                    weights                                           to estimate the probability of making a correct decision or an
We propose a simple Hebbian weight learning algorithm for             expected value of the reward, as for example in reinforcement
the weights wi j . The learning algorithm is a modification           learning methods, since only the values of the units are used
of a classical Widrow-Hoff rule (see, e.g., Hertz, Krogh, &           in the update rule. With this rule the weights track the shape
Palmer, 1991). In rules of this type, the connection strength         of the vectors being passed from the LIP layer. The weights
being modified acts as a filter that tracks an input signal. At       thus tend to oscillate around the means of the accumulator
                                                                  1819

values, hx j (t)i.                                                                                                                                                                                            θ=0.50
                                                                                                                                                                                                                               2
                                                                                                                                                                                                                                               θ=1.00
                                                                                                                                                                                                                                                                2
                                                                                                                                                                                                                                                                                θ=1.75
                                                                                                                                                                                                                                                                                                   2
                                                                                                                                                                                                   0.3                                                               0.3
   The accumulator values on average take on the shape of                                                                                                                                                          ER = 0.47
                                                                                                                                                                                                                                    0.3             ER = 0.29                          ER = 0.30
                                                                                                                                                                                                                                                                                                       signal strength
                                                                                                                                                                                                                   RT = 0.11                        RT = 0.30                          RT = 0.62
the signal vector from the MT layer. This can be proved an-
                                                                                                                                                                                   weights
                                                                                                                                                                                                                                                                    0.15
                                                                                                                                                                                               0.15                            1 0.15                           1                                  1
alytically, but here we show only simulation results. The up-
date rule (3-4) thus causes the weights to track values whose                                                                                                                                                                        0                                0
                                                                                                                                                                                                 0
means take on the shape of the MT-to-LIP signal vectors.                                                                                                                                     −0.01
                                                                                                                                                                                                         10     20      30
                                                                                                                                                                                                                               0 −0.07
                                                                                                                                                                                                                                          10     20      30
                                                                                                                                                                                                                                                                0 −0.08
                                                                                                                                                                                                                                                                           10     20         30
                                                                                                                                                                                                                                                                                                   0
Therefore the weights tend, on average, to mimic the shape
of the signal vector, with oscillations about this shape that
                                                                                                                                                                                                              θ=0.50                           θ=1.00                           θ=1.75
depend on the learning rate.                                                                                                                                                                       0.2
                                                                                                                                                                                                                               2                                2
                                                                                                                                                                                                                                                                     0.2
                                                                                                                                                                                                                                                                                                   2
                                                                                                                                                                                                                   ER = 0.58        0.2             ER = 0.36                          ER = 0.33
                                                                                                                                                                                                                                                                                                       signal strength
                                                                                                                                                                                                                   RT = 0.09                        RT = 0.28                          RT = 0.62
                                           Results of simulations                                                                                                                                                                                                    0.1
                                                                                                                                                                                         weights
                                                                                                                                                                                                                                    0.1
                                                                                                                                                                                                                                                                1                                  1
                                                                                                                                                                                                   0.1                         1
                                                                                                                                                                                                                                                                      0
Figs. 4 and 5 show results of simulations using the update                                                                                                                                                                           0
rule (3 - 4). The weights are initially chosen randomly, with a                                                                                                                                                                    −0.1
                                                                                                                                                                                                                                                                0
                                                                                                                                                                                                                                                                    −0.1
                                                                                                                                                                                                                                                                                                   0
                                                                                                                                                                                                         10     20      30                10     20      30                10     20         30
peak added at wii . Fig. 4 shows how the weights evolve over
time, and how this affects the performance of the subject. The
reward rate continually increases on average, and the ER con-
                                                                                                                                                                               Figure 5: Averaged weights over 150 blocks of 500 trials. In
tinually decreases. The bottom panels show the weights to SC
                                                                                                                                                                               the top row φ = 4; in the bottom row φ = 8.
corresponding to i = 14, or to angle 140◦ . The weights for the
other alternatives behave similarly. Simulations in which the
weights are chosen differently show similar improvements in                                                                                                                    the weights over each block, and then took the average over
performance and similar matching of the weight profiles to                                                                                                                     150 blocks of trials. Fig. 5 shows the averaged weights for
the signal vector shapes. Cases in which the weights are all                                                                                                                   different values of the threshold, as well as different values of
chosen randomly show a more dramatic improvement in re-                                                                                                                        the spread in the signals. We see that on average, the weight
ward rate (RR) since then the accuracy will initially be very                                                                                                                  profile shape is very close to the signal shape. Also indicated
low. Fig. 4 shows that even when the weight has a peak at the                                                                                                                  in these figures are the ERs and MRTs for these blocks of
right position, a dramatic improvement occurs: for example,                                                                                                                    trials. Notice that in the lower left panel, the ER = .58 is
the RR more than doubles and the RT and ER both decrease                                                                                                                       not much smaller than would be achieved by random guess-
over time.                                                                                                                                                                     ing (.75). In this case the threshold is very small, as is the
                                                                                                                                                                               corresponding MRT of .09. In this situation it will take the
               1.2                                                                     0.8
                                                                                                                       cumulative MRT                                          weights much longer to learn the shape of the signal vectors,
                1                                                                                                      cumulative ER
                                                                                       0.6                             ER in last 100 trials                                   since most of the time the decision will be incorrect. This
 reward rate                                                              ER and MRT
               0.8                                                                                                                                                             is why the weights appear more erratic in this frame than in
                                                                                       0.4
               0.6                                                                                                                                                             the others. However, even in this case, the average values of
               0.4                         cumulative RR
                                                                                       0.2                                                                                     the weights take the same shape as the signal vector. Similar
                                           RR in last 50 trials
               0.2                                                                      0
                                                                                                                                                                               comments apply, mutatis mutandis, to the upper left panel.
                     0   100       200           300       400    500                        0        100       200           300        400        500
                                         trial                                                                        trial                                                       Generally, the model is insensitive to changes in the pa-
                              1 trials
                                                       2
                                                                       251 trials
                                                                                                      2
                                                                                                                              500 trials
                                                                                                                                                      2
                                                                                                                                                                               rameters a, c, k, m, in the sense that the weights tend on aver-
                                                                                                                                                                               age toward the optimal weight shape mimicking the shape of
                                                                                                                                                                               the signal vectors. If the learning rate α is made smaller, the
                                                                                                                                                          signal strength
 weights
               0.5
                                                       1                                              1                                               1
                                                                                                                                                                               weights take longer to track to the shape of the signals, but
                                                            0                                               0                                                                  there is less variation around these mean values.
                                                                                                                                                                                  Fig. 6 shows the dynamics of evidence accumulation
                         10      20        30                     10         20                  30                     10          20         30
                                                                                                                                                                               within trials, demonstrating that Gaussian bumps of activa-
                                                                                                                                                                               tion arise on the LIP layer (preserving the Gaussian input
                                                                                                                                                                               signal profiles, and therefore producing Gaussian LIP-to-SC
Figure 4: Effects of weight learning rule. The threshold is                                                                                                                    weights through Hebbian learning).
fixed at z = 1. There are four alternatives (3, 6, 14, 22), and
the learning rate is α = .05. In the bottom panel the signal                                                                                                                                                                         Discussion
strength is plotted on the right axis (circles), and the weights                                                                                                               The simple rule (3-4) works remarkably well at learning the
are shown on the left axis (stars). The inter-trial delay used in                                                                                                              shapes of the signal vectors from MT to LIP. This leads to
the calculation of reward rate (RR) here is 500 msec.                                                                                                                          a dramatic improvement in performance, and occurs without
                                                                                                                                                                               any direct connection to the MT layer. The three layer model
  Figure 4 shows one block of 500 trials. In order to see                                                                                                                      incorporates integration of information, a rule for making the
how the weight update rule behaves on average, we carried                                                                                                                      decision, as well as a simple algorithm for learning to op-
out the same simulation for a number of blocks and averaged                                                                                                                    timize reward rates by learning the shapes of the vectors of
                                                                                                                                                                            1820

                  A                                                      B                    Weighted LIP activation
                                                                                              LIP unit 6 activation                             C
                               Average activation for stimulus type 2                 4       Avg weighted SC input (choice 2)
                  2                                                                           Avg LIP unit 6 activation (choice 2)
                                                                                                                                                          2.5
                       *              *                 *                                     Avg weighted SC input (choice 1)
    Activation
                                                                                              Avg LIP unit 1 activation
                  1                                                                           Avg weighted SC input (choice 3)                                                              Threshold
                                                                                      3       Avg LIP unit 14 activation                                   2
                  0
                                                                                                                                                          1.5
                                                                                      2
                 −1
                                                                        Activation                                                          Activation
                                                                                                                                                           1
                   0       2      4   6   8    10 12 14 16 18
                                                                                      1                                                                   0.5
                               Average weighted input to SC units
                  2
                       * 1            * 2               * 3                           0                                                                    0
    SC input
                  1
                                                                                                                                                         −0.5
                                                                                                                                                                         Weighted LIP activation (choice 2)
                  0                                                                  −1                                                                                  LIP unit 6 activation
                                                                                                                                                          −1             Avg weighted LIP activation (choice 2)
                                                                                                                                                                         Avg LIP unit 6 activation
         −1                                                                                                                                              −1.5
                                                                                     −2
 LIP unit #   2 4 6 8 10 12 14 16 18                                                      0   0.2     0.4      0.6     0.8      1.0   1.2                       0   0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2
 Visual angle 40o 120o 200o 280o  360o
                                                                                                            Time (seconds)                                                       Time (seconds)
Figure 6: Panel A, top, shows LIP unit activations at several time points within a decision. Activations are averaged over many
instances of stimulus type 2, which produces maximal activation in LIP unit 6 (visual angle 120◦ ; here we arbitrarily quantized
visual angle into 18 levels). Panel A, bottom, shows the weighted values of these activations feeding into each of 3 SC units.
Panel B shows the average state of evidence accumulation for choice 2 (input to SC unit 2; blue) and average LIP unit 6 activity
(red) within fixed-viewing time trials, without thresholds applied to the evidence (the interrogation protocol). Panel C shows the
average state of weighted evidence accumulation for choice 2 (blue) and average unit 6 activity (red) within free response trials
(black line indicates threshold). The weighted sum produces a higher signal-to-noise ratio, and therefore better performance
than evidence from unit 6 alone. Red and blue traces fall off over time because the average is based on fewer and fewer trials
as time progresses (more and more decisions have already taken place by the end of the plot).
neural signals coming from an input layer. These features are                                                  City, CA: Addison-Wesley Publishing Company Advanced
essential elements of a complete decision-theoretic model.                                                     Book Program.
                                                                                                             Law, C.-T., & Gold, J. (2008). Neural correlates of perceptual
                                      Acknowledgments                                                          learning in a sensory-motor, but not a sensory cortical area.
P. Simen was supported by a postdoctoral training fellowship                                                   Nat. Neurosci., 11, 505-513.
from the National Institute of Mental Health (MH080524).                                                     McMillen, T., & Behseta, S. (2010). On the effects of sig-
                                                                                                               nal acuity in a multi-alternative model of decision making.
                                              References                                                       Neural Comput., 22(2), 539-580.
                                                                                                             McMillen, T., & Holmes, P. (2006). The dynamics of choice
Bogacz, R., Brown, E., Moehlis, J., Hu, P., Holmes, P., &
                                                                                                               among multiple alternatives. J. Math. Psych., 50(1), 30-57.
  Cohen, J. (2006). The physics of optimal decision making:
                                                                                                             Pouget, A., Deneve, S., Ducom, J.-C., & Latham, P. (1999).
  A formal analysis of performance in two-alternative forced
                                                                                                               Narrow vs. wide tuning curves: what’s best for a population
  choice tasks. Psych. Rev., 113(4), 700-765.
                                                                                                               code? Neural Comput., 11, 85-90.
Bogacz, R., & Gurney, K. (2007). The basal ganglia and
                                                                                                             Seriés, P., Latham, P., & Pouget, A. (2004). Tuning curve
  cortex implement optimal decision making between alter-
                                                                                                               sharpening for orientation selectivity: coding efficiency
  native actions. Neural Comput., 19(2), 442–477.
                                                                                                               and the impact of correlations. Nat. Neurosci., 7(10), 1129-
Butts, D. A., & Goldman, M. S. (2006). Tuning curves,
                                                                                                               1135.
  neuronal variability, and sensory coding. PLoS Biol, 4(4),
                                                                                                             Shadlen, M., & Newsome, W. (2001). Neural basis of a
  e92.
                                                                                                               perceptual decision in the parietal cortex (area LIP) of the
Dragalin, V., Tartakovsky, A., & Veeravalli, V. (1999). Multi-
                                                                                                               rhesus monkey. J. Neurophysiol., 86, 1916-1936.
  hypothesis sequential probability ratio tests, part I: Asymp-
                                                                                                             Usher, M., & McClelland, J. (2001). On the time course
  totic optimality. IEEE Trans. Inform. Theory, 45, 2448-
                                                                                                               of perceptual choice: The leaky competing accumulator
  2461.
                                                                                                               model. Psych. Rev., 108, 550-592.
Ghose, G., Yang, T., & Maunsell, J. (2002). Physiological
                                                                                                             Wald, A., & Wolfowitz, J. (1948). Optimal character of the
  correlates of perceptual learning in monkey V1 and V2. J.
                                                                                                               sequential probability ratio test. Ann. Math. Statist., 19,
  Neurophysiol., 87, 1867-1888.
                                                                                                               326-339.
Gold, J., & Shadlen, M. (2001). Neural computations that un-
                                                                                                             Zhang, K., & Sejnowski, T. (1999). Neural tuning: to sharpen
  derlie decisions about sensory stimuli. Trends in Cognitive
                                                                                                               or broaden? Neural Comput., 11, 75-84.
  Sciences, 5, 10-16.
Hertz, J., Krogh, A., & Palmer, R. (1991). Introduc-
  tion to the theory of neural computation. Redwood
                                                                                                     1821

