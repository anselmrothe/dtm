UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Implicit and Explicit Processes in Recursive Sequence Structure Learning
Permalink
https://escholarship.org/uc/item/0j15m7qf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Author
Alexandre, Jamie
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

  Modeling Implicit and Explicit Processes in Recursive Sequence Structure Learning
                                              Jamie D. Alexandre (jdalexan@ucsd.edu)
                                          Department of Cognitive Science, 9500 Gilman Drive
                                                          La Jolla, CA 92093 USA
                              Abstract                                  the class of regular grammars, which doesn’t shed light onto
   Recursive structure is viewed as a central property of human
                                                                        the acquisition or processing of context-free or recursive
   language, yet the mechanisms that underlie the acquisition and       structure. The goal of the present study is to obtain estimates
   processing of this structure are subject to intense debate. The      of a subject’s online string continuation expectancies while
   artificial grammar learning paradigm has shed light onto             responding to sequences generated by a context-free
   syntax acquisition, but has rarely been applied to the more          grammar (palindromes), so that these may be compared with
   complex, context-free grammars that are needed to represent          the predictions made by a variety of language models trained
   recursive structure. We adapt the artificial grammar serial          on the same input history as the subject. The traditional
   reaction time task to study the online acquisition of recursion,
   and compare human performance to the predictions made by a           measures of successful acquisition in artificial grammar
   number of computational language models, chosen to reflect           experiments – such as grammaticality judgments or recall
   multiple levels and types of syntactic complexity (n-grams,          error rates – are not able to provide the incremental (symbol-
   hidden markov models, simple recurrent networks, and                 by-symbol) expectancy data that we require. We adapt a
   Bayesian-induced probabilistic context-free grammars).               paradigm first employed by Cleeremans & McClelland
   Evidence is found for a dissociation between explicit and            (1991), known as a serial reaction time task, in which
   implicit mechanisms of sequence processing, with the SRN
                                                                        subjects respond to a sequences of stimuli (with a button
   more highly correlated with implicit performance, and the
   PCFG more correlated with explicit awareness of the                  mapped onto each stimulus class) by pressing the
   sequential structure.                                                corresponding button as quickly as possible after perceiving
                                                                        stimulus onset. The resulting reaction times are then
   Keywords: artificial grammar learning; syntax; recursion;            correlated with the probabilities generated by the competing
   serial reaction time task; simple recurrent network; context-
   free grammars; implicit/explicit processes.                          computational models.
                          Introduction                                                             Surprisal
   The nature of linguistic structure, and the computational               Surprisal, or self-information, is a notion from information
mechanisms by which humans comprehend it, have long                     theory that quantifies the amount of novel information that a
been subject to heated debate. Recursion – the ability to               particular event carries with it. An event’s surprisal is
hierarchically embed elements within instances of                       defined as its negative log probability:
themselves – has been a central point of contention.
Although the recursive structure of language was not a new                                      –log( P(x | context) )
idea at the time, Chomsky formalized the notion of syntactic
recursion, touting it as the fundamental property that allows              The concept of surprisal has been used in
for human linguistic ability, a thesis he continues to                  psycholinguistics as a potential measure of incremental
popularize today (Chomsky, 1956; Hauser, Chomsky, &                     processing difficulty, and is thus expected to correlate with
Fitch, 2002).                                                           behavioral measures such as reading times in eye-tracking
   In the Chomskyan tradition, the human syntactic system               studies, and response times in self-paced reading studies
implements a set of rules that allow for theoretically                  (Hale, 2001; Levy, 2008).
unbounded levels of recursive embedding (“competence”),                    The surprisal model requires that we adopt some measure
but this system is then subject to processing constraints, such         of the probability of a word’s occurrence given the preceding
as working memory limitations, that explain our limited                 sentential context. Hale (2001) uses a probabilistic Earley
ability to process recursive structures beyond a few levels of          parsing algorithm to generate incremental word probabilities,
embedding (“performance”). Other theorists, particularly                using the resulting surprisal values to explain the garden path
from the connectionist camp, have attempted to explain the              effect. Levy (2008) uses a similar model to explain a wide-
(limited) human ability to process recursive structure without          range of effects found in the psycholinguistic literature, such
hypothesizing unbounded competence, by modeling                         as predictability (e.g. effect of Cloze probability), locality
syntactic processing in systems that do not make use of rules           effects (e.g. preference for local dependencies),
or explicit representations (e.g. Elman, 1990; Pollack, 1990;           competition/dynamical models (e.g. greater ease in highly
Christiansen & Chater, 1999).                                           constrained contexts), the tuning hypothesis (e.g. effect of
   The artificial grammar learning paradigm (initiated by               structural frequency), and connectionist models (e.g.
Reber, 1967) has been used to examine processes of                      predictions made by an SRN). The case of the SRN is
syntactic acquisition, but this has been largely restricted to          particularly interesting, because there are significant
                                                                        divergences between the predictions made by an SRN and a
                                                                     1381

PCFG-based surprisal model, particularly for constructions
such as recursive center-embeddings, which PCFGs process            Hidden Markov Model (HMM)
flawlessly, and SRNs – much like humans – have difficulty
processing beyond a few levels of embedding (Christiansen              Whereas n-gram transition probabilities are defined
& Chater, 1999).                                                    between sets of adjacent words, the transitions in a hidden
   Frank (2009) tested a surprisal model against human eye-         markov model (HMM) are defined over a set of “hidden”
tracking data from the Dundee corpus, comparing PCFG-               states, and these states, in turn, generate the individual
with SRN-generated probabilities, and found that the PCFG           words. The idea is that there is an underlying “hidden
produced more accurate objective probabilities, but that the        markov process” that we cannot access directly, and all we
SRN produced probabilities that better matched the human            can observe is the final sequence of words that is produced
data. He concludes from this, firstly, that subjective              by this underlying state sequence. Computationally, HMMs
probabilities diverge from the objective probabilities, and         roughly correspond to regular languages at the bottom of the
secondly, that the SRN may in fact be a better model of             Chomsky hierarchy.
human performance. Other surprisal studies have used n-                We use the standard Baum-Welch algorithm (Baum et al,
gram statistics, such as a trigram model with Kneser-Ney            1970) to estimate the HMM’s transition and emission
smoothing (Smith & Levy, 2008), and also shown close                matrices from the training corpus (the preceding sequences)
correspondences with human eye-tracking data.                       for an HMM with 5 hidden states. The trained HMM is then
                                                                    used to compute the incremental posterior probabilities of
                      Language Models                               each symbol given its preceding context. As always, the
   A probabilistic language model is a distribution over the        predictions only used the preceding sequences as a training
strings (sentences) in a language. The models considered in         corpus (so as to be comparable to the human data).
this paper also all support incremental prediction; that is,
given a sentence prefix, they assign a distribution over the        Simple Recurrent Network (SRN)
symbols that might come next.
   To allow for comparison with the human data, each of the            A simple recurrent network (SRN) is a standard three-
models is trained on the precise input that a subject has been      layer feed-forward network, with the addition of a context
exposed to at every point in the experiment (rather than            layer that maintains a copy of the hidden layer’s state from
training on a larger corpus, or simply using the probabilities      the previous timestep, and then allows the nodes in this
assigned by the model that generated the stimuli). This             context layer to feed back into the hidden layer during the
allows us to observe how a subject’s predictions change over        next timestep, alongside the next input (Elman, 1990). The
the course of learning, to gain insight into the rate at which a    context layer in an SRN effectively implements time-tapped
system is acquired, as well as possible shifts in strategy,         feedback loops from every node in the hidden layer back to
rather than simply comparing fully trained systems.                 each of the nodes in the hidden layer (delayed by one
   It is also important to note that none of the model              timestep). The addition of recurrent hidden layer connections
parameters are fit to the human data; a model is trained to         allows an SRN to learn to use its hidden layer representations
predict a sequence’s continuation based on the set of               to maintain task-relevant contextual information over
sequences it has seen up to that point in the experiment,           theoretically unbounded (though in most cases, rapidly
making use of the algorithmic and representational resources        decaying) distances.
at its disposal, but agnostic to human performance.                    The SRN used in this paper contained 9 input nodes (one
   The models were chosen from amongst those most                   for each symbol, plus a sequence boundary marker), 16
commonly used within computational linguistics to model             hidden nodes, and 9 output nodes. The network was trained
sequential structure, at various levels of complexity (some         using standard back-propagation, with a learning rate of 0.5
corresponding roughly to levels in the Chomsky hierarchy).          and no momentum, on a single pass through the sequences.
                                                                    Output activations at every timestep were converted into
N-grams (bigrams/trigrams)                                          probabilities through the Luce choice rule (in effect,
   One of the simplest but most commonly used language              normalizing the network’s output vector).
models, n-grams calculate the probability of a symbol in
terms of the frequency with which it occurs in its                  Probabilistic Context-Free Grammar (PCFG)
immediately preceding context. Here, we will consider
bigrams (which take into account the preceding symbol of               Context-free grammars (CFGs) have played a central role
context) and trigrams (which take into account the 2                in linguistic theories of syntax ever since Chomsky (1956)
preceding symbols). The predictions made by the n-grams at          proposed them as being necessary (and almost sufficient) to
every step were based on training on all preceding sequences        account for the types of recursive phrase structure observed
(excluding the sequences that had not yet been seen).               in human language. A probabilistic context-free grammar
                                                                    adds probabilities to the production rules in a context-free
                                                                    grammar, allowing us to calculate a distribution over strings
                                                                    in the language.
                                                                 1382

   Once we know the parameters of the grammar (see below),         Participants Eight subjects (mean age 20.5, all right-
incremental predictions can be computed as follows (adapted        handed), drawn from the UCSD undergraduate subject pool,
from Jelinek & Lafferty, 1991):                                    received 2 hours of course credit for their participation.
   1.      The probability of a string is the sum of the
           probabilities of all its parse trees.                   Stimuli Sequences were generated from the following
   2.      The probability of a string prefix is a sum over the    grammar in Table 1.
           probabilities of all possible completions of the
           prefix.                                                    Table 1: Context-free grammar used to generate stimuli.
   3.      The probability that a particular symbol wi will
           appear following the string prefix w1..wi-1 can be                     Probability     Production Rule
           computed by dividing the probability of the prefix                        0.193            S Æ T0 S T0
           with that symbol appended, P(w1..wi), by the                              0.146            S Æ T1 S T1
           probability of the prefix, P(w1..wi-1)                                    0.112            S Æ T2 S T2
                                                                                     0.128            S Æ T3 S T3
   Stolcke (1995) modified the Earley parsing algorithm to                           0.077            S Æ T4 S T4
compute the above incremental probabilities efficiently, and                         0.082            S Æ T5 S T5
we use an implementation by Levy (2008) in the present                               0.159            S Æ T6 S T6
work.                                                                                0.103              S Æ T7
   Learning the parameters of a PCFG from an unparsed
corpus is not a trivial task, however. Here, we use a
                                                                      This grammar generates palindromes, a particular type of
Bayesian framework developed by Mark Johnson1 that uses            “mirror recursion” in which the right-hand side of the
Gibbs sampling to learn the probabilities for a set of
                                                                   sequence is a mirror image (flipped left-to-right) of the left-
production rules, given a corpus of training sequences. All        hand side. The 7th symbol serves as a consistent center
combinations of production rules with 8 states (in Chomsky
                                                                   marker, making the grammar deterministic. An example
Normal Form, e.g. A->BC) were included in set of candidate         sequence would be “0 4 1 3 7 3 1 4 0”.
rules, and the sampler was given a prior of alpha=0.0001.
                                                                      Palindomes are the canonical example of context-free
The counts on the final sample grammar were normalized             structures, and possibly the simplest type of grammar that is
into probabilities. As with all the other models, the
                                                                   context-free and thus cannot be fully captured by finite state
predictions made for every symbol were based on re-training        models such as an HMM, or by n-gram statistics.
after every sequence, using only on the sequences that
                                                                      An experimental session consisted of 16 blocks of 25
occurred prior to that point in the experiment, so that the        sequences each, with sequences ranging in length from 5 to
models have precisely the same information available to
                                                                   15. Each of the 8 subjects were presented with the same set
them at each timestep as the human subjects. This entire           of sequences, but with a different mapping of symbols to
process was repeated 5 times, and the resulting sequences of
                                                                   buttons, shuffled in a Latin-square design such that every
probabilities were averaged together.                              symbol was mapped onto each of the 8 buttons for exactly
                                                                   one subject (to balance out any effects of button location or
                           Experiment                              between-button distances).
Methods                                                            Procedure Subjects were told that the purpose of the
Interface Care was taken in designing and constructing an          experiment was to study the “effects of practice on reaction
interface device for the task, due to concerns about               times”, and were told to “hit each button as quickly as
measurement noise. The button box (Figure 1) consists of 8         possible when that button’s light goes on”. No mention was
finger-sized push buttons arranged in a 2x4 array, with each       made regarding the structured nature of the stimuli; as far as
button containing its own separately controllable LED for          the subjects were concerned, the sequences were entirely
use as a response cue. The buttons and LEDs are interfaced         random.
to the PC via a USB-powered LabJack U3 DAQ device,                    Sequences were presented rapidly, with the next light in a
which has very high sampling rates and low command-                sequence turning on 120ms after the previous button had
response latencies, allowing for RTs to be measured to             been released. After the end of an individual sequence there
millisecond accuracy.                                              was a 2 second pause before the next sequence began.
                                                                      In between blocks, subjects were presented with a
                                                                   feedback screen indicating their performance on the block
                                                                   relative to their performance on earlier blocks (plotting their
                                                                   RT contour over time), and also relative to previous subjects,
                                                                   by means of a highscores list derived from earlier pilot
                                                                   testing. Subjects were given a chance to take a short break
                                                                   in between blocks.
              Figure 1: Button box used in experiment.
                                                                      After completing the experiment, subjects were
   1                                                               interviewed about the strategies they had employed in the
     http://www.cog.brown.edu/~mj/Software.htm
                                                                1383

task, the factors they thought affected their performance, and                                     possibility, partial correlations between the human reaction
what sorts of patterns (if any) they had noticed in the                                            times and the models are computed after regressing out the
sequences.                                                                                         bigram and trigram statistics. The residual correlations are
                                                                                                   plotted in Figure 3.
Results and Analysis
Reaction times longer than 1000ms (greater than ~4.2 std                                                                                  0.4
above mean) were excluded from analysis, to eliminate
                                                                                                         Correlation (Surprisal vs RT)
extreme outliers caused by events not related to the task                                                                                 0.3
(such as distractions, subject sneezing, etc). Only 0.2% of
the trials were excluded by this criterion. In addition, the
                                                                                                                                          0.2
first trial of every sequence was excluded from correlation
analyses, as earlier pilot testing using random sequences
                                                                                                                                          0.1
showed that mean reaction times for these sequence-initial
trials were ~70ms slower than for the remainder of the
                                                                                                                                           0
sequence. Reaction times for error trials (when the incorrect
button was pressed) were measured from when the light went
on to when the correct button was pressed, ignoring the                                                                                  -0.1
intervening erroneous button press. Subjects made an                                                                                            1-4             5-8             9-12           13-16
average of 65 errors each (1.7% of the trials), and these trials                                                                                      Blocks (average of 233 trials per block)
were not excluded from the analysis, but doing so has no
                                                                                                                     Figure 3: Partial correlations between model probabilities
noticeable effect.
                                                                                                                      and reactions times, regressing out n-gram probabilities.
   The median reaction time for each trial is calculated across
subjects, and then the resulting sequence of reaction times is                                        As is to be expected, the bigram and trigram correlations
correlated with the sequences of surprisal values (negative                                        become insignificant. The HMM correlations are also
log probability) generated by each of the models. The                                              eliminated after the first couple of blocks (at which point
experiment is divided up into four parts to visualize how the                                      none of the models have learned very much), suggesting that
correlations change over the course of training. Standard                                          the HMM was not explaining anything significant about the
correlation coefficients and 95% confidence intervals are                                          human behavior beyond n-gram statistics. Both the SRN and
plotted in Figure 2. Note that each of the models is                                               PCFG, however, maintain significant correlations
significantly correlated with the human reaction time data                                         throughout, suggesting that they are capturing more about
throughout the experiment, though with no model clearly                                            the human reaction times than simply a sensitivity to n-gram
dominating (except perhaps a slight preference for the SRN).                                       statistics.
                                                                                                      We might then wonder whether a common component is
                                 0.6
                                                                                                   responsible for both the SRN and PCFG correlations, or if
                                                                                                   they are each accounting for distinct aspects of the human
 Correlation (Surprisal vs RT)
                                 0.5
                                                                                                   behavior. To test this, we regress out all models except for
                                                                                                   the model of interest, and see how much of the variance
                                 0.4
                                                                                                   remains for that model to explain.
                                                                                                      Regressing out all the models besides the PCFG reduces
                                 0.3                                                               its correlations very slightly, but they remain highly over the
                                                                                                   course of a session, as can be seen in Figure 4 below.
                                 0.2
                                 0.1
                                                                                                                                         0.25
                                                                                                   Correlation (Surprisal vs RT)
                                  0
                                       1-4             5-8             9-12             13-16                                             0.2
                                             Blocks (average of 233 trials per block)
                                                                                                                                         0.15
                                 Figure 2: Correlations between models and human
                                  reaction times over the course of the experiment.                                                       0.1
   Several possible interpretations exist at this point. Since                                                                           0.05
the models themselves are quite strongly inter-correlated, it
is possible that the correlations for each of the models could                                                                             0
be explained by a common shared component. In particular,
each of the models is capable of representing n-gram                                                                                            1-4            5-8             9-12            13-16
                                                                                                                                                      Blocks (average of 233 trials per block)
statistics, so perhaps this could explain some portion of the
correlation in the other models.          To investigate this                                                   Figure 4: Partial correlations, regressing out all but PCFG.
                                                                                                1384

   Similarly, regressing out all models other than the SRN
has very little effect on the SRN correlations, which remain                                                                                  0.4
strong throughout, despite declining somewhat towards the
                                                                                                              Correlation (Surprisal vs RT)
end (Figure 5).                                                                                                                               0.3
                                         0.4
         Correlation (Surprisal vs RT)
                                                                                                                                              0.2
                                         0.3
                                                                                                                                              0.1
                                         0.2
                                                                                                                                               0
                                         0.1
                                                                                                                                                    1-4             5-8             9-12           13-16
                                           0                                                                                                              Blocks (average of 233 trials per block)
                                                                                                               Figure 7: Subjects who were explicitly aware of structure;
                                                1-4             5-8             9-12           13-16             partial correlations, regressing out n-grams and hmm.
                                                      Blocks (average of 233 trials per block)
                Figure 5: Partial correlations, regressing out all but SRN.                                  The subjects who were able to report explicit knowledge
                                                                                                          of aspects of the palindromic structure, by the end of the
  These results seem to suggest that multiple simultaneous                                                experiment, showed the strongest correlation with the PCFG
processes are playing a role in human behavior on the task;                                               (Figure 7), whereas the SRN correlated more strongly with
on the one hand, an associative, incremental component                                                    the group that gained no explicit awareness of the structure
captured by the SRN, and on the other hand, a more rule-                                                  (Figure 6), indicating that the variance explained by the SRN
based, recursive component exemplified by the PCFG. As                                                    may reflect a more automatic, implicit processing of the
SRN models have frequently been used to model implicit                                                    sequential structure (as suggested, for example, by
learning (e.g. Cleeremans, 1993; Misyak et al, 2009),                                                     Cleeremans, 1993), whereas the acquisition of recursive,
whereas PCFGs are more often associated with explicit rule-                                               rule-like structures may involve more explicit, conscious
based knowledge, we examined individual differences                                                       processing. It was not possible to query subjects partway
between subjects with regards to implicit and explicit                                                    through the experiment about whether they had noticed any
learning, to see if this might help to explain this dissociation.                                         patterns without drawing their attention to the existence of
  In the post-testing questionnaire, 3 of the 8 subjects                                                  structure, but the sudden divergence between the PCFG and
identified some type of structure within the sequences; some                                              SRN in Figure 7 lines up well with subjects’ comments
referred to it as a “circular” or “mirror” pattern, and one also                                          during the post-test interview that they had begun to notice
gave explicit palindromic examples. The 5 remaining                                                       the pattern somewhere in the “middle of the experiment”.
subjects had not noticed any regularity to the sequences,                                                    It is also instructive to examine the pattern of reaction
even when probed further (2 of these “felt” like there might                                              times over the course of an average sequence. As the
be some pattern, but could not articulate any details). We                                                sequences are of different lengths, position on the x-axis is
separated these two groups from one another and once again                                                represented as percentage of the way through a sequence
calculated partial correlations (regressing out n-grams and                                               (Figure 8).
the hmm).
                                         0.4
 Correlation (Surprisal vs RT)
                                         0.3
                                         0.2
                                         0.1
                                           0
                                         -0.1
                                                1-4             5-8             9-12           13-16
                                                      Blocks (average of 233 trials per block)
         Figure 6: Subjects with no explicit awareness of structure;                                          Figure 8: Comparison of RTs and model surprisal over the
           partial correlations, regressing out n-grams and hmm.                                                course of an average sequence (scaled by percentage).
                                                                                                       1385

   There are several things to note in this reaction time data.
Firstly, subjects seem to show a strong advantage in the                                      References
second half of the sequence, which is consistent with the
symbols in the second half being completely determined by           Baum, L. E., Petrie, T., Soules, G., and Weiss, N. (1970). A
the symbols in the first half (due to the palindromic nature of       maximization technique occurring in the statistical
the sequences), and which is seen most strongly both in the           analysis of probabilistic functions of markov chains. The
PCFG and in the learners with explicit awareness of the               Annals of Mathematical Statistics, 41(1):164-171.
structure. Secondly, this advantage is greater immediately          Chomsky, N. (1956). Three models for the description of
following the center symbol and reaction time and then                language. IEEE Transactions on Information Theory,
increases slightly as the sequence continues. This is                 2(3):113-124.
consistent with the fact that later symbols in the second half      Christiansen, M. H. and Chater, N. (1999). Toward a
involve longer-range dependencies, and thus may reflect               connectionist model of recursion in human linguistic
working memory limitations. The reason for the peak seen              performance. Cognitive Science, 23(2):157-205.
halfway through the sequences in both the implicit learners         Cleeremans, A. (1993). Mechanisms of implicit learning:
and the SRN is at first unclear, but it is tempting to interpret      connectionist models of sequence processing. MIT Press.
it as reflecting the cognitive load involved in needing to flip     Cleeremans, A. and McClelland, J. L. (1991). Learning the
around the first half of the sequence in order to predict the         structure of event sequences. Journal of experimental
second half, although we might expect this to appear in the           psychology. General, 120(3):235-253.
explicit rather than the implicit subjects.                         Elman, J. L. (1990). Finding structure in time. Cognitive
                                                                      Science, 14(2):179-211.
Discussion                                                          Frank, S. (2009). Surprisal-based comparison between a
   We attempted to shed light on the mechanisms underlying            symbolic and a connectionist model of sentence
human processing of recursive structure, by extending the             processing. Proceedings of the 31st Annual Cognitive
artificial grammar serial reaction time paradigm in two ways;         Science Society Conference (pp. 1139-1144). Austin, TX:
firstly, by training subjects on more complex grammars than           Cognitive Science Society.
are typically used (context-free grammars); and secondly, by        Hale, J. (2001). A probabilistic Earley parser as a
comparing performance not only to transitional n-gram                 psycholinguistic model. Proceedings of NAACL, vol. 2:
probabilities and connectionist models, but also to a                 159–166.
Bayesian-induced PCFG model, trained on the exact same              Hauser, M. D., Chomsky, N., and Fitch, W. T. (2002). The
set of sequences as the subjects. Evidence was found for a            faculty of language: What is it, who has it, and how did it
dissociation between implicit and explicit modes of                   evolve? Science, 298(5598):1569-1579.
processing, and these modes were seen to correlate most             Jelinek, F. and Lafferty, J. D. (1991). Computation of the
strongly with the predictions of the SRN and the PCFG,                probability of initial substring generation by stochastic
respectively.                                                         context-free grammars. Computational Linguistics,
   It may also be fruitful to examine the effects of making           17(3):315-323.
subjects explicitly aware of the structure prior to beginning       Levy,      R.       (2008).    Expectation-based       syntactic
the task, as the results of the present study would suggest this      comprehension. Cognition, 106:1126-1177.
would lead to greater correlation with the predictions of the       Misyak, J.B., Christiansen, M.H. & Tomblin, J.B. (2009).
PCFG. It would also be useful to provide a longer training            Statistical learning of nonadjacencies predicts on-line
period, to shed light on how these processes change over the          processing of long-distance dependencies in natural
course of more extensive exposure.                                    language. Proceedings of the 31st Annual Cognitive
                                                                      Science Society Conference (pp. 177-182). Austin, TX:
                                                                      Cognitive Science Society.
                                                                    Smith, N. and Levy, R. (2008). Optimal Processing Times in
                                                                      Reading: a Formal Model and Empirical Investigation.
                                                                      Proceedings of the 30th Annual Meeting of the Cognitive
                                                                      Science Society (oral presentation).
                                                                    Pollack, J. B. (1990). Recursive distributed representations.
                                                                      Artif. Intell., 46(1-2):77-105.
                                                                    Reber, A. S. (1967). Implicit learning of artificial grammars.
                                                                      Journal of Verbal Learning and Verbal Behavior,
                                                                      6(6):855-863.
                                                                    Stolcke, A. (1995). An efficient probabilistic context-free
                                                                      parsing algorithm that computes prefix probabilities.
                                                                      Computational Linguistics, MIT Press for the Association
                                                                      for Computational Linguistics, 21.
                                                                 1386

