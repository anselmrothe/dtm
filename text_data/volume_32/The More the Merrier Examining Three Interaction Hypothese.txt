UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The More the Merrier? Examining Three Interaction Hypothese
Permalink
https://escholarship.org/uc/item/9917w3sq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Chi, Min
VanLehn, Kurt
Litman, Diane
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                The More the Merrier? Examining Three Interaction Hypotheses
                                                      Min Chi (minchi@cs.cmu.edu)
                           Machine Learning Department, Carnegie Mellon University, 5000 Forbes Avenue
                                                           Pittsburgh, PA 15213 USA
                                                Kurt VanLehn (Kurt.Vanlehn@asu.edu)
                   School of Computing, Informatics and Decision Systems Engineering, Arizona State University
                                                             Tempe, AZ 85287 USA
                                                   Diane Litman (litman@cs.pitt.edu)
                         Department of Computer Science, University of Pittsburgh, 210 South Bouquet Street
                                                           Pittsburgh, PA 15260 USA
                               Abstract                                    2008). In a meta-analysis of the tutoring literature, VanLehn
                                                                           found little support for the monotonic interactivity hypothesis
   While high interactivity is one of the key characteristics of one-
   on-one human tutoring, a great deal of controversy surrounds            and instead proposed the interaction plateau: the hypothesis
   the issue of whether interactivity is indeed the key feature of         that increased interactivity increases learning up to a point
   tutorial dialogue that impacts students’ learning. In this paper        (roughly, the level of interactivity afforded by conventional
   we investigate three interaction hypotheses: a widely-believed
   monotonic interactivity hypothesis, a better supported interac-         step-based ITSs); beyond that threshold, however it does not
   tion plateau hypothesis, and our tactical interaction hypothesis.       yield any noticeable increases in learning (VanLehn, submit-
   The monotonic interaction hypothesis predicts that increasing           ted).
   interactivity causes an increase in learning; the plateau hypoth-
   esis states that increasing interactivity yields increasing learn-         On the other hand, for any form of tutoring the tutor’s
   ing until it hits a plateau, and further increases in interactiv-       behaviors can be viewed as a sequential decision processes
   ity do not cause noticeable increases in learning. Finally, the
   tactical interaction hypothesis predicts that interactivity only        wherein, at each discrete step, the tutor is responsible for se-
   increases learning when interactions are guided by effective            lecting the next action to take. Each of these tutorial deci-
   tutorial tactics. In this paper, we examine each hypothesis in          sions affects successive actions. Some existing theories of
   the context of an empirical study, the results of which support
   the tactical interaction hypothesis.                                    learning suggest that when making tutorial decisions, a tu-
   Keywords: machine learning; reinforcement learning; peda-               tor should adapt its actions to the students’ needs based upon
   gogical strategy; Intelligent Tutoring Systems.                         their current knowledge level, affective state, and other salient
                                                                           features (Vygotsky, 1971; Collins et al., 1989; Koedinger &
                           Introduction                                    Aleven, 2007). Most studies cited above made use of human
One-on-one tutoring is a highly effective educational inter-               tutors for their highly-interactive condition, simply assuming
vention. Tutored students often perform significantly better               that expert tutors will take optimal actions. However, Chi et
than students in classroom settings (Bloom, 1984). Computer                al. and others have argued that human tutors may not always
learning environments that mimic aspects of human tutors                   make optimal tutorial decisions (M. T. H. Chi et al., 2001,
have also been highly successful. Intelligent Tutoring Sys-                2008). Given that tutoring is a rather complex procedure and
tems (ITSs) have been shown to be highly effective at improv-              tutors have to make many decisions fairly rapidly, even expert
ing students’ learning in real classroom settings (Koedinger               human tutors may not take the full advantage of the tutorial
et al., 1997; VanLehn, 2006). A key characteristic of one-on-              alternatives.
one tutoring, with both human and computer tutors, is high                    Therefore, in this paper we propose a third hypothesis: the
interactivity.                                                             tactical interaction hypothesis. It states that interactivity only
   A common assumption, often referred as the monotonic in-                increases learning when interactions are guided by effective
teraction hypothesis (VanLehn, Graesser, et al., 2007) is that             tutorial tactics. By “tutorial tactics” we refer to the policies
greater interactivity leads to greater learning.                           used for selecting the tutorial action taken at each step when
   However, several studies have failed to confirm this hy-                there are multiple actions available. In other words, we hy-
pothesis. Experiments with human tutors found no sig-                      pothesize that the tutors’ success will not be governed by how
nificant differences in learning gains when content was                    often they give interactive prompts or ask the students ques-
carefully controlled and interactivity was directly manipu-                tions but how well.
lated (M. T. H. Chi et al., 2001, 2008; Rose et al., 2001).                   To investigate the three hypotheses, we focused on two tu-
Experiments that compared human tutors and several Natural                 torial actions: elicit and tell. During the course of one-on-one
Language dialogue-based computer tutors also found no sig-                 tutoring, tutors often face a simple question, should they elicit
nificant differences in learning as interactivity varied across            the next step information from the student, or should they tell
students (Evens & Michael, 2006; VanLehn, Graesser, et al.,                the student the next step directly? There are many theories,
2007; Reif & Scott, 1999; Katz et al., 2003; Fossati et al.,               but no widespread consensus on how or when an elicit or a tell
                                                                       2870

                             (a) Elicit Version                                                  (b) Tell Version
     1. T: So let’s start with determining the value of v1.             1. T: So let’s start with determining the value of v1.
     2. T: Which principle will help you calculate the rock’s instan-   2. T: To calculate the rock’s instantaneous magnitude of veloc-
         taneous magnitude of velocity at T1? {ELICIT}                      ity at T1, we will apply the definition of kinetic energy again.
                                                                            {TELL}
     3. S: definition of kinetic energy
     4. T: Please write the equation for how the definition of kinetic
         energy applies to this problem at T1 {ELICIT}                  3. T: Let me just write the equation for you:               KE1 =
                                                                            0.5*m*v1^2. {TELL}
     5. S: ke1 =0.5*m*v1^2
                                                                        4. T: From KE1 = 0.5*m*v1^2, · · ·
     6. T: From KE1 = 0.5*m*v1^2, · · ·
                                                           Figure 1: Elicit vs. Tell
should be taken (Vygotsky, 1971; Aleven et al., 2004; Collins            et al., 2002). In this work, rather than implementing peda-
et al., 1989). Generally speaking, eliciting more information            gogical policies drawn from human experts or theories, we
from the student will result in a more interactive tutorial dia-         applied and evaluated RL to derive pedagogical tutorial tac-
logue. Figure 1 compared a pair of dialogues extracted from              tics using pre-existing interactivity data.
logs in this study. Both dialogues begin and end with the
same tutor turn (lines 1 and 6 in (a) and 1 and 4 in (b)). In                                   General Approach
dialogue (a) the tutor chooses to elicit twice (lines 2-3 and
4-5 respectively). Dialogue (b), by contrast, covers the same            For this study, we induced two sets of tutorial tactics: the
domain content with two tell actions (lines 2 and 3). As a               Normalized Gain (NormGain) tactics, derived with the goal
consequence, dialogue (a) is more interactive than (b).                  of making tutorial decisions that contribute to students’ learn-
                                                                         ing, and the Inverse Normalized Gain (InvNormGain) tactics,
   In this paper, we quantify the interactivity of a dialogue via
                                                                         induced with the goal of making less beneficial, or possibly
the Interactivity ratio (I-ratio) which we define as the number
                                                                         useless, decisions. The two sets were then compared with
of elicitation decisions divided by the total number of elicit or
                                                                         human students on Cordillera (VanLehn, Jordan, & Litman,
tell decisions in a given dialogue. The higher this value, the
                                                                         2007), a Natural Language Tutoring System teaching students
more interactive the tutorial dialogue.
                                                                         introductory college physics. Using Cordillera in lieu of hu-
                                       NElicit                           man tutors allowed us to rigorously control the content and
                    I − ratio =                                   (1)    vary only the interactivity. In order to avoid artifacts due to
                                   NElicit + NTell
                                                                         imperfect natural language understanding, Cordillera incor-
   Unlike the monotonic and plateau hypotheses, validation               porated a human wizard whose sole task was to rapidly match
of the tactical interaction hypothesis requires effective tu-            students’ actual utterance to one of the expected student ut-
torial tactics. In most computer learning environments the               terances displayed in a menu. The wizard made no tutorial
pedagogical tutorial tactics are hard-coded rules designed to            decisions.
implement preexisting cognitive and/or pedagogical theories.                 In the learning literature, it is commonly assumed that rel-
Typically, these theories are considerably more general than             evant knowledge in domains such as math and science is
the specific interaction decisions that designers must make.             structured as a set of independent but co-occurring Knowl-
This makes it difficult to tell if a specific policy is consistent       edge Components (KCs) and that KC’s are learned indepen-
with the theory. Moreover, it is often difficult to empirically          dently. A KC is “a generalization of everyday terms like con-
evaluate these tactics because the tutor’s overall effectiveness         cept, principle, fact, or skill, and cognitive science terms like
depends upon many factors, such as the usability of the sys-             schema, production rule, misconception, or facet” (VanLehn,
tem, how easily the dialogues are understood, and so on. Ide-            Jordan, & Litman, 2007). For the purposes of tutoring, these
ally, several versions of a system are created, each employing           are the atomic units of knowledge. It is assumed that a tu-
different tutorial tactics. Data is then collected with human            torial dialogue focusing on a single KC will not affect the
subjects interacting with these different versions of the sys-           student’s understanding of any other KC. This is an idealiza-
tem and the results are compared. Due to the high cost of ex-            tion, but it has served developers well for many decades, and
periments, however, only a handful of policies are typically             is a fundamental assumption of many cognitive models (An-
explored. Yet, many other reasonable policies are possible.              derson, 1983; Newell, 1994). When dealing with a specific
   In recent years, work on the design of dialogue systems has           KC, the expectation is that the tutor’s best policy for teaching
involved several data-driven methodologies. Among these,                 that KC (e.g., when to Elicit vs. when to Tell) would be based
Reinforcement Learning (RL) has been widely applied(Singh                upon the student’s mastery of the KC in question, its intrinsic
                                                                     2871

 difficulty, and other relevant, but not necessarily known, fac-               the dialogue was in Si and the tutor took Ak . Once a complete
 tors specific to that KC. In other words, an optimal policy for               MDP is constructed, a dynamic programming approach can
 one KC might not be optimal for another. In this study, we fo-                be used to learn the optimal control policy π∗ and here we
 cused on eight KCs. We induced eight policies and conducted                   used the toolkit developed by Tetreault and Litman (Tetreault
 eight tests of the three hypotheses, one per KC.                              & Litman, 2008).
    Later results indicated that on average the percentage of                     In this study, the reward functions for inducing both the
 elicit prompts students received during the tutoring is more                  NormGain and the InvNormGain sets were based on Normal-
 than 70% for both groups in this study, thus based on the                     ized Learning Gain (NLG) defined as: NLG = posttest−pretest
                                                                                                                                        1−pretest
 standard set in (VanLehn, submitted) the tutorial dialogues                   because it measures a student’s gain irrespective of his/her
 reported here are well beyond the threshold of the level of in-               incoming competence. Here posttest and pretest refer to
 teractivity afforded by conventional step-based ITSs. There-                  the students’ test scores before and after the training re-
 fore, we expect that on each KC:                                              spectively; and 1 is the maximum score. More specifically,
                                                                               the NormGain tutorial tactics induced by using the student’s
1. If the monotonic hypothesis is correct, the group that
                                                                               NLG × 100 as the final reward while the InvNormGain ones
    learned more would have a higher I-ratio.
                                                                               was induced by using the student’s (1 − NLG) × 100 as the
2. If the interaction plateau hypothesis is correct, both Norm-                final reward. Apart from the reward functions, the two sets
    Gain and InvNormGain students would learn equally well                     were induced using the same general procedure.
    regardless of interactivity difference.                                       In order to learn a policy for each KC, we annotated our
                                                                               tutoring dialogues and action decisions based on which KCs
3. If the tactical interaction hypothesis is correct and our RL-               a tutor action or tutor-student pair of turns covered (kappa
    based tutorial tactics are indeed effective, NormGain stu-                 ≥ 0.77 for each of the eight KCs). Additionally, we have
    dents would learn more than InvNormGain peers regard-                      mapped students’ pre- and post-test scores to the relevant KCs
    less of interactivity difference.                                          for each test item. The rest of this section presents a few crit-
 First we will briefly describe how we apply machine learning                  ical details of the process, but many others must be omitted
 to induce tutorial dialogue tactics. Then we will describe our                to save space. Overall, the RL approach in this study dif-
 study and its results.                                                        fered from that of the previous study (M. Chi et al., 2009) in
                                                                               many aspects. First, we have three training corpora in this
       Applying RL to Induce Tutorial Tactics                                  study: the Exploratory corpus collected in 2007, the Dich-
 Much of the previous research on the use of RL to improve                     Gain corpus collected in 2008, and a Combined training cor-
 dialogue systems has typically used Markov Decision Prob-                     pus. Second, in order to examine a range of possible tactics
 lems (MDPs) (Sutton & Barto, 1998) to model dialogue data                     we included 50 features based upon six categories of features
 (Singh et al., 1999). An MDP formally corresponds to a 4-                     considered by previous research(Moore et al., 2004; Forbes-
 tuple (S, A, T, R), in which: S = {S1 , · · · , Sn } is a state space;        Riley et al., 2007) to be relevant. Additionally, we also used a
 A = {A1 , · · · , Am } is an action space represented by a set of             different method of searching the power set of the 50 features.
 action variables; T : S × A × S → [0, 1] is a set of transi-                  Finally we directly used the NLG × 100 for inducing Norm-
 tion probabilities P(S j |Si , Ak ), which is the probability that            Gain policies and(1−NLG)×100 for inducing InvNormGain
 the model would transition from state Si to state S j after the               ones instead of dichotomizing the NLGs when inducing poli-
 agent takes action Ak ; R : S × A × S → R assigns rewards to                  cies previously.
 state transitions. Finally, π : S → A is defined as a policy,                    Figure 2 shows an example of a learned NormGain pol-
 which determines which action the agent should take in each                   icy on one KC, “Definition of Kinetic Enegy”. The policy
 state in order to maximize the expected reward.                               involves three features:
    The central idea behind our approach is to transform the                       [StepDifficulty:] encodes a step’s difficulty level. Its value is
 problem of inducing effective pedagogical tactics into com-                   estimated from the students’ log files based on the percentage of
 puting an optimal policy for choosing actions in an MDP. In-                  correct answers given on the step.
 ducing pedagogical tactics can be represented using an MDP:                       [TutorConceptsToWords:] which represents the ratio of the
 the states S are vector representations composed of relevant                  physics concepts to words in the tutor’s dialogue. This feature also
 student-tutor interaction characteristics; A = {Elicit, Tell} in              reflects how often the tutor has mentioned physics concepts overall.
 this study, and the reward function R is calculated from the                      [TutorAvgWordsSession:] The average number of words in the
 system’s success measures and we used learning gains. Once                    tutor’s turn in this session. This feature reflects how verbose the
 the (S, A, R) has been defined, the transition probabilities T                tutor is in the current session.
 are estimated from the training corpus, which is the collection
                                                     ,m
 of dialogues, as: T = {p(S j |Si , Ak )}k=1,···
                                             i, j=1,··· ,n . More specifi-        MDP generally requires discrete features and thus all the
 cally, p(S j |Si , Ak ) is calculated by taking the number of times           continuous features need to be discretized. The top half of
 that the dialogue is in state Si , the tutor took action Ak , and the         Figure 2 lists how each of the three features was discretized.
 dialogue was next in state S j divided by the number of times                 For example, For StepDifficulty, if its value is above 0.38, it
                                                                           2872

 [Feature:]
                                                                        4) solved the same seven training problems in the same or-
   StepDifficulty:                 [0, 0.38) → 0;  [0.38, 1] → 1        der on Cordillera; and 5) finally took a posttest. The pretest
   TutorConceptsToWords:           [0, 0.074) → 0; [0.074, 1] → 1       and posttest were identical. Except for following the policies
   TutorAvgWordsSession:           [0, 22.58) → 0; [22.58, ∞) → 1       (NormGain vs. InvNormGain), the remaining components
 [Policy:]                                                              of Cordillera, including the GUI interface, the same training
     Elicit: 0:0:0 0:0:1 1:0:1 1:1:0 1:1:1                              problems, and the tutorial scripts, were identical for all stu-
     Tell: 0:1:0
     Else:0:1:1 1:0:0                                                   dents.
  Figure 2: A NormGain Policy on KC20 For ET Decisions                  Grading
                                                                        The tests contained 33 test items covering 168 KC occur-
is 1 (difficult) otherwise, it is 0 (easy). The lower half of Fig-      rences. Each occurrence was graded by a single experienced
ure 2 shows there are 8 rules learned: in 5 situations the tutor        grader who was not aware of the study condition from which
should elicit, in one situation it should tell; in the remaining        it arose. These were then summed and normalized to the
2 cases either will do. For example, when all three features            range of [0,1]. Other grading rubrics were also tried. They
are zero (which means when the step is easy, the tutor ratio            presented the same pattern of results as the ones presented
of physics concepts to words so far is low, and the tutor is not        next.
very wordy in the current session), then the tutor should elicit
as 0:0:0 is listed next to the [elicit]. As you can see, three fea-                                Results
tures already provide relatively complex tutorial tactics and
the induced policies were not like most of the tutorial tactics         No significant difference was found between the two condi-
derived from analyzing human tutorial dialogues.                        tions in terms of the total training time spent on Cordillera:
   The resulting NormGain and InvNormGain policies were                 t(55) = 0.27, p = .79. The NormGain group spent (M =
then implemented back into Cordillera yielding two new                  259.98 mins, SD = 59.22) and the InvNormGain group spent
versions of the system, named NormGain-Cordillera and                   (M = 264.57 mins, SD = 67.60). For each student, Cordillera
InvNormGain-Cordillera respectively. The induced tutorial               had made on average 260 decisions on whether to Elicit or to
tactics were evaluated on real human subjects to see whether            tell during the training and on a KC by KC basis, the number
the NormGain students would out-perform the InvNormGain                 of such decisions varies from 4 on KC1 to 72 on KC20 .
peers.
                                                                        Learning Performance
                              Methods                                   First, we investigated whether students learned by training on
Participants                                                            Cordillera. A one-way ANOVA was used to test for learning
Data were collected over a period of two months during the              performance differences between the pre- and posttests. Both
summer of 2009. Participants were 64 college students who               groups made reliable learning gains from pre-test to post-test:
received payment for their participation. They were required            F(1, 56) = 31.34, p = .000 for the NormGain condition and
to have a basic understanding of high-school algebra. How-              F(1, 54) = 6.62, p = .013 for the InvNormGain condition re-
ever, they could not have taken any college-level physics               spectively. On a KC by KC basis, the NormGain conditions
courses. Students were randomly assigned to the two con-                learned reliably on all the eight primary KCs while the In-
ditions. Each took from one to two weeks to complete the                vNormGain learned reliably on five primary KCs save for
study over multiple sessions. In total, 57 students completed           KC14, KC22, and KC28.
the study (29 in the NormGain condition and 28 in the In-                  Next, we compared the learning performance between the
vNormGain condition).                                                   two conditions. Random assignment appears to have bal-
                                                                        anced the incoming student competence across conditions.
Domain & Procedure                                                      There were no statistically significant differences between
The tutoring addressed work-energy problem solving from                 the two conditions on the mathSAT scores nor in the pre-
a first-year college physics course. The eight primary KCs              test scores: t(55) = 0.71, p = .48. On a KC by KC basis,
were: the weight law (KC1 ), definition of work (KC14 ), Def-           no significant difference was found between the two condi-
inition of Kinetic Energy (KC20 ), Gravitational Potential En-          tions across all eight primary KCs except that on KC27, the
ergy (KC21 ), Spring Potential Energy (KC22 ), Total Mechani-           NormGain group score marginally higher than the InvNorm-
cal Energy (KC24 ), Conservation of Total Mechanical Energy             Gain group: t(55) = 1.74, p = 0.088 (see Table 1). In order
(KC27 ), and Change of Total Mechanical Energy (KC28 ).                 to account for varying pretest scores, the adjusted Post-test
   All participants in the study followed the same procedures           scores were compared between the two conditions by run-
and used the same training and testing materials as were used           ning an ANCOVA using the corresponding pre-test score as
when collecting the training corpora. More specifically, the            the covariate.
participants all: completed 1) a background survey; 2) read a              The NormGain condition out-performed the InvNorm-
text covering the target domain knowledge; 3) took a pretest;           Gain on the overall adjusted posttest scores: F(1, 54) =
                                                                    2873

              Table 1: Between-Group Comparison on Pre-Test and Adjusted Post-Test Scores Across Primary KCs
                   KC           TestScore         NormGain         InvNormGain                    Stat                 d
                  KC1      Pretest                0.42 (0.15)        0.39 (0.22)      t(55) = 0.66, p = 0.51         0.16
                           Adjusted Posttest      0.64 (0.12)        0.54 (0.12)      F(1, 54) = 9.80, p = 0.0028    0.85
                  KC14     Pretest                0.43 (0.23)        0.44 (0.25)      t(55) = −0.17, p = 0.86        -0.04
                           Adjusted Posttest      0.65 (0.17)        0.53 (0.17)      F(1, 54) = 6.47, p = 0.014     0.72
                  KC20     Pretest                0.38 (0.17)        0.37 (0.22)      t(55) = 0.31, p = 0.76         0.05
                           Adjusted Posttest      0.67 (0.11)        0.58 (0.11)      F(1, 54) = 10.30, p = 0.002    0.83
                  KC21     Pretest                0.45 (0.20)        0.43 (0.24)      t(55) = 0.35, p = 0.72         0.09
                           Adjusted Posttest      0.75 (0.13)        0.65 (0.13)      F(1, 54) = 7.62, p = 0.008     0.78
                  KC22     Pretest                0.42 (0.25)        0.39 (0.26)      t(55) = 0.41, p = 0.68         0.12
                           Adjusted Posttest      0.63 (0.17)        0.51 (0.17)      F(1, 54) = 7.77, p = 0.007     0.72
                  KC24     Pretest                0.46 (0.15)        0.41 (0.23)      t(55) = 0.89, p = 0.38         0.26
                           Adjusted Posttest      0.64 (0.11)        0.58 (0.11)      F(1, 54) = 4.22, p = 0.045     0.56
                  KC27     Pretest                0.53 (0.21)        0.42 (0.24)      t(55) = 1.74, p = 0.088        0.5
                           Adjusted Posttest      0.74 (0.18)        0.63 (0.18)      F(1, 54) = 5.88, p = 0.019     0.62
                  KC28     Pretest                0.37 (0.20)        0.36 (0.26)      t(55) = 0.13, p = 0.90         0.04
                           Adjusted Posttest      0.53 (0.17)        0.47 (0.17)      F(1, 54) = 1.61, p = 0.21      0.36
10.689, p = .002, d1 = 0.86. On a KC by KC basis, Ta-
ble 1 summarize the comparisons on the pre-test and ad-
justed posttest scores between the two conditions. The third
and fourth columns in Table 1 list the means and SDs σ of
the NormGain and InvNormGain groups’ pretest or adjusted
posttest scores on the corresponding KC. The fifth column
lists the corresponding statistical comparison and the sixth
column lists the Cohen’s d of the comparison. Table 1 shows
that the NormGain condition out-performed the InvNorm-
Gain across all primary KCs (in bold) except for KC28, on
which no significant difference was found between the two
groups.                                                                                 Figure 3: Compare I-ratio Across KCs
I-ratios                                                                   Examining The Three Interaction Hypothesis
                                                                           The monotonic interactivity hypothesis states that more inter-
We next investigated the interactive characteristics of the de-            activity should lead to increased learning. Because the Norm-
rived tutorial tactics by comparing the tutorial dialogues’ I-             Gain group learned more than the InvNormGroup across all
ratios between the two groups. Surprisingly, there were no                 eight KCs except KC28 , which was a null result, the Norm-
significant differences between the two groups on the over-                Gain group should also have a larger I-ratio on all seven KCs.
all I-ratio: t(55) = −0.395, p = 0.694. More specifically, we              From Figure 3, it was shown that this was not the case for
have M = 0.758, SD = 0.073 (maximum is 1) for the Norm-                    KC1 , KC24 and KC27 . Thus, our data are not consistent with
Gain group and M = 0.763, SD = 0.018 for the InvNormGain                   the monotonic interactivity hypothesis.
group respectively.                                                           The interaction plateau hypothesis states that increasing in-
   However, once the results were examined on a KC by                      teractivity yields increasing learning until it hits a plateau,
KC basis there were significant differences between the two                and further increases in interactivity do not cause noticeable
groups on each of the eight primary KCs . Figure 3 shows                   increases in learning. The main difference between this hy-
that the NormGain condition was more likely to get elicits                 pothesis and monotonic interactivity hypothesis is once be-
than the InvNormGain condition on KC14 , KC20 , KC21 , and                 yond a certain level of interactivity whether increasing inter-
KC22 ; and the InvNormGain condition was more likely to get                action would impact students’ learning gain or not. In order
elicits than the NormGain condition on KC1 , KC24 , KC27 , and             to test this hypothesis, we mainly focused on the six KCs (all
KC28 .                                                                     but KC14 and KC28 ). This is because on these six KCs both
                                                                           NormGain and InvNormGain groups’ I-ratios were more than
    1 Cohen’s d, which is defined as the mean learning gain of the ex-     48% (see Figure 3) which is well beyond the threshold of
perimental group minus the mean learning gain of the control group,        the level of interactivity afforded by conventional step-based
divided by the groups’ pooled standard deviation.                          ITSs based on the definition set in (VanLehn, submitted). If
                                                                       2874

the interaction plateau hypothesis is true, then the NormGain                B. du Boulay, & A. C. Graesser (Eds.), Aied (p. 197-204). IOS
group should learn just as much as the InvNormGain group on                  Press.
                                                                          Chi, M. T. H., Roy, M., & Hausmann, R. G. M. (2008). Observing
each of the six KCs. Table 1 however shows that the Norm-                    tutorial dialogues collaboratively: Insights about human tutoring
Gain group learned more than the InvNormGain group across                    effectiveness from vicarious learning. Cognitive Science, 32(2),
all six KCs. Thus, the interaction plateau hypothesis is not                 301-342.
consistent with our data.                                                 Chi, M. T. H., Siler, S., Jeong, H., Yamauchi, T., & Hausmann,
                                                                             R. G. (2001). Learning from human tutoring. Cognitive Science,
   Finally, the tactical interaction hypothesis states that inter-           25, 471-533.
activity does not increase learning unless they are governed              Collins, A., Brown, J. S., & Newman, S. E. (1989). Cognitive
by effective tutorial tactics. If this is true and all our de-               apprenticeship: Teaching the craft of reading, writing and mathe-
                                                                             matics. In L. B. Resnick (Ed.), Knowing, learning and instruction
rived RL-based policies were indeed effective, the NormGain                  (p. 453-494).
group would learn more than the InvNormGain group across                  Evens, M., & Michael, J. (2006). One-on-one tutoring by humans
all KCs. This hypothesis was supported by seven of the KCs,                  and machines. Mahwah, NJ: Erlbaum.
and on KC28 there was only an unreliable trend in the ex-                 Forbes-Riley, K., Litman, D. J., Purandare, A., Rotaru, M., &
                                                                             Tetreault, J. R. (2007). Comparing linguistic features for model-
pected direction. Thus, of all three hypotheses, the tactical in-            ing learning in computer tutoring. In R. Luckin, K. R. Koedinger,
teraction hypothesis receives the most support from our data.                & J. E. Greer (Eds.), Aied (Vol. 158, p. 270-277). IOS Press.
                                                                          Fossati, D., Eugenio, B. D., Brown, C., & Ohlsson., S. (2008).
                             Discussion                                      Learning linked lists: Experiments with the ilist system. In In-
                                                                             telligent tutoring systems (Vol. 5091, p. 80-89). Springer.
Overall, our results inform the ongoing discussion of Socratic            Katz, S., Allbritton, D., & Connelly, J. (2003). Going beyond the
vs. didactic tutoring by suggesting that a tutor’s success is not            problem given: How human tutors use post-solution discussions
governed by how often they prompt or ask the students ques-                  to support transfer. International Journal of AI and Education,
                                                                             13, 79-116.
tions but how well. In particular, the reason human tutors                Koedinger, K. R., & Aleven, V. (2007). Exploring the assistance
so often failed to be more effective than simple, unoptimized                dilemma in experiments with cognitive tutors. Educational Psy-
dialogue-based tutors in those previous studies may be that                  chology Review, 19(3), 239-264.
                                                                          Koedinger, K. R., Anderson, J. R., Hadley, W. H., & Mark, M. A.
effective policies for tutorial interaction are complex and not              (1997). Intelligent tutoring goes to school in the big city. Inter-
easily derived from the tutors’ experience. This in turn sug-                national Journal of AI in Education, 8(1), 30-43.
gests that an optimized dialogue-based tutoring system, such              Moore, J. D., Porayska-Pomsta, K., Varges, S., & Zinn, C. (2004).
as NormGain-Cordillera, would be potentially even more ef-                   Generating tutorial feedback with affect. In V. Barr & Z. Markov
                                                                             (Eds.), Flairs conference. AAAI Press.
fective than expert human tutors. Although controlling for                Newell, A. (1994). Unified theories of cognition. Harvard University
content is difficult when human tutors are involved, testing                 Press; Reprint edition.
this speculative hypothesis would certainly be interesting.               Reif, F., & Scott, L. A. (1999). Teaching scientific thinking skills:
   Finally, this study suggests that instead of using an overall             Students and computers coaching each other. American Journal
                                                                             of Physics, 67(9), 819-831.
tutorial tactics for all KCs, inducing KC-based tutorial tactics          Rose, C. P., Moore, J. D., VanLehn, K., & Allbritton, D. (2001).
seems is necessary in that the induced tutorial tactics seems                A comparative evaluation of socratic versus didactic tutoring. In
generated different tutorial decisions for different KCs in this             Proc. of cognitive sciences society (p. 869-874).
                                                                          Singh, S. P., Kearns, M. J., Litman, D. J., & Walker, M. A.
study. Additionally, our results demonstrate that RL may be                  (1999). Reinforcement learning for spoken dialogue systems. In
fruitfully applied to derive adaptive pedagogical tutorial tac-              S. A. Solla, T. K. Leen, & K.-R. Müller (Eds.), Nips (p. 956-962).
tics from student-computer interactivity data. However, this                 The MIT Press.
technique is not yet well understood. It is not completely                Singh, S. P., Litman, D. J., Kearns, M. J., & Walker, M. A. (2002).
                                                                             Optimizing dialogue management with reinforcement learning:
clear to us, for instance, why our first attempt at inducing                 Experiments with the njfun system. J. Artif. Intell. Res. (JAIR),
policies was suboptimal. In future work, we plan to explore                  16, 105-133.
the use of richer POMDP models, and do additional empirical               Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning. MIT
                                                                             Press Bradford Books.
evaluation of the RL approach.                                            Tetreault, J. R., & Litman, D. J. (2008). A reinforcement learning
   Acknowledgments NSF (#0325054) supported this work                        approach to evaluating state representations in spoken dialogue
and NSF (#SBE-0836012) supported its publication. We                         systems. Speech Communication, 50(8-9), 683-696.
thank LRDC for providing all the facilities for this work.                VanLehn, K. (2006). The behavior of tutoring systems. Interna-
                                                                             tional Journal AI in Education, 16(3), 227-265.
                                                                          VanLehn, K. (submitted). The two-sigma effect revisited: A meta-
                             References                                      analysis of human tutoring and several types of computer tutor-
Aleven, V., Ogan, A., Popescu, O., Torrey, C., & Koedinger, K. R.            ing.
   (2004). Evaluating the effectiveness of a tutorial dialogue system     VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney, A.,
   for self-explanation. In Intelligent tutoring systems (Vol. 3220,         & Rose, C. P. (2007). When are tutorial dialogues more effective
   p. 443-454). Springer.                                                    than reading? Cognitive Science, 31(1), 3-62.
Anderson, J. R. (1983). The architecture of cognition. Cambridge,         VanLehn, K., Jordan, P., & Litman, D. (2007). Developing ped-
   Mass. : Harvard University Press.                                         agogically effective tutorial dialogue tactics: Experiments and a
Bloom, B. S. (1984). The 2 sigma problem: The search for methods             testbed. In Proc. of slate workshop on speech and language tech-
   of group instruction as effective as one-to-one tutoring. Educa-          nology in education isca tutorial and research workshop.
   tional Researcher, 13, 4-16.                                           Vygotsky, L. (1971). Interaction between learning and develop-
Chi, M., Jordan, P. W., VanLehn, K., & Litman, D. J. (2009). To              ment. In T. M. Cole (Ed.), In mind in society. (p. 79-91). Harvard
   elicit or to tell: Does it matter? In V. Dimitrova, R. Mizoguchi,         University Press: Cambridge Massachusetts.
                                                                      2875

