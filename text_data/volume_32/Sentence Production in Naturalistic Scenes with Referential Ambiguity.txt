UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Sentence Production in Naturalistic Scenes with Referential Ambiguity
Permalink
https://escholarship.org/uc/item/0jc9s5hk
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Coco, Moreno I.
Keller, Frank
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

           Sentence Production in Naturalistic Scenes with Referential Ambiguity
                                              Moreno I. Coco (M.I.Coco@sms.ed.ac.uk) and
                                                     Frank Keller (keller@inf.ed.ac.uk)
                                                School of Informatics, University of Edinburgh
                                                 10 Crichton Street, Edinburgh EH8 9AB, UK
                               Abstract                                   visual context that consists of naturalistic scenes (rather than
                                                                          line drawings) and in which multiple objects can correspond
   Language production often happens in a visual context, for ex-
   ample when a speaker describes a picture. This raises the ques-        to a given linguistic referent (in contrast to Griffin and Bock
   tion whether visual factors interact with conceptual factors dur-      2000). This enables us to study how scene complexity and
   ing linguistic encoding. To address this question, we present an       referential ambiguity affect the eye-voice span. Furthermore,
   eye-tracking experiment that manipulates visual clutter (den-
   sity of objects in the scene) and animacy in a sentence produc-        we are interested in the interaction of visual and conceptual
   tion task using naturalistic, referentially ambiguous scenes. We       factors during linguistic encoding. The visual factor we fo-
   found that clutter leads to more fixations on target objects be-       cus on is clutter (density of objects in the scene); clutter
   fore they are mentioned, contrary to results for visual search,
   and that this effect is modulated by animacy. We also tested           has been investigated in the visual processing literature and
   the eye-voice span hypothesis (objects are fixated before they         found to affect visual search (Henderson et al., 2009). The
   are mentioned), and found that a significantly more complex            conceptual factor we investigate is the animacy of the ref-
   pattern obtains in naturalistic, referentially ambiguous scenes.       erent; animacy has been manipulated in the psycholinguistic
   Keywords: language production; eye-tracking; naturalistic              literature and found to affect sentence production (Branigan
   scenes; eye-voice span; referential ambiguity.
                                                                          et al., 2008). Here, we address the question whether these two
                                                                          factors representing different modalities contribute indepen-
                           Introduction                                   dently to the formation of reference in sentence production,
Language production often happens in a visual context, for                or whether they interact.
example when the speaker describes a picture, gives direc-
tions on a map, or explains the function of an artifact. In these                                 Background
situations, the speaker needs to select which objects to talk
about, and in which order. He/she also needs to disambiguate              The recent visual cognition literature has emphasized the im-
the utterance referentially. For instance, if there are multiple          portance of contextual information for visual processing. For
clipboards in the visual context, then the speaker has to en-             example, prior information about object categories facilitates
code additional visual information to pick out one of them                visual search (Malcolm and Henderson, 2009; Schmidt and
uniquely (e.g., the brown clipboard or the clipboard on the               Zelinksy, 2009). This effect occurs if participants are asked
table).                                                                   to look for an object embedded in a scene or an object ar-
   Most work in psycholinguistics has dealt with isolated sen-            ray (Brockmole and Henderson, 2006), or if categorical tem-
tences, but there is some existing research investigating how             plates are provided which the visual system can use to deter-
language is processed in a visual context. A prominent line of            mine where the target object is located (Vo and Henderson,
research employs the visual world paradigm (VWP; Tanen-                   2010). It seems likely that similar contextual guidance effects
haus et al. 1995; Altmann and Kamide 1999) for this pur-                  (Torralba et al., 2006) also occur if the context is provided by
pose. In a typical VWP study, participants’ eye-movements                 another modality, e.g., by the linguistic material involved in a
are recorded while they view a visual scene and listen to a               language production task.
sentence at the same time. Some VWP experiments have in-                     In such task, speakers will often be faced with referen-
vestigated language production; the most well-known exam-                 tial ambiguity, which they resolve by including disambiguat-
ple is Griffin and Bock’s (2000) study, in which participants             ing material in a sentence. For example, spatial prepositions
were asked to describe line drawings depicting two objects                can be used to locate an object in relation to the surround-
(e.g., a turtle and a kangaroo) performing a transitive event             ing space, e.g., the clipboard on the table or adjectives can
(e.g., splashing). The key finding of this study was that speak-          be used to contrast the intended referent with a competitor,
ers fixate visual referents in the order in which they are men-           e.g., the brown clipboard. Before any linguistic encoding can
tioned, and they begin fixating an object about 900 ms be-                take place, however, the disambiguation has to happen at the
fore naming it. The span between fixating and naming a ref-               visual level. When a target object is selected as a referent (be-
erent is known as the eye-voice span; other studies (e.g., Qu             cause it will be mentioned in a sentence), the visual system
and Chai 2008) have reported eye-voice spans consistent with              has to retrieve scene and object information that can be used
those found by Griffin and Bock (2000).                                   to refer to the object unambiguously. One can therefore hy-
   The aim of the present paper is to establish whether the               pothesize that if participants are faced with a linguistic task
simple relationship between language production and eye-                  (e.g., scene description), then contextual guidance is afforded
movements implied by the eye-voice span extends to more                   not only by visual information, but also driven by linguistic
realistic situations. We investigate language production in a             processing and the need to disambiguate.
                                                                      1070

                          Experiment
In this experiment, we investigated how visual attention is
influenced by contextual factors during sentence produc-
tion. Participants had to describe a visual scene after being
prompted with a cue word. This cue word was ambiguous,
i.e., two objects in the scene could be referred to by the cue.
We manipulated the animacy of the cue (e.g., man vs. clip-
board), expecting an effect on both linguistic encoding and
visual attention. Animate objects are associated with a larger
number of conceptual structures in encoding (Branigan et al.,
2008); we should therefore observe more sentences contain-
ing action information in this case (e.g., the man is reading
a letter). At the same time, we expect visual attention to be
localized on animate targets, an effect that has already been
demonstrated in visual search (Fletcher-Watson et al., 2008).
   The second experimental manipulation concerned a visual           Figure 1: Example of an experimental trial, with visual region of in-
factor, viz., clutter, defined as the density of visual informa-     terest considered for analysis. P RIMARY indicates that the A NIMATE
                                                                     and I NANIMATE visual objects are spatially close and semantically
tion (Rosenholtz et al., 2007). Again, this is a factor that         connected (e.g., the MAN is doing an action using the CLIPBOARD).
has shown effects on the performance and accuracy of visual          S ECONDARY is used to indicate the remaining referent of the am-
search: the more cluttered the scene is, the less efficient the      biguous pair. BACKGROUND and C LUTTER are defined in opposi-
                                                                     tion: BACKGROUND is everything other than C LUTTER.
identification of target object (Henderson et al., 2009). In a
language production task, however, the effect of clutter can
be expected to change, due to the disambiguation strategies
required. Clutter could have a beneficial effect: the more vi-          An EyeLink II head-mounted eye-tracker was used to mon-
sual information there is, the more disambiguating material          itor participants’ eye-movements with a sampling rate of
can be retrieved; clutter could therefore facilitate language        500 Hz. Images were presented on a 21” multiscan moni-
production.                                                          tor at a resolution of 1024 x 768 pixels; participants’ speech
   Finally, this experiment makes it possible to investigate the     was recorded with a lapel microphone. Only the dominant
effect of referential ambiguity on the eye-voice span. In pre-       eye was tracked. A cue word appeared for 750 ms at the cen-
vious work, the relationship between linguistic and visual ref-      ter of the screen, after which the scene followed and sound
erents was unambiguous: looks to the visual referent always          recording was activated. Drift correction was performed at
preceded naming (Griffin and Bock, 2000) and this trend              the beginning and between each trial. There was no time limit
exponentially increases towards the mention (Qu and Chai,            for the trial duration and to pass to the next trail participants
2008). In our setting, we expect a more complex gaze-to-             pressed a button on the response pad. The experimental task
name relationship caused by a process of visual disambigua-          was explained using written instructions and took approxi-
tion that arises both before and after the intended referent is      mately 30 minutes to complete.
mentioned.
                                                                     Data Analysis
Method                                                               We defined regions of interest (ROIs) both for the visual
We used a factorial design that crossed the two factors Clut-        and the linguistic data. The visual data was aggregated into
ter (Minimal/Cluttered) and Cue (Animate/Inanimate). Par-            six different regions: P RIMARY and S ECONDARY A NIMATE,
ticipants’ eye-movements were recorded while they described          P RIMARY and S ECONDARY I NANIMATE, BACKGROUND,
photo-realistic scenes after being prompted with a cue word,         and C LUTTER (see Figure 1).
which ambiguously corresponded to two visual referents in               For the linguistic data, we made a general division between
the scene (see Figure 1).                                            time windows Before and During production. This allows us
   We created 24 experimental items using photo-realistic            to capture the overall trend of the two main phases of a trial.
scenes drawn from six indoor scenarios (e.g., Bathroom, Bed-         For the analysis of eye-voice span, we consider a window of
room; four scenes per scenario). In each scene, we inserted          2000 ms before the referent was mentioned, similar to Qu
two animate and two inanimate objects using Photoshop,               and Chai 2008. The resolution of visual ambiguity is ana-
which correspond to the two Cue conditions; Clutter was ei-          lyzed using a window of 1600 ms (divided into 40 time slices
ther added or removed.                                               40 ms each): 800 ms before and after the mention of Cue.
   Twenty-four native speakers of English, all students of the       This makes it possible to explore how the linguistic referent
University of Edinburgh, were each paid five pounds for tak-         is visually located before being mentioned and just after.
ing part in the experiment. They each saw 24 items random-              In order to unambiguously analyze fixated and named ref-
ized and distributed in a Latin square design that made sure         erents, we aggregate eye-movements responses in four blocks
that each participant only saw one condition per scene.              (Primary, Secondary, Ambiguous and Both) by manually
                                                                 1071

checking which referent was mentioned in each sentence.1
We introduced referential ambiguity as predictor in the infer-
ential model described below to investigate how looks to the
mentioned object differ from those to its competitor. For rea-
son of space, we only present the analysis for the Primary
objects mentioned. The effect of mention on eye-movements’
pattern is evaluated by comparing Primary with Secondary
objects.
    As an initial exploration of our data, we investigate the
overall trend of fixations Before and During production. Pro-
duction is a task with large between-participant variability,
e.g., one participant will spend 2000 ms Before and 1000 ms
During production, whereas another one will show the op-
posite pattern. Normalizing the production data is therefore
crucial, in particular as we want to interpret eye-movements
in relation to phases of linguistic processing. We normalize
each sequence Sold i   of eye-movements by mapping it onto a
normalized time-course of fixed length Snew      i . The length of
                                                                           Figure 2: Normalized proportions of looks (60 bins) across the four
  i
Snew is set on the basis of the shortest eye-movement sequence             conditions, Before and During production, for the different visual
                i )] found between Before and During produc-
mini [length(Sold                                                          ROIs. The purple dashed vertical line indicates Before (to the left)
                                                                           and During (to the right) production. The four conditions are coded
tion, across all participants.2 For each sequence Sold     i , we ob-
                                                                           as following: Animate/Cluttered: red, full-square; Animate/Minimal:
                                          i
tain the number of old time-points k corresponding to a new                red, empty square); Inanimate/Cluttered: blue, full circle; Inani-
time-unit u, as ki = length(Sold   i )/length(Si ). Proportions            mate/Minimal: blue, empty circle
                                                   new
are then calculated over ki old time-points and subsequently
mapped into the corresponding unit u of the normalized time-
course. In the Results section, we show plots of normalized                test comparing models each time a new parameter is included.
proportions for Primary and Secondary (Animate and Inani-                  If the fit improves, we accept the new model, otherwise we
mate) across conditions, Before and During production.                     keep the old one. We include predictors, random intercepts
    To explore the eye-voice span hypothesis, we compute the               and slopes ordered by their log-likelihood impact on model
number of fixations to the mentioned object compared to the                fit. We iterate until there is no more improvement on the fit;
competitor. We also look at latencies, i.e., the onset of the last         leaving us with the best model. In the result section, we show
fixation to the referent or competitor before the mention, and             plots of the values predicted by the model for each condition.
gaze duration as a function of latencies, i.e., the time spent
looking at the referent or competitor for the different laten-
                                                                           Results and Discussion
cies.                                                                      Before and During Production We first look at how fixa-
    We also report inferential statistics for the referent region          tions are distributed when we collapse the two main phases of
(for the time windows previously described). The dependent                 the experiment: Before and During production. This analysis
measure is the empirical logit (Barr, 2008), calculated as                 does not distinguish whether the Primary or Secondary ref-
                 0.5+φ                                                     erent was mentioned. Figure 2 shows normalized proportions
emplog = ln 0.5+(1−φ)    , where φ is the number of fixations on
the region of interest. The analysis is performed using the                of looks on the competitor visual objects corresponding to the
framework of linear-mixed effect (LME) models as imple-                    Cue (Animate/Inanimate).
mented by the R-package lme4 (Baayen et al., 2008). The                        The first thing to note is that for the visual ROI correspond-
predictors included were Animacy, Clutter, Time and Object.                ing to the Primary referent, the pattern of fixations is more
The random factors were Participant and Item. To reduce co-                complex than for the ROI of the Secondary referent. The spa-
linearity, factors were centered.                                          tial proximity and semantic relatedness of the two Primary
    The model selection followed a conservative stepwise for-              referents result in a more complex pattern of interaction. The
ward procedure that tests model fit based on a log-likelihood              clearest effect is found in relation with the animacy of Cue;
                                                                           we observe more fixations to the animate referent when the
    1 P RIMARY means that the Primary Animate or Inanimate is men-         cue is also animate. When looking at the Primary ROI, the
tioned (e.g., The man is writing on the clipboard). S ECONDARY             effect is seen at the beginning of both the Before and the Dur-
is used when the Secondary Animate or Inanimate is mentioned               ing region. At the beginning of the trial, the visual system
(e.g., The man is reading a letter). A MBIGUOUS is used when is un-
clear which one is referred to (e.g., the man is sitting on the couch).    retrieves information about the cued objects; when produc-
B OTH indicates that both referents are mentioned (e.g., the man is        tion starts, the referents are fixated again, probably before be-
writing on a clipboard while the other man reads a newspaper).             ing mentioned. For the Secondary ROIs, the relation with the
    2 We remove outliers that are two standard deviation away from
                                                                           Cue is stronger, probably reinforced by the referential com-
the mean, after having log-transformed our data. The data are not
normally distributed, due to right skewness. The log-transformation        petition. Moreover, the pattern of looks is much clearer than
helps us to reduce the skew.                                               for the Primary ROI. This confirms that spatial proximity and
                                                                       1072

Table 1: Eye-voice span statistics. Excluding indicates that the per-
centage is calculated considering only those cases in which either
the referent or competitor have been fixated, Including takes into
account also cases where both have been fixated.
  Measure                              Referent    Competitor
  Percentage of looks    Including       71.65        43.30
                         Excluding       36.44         8.09
  Mean Latency           Including     1032 ms      1203 ms
                         Excluding     1012 ms      1325 ms
  Gaze Duration          Including      489 ms       432 ms
                         Excluding      568 ms       623 ms
semantic relatedness increase the interaction between visual
referents. Clutter does not have a strong effect, though there           (a) Frequencies of latencies at different temporal blocks (from two
is a small increase of looks when the scene is minimal and the           seconds to mention): red is the referent, blue the competitor. The
animacy of the target matches that of the cue.                           latency measures the time elapsed from the beginning of the last fix-
                                                                         ation to the object (referent or competitor) until is mentioned.
Eye-Voice Span We analyzed eye-voice span to investigate
the gaze-to-name relation for the mentioned referent and its
competitor. Table 1 shows percentages of looks to referent or
competitor with mean latencies and gaze durations.3
   There is a preference for looks to the referent over looks to
the competitor, with a latency of about one second, confirm-
ing previous findings (Griffin and Bock, 2000). In a minor-
ity of cases, participants only look at the referent (36.44%);
competition between the two ambiguous visual referents is
the norm (71.65%). Moreover, we notice that the competitor
is fixated earlier than the referent and the duration is shorter
for the Including condition (which includes trials in which
both referents have been fixated). This may indicate that the
final decision on which referent is mentioned is made after              (b) Mean gaze duration as a function of latency. The mean of gaze
discarding the competitor.                                               duration is calculated for the different blocks of latencies. We analyze
                                                                         only cases where gaze duration is shorter than latency, thus avoiding
   Figure 3(a) shows frequencies of Latencies at different               cases where fixations spill over into the region after mention.
temporal blocks (200 ms each) within a total window of two
seconds. We find that latency frequency decreases towards the                             Figure 3: Eye Voice Span statistics.
mention for both the referent and the competitor. This finding
contrasts with Qu and Chai (2008) who found the opposite
trend, i.e., the closer to the mention, the more gazes are as-           Inferential Analysis We now analyze the pattern of eye-
sociated with the referent object. Note also that this effect            movements before and after the mention of the cue word. To
cannot only be due to the presence of a competitor, e.g., com-           save space, we focus on the case where the Primary visual
parative looks before mention, as these present a similar de-            object is mentioned. Based on the eye-voice span analysis, we
creasing trend.                                                          expect to find a decreasing trend of looks before the referent
   In Figure 3(b) we show mean gaze duration as a function               is mentioned, and the presence of competition should weaken
of the different latencies. Again, a decreasing trend is clearly         the gaze-to-name relationship.
visible: the closer the latency to the mention, the shorter the             Recall that our experiment had two factors (Cue: ani-
gaze duration. Interestingly there is a peak of gaze duration            mate/inanimate; Clutter: minimal/cluttered); we also include
at 1600/1400 ms. The higher duration found at this latency               the object fixated (Object: primary/secondary) and Time (in
might be an indicator of referential selection (gaze-to-name             40 ms slices, see Data Analysis above) in the analysis. Fig-
binding). We also find evidence of competition at 600/400 ms,            ure 4 plots LME predicted values for the four conditions, Be-
where the competitor receives longer gazes compared to ref-              fore and After mention.4
erent. A last visual check on the competitor is probably per-               Beginning with the animate visual objects in Figure 4, we
formed before referentiality is encoded linguistically.                  expect the Primary Animate to receive more looks than the
                                                                         Secondary Animate, and the number of looks should increase.
    3 The measures are calculated only when the Primary and Sec-         We observe a preference for looks to Primary Animate,
ondary referent are mentioned; thus, we exclude the Both and Am-
biguous cases, for which it was not possible to establish unambigu-          4 The intercepts for Before and During are different because they
ous eye-voice span relation.                                             are calculated over distinct time intervals.
                                                                     1073

                                                                            ent information when scenes are minimal. In contrasts with
                                                                            previous findings, we observe increasing looks to the refer-
                                                                            ent after mention (βPrimary:Time = 0.0530; p < 0.001). This ef-
                                                                            fect could be due to referential ambiguity: the visual system
                                                                            is connecting disambiguating material retrieved before men-
                                                                            tion to the referent just uttered. For the Secondary Animate,
                                                                            we find an increasing trend of looks when Cue is Inanimate
                                                                            and especially for minimal scenes (βInanimate:Time = 0.056, p <
                                                                            0.001; βMinimal:Time = 0.041, p < 0.01). The minimality of the
                                                                            scene gives prominence to animate referents; probably the
                                                                            spatial and semantic proximity of one of Primary Inanimate
                                                                            and the Primary Animate also trigger comparative looks to
                                                                            Secondary Animate, i.e., participants check whether it can
                                                                            also be contextually related to the cue.
                                                                               After the referent is mentioned (Primary in this case),
                                                                            looks to the Secondary Animate decrease over time in all
                                                                            conditions. Competition is triggered by visual ambiguity, but
                                                                            once the association of the visual with the linguistic refer-
                                                                            ent has been established (i.e., after the mention), participants
                                                                            look back to the referent mentioned, presumably finalizing
                                                                            the choice made.
                                                                               Looking at inanimate referents in Figure 4, we observe
                                                                            a statistically significant preference for looks to the Pri-
                                                                            mary Inanimate (βPrimary = 0.0621; p < 0.05). This prefer-
                                                                            ence could be due to the spatial proximity and the seman-
                                                                            tic relation with the primary animate, which makes the pri-
                                                                            mary inanimate more likely to be encoded either as a di-
                                                                            rect object or as subject of the description. As a conse-
Figure 4: Linear mixed effect model: plot of predicted values (40           quence, we find an interaction with the animacy of the Cue
windows of 40 ms each) across the four conditions, Before and After         (βAnimate:Primary = 0.0155; p < 0.05) but not a main effect
referent, on the different visual ROIs. The First referent is mentioned
and the dashed line indicates when.                                         (βInanimate = 0.017; p > 0.1). In contrast with standard vi-
                                                                            sual search task, where performance degrades as a func-
                                                                            tion of clutter, here we observe instead a positive interaction
but the difference is not statistically significant (βPrimary =             of Clutter and Cue on the target (βInanimate:Cluttered:Primary =
0.0255; p > 0.1). However, we find a main effect of Cue                     0.028, p < 0.001), which increase over time (βCluttered:Time =
(βAnimate = 0.0543; p < 0.01): an animate cue facilitates                   0.054, p < 0.01). The visual system is not performing a
looks to Animate visual objects. When looking at the time                   search task, rather it is sourcing information to ground lan-
course, we find a general decreasing trend (βPrimary:Time =                 guage processing. In a cluttered scene, an inanimate refer-
−0.022; p < 0.01), partly compensated by a three-way inter-                 ent could be spatially related to many other different objects,
action of Animacy, Object, and Time (βAnimate:Primary:Time =                whereas a minimal scene has fewer points to anchor the ref-
0.049; p < 0.001). Moreover, we observe a two-way interac-                  erent. The visual system therefore needs to select among the
tion of Clutter and Time (βMinimal:Time = 0.024; p < 0.01): a               different spatial relations to find one that optimally situates
minimal scene makes it difficult to retrieve disambiguating                 the object within the contextual information.
information for the animate referent, forcing the visual sys-                  For the secondary inanimate, there is a negative relation-
tem to look for this information on the referent itself. It is              ship between the animacy of Cue and the minimality of Clut-
also conceivable that the minimality of the scene makes vi-                 ter (βAnimate:Minimal:Secondary = −0.0719; p < 0.001); the prox-
sual responses similar to those found for line drawings (Grif-              imity and relatedness of the primary inanimate and the pri-
fin and Bock, 2000); thereby explaining the increasing trend.               mary animate is highlighted when visual information is min-
In a cluttered environment, instead, there are more ways to                 imal, which results in the secondary inanimate being fixated
relate the referent to the surrounding context, hence helping               less.
language production to disambiguate. This explains the de-
creasing trend of fixations on the referent in the cluttered con-                               General Discussion
dition.                                                                     Referential ambiguity is a common phenomenon in everyday
   After mention, we observe interactions of Cue with                       experience. In a naturalistic scene, the same object (e.g., a
Clutter (βAnimate:Minimal = 0.0165; p < 0.001) and Object                   clipboard) can occur multiple times (e.g., on a desk or on
(βAnimate:Primary = 0.017; p < 0.01), confirming both the fa-               a counter). This fact turns into linguistic ambiguity when a
cilitation of the cued referent and the preference for refer-               referent has to be selected from the set of visual competi-
                                                                        1074

tors. Typically, referential ambiguity is resolved by encoding          referents are fixated in the order in which they are mentioned,
sufficient contextual information to discriminate the intended          with a fixed eye-voice span between fixation and mention,
referent from competitors (e.g., the clipboard on the desk).            does not seem to generalize to more realistic settings in which
However, this process of ambiguity resolution cannot be ex-             speakers describe naturalistic scenes that involve referential
plained by linguistic factors alone, especially given that the          ambiguity.
disambiguating material needs to be selected by the visual
system prior to any encoding. We therefore hypothesized that                                      References
visual factors interact with well-established conceptual fac-           Altmann, G. and Kamide, Y. (1999). Incremental interpre-
tors active during language production.                                    tation at verbs: restricting the domain of subsequent refer-
    We reported the results of an eye-tracking language scene              ence. Cognition, 73(3):247–264.
description experiment that support this hypothesis. We ex-             Baayen, R., Davidson, D., and Bates, D. (2008). Mixed-
plored how the conceptual properties of the target referent                effects modeling with crossed random effects for subjects
(factor Cue: animate/inanimate) and the density of visual in-              and items. Journal of memory and language, 59:390–412.
formation (factor Clutter: minimal/cluttered) interact during           Barr, D. (2008). Analyzing ’visual world’ eyetracking data
the resolution of referential ambiguity. The results showed                using multilevel logistic regression. Journal of memory
that the animacy of the cue facilitates looks to animate ob-               and language, 59(4):457–474.
jects, especially at the beginning of two main phases of lin-           Branigan, H., Pickering, M., and Tanaka, M. (2008). Contri-
guistic production: before and during the mention of the ref-              bution of animacy to grammatical function assignment and
erent. The data indicate that a visual search is performed to lo-          word order during production. Lingua, 2(118):172–189.
calize the objects matching the cue word (Malcolm and Hen-              Brockmole, J. R. and Henderson, J. M. (2006). Using real-
derson, 2009). Our results also contrasted interestingly with              world scenes as contextual cues for search. Visual Cogni-
findings for visual search, where clutter decreases search per-            tion, 13:99–108.
formance (Henderson et al., 2009). In cases in which an an-             Fletcher-Watson, S., Findlay, J., Leekam, S., and Benson, V.
imate referent is mentioned, we found that there were fewer                (2008). Rapid detection of person information in a natural-
fixations to the target object in the cluttered condition com-             istic scene. Perception, 37(4):571–583.
pared to the uncluttered one. In other words, clutter makes             Griffin, Z. and Bock, K. (2000). What the eyes say about
language production easier, not harder: the visual system is               speaking. Psychological science, 11:274–279.
not just searching for the target object, but it is also retrieving     Henderson, J. M., Chanceaux, M., and Smith, T. J. (2009).
visual information that can be used to linguistically anchor it            The influence of clutter on real-world scene search: Evi-
(e.g., for disambiguation). The more clutter there is, the eas-            dence from search efficiency and eye movements. Journal
ier this process becomes, explaining the reduced number of                 of Vision, 9(1)(32):1–8.
fixations in the cluttered condition.                                   Malcolm, G. and Henderson, J. M. (2009). The effects of
    Turning at the relation between fixating and naming an ob-             target template specificity on visual search in real-world
ject (the eye-voice span), previous work found that referents              scenes: Evidence from eye movements. Journal of Vision,
are fixed shortly before being mentioned (Griffin and Bock,                9(11)(8):1–13.
2000). It has also been observed that fixation probability in-          Qu, S. and Chai, J. (2008). Incorporating temporal and se-
creases with decreasing distance to the mention (Qu and Chai,              mantic information with eye gaze for automatic word ac-
2008). In our data, we found a numerical preference for looks              quisition in multimodal conversational systems. In Pro-
to the mentioned referent over looks to the competitor, but                ceedings of the 2008 Conference on Empirical Methods in
this preference was not confirmed in the inferential analysis              Natural Language Processing (EMNLP). Honolulu.
(see Figure 4). Only if the primary inanimate was mentioned,            Rosenholtz, R., Li, Y., and Nakano, L. (2007). Measuring
it was fixated significantly more than the secondary inani-                visual clutter. Journal of Vision, 7:1–22.
mate. This preference is likely due to the proximity, spatial           Schmidt, J. and Zelinksy, G. (2009). Search guidance
and semantic, between the primary animate and inanimate.                   is proportional to the categorical specificity of a tar-
Moreover, we found that fixation probability decreased with                get cue. Quarterly Journal of Experimental Psychology,
decreasing distance to the mention, contrary to previous re-               62(10):1904–1914.
sults, in particular when the scene was cluttered. The compe-           Tanenhaus, M., Spivey-Knowlton, J., Eberhard, K., and Se-
tition between visual referents seems to override the standard             divy, J. (1995). Integration of visual and linguistic in-
eye-voice span effect. Interestingly, we also observed an in-              formation in spoken language comprehension. Science,
creasing trend of fixation to the referent object after its men-           (268):632–634.
tion. Once production has started, the visual system needs              Torralba, A., Oliva, A., Castelhano, M., and Henderson, J.
to retrieve contextual information to produce disambiguating               (2006). Contextual guidance of eye movements and at-
linguistic material, resulting in an increase in the number of             tention in real-world scenes: the role of global features in
looks after mention.                                                       object search. Psychological review, 4(113):766–786.
    Taken together, our results indicate that visual factors such       Vo, M. and Henderson, J. (2010). The time course of ini-
as clutter interact with conceptual factors such as animacy                tial scene processing for eye movement guidance in natural
in language production. The simple view according to which                 scene search. Journal of Vision, 10(3):1–13.
                                                                    1075

