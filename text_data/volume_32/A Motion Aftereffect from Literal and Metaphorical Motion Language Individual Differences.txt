UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Motion Aftereffect from Literal and Metaphorical Motion Language: Individual Differences

Permalink
https://escholarship.org/uc/item/3wk172jv

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Dils, Alexia Toskos
Boroditsky, Lera

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

A Motion Aftereffect from Literal and Metaphorical Motion Language:
Individual Differences
Alexia Toskos Dils (atoskos@stanford.edu)
Lera Boroditsky (lera@stanford.edu)
Stanford University, Department of Psychology
Jordan Hall, 450 Serra Mall, Building 420, Stanford, CA 94305 USA

Abstract

comprehension and that processing language affects
performance in subsequent perceptual tasks (e.g., Bergen,
Lindsay, Matlock, & Narayanan, 2007; Meteyard, Bahrami,
& Vigliocco, 2007; Richardson, Spivey, Barsalou, &
McRae, 2003; Rinck & Bower, 2000; Rinck, Hähnel,
Bower, & Glowalla, 1997; Spivey & Geng, 2001; Stanfield
& Zwaan, 2001; Zwaan, Madden, Yaxley, & Aveyard,
2004; Zwaan, Stanfield, & Yaxley, 2002;).
What mechanism might underlie these interactions
between linguistic processing and perception?
The
explanation frequently offered is that the representations
generated during the course of language comprehension
share processing resources with perception, recruiting some
of the very same brain regions (Barsalou, 1999). As
evidence for this possibility fMRI measures have revealed
that classically ‘perceptual’ brain areas are recruited in
service of language comprehension (e.g., Saygin,
McCullough, Alac, & Emmorey, 2010). While these
findings are consistent with the hypothesis, questions
remain. The spatial resolution of current fMRI technology
is coarse.
A typical voxel (the smallest unit of
measurement) may include 100,000 neurons. It is possible
then that what appear in fMRI to be the same regions
activated in linguistic and visual tasks are in fact
neighboring (or closely interleaved) but distinct neural
populations, potentially with quite different computational
properties.
One powerful paradigm for determining whether neural
populations involved in particular tasks indeed overlap is
that of adaptation. In this paper, we make use of one such
adaptation measure, the motion aftereffect (MAE). The
MAE arises when direction-selective neurons in the human
MT+ complex lower their firing rate as a function of
adapting to motion in their preferred direction. The net
difference in the firing rate of neurons selective for the
direction of the adapting stimulus relative to those selective
for the opposite direction of motion produces a motion
illusion. For example, after adapting to upward motion,
people are more likely to see a stationary stimulus or a field
of randomly moving dots as moving downward, and vice
versa (e.g., Blake & Hiris, 1993). To quantify the size of the
aftereffect, one can parametrically vary the degree of motion
coherence in the test display of moving dots (as in Blake &
Hiris, 1993). The amount of coherence necessary to null the
MAE (i.e. to make people equally likely to report the
motion as upward or downward) provides a nice measure of
the size of the aftereffect produced by the adapting stimulus.

Do people spontaneously form visual mental images when
understanding language, and if so how truly visual are these
representations?
We test whether processing linguistic
descriptions of motion produces sufficiently vivid mental
images to cause direction-selective motion adaptation in the
visual system (i.e., cause a motion aftereffect illusion). We
tested for motion aftereffects (MAEs) following explicit
motion imagery, and after processing literal or metaphorical
motion language. Intentionally imagining motion produces an
aftereffect in the overall sample with some participants
showing a greater aftereffect than others. We then find that
participants who show the strongest imagined motion
aftereffects also show aftereffects in the natural course of
processing motion language (without instructions to imagine).
Individuals who do not show strong motion aftereffects as a
result of imagining motion also do not show them from
processing motion language. However, the aftereffect from
language gained strength as people were exposed to more and
more of a motion story. For the last two story installments
(out of 4), understanding motion language produced reliable
MAEs across the entire sample. The results demonstrate that
processing language can spontaneously create sufficiently
vivid mental images to produce direction-selective adaptation
in the visual system. The timecourse of adaptation suggests
that individuals may differ in how efficiently they recruit
visual mechanisms in the service of language understanding.
Further, the results reveal an intriguing link between the
vividness of mental imagery and the nature of the processes
and representations involved in language understanding.
Keywords:
embodiment,
language
comprehension,
perception, motion aftereffect, individual differences

Introduction
A good story can draw you in, conjure up a rich visual
world, give you goose-bumps, or even make you feel like
you were really there. To what extent is hearing a story
about something similar to really witnessing it? What is the
nature of the representations that arise in the course of
normal language processing? Do people spontaneously
form visual mental images when understanding language,
and if so how truly visual are these representations? In this
paper we make use of the motion aftereffect illusion to test
whether processing linguistic descriptions of motion
produces sufficiently vivid mental images to cause
direction-selective adaptation in the visual system (i.e.,
cause a motion aftereffect).
A number of findings suggest that people do
spontaneously engage in imagery during language

895

Winawer, Huk, and Boroditsky (2008, 2010) adapted this
technique to test for MAEs after participants either viewed
still images implying motion (e.g., a runner in mid-leap), or
simply imagined motion without any visual stimulus. Both
implied and purely imagined motion produced reliable
MAEs. These studies support fMRI findings suggesting the
hMT+ complex is recruited in the service of mental imagery
(Goebel, Khorram-Sefat, Muckli, Hacker, & Singer, 1998;
Grossman & Blake, 2001), and further suggest that this
activation is driven by direction-selective neurons.
Here we explore whether natural language comprehension
can likewise produce MAEs. To the extent that people
spontaneously engage in imagery in service of language
comprehension, understanding motion language should
yield MAEs (albeit likely weaker than those produced
during explicit, effortful imagery). The present study was
designed to test this prediction. Participants listened to
stories describing motion in a particular direction and then
judged the direction of a moving field of dots. The direction
in which motion language affects subsequent motion
perception speaks to the mechanisms underlying language
comprehension. One possibility is that motion language
adapts the same direction-selective mechanisms that
subserve motion perception; this would cause people to see
a real visual stimulus (e.g., dynamic dots) as moving in a
direction opposite to that described in the adapting
language. Another possibility is that understanding motion
language recruits higher-level convergence areas that
process visual motion, resulting in a bias to see dot motion
in the same direction. Such a congruence effect is reported
by Sadaghiani et al (2009) who showed that hearing the
words ‘right’ and ‘left’ biased participants to see an
apparent motion stimulus as moving in the same direction.
fMRI data revealed that this audiovisual interaction was
driven more by activity in the anterior intraparietal sulcus
(IPS) than hMT+. A third possibility is of course that
motion language does not recruit visual motion processing
resources of any kind, resulting in no bias in dot motion
perception.
Further, the direction and extent of transfer from language
to perception may depend on an individual’s visual motion
imagery ability. People differ from one another in mental
imagery ability, and these differences correlate with
individual differences in spatial tasks and object perception
(Kozhevnikov, Kosslyn, & Shephard, 2005). In Winawer et
al (2010), most but not all participants showed MAEs as a
function of imagining motion, and the degree of adaptation
differed across people. We reasoned that people who show
stronger adaptation as a result of imagining, should be more
likely to show adaptation as a result of understanding
motion language. It would be reasonable to expect that
individuals who do not show an MAE as a result of
explicitly imagining motion should also not show one as a
result of processing motion language. To test for this
possibility, we tested each participant both in an explicit
visual imagery condition (as in Winawer et al (2010)), and
in conditions where linguistic motion was used as an

adapting stimulus. This allowed us to compare the effects
of language for each participant with those of explicit
imagery.
Finally, the present study is designed to test whether
literal and metaphorical descriptions of motion recruit
similar perceptual processes. To this end, we contrasted
literal motion stories that described the motion of physical
objects with metaphorical motion stories that used motion
verbs to talk about changes in abstract entities (e.g. rising
and falling stock prices).

Experiment
The experiment consisted of five parts: (1) a baseline task in
which we measured participants’ motion direction
sensitivity, (2) a familiarization task in which participants
viewed the stimuli to be imagined later in the study (3) the
main experimental task in which we tested for MAEs
following imagining motion or listening to stories
describing motion, (4) a memory task in which we measured
participants’ recognition memory for the stories, and (5) an
exit questionnaire in which we ascertained participants
knowledge of the motion aftereffect and their explicit
predictions about the direction of effects.

Figure 1: Schematic of experimental design highlighting
the block and trial structure of the main adaptation task. In
the imagery blocks, an upward or downward facing arrow
superimposed on a static image of the grating indicated the
direction in which to imagine the stripes moving. This cue
faded slowly over the course of a second. Once the cue
disappeared completely, a flickering fixation cross appeared
at the center of the screen. Participants were instructed to
fixate on the cross while imagining the stripes and to use the
rate of the flicker to help them remember how fast the
stripes should move. Participants were also instructed to
use the fixation cross as a cue for when to start and stop
imagining motion. In language blocks, participants listened
to stories using headphones while fixating a dot centered on
the monitor. Participants were told to listen carefully to the
stories, as there would be a memory test. They were not
instructed to imagine.

896

Methods

3. The squirrels continue to sprint downwards in a flash. They pour onto the wall and
surge directly toward the bottom.

Participants Sixty Stanford students participated in
exchange for payment.

4. Your eyes remain focused on the mob of squirrels teeming down the wall. You can
no longer pick out individuals as they dash for the bottom.
Metaphorical Motion: Upward Story (Four installments)
1. You are standing in the middle of the trading floor at the New York stock exchange
one busy morning. The room is buzzing with announcements of rising stock prices.
First JP Morgan rockets dramatically. Accenture and Delaware blaze to new heights.
Suddenly, Lincoln’s stock surges, along with Time Warner. You hear animated
reports of Toyota, Coca Cola, and The Gap going sky-high! You can hardly believe
it, but Google’s stock soars higher than ever. Walmart zips skyward, too. All
morning, you marvel at the continually spiking stocks!

Stimuli and Procedure
Main Experimental Task: The task design, procedure, and
visual stimuli used were modeled on those used in Winawer
et al (2010). On each trial participants judged the direction
of dot motion after either listening to stories describing
motion or engaging in explicit visual motion imagery.
Trials were presented in 12 interleaved blocks. There were
6 block types, 3(motion type: imagined motion, literal
motion, or metaphorical motion) by 2(motion direction:
upward or downward).
Adaptation Stimuli: In the literal motion condition the
stories used motion language to describe the movement of
physical objects (e.g., squirrels, ping-pong balls). In the
metaphorical motion condition, the stories used motion
language to describe changes in abstract entities (e.g. stock
prices, emotions). 12 literal and 12 metaphorical stories
were used with an upward and a downward version for each,
yielding a total of 48 stories. Individual participants heard
24 stories (either the upward or the downward version of
each story, but not both). Example stories are in Table 1. In
the imagery condition, participants were instructed to
imagine upward and downward moving gratings (as in
Winawer et al. (2010)). The trial structure for the language
and imagery conditions is depicted in Figure 1.

2. You hear that Ford and Exxon Mobile are really ramping up. Hewlett Packard is
erupting too!
3. Next you hear that Nokia is boosting quickly.
Verizon are surging dramatically.

Likewise, Sprint, AT&T and

4. Stock prices heighten rapidly for Proctor and Gamble as well as Clorox.
McDonalds’ stock also jets to new heights!
Metaphorical Motion: Downward Story (Four installments)
1. You are standing in the middle of the trading floor at the New York stock exchange
one busy morning. The room is buzzing with announcements of falling stock prices.
First JP Morgan plummets dramatically. Accenture and Delaware tumble to new
lows. Suddenly, Lincoln’s stock plunges, along with Time Warner. You hear
agitated reports of Toyota, Coca Cola, and The Gap hitting record lows! You can
hardly believe it, but Google’s stock sinks lower than ever. Walmart zips downward,
too. All morning, you marvel at the continually diving stocks!
2. You hear that Ford and Exxon Mobil are really sinking down. Hewlett Packard is
taking a nose-dive too!
3. Next you hear that Nokia is slumping quickly. Likewise, Sprint, AT&T and
Verizon are tumbling dramatically.
4. Stock prices level rapidly for Proctor and Gamble as well as Clorox. McDonalds’
stock also plunges to new lows!

Table 1: Sample stories heard by participants.
Block structure: In the two language conditions, each
block consisted of 3 stories with 4 installments each, for a
total of 12 trials per block. Each story was broken up into
one longer paragraph and three shorter ‘top-up’ installments
so that multiple measurements could be collected for each
story. The longer installments lasted on average 40.00
seconds, and the top-up installments 8.29 seconds. The
imagery blocks mirrored this structure.
Participants
imagined motion for 40 seconds, and on the three
subsequent ‘top-up’ trials, participants imagined motion for
8 seconds. This pattern was repeated 2 more times within
the block to parallel the 3 stories used per block in the
language conditions.
Adaptation Test: Following each story or imagery
installment, participants judged the direction of motion
coherence in a field of moving dots without feedback. The
moving dot stimuli were presented as in Winawer et al.
(2010). Each dot display had net motion coherence either
up or down. For each subject, two coherence values were
sampled: 12.5% and 25% of the coherence necessary for
asymptotic performance (as assessed individually for
participants in the baseline task). Coherence and direction of
motion were fully crossed and balanced across trials and
participants.
Exit questionnaire: At the end of the experiment we
ascertained participants’ familiarity with the motion
aftereffect and also asked them to generate a prediction

Literal Motion: Upward Story (Four installments)
1. You are running a psychology experiment in which you have trained hundreds of
squirrels to race each other up a wall for a piece of food. Now you want to see what
happens when they are all released at the foot of the wall at once. You watch through
a small window in the next room as the cages are opened and the squirrels leap onto
the wall in a frenzy. The little fur balls scurry up the wall in one relentless stream,
despite obvious defeat in the race. Zip! The brown creatures surge up the wall with
amazing agility. You see the same behavior in squirrel after squirrel – one swift jump
onto the wall and an instantaneous burst upward. Zoom! The squirrels rush up the
wall like a giant current. As if in a trance, the squirrels swiftly stream past your eyes
in their race for the top of the wall.
2. Zoom! More and more squirrels jump onto the wall and scurry upwards. You
watch them course up the wall in a blur.
3. The squirrels continue to sprint upwards in a flash. They spout onto the wall and
surge directly toward the top.
4. Your eyes remain focused on the mob of squirrels teeming up the wall. You can no
longer pick out individuals as they dash for the top.
Literal Motion: Downward Story (Four installments)
1. You are running a psychology experiment in which you have trained hundreds of
squirrels to race each other down a wall for a piece of food. Now you want to see
what happens when they are all released at the top of the wall at once. You watch
through a small window in the next room as the cages are opened and the squirrels
descend onto the wall in a frenzy. The little fur balls scurry down the wall in one
relentless stream, despite obvious defeat in the race. Zip! The brown creatures surge
down the wall with amazing agility. You see the same behavior in squirrel after
squirrel – one swift drop onto the wall and an instantaneous burst downward. Zoom!
The squirrels rush down the wall like a giant current. As if in a trance, the squirrels
swiftly stream past your eyes in their race for the bottom of the wall.
2. Zoom! More and more squirrels drop onto the wall and scurry downwards. You
watch them course down the wall in a blur.

897

about which way they thought the effect would go.
Participants were asked: Have you ever heard of the Motion
Aftereffect or Waterfall Illusion? and After viewing upward
motion, which way would you expect a static image to
appear to move?

In the overall sample, participants showed a reliable MAE
after imagining motion (M = 5.7% normalized coherence,
SD = 9.8%) (F(1,53) = 18.26, p < .001) (replicating
Winawer et al, 2010), but not after listening to motion
stories (M = 0.8% normalized coherence, SD = 9.2%)
(F(1,53) = 0.40, p > .5). The two conditions differed
reliably from one another (F(1,53) = 10.81, p < .005).
We reasoned that individuals who do not show MAEs as a
result of explicitly imagining motion should also not show
them as a result of processing motion language. However,
participants who do show MAEs from motion imagery may
show them from processing motion language as well.
Indeed, there was a significant correlation between the
effects of motion imagery and motion language (r(52) = .34,
p < .02), such that stronger adaptation from imagining
motion predicted stronger adaptation from understanding
motion language (Figure 3).

Results
The distance between the null points of the logistic fits for
upward and downward motion (normalized coherence
values at which participants are equally likely to report
upward and downward motion) was computed for both the
imagined and linguistic motion conditions for each
participant.
Positive values reflect adaptation.
Six
participants whose results exceeded three standard
deviations from the mean for all participants were excluded
from subsequent analyses. The literal and metaphorical
linguistic motion conditions did not significantly differ from
one another (t(53) = 0.219, p > .5), and so were combined
for analysis. Results are plotted in Figures 2-4.
a
After upward adaptation
After downward adaptation

Figure 3: Correlation across all participants between the
separation in motion response functions for imagined and
linguistic motion, r(52) = .34, p < .02.
To confirm that participants who showed adaptation to
imagined motion also showed it in response to linguistic
motion, we sorted participants based on the magnitude and
sign of the effect of explicit motion imagery and divided
them into three groups of equal size (Imagery Mdns =
15.1%, 3.8%, -1.7%, and SIQRs = 6.6%, 1.6%, 5.0%
normalized coherence) (Figure 4). We will refer to these as
strong, weak, and no MAE groups respectively.
Indeed, the group that showed strong MAEs after
explicitly imagining motion also showed reliable MAEs
after listening to motion language (Language Mdn = 5.6%,
SIQR = 4.7%) (n = 18, p < .031, sign-test, 2-tailed). There
was no difference in the strength of this adaptation effect
between the literal and metaphorical language conditions, n
= 18, p > .40. The two groups that showed weak or no
MAEs from imagery, did not show reliable MAEs from
language: (Mdn = -1.7%, SIQR = 5.0%) (n = 18, p > .05),
and (Mdn = 0.8%, SIQR = 5.1%) (n = 18, p > .5) for groups
that showed weak or no MAEs respectively. The effects of

b

Figure 2: (a) Proportion of “UP” responses following
imagined motion and linguistic motion across all
participants. Error bars represent standard error. (b)
Separation in motion response functions for imagined and
linguistic motion across all participants. Positive values
reflect adaptation. Error bars denote s.e.m.

898

language in the strongest MAE group differed reliably from
the other two groups, χ2(1, N=54)=7.27, p<.01.

aftereffect (the remaining 11 omitted this portion of the
study). Only three reported having heard of the motion
aftereffect. Participants’ expectations about the direction in
which adapting to visual motion in one direction might
affect subsequent visual processing did not reliably bias
(F(1,39) = 0.37, p>.50) or interact with (F(1,39) = 0.33,
p>.50) the effects of imagined and linguistic motion. This
finding confirms that the results obtained in this study are
not a product of participants’ expectations or explicit biases
regarding the direction of the effects.

Figure 4: Participants were sorted based on the size of the
aftereffect in the imagery condition and divided into three
equal-sized groups. The plot shows the median separation
between motion response functions for each group. Error
bars denote SIQR.
To examine the timecourse of the MAE from imagined
and linguistic motion, we subtracted the proportion of “up”
responses following upward motion from those following
downward motion across adaptation installments (e.g., the 4
installments of a story, or the analogous 4 imagery
installments). The mean difference by installment across all
participants is plotted in Figure 5. In the explicit imagery
trials, the MAE appears after the initial 40-second
installment of imagining (as would the MAE from real
visual motion), and participants remain adapted for
subsequent installments (there is no linear effect of
installment, F(1,53) = 0.076, p > .5). In the two language
conditions, however, the MAE does not emerge until later
installments (there is a reliable linear effect of installment,
F(1,53) = 6.59, p < .05). After the 3rd and 4th story
installment, there is a reliable motion aftereffect including
all participants, M=4.0%, SD = 12.7%; F(1,53) = 5.42, p <
.05. Motion language appears to produce a reliable MAE
across the entire sample only after sufficient exposure to
each story.
These findings raise the possibility that individual
differences in the MAE from linguistic motion reflect
differences in how efficiently people recruit visual
direction-selective mechanisms rather than qualitative
differences in which mechanisms are recruited. Indeed, the
linear effect of story installment does not differ among those
who show strong, weak, and no MAEs from motion
imagery (F(2,51)=.144, p>.5), with everyone showing the
same trend toward more adaptation as they get further into
the story.
Testing for effects of explicit bias: Of the 54 participants
included in the analysis, 43 completed an exit questionnaire
about their knowledge and predictions about the motion

Figure 5: Mean difference in proportion upward responses
following upward and downward motion across the four
motion installments. The data are plotted for the overall
sample. Positive values reflect adaptation, and error bars
denote s.e.m.

Discussion
We tested whether processing linguistic descriptions of
motion produces sufficiently vivid mental images to cause
direction-selective motion adaptation in the visual system
(i.e., cause a motion aftereffect illusion). We predicted that
the perceptual consequences of processing language should
depend on an individual’s mental imagery ability. Imagery
ability was operationalized as the extent to which explicit
visual motion imagery produced an MAE in each
participant. Put another way, imagery ability or vividness is
the extent to which people recruit perceptual resources
heavily enough to adapt them during explicit imagery.
We replicated previous work showing that intentionally
imagining motion produces an aftereffect. We then found
that participants who show the imagined motion aftereffect
most strongly also show this aftereffect in the natural course
of processing motion language (without instructions to
imagine). The same effects held for both literal and
metaphorical language. Individuals who did not show a
motion aftereffect as a result of imagining motion also did
not show an aftereffect from processing motion language
overall. However, the aftereffect from language gained
strength with the number of story installments. For the last
two installments (out of 4), understanding motion language

899

produced reliable MAEs across the entire sample. This
finding suggests the possibility that individuals may differ
in how efficiently they recruit visual mechanisms in service
of language comprehension. Future work will examine the
effects of systematically varying exposure to motion
language and the degree of story immersion on the MAE.
Participants’ knowledge of the MAE and their explicit
predictions about the direction that the MAE should go did
not predict their pattern of results. This helps us ensure that
the patterns observed were not simply due to participants’
explicit biases or expectations.
A further question concerns the effects from metaphorical
motion language. Some researchers have found that literal
and metaphorical language produce similar transfer effects
to perceptuo-motor tasks (e.g., Boulenger, Hauk, &
Pulvermüller, 2009; Glenberg & Kaschak, 2002; Richardson
et al., 2003), while others have found no evidence for
transfer from metaphorical language (Bergen et al., 2007).
In our study, literal and metaphorical motion language
produced the same effects. Our stimuli and methods differ
from previous studies in many ways. One potentially
important difference is that our stimuli were connected
narratives that built over time, whereas the studies just cited
used isolated sentences. Our results suggest that for
language processing to produce effects on low-level visual
processing, a greater amount of exposure to or immersion in
a connected narrative may be necessary.
The results of the present study demonstrate that at least
for a subset of the population, processing language
spontaneously creates sufficiently vivid mental images to
produce direction-selective adaptation in the visual system.
Future work will examine the source and possible cognitive
consequences of the individual differences we observed.
Why might some people be better able to recruit or
effectively modulate the activity of sensory neurons through
top-down processes? Further, are there resulting systematic
differences in the content and nature of representations
people form in the service of understanding language?

Glenberg, A. M., & Kaschak, M.P. (2002). Grounding
language in action. Psychonomic Bulletin & Review, 9,
558-565.
Goebel, R., Khorram-Sefat, D., Muckli, L., Hacker, H., &
Singer, W. (1998). The constructive nature of vision:
Direct evidence from functional magnetic resonance
imaging studies of apparent motion and motion imagery.
European Journal of Neuroscience, 10(5), 1563–1573.
Grossman, E. D., & Blake, R. (2001). Brain activity evoked
by inverted and imagined biological motion. Vision
Research, 41(10–11), 1475–1482.
Kozhevnikov, M., Kosslyn, S., & Shephard, J. (2005).
Spatial versus object visualizers: A new characterization
of visual cognitive style. Memory and Cognition, 33,
710-726.
Meteyard, L., Bahrami, B., & Vigliocco, G. (2007). Motion
detection and motion verbs. Psychological Science, 18,
1007-1013.
Richardson, D. C., Spivey, M. J., Barsalou, L. W., &
McRae, K. (2003). Spatial representations activated
during real-time comprehension of verbs. Cognitive
Science, 27, 767–780.
Rinck, M., & Bower, G., H. (2000). Temporal and spatial
distance in situation models. Memory and Cogntion, 28,
1310-1320.
Rinck, M., Hähnel, A., Bower, G., & Glowalla, U. (1997).
Journal of Experimental Psychology: Learning, Memory,
and Cognition, 23, 622-637.
Sadaghiani, S., Maier, J. X., & Noppeney, U. (2009).
Natural, metaphoric, and linguistic auditory direction
signals have distinct influences on visual motion
processing. Journal of Neuroscience, 29, 6490 – 6499.
Saygin, A.P., McCullough, S., Alac, M., Emmorey, K.
(2009). Modulation of BOLD response in motion
sensitive lateral temporal cortex by real and fictive motion
sentences. Journal of Cognitive Neuroscience. Early
Access
publication
on
Nov.
19,
2009,
doi:10.1162/jocn.2009.21388.
Spivey, M., & Geng, J. (2001). Oculomotor mechanisms
activated by imagery and memory: Eye movements to
absent objects. Psychological Research, 65, 235-241.
Stanfield, R., & Zwaan, R. (2001). The effect of implied
orientation derived from verbal context on picture
recognition. Psychological Science, 12, 153-156.
Winawer J, Huk A, Boroditsky L. (2008) A motion
aftereffect from viewing still photographs depicting
motion. Psychological Science, 19, 276-283.
Winawer J, Huk A, Boroditsky L. (2010). A motion
aftereffect from visual imagery of motion. Cognition,
114, 276-284.
Zwaan, R. A., Madden, C. J., Yaxley, R. H., & Aveyard, M.
E. (2004). Moving words: dynamic representations in
language comprehension. Cognitive Science, 28, 611-619.
Zwaan, R., Stanfield, R., & Yaxley, R. (2002). Language
comprehenders mentally represent the shapes of objects.
Psychological Science, 13, 168-171.

Acknowledgments
We thank Jonathan Winawer and members of Cognation for
their helpful feedback. This research was supported by an
NSF Career Award Grant given to Lera Boroditsky.

References
Barsalou, L. W. (1999). Perceptual symbol systems.
Behavioral and Brain Sciences, 22, 577-660.
Bergen, B. K., Lindsay, S., Matlock, T., & Narayanan, S.
(2007). Spatial and linguistic aspects of visual imagery in
sentence comprehension. Cognitive Science, 31, 733-764.
Blake, R., & Hiris, E. (1993). Another means for measuring
the motion aftereffect. Vision Research, 33, 1589-1592.
Boulenger, V., Hauk, O., & Pulvermüller, F. (2009).
Grasping ideas with the motor system: semantic
somatotopy in idiom comprehension. Cerebral Cortex,
19, 1905-1914.

900

