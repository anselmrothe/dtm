UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Encoding Sequential Information in Vector Space Models of Semantics: Comparing
Holographic Reduced Representation and Random Permutation
Permalink
https://escholarship.org/uc/item/7wc694rn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Recchia, Gabriel
Jones, Michael
Sahlgren, Magnus
et al.
Publication Date
2010-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

            Encoding Sequential Information in Vector Space Models of Semantics:
      Comparing Holographic Reduced Representation and Random Permutation
       Gabriel Recchia (grecchia@indiana.edu)                                Michael N. Jones (jonesmn@indiana.edu)
          Cognitive Science Program, 1910 E 10th St.                         Department of Psychological and Brain Sciences
       Indiana University, Bloomington, Indiana USA                           Indiana University, Bloomington, Indiana USA
           Magnus Sahlgren (mange@sics.se)                                   Pentti Kanerva (pkanerva@berkeley.edu)
            Swedish Institute of Computer Science                              Redwood Center for Theoretical Neuroscience
             Box 1263, SE-164 29 Kista, Sweden                              University of California, Berkeley, California, USA
                            Abstract                                   algorithm is applied to reduce the matrix’s dimensionality;
  Encoding information about the order in which words                  words are represented as vectors whose dimensions refer to
  typically appear has been shown to improve the performance           the largest eigenvalues of the reduced representation.
  of high-dimensional semantic space models. This requires an             Despite their successes both as tools and as psychological
  encoding operation capable of binding together vectors in an
  order-sensitive way, and efficient enough to scale to large text     models, vector-space models suffer from several
  corpora. Although both circular convolution and random               shortcomings. Most prominently, the models have been
  permutations have been enlisted for this purpose in semantic         criticized as “bag of words” models that encode only the
  models, these operations have never been systematically              contexts in which words co-occur, but ignore word-order
  compared. In Experiment 1 we compare their storage capacity          information. The role of word order was traditionally
  and probability of correct retrieval; in Experiments 2 and 3         thought to apply only to the rules of word usage (grammar)
  we compare their performance on semantic tasks when
                                                                       rather than to the lexical meaning of the word itself.
  integrated into existing models. We conclude that random
  permutations are a scalable alternative to circular convolution      However, temporal information is now taking a more
  with several desirable properties.                                   prominent role in the lexical representation of a word’s
                                                                       meaning. Recently, Elman (2009) has convincingly argued
  Keywords: semantic representation, semantic space models,
                                                                       that an inherent part of a word’s lexical representation is
  binding, convolution, permutation, random indexing.
                                                                       information about its common temporal context, event
                                                                       knowledge, and habits of usage (cf. McKoon & Ratcliff,
                        Introduction
                                                                       2003; see also Hare et al., 2009).
Vector-space models of lexical semantics have seen                        A second issue for these models is lack of scalability
considerable recent attention in the psychological literature          (Recchia & Jones, 2009; Kanerva, Kristofersson, & Holst,
both as automated tools to estimate semantic similarity                2000), due to reliance on computationally complex
between words, and as psychological models of how                      decomposition techniques to reveal the latent components in
humans learn and represent word meaning from repeated                  a word-by-document matrix (e.g., singular value
contextual co-occurrences. In general, these models build              decomposition). Not only is decomposition computationally
semantic representations for words from statistical                    expensive, the entire word-by-document matrix must be
redundancies observed in a large corpus of text (e.g.,                 stored in memory during the operation. The problem is
Landauer & Dumais, 1997; Lund & Burgess, 1996). As                     exacerbated by the fact that as the size of the corpus
tools, the models have provided invaluable metrics of                  increases, the number of both rows and columns in the
semantic similarity for stimulus selection and control in              matrix increase significantly, the number of columns
behavioral experiments using words, sentences, and larger              growing linearly with added documents, and the number of
units of discourse. As psychological models, the vectors               rows growing approximately in proportion to the square root
derived from distributional models serve as useful                     of the number of tokens (Heap’s law). The corpora that
representations in computational models of word                        vector-space models like LSA are most commonly trained
recognition, priming, and higher-order comprehension                   upon in the literature contain approximately the number of
(Landauer et al., 2007). In addition, the abstraction                  tokens that children are estimated to have experienced
algorithms themselves are often proposed as models of the              before age 3, not counting words that they produce during
cognitive mechanisms humans use to learn meaning from                  this time (Riordan & Jones, 2007; Risley & Hart, 2006).
repeated episodic experience.                                          Recently, Recchia and Jones (2009) demonstrated that
  A classic example of a vector-space model is Landauer                although simple semantic metrics such as pointwise mutual
and Dumais’ (1997) Latent Semantic Analysis (LSA). LSA                 information (PMI) are outperformed by more complex
begins with a word-by-document co-occurrence matrix                    models such as LSA on small corpora, PMI is capable of
representation of a text corpus. A lexical association                 much better correspondence to human-derived semantic
function is applied to dampen the importance of each word              similarity judgments due to its ability to scale to large
proportionate to its entropy over documents. Finally, an               corpora. This led the authors to favor simple and scalable
                                                                   865

  algorithms to more complex non-scalable algorithms,                 reversible. However, RPs are much more efficient to
  concordant with approaches that have met with success in            compute. In language applications of BEAGLE, the
  the computational linguistics literature (e.g. Banko & Brill,       computationally expensive convolution operation is what
  2001).                                                              limits the size of a text corpus that the model can encode. As
                                                                      Recchia and Jones (2009) have demonstrated, scaling a
                    Encoding Word Order                               semantic model to more data produces much better fits to
     Two recent vector-space models that directly address the         human semantic data. Hence, both order information and
  concerns of word order and scalability are Jones and                magnitude of linguistic input have been demonstrated to be
  Mewhort’s BEAGLE (2007) and the “random permutation”                important factors in human semantic learning. If RPs have
  model of Sahlgren, Holst, and Kanerva (2008) (henceforth            similar characteristics to convolution, they may afford
  referred to as RPM). Rather than starting with a word-by-           encoding very large-scale order information, and much
  document matrix, BEAGLE and RPM maintain static,                    better approximations to human semantic structure.
  randomly generated signal vectors intended to represent the            This work is further motivated by the cognitive
  invariant properties of each word (such as its orthography or       implications of circular convolution and RPs. Vector
  phonology), as well as dynamic memory vectors that store            representations constructed by means of circular
  information about each word’s semantic representation. To           convolution have been frequently described as
  represent statistical information about the word, BEAGLE            psychologically or neurally plausible (Levy, 2007; Jones &
  and RPM bind together collections of signal vectors into            Mewhort, 2007), due to several features that they share with
  order vectors that are added to memory vectors during               connectionist networks: distributed encoding, robustness to
  training. Integrating word-order information has yielded            noise, affordance of generalization, error correction, pattern
  greater success at fitting a variety of human semantic data         completion, and easy associative access (Plate, 2003).
  than encoding only contextual information (e.g., Jones,             Furthermore, implementing neural networks that instantiate
  Kintsch, & Mewhort, 2006; Jones & Mewhort, 2007).                   convolution-like operations is straightforward (Plate, 2000;
  Because they require neither the overhead of a large word-          but compare Pike, 1986). Similarly, RPs possess many
  by-document matrix nor computationally intensive matrix             properties relevant to human cognition. Not only have they
  decomposition techniques, both models are significantly             been proposed as a particularly versatile multiplication
  more scalable than traditional vector-space models.                 operator for constructing vector representations that are
     Although BEAGLE and RPM differ in several ways,                  highly distributed, tolerant of noise in the input, robust to
  arguably the most important difference lies in the nature of        error and component failure, and mathematically compatible
  the binding operation used to create order vectors. BEAGLE          with several known properties of neural circuitry (Kanerva,
  uses circular convolution, a binary operation (henceforth           2009), RPs are trivially easy to implement in connectionist
  denoted as *) performed on two vectors such that every              terms; a RP can simply be thought of as a two-layer network
  element i of (x * y) is given by:                                   connected by randomly placed one-to-one copy connections.
                                                                      Thus, comparing circular convolution and RPs affords us a
                D−1                                                   better understanding of two psychologically plausible
            ∑        x j ⋅ y( i− j ) mod D ,              (1)         operations for encoding semantic information that have
                j= 0
                                                                      never been systematically compared.
                                                                         We conducted three experiments intended to compare
  where D is the dimensionality of x and y. Circular
                                                                      convolution and RPs as means of encoding word-order
  convolution can be seen as a modulo-n variation of the
                                                                      information with respect to performance and scalability. In
€ tensor product of two vectors x and y such that (x * y) is of       Experiment 1, we conducted an empirical comparison of the
  the same dimensionality as x and y. Furthermore, although
                                                                      storage capacity and the probability of correct decoding
  (x * y) is dissimilar from both x and y by any distance
                                                                      under each method. In Experiment 2, we compared RPs
  metric, approximations of x and y can be retrieved via the
                                                                      with convolution in the context of a simple vector
  inverse operation of correlation.
                                                                      accumulation model equivalent to BEAGLE’s “order space”
     In contrast, RPM employs random permutations,
                                                                      (Jones and Mewhort, 2007) on a small battery of semantic
  henceforth referred to as RPs. True to their name, RPs are
                                                                      evaluation tasks when trained on a Wikipedia corpus. The
  functions that map input vectors to output vectors such that
                                                                      model was trained on both the full corpus and a random
  the outputs are simply randomly shuffled versions of the
                                                                      subset; results improved markedly when RPs are allowed to
  inputs. Just as (x * y) yields a vector that differs from x and
                                                                      scale up to the full Wikipedia corpus, which proved to be
  y but from which approximations of x and y can be
                                                                      intractable for the convolution-based model. Finally, in
  retrieved, the sum of two RPs of x and y, Πx + Π2y (where
                                                                      Experiment 3, we specifically compared BEAGLE to RPM,
  Π2y is defined as Π(Πy)) yields a vector dissimilar from x          which differs from BEAGLE in several important ways
  and y but from which approximations of the original x and y         other than its binding operation, to assess whether using RPs
  can be retrieved via the inverse permutations Π-1 and Π-2.          in the context of RPM improves their performance further.
     Both systems offer efficient storage properties,                 We conclude that random permutations are a promising and
  compressing order information into a single composite               scalable alternative to circular convolution.
  vector representation, and both encoding operations are
                                                                  866

                       Experiment 1                                 the cosine between yi and Π−2t, between yi and Π−3t, etc.,
Plate (2003) made a compelling case for circular                    until this similarity value exceeds some high threshold; this
convolution in the context of holographic reduced                   indicates that the algorithm has probably “found” yi in the
representation, demonstrating its utility in constructing           trace. At that point, t is permuted one more time, yielding
distributed representations with high storage capacity and          x′, a noisy approximation of yi’s associate xi. This
high probability of correct retrieval. However, the storage         approximation x′ can then be compared with clean-up
capacity and probability of correct retrieval with RPs has          memory to retrieve the original associate xi.
not been closely investigated. This experiment compared the            Alternatively, rather than selecting a threshold, t may be
probability of correct retrieval of RPs with circular               permuted some finite number of times1 and its cosine
convolution under varying dimensionality and number of              similarity to yi calculated for each permutation. Let n
vectors stored.                                                     indicate the inverse permutation for which cos(Π−nt, yi) is
                                                                    the highest. We can permute one more time to get Π-n-1t,
Method                                                              that is, our noisy approximation x′. This method is
As a test of the capacity of convolution-based associative          appropriate if we always want our algorithm to return an
memory traces, Plate (2003, Appendix D) describes a                 answer (rather than, say, timing out before the threshold is
simple paired-associative retrieval task in which the               exceeded), and is the method we used for this experiment.
algorithm must select, from set E of m possible random                 The final clean-up memory step is identical to that used
vectors, the vector xi that is bound to its associate yi . The      by Plate (2003): we calculate the cosine between x′ and each
retrieval algorithm is provided with a trace vector of the          vector in the clean-up memory E, and retrieve the vector in
form t = (x1 * y1) + (x2 * y2) + (x3 * y3) + … that stores a        E for which this cosine is highest. As when evaluating
total of k vectors. All vectors are of dimensionality D, and        convolution, we keep m (the number of vectors in E) fixed
each xi and yi is a vector with elements independently drawn        at 1,000 while varying the number of stored vectors k and
from Ν(0, 1/D). The retrieval algorithm is provided with the        the dimensionality D.
trace t and the probe yi , and works by first calculating a =
(yi # t), where # is the correlation operator described in          Results
detail in Plate (2003, pp. 94-97). It then retrieves the vector     Figure 1 reports retrieval accuracies for convolution-based
in the “clean-up memory” set E that is the most similar to a.       associative memory traces, while Figure 2 reports retrieval
This is accomplished by calculating the cosine between a            accuracies for the RP formulation of the task. 500 vector
and each vector in the set E, and retrieving the vector from        pairs were sampled randomly from a pool of 1,000 possible
E for which the cosine is highest. If this vector is not equal      random vectors with replacement and the proportion of
to xi , this counts as a retrieval error. We replicated Plate’s     correct retrievals was computed. All 1,000 vectors in the
method to empirically derive retrieval accuracies for a             pool were potential candidates; thus, an accuracy of 0.1%
variety of choices of k and D, keeping m fixed at 1,000.            would represent chance performance. The horizontal axes of
   Sahlgren et al. (2008) essentially bind signal vectors to        all figures indicate the total number of pairs stored in the
positions by means of successive self-composition of a              trace (i.e., half the total number of vectors in the trace).
permutation function Π, and construct trace vectors by
superposing the results. Because the signal vectors are
random, any permutation function that maps each element
of the input onto a different element of the output will do;
we adopt Sahlgren et al.’s suggestion of using rotation of a
vector by one position for Π for the sake of simplicity. We
also use their notation of Πnx to mean “Π composed with
itself n times;” thus, Π2x = Π(Πx), Π3(x) = Π(Π2x)), and so
forth. The notion of a trace vector of paired associations can
then be recast in RP terms as follows:
     t = (Πy1 + Π2x1) + (Π3y2 + Π4x2) + (Π5y3 + Π6x3) + …
where the task again is to retrieve some yi’s associate xi
when presented only with yi and t. A retrieval algorithm for
accomplishing this can be described as follows: Given a              Figure 1. Retrieval accuracies for convolution-based associative traces.
probe vector yi , the algorithm applies the inverse of the
initial permutation to trace vector t, yielding Π−1t. Next, the
                                                                       1
cosine between Π−1t and the probe vector yi is calculated,               In Plate’s (2003, p. 252) demonstration of the capacity of
yielding a value that represents the similarity between yi and      convolution-based associative memories, the maximal number of
Π−1t. These steps are then iterated: the algorithm calculates       pairs stored in a single trace was 14; we likewise restrict the
                                                                    maximal number of pairs in a single trace to 14 (28 items total).
                                                                867

                                                                                           Experiment 2
                                                                    In order to move from the paired-associates problem of
                                                                    Experiment 1 to a real language task, we evaluated how a
                                                                    simple vector accumulation model akin to Jones &
                                                                    Mewhort’s (2007) encoding of order-only information in
                                                                    BEAGLE would perform on a set of semantic tasks if RPs
                                                                    were used in place of circular convolution. In Experiment 2,
                                                                    we replaced the circular convolution component of
                                                                    BEAGLE with RPs so that we could quantify the impact
                                                                    that the choice of operation alone had on the results. Due to
                                                                    the computational efficiency of RPs, we were able to scale
                                                                    them to a larger version of the same textbase, and
                                                                    simultaneously explore the effect of scalability on order.
Figure 2. Retrieval accuracies for RP-based associative traces.
Discussion                                                          Method
Circular convolution has an impressive storage capacity and         Order information was trained using both the BEAGLE
excellent probability of correct retrieval at high                  model and a modified implementation of BEAGLE in which
dimensionalities; the results were comparable to those              the circular convolution operation was replaced with RPs as
reported by Plate (2003, p. 252) in his test of convolution-        they are described in Sahlgren at al. (2008). A brief example
                                                                    will illustrate how this replacement changes the algorithm.
based associative memories. However, RPs seem to share
                                                                    Recall that in BEAGLE, each word w is assigned a static
these desirable properties as well. In fact, the storage
capacity of RPs seems to drop off significantly more slowly         “environmental” signal vector ew as well as a dynamic
than does the storage capacity of convolution as                    memory vector mw that is updated during training. Recall
dimensionality is reduced.                                          also that the memory vector of a word w is updated by
  This information capacity is particularly interesting given       adding the sum of the convolutions of all n-grams (up to
that, with respect to basic encoding and decoding                   some maximum length λ) containing w. Upon encountering
operations, RP is computationally more efficient than               the phrase “one two three” in a corpus, the memory vector
convolution. Encoding n-dimensional bindings with circular          for “one” would normally be updated as follows:
convolution using equation (1) is a very slow O(n2)                            mone = mone + (Φ ∗ etwo) + (Φ ∗ etwo * ethree)
operation. This can be sped to O(n) by means of the Fast
                                                                    where Φ is a placeholder signal vector that represents the
Fourier transform (Jones, 2007; Plate, 2003). The algorithm
to bind two vectors a and b in O(n) time involves                   word whose representation is being updated. In the modified
calculating discrete Fourier transforms of a and b,                 BEAGLE implementation used in this experiment, the
multiplying them pointwise to yield a new vector c, and             memory vector for “one” would instead be updated as:
calculating the inverse discrete Fourier transform of c.                             mone = mone + Πetwo + Π2ethree
Encoding with RPs can also be accomplished in O(n) time,
                                                                    The modified BEAGLE implementation was trained on a
but with steps that are not as computationally expensive. To
                                                                    2.33 GB corpus (418 million tokens) of documents from
bind two vectors a and b, the elements of a are permuted by
                                                                    Wikipedia. Training on a corpus this large proved
directly copying then into a new vector, but with the
                                                                    intractable for the slower convolution-based approach.
mapping of their indices determined by the permutation
                                                                    Hence, we also trained both models on a 35 MB, six-
function. For example, if the permutation function were
                                                                    million-token subset of this corpus constructed by sampling
chosen to be rotation by one position and vectors were of
                                                                    random 10-sentence documents from the larger corpus
dimensionality D, each value at index i in the vector a
                                                                    without replacement. Accuracy was evaluated on two
would be copied to index (i + 1) mod D in the new vector.
                                                                    synonymy tests: the English as a Second Language (ESL)
The vector b is permuted in the same way, but using a
                                                                    and the Test of English as a Foreign Language (TOEFL)
different permutation function (e.g., (i + 2) mod D). Finally,
                                                                    synonymy assessments. Rank correlations to human
a and b are added to yield a final binding c.
                                                                    judgments of the semantic similarity of word pairs were
  Noisy decoding—the retrieval of a noisy version of one or
                                                                    calculated using the similarity judgments obtained from
more bound associates from a trace (which may then be
                                                                    Rubenstein and Goodenough (G, 1965), Miller and Charles
passed to clean-up memory to unambiguously determine the
                                                                    (M&C, 1991), Resnik (R, 1995), and Finkelstein et al.
identity of the associate)—also operates in O(n) time in both
                                                                    (F&al, 2002). A description of these measures can be found
representations. As with encoding, the operation is O(n), but
                                                                    in Recchia and Jones (2009).
fewer operations are required (a single permutation decodes
one associate, rather than an involution + two discrete             Results and Discussion
Fourier transforms + an elementwise multiplication + one
inverse discrete Fourier transform).                                Table 3 provides a comparison of two variants of the
                                                                    BEAGLE model, each trained on order information only.
                                                                868

“Convolution” refers to the original BEAGLE as described                         have been referring to as RPM. In many ways the two are
in Jones & Mewhort, while “Random Permutations” refers                           similar: Like BEAGLE, RPM can construct a semantic
to a version in which order information is encoded using                         space by (1) adding only order vectors to memory vectors
RPs rather than circular convolution. Three points about                         during training, yielding an “order space,” and (2) by adding
these results merit special attention. First, there are no                       order vectors as well as “context vectors,” yielding a
significant differences between the performance of                               “composite space.” Besides using RPs in place of circular
convolution and RPs on the small corpus. Both performed                          convolution, the specific implementation of RPM reported
nigh-identically on F and TOEFL; neither showed any                              by Sahlgren et al. differs from BEAGLE in several ways
significant correlations with human data on R&G, M&C, R,                         (signal-vector representation, window size, lexicon size, and
nor performed better than chance on ESL.                                         the stoplist). This experiment aims to assess RPM’s
                                                                                 performance with another corpus and on other semantic
                                                                                 tasks besides TOEFL, and to determine if performance
Table 3. Comparisons of variants of BEAGLE that differ by                        improves under RPM parameter settings (compared to the
binding operation. Accuracy scores are reported for ESL &                        BEAGLE settings in Experiment 2).
TOEFL; remaining tasks are Spearman rank correlations.
                                                                                 Method
    Criterion                 Wikipedia subset                Full Wikipedia     The same evaluation method was applied as in Experiment
                                             Random              Random
                     Convolution         Permutations         Permutations       2, but with BEAGLE being compared directly to RPM.
      ESL                 .20                    .26                 .32         Both models were trained in order and composite space.
     TOEFL               .46†                   .46†                .63†
      R&G                 .07                   -.06                .32*         Results and Discussion
      M&C                 .08                   -.01                .33*
        R                 .06                   -.04                .35*         Table 4 reports the results of BEAGLE and RPM trained in
      F&al               .13*                   .12*                .33*         order space, while Table 5 reports results in composite
*
  Significant correlation, p < .05, one-tailed.                                  space (context + order information). As in Experiment 2,
†
  Accuracy score differs significantly from chance, p < .05, one-tailed.
                                                                                 RPs but not convolution proved capable of scaling up to the
                                                                                 full Wikipedia corpus. We replicated Sahlgren et al.’s
                                                                                 (2008) performance on TOEFL in order space at this
Second, both models performed the best by far on the                             dimensionality, but this Wikipedia implementation of RPM
TOEFL synonymy test, supporting Sahlgren’s et al. (2008)                         fell short of the ~.73 accuracy they reported on TOEFL at a
claim that order information may indeed be more useful for                       dimensionality of 2000 in composite space; the difference is
synonymy tests than tests of semantic relatedness, as                            most likely due to the different corpora used in the two
paradigmatic rather than syntagmatic information sources                         evaluations. On the small corpus, switching from order
are most useful for the former. However, it is unclear                           space to composite space did not yield significant
exactly why neither model did particularly well on ESL2, as                      differences for either model when contrasted with the use of
many models have achieved scores on it comparable to their                       order space alone. On the large corpus, however, when
scores on TOEFL (Recchia & Jones, 2009). Finally, only                           contrasted with RPs in Experiment 2 (Table 3), RPM
RPs were able to scale up to the full Wikipedia corpus, and                      performed far better on several evaluations, most notably
doing so yielded extreme benefits for every task. This is a                      the correlations to the R&G, M&C, and R similarity
very strong point in favor of RPs, and suggests that                             judgments. It is intriguing that the version of RPM trained
sequential information can even be useful for tasks that                         on the full Wikipedia in order space was able to perform
involve semantic relatedness but not synonymy per se                             well on several tasks that are typically conceived of as tests
(R&G, M&C, R, F), provided that the model is trained at a                        of associative relatedness and not tests of synonymy per
sufficiently large scale.                                                        se—for example, .70 on the Miller & Charles pairs (Table
                                                                                 4).
                              Experiment 3
In Experiment 2 we saw that importing RPs into BEAGLE
yielded comparable results on a small corpus and                                 Table 4. BEAGLE and RPM in order space.
considerable improvement in scalability. Here we compare
                                                                                    Criterion                  Wikipedia subset                Full Wikipedia
BEAGLE to the original model of Sahlgren et al., which we
                                                                                                         BEAGLE                 RPM                 RPM
    2                                                                                  ESL                   .20                  .27               .38†
       Note that the absolute performance of these models is                         TOEFL                  .46†                 .37†               .65†
irrelevant to the important comparisons. Many factors (e.g.,                          R&G                    .07                  .15               .50*
frequency thresholding, morphological normalization, corpus                           M&C                    .08                  .16               .70*
siz/type) are known to improve performance on synonymy tests;                           R                    .06                  .11               .63*
we held these constant, which produced poor absolute performance                      F&al                  .13*                 .18*               .32*
                                                                                 *
(but see Sahlgren et al., 2008). The key comparisons are the                       Significant correlation, p < .05, one-tailed.
                                                                                 †
                                                                                   Accuracy score differs significantly from chance, p < .05, one-tailed.
consistency of the operations on the same textbase, and the relative
performance boost when data are scaled up.
                                                                             869

Table 5. BEAGLE and RPM in composite space.                                         of the Association for Computational Linguistics (pp. 26-33). Toulouse,
                                                                                    France: Association for Computational Linguistics.
                                                                                 Burgess, C., & Lund, K. (2000). The dynamics of meaning in memory. In
   Criterion                  Wikipedia subset               Full Wikipedia
                                                                                    E. Dietrich & A. Markman (Eds.), Cognitive dynamics: Conceptual and
                        BEAGLE                 RPM                RPM               representational change in humans and machines (pp. 117-156).
      ESL                   .24                  .27               .42†          Elman, J. L. (2009). On the meaning of words and dinosaur bones: Lexical
     TOEFL                 .47†                 .40†               .66†             knowledge without a lexicon. Cognitive Science, 33, 547-582.
      R&G                   .10                  .10               .49*          Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Z., S., Wolfman, G.,
      M&C                   .09                  .12               .70*             & Ruppin, E. (2002). Placing search in context: The concept revisited.
        R                   .09                  .03               .60*             Association for Computing Machinery Transactions on Information
                               *                    *
      F&al                 .23                  .19                .32*             Systems, 20(1), 116-131.
*
  Significant correlation, p < .05, one-tailed.                                  Hare, M., Jones, M. N., Thomson, C., Kelly, S., & McRae, K. (2009).
†
  Accuracy score differs significantly from chance, p < .05, one-tailed.            Activating event knowledge. Cognition, 111(2), 151-167.
                                                                                 Howard, M. W., Shankar, K. H., and Jagadisan, U. K. K. (In press).
                                                                                    Constructing semantic representations from a gradually-changing
                        General Discussion                                          representation of temporal context. Topics in Cognitive Science.
                                                                                 Jones, M. N. (2007). Holographic neural networks. Poster presented at the
   Experiment 1 demonstrates that RPs are capable of high                           48th Meeting of the Psychonomic Society. Long Beach, CA.
retrieval accuracy even when many paired associates are                          Jones, M. N., Kintsch, W., & Mewhort, D. J. K. (2006). High-dimensional
stored in a single trace, and their storage capacity appears to                     semantic space accounts of priming. Journal of Memory and Language,
                                                                                    55, 534-552.
be slightly better than that of circular convolution for low                     Jones, M. N. & Mewhort, D. J. K. (2007). Representing word meaning and
dimensionalities. Experiments 2 and 3 reveal that both                              order information in a composite holographic lexicon. Psychological
methods achieve approximately equal performance on a                                Review, 114, 1-37.
battery of semantic tasks when trained on a small corpus,                        Miller, G. A. and Charles, W. G. (1991). Contextual correlates of semantic
                                                                                    similarity. Language and Cognitive Processes, 6(1), 1-28.
but that RPs are ultimately capable of achieving superior                        McKoon, G. & Ratcliff, R. (2003). Meaning through syntax: Language
performance due to their higher scalability. In all, these                          comprehension and the reduced relative clause construction.
results suggest that RPs are worthy of further study both as                        Psychological Review, 110, 490-525.
encoders of sequential information in word space models                          Kanerva, P. (2009). Hyperdimensional computing: An introduction to
                                                                                    computing in distributed representations with high-dimensional random
and as operators capable of storing associative information                         vectors. Cognitive Computation, 1, 139-159.
more generally. It should be noted that Sahlgren et al.                          Kanerva, P., Kristoferson, J., & Holst, A. (2000). Random indexing of text
(2008) found better synonymy performance when RPs were                              samples for latent semantic analysis. In Proceedings of the 22nd Annual
trained on “direction” vectors rather than order vectors;                           Conference of the Cognitive Science Society, (p. 1036). Hillsdale, NJ:
                                                                                    Erlbaum.
direction vectors simply encode whether words appear                             Landauer, T. K. & Dumais, S. T. (1997). A solution to Plato’s problem:
before or after a word in the temporal stream, but ignore the                       The latent semantic analysis theory of acquisition, induction and
order chain. Given the computational efficiency of this                             representation of knowledge. Psychological Review, 104, 211-240.
approach, future work should explore the effects of scaling                      Landauer, T., McNamara, D. S., Dennis, S., & Kintsch, W. (Eds.) (2007).
                                                                                    Handbook of latent semantic analysis. Mahwah, NJ: Erlbaum.
to large-scale data on RP direction vectors.                                     Levy, S. D. (2007). Changing semantic role representations with
   Both convolutions and RPs are naturally derived from                             holographic memory. (Report No. FS-07-04). In Computational
properties of the human cognitive system, namely groups of                          approaches to representation change during learning and development:
neurons connected with a certain degree of randomness (see                          Papers from the 2007 AAAI Symposium. Menlo Park, CA: AAAI Press.
                                                                                 Pike, R. (1986). Comparison of convolution and matrix distributed memory
Plate, 2003 for convolution and Kanerva, 2009 for RPs; also                         systems for associative recall and recognition. Psychological Review, 91,
see Howard et al. [in press] for a related model using neural                       281-293.
properties of temporal context encoding). The current work                       Plate, T. (2003). Holographic reduced representation: Distributed
demonstrates that when a model is able to apply these                               representation for cognitive structures. CSLI Publications.
                                                                                 Recchia, G. L., & Jones, M. N. (2009). More data trumps smarter
associative learning mechanisms across a large amount of                            algorithms: Comparing pointwise mutual information to latent semantic
episodic experience with linguistic structure, it produces                          analysis. Behavior Research Methods, 41(3), 647-56.
much better approximations of human semantic structure.                          Resnik, P. (1995). Using information content to evaluate semantic
As Elman (2009) has argued, the encoding of large-scale                             similarity. In C. S. Mellish (Ed.), Proceedings of the Fourteenth
                                                                                    International Joint Conference on Artificial Intelligence (IJCAI) (pp.
order information is a core component of a word’s lexical                           448-453). Montréal, Canada: Morgan Kaufmann.
representation that is often overlooked. Future work needs                       Risley, T. R. & Hart, B. (2006). Promoting early language development. In
to explore application of large-scale RP encoding to more                           N. F. Watt, C. Ayoub, R. H. Bradley, J. E. Puma & W. A. LeBoeuf
complex semantic and linguistic tasks.                                              (Eds.), The crisis in youth mental health: Critical issues and effective
                                                                                    programs, Volume 4, Early intervention programs and policies, 83-88.
                                                                                    Westport, CT: Praeger.
                        Acknowledgements                                         Riordan, B., & Jones, M. N. (2007). Comparing semantic space models
                                                                                    using child-directed speech. In D. S. MacNamara & J. G. Trafton (Eds.),
This research was supported in part by a grant from Google                          Proceedings of the 29th Annual Conference of the Cognitive Science
Inc. to MNJ and a Shared University Research Grant from                             Society, 599-604.
IBM to Indiana University.                                                       Rubenstein, H., & Goodenough, J. (1965). Contextual correlates of
                                                                                    synonymy. Communications of the Association for Computing
                                                                                    Machinery, 8(10), 627-633.
                                References                                       Sahlgren, M., Holst, A., & Kanerva, P. (2008). Permutations as a means to
Banko, M. & Brill, E. (2001). Scaling to very very large corpora for natural        encode order in word space. In Proceedings of the 30th Annual
   language disambiguation. In Proceedings of the 39th Annual Conference            Conference of the Cognitive Science Society, p. 1300-1305.
                                                                             870

