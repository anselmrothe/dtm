UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Meaning Representation in Natural Language Categorization
Permalink
https://escholarship.org/uc/item/64r68052
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Fountain, Trevor
Lapata, Mirella
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                   Meaning Representation in Natural Language Categorization
                                          Trevor Fountain (t.fountain@sms.ed.ac.uk) and
                                                  Mirella Lapata (mlap@inf.ed.ac.uk)
                                               School of Informatics, University of Edinburgh
                                                10 Crichton Street, Edinburgh EH8 9AB, UK
                               Abstract                                       Although prototype theory provides a superior and work-
                                                                          able alternative to the classical theory it has been challenged
   A large number of formal models of categorization have been
   proposed in recent years. Many of these are tested on artificial       by the exemplar approach (Medin and Schaffer, 1978). In this
   categories or perceptual stimuli. In this paper we focus on cat-       view, categories are defined not by a single representation but
   egorization models for natural language concepts and specif-           rather by a list of previously encountered members. Instead
   ically address the question of how these may be represented.
   Many psychological theories of semantic cognition assume               of maintaining a single prototype for FRUIT that lists the fea-
   that concepts are defined by features which are commonly               tures typical of fruits, an exemplar model simply stores those
   elicited from humans. Norming studies yield detailed knowl-            instances of fruit to which it has been exposed (e.g., apples,
   edge about meaning representations, however they are small-
   scale (features are obtained for a few hundred words), and ad-         oranges, pears). A new object is grouped into the category if
   mittedly of limited use for a general model of natural language        it is sufficiently similar to one or more of the FRUIT instances
   categorization. As an alternative we investigate whether cate-         stored in memory.
   gory meanings may be represented quantitatively in terms of
   simple co-occurrence statistics extracted from large text col-             In the past much experimental work has tested the predic-
   lections. Experimental comparisons of feature-based catego-            tions of prototype- and exemplar-based theories in laboratory
   rization models against models based on data-driven represen-
   tations indicate that the latter represent a viable alternative to     studies involving categorization and category learning. These
   the feature norms typically used.                                      experiments tend to use perceptual stimuli and artificial cat-
                                                                          egories (e.g., strings of digit sequences such as 100000 or
                           Introduction                                   0111111). Analogously, much modeling work has focused
                                                                          on the questions of how categories and stimuli can be rep-
Considerable psychological research has shown that people
                                                                          resented (Griffiths et al., 2007a; Sanborn et al., 2006) and
reason about novel objects they encounter by identifying the
                                                                          how best to formalize similarity. The latter plays an impor-
category to which these objects belong and extrapolating
                                                                          tant role in both prototype and exemplar models as correct
from their past experiences with other members of that cat-
                                                                          generalization to new objects depends on identifying previ-
egory. This task of categorization, or grouping objects into
                                                                          ously encountered items correctly.
meaningful categories, is a classic problem in the field of cog-
nitive science, one with a history of study dating back to Aris-              In this paper we focus on the less studied problem of cat-
totle. This is hardly surprising, as the ability to reason about          egorization of natural language concepts. In contrast to the
categories is central to a multitude of other tasks, including            numerous studies using perceptual stimuli or artificial cate-
perception, learning, and the use of language.                            gories, there is surprisingly little work on how natural lan-
   Numerous theories exist as to how humans categorize ob-                guage categories are learned or used by adult speakers. A few
jects. These theories themselves tend to belong to one of three           notable exceptions are Heit and Barsalou (1996) who attempt
schools of thought. In the classical (or Aristotelian) view cat-          to experimentally test an exemplar model within the context
egories are defined by a list of “necessary and sufficient”               of natural language concepts, Storms et al. (2000) who eval-
features. For example, the defining features for the concept              uate the differences in performance between exemplar and
BACHELOR might be male, single, and adult. Unfortunately,                 prototype models on a number of natural categorization tasks,
this approach is unable to account for most ordinary usage                and Voorspoels et al. (2008) who model typicality ratings for
of categories, as many real-world objects have a somewhat                 natural language concepts. A common assumption underly-
fuzzy definition and don’t fit neatly into well-defined cate-             ing this work is that the meaning of the concepts involved in
gories (Smith and Medin, 1981).                                           categorization can be represented by a set of features (also
   Prototype theory (Rosch, 1973) presents an alternative for-            referred to as properties or attributes).
mulation of this idea, in which categories are defined by an                  Indeed, featural representations have played a central role
idealized prototypical member possessing the features which               in psychological theories of semantic cognition and knowl-
are critical to the category. Objects are deemed to be members            edge organization and many studies have been conducted to
of the category if they exhibit enough of these features; for             elicit detailed knowledge of features. In a typical procedure,
example, the characteristic features of FRUIT might include               participants are given a series of object names and for each
contains seeds, grows above ground, and is edible. Roughly                object they are asked to name all the properties they can
speaking, prototype theory differs from the classical theory in           think of that are characteristic of the object. Although fea-
that members of the category are not required to possess all              ture norms are often interpreted as a useful proxy of the struc-
of the features specified in the prototype.                               ture of semantic representations, a number of difficulties arise
                                                                      1916

when working with such data (e.g., Sloman and Ripps 1998;            scribe three corpus-based models of meaning representation,
Zeigenfusse and Lee 2009). For example, the number and               highlight their differences, and motivate their selection.
types of attributes generated can vary substantially as a func-
tion of the amount of time devoted to each object. There are         Feature Norms
many degrees of freedom in the way that responses are coded          As mentioned earlier, many behavioral experiments have
and analyzed. It is not entirely clear how people generate fea-      been conducted to elicit semantic feature norms across lan-
tures and whether all of these are important for representing        guages. One of the largest samples for English has been col-
concepts. Finally, multiple subjects are required to create a        lected by McRae et al. (2005). Their norms consist of 541
representation for each word, which limits elicitation studies       basic-level concepts (e.g., DOG and CHAIR) with features col-
to a small number of words and consequently the scope of             lected in multiple studies over several years. For each concept
any computational model based on these feature norms.                several annotators were asked to produce a number of relevant
   Even when the stimuli in question are of an abstract or           features (e.g., barks, has-four-legs, and used-for-sitting). The
linguistic nature, the features elicited are assumed to be rep-      production frequency of a feature given a particular concept
resentative of the underlying referents. As an alternative we        can be viewed as a form of weighting indicating the feature’s
propose to model the categorization of linguistic stimuli ac-        importance for that concept. A spatial representation of word
cording to their distribution in corpora. Words whose refer-         meaning can be extracted from the norms by constructing a
ents exhibit differing features likely occur in correspondingly      matrix in which each row represents a word and each column
different contexts; our question is whether these differences        a feature for that word. Cells in the matrix correspond to the
in usage can provide a substitute for featural representations.      frequency with which a feature was produced in the context
   The idea that words with similar meaning tend to be dis-          of a given word. An example of such a space is shown in Ta-
tributed similarly across contexts is certainly not a novel          ble 2 (a) (the numbers correspond to production frequencies,
one. Semantic space models, among which Latent Seman-                e.g., 12 participants thought has-legs is a feature of TABLE).
tic Analysis (LSA, Landauer and Dumais 1997) is perhaps                 Unfortunately, McRae et al.’s (2005) norms do not include
known best, operationalize this idea by capturing word mean-         any explicit relational information. Because we are interested
ing quantitatively in terms of simple co-occurrence statis-          in using the norms in a model of categorization it was nec-
tics (between words and paragraphs or documents). More re-           essary for us to augment the concepts with category labels
cently, topic models (Griffiths et al., 2007b) have arisen as a      (e.g., ‘dog’ is an ANIMAL) and typicality ratings (e.g., ‘dog’
more structured representation of word meaning. In contrast          is a typical ANIMAL whereas ‘Snoopy’ isn’t). We collected
to more standard semantic space models where word senses             this information using Amazon Mechanical Turk1 , an online
are conflated into a single representation, topic models as-         labor marketplace which has been used in a wide variety of
sume that words observed in a corpus manifest some latent            elicitation studies and has been shown to be an inexpensive,
structure — word meaning is a probability distribution over a        fast, and (reasonably) reliable source of non-expert annota-
set of topics (corresponding to coarse-grained senses). Each         tion for simple tasks (Snow et al., 2008).
topic is a probability distribution over words whose content            We obtained category labels as follows. We presented each
is reflected in the words to which it assigns high probability.      participant with twenty unrelated, randomly selected con-
   In this work we investigate whether semantic represen-            cepts from McRae et al.’s (2005) data set and asked them to
tation models based on the statistical analysis of large text        label each with the category to which it best belonged. Re-
collections can provide a viable alternative to feature norms        sponses were in the form of free text, i.e., participants were
for natural language categorization. Specifically, we com-           asked to key in a label rather than select one from a list. Each
pare categorization models that represent concepts by fea-           concept was labeled by ten participants; concepts were then
tures against LSA, Latent Dirichlet Allocation (LDA, Grif-           grouped according to the resulting categories. Because an-
fiths et al. 2007b; Blei et al. 2003), a well-known topic model,     notations collected from Mechanical Turk can be noisy we
and a semantic space that takes syntactic information into           then discarded those categories containing fewer than five
account (Padó and Lapata, 2007). These semantic represen-           unique concepts, leaving 41 categories for 541 exemplars.
tations are used as input to two well-established categoriza-        These category labels are listed in Table 1. To fully integrate
tion models, namely Nosofsky’s (1988) generalized context            them into the norms it was necessary to collect semantic fea-
model (GCM) and a prototype model derived from the GCM.              tures for them. To do this, we replicated the norming study
We evaluate the performance of these models on three adult           of McRae et al. (2005), again using Mechanical Turk. Par-
categorization tasks — category naming, typicality rating,           ticipants were presented with a single concept (drawn from
and exemplar generation — which have been previously mod-            the set of category labels collected in our previous study) and
eled using exclusively feature norms (Storms et al., 2000).          asked to generate ten relevant features. Instructions and ex-
Our results indicate that LSA-based meaning representations          amples were taken from McRae et al. (2005). For each cate-
outperform more sophisticated alternatives across the board,         gory label we collected features from 30 participants, result-
whilst lagging behind feature norms only by a small margin.          ing in a large number of features per item. These features
                                                                     were then mapped into the features already present in the
                 Meaning Representation                              norms; as in McRae et al. (2005) this mapping was performed
                                                                     manually.2
In this section we briefly describe the feature norms used in
our experiments. These were based on an existing general                 1
                                                                           http://www.mturk.com
purpose database (McRae et al., 2005) which we augmented                 2
                                                                           The extended database can be downloaded from http://
in several ways to suit our categorization tasks. We also de-        homepages.inf.ed.ac.uk/s0897549/data/.
                                                                 1917

      INSTRUMENT     keyboard       FURNITURE       chair        HOUSING          apartment     DEVICE              stereo
      REPTILE        rattlesnake    CONTAINER       bin          VEHICLE          bike          TRANSPORTATION      van
      CLOTHING       jeans          STRUCTURE       building     VEGETABLE        carrot        FOOD                bread
      HARDWARE       drill          APPLIANCE       stove        BIRD             seagull       GARMENT             coat
      HOUSE          cottage        PLANT           vine         TOOLS            hammer        FISH                trout
      EQUIPMENT      football       UTENSIL         ladle        THING            doll          ENCLOSURE           fence
      TOY            surfboard      KITCHEN         dish         RODENT           rat           INSECT              grasshopper
      BUG            beetle         HOME            house        FRUIT            grapefruit    SPORTS              helmet
      MAMMAL         horse          OBJECT          door         ACCESSORIES      necklace      COOKWARE            pan
      STORAGE        cabinet        BUILDING        apartment    ANIMAL           cat           WEAPON              bazooka
 Table 1: Category labels with most typical exemplars produced by participants in category naming and typicality rating study.
   This augmented dataset could be used as-is to evaluate a           hension, synonym selection, and human similarity judgments
model of categorization on either a category naming or an             (see Landauer and Dumais 1997 and the references therein).
exemplar generation task (we describe these tasks in detail           LSA provides a simple procedure for constructing spatial
in the following section). We further wished to use typical-          representations of word meanings. The same is true for de-
ity rating as an additional means for evaluation (Voorspoels          pendency vectors where co-occurrence statistics are com-
et al., 2008). We therefore elicited typicality ratings again via     puted between words attested in specific syntactic relations
Mechanical Turk. Participants were presented with a single            (e.g., object-of, subject-of). The assumption here is that syn-
category (e.g., FRUIT) along with twenty randomly selected            tactic information provides a linguistically informed context,
exemplars belonging to the category (e.g., ‘cherry’, ‘apple’,         and therefore a closer reflection of lexical meaning. LDA, in
and ‘tomato’) and asked to rate the typicality of each exem-          contrast, imposes a probabilistic model onto those distribu-
plar among members of the category. Typicality ratings for            tional statistics, under the assumption that hidden topic vari-
each exemplar-category pair were collected from 20 partici-           ables drive the process that generates words. Both spatial and
pants and an overall rating for each exemplar was computed            topic models represent the meanings of words in terms of an
by taking their mean. The highest rated exemplar for each             n-dimensional series of values, but whereas semantic spaces
category is shown in Table 1.                                         treat those values as defining a vector with spatial properties,
   We assessed the quality of the data obtained from Mechan-          topic models treat them as a probability distribution.
ical Turk by calculating their reliability, namely the likeli-
hood of a similarly-composed group of participants presented          Latent Semantic Analysis To create a meaning repre-
with the same task under the same circumstances produc-               sentation for words LSA constructs a word-document co-
ing identical results. We split the collected typicality ratings      occurrence matrix from a large collection of documents. Each
randomly into two halves and computed the correlation be-             row in the matrix represents a word, each column a docu-
tween them; this correlation was averaged across three ran-           ment, and each entry the frequency with which the word ap-
dom splits. These correlations were adjusted by applying the          peared within that document. Because this matrix tends to be
Spearman-Brown prediction formula (Storms et al., 2000;               quite large it is often transformed via a singular value de-
Voorspoels et al., 2008). The reliability of the ratings aver-        composition (Berry et al., 1995) into three component ma-
aged over 41 concepts was 0.64 with a standard deviation              trices: a matrix of word vectors, a matrix of document vec-
of 0.03. The minimum reliability was 0.52 (INSTRUMENT);               tors, and a diagonal matrix containing singular values. Re-
the maximum was 0.75 (FURNITURE). Reliability on the cat-             multiplying these matrices together using only the initial por-
egory naming task was computed similarly, with an average             tions of each (corresponding to the use of a lower dimen-
of 0.72, a maximum of 0.91 (INSTRUMENT), and a minimum                sional spatial representation) produces a tractable approxima-
of 0.13 (STRUCTURE). These reliability figures may seem low           tion to the original matrix. This dimensionality reduction can
compared with Storms et al. (2000) who perform a similar              be thought of as a means of inferring latent structure in distri-
study. However, note that they conduct a smaller scale ex-            butional data whilst simultaneously making sparse matrices
periment; they only focus on eight common natural language            more informative. The resulting lower-dimensional vectors
concepts (whereas we include 41), and 12 exemplars for each           can then be used to represent the meaning of their correspond-
concept (our exemplars are 541).                                      ing words; example representations in LSA space are shown
                                                                      in Table 2 (b) (vector components represent tf-idf scores).
Data-driven Approaches                                                Dependency Vectors Analogously to LSA, the dependency
In addition to feature norms, we obtained semantic represen-          vectors model constructs a co-occurrence matrix in which
tations for categories and exemplars from natural language            each row represents a single word; unlike LSA, the columns
corpora. We compared three computational models: Latent               of the matrix correspond to other words in whose syntac-
Semantic Analysis (LSA; Landauer and Dumais 1997), La-                tic context the target word appears. These dimensions may
tent Dirichlet Allocation (LDA; Griffiths et al. 2007b; Blei          be either the context word alone (e.g., walks) or the context
et al. 2003), and Dependency Vectors (DV; Padó and La-               word paired with the dependency relation in which it occurs
pata 2007). LSA has historically been a popular method                (e.g., subj-of-walks). Many variants of syntactically aware se-
of extracting meaning from corpora, and has been success-             mantic space models have been proposed in the literature. We
ful at explaining a wide range of behavioral data — ex-               adopt the framework of Padó and Lapata (2007) where a se-
amples include lexical priming, deep dyslexia, text compre-           mantic space is constructed over dependency paths, namely
                                                                  1918

sequences of dependency edges extracted from the depen-                                    (a) Feature Norms
dency parse of a sentence. Three parameters specify the se-                       has 4 legs used for eating             is a pet ...
mantic space: (a) the content selection function determines              TABLE         12                 9                   0   ...
which paths contribute towards the representation (e.g., paths           DOG           14                 0                 15    ...
of length 1), (b) the path value function assigns weights to
paths (e.g., it can be used to discount longer paths, or give                                    (b) LSA
more weight to paths containing subjects and objects as op-                      Document 1 Document 2 Document 3 ...
posed to determiners or modifiers.), and (c) the basis map-              TABLE        0.02              0.98               -0.12  ...
ping function creates the dimensions of the semantic space               DOG          0.73            -0.02                 0.01  ...
by mapping paths that end in the same word to the same di-
mension. A simple dependency space in shown in Table 2 (c)                                        (c) DV
(vector components represent co-occurrence frequencies).                         subj-of-walk      subj-of-eat       obj-of-clean ...
                                                                         TABLE           0                3                 28    ...
Latent Dirichlet Allocation Unlike LSA and DV, LDA is                    DOG           36                48                 19    ...
a probabilistic model of text generation. Each document is
modeled as a distribution over K topics, which are them-                                         (d) LDA
selves characterized as distribution over words. The individ-                       Topic 1          Topic 2             Topic 3  ...
ual words in a document are generated by repeatedly sam-                 TABLE        0.02              0.73               0.04   ...
pling a topic according to the topic distribution and then sam-          DOG          0.32              0.01               0.02   ...
pling a single word from the chosen topic. Under this frame-
work the problem of meaning representation is expressed as          Table 2: Semantic representations for ‘table’ and ‘dog’ using
one of statistical inference: give some data — words in a cor-      feature norms, Latent Semantic Analysis (LSA), Dependency
pus, for instance — infer the latent structure from which it        Vectors (DV), and Latent Dirichlet Allocation (LDA).
was generated. Word meaning in LDA is represented as a
probability distribution over a set of latent topics. In other      exemplars and inclusion of an unknown item in a category is
words, the meaning of a word is a vector whose dimensions           determined by the net similarity between the item and each of
correspond to topics and values to the probability of the word      the category’s exemplars. Specifically, the similarity ηw,j of
given these topics; the likelihood of seeing a word summed          a novel item w to the category c is calculated by summing its
over all possible topics is always one. Example representa-         similarity to all stored items i belonging to c:
tions of words in LDA space appear in Table 2 (d) (vector
components are topic-word distributions).                                                             X
                                                                                             ηw,c =         ηw,i                      (1)
Implementation All three models of word meaning were                                                   i∈c
trained on the British National Corpus. For the LSA model
we used the implementation provided in the Infomap toolkit3 ,       To calculate the inter-item similarity ηw,i we compute the co-
with words represented as vectors in a 100-dimensional              sine of the angle between the vectors representing w and i:
space; for the DV model we used the implementation4 of
Padó and Lapata (2007) with dependency paths up to length 3                                                  vw · vi
                                                                                      ηw,i = cos(θ) =                                 (2)
and a length-based path value function that assigns each path                                              ||vw ||.||vi ||
a value inversely proportional to its length, thus giving more
weight to shorter paths corresponding to more direct relation-      Following Vanpaemel et al. (2005), we can modify Equa-
ships. We obtained dependency information from the output           tion (1) into a prototype model by replacing the list of stored
of MINIPAR, a broad coverage dependency parser (Lin, 2001).         exemplars with a single ‘prototypical’ exemplar cj :
Infrequent dependencies attested less than 500,000 times in
the BNC were discarded. The LDA model used the imple-                                          ηw,c = ηw,cj                           (3)
mentation5 of Phan et al. (2008) with 100 topics. Inference
in this model is based on a Gibbs sampler which we ran              For the category prototype cj we use the representation of the
for 2,000 iterations. Additionally, LDA has two hyperparam-         category label, e.g., the prototype for the category FRUIT is
eters α and β which were set to 0.5 and 0.1, respectively.          the semantic representation of the word ‘fruit’. The similarity
                                                                    between an item and a category thus reduces to the cosine
                       Categorization                               distance between the item and prototype representations.
Models                                                              Tasks
The semantic representations described above served as the          We evaluated the performance of our models on three cate-
input to two categorization models, representative of the           gorization tasks introduced in Storms et al. (2000): category
exemplar-based and prototype-based approaches. In the gen-          naming, typicality rating, and exemplar generation.
eralized context model (GCM, Nosofsky 1988; Medin and                  In category naming the model is presented with a previ-
Schaffer 1978) categories are represented by a list of stored       ously unencountered word and must predict the most appro-
                                                                    priate category to which it belongs, e.g., the exemplar ‘apple’
    3
      http://infomap.stanford.edu/                                  would be most correctly identified as a member of the cat-
    4
      http://www.nlpado.de/˜sebastian/dv.html                       egory FRUIT, or (with lesser likelihood) FOOD or TREE. In
    5
      http://gibbslda.sourceforge.net/                              the exemplar model (see (1)), we measure the similarity ηw,c
                                                                1919

                                                                              Correlation w/ typicality ratings                                           Mean overlap (/20 exemplars)
                                          1                                                                        1                                                                                           20
               Proportion correct
                                     0.8                                                                          0.8
                                                                                                                                                                                                               15
                                     0.6                                                                          0.6
                                                                                                                                                                                                               10
                                     0.4                                                                          0.4
                                                                                                                                                                                                                5
                                     0.2                                                                          0.2
                                          0                                                                        0                                                                                            0
                                                 DV    LDA    LSA    Norms                                                DV    LDA    LSA   Norms                                                                         DV    LDA    LSA    Norms
                                               (a) Category Naming                                                      (b) Typicality Rating                                                                           (c) Exemplar Generation
                                              Figure 1: Performance of exemplar model using feature norms and data-driven meaning representations.
                                                                              Correlation w/ typicality ratings                                                                 Mean overlap (/20 exemplars)
                                     1                                                                             1                                                                                            20
 Proportion correct
                                    0.8                                                                           0.8
                                                                                                                                                                                                                15
                                    0.6                                                                           0.6
                                                                                                                                                                                                                10
                                    0.4                                                                           0.4
                                                                                                                                                                                                                    5
                                    0.2                                                                           0.2
                                     0                                                                             0                                                                                                0
                                                DV    LDA    LSA    Norms                                                 DV    LDA    LSA   Norms                                                                          DV    LDA    LSA    Norms
                                              (a) Category Naming                                                       (b) Typicality Rating                                                                            (c) Exemplar Generation
                                              Figure 2: Performance of prototype model using feature norms and data-driven meaning representations.
of the novel word against all previously encountered exem-                                                                            mance on the exemplar generation task by computing the av-
plars and select the category with the highest net similarity                                                                         erage overlap (across categories) between the exemplars gen-
between its exemplars and the word in question; for the pro-                                                                          erated by the model and those ranked as most typical of the
totype model (see (3)) this is the category with the highest                                                                          category by our participants.
similarity between the word and the category’s label. Per-
formance on the category naming task was determined in a                                                                                                                                                            Results
leave-one-out fashion: a single exemplar was removed from                                                                             Figure 1 summarizes our results with the exemplar model
the training examples and then categorized. This was repeated                                                                         and four meaning representations: McRae et al.’s (2005) fea-
for each exemplar in the training set. The latter consisted of                                                                        ture norms (Norms), Latent Semantic Analysis (LSA), Latent
41 subject-produced category labels each with an average of                                                                           Dirichlet Allocation (LDA), and Dependency Vectors (DV).
30 exemplars.                                                                                                                         Results are shown for category naming (Figure 1(a)) typical-
   In a typicality rating task the model is presented with both                                                                       ity rating (Figure 1(b)) and exemplar generation (Figure 1(c)).
an exemplar and label of the category to which it belongs, and                                                                        We examined performance differences between models us-
must predict the degree to which it is common amongst mem-                                                                            ing a χ2 test (category naming and exemplar generation) and
bers of that category. For the category FOOD, for example,                                                                            Fisher’s r-to-z transformation (to compare correlation coeffi-
‘pizza’ or ‘bread’ would be considered highly typical exem-                                                                           cients for the typicality rating task).
plars, while ‘lutefisk’ or ‘black pudding’ would likely be con-                                                                          On category naming the exemplar model performs signif-
sidered much more atypical. The predicted typicality rating                                                                           icantly better with the feature norms than when using any
for a word and a category is simply the similarity between the                                                                        of the three corpus-derived representations (p < 0.01); how-
two. In the exemplar model this is the sum similarity between                                                                         ever, LSA performs significantly better (p < 0.05) than DV
the word and each of the category’s exemplars; in the proto-                                                                          or LDA. On typicality rating there is no significant differ-
type model this is the similarity between the category’s label                                                                        ence between the feature norms and LSA. The norms are
and the word. Performance on the typicality rating task was                                                                           significantly better (p < 0.01) than either DV or LDA, while
evaluated by computing the correlation between the models’                                                                            LSA surpasses both of the other two corpus-derived represen-
predicted typicality ratings and the average value predicted                                                                          tations (p < 0.01). Additionally, LDA performs significantly
by the participants of our rating study. The dataset included                                                                         better than DV (p < 0.05). On the exemplar generation task
typicality ratings for 1,228 exemplar-category pairs.                                                                                 the feature norms are significantly better (p < 0.01) than any
   In an exemplar generation task the model is given a cat-                                                                           of the corpus-based representations; similarly, LSA performs
egory label and must generate exemplars typical of the cat-                                                                           significantly better than LDA or DV (p < 0.01), while LDA
egory, e.g., for FOOD we might generate ‘pizza’, ‘bread’,                                                                             again outperforms the dependency space (p < 0.05).
‘chicken’, etc. Given a category the model selects from the                                                                              Our results with the prototype model are shown in Figure 2
exemplars known to belong those that are most typical; typi-                                                                          and broadly follow a similar pattern. On category naming the
cality is again approximated by word-category similarities as                                                                         feature norms outperform any of the corpus-based representa-
determined by the model-specific ηw,c . We evaluate perfor-                                                                           tions (p < 0.01), LSA is significantly better than LDA which
                                                                                                                                1920

in turn is better than DV (p < 0.05). On typicality rating          Secondly, we expect that developing specialized models for
there is no significant difference between the feature norms        natural language categorization that are tailored to data-
and LSA; the difference between LSA and either of the other         driven meaning representations would improve performance.
two representations is significant (p < 0.01). On the exem-
plar generation task feature norms significantly outperform                                      References
all other representations (p < 0.01); LSA is significantly bet-     Andrews, M., Vigliocco, G., and Vinson, D. (2009). Integrating
                                                                       experiential and distributional data to learn semantic representa-
ter (p < 0.01) than LDA or DV.                                         tions. Psychological Review, 116(3):463–498.
                                                                    Berry, M., Dumais, S., and O’Brien, G. (1995). Using linear algebra
                          Discussion                                   for intelligent information retrieval. SIAM review, 37(4):573–595.
                                                                    Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet
In this work we have quantitatively evaluated feature norms            allocation. Journal of Machine Learning Research, 3:993–1022.
and alternative corpus-based meaning representations on             Griffiths, T. L., Canini, K. R., Sanborn, A. N., and Navarro, D. J.
three natural language categorization tasks. Perhaps unsur-            (2007a). Unifying rational models of categorization via the heri-
                                                                       archical dirichlet process. Proceedings of the Twenty-Ninth An-
prisingly our results indicate that feature norms are more ac-         nual Conference of the Cognitive Science Society.
curate representations when compared to corpus-based mod-           Griffiths, T. L., Tenenbaum, J. B., and Steyvers, M. (2007b). Topics
els. As feature norms rely on explicit human judgment, they            in semantic representation. Psychological Review, 114:2007.
are able to capture the dimensions of meaning that are psy-         Heit, E. and Barsalou, L. (1996). The instantiation principle in nat-
                                                                       ural language categories. Memory, (4):413–451.
chologically salient. Corpus-based models on the other hand         Landauer, T. and Dumais, S. (1997). A solution to Plato’s problem:
learn in an unsupervised fashion and require no human in-              The latent semantic analysis theory of acquisition, induction, and
volvement or external sources of knowledge.                            representation of knowledge. Psychological review, 104(2):211–
                                                                       240.
   Overall we find LSA to be a reasonable approximation             Lin, D. (2001). LaTaT: Language and text analysis tools. In Pro-
of feature norms, superior to both LDA and the syntacti-               ceedings of the 1st Human Language Technology Conference,
cally more aware dependency vectors. This result is consis-            pages 222–227, San Francisco, CA.
tent across models (exemplar vs. prototype) and tasks. Im-          McRae, K., Cree, G. S., Seidenberg, M. S., and McNorgan, C.
                                                                       (2005). Semantic feature production norms for a large set of
portantly, the LSA model is language-independent and capa-             living and non-living things. Behavioral Research Methods In-
ble of extracting representations for an arbitrary number of           struments & Computers, 37(4):547–559.
words. By contrast, feature norms tend to cover a few hundred       Medin, D. L. and Schaffer, M. M. (1978). Context theory of classi-
words and involve several subjects over months or years. Al-           fication learning. Psychological Review, 85(3):207–238.
beit in most cases better than our models, feature norms them-      Nosofsky, R. M. (1988). Exemplar-based accounts of relations
                                                                       between classification, recognition, and typicality. Journal of
selves yield relatively low performance on all three tasks we          Experimental Psychology: Learning, Memory, and Cognition,
attempted using either an exemplar or prototype model (see             14:700–708.
Figures 1 and 2). We believe the reasons for this are twofold.      Nosofsky, R. M. (1992). Exemplars, prototypes, and similarity rules.
Firstly, McRae et al.’s 2005 norms were not created with cat-          In Healy, A. F., Josslyn, S. M., and Shiffrin, R. M., editors, From
                                                                       Learning Theory to Connectionist Theory: Essays in Honor of
egorization in mind, we may obtain better predictions with             William K. Estes, volume 1, pages 149–167. Hillsdale, NJ: Erl-
some form of feature weighting (see Storms et al. 2000). Sec-          baum.
ondly, the tasks seem hard even for humans as corroborated          Padó, S. and Lapata, M. (2007). Dependency-based construction of
                                                                       semantic space models. Computational Linguistics, 33(2):161–
by our reliability ratings.                                            199.
   The differences in performance between LSA, LDA, and             Phan, X.-H., Nguyen, L.-M., and Horiguchi, S. (2008). Learning
DV can be explained by differences between the notion of               to classify short and sparse text & web with hidden topics from
similarity implicit in each. Closely related words in LDA ap-          large-scale data collections. In Proceedings of The 17th Interna-
                                                                       tional World Wide Web Conference (WWW 2008), pages 91–100.
pear in the same topics, which are often corpus-specific and        Rosch, E. (1973). Natural categories. Cognitive Psychology, pages
difficult to interpret; words belonging to different categories        328–350.
may be deemed similar yet be semantically unrelated. By             Sanborn, A. N., Griffiths, T. L., and Navarro, D. J. (2006). A more
contrast, the poor performance of the DV model is somewhat             rational model of categorization. In Proceedings of the Twenty-
                                                                       Eighth Annual Conference of the Cognitive Science Society.
disappointing. Our experiments used a large number of de-           Sloman, S. A. and Ripps, L. J. (1998). Similarity as an explanatory
pendency relations; it is possible that a more focused seman-          construct. Cognition, (65):87–101.
tic space with a few target relations may be more appropriate.      Smith, E. and Medin, D. (1981). Categories and Concepts. Harvard
                                                                       University Press.
   Finally, our simulation studies reveal that an exemplar
model is a better predictor of categorization performance than      Snow, R., OĆonnor, B., Jurafsky, D., and Ng, A. (2008). Cheap and
                                                                       fast – but is it good? evaluating non-expert annotations for natural
a prototype one. This result is in agreement with previous             language tasks. Proceedings of EMNLP 2008.
studies (Voorspoels et al., 2008; Storms et al., 2000) show-        Steyvers, M. (2009). Combining feature norms and text data with
ing that exemplar models perform consistently better across            topic models. Acta Psychologica. (in press).
a broad range of natural language concepts from different se-       Storms, G., Boeck, P. D., and Ruts, W. (2000). Prototype and
                                                                       exemplar-based information in natural language categories. Jour-
mantic domains. This finding is also in line with studies in-          nal of Memory and Language, 42:51–73.
volving artificial stimuli (e.g., Nosofsky 1992).                   Vanpaemel, W., Storms, G., and Ons, B. (2005). A varying abstrac-
   Directions for future work are two-fold. Firstly, we wish           tion model for categorization. In Proceedings of the 27th Annual
to explore alternative meaning representations more suited to          Conference of the Cognitive Science Society.
the categorization task. A potential candidate is the feature-      Voorspoels, W., Vanpaemel, W., and Storms, G. (2008). Exemplars
                                                                       and prototypes in natural language concepts: A typicality-based
topic model (Steyvers, 2009; Andrews et al., 2009), in which           evaluation. Psychonomic Bulletin & Review, 15(3):630–637.
documents are represented by a mixture of learned topics            Zeigenfusse, M. D. and Lee, M. D. (2009). Finding the features that
in addition to predefined topics derived from feature norms.           represent stimuli. Acta Psychological. (in press).
                                                                1921

