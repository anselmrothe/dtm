UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Spiking Neuron Model of Serial-Order Recall
Permalink
https://escholarship.org/uc/item/0g33q7kj
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Choo, Feng-Xuan
Eliasmith, Chris
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                A Spiking Neuron Model of Serial-Order Recall
                                              Feng-Xuan Choo (fchoo@uwaterloo.ca)
                                            Chris Eliasmith (celiasmith@uwaterloo.ca)
                                       Center for Theoretical Neuroscience, University of Waterloo
                                                     Waterloo, ON, Canada N2L 3G1
                               Abstract                                vectors such that the result is another vector that is similar to
                                                                       the original input vectors. Third, a binding operation (⊗) is
   Vector symbolic architectures (VSAs) have been used to model
   the human serial-order memory system for decades. Despite           used to combine vectors such that the result is a vector that
   their success, however, none of these models have yet been          is dissimilar to original vectors. Last, an approximate inverse
   shown to work in a spiking neuron network. In an effort to take     operation (denoted with ∗ , such that A∗ is the approximate in-
   the first step, we present a proof-of-concept VSA-based model
   of serial-order memory implemented in a network of spiking          verse of A) is needed so that previously bound vectors can be
   neurons and demonstrate its ability to successfully encode and      unbound.
   decode item sequences. This model also provides some insight                                 A ⊗ B ⊗ B∗ ≈ A                       (1)
   into the differences between the cognitive processes of mem-
   ory encoding and subsequent recall, and establish a firm foun-      Just like addition and multiplication, the VSA operations are
   dation on which more complex VSA-based models of memory
   can be developed.                                                   associative, commutative, and distributive.
   Keywords: Serial-order memory; serial-order recall; vector             The class of VSA used in this model is the Holographic
   symbolic architectures; holographic reduced representation;         Reduced Representation (HRR) (Plate, 2003). In this repre-
   population coding; LIF neurons; neural engineering frame-           sentation, each element of an HRR vector is chosen from a
   work
                                                                       normal distribution with a mean of 0, and a variance of 1/n
                                                                       where n is the number of elements there are in the vector. The
                           Introduction
                                                                       standard addition operator is used to perform the superposi-
The human memory system is able to perform a multitude                 tion operation, and the circular convolution operation is used
of tasks, one of which is the ability to remember and recall           to perform the binding operation. The circular convolution of
sequences of serially ordered items. In human serial recall            two vectors can be efficiently computed by utilizing the Fast
experiments, subjects are presented items at a fixed interval,         Fourier Transform (FFT) algorithm:
typically in the range of two items per second up to one item
every 4 seconds. After the entire sequence has been presented                            x ⊗ y = F −1 (F (x)    F (y)),              (2)
the subjects are then asked to recall the items presented to
them, either in order (serial recall), or in any order the sub-        where F and F −1 are the FFT and inverse FFT operations
ject desires (free recall). Plotting the recall accuracy of the        respectively, and is the element-wise multiplication of the
subjects, experimenters often obtain a graph with a distinc-           two vectors. The circular convolution operation, unlike the
tive U-shape. This unique shape arises from what is known              standard convolution operation, does not change the dimen-
as the primacy and recency effects. The primacy effect refers          sionality of the result vector. This makes the HRR extremely
to the increase in recall accuracy the closer the item is to the       suitable for a neural implementation because it means that the
start of the sequence, and the recency effect refers to the same       dimensionality of the network remains constant regardless of
increase in recall accuracy as the item gets closer to the end         the number of operations performed.
of the sequence.
   Many models have been proposed to explain this peculiar                The VSA-based Approach to Serial Memory
behaviour in the recall accuracy data. Here we will concen-            There are multiple ways in which VSAs can be used to
trate on one class of models which employ vector symbolic              encode serially ordered items into a memory trace. The
architectures (VSAs) to perform the serial memory and re-              CADAM model (Liepa, 1977) provides a simple example of
call. Using VSAs to perform serial memory tasks would be               how a sequence of items can be encoded as a single mem-
insufficient however, if the VSA-based model cannot be im-             ory trace. In the CADAM model, the sequence containing
plemented in spiking neurons, and thus, cannot be used to              the items A, B, and C would be encoded as in single memory
explain what the brain is actually doing. In this paper, we            trace, MABC as follows:
thus present a proof-of-concept VSA-based model of serial
recall implemented using spiking neurons.                                                MA = A
                                                                                        MAB = A + A ⊗ B
               Vector Symbolic Architecture                                            MABC = A + A ⊗ B + A ⊗ B ⊗ C
There are four core features of vector symbolic architectures.
First, information is represented by randomly chosen vectors              The model presented in this paper, however, takes inspira-
that are combined in a symbol-like manner. Second, a super-            tion from behavioural data obtained from macaque monkeys.
position operation (here denoted with a +) is used to combine          This data suggests that each sequence item is encoded using
                                                                   2188

ordinal information (Orlov, Yakovlev, Hochstein, & Zohary,           the population, it should be possible to derive decoding vec-
2000), rather than being “chained” together as in the CADAM          tors φ that can be used to estimate the original input. Elia-
model. To achieve this, additional vectors are used to repre-        smith and Anderson (2003) demonstrate that these decoding
sent the ordinal information of each item. In the subsequent         vectors can be found using the following equation.
equations, this ordinal vector is represented as Pi , where i
indicates the item’s ordinal number in each sequence. The                        φ = Γ−1 ϒ, where
memory trace MABC would thus be computed like so:
                                                                                      Z                                 Z                  (9)
                                                                               Γi j =    ai (x)a j (x) dx         ϒi =    ai (x)x dx
                  MA = P1 ⊗ A                                 (3)
                 MAB = P1 ⊗ A + P2 ⊗ B                        (4)    By weighting the decoding vectors with the post-synaptic cur-
                                                                     rent h(t) generated by each spike, it is then possible to con-
                MABC = P1 ⊗ A + P2 ⊗ B + P3 ⊗ B               (5)
                                                                     struct x̂(t), an estimate of the input vector. Equation (10)
   The encoding strategy presented above does not seem to            demonstrates how this is achieved. The parameters used to
have any mechanism by which to explain the primacy or re-            generate the shape of h(t) is determined by the neurophysiol-
cency effects seen in human behavioural data. In order to            ogy of the neuron population.
achieve these effects, additional components are added to the
model. These components are discussed further below.                                     x̂(t) = ∑δ(t − tin ) ∗ h(t)φi
                                                                                                   i,n
                  Neural Representation                                                        = ∑h(t − tin )φi                           (10)
To implement any of these models, we need to determine how                                         i,n
a vector can be represented by a population of spiking neu-
                                                                     The encoding and decoding vectors also provides a method
rons. In 1986, Georgopoulos et al. demonstrated that in the
                                                                     by which the optimal connection weights between two neu-
brain, 2D movement directions are encoded by a large pop-
                                                                     ral populations can be. If for example, the transformation
ulation of neurons, with each neuron being most active for
                                                                     between two populations of neurons is a simple scaling oper-
one specific direction – their preferred direction. The activity
                                                                     ation, where the output of the second group of neurons should
of each neuron would then indicate the similarity of the in-
                                                                     be Cx, then the connection weights w between the populations
put vector to each neuron’s preferred direction vector. Since
                                                                     should be
the movement direction is essentially a two-dimensional vec-
                                                                                                  wi j = Cα j φ̃ j φi                     (11)
tor, this method of vector representation can be extended to
multiple dimensions as well. For a population of neurons, the        Extending Equation (7) for linear operations is also straight-
current J flowing into neuron i can then be calculated by the        forward. Consider three neural populations: one to represent
following equation.                                                  the input x, another to represent the input y, and a third that we
                                                                     wish to have compute the linear combination Cx + Dy. The
                    Ji (x) = αi (φ̃i · xi ) + Jibias          (6)
                                                                     activity of the neurons in final population can be determined
In the above equation, the dot product computes the similarity       by
between the input vector x and the neuron’s preferred direc-                                 "                                        #
tion vector φ̃. The neuron gain is denoted by α, while J bias
denotes a fixed background input current. The current Ji can
                                                                       ck (Cx + Dy) = Gk      ∑wki ai (x) + ∑wk j b j (y) + Jkbias      , (12)
                                                                                               i                  j
then be used as the input to any neuron model G[·] to ob-
tain the activity for neuron i. In this model, we use the leaky      where ai , b j , and ck are the activities of the neurons in the
integrate-and-fire (LIF) neuron model, characterized as such:        first, second and third neural populations respectively. Em-
                                                                     ploying Equation (11), the synaptic connection weights can
                                                1
         ai (x) = Gi [Ji (x)] =                   
                                                       th
                                                           , (7)    also be determined. Letting wki be the connection weights
                                τre f − τRC ln 1 − JJi (x)           between the first and third population, and wk j be the connec-
                                                                     tion weights between the second and third population, they
where ai (x) is the average firing rate of the neuron i, τre f       work out to be:
is the neuron refractory time constant, τRC is the neuron RC
                                                                                                                                   y
time constant, and Jth is the neuron threshold firing current.                   wki = αk φ̃k φxi       and         wk j = αk φ̃k φ j     (13)
For a time-varying input x(t), the equations remain the same,
with the exception that the activity of the neuron is no longer      Note that in the equation above, the superscripts serves to
an average firing rate, but rather a spike train:                    disambiguate the decoders, where φx signifies the decoders
                                                                     that represent x, and likewise for φy . Eliasmith and Anderson
                      a(x(t)) = ∑ δ(t − tn )                  (8)    (2003) go into greater detail on how to use this general frame-
                                   n
                                                                     work, known as the Neural Engineering Framework, to derive
Since the spike train represents the neuron’s response to the        the appropriate decoders and connection weights to perform
input vector x, given the spike trains from all the neurons in       arbitrary non-linear operations as well.
                                                                 2189

                     The Neural Model                                     From the above encoding equations, several issues become
The neural model implemented in this paper is divided into             evident. First, two operations need to be implemented – a cir-
two neural processes. One encodes an item sequence into a              cular convolution and an addition operation. Second, a mem-
single memory trace, and the other decodes an encoded mem-             ory module is needed to hold the value of Mi−1 while the new
ory trace to retrieve its constituent items.                           memory trace Mi is computed. With these components, and
                                                                       the rehearsal and decay mechanisms described above, a high
Sequence Encoder                                                       level block diagram of the complete encoding network can be
Analysis of Equations (3) to (5) show that the memory trace            constructed, as shown in Figure 1.
for an arbitrary sequence of items can be constructed by com-
puting the convolution of the last item vector with its ordinal                                                   Rehearsal
                                                                                                     
                                                                                         Ii                      Component
vector, and then adding the result of the convolution to the
                                                                                         Pi
memory trace of the sequence less the final item. From this,
a generic sequence encoding equation can be derived (from
here on referred to as the basic encoding equation).                                           Bi-1        Ri-1
                       Mi = Mi−1 + Pi ⊗ Ii                    (14)
                                                                                          +                        +
In the equation above, Mi represents the memory trace after
encoding the ith item. Pi and Ii represents the ith item’s ordinal                 Memory Module                Memory
vector and item vector respectively.                                                 (with decay)               Module
   As mentioned previously, the encoding equation in its ba-
sic form does not account for the primacy and recency effects                               Bi                  Ri
seen in human behavioural data. To achieve the primacy ef-                                                           Encoded
fect, rehearsal is simulated by adding an additional weighted                                                Mi
                                                                                 Input Buffer                        Sequence
copy of the old memory trace to the memory trace being cal-                      Component
                                                                                                      +               Vector
culated for the current item. In essence, as each item is re-
hearsed, a weighted copy of the item is added to the memory
trace to “boost” the item’s representation within the mem-                 Figure 1: Encoding network functional block diagram.
ory trace. In the equation below, the memory trace of the
rehearsal-based encoding is denoted by Ri and the weight ap-
plied to the rehearsed contribution of the old memory trace            Sequence Decoder
is denoted by α. In the model implemented for this paper, α            The decoding process is much simpler than the encoding pro-
was set to 0.3.                                                        cess. The first step of the decoding process is to convolve
                                                                       the encoded memory trace with the inverse of the desired or-
                   Ri = Ri−1 + Pi ⊗ Ii + αRi−1                (15)     dinal vector. For example, if the system is trying to decode
                      = (1 + α)Ri−1 + Pi ⊗ Ii                 (16)     the second item in the sequence, the encoded memory trace
                                                                       would be convolved with the inverse of P2 . Next, the result
   To achieve the recency effect, an separate memory compo-
                                                                       of this convolution is fed to a cleanup memory module. The
nent is added to play the role of a sensory input buffer. The
                                                                       cleanup memory module contains a copy of all the item vec-
input buffer encodes items in a similar fashion to Equation
                                                                       tors in the original sequence, and when given an input, will
(14) with a decay added to the old memory trace. This decay
                                                                       determine which of the original item vectors best matches the
causes the input buffer to store only the most recently pre-
                                                                       input vector. An example of this decoding process follows.
sented items, thus mimicking the basic recall characteristics
                                                                       To simplify the example, only the basic encoding equation is
of the human working memory system. In the neural imple-
                                                                       used.
mentation of this model, the decay is achieved by tuning the
integrators used in the memory modules to slowly drift to zero               MABC = P1 ⊗ A + P2 ⊗ B + P3 ⊗ B
if no additional input is applied to them. Equation (17) illus-
trates how this decay can be represented mathematically, with                   CB = MABC ⊗ P2∗
the memory trace of the input buffer represented by Bi , and                        = P1 ⊗ A ⊗ P2∗ + P2 ⊗ B ⊗ P2∗ + P3 ⊗ B ⊗ P2∗
the rate of decay represented by β.                                                 ≈ P1 ⊗ A ⊗ P2∗ + B + P3 ⊗ B ⊗ P2∗
                       Bi = βBi−1 + Pi ⊗ Ii                   (17)               IB = cleanup(CB ) ≈ B
   The final memory trace of the encoded item sequence is              From the example above, we see that convolving the mem-
then computed by combining the memory trace from the                   ory trace MABC with the inverse of P2 results in a vector with
rehearsal component and the memory trace from the input                the desired item vector B combined with the unwanted vec-
buffer component.                                                      tors (P1 ⊗ A ⊗ P2∗ ) and (P3 ⊗ B ⊗ P2∗ ). However, since the
                           Mi = Ri + Bi                       (18)     cleanup memory module only contains the item vectors from
                                                                   2190

the original sequence and not the superfluous vectors, feeding        Cleanup Memory The cleanup memory network used in
the result of the convolution through the cleanup memory iso-         this model is an extension of the cleanup memory presented
lates the item vector B, producing the desired result. Figure 2       in (Stewart, Tang, & Eliasmith, 2009). In essence, the im-
illustrates the high level block diagram used to implement the        plementation of cleanup memory involves creating multiple
decoding network.                                                     neural populations, each assigned to one item vector from the
                                                                      original item sequence. The preferred direction vectors φ̃ for
                                                                      each neuron in one population is predefined to match the item
       Menc                                         Recalled
                                    Cleanup                           vector it is meant to clean up. From Equation (6), we see that
         Pi                       Memory
                                                      Item
                                                                      the the similarity (dot product) is calculated to determine the
                (*)                                  Vector
                                                                      activity of the neuron. By predefining φ̃, we can then deter-
                                                                      mine the similarity of the decoded item vector to each of the
                                                                      original item vectors, thus determining which of the original
    Figure 2: Decoding network functional block diagram.
                                                                      item vectors best matches the decoded item vector.
Performing the Binding Operation Referring back to                    Combining the Encoder and Decoder
Equation (2) we see that the binding operation can be cal-            Getting the spiking neuron model to encode a sequence, and
culated using the FFT and IFFT algorithms, so the first step          subsequently decode the memory trace is achieved by chain-
to implementing the binding operation in neurons is to imple-         ing the encoder and the decoder together. Control signals are
ment these two operations. The equations that compute the             used to ensure that the decoding network only commences af-
FFT and IFFT algorithms are as follows:                               ter the encoder has finished encoding the last item vector. Fig-
                                                                      ure 3 shows the results of the complete network encoding and
                      N−1      2πi
                                                                      decoding a example twenty-dimensional 4-itemed sequence.
    FFT :       Xk =   ∑ xn e− N kn      k = 0, ..., N − 1
                      n=0
                                                             (19)
                         N−1
                       1         2πi
   IFFT :        xn = ∑ Xk e N kn         n = 0, ..., N − 1
                      N k=0
Taking a closer look at the equations above, we see that they
can be implemented efficiently as a multiplication between
the input vector and a matrix containing the FFT (or IFFT)
coefficients. From Equation (11), we can then set the synap-
tic connection weight matrix as the Fourier transform coef-
ficients to calculate the required FFT and IFFT operations.
The one caveat to this approach is that the real and imagi-
nary components of the Fourier transform have to calculated
separately and then recombined (with the appropriate sign
changes) when the final result is calculated.
   With the neural implementation of the Fourier transforms
solved, the implementation of the circular convolution bind-
ing operation becomes trivial since the only other opera-             Figure 4: Plot of the recall accuracy data comparing results
tion needed is an element-wise multiplication. This can be            from human behavioural studies (from Henson et al. (1996),
achieved by utilizing multiple neural populations, each han-          Figure 1), an ideal model implemented in Matlab R , and the
dling one element in the element-wise multiplication.                 spiking neuron model.
The Memory Module Since the circular convolution and
addition operations are essentially feed-forward neural net-
works, the memory module in this model needs to be able                                          Results
to drive the network with a constant value and store the new          The results of the simulation of the spiking neuron imple-
value at the same time. This is achieved by the use of gated          mentation of the ordinal serial encoding process is displayed
integrators. When the integrator is not being gated, it attempts      in Figure 4. From the graph it can be seen that both the
to match the value of the input signal. When the integrator is        ideal Matlab R -implemented model and the spiking neuron
gated, it no longer responds to the input value, and outputs          model are a good match to the human data. The slightly re-
the previously stored value. By placing two gated integrators         duced primacy in the neuronal implementation suggests that
in parallel controlled by complementary gating signals, the           the simplistic implementation of the rehearsal mechanism can
memory module is able to simultaneously store the new input           be improved. Figure 5 compares the transposition gradients –
value while outputting the previously stored value.                   which is the count of the recall occurrences of each item for
                                                                  2191

Figure 3: Simulation results from the spiking neuron implementation of the sequence encoder network. A 4-itemed sequence
of 20-dimensional item vectors was presented to the network at a half-second interval (two items per second).
(Left) The output of the encoder, Mi , showing the encoded memory trace for each item vector presented. Referring to Equa-
tion (18) , the graph at t = 0.5 seconds shows M1 = R1 + B1 , the graph at t = 1 second shows M2 = R2 + B2 , and so forth for.
The final encoded memory trace for the entire sequence is the output of the encoder network at t = 2 seconds.
(Center) The spike raster plot of the neurons in the output neuron population of the encoder network as it is encoding the
sequence in the top figure. The spike raster is displayed for every 20th neuron.
(Right) The similarity plot of each extracted item vector to each one of the four original item vectors. The similarity value
between the vectors is obtained using the dot product operation. The graph shows the network correctly identifying the first,
second, and last item. The third item is incorrectly identified because the similarity measures of the first three items are too
close together for the system to accurately distinguish the correct answer.
Figure 5: Plot of the transposition gradients comparing results from human behavioural studies (from Henson et al. (1996),
Figure 2, for non-confusable items), an ideal model implemented in Matlab R , and the spiking neuron model. Comparing the
plots, both the ideal model and the spiking neuron model are able to replicate the transposition curves in the human data.
                                                               2192

each serial position – also reveals that both the ideal imple-       trace within the inter-item interval (time between each item
mentation and the spiking neuron implementation are able to          presentation). Such a rehearsal mechanism will also enable
reproduce the transposition effects seen in humans. Both of          the model to be compared with serial recall studies involving
these simulations were run using six-itemed sequences con-           list sizes that exceed the typical human memory span of 4 to
sisting of fifty-dimensional HRR vectors, and were run for an        7 items.
average of 200 trials each.                                             Additionally, the current implementation of cleanup mem-
                                                                     ory has a fixed vocabulary of item vectors that is predefined
                          Discussion                                 when the network is created. This means that the items in
From the results it can be seen that both the ideal implemen-        cleanup memory are static and do not change over time. It
tation and the spiking neuron model demonstrate the ability          seems inconceivable that this is what occurs in the brain.
to reproduce the primacy, recency, and transposition effects         Rather, future cleanup memory implementations should be
seen in human data. Furthermore, unlike other models which           dynamic, with the ability to “load” and “unload” arbitrary
entail a host of tunable parameters to fit the human data, this      item vectors into its vocabulary.
model only utilizes two tunable parameters: the amount of
                                                                                              References
contribution to the memory trace in the rehearsal component,
and the decay rate of the input buffer component.                    Baddeley, A. (2007). Working memory, thought, and action.
    The model presented here also provides some insight into            Oxford: Oxford University Press.
the neurophysiological requirements of serial memory. It             Eliasmith, C., & Anderson, C. H. (2003). Neural engineer-
demonstrates the need for a working memory system capa-                 ing: computation, representation, and dynamics in neuro-
ble of simultaneous storage and retrieval. This model also              biological systems. Cambridge, MA: The MIT Press.
maps on very well to Baddeley’s model of working mem-                Georgopoulos, A. P., Schwartz, A., & Kettner, R. E. (1986).
ory (Baddeley, 2007), with the input buffer component acting            Neuronal population coding of movement direction. Sci-
as the phonological loop, and the rehearsal component func-             ence, 233, 1416–1419.
tioning as the episodic buffer.                                      Henson, R. N., Norris, D. G., Page, M. P., & Baddeley, A. D.
    Despite their complexity, there are advantages of creating a        (1996). Unchained memory: error patterns rule out chain-
spiking neuron model in comparison to theoretical models, or            ing models of immediate serial recall. The Quarterly Jour-
models implemented using rate neurons. It provides the abil-            nal of Experimental Psychology, 49A(1), 80–115.
ity to compare the spike data of the model to data collected         Liepa, P. (1977). Models of content addressable distributed
from neural recordings. For example, data collect in Warden             associative memory. (Unpublished manuscript)
and Miller’s 2007 paper shows that the neurons change their          Orlov, T., Yakovlev, V., Hochstein, S., & Zohary, E. (2000,
preferred items as more items are introduced into the system.           March). Macaque monkeys categorize images by their or-
Although the analysis has yet to be completed at the time this          dinal number. Nature, 404.
paper was written, it can be inferred that because the encoded       Plate, T. A. (2003). Holographic reduced representations:
sequence vector changes as more items are added, a neuron               distributed representations for cognitive structures. Stan-
that is responsive to one configuration of the sequence vector          ford, CA: CSLI Publications.
would either be less responsive or not responsive at all when        Stewart, T. C., Tang, Y., & Eliasmith, C. (2009). A biologi-
a new item is added – changing the configuration of the en-             cally realistic cleanup memory: autoassociation in spiking
coded sequence vector – as it does in this model.                       neurons. 9th International Conference on Cognitive Mod-
                                                                        elling.
    Several studies (e.g. Chein & Fiez, 2001) have also iden-
                                                                     Warden, M. R., & Miller, E. K. (2007). The representation of
tified brain areas that are active during serial memory tasks.
                                                                        multiple objects in prefrontal neuronal delay activity. Cere-
Moreover, the studies have demonstrated that there are simi-
                                                                        bral Cortex, 17, 141–150.
larities and more importantly, differences, between the areas
of activity during the encoding phase and recall phase. By as-
signing different components of the model to different brain
areas (for example, the input buffer component to the tempo-
ral lobe, near the auditory cortex, and the rehearsal compo-
nent to the lateral prefrontal cortex), it would be possible to
determine if the pattern of activities recorded in these studies
matches the pattern of activities produced by this model.
                        Future Work
As mentioned in the results section, the performance of the
rehearsal component needs to be improved slightly. Possible
ways of doing this is by having an active rehearsal mecha-
nism which decodes and then re-encodes the stored memory
                                                                 2193

