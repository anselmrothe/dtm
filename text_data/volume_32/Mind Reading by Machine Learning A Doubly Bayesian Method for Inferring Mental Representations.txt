UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Mind Reading by Machine Learning: A Doubly Bayesian Method for Inferring Mental
Representations
Permalink
https://escholarship.org/uc/item/3ph1p80s
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Huszar, Ferenc
Noppeney, Uta
Lengyel, Mate
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                 Mind Reading by Machine Learning:
      A Doubly Bayesian Method for Inferring Mental Representations
                                        Ferenc Huszár (fh277@eng.cam.ac.uk)
       Computational and Biological Learning Lab, Dept Engineering, U Cambridge, Cambridge CB2 1PZ, UK
                               Uta Noppeney (uta.noppeney@tuebingen.mpi.de)
           Max Planck Institute for Biological Cybernetics, Spemannstrasse 41, Tübingen 72076, Germany
                                     Máté Lengyel (m.lengyel@eng.cam.ac.uk)
       Computational and Biological Learning Lab, Dept Engineering, U Cambridge, Cambridge CB2 1PZ, UK
                           Abstract                               of human learning and cognition to model the relevant
                                                                  mental content of a subject either implicitly or explicitly
   A central challenge in cognitive science is to measure and
   quantify the mental representations humans develop – in        as a ‘subjective’ distribution over possible stimuli (Chater
   other words, to ‘read’ subject’s minds. In order to elimi-     et al., 2006; Sanborn & Griffiths, 2008). In this study
   nate potential biases in reporting mental contents due         we adopted this representation, and our goal was to
   to verbal elaboration, subjects’ responses in experiments
   are often limited to binary decisions or discrete choices      estimate subjects’ subjective distributions solely from
   that do not require conscious reflection upon their mental     their responses in simple binary decision tasks without
   contents. However, it is unclear what such impoverished        making any assumptions about the process by which those
   data can tell us about the potential richness and dy-
   namics of subjects’ mental representations. To address         subjective distributions were acquired, i. e. learning.
   this problem, we used ideal observer models that for-             Ideal observer models are widely used for explaining
   malise choice behaviour as (quasi-)Bayes-optimal, given        human behaviour in various psychophysics tasks (Geisler,
   subjects’ representations in long-term memory, acquired
   through prior learning, and the stimuli currently avail-       2003). They formalise (quasi-)optimal decision making
   able to them. Bayesian inversion of such ideal observer        strategies given the information available to subjects and
   models allowed us to infer subjects’ mental representation     their background knowledge about the task, which in
   from their choice behaviour in a variety of psychophysical
   tasks. The inferred mental representations also allowed        our case includes their subjective distributions. While
   us to predict future choices of subjects with reasonable       previous studies mostly used ideal observer models to de-
   accuracy, even in tasks that were different from those in      termine optimal performance in particular tasks to which
   which the representations were estimated. These results
   demonstrate a significant potential in standard binary         human performance could then be compared, we treat
   decision tasks to recover detailed information about sub-      them as stochastic models formalising the link between
   jects’ mental representations.                                 subjective distributions (the unobserved variable), and
                                                                  test stimuli and responses (the observed variables). Our
                       Introduction                               main observation is that such models can be used to
Cognitive science studies the mental representations hu-          provide the likelihood in a Bayesian statistical analysis
mans (and other animals) develop and the way these                of subjective distributions, thus enabling one to infer
representations are used to perform particular tasks. A           mental contents from task responses in a principled way.
central challenge is to measure and quantify such men-               We term our approach doubly Bayesian, as we assume
tal representations experimentally – in other words, to           that subjects act as quasi-ideal observers, which entails
‘read’ subjects’ minds. A classical approach to this is           Bayesian inference on their side; and then we use these
to ask subjects directly to report their mental contents          ideal-observer models in a Bayesian framework to infer a
verbally. Unfortunately, this procedure is prone to intro-        posterior distribution of possible subjective distributions.
ducing biases arising from verbal processing, and from
the educational and cultural backgrounds of subjects                     Inferring subjective distributions
(Ericsson & Simon, 1980; Russo et al., 1989). In order            The graphical model (Koller & Friedman, 2009) in Fig. 1A
to eliminate these biases, an alternative approach is to          describes our model of a subject’s behaviour in a session
limit subjects’ responses to simple binary decisions or           of a psychophysics experiment. We assume that the sub-
discrete choices that do not require conscious reflection         ject entertains a subjective distribution P over possible
upon their mental contents. However, it is unclear what           stimuli, and that this distribution does not change over
such impoverished data can tell us about the potential            the analysed session. In trial i of the experiment, the
richness and dynamics of subjects’ mental contents.               subject is presented a set of test stimuli Si and gives a re-
   A powerful computational framework formalises the              sponse ri . The value of ri depends on the current stimuli
goal of learning as estimating the probability distribution       Si , the subjective distribution P, and ‘link’ parameters
or density of stimuli (Hinton & Sejnowski, 1986; Dayan            ΘO describing further aspects of observation and decision
& Abbott, 2001). This motivates many formal theories              making, such as attention, perceptual noise, etc.
                                                              2810

 A             general model       B              CAT                    imal posterior probability. As a more realistic model
                                                                         of human decision making we used a soft-max function
                                               P        Q
                                                                         (parametrised by ΘO ) of log posterior probabilities, that
                    P         ΘO                                         describes quasi-optimal decision making (Sanborn & Grif-
                                              s∗       s∗                fiths, 2008; Orbán et al., 2008).
                                                                            Our goal is to estimate latent parameters P and ΘO
                                               s        s                from a series of stimulus-response pairs {Si , ri }N     i=1 . As
          Si                  ri                                         responses given in subsequent trials of the experimental
                                              H1       H2
                         i = 1...N
                                            ‘cat. 1’ ‘cat. 2’
                                                                         session are assumed to be conditionally independent, the
                                                                         likelihood of latent parameters becomes
 C               DISC              D             PREF
                                                                                                            N
          P                P          P       Q        Q      P                                            Y
                                                                                  p(r1:N |S1:N , P, ΘO ) =     p(ri |Si , P, ΘO )
                                                                                                           i=1
         s∗12         s∗1      s∗2    s∗1     s∗2      s∗1    s∗2
                                                                         To allow for full Bayesian inference we specified prior
     s1       s2      s1       s2     s1      s2       s1     s2         distributions over the subjective distribution, P, and link
                                                                         parameters, ΘO . We chose to model subjective distribu-
         H1                H2             H1               H2            tions as mixtures of Gaussians (MoG’s). This parametric
       ‘same’         ‘different’  ‘s1 is familiar’ ‘s2 is familiar’     family of distributions is flexible enough to model com-
                                                                         plex subjective distributions in low dimensional feature
Figure 1: A, Graphical model describing the subject’s                    spaces and allows for analytical computation of likelihood
behaviour in an experimental session of N consecutive                    ratios in the binary tasks considered here. Importantly,
trials. We assume that the subject represents a subjec-                  this prior reflected no information about the distribution
tive distribution, P, over possible stimuli, and in trial                of stimuli with which subjects were trained (i. e. the dis-
i their response ri depends on the currently observed                    tribution to which their subjective distributions could
test stimuli Si , their subjective distribution, P, and some             be expected to be close), except for the general domain
other parameters influencing their responding, ΘO . Our                  of possible stimulus values. The MoG representation is
goal is to infer P and ΘO from the observed sequence                     not a vital part of our general approach: other repre-
of stimulus-response pairs. B-D, Generative models                       sentations and priors may be more appropriate in some
for the three task types (CAT, DISC, and PREF, see                       cases.
descriptions under ‘Experimental data sets’). Subjects                      Given the prior and the likelihood defined above, we
assume that their observations, s, are perceptual noise-                 inferred a posterior over P and ΘO via Bayes’ rule:
corrupted versions of the ‘true’ stimuli, s∗ , sampled by                                                         N
the experimenter from a distribution that is the same as                    p(P, ΘO |r1:N , S1:N ) ∝ p(P)p(ΘO )
                                                                                                                 Y
                                                                                                                      p(ri |Si , P, ΘO )
their subjective distribution, P, or an alternative distribu-                                                    i=1
tion, Q (which is assumed to be uniform for tractability),
depending on the particular hypothesis, H.                               Unfortunately, calculating the posterior exactly is in-
                                                                         tractable, so we have to resort to approximate inference
                                                                         techniques, for which we implemented a Hamiltonian
   In order to quantify the dependence between subjects’                 Monte Carlo algorithm (Neal, 2010).
choices and their subjective distributions, response prob-
abilities, p(ri |Si , P, ΘO ), were specified by quasi-ideal ob-         Experimental data sets
server models. These models formalise subjects’ choices                  Two experimental data sets were analysed, each collected
as functions of the posterior probabilities of the two hy-               using simple visual stimuli and requiring binary responses
potheses corresponding to either response being correct.                 from subjects.
 Each hypothesis amounts to a different model of how
stimuli might have been generated, and so the posterior                  One-dimensional feature space The first set of ex-
over hypotheses is inferred by a Bayesian inversion of                   perimental data was the fish categorisation data set col-
these generative models. Fig. 1B-D shows such generative                 lected by Sanborn & Griffiths (2008). In this experiment,
models in three tasks considered later in this paper (for                the stimuli used were schematic images of fish of fixed
more detail, see the supplementary material1 ). Once pos-                length and variable height, i. e. the relevant feature space
terior probabilities are available, the statistically optimal,           was one dimensional (see Fig. 2A). Subjects were trained
although psychologically unrealistic, strategy would be                  (with corrective feed-back) in a supervised binary cat-
to deterministically choose the response with the max-                   egorisation task (CAT) to distinguish fish drawn from
                                                                         a Gaussian training distribution from fish drawn from
    1
      available online at mlg.eng.cam.ac.uk/ferenc/mindreading           a uniform distribution. The mean and variance of the
                                                                     2811

training distribution was varied across four conditions         subjects did not learn the training distribution in this
(Fig. 2B, red curves), with 9-11 subjects in each condi-        experiment very well (see also below), although some
tion. Subjects also performed a stimulus preference task        resemblance between training and inferred subjective dis-
(PREF), in which they had to choose the stimulus which          tributions were recovered for a few subjects (e. g. subjects
seemed more likely to be drawn from the training dis-           1, 4, 11 and 12). The subjective distributions inferred for
tribution. In this task, no feedback was provided. The          the same subject in the two different tasks also revealed
experiment started with an initial block of 120 CAT trials      some consistency of these distributions.
(to train subjects) followed by four blocks of PREF task           Figs. 2-3B illustrate the primary goal of our study:
alternating with four blocks of CAT task, each block            to provide a method for inferring and visualising sub-
consisting of 60 trials. In a final block of CAT trials, no     jective distributions based on subjects’ responding in
feedback was provided. For our analysis we neglected the        psychophysics experiments. However, as subjective dis-
initial training session. We used the next 180 PREF and         tributions cannot be observed or measured directly, there
180 CAT trials to infer subjects’ subjective distributions      is no obvious way to assess the degree to which these
and reserved the last 60 PREF and 60 CAT trials for             inferences are ‘correct’. One possibility, pursued above,
cross-validation.                                               is to compare the inferred distributions to the distribu-
Three-dimensional feature space The stimuli in                  tions subjects were trained on (assuming that subjects
the second experiment were trapezoids with three fea-           are approximately ideal learners and decision makers).
tures varying systematically: colour (gray-scale), size,       While a match between the inferred subjective distribu-
and shape (ratio of parallel sides), each parametrised          tion and the training distribution (Fig. 2B) can be taken
by continuous values between 0 and 1 (Fig. 3A). This            as indicative of valid inferences, a lack of match (Fig. 3B)
experiment involved one-back discrimination (DISC) and          is harder to interpret. In particular, one cannot distin-
stimulus preference tasks (PREF). During DISC trials,           guish between the algorithm giving incorrect results or
which also served to train subjects on a particular dis-        subjects behaving sub-optimally (because of a failure to
tribution of stimuli, subjects were presented with one          learn, or a failure to use learned information to direct
stimulus per trial, and had to judge (without feedback)         choices). Therefore we sought to establish the quality of
whether it was the same or different than the one pre-          the inferences of our method in a more reliable way.
sented in the previous trial. In actuality, 10% of stimuli      Predicting human behaviour A standard way to as-
were exact repetitions of stimuli presented in the previous     sess the quality of a statistical model of a data set is to
trial, the rest was sampled independently from the train-       test its predictive performance in cross-validation: infer
ing distribution. Two different training distributions were     its parameters (hidden variables) based on a subset of the
used in the two conditions (Fig. 3B, left panels), with six     data, and measure how well it predicts the held-out part
subjects in each condition. During PREF trials subjects         of the data set. Our method is readily amenable to this
had to choose (without feed-back) the stimulus which            cross-validation approach since it defines an explicit sta-
appeared to be more familiar based on the stimuli they          tistical model for predicting subjects’ responses based on
had seen during training. The experiment started with           the stimuli they see (Fig. 1A). Making such predictions is
300 DISC training trials, followed by 100 PREF trials           not only important for validation purposes in the context
and another 200 DISC trials. In our analysis we neglected       of the present study, but may also be relevant in its own
the first 100 DISC trials, used 300 DISC and 50 PREF            right in applications in which e. g. customer choices need
trials to infer subjective distribution and preserved 100       to be predicted based on their previous choices.
DISC and 50 PREF trials for cross-validation.                      For cross-validation, we inferred subjects’ subjective
                                                                distributions and link parameters from the first blocks of
                          Results                               trials of a task and based on the inferred model predicted
Inferring subjective distributions After exten-                 their responses in the final block of trials in the same
sively validating out method on synthetic datasets (sup-        task (Fig. 4, double Bayes). Ideally, subjective distribu-
plementary material1 ), we inferred human subjective            tions are independent of the type of task subjects are
distributions from the two experiments described ear-           performing, and hence one would even expect to be able
lier. Fig. 2B shows results on the experiment with a            to infer the subjective distribution from behaviour in
one-dimensional feature space. The inferred subjective          one task and, based on that, predict choices in an other
distributions reflected qualitative aspects of the distribu-    task. Thus, we also performed a stronger cross-validation
tions of stimuli on which subjects were trained in different    test in which we measured such across-task predictive
conditions. This match between inferred and training            performance (Fig. 4, double Bayes-CT ).
distributions became especially clear in the categorisation        Subjects’ responding is inherently stochastic, therefore
task.                                                           the absolute predictive performance of our model is not
   Fig. 3B shows results on the experiment with a three-        particularly informative in itself. In order to establish
dimensional feature space. These results suggest that           some relevant baseline performance, we implemented al-
                                                            2812

                                 B                                       condition 1                              condition 2       condition 3          condition 4
    A                             PREF           P(s)
                                  CAT            P(s)
             s (fish height)
                                                                s (fish height)
Figure 2: A, One dimensional stimuli used in the first set of experiments. B, Subjective distributions in a one-
dimensional feature space. Rows correspond to task types, columns correspond to experimental conditions using
training distributions with different means (1 & 3 vs. 2 & 4) or variances (1 & 2 vs. 3 & 4). Red lines show training
distributions, blue lines show the posterior mean subjective distribution of each subject. Shading of blue lines indicates
point-wise marginal posterior uncertainty: lighter means higher uncertainty (s.e.m. divided by the mean).
                                                                                                               ternative models for predicting subjects’ responses. Since
        1D feature space          3D feature space
             PREF                      PREF                                                                    the task of predicting responses based on the stimuli that
                                                                                                               subjects see is formally equivalent to a binary classifica-
   0.55                        0.58                                                                            tion task (see supplementary material1 ), we implemented
                                                                                                               a Gaussian process classifier (Fig. 4, GP classifier ) (Ras-
                                                                                                               mussen & Williams, 2006). The GP classifier is a particu-
     0.5                        0.5                                                                            larly powerful algorithm applicable for such classification
                                                                                                               tasks, but it is also a black-box model in the sense that it
                                                                                                               has no explicit notion of subjective distributions. There-
               CAT                                       DISC                                                  fore, it provides an interesting baseline by giving about
   0.75                        0.85
                                                                                                               the best predictive performance that can be achieved
                                                                                                              without modelling subjects’ mental representations.
                                                                                                                  As an alternative method that did have an explicit
     0.5                        0.5                                                                            notion of subjective distributions, we implemented an
                                                                                                              ‘ideal learner’ version of our model, which has the training
                                                                                                               distribution as its subjective distribution for all subjects,
           double Bayes -CT                                                                                    but its link parameters (parametrising stochasticity in
                                         GP classifier   ideal learner    double Bayes
                                                                                         double Bayes-CT
                                                                                                               decision making) are still fitted to each subject’s data
           double Bayes                                                                                        individually (Fig. 4, ideal learner ). This model controls
           MCMC people (-CT)
                                                                                                               for the importance of individual differences in the inferred
                                                                                                               subjective distributions in our method, and also tests
           ideal learner                                                                                       the validity of the assumption that subjects act as ideal
                                                                                                               learners in these experiments.
           GP classifier
                                                                                                                  Finally, we also implemented as an alternative method
                                                                                                               a previously published algorithm (‘MCMC with people’)
Figure 4: Predicting human responses by alternative                                                            to infer subjective distributions (Sanborn & Griffiths,
methods. Bars show across-subject averages (± s.e.m.)                                                         2008). Although this algorithm can only be applied to
of probabilities of correct predictions. In the PREF                                                           specifically designed stimulus preference experiments, one
tasks our method double Bayes significantly outperformed                                                       of our data sets includes data from such an experiment,
both the GP classifier and the ideal learner in both                                                           so we tested the performance of the algorithm on that
experiments and also MCMC people in the 1D experiment                                                          data set by performing both within-task and across-task
(p < 0.05). In the CAT task, the MCMC people method                                                            cross-validation (Fig. 4, MCMC people (-CT)).
was used for across-task predictions (-CT ).
                                                                                                           2813

                            B                          training            posterior mean subjective distributions
                                                                      S1      S2           S3            S4          S5        S6
   A                            condition 1
                                              colour
                            DISC       PREF
                                                               ap
                                                                  e
                                                       size   sh
   colour
                                                                      S7      S8           S9           S10          S11       S12
                        e
                    ap
                               condition 2
                   sh
            size
                                              colour
                            DISC      PREF
                                                               ap
                                                                  e
                                                       size   sh
Figure 3: A, Stimuli used in the second experiment had three continuous features, size, colour and shape.
B, Subjective distributions in the three-dimensional feature space. Left, in red: training distributions; right, in blue:
posterior mean subjective distributions (blue) for each subject individually (columns, S1-S12). Rows correspond
to different conditions, using different training distributions, and different task types to test subjects’ subjective
distributions. In the discrimination task, subjects 7 and 8 responded irrespective of stimuli.
   Fig. 4 shows the predictive performances of these meth-                                            Discussion
ods. The absolute difficulty of predicting responses
greatly varied across tasks, the discrimination task and                      We have presented a new computational method for in-
the categorisation tasks being considerably easier than                       ferring subjects’ mental representation of stimuli from
the preference task, but the relative performances of the                     their responses on simple binary decision tasks. Since
different methods showed consistent patterns across the                       Bayesian inference was intractable, we implemented a
different experiments and tasks. When comparing within-                       Hamiltonian Markov chain Monte Carlo method for nu-
task predictive performances, our method was the best,                        merical analysis, which we have extensively validated and
or among the best, in all tasks. Notably, it outperformed                     tested on real-world data sets. We found that the method
the ‘MCMC with people’ method even in the case when                           was able to recover subjective distributions of humans
that method was applicable at all.                                            when they were trained on stimuli with known structure
                                                                              and to predict future responses better than other model-
   In most cases, the three subjective distribution-based                     based and ‘black-box’ methods. We have also shown that
methods (double Bayes, ideal learner, MCMC people)                            – using our method – information gained in one type of
outperformed the GP classifier, showing that making pre-                      task could be transferred and applied to predict responses
dictions about subjects’ responding benefits substantially                    in another task which we take as further evidence for
from representing and inferring subjective distributions                      the veridicality and task-invariance of the mental repre-
explicitly. This is especially true in across-task cross-                     sentations we inferred. These results also offer a way to
validation which is impossible with a GP classifier in lack                   reconcile cognitivism with behaviourism inasmuch as they
of any parameter that could be shared between tasks.                          demonstrate that even when the only goal is to predict
Yet, in two out of four cases our method had higher ac-                       responses from stimuli, modelling mental representations
curacy even when comparing its across-task performance                        explicitly is quantifiably useful.
against within-task performance of the GP classifier.
                                                                                 There is a long tradition in experimental psychology
  Methods using subject-specific subjective distributions                     and cognitive science to use simple statistics of task per-
(double Bayes, MCMC people) also performed at least                           formance, such as percent correct rates, or reaction times,
as well as the ideal learner, confirming the validity of                      as indices of learning (Gallistel, 1993). These ‘naı̈ve’
the individual differences in subjective distributions these                  methods, even in their statistically most sophisticated
methods inferred, and showing that the poor match found                       forms (Gallistel et al., 2004; Kakade & Dayan, 2002;
between training and subjective distributions in some                         Smith et al., 2005; Preminger et al., 2009; Katkov et al.,
cases (Fig. 3B) were real and not a failure of our algorithm                  2007), boil down to estimating a single (time-dependent)
to recover ‘better looking’ subjective distributions.                         scalar measure of memory strength, i. e. the degree of
                                                                           2814

match between subjects’ mental representations and that                                 References
required by the experimenter (which would presumably             Chater, N., Tenenbaum, J. B., & Yuille, A. (2006). Prob-
allow subjects to perform perfectly). However, by reduc-           abilistic models of cognition: Conceptual foundations.
ing mental contents to simple memory strength measures,            Trends Cogn Sci , 10 (7), 287–291.
these methods fail to provide a detailed picture of struc-
                                                                 Dayan, P., & Abbott, L. F. (2001). Theoretical neuro-
tured mental representations which is what we aimed to
                                                                   science. Cambridge, MA: The MIT Press.
achieve in the present study.
                                                                 Ericsson, K. A., & Simon, H. A. (1980). Verbal reports
   While structured probabilistic models of cognition have
become mainstream more recently (Chater et al., 2006),             as data. Psychol Rev , 87 , 215–251.
they have mostly been used in normative theories to              Gallistel, C. R. (1993). The organization of learning.
account for general, qualitative principles of learning (e. g.     Cambridge, MA: The MIT Press.
patterns of generalisation) rather than to quantitatively        Gallistel, C. R., Fairhurst, S., & Balsam, P. (2004). The
estimate individual subjects’ mental representations in            learning curve: Implications of a quantitative analysis.
specific experiments. Our approach is complementary to             Proc Natl Acad Sci USA, 101 (36), 13124–13131.
these as it makes no assumptions about learning itself.          Geisler, W. S. (2003). Ideal observer analysis. In
   Our work is most closely related to more recent work            L. Chalupa & J. Werner (Eds.), The visual neuro-
by Paninski (2006) and Sanborn & Griffiths (2008) who              sciences. The MIT Press.
both used ideal observer models to infer subjective distri-      Hinton, G. E., & Sejnowski, T. J. (1986). Learning
butions. In the paper by Paninski (2006) continuous deci-          and relearning in Boltzmann machines. In Parallel
sion tasks were considered (in which subjects’ responses           distributed processing. MIT Press.
are analogue rather than discrete), and the method de-           Kakade, S., & Dayan, P. (2002). Acquisition and extinc-
veloped there does not seem to generalise well to the              tion in autoshaping. Psych Rev , 109 (33), 533–544.
binary decision tasks considered here (and used exten-
                                                                 Katkov, M., Tsodyks, M., & Sagi, D. (2007). Inverse
sively in experimental psychology), because the linear
                                                                   modeling of human contrast response. Vision Res,
programming problem that needs to be solved becomes
seriously under-constrained. Our analysis of the pref-             47 (22), 2855–67.
erence task is taken from previous work by Sanborn &             Koller, D., & Friedman, N. (2009). Probabilistic graphical
Griffiths (2008), but they used it to construct a particular       models: Principles and techniques. MIT Press.
kind of stimulus preference task in which subjects’ re-          Neal, R. M. (2010). MCMC using hamiltonian dynamics.
sponding itself implements a Markov chain Monte Carlo              In S. Brooks et al. (Ed.), Handbook of Markov chain
sampler. This is a most elegant idea, but does not trans-          Monte Carlo. Chapman & Hall / CRC Press.
late in any obvious way to other task types, or indeed to        Orbán, G., Fiser, J., Aslin, R. N., & Lengyel, M. (2008).
preference tasks which were not constructed according              Bayesian learning of visual chunks by human observers.
to their particular rules. Our method does not suffer              Proc Natl Acad Sci USA, 105 (7).
from these limitations because of its doubly Bayesian            Paninski, L. (2006). Nonparametric inference of prior
nature: once ideal observer behaviour based on Bayesian            probabilities from Bayes-optimal behavior. In Y. Weiss
analysis is formalised, the method offers an automatic             et al. (Ed.), NIPS 18. MIT Press.
and principled way of inferring subjective distributions.        Preminger, S., Blumenfeld, B., Sagi, D., & Tsodyks,
   A natural way to extend our work in the future will             M. (2009). Mapping dynamic memories of gradually
be to consider dynamical priors over subjective distribu-
                                                                   changing objects. Proc Natl Acad Sci USA, 106 (13).
tions in order to track their temporal evolution, inferring
                                                                 Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian
changes brought about by learning. The machine learning
                                                                   processes for machine learning. The MIT Press.
literature offers powerful tools for carrying out inference
in such dynamical models.                                        Russo, J. E., Johnson, E. J., & Stephens, D. L. (1989).
                                                                   The validity of verbal protocols. Memory and Cogni-
                 Acknowledgements                                  tion, 17 (6), 759–769.
We thank P. Dayan, C. Rasmussen, and A. Sanborn for              Sanborn, A., & Griffiths, T. (2008). Markov chain Monte
useful discussions and K. Jucicaite and S. Ölschläger for        Carlo with people. In J. Platt et al. (Ed.), NIPS 20.
help with acquiring psychophysics data. We are grateful          Smith, A. C., Stefani, M. R., Moghaddam, B., & Brown,
for A. Sanborn and T. Griffiths for providing access to            E. N. (2005). Analysis and design of behavioral exper-
their experimental data. This project was supported by             iments to characterize population learning. J Neuro-
the Wellcome Trust (FH, ML), the Gatsby Charitable                 physiol , 93 (3), 1776–1792.
Foundation (FH), and the Max Planck Society (UN).
                                                             2815

