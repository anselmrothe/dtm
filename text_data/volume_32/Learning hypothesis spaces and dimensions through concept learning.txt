UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning hypothesis spaces and dimensions through concept learning
Permalink
https://escholarship.org/uc/item/85p3r8f5
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Austerweil, Joseph
Griffiths, Thomas
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

            Learning hypothesis spaces and dimensions through concept learning
                                     Joseph L. Austerweil (Joseph.Austerweil@gmail.com)
                                       Thomas L. Griffiths (Tom Griffiths@berkeley.edu)
                   Department of Psychology, University of California, Berkeley, Berkeley, CA 94720-1650 USA
                               Abstract                               with “city-block” distance or the L1 metric, while integral
                                                                      dimensions are associated with Euclidean distance or the L2
   Generalizing a property from a set of objects to a new object
   is a fundamental problem faced by the human cognitive sys-         metric (Garner, 1974). These different metrics also have con-
   tem, and a long-standing topic of investigation in psychology.     sequences beyond generalization behavior, influencing how
   Classic analyses suggest that the probability with which peo-      people categorize objects varying along different dimensions
   ple generalize a property from one stimulus to another depends
   on the distance between those stimuli in psychological space.      (Handel & Imai, 1972) and whether people can selectively
   This raises the question of how people identify an appropri-       attend to each dimension (Garner & Felfoldy, 1970).
   ate metric for determining the distance between novel stim-           Analyses of human generalization have tended to treat the
   uli. In particular, how do people determine if two dimensions
   should be treated as separable, with distance measured along       metric as a fixed property of stimuli. However, determining
   each dimension independently (as in an L1 metric), or integral,    the appropriate metric on a psychological space is an impor-
   supporting Euclidean distance (as in an L2 metric)? We build       tant step towards developing an appropriate representation for
   on an existing Bayesian model of generalization to show that
   learning a metric can be formalized as a problem of learning       the properties of novel objects. If two dimensions are sepa-
   a hypothesis space for generalization, and that both ideal and     rable, then those dimensions form privileged axes for repre-
   human learners can learn appropriate hypothesis spaces for a       senting locations in the psychological space, and it is easier
   novel domain by learning concepts expressed in that domain.
   Keywords: generalization; categorization; Bayesian model-          to learn categories defined by rules that align with those axes
   ing; similarity; integral and separable dimensions                 (Kruschke, 1993). This is qualitatively different from an inte-
                                                                      gral representation, in which there are no natural axes for rep-
                           Introduction                               resenting the space. Identifying whether dimensions should
Almost every two objects, events, or situations (or the sen-          be separable or integral is thus just as basic a step towards
sory data for the same object at two different moments) that          forming a representation for a novel domain as determining
we encounter are unique. Despite this fact, when people (and          the number of dimensions, or the locations of each stimulus
animals) learn that one stimulus has a property, they reli-           in the resulting space.
ably and systematically believe certain other stimuli have that          In this paper, we consider how a learner could identify the
property and others do not (Shepard, 1987). For example, if           appropriate metric for representing a novel domain, compar-
you learn a dark, large circle is a gnarble, how likely is a          ing an ideal Bayesian learner with human judgments. The
dark, slightly smaller circle or a dark very small circle to be       starting point for this investigation is an existing Bayesian
a gnarble? This is the problem of generalization, which is            model of generalization, introduced by Shepard (1987) and
pervasive across cognitive science. It occurs in many forms           extended by Tenenbaum and Griffiths (2001). In this model,
from higher-level cognition (e.g., concept learning, Tenen-           the property of interest is possessed by all stimuli within an
baum, 2000) to linguistics (e.g., word learning, Xu & Tenen-          unknown region of psychological space, and the probabil-
baum, 2007) to perception (e.g., color categorization, Kay &          ity of generalizing to a new stimulus is computed by sum-
McDaniel, 1978). How should an ideal learner generalize a             ming over all candidate regions containing the new stimu-
property from a group of stimuli observed to have the prop-           lus and the previous stimuli observed to have some property,
erty to other stimuli?                                                weighted by the posterior probability of that region. The dif-
   One of the most celebrated theoretical results of cogni-           ference between separable and integral dimensions emerges
tive psychology provides a deceptively simple answer to this          as the result of probabilistic inference with different hypoth-
question, indicating that we should generalize a property from        esis spaces of regions (Shepard, 1987, 1991; Davidenko &
one object to another object when the two objects are simi-           Tenenbaum, 2001). The hypothesis spaces that produce gen-
lar, or equivalently, close in some psychological space (Shep-        eralization corresponding to separable and integral dimen-
ard, 1987). However, this establishes a new problem: How              sions consist of axis-aligned and axis-indifferent regions in
should the distance between objects be measured? More                 the space, respectively (see Figure 1). Axis-aligned regions
formally, the problem is one of identifying a metric on a             produce stronger generalization along the axes, while axis-
space, a basic challenge that also arises when using machine          indifferent regions produce generalization that depends only
learning methods that rely on computing distances, such as            on the Euclidean distance between stimuli.
nearest-neighbor classification (Xing, Ng, Jordan, & Russell,            This analysis of separable and integral dimensions lays the
2002; Davis, Kulis, Jain, Sra, & Dhillon, 2007). Cognitive            groundwork for our account of how people learn an appro-
psychologists have determined that people use two different           priate metric for a novel space. Learning a metric thus be-
kinds of metrics when forming generalizations about multi-            comes a matter of inferring an appropriate hypothesis space
dimensional stimuli: separable dimensions are associated              on which to base generalization. We define a hierarchical
                                                                   73

Bayesian model that makes this inference from a set of ob-             Separable and Integral Dimensions
served concepts. We demonstrate that this model infers a city-         Psychological explorations of human similarity metrics of
block or Euclidean generalization metric when given axis-              multidimensional stimuli discovered two different ways in
aligned or axis-indifferent concepts, respectively, and that           which people use these dimensions: separable and integral
people infer a hypothesis space for generalization based on            (Shepard, 1987). Separable dimensions can be interpreted in-
the concepts they learn in a way that is consistent with this          dependently and form natural axes for representing a space,
ideal observer analysis. This extends previous results by              while integral dimensions are difficult to perceive indepen-
Goldstone (1994) who changed dimensions from being in-                 dently. The dimensional structure of stimuli affects many as-
tegral to separable via repeated training of a single concept.         pects of human information processing, including the ease
   The plan of the paper is as follows. The next section pro-          of categorizing objects into groups and perceived distance
vides the theoretical background for our approach, summariz-           between objects (Garner, 1974). For example, Garner and
ing the basic generalization model, revisiting some of the lit-        Felfoldy (1970) found that categorization time was facili-
erature on separable and integral dimensions, and laying out           tated for objects with integral dimensions (e.g., saturation and
our approach to hypothesis space learning. We then present a           lightness of a color) into groups where the values of the di-
test of the predictions of this model with human learners. Fi-         mensions of the objects in each group are correlated (light and
nally, we conclude the paper with a discussion of our results          desaturated vs. dark and saturated). However, there was inter-
and possible future directions.                                        ference for objects categorized into groups of objects where
                                                                       the values of the dimensions are orthogonal (light and satured
                       Theoretical Framework                           vs. dark and desaturated). Conversely, there were no major
Our theoretical framework builds directly on the Bayesian              differences in categorization time for these types of catego-
generalization model introduced in Shepard (1987) and                  rization structures when the dimensions were separable.
Tenenbaum and Griffiths (2001), so we begin by summariz-                   Dimensional structure also affects the perceived distances
ing the key ideas behind this approach. We then show how               between objects (Shepard, 1991). The perceived distance
this approach produces separable and integral generalization,          metric for objects with separable dimensions is the “city-
and how it can be extended to allow an ideal learner to infer          block” distance, also known as the L1 metric, with the dis-
an appropriate representation for novel stimuli.                       tance between two stimuli xi and x j being d(xi , x j ) = ∑k |xik −
                                                                       x jk |, where k ranges over dimensions and xk is the value of
The Bayesian Generalization Model
                                                                       stimulus x on dimension k. The perceived distance metric for
Let X be the stimulus space and H be the hypothesis space,             objects with integral dimensions    p is the Euclidean distance,
where h ∈ H is a hypothesis as to which objects have and               or L2 metric, with d(xi , x j ) = ∑k (xik − x jk )2 . The use of
do not have the property of interest (i.e., a hypothesis is a          these different distance metrics is consistent with the different
set of x ∈ X ). After observing that a set of stimuli X =              properties of separable and integral dimensions: city-block
{x1 , . . . , xn }, xi ∈ X , stimuli have some property, how should    distance sums the distance along each axis separately for all
you update your belief in: (1) which property it is and (2)            points in the space, while Euclidean distance is insensitive to
which other stimuli have that property? Assuming that stim-            whether a point is located along an axis, and is thus invariant
uli are generated uniformly and independently under the true           to changes in the axes used to represent the space. Recent ex-
hypothesis at random for the property (p(X|h) = ∏i p(xi |h) =          tensions of classic multidimensional scaling techniques bear
|h|−n for a hypothesis containing all stimuli in the given set;        out these results, and provide a way to identify whether peo-
p(X|h) = 0 otherwise) and taking some prior over hypothe-              ple seem to use separable or integral dimensions in their rep-
ses p(h), the posterior probability that a hypothesis h is the         resentation of a set of stimuli (Lee, 2008).
property that n given stimuli share is                                     In the Bayesian generalization model introduced in the pre-
                                                                       vious section, the difference between integral and separa-
                                     p(h) ∏ni=1 p(xi |h)
                    p(h|X) =                                    (1)    ble dimensions emerges from using two different hypothesis
                                ∑h′ ∈H p(h′ ) ∏ni=1 p(xi |h′ )         spaces (Shepard, 1987). Using a hypothesis space in which
which is simply Bayes’ rule. Using Equation 1, we can derive           regions are aligned with the axes results in behavior consis-
the probability of generalizing from X to some other stimu-            tent with separable dimensions, while a hypothesis space in
lus y as the sum over the posterior probability of hypotheses          which regions are indifferent to the axes results in behav-
containing y                                                           ior consistent with integral dimensions. Figure 1 shows a
                            p(y|X) = ∑ P(h|X)                   (2)    schematic of two such hypothesis spaces, restricted to rect-
                                       h:y∈h                           angular regions in two dimensions, together with the general-
                                                                       ization gradient for a single exemplar concept in each space.1
which constitutes a form of hypothesis averaging (Robert,
2007). The predictions of the model depends intimately on                   1 We calculated the generalization gradients by sampling from
the nature of the hypotheses under consideration, with dif-            the prior distribution over hypotheses for the axis-aligned and axis-
                                                                       indifferent hypothesis spaces, then weighting each hypothesis by the
ferent hypothesis spaces leading to different generalization           likelihood given the single exemplar E5. The gradients were evalu-
patterns.                                                              ated on a discretized 9 × 9 grid.
                                                                    74

(a)                                  (b)                                    space Hm and P(Hm ) is the prior probability of hypothesis
                                                                            space Hm . The probability of concepts C and current stimuli
                                                                            X under hypothesis space Hm is
      Axis-aligned (separable)            Axis-indifferent (integral)               P(C , X|Hm ) =    ∏      ∑   P(h|Hm ) ∏ P(x|h)      (5)
                                                                                                   C∈(C ∪X) h∈Hm          x∈C
      Separable Predictions                 Integral Predictions
 A                                    A
                                                                            where C plays the same role as X, but for the previously ob-
 B                                    B
                                                                            served concepts.
 C                                    C
                                                                               Intuitively, the model can be thought as being composed of
 D                                    D
                                                                            m Bayesian generalization “submodels” (each with their own
 E                                    E
                                                                            hypothesis space). The model’s generalization judgments are
 F                                    F
                                                                            made by averaging over the generalizations made by the indi-
 G                                    G
                                                                            vidual submodels (given the current stimulus X) weighted by
 H                                    H                                     how well the submodel explains the previously and currently
 I                                    I                                     observed stimuli. Thus, the model “learns” to use hypothesis
   1 2 3 4 5 6 7 8 9                    1 2 3 4 5 6 7 8 9
                                                                            spaces that explain the observed concepts well.
Figure 1: Hypothesis spaces and generalization gradients. (a)
                                                                                  Human Learning of Hypothesis Spaces
Axis-aligned (separable) and axis-indifferent (integral) hy-
pothesis spaces. (b) Resulting generalization gradients for                 The model presented in the previous section predicts that a
each hypothesis space given a single exemplar of a concept.                 learner should be able to infer whether dimensions are inte-
                                                                            gral or separable for a novel domain after seeing some ex-
                                                                            amples of concepts expressed in that domain. Preliminary
The generalization gradient resulting from the axis-aligned                 support for this idea is provided by the results of Goldstone
hypothesis space given a single exemplar of a concept de-                   (1994), who showed that teaching people a novel axis-aligned
creases with distance under a city-block metric, while the gra-             concept could affect generalization along that axis in both in-
dient resulting from the axis-indifferent hypothesis space de-              tegral and separable spaces. However, shifting a represen-
creases with Euclidean distance. Models using the appropri-                 tation all the way towards integral or separable dimensions
ate hypothesis spaces capture generalization judgments well                 will require learning more than one concept. To test whether
for concept learning tasks using separable and integral dimen-              human learners behaved in this way, we conducted an exper-
sions for both single and multiple exemplars (Davidenko &                   iment in which we examined how the generalization judg-
Tenenbaum, 2001; Tenenbaum, 1999).                                          ments that people produce depend on the concepts they have
                                                                            learned. We used rectangles varying in width and height as
Learning a Hypothesis Space
                                                                            our set of stimuli, and participants learned 20 concepts that
The Bayesian generalization framework naturally extends to                  were either aligned with or orthogonal to these dimensions
learning an appropriate hypothesis space by introducing the                 (rectangles with the same aspect ratio or area). The key pre-
hypothesis space itself as a higher-level random variable in                diction was that participants observing axis-aligned concepts
a hierarchical Bayesian model. Given an enumerable set of                   should show a generalization gradient consistent with a city-
hypothesis spaces M = {H1 , . . . , HM }, the probability that              block metric, whereas participants observing concepts indif-
an ideal observer generalizes to a new stimulus y given a set               ferent to these axes should show a generalization gradient
of stimuli X have a property and a set of previously observed               consistent with a Euclidean metric. This prediction results
concepts C (where each concept itself is a set of stimuli) is               from the different hypothesis spaces the two groups of partic-
                             M                                              ipants should infer are appropriate for these domains.
           P(y|X, C ) =     ∑ P(y|Hm , X)P(Hm |C , X)                (3)
                                                                            Stimuli and Methods
                           m=1
                                                                            The stimuli for this experiment were rectangles where the two
where the first term is the probability of generalizing from X
                                                                            manipulated dimensions were the width and height (ranging
to y under hypothesis space Hm (as specified by Equation 2),
                                                                            from 13 to 115 pixels in increments of approximately 25 pix-
and the second term is the posterior probability of hypothe-
                                                                            els). The stimulus set is shown in Figure 2. We chose rectan-
sis space Hm given the previous concepts C and the observed
                                                                            gles because it is easy to think of concepts on our two manip-
stimuli of the current concept of interest. This posterior prob-
                                                                            ulated dimensions (same width or height) and the diagonals
ability can be computed by applying Bayes’ rule
                                                                            of the dimensions (same aspect ratio or area). Previously,
                                 P(C , X|Hm )P(Hm )                         Krantz and Tversky (1975) found people weakly favor using
           P(Hm |C , X) =                                            (4)    area and aspect ratio as separable dimensions (the diagonals
                               ∑m=1 P(C , X|Hm )P(Hm )
                                M
                                                                            of separable dimension space). However, people can use any
where P(C , X|Hm ) is the probability of observing a set of con-            of the four potential dimensions for generalization depend-
cepts C and the currently observed stimuli under hypothesis                 ing on the context rectangles are in. This natural flexibility
                                                                         75

 A                                                                                 Separable                            Integral
                                                                       A                                    A
  B                                                                    B                                    B
                                                                       C                                    C
  C                                                                    D                                    D
                                                                       E                                    E
 D                                                                     F                                    F
                                                                       G                                    G
  E
                                                                       H                                    H
  F                                                                     I                                   I
                                                                          1 2 3 4 5 6 7 8 9                   1 2 3 4 5 6 7 8 9
 G
                                                                         Figure 3: The 20 concepts for each training condition. Each
 H                                                                       concept is the collection of objects on a straight line on the
                                                                         grid. The separable concepts are axis aligned and the integral
  I                                                                      concepts are indifferent to axes.
        1      2       3      4       5      6      7      8       9
     Figure 2: Stimuli used in our experiment (not to scale).            concepts consisting of single objects ({B2, B8, E5, H2, H8}
                                                                         were tested) over the total 9 × 9 set of objects.
makes rectangles an ideal candidate for training participants            Results
to represent rectangles using different dimensional structures.          Figure 4 shows averaged results for single exemplar gener-
    There were two phases to the experiment: training and test.          alization for the test phase in the two conditions. The single
For the training phase, there were two between-subjects con-             exemplar concept results were re-aligned to {E, 5} and then
ditions: the separable condition (n = 15), in which people               averaged over the five concepts per participant and over par-
observed axis-aligned concepts, and the integral condition               ticipants. We then took the difference between the general-
(n = 18)2 in which people observed axis-indifferent concepts.            ization gradients for the two conditions, and compared them
The test phase was the same for all participants. The cover              with the difference between the generalization gradients pro-
story for the experiment was:                                            duced by the Bayesian model. The integral group generalizes
    On a small island in the Pacific Ocean, scientists found the an-     more on the diagonals and less on the axes than the separable
    cient ruins of a small civilization. While excavating the ruins,     group as predicted if the integral and separable groups used
    they discovered objects on the doors of particular houses. They      Euclidean and city-block distance metrics respectively.
    believe that the objects carry information about the people in          To test quantitatively that the two groups learn integral
    the houses. Some of the objects the scientists found had names       and separable dimensions, we found that the integral train-
    written under them.                                                  ing group generalized significantly more often on diago-
                                                                         nals than axes (averaging over {C, D, F, G} × {3, 4, 6, 7} vs.
Stimuli were then presented as objects with names, and peo-              C5, D5, F5, G5, t(32) = 3.23, p < 0.005). Within the separa-
ple guessed what other objects would share the same name.                ble group, the generalization judgments on the axes were sig-
    The 20 concepts shown to the training groups are shown in            nificantly greater than the diagonals (t(34) = 2.66, p < 0.05);
Figure 3 (each concept is a straight line picking out several            however, the integral group did not differentiate between
points, corresponding to stimuli). The concepts for the two              changes on the axes and the diagonals (t(30) = 0.43, p =
conditions were chosen such that each condition saw each                 0.43). Interestingly, both groups of participants treated the
object an equal number of times, there were two to four ob-              positive diagonal (F3, F4, G3, G4,C6,C7, D6, D7) differently
jects in each concept, and the concepts spanned the space of             than the negative diagonal (C3,C4, D3, D4, F6, F7, G6, G7)
objects. The 20 concepts were presented to participants in a             (t(34) = 2.58, p < 0.05 for separable and t(30) = 2.63, p <
random order as examples of objects that were called different           0.05 for integral). This replicates Krantz and Tversky
nonsense names randomly chosen from a standardized list.                 (1975)’s finding that people tend to generalize rectangles
While the objects in each concept were on the screen, partici-           based on constant aspect ratio. This is not surprising as con-
pants were asked whether or not they thought every object in             stant aspect ratio is an important invariance of an object’s
{A,C, E, G, I} × {1, 3, 5, 7, 9} shown individually below the            projection on the retina as it changes in depth (keeping the
objects in the concept could be called that name.                        viewpoint orientation constant) due to perspective projection
    The test phase of the experiment was identical to the first          (Palmer, 1999).
phase except participants’ generalizations were tested for                  Finally, we calculated a mixed effect 2 × 2 ANOVA that
     2 The different number of participants in each group was due to     corrobrates the conclusions of our other statistical tests. It
the computer crashing mid-experiment.                                    identified a main effect of generalizing on the diagonal vs. the
                                                                     76

                  Integral - Separable Predictions                            Integral - Separable Results
          -4                                                       -4
          -3                                                       -3
          -2                                                       -2
          -1                                                       -1
           0                                                        0
           1                                                        1
           2                                                        2
           3                                                        3
           4                                                        4
                 -4 -3 -2 -1           0    1    2    3     4              -4 -3 -2 -1          0     1    2    3    4
Figure 4: Predictions of the difference between the two Bayesian models formed by model averaging given the separable and
integral concepts, and difference between the human generalization results from the two conditions. The results are presented
as bubble plots where the size of the bubble represents the degree of generalization. Solid and open bubbles represent positive
and negative values respectively. Each single exemplar concept results were re-aligned to E5 and then averaged over the five
concepts per participant and over participants. Notice how the differences on the axes aligned with the given stimulus (E5) are
negative and the differences on the diagonals are positive.
axes (F(1, 32) = 44.258, p < 0.001) and an interaction be-          further test this account of separable and integral dimensions
tween generalizing on the diagonal vs. the axes and the train-      by exploring if after training participants show other conse-
ing group (F(1, 32) = 10.453, p < 0.005). This suggests that        quences of having separable or integral dimensions, such as
in the future we should include a hypothesis space into our         classification and attentional effects. Additionally, this would
hierarchy that includes regions varying on the axes and the         address a potential confound that the training affects the at-
positive diagonal (but not the negative diagonal).                  tention participants pay to each dimension. Fortunately, our
                                                                    larger conclusion that people use the concepts they are given
                          Discussion                                to learn the appropriate hypothesis space for a domain holds
Generalization is an essential problem that basically every         regardless of the potential confound (as this conclusion is ag-
cognitive system needs to solve in virtually every domain.          nostic to the exact mechanism affecting generalization).
Previous analyses of the generalization problem (Shepard,
1987; Tenenbaum & Griffiths, 2001) indicated how an ideal              One attractive aspect of this analysis (over using a differ-
learner should act assuming that an appropriate representa-         ent solution, like model selection) is that it provides a way
tion of the stimuli and hypothesis space for generalizations        to explain why the empirical literature suggests that integral-
is known. However, how people arrive at a representation            ity has been found to be a fuzzy rather than a binary dis-
and hypothesis space has been left as an open question. As          tinction (Garner, 1974). Such fuzzy boundaries emerge as a
it seems unlikely that people would be born with the appro-         consequence of Bayesian inference when there is uncertainty
priate representation and hypothesis space for all possible do-     to which hypothesis space is appropriate for generalization.
mains, people need to be able to infer this information from        We would predict that the “integrality” of natural dimensions
their observations of the properties of stimuli. Using the prob-    are a consequence of how real world objects are categorized
lem of learning a metric as an example, our analysis shows          along those dimensions. For example, the reason why the
how an ideal learner would go about inferring such hypoth-          saturation and brightness of a color are integral is because in
esis spaces, and our experimental results suggest that people       our environment we do not make distinctions between col-
do so in a way that is consistent with this model.                  ors at different saturations and brightnesses. “Light” green
   To our knowledge, our results provide the first behavioral       is a typical color word; however, “saturated” green is an es-
evidence that people can learn whether stimuli should be rep-       oteric word, reserved only for artists, designers, and percep-
resented with separable or integral dimensions. Our results         tual psychologists. In fact, Goldstone (1994) and Burns and
also provide compelling support for the idea that the dif-          Shepp (1988) found that these dimensions are separable in
ference between separable and integral dimensions can be            people who regularly distinguish between the two (color ex-
thought of as the result of different hypothesis spaces for gen-    perts and participants trained to distinguish between the two),
eralization, building on (Shepard, 1987, 1991; Davidenko &          which implies that they have concepts aligned with the axes
Tenenbaum, 2001). In future work, it would be interesting to        of brightness and saturation.
                                                                 77

   Another important implication of our results is that humans      Davis, J. V., Kulis, B., Jain, P., Sra, S., & Dhillon, I. S. (2007).
learn the metric appropriate for generalization in a particu-         Information-theoretic metric learning. In Proceedings of
lar domain from the concepts they observe. It would be in-            the 24th International Conference on Machine Learning.
teresting to compare how metric learning algorithms devel-            Corvallis, OR.
oped in machine learning (e.g., Xing et al., 2002; Davis et         Garner, W. R. (1974). The Processing of Information and
al., 2007) compare to human metric learning on this task,             Structure. Maryland: Erlbaum.
and after learning other types of concepts. This could pave         Garner, W. R., & Felfoldy, G. L. (1970). Integrality of stimu-
the way towards new machine learning algorithms that auto-            lus dimensions in various types of information processing.
matically infer dimensions intuitive to people from a given           Cognitive Psychology, 1, 225-241.
set of concepts. Dimensionality reduction techniques like           Goldstone, R. (1994). Influences of categorization on percep-
multi-dimensional scaling and principal component analysis            tual discrimination. Journal of Experimental Psychology:
are some of the most widely used tools for scientific data anal-      General, 2(123), 178-200.
ysis, but only produce the equivalent of integral dimensions.       Handel, S., & Imai, S. (1972). The free classification of
An algorithm that determines whether a space is better rep-           analyzable and unanalyzable stimuli. Perception & Psy-
resented by separable or integral dimensions, and produces            chophysics, 12, 108-116.
interpretable separable dimensions, would be a valuable ad-         Kay, P., & McDaniel, C. K. (1978). The linguistic signifi-
dition to any data analysis toolkit.                                  cance of the meanings of basic color terms. Language, 54,
   Though Bayesian models have become very popular                    610-646.
and successful at explaining different cognitive phenomena          Krantz, D. H., & Tversky, A. (1975). Similarity of rect-
(Chater, Tenenbaum, & Yuille, 2006), the hypothesis spaces            angles: An analysis of subjective dimensions. Journal of
used in the models are handpicked by the modeler and usu-             Mathematical Psychology, 12, 4-34.
ally specific to the particular investigated phenomenon. This       Kruschke, J. K. (1993). Human category learning: Implica-
leaves open the question of how people choose the hypothe-            tions for backpropagation models. Connection Science, 5,
ses for a set of observed stimuli. Our framework presents             3-36.
an answer to this problem – a hypothesis space is used for          Lee, M. D. (2008). Three case studies in the Bayesian analy-
a set of observed stimuli depending on how well it explains           sis of cognitive models. Psychonomic Bulletin and Review,
the observed stimuli and its prior probability. We provide            15(1), 1-15.
behavioral evidence for our framework in the case study of          Palmer, S. E. (1999). Vision Science. Cambridge, MA: MIT
learning whether or not two dimensions should be separable            Press.
or integral. Futhermore, this introduces an interesting equiv-      Robert, C. P. (2007). The Bayesian choice: A Decision-
alence between learning the structure of dimensions used to           theoretic Motivation. New York: Springer.
represent stimuli and the set of candidate hypotheses for gen-      Shepard, R. N. (1987). Towards a universal law of generaliza-
eralization, which we plan to investigate in future research.         tion for psychological science. Science, 237, 1317-1323.
                                                                    Shepard, R. N. (1991). Integrality versus separability of stim-
Acknowledgments. We thank Rob Goldstone, Stephen Palmer,              ulus dimensions: from an early convergence of evidence to
Karen Schloss, Tania Lombrozo, and the Berkeley Computational         a proposed theoretical basis. In The Perception of Struc-
Cognitive Science Lab for insightful discussions, and Shubin Li,      ture: Essays in Honor of Wendell R. Garner (p. 53-71).
Brian Tang, and David Belford for help with experiment construc-      Washington, DC: American Psychological Association.
tion, running participants, and data analysis and four anonymous    Tenenbaum, J. B. (1999). Bayesian modeling of human
reviewers and Nick Chater for their comments on a previous draft      concept learning. In M. S. Kearns, S. A. Solla, & D. A.
of the paper. This work was supported by grant FA9550-07-1-0351       Cohn (Eds.), Advances in Neural Information Processing
from the Air Force Office of Scientific Research.                     Systems 11 (p. 59-65). Cambridge, MA: MIT Press.
                                                                    Tenenbaum, J. B. (2000). Rules and similarity in concept
                          References                                  learning. In S. A. Solla, T. K. Leen, & K.-R. Muller (Eds.),
                                                                      Advances in Neural Information Processing Systems 12.
Burns, B., & Shepp, B. E. (1988). Dimensional interactions            Cambridge, MA: MIT Press.
   and the structure of psychological space: The representa-        Tenenbaum, J. B., & Griffiths, T. L. (2001). Generalization,
   tion of hue, saturation, and brightness. Perception and Psy-       similarity, and Bayesian inference. Behavioral and Brain
   chophysics, 43, 494-507.                                           Sciences, 24, 629-641.
Chater, N., Tenenbaum, J. B., & Yuille, A. (2006). Spe-             Xing, E. P., Ng, A. Y., Jordan, M. I., & Russell, S. (2002).
   cial issue on “Probabilistic models of cognition”. Trends in       Distance metric learning, with application to clustering
   Cognitive Sciences, 10(7), 287-344.                                with side-information. In Advances in Neural Information
Davidenko, N., & Tenenbaum, J. B. (2001). Concept gener-              Processing Systems (Vol. 12). Cambridge, MA: MIT Press.
   alization in separable and integral stimulus spaces. In Pro-     Xu, F., & Tenenbaum, J. B. (2007). Word learning as
   ceedings of the 23rd Annual Conference of the Cognitive            Bayesian inference. Psychological Review, 114, 245-272.
   Science Society. Mahwah, NJ.
                                                                 78

