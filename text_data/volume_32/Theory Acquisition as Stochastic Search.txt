UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Theory Acquisition as Stochastic Search
Permalink
https://escholarship.org/uc/item/9hj430zn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Ullman, Tomer
Goodman, Noah
Tenenbaum, Joshua
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                      Theory Acquisition as Stochastic Search
                                 Tomer D. Ullman, Noah D. Goodman, Joshua B. Tenenbaum
                                                      {tomeru, ndg, jbt}@mit.edu
                                                Department of Brain and Cognitive Sciences
                                                   Massachusetts Institute of Technology
                                                           Cambridge, MA 02139
                              Abstract                                  rational or algorithmic. Different children see different ran-
   We present an algorithmic model for the development of chil-         dom fragments of evidence and make their way to adult-like
   dren’s intuitive theories within a hierarchical Bayesian frame-      intuitive theories at different paces and along different paths.
   work, where theories are described as sets of logical laws           It seems unlikely that children can simultaneously evaluate
   generated by a probabilistic context-free grammar. Our algo-
   rithm performs stochastic search at two levels of abstraction        many candidate theories at once; on the contrary, they appear
   – an outer loop in the space of theories, and an inner loop in       to hold just one theory in mind at any time. Transitions be-
   the space of explanations or models generated by each the-           tween theories appear to be local, myopic, and semi-random,
   ory given a particular dataset – in order to discover the theory
   that best explains the observed data. We show that this model        rather than systematic explorations of the hypothesis space.
   is capable of learning correct theories in several everyday do-      They are prone to backtracking or “two steps forward, one
   mains, and discuss the dynamics of learning in the context of        step back”. We suggest that these dynamics are indicative
   children’s cognitive development.
                                                                        of a stochastic search process, much like the Markov chain
                          Introduction                                  Monte Carlo (MCMC) methods that have been proposed for
As children learn about the world, they learn more than just            performing approximate probabilistic inference in complex
a large stock of specific facts. They organize their knowl-             generative models. We show how a search-based learning al-
edge into abstract coherent frameworks, or intuitive theo-              gorithm can begin with little or no knowledge of a domain,
ries, that guide inference and learning within particular do-           and discover the underlying structure that best organizes it
mains (Carey, 1985; Wellman & Gelman, 1992). Much re-                   by generating new hypotheses and checking them against its
cent work in computational cognitive modeling has attempted             current conceptions of the world using a hiearchical Bayesian
to formalize how intuitive theories are structured, used and            framework. New hypotheses are accepted probabilistically if
acquired from experience (Tenenbaum, Griffiths, & Kemp,                 they can better account for the observed data, or if they com-
2006), working broadly within a hierarchical Bayesian frame-            press it in some way. Such a search-based learning algorithm
work shown in Figure 1 (and explained in more detail below).            is capable of exploring a potentially infinite space of theories,
While this program has made progress in certain respects, it            but given enough time and sufficient data it tends to converge
has treated the problem of theory acquisition only in a very            on the correct theory – or at least some approximation thereof,
ideal sense. The child is assumed to have a hypothesis space            corresponding to a small set of abstract predicates and laws.
of possible theories constrained by some “Universal Theory”,               The plan of the paper is as follows. We first introduce our
and to be able to consider all possible theories in that space, in      framework for representing and evaluating theories, based
light of a given body of evidence. Given sufficient evidence,           on first-order logic and Bayesian inference in a hierarchi-
and a suitably constrained hypothesis space of theories, it has         cal probabilistic model that specifies how the theory’s logical
been shown that an ideal Bayesian learner can identify the              structure constrains the data observed by a learner. We then
correct theory underlying core domains of knowledge such as             describe our algorithmic approach to theory learning based
causality (Goodman, Ullman, & Tenenbaum, 2009), kinship                 on MCMC search, using simulated annealing to aid conver-
and other social structures (Kemp, Goodman, & Tenenbaum,                gence. Finally we study the search algorithm’s behavior on
2008). These Bayesian computational analyses have not to                two case studies of theory learning in everyday cognitive do-
date been complemented by working algorithmic models of                 mains: the taxonomic organization of object categories and
the search process by which a child can build up an abstract            properties, and a simplified version of magnetism.
theory, piece by piece, generalizing from experience. Here
we describe such an algorithmic model for Bayesian theory                                   Formal framework
acquisition. We show that our algorithm is capable of con-              We work with the hierarchical probabilistic model shown in
structing correct if highly simplified theories for several ev-         Figure 1, based on those in (Katz, Goodman, Kersting, Kemp,
eryday domains, and we explore the dynamics of its behavior             & Tenenbaum, 2008; Kemp et al., 2008). We assume that a
– how theories can change as the learner’s search process un-           domain of cognition is given, comprised of one or more sys-
folds as well as in response to the quantity and quality of the         tems, each of which gives rise to some observed data. The
learner’s observations.                                                 learner’s task is to build a theory of the domain: a set of
   At first glance, the dynamics of theory acquisition in child-        abstract concepts and explanatory laws that together gener-
hood look nothing like the ideal learning analsyes of hierar-           ate a hypothesis space and prior probability distribution over
chical Bayesian models – and may not even look particularly             candidate models for systems in that domain. The laws and
                                                                    2840

 Universal                                                                                                     inherits down is a relations” and “The is a relation is transi-
 Theory                        Probabilistic Horn Clause Grammar
                                                                                                               tive” (laws 3 and 4 on the right side of Figure 1). A system
                        Magnetism                               Taxonomy                                       consists of a specific set of categories and properties, such as
            Core Predicates: p(X), q(X)            Core Predicates: f(X,Y), g(X,Y)
            Surface Predicates: interacts(X,Y)     Surface Predicates: has_a(X,Y), is_a(X,Y)
                                                                                                               salmon, eagle, breathes, can fly, and so on. A model specifies
 Theory     Laws:                                  Laws:                                                       the minimal is a and has a relations, typically corresponding
            interacts(X,Y)       p(X) p(Y)         is_a(X,Y)             g(X,Y)
            interacts(X,Y)       p(X) q(Y)          has_a(X,Y)           f(X,Y)                                to a tree of is a relations between categories with properties
             interacts(X,Y)      interacts(Y,X)     has_a(X,Y)           is_a(X,Z) has_a(Z,Y)
                                                    is_a(X,Y)            is_a(X,Z) is_a(Z,Y)                   attached by has a relations at the broadest category they hold
         p(X): “magnets”
                                                                                                               for: e.g., “A canary is a bird”, “A bird is an animal”, “An an-
                            “non-magnetic objects”                         breathes            g(X,Y): “is_a”
                                                                                               f(X,Y): “has_a” imal can breathe”, and so on. The laws then determine that
                                                                         animal
                                                              can_ﬂy                      can_swim             properties inherit down chains of is a relations to generate
 Model
         q(X): “magnetic objects”                              bird                       ﬁsh                  many other true facts that can potentially be observed, e.g.,
                                                                                                               “A canary can breathe”.
                                                   canary            eagle        shark          salmon
                                                                                                                  Equipped with this hierarchical generative model, a learner
                                                   can_sing        has_claws      can_bite          is_pink
                                                                                                               can work backwards from observed data to multiple levels of
                                                                                                               latent structure. Given the correct theory, the learner can infer
                                                   “a shark is a ﬁsh”
                                                   “a bird can ﬂy”                                             the most likely model underlying a set of noisy, sparse obser-
 Data                                              “a canary can ﬂy”
                                                                                                               vations and predict facts that have not been directly observed
                                                            ..
                                                   “a salmon can breathe”
                                                             .                                                 (Katz et al., 2008; Kemp et al., 2008). If the true theory is
                                                                                                               unknown, the learner can consider a hypothesis space of can-
Figure 1: A hierarchical Bayesian framework for theory acquisition                                             didate theories, generated by higher-level “Universal Theory
                                                                                                               (UT )” knowledge. UT defines a distribution over the space
concepts are written in logical form, a “language of thought”,                                                 of possible theories, P(T |UT ), which can then be used by a
typically a subset of first-order logic. The learner’s model of a                                              learner to infer the correct theory describing a domain, ac-
system specifies what is true of that system, and thereby gen-                                                 cording to the standard Bayesian formulation:
erates a probability distribution over possible observations
that can be made for that system.                                                                                              P(T |D,UT ) ∝ P(D|T )P(T |UT )                (1)
   For example, consider a child learning about the domain
of magnetism. She might begin by playing with a few pieces                                                     Bayes’ rule here captures the intuition of Occam’s razor. The
of metal and notice that some of the objects interact, exerting                                                theory that best explains the data, or has highest posterior
strange pulling or pushing forces on each other. She could                                                     probability P(T |D,UT ), should be based on two considera-
describe the data directly, as “Object i interacts with object                                                 tions: how well the theory fits the data, as measured by the
f”, “Object i interacts with object j”, and so on. Or she could                                                likelihood P(D|T ), and how simple or short is the theory, as
form a simple theory, in terms of abstract concepts such as                                                    measured by the prior P(T |UT ). We now define these hy-
magnet, magnetic object and non-magnetic object, and laws                                                      pothesis spaces and probabilities more formally, and then de-
such as “Magnets interact with other magnets”, “Magnets                                                        scribe a learning algorithm that searches the space of theories
interact with magnetic objects”, and “Interactions are sym-                                                    by proposing small random changes to the current theory and
metric” (but no other interactions take place). Systems in this                                                accepting changes stochastically based on whether they are
domain correspond to specific subsets of objects, such as the                                                  likely to lead to higher overall probability.
set of objects {a, . . . , i} in Figure 1. A model of a system spec-                                              A language for theories. Following (Katz et al., 2008) we
ifies the minimal facts needed to apply the abstract theory to                                                 represent the laws in a theory as Horn clauses: logical expres-
the system, in this case which objects are magnetic, which are                                                 sions of the form t ← (p ∧ q ∧ ... ∧ r). Horn clauses express
magnets, and which are non-magnetic. From these core facts                                                     logical implications – a set of conjunctive conditions under
the laws of the theory determine all other true facts – in our                                                 which t holds – but can also capture intuitive causal relations
example, this means all the pairwise interactions between the                                                  under the assumption that any propositions not generated by
objects: e.g., objects i and j, being magnets, should interact,                                                the theory are assumed to be false. In our formulation, the
but i and e should not, because e is non-magnetic. Finally, the                                                clauses contain two kinds of predicates: “core” and “surface”.
true facts generate the actual data observed by the learner via                                                Core predicates are a minimal set of predicates that determine
a noisy observation process.                                                                                   all other predicates when combined with the theory’s laws.
   While the abstract concepts in this simplified magnetism                                                    Surface predicates are derived from other predicates, either
theory are attributes of objects, more complex relations are                                                   surface or core, via the laws. Predicates may or may not be
possible. Consider for example a domain of taxonomy, as                                                        directly observable in the data. The core predicates can be
in Collins and Quillian’s classic model of semantic memory                                                     seen as compressing the full model into just the minimal bits
as an inheritance hierarchy (Collins & Quillian, 1969). Here                                                   necessary to specify all true facts. In the magnetism example
the abstract concepts are is a relations between categories and                                                above, the core could be expressed in terms of two predicates
has a relations between categories and properties. The theory                                                  p(X) and q(X). Based on an assignment of truth values to
underlying taxonomy has two basic laws: “The has a relation                                                    these core predicates, the
                                                                                                           2841

    Top level theory                                                         P(X,Y)  ←  P(X,Z)∧P(Z,Y)          P(X,Y)  ←   P(X)∧P(Y)
       (S1)                    S        ⇒    (Law) ∧ S                       P(X,Y)  ←  P(Z,X)∧P(Z,Y)          P(X,Y)  ←   P(Y,X)
       (S2)                    S        ⇒    (Tem) ∧ S                       P(X,Y)  ←  P(X,Z)∧P(Y,Z)          P(X,Y)  ←   P(X,Y)
      (S3)                     S        ⇒    Stop                            P(X,Y)  ←  P(Z,X)∧P(Y,Z)          P(X)    ←   P(X)
                                                                             P(X,Y)  ←  P(X,Y)∧P(X)            P(X)    ←   P(X,Y)∧P(X)
    Random law generation                                                    P(X,Y)  ←  P(Y,X)∧P(X)            P(X)    ←   P(Y,X)∧P(X)
      (Law)                    Law      ⇒    (Ple f t ← Pright ∧ Add)        P(X,Y)  ←  P(X,Y)∧P(Y)            P(X)    ←   P(X,Y)∧P(Y)
      (Add1)                   A        ⇒    P ∧ Add                         P(X,Y)  ←  P(Y,X)∧P(Y)            P(X)    ←   P(Y,X)∧P(Y)
      (Add2)                   A        ⇒    Stop
                                                                               Figure 3: The list of templates available to in the PHCG.
    Predicate generation
      (Ple f t 1)              Ple f t  ⇒    sur f ace1()                the product of the probabilities of choices made at each point
      ..
       .                                                                 in this derivation. All these probabilities are less than one, so
       (Ple f t α)             Ple f t  ⇒    sur f aceα()                overall the prior favors simpler theories with shorter deriva-
       (Pright 1)              Pright   ⇒    sur f ace1()                tions. The precise probabilities of different rules in the gram-
       ..                                                                mar are treated as latent variables and integrated out, favoring
        .
        (Pright α)             Pright   ⇒    sur f aceα()                re-use of the same predicates and law components within a
        (Pright (α + 1))       Pright   ⇒    core1()                     theory (Goodman et al., 2008).
         ..                                                                 Law templates. We make the grammar more likely to gen-
          .
          (Pright (α + β))     Pright   ⇒    coreβ()                     erate useful laws by equipping it with templates, or canonical
                                                                         forms of laws that capture structure likely to be shared across
    Law templates                                                        many domains. While it is possible for the PHCG to reach
         (Tem1)                Tem      ⇒    template1()
        ..                                                               each of these law forms without the use of templates, their
         .                                                               inclusion allows the most useful laws to be invented more
         (Temγ)                Tem      ⇒    templateγ()                 readily. They can also serve as the basis for transfer learn-
Figure 2: Production rules of the Probabilistic Horn Clause Gram-        ing across domains. For instance, instead of having to re-
mar. S is the start symbol and Law, Add, P and Tem are non-              invent transitivity anew in every domain with some specific
terminals. α, β, and γ are the numbers of surface predicates, core       transitive predicates, a learner could recognize that the same
predicates, and law templates, respectively.
                                                                         transitivity template applies in several domains. It may be
                                                                         costly to invent transitivity for the first time, but once found
learner can use the theory’s laws such as
                                                                         – and appreciated! – its abstract form can be readily re-
interacts(X, Y) ← p(X) ∧ q(Y) to derive values for the
                                                                         used. The specific law templates used are described in Figure
observable surface predicate interacts(X, Y). Notice that
                                                                         3. Each “P(·)” symbol stands for a non-terminal represent-
p(X) and q(X) are abstract predicates, which acquire their
                                                                         ing a predicate of a certain -arity. This non-terminal is later
meaning as concepts picking out magnets or magnetic objects
                                                                         instantiated by a specific predicate. For example, the tem-
respectively in virtue of the role they play in the theory’s
                                                                         plate P(X, Y) ← P(X, Z) ∧ P(Z, Y) might be instantiated as
laws. In constructing such a theory the learner essentially
                                                                         is a(X, Y) ← is a(X, Z) ∧ is a(Z, Y) (a familiar transitive law)
creates new concepts (Carey, 1985). Entities may be typed
                                                                         or as has a(X, Y) ← is a(X, Z) ∧ has a(Z, Y) (the other key
and predicates restricted based on type constraints: e.g., in
                                                                         law of taxonomy, stating that “has a is transitive over is a”).
taxonomy, has a(X, Y) requires that X be a category and Y
                                                                            The theory likelihood P(D|T ). An abstract theory makes
be a property, while is a(X, Y) requires that X and Y both be
                                                                         predictions about the observed data in a domain only indi-
categories. Forcing candidate models and theories to respect
                                                                         rectly, via the models it generates. A theory typically gen-
these type constraints provides the learner with a valuable
                                                                         erates many possible models: even if a child has the correct
and cognitively natural inductive bias.
                                                                         theory and abstract concepts of magnetism, she could catego-
   The theory prior P(T |UT ). We posit UT knowledge in
                                                                         rize a specific set of metal bars in many different ways, each
the form of a probabilistic context-free Horn clause grammar
                                                                         of which would predict different interactions that could be
(PHCG) that generates the hypothesis space of possible Horn-
                                                                         observed as data. Expanding the theory likelihood,
clause theories, and a prior P(T |UT ) over this space (Figure
2). This grammar and the Monte Carlo algorithms we use to
                                                                                          P(D|T ) = ∑ P(D|M)P(M|T )                      (2)
sample or search over the theory posterior P(T |D,UT ) are                                              M
based heavily on Goodman, Tenenbaum, Feldman, and Grif-
fiths (2008), who introduced the approach for learning single            we see that theory T predicts data D well if it assigns high
rule-based concepts rather than the larger theory structures we          prior P(M|T ) to models M that make the data probable under
consider here. We refer readers to Goodman et al. (2008) for             the observation process P(D|M).
many technical details. Given a set of possible predicates in               The model prior P(M|T ) reflects the intuition that a the-
the domain, the PHCG draws laws from a random construc-                  ory T explains some data well if T requires few additional
tion process (Law) or from law templates (Tem; explained                 degrees of freedom beyond its abstract concepts and laws
in detail below) until the Stop symbol is reached, and then              to make its predictions. That is, few specific and contin-
grounds out these laws as horn clauses. The prior p(T |UT ) is           gent facts about the system under observation are required
                                                                     2842

in addition to the theory’s general prescriptions. This intu-        Bayesian analysis, we suggest that it could also capture in a
ition is captured by a prior that encourages the core predi-         schematic form the dynamic processes of theory acquisition
cates to be as sparse as possible, penalizing theories that can      and change in young children. Stochastic proposals to add a
only fit well by “overfitting” with many extra degrees of free-      new law or change a predicate within an existing law are con-
dom. Formally, following (Katz et al., 2008), we model all           sistent with some previous characterizations of children’s the-
values of the core predicates as independent Bernoulli ran-          ory learning dynamics (Siegler & Chen, 1998). These dynam-
dom variables with conjugate beta priors encouraging most            ics were previously proposed on purely descriptive grounds,
variables to have the same value (on or off). We assume that         but here they emerge as a consequence of a rational learning
any proposition potentially in the model M is false unless it is     algorithm for effectively searching an infinite space of logical
a core predicate turned on by this Bernoulli process or is de-       theories.
rived from the core predicates through the theory’s laws (the           Approximating the theory score. Computing the theory
minimal model assumption of logic programming).                      likelihood P(D|T ), necessary to compare alternative theories
   Finally, the model likelihood P(D|M, T ) comes from as-           in Equation 3, requires a summation over all possible mod-
suming that we are observing randomly sampled true facts             els consistent with the current theory (Equation 2). Because
(sampled with replacement, so the same fact could observed           this sum is typically very hard to evaluate exactly, we ap-
on multiple occasions), which also encourages the model ex-          proximate P(D|T ) with P(D|M ∗ )P(M ∗ |T ), where M ∗ is an
tension to be as small as possible.                                  estimate of the maximum a-posteriori (MAP) model inferred
   Stochastic search in theory space: a grammar-based                from the data: the most likely values of the core predicates.
Monte-Carlo algorithm. Following Goodman et al. (2008),              The MAP estimate M* is obtained by running a Gibbs sam-
we use a grammar-based Metropolis-Hastings (MH) al-                  pler over the values of the core predicates, as in (Katz et al.,
gorithm to sample theories from the posterior distribution           2008), annealing slightly on each Gibbs sweep to speed con-
over theories conditioned on data, P(T |D,UT ). This algo-           vergence and lock in the best solution. The Gibbs sampler
rithm is applicable to any grammatically structured theory           over models generated by a given theory is thus an “inner
space, such as the one generated by our PHCG. The MH                 loop” of sampling in our learning algorithm, operating within
algorithm proceeds by randomly proposing changes to the              each step of an “outer loop” sampling at a higher level of ab-
current theory, and accepting or rejecting these changes.            stract knowledge, the MH sampler over theories generated by
Each proposed change to the current theory corresponds               UT knowledge.
to choosing a grammatical constituent of the theory then
regenerating it from the PHCG. For example, if our theory of                                  Case Studies
magnetism includes the law interacts(X, Y) ← p(X) ∧ q(Y),            We now explore the performance of this stochastic approach
the MH procedure might propose to add or delete a                    to theory learning in two case studies, using simulated data
predicate     (e.g.,    interacts(X, Y) ← p(X) ∧ q(Y) ∧ p(Y)         from the domains of taxonomy and magnetism introduced
or interacts(X, Y) ← p(X)), to change one pred-                      above. We examine the learning dynamics in each domain
icate to an alternative of the same form (e.g.,                      and make more explicit the possible parallels with human the-
interacts(X, Y) ← p(X) ∧ p(Y)) or a different form                   ory acquisition.
if available (e.g.,          interacts(X, Y) ← p(X) ∧ r(X, Y));
to resample the law from a template (e.g.,
                                                                     Taxonomy
interacts(X, Y) ← r(X, Z) ∧ r(Z, Y)); or to add or delete            Katz et al. (2008) defined a similar hierarchical Bayesian
a whole law.                                                         framework and showed that a theory of taxonomic reason-
   These proposals are accepted with probability equal to the        ing about properties and categories in an inheritance hierar-
minimum of 1 and the MH acceptance ratio,                            chy could be correctly selected from among several alterna-
                                                                     tives, on the basis of data. However, they did not address the
                     P(T 0 |D,UT ) Q(T |T 0 )                        harder challenge of constructing the theory from the ground
                                   ·                         (3)     up, or selecting it from an effectively infinite hypothesis space
                     P(T |D,UT ) Q(T 0 |T )
                                                                     of theories (which could be used to describe many other do-
where T is the current theory, T 0 is the new proposed the-          mains). That is our goal here. Following Katz et al. (2008),
ory, and Q(·|·) is the transition probability from one theory        we take the correct theory to have two unobservable core
to the other, derived from the PHCG (Goodman et al., 2008).          predicates, g(X, Y) and f(X, Y), and two observable surface
To aid convergence we raise the posterior ratio to a power           predicates, is a(X, Y) and has a(X, Y). There are four laws:
greater than 1, which we increase very slightly after each MH
step in a form of simulated annealing. The learner initially               Law 1:       has a(X, Y) ← f(X, Y)
explores alternative theories freely, but with time becomes in-            Law 2:         is a(X, Y) ← g(X, Y)
creasingly likely to reject theory changes unless they lead to             Law 3:       has a(X, Y) ← is a(X, Z) ∧ has a(Z, Y)
an improved posterior probability.                                         Law 4:         is a(X, Y) ← is a(X, Z) ∧ is a(Z, Y)
   While this MH algorithm could be viewed merely as a way
to approximate the calculations necessary for a hierarchical          Laws 1 and 2 set up the core predicates to represent the mini-
                                                                 2843

                                                                          through theory space to a given endpoint, a sequence of ran-
                                                                          dom MH proposals may still prefer some orders of knowl-
                                                    2
                                                                          edge acquisition over others. Here, when law 4 is discovered
                                                                          (on 8/10 runs), it is always acquired after law 3. This is be-
                                                                          cause law 4 (transitivity of category membership) provides
Log posterior
                                                                          much more explanatory power – and hence is more stable
                                                                          under our stochastic theory-learning dynamics – given law 3
                                                                          (property inheritance) and a reasonable domain model spec-
                1
                                                                          ifying which properties hold for which categories. This or-
                                                                          der is also consistent with the order of acquisition in human
                                                                          cognitive development (Wellman & Gelman, 1992): children
                            Simulation iterations
                                                                          learn to generalize properties of biological categories to in-
 Figure 4: Log posterior score for representative runs of theory learn-   stances well before they learn that categories can be arranged
 ing in Taxonomy. Dashed lines show different runs. Solid line is the     in a multilevel hierarchy supporting transitive inferences of
 average across all runs. Node 1 marks the acquisition of law 3, node
 2 marks the acquisition of law 4.                                        category membership.
 mal is a and has a links, on top of which are defined the laws           Magnetism
 of property inheritance (Law 3) and transitive category mem-             After showing that stochastic search can learn the correct laws
 bership (Law 4). We take laws 1 and 2 as given, assuming                 in a domain theory, we now consider a second case study in
 the structure and meaning of the core predicates as Katz et al.          which the acquisition of new laws corresponds to a shift in
 did, and ask whether a learner can successfully construct laws           the meaning of the core predicates, and new (i.e., previously
 3 and 4. Following Katz et al., we consider a concrete domain            unassigned) core predicates are introduced during learning –
 with 7 categories and 7 properties in a balanced taxonomy, as            akin to some of the conceptual changes described by Carey
 shown in Figure 1. Observations include all positive facts as-           (1985). Our domain here is the simple version of mag-
 serting that a property is true of a category, as in “An eagle has       netism described above, with two unobservable core predi-
 claws”. (The data used for this section and the following case           cates: p(X) and q(X), and one observable surface predicate:
 study can be found at http://web.mit.edu/tomeru/www/tlss.)               interacts(X, Y). There are three laws:
 We ran 10 simulations for 1300 iterations of the outer MH
 loop. Learning curves for representative runs as well as the                   Law 1:        interacts(X, Y) ← p(X) ∧ p(Y)
 average over all runs are shown in Figure 4. Out of 10 simu-                   Law 2:        interacts(X, Y) ← p(X) ∧ q(Y)
 lations, 8 found the correct theory within the given number of
 iterations, and 2 discovered a partial theory which included                   Law 3:        interacts(X, Y) ← interacts(Y, X)
 only law 3 (property inheritance). Several observations are              We consider a concrete system with 3 magnets, 5 magnetic
 worth noting.                                                            objects and 2 non-magnetic objects. These concepts are ini-
    Abstract learning is possible. Using only stochastic local            tially unknown to the learner. The core predicates p(X) and
 search moves, a learner can navigate the space of potential              q(X) are completely abstract and initially uninterpreted. They
 theories to discover the laws underlying the domain. Even a              will acquire their meaning as concepts picking out magnets
 relatively small dataset (with 7 categories and 7 properties) is         and magnetic objects respectively in virtue of the role they
 sufficient to learn the correct abstract domain theory.                  play in the theory’s laws, specifying that objects in one sub-
    Individual learning curves show sudden changes and high               set (the p’s) interact with each other and with objects in a
 variability in what is learned when, while on average learn-             second set (the q’s), but q’s do not interact with each other.
 ing is smooth and follows a characteristic timecourse. The               In constructing a theory, the learner introduces these abstract
 learning algorithm’s local dynamics are highly stochastic and            predicates via new laws, or new roles in existing laws, and
 variable across runs, because of the randomness in what the-             thereby essentially creates these concepts where she did not
 ory changes are proposed when, and the fact that a small the-            have them before (Carey, 1985).
 ory change can make a big difference in predictive power. Yet               We ran 10 simulations for 1600 iterations of the outer MH
 there is still a meaningful sense in which we can talk about             loop. Representative runs are displayed in Figure 5, as well as
 “typical” learning behavior, even though any one learner may             the average over all the runs. The results were similar to the
 not look much like this average. If stochastic local search is           taxonomy case study in several respects, which we also ex-
 a key component in children’s theory construction, it could              pect to hold for a variety of other domains. The correct theory
 explain why cognitive development shows this same dual                   was usually learned, with some variation: 9/10 simulations
 nature: systematic and graded progression at the popula-                 found the correct theory or a variant of it, and one discov-
 tion level, despite random, discontinuous and highly variable            ered a partial theory containing only law 1. Only some runs
 learning rates in any one child.                                         learned the exact form of law 3, asserting that interactions
    Although proposals are random, there is a systematic and              are symmetric. Others found variants that were extension-
 rational order to learning. While there are many routes                  ally equivalent to symmetry in this domain, but slightly more
                                                                      2844

                                                                              man cognitive development. These results suggest that previ-
                                                                              ous ideal learning analyses of Bayesian theory acquisition can
                                                                              be realized approximately by algorithms that are cognitively
                                                                              plausible for child learners, and indeed potentially descriptive
Log posterior
                                                                              of the dynamics of development.
                                                                                 Previous hierarchical Bayesian analyses of learning ab-
                     1
                                                                              stract knowledge have focused on the role of accumulating
                                                                              data in driving changes to the learner’s hypotheses (Kemp &
                                                                              Tenenbaum, 2008). In contrast, here we have focused on how
                                                                              changes to the learner’s theories and abstract concepts are
                                                                              driven by a different source, the stochastic dynamics of the
                                Simulation iterations                         learning algorithm. Data-driven and algorithm-driven theory
                                                                              change can have a similar character, first discovering simpler,
 Figure 5: Representative runs of theory learning in Magnetism.
 Dashed lines show different runs. Solid line is the average across           rougher approximations to reality and then refining those to
 all runs. Node 1 marks the acquisition of law 1 and the confounding          more complex, accurate representations; sometimes chang-
 of magnets with magnetic objects. Lower right panel zooms into the           ing by adjusting small details, but other times by making
 end of the simulation, showing acquisition of the final correct theory.
                                                                              large qualitative transitions or discoveries. In future work we
 complex in their logical form. Individual runs of learning                   plan to explore further the similarities, differences and inter-
 showed discrete jumps with high variability, while average-                  actions between these two drivers of learning dynamics, both
 case behavior was smooth, with systematic order effects. Law                 in computational analyses and experimental work. We hope
 3 is never learned first, because alone it has no explanatory                to establish tighter quantitative correspondences with human
 power. Either law 1 or the combination of laws 2 and 3 tend                  learning curves in development, as well as with controlled
 to be learned first, followed by the other, although sometimes               laboratory studies of theory learning in adults, where some of
 laws 1 and 2 are learned first, followed by law 3. Law 1 tends               the same mechanisms might be at work. We will also con-
 to be learned first overall because it is most likely under the              sider a broader range of algorithmic approaches, stochastic
 prior (which is also the proposal distribution for local search              as well as deterministic, evaluating them both as behavioral
 moves), and also because, as explained below, it represents a                models and as effective computational approximations to the
 reasonable first approximation to the domain’s structure.                    theory search problem for larger domains.
    The algorithm’s learning dynamics in this case study are                     Acknowledgments. We thank Yarden Katz for valuable discus-
 particularly interesting for how they parallel key transitions in            sions. This work was funded in part by grants from the McDonnell
                                                                              Causal Learning Collaborative, AFOSR (FA9550-07-1-0075), and
 childrens’ cognitive development: restructuring or construc-                 ONR (N00014-09-0124).
 tion of new concepts, as when one concept differentiates into
 two (Carey, 1985). When our simulations of learning about                                              References
 magnetism construct law 1 first, without laws 2 and 3, they                  Carey, S. (1985). Conceptual change in childhood. Cambridge,
                                                                                MA: MIT Press/Bradford Books.
 find a simpler theory capturing many of the observed facts                   Collins, A., & Quillian, M. (1969). Retrieval time from semantic
 at the cost of over-generalizing. That is, under law 1 alone,                  memory. Journal of Verbal Learning and Verbal Behavior, 8,
 the optimal setting of the core predicates – the most proba-                   240–247.
                                                                              Goodman, N. D., Tenenbaum, J. B., Feldman, J., & Griffiths, T. L.
 ble model – equates magnets and magnetic objects, making                       (2008). A rational analysis of rule-based concept learning. Cog-
 p(X) true for both. This is a good first approximation, even                   nitive Science, 32(1), 108—154.
 as it collapses two categories of objects with fundamentally                 Goodman, N. D., Ullman, T. D., & Tenenbaum, J. B. (2009). Learn-
 different causal properties: the generators of magnetic force                  ing a theory of causality. In Proceedings of the 31st annual con-
                                                                                ference of the cognitive science society.
 (the “magnets”) and the objects on which that force acts (the                Katz, Y., Goodman, N. D., Kersting, K., Kemp, C., & Tenenbaum,
 “magnetic objects”). Only once all three laws have been con-                   J. B. (2008). Modeling semantic cognition as logical dimension-
 structed does the learner come to distinguish between mag-                     ality reduction. Proceedings of the Thirtieth Annual Conference
                                                                                of the Cognitive Science Society.
 nets and magnetic objects, reflected in the difference between               Kemp, C., Goodman, N. D., & Tenenbaum, J. B. (2008). Theory ac-
 the roles played by the two core predicates p(X) and q(X).                     quisition and the language of thought. In Proceedings of thirtieth
 Only once law 2 is available does the learner have reason to                   annual meeting of the cognitive science society.
                                                                              Kemp, C., & Tenenbaum, J. B. (2008). The discovery of structural
 restrict the extension of p(X) to just magnets, excluding other                form. Proceedings of the National Academy of Sciences, 105(31),
 magnetic objects.                                                              10687–10692.
                                                                              Siegler, R. S., & Chen, Z. (1998). Developmental differences in
                Conclusion and Future Directions                                rule learning: A microgenetic analysis. Cognitive Psychology,
                                                                                36, 273-310.
 We have presented an algorithmic model of theory acquisition                 Tenenbaum, J. B., Griffiths, T. L., & Kemp, C. (2006). Theory-based
 as stochastic search in a hierarchical Bayesian framework and                  bayesian models of inductive learning and reasoning. Trends in
 explored its dynamics in two case studies. We were encour-                     Cognitive Sciences, 10, 309–318.
                                                                              Wellman, H. M., & Gelman, S. A. (1992). Cognitive development:
 aged by the general pattern of successes on these examples                     Foundational theories of core domains. Annual Review of Psy-
 and by several qualitative parallels with phenomena of hu-                     chology, 43, 337-375.
                                                                           2845

