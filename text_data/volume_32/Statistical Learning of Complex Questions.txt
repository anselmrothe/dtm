UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Statistical Learning of Complex Questions
Permalink
https://escholarship.org/uc/item/0s62x4q5
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Author
Fitz, Hartmut
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                     Statistical Learning of Complex Questions
                                                        Hartmut Fitz (h.fitz@rug.nl)
                                Center for Language and Cognition Groningen, Oude Kijk in’t Jatstraat 26
                                                      9712EK Groningen, the Netherlands
                                Abstract                                is accreting evidence, for instance, that learning the syn-
   The problem of auxiliary fronting in complex polar questions         tax of questions does not involve learning movement rules
   occupies a prominent position within the nature versus nurture       (Dabrowska & Lieven, 2005; Estigarribia, 2009). An inad-
   controversy in language acquisition. We employ a model of            equate description of the learning target in terms of transfor-
   statistical learning which uses sequential and semantic infor-
   mation to produce utterances from a bag of words. This linear        mational rules might obscure empiricist solutions to the prob-
   learner is capable of generating grammatical questions without       lem. Secondly, auxiliary fronting has been isolated from all
   exposure to these structures in its training environment. We         the rest of language. Although there is some consensus that
   show that the model performs superior to n-gram learners on
   this task. Implications for nativist theories of language acqui-     structures (1-a) are highly infrequent, the input environment
   sition are discussed.                                                of a child might provide other sources of indirect evidence
   Keywords: Language acquisition; complex syntax; poverty of           for the correct rule (Pullum & Scholz, 2002). Another crit-
   the stimulus; statistical learning; distributional information.      ical assumption is that the structure-independent rule (1-b)
                                                                        is simpler and should be preferred in the absence of innate
                           Introduction                                 constraints. Yet, if there is no reason to believe that children
It is a central question in language acquisition which aspects          should overgeneralize there is no explanatory necessity for
of our knowledge of language are learned from experience                such constraints; the nativist argument would be preempted.1
and which are part of our biological endowment for language.               Despite these reservations, it is clear that any theory of lan-
Nativist arguments often identify some property of a language           guage acquisition which places more emphasis on the role of
and argue that it is not learnable from typical child-directed          experience needs to explain how the syntax of complex ques-
speech. By abductive reasoning, innate language-specific                tions can be acquired. Ideally, such an explanation demon-
knowledge is offered as the best explanation of why children            strates that a concrete, implemented learning mechanism built
come to know this property regardless. The problem of auxil-            on justifiable assumptions can acquire auxiliary fronting from
iary fronting in so-called complex polar questions (CPQ here-           plausible input distributions.
after) is a key issue in this nature versus nurture debate.
   According to Chomsky (1980), English yes/no-questions                           Linear versus hierarchical models
are formed from declaratives by displacing an auxiliary. The            Several models of language learning have recently been pro-
sentence “The man is happy” transforms into a question by               posed which explicitly address the issue of auxiliary fronting.
subject-auxiliary inversion: “Is the man happy?”. Declara-              These models can roughly be divided into linear and hier-
tives with a relative clause can contain two identical auxil-           archical approaches. Linear models do not explicitly repre-
iaries as in “The man that is hungry is happy”. Chomsky                 sent the hierarchical structure of a sentence’s organization
asked how children could learn that the main clause auxil-              into phrases and clauses. All models briefly discussed here
iary should be placed in front, rather than the auxiliary which         share the assumption that CPQs do not occur in child-directed
comes first. Only the former rule yields a grammatical CPQ.             speech, they learn solely from indirect evidence.
                                                                           In the framework of data-oriented parsing, Bod (2009)
(1)      a. Is the man that is hungry happy?                            showed that derivations of parse trees for grammatical CPQs
         b. *Is the man that hungry is happy?                           are shorter (or more probable) than those for ungrammatical
He claimed that children have no basis in experience to adopt           CPQs given the training data. The model assumes that sub-
the correct rule since examples such as (1-a) do not occur in           trees are representational primitives in the mind of a human
child-directed speech. In addition, children should adopt the           learner. Structure-dependent processing is built into the learn-
rule which generates (1-b) because (i) it is supported by ex-           ing mechanism but it is still a question of linguistic experi-
perience of simple yes/no-questions and (ii) the correct rule           ence whether the correct generalizations are supported in this
is “far more complex” in that it requires sensitivity to the            model. Perfors, Tenenbaum, and Regier (2006) demonstrated
hierarchical structure of a sentence. But children rarely, if           that an ideal Bayesian learner favors a hierarchical over a lin-
ever, make mistakes as in (1-b) (Crain & Nakayama, 1987;                ear grammar to fit a training corpus. This grammar could
Ambridge, Rowland, & Pine, 2008). They do not seem to                   parse grammatical CPQs while the linear grammar could not.
generalize in a structure-independent way. To explain this              The model, however, did not strictly learn grammars from
error-free behavior, Chomsky postulated innate structure-de-            data, but rather selected one from a given set. How grammar
pendent constraints on learning.                                        selection bears on the process of child-language acquisition
   The above formulation of this poverty-of-the-stimulus ar-                1 A more detailed discussion of the assumptions behind this na-
gument makes a number of controversial assumptions. There               tivist argument can be found in Fitz (2009).
                                                                    2692

needs to be elucidated. They argued that linear models have          er for short). This model of syntax learning was introduced
little to contribute to the auxiliary fronting debate because        in Chang, Lieven, and Tomasello (2008) where it was tested
structure-dependent processing requires hierarchical repre-          on a variety of typologically-distinct languages. Formal def-
sentations. This assumption has been challenged by a num-
ber of linear approaches. If a linear model learning auxiliary
                                                                                                                  Adjacency
fronting behaved in a manner consistent with structure-de-                   ?
                                                                                   is
pendent processing, this would suggest that explicit represen-                           the
                                                                                                boy
tations of hierarchical structure might be superfluous. Clark                                           that
                                                                                                                is
                                                                                                                      dirty
and Eyraud (2006) proposed a linear alignment learner which                                                                   happy
substituted relativized NPs for simple NPs if they occurred in
                                                                            Prominence
identical contexts in the corpus. As a result, the learner could
generate grammatical CPQs if and only if their component
clauses occurred in training. A simple recurrent network was         Figure 1: Adacency-prominence statistics for the CPQ Is the
used by Lewis and Elman (2001) to successfully learn CPQs            boy that is dirty happy? (adapted from Chang et al. (2008)).
from artificial language input. The scope of this approach is
difficult to assess since the model seems to have been tested        initions of the two kinds of statistics are given in Table 1.
on a single item only. The most widely received linear ap-           Note that the adjacency statistic differs from forward transi-
proach used n-gram learners on untagged corpora of child-
directed speech (Reali & Christiansen, 2005). The authors                               Table 1: AP-learner statistics.
showed that a Bigram model could reliably classify pairs of
grammatical/ungrammatical CPQs by assigning higher sen-              C(wn−1 , wn )     Frequency of bigram wn−1 wn
tence probability to the former on 96% of the tested items.          Pair(wa , wb )    Frequency of words wa , wb occurring
They suggested that indirect statistical information extracted                         together in the same sentence in any order
from strings of words might be sufficient for children to in-        P(wa , wb )       Frequency of word wa occurring before wb
fer the correct rule of auxiliary fronting. These results were                         in a sentence at any distance
scrutinized by Kam, Stoyneshka, Tornyova, Fodor, and Sakas           Length            Number of words in bag-of-words
(2008) who argued that the success of the Bigram model was           η                 Balance parameter2
largely due to a single distinguishing bigram which was sup-
                                                                     Adjacency         Adj(wn ) = C(wn−1 , wn )/Pair(wn−1 , wn )
ported by accidental phonological facts about English. When
they added structural and lexical diversity to the test items,       Prominence        Pro(wn ) = ∑wb P(wn , wb )/Pair(wn , wb )
the model failed. Moreover, they argued that the bigram ap-                            where wb are all words in the bag (except wn )
proach might not be valid cross-linguistically.
                                                                     Adjacency-Prominence
         The Adjacency-Prominence learner                                              AP(wn ) = Length × Adj(wn ) + Pro(wn ) × η
In our own work we aimed at showing that these difficul-
ties could be overcome by a linear statistical model which           tional probabilities because bigram counts are normalized by
in addition to n-gram based sequence learning uses meaning           the frequency of word pairs instead of the first unigram. The
to constrain sentence production. The statistical information        prominence statistic of a word is a sum over its prominence
on which the learner draws had two components. The ad-               relation with other words. To give a comparable weight to
jacency statistic was collected over bigrams in the training         the adjacency statistic, it was multiplied by the number of re-
corpus. It measured how often two words which co-occurred            maining words in an utterance.
in sentences, occurred adjacent to each other. The key addi-
                                                                     Evaluation
tion over n-gram models was the prominence statistic. The
learner tracked which words frequently preceded other words          The performance of the AP-learner was evaluated in a sen-
in the input environment. Words which on average were                tence generation task. We assumed that speakers aim to pro-
found earlier in a sentence than other words were considered         duce utterances which express the meaning they intend to
more prominent. Using this statistic, a hierarchy was created        convey. To approximate constraints that meaning places on
which ordered words in an utterance in terms of their promi-         sentence production, a target utterance was split into an un-
nence. More prominent words then tended to be sequenced              ordered bag-of-words. The learner then had to use its syn-
earlier in production. While the adjacency statistic selected        tactic knowledge, extracted from the training corpus, to order
words based on the previous word in an utterance, the promi-         this bag-of-words. Sentences were produced incrementally
nence statistic selected words based on their prominence re-         one word at a time. At each word position, all words in the
lation with remaining words in an utterance. This process            bag were competing for the next slot in the utterance. The
is illustrated schematically in Figure 1. Both statistics were           2 This parameter was used to calibrate the contribution of both
combined into the Adjacency-Prominence learner (AP-learn-            statistics to word choice. It was held fixed across experiments.
                                                                 2693

learner could use forward probabilities from the preceding            Table 2: Structures generated by the artificial language.
word (adjacency) but also the prominence ordering over the
words left in the bag to predict the next word. The promi-           Sentence type               Example
nence value for a given word could dynamically change as             Simple declarative          The guys buy it.
the set of word options diminished during production.                Simple polar question       Was the dog sleeping?
   Training and test items were identified as questions or           Complex declarative         A girl that is hitting him plays.
declaratives by prepending a marker quest/decl to each sen-          Complex polar question      Is a cat that is grumpy thirsty?
tence. Utterance generation was initialized by creating a bag-
of-words including the marker for the target sentence. For
each word in the bag, the adjacency and prominence statistic       dependent generalizations such as
was collected and the word with the highest combined value         (2)      Are the boys that running are eating?
was selected (see Table 1). The word was appended to the
marker and removed from the bag-of-words. This procedure           may not occur in development because they violate word co-
continued recursively until the bag was empty. The string of       occurrence patterns in English (boldface bigram). In similar
words produced by the learner was compared with the tar-           vein, Kam et al. (2008) argued that the good performance of
get utterance and its grammatical alternatives. For instance,      the Reali and Christiansen (2005) model was due to these
the bag-of-words obtained from “Is the dog that is running         relative clause initial bigrams. To ensure that our learner
happy?” also generated “Is the dog that is happy running?”. If     could, in principle, generalize erroneously, we separated plu-
the learner produced either form, the sentence prediction ac-      ral markers and inflectional morphemes for tense and aspect
curacy count was incremented. Likewise, if either of the un-       from the word stem. Thus sentence (2) was represented in our
grammatical alternatives (with a displaced embedded clause         artificial language as
auxiliary) was produced, the output was counted as a struc-
ture-independent generalization error.                             (3)      Are the boy -s that run -ing are eat -ing?
   Reali and Christiansen (2005) tested their n-gram learn-        The boldface bigram occurred frequently in the training cor-
ers in a grammaticality judgement task in which CPQs with          pus, for example in sentences such as “The boy -s that run
lower cross-entropy were classified as grammatical. Our            are kick -ing the toy”. This made it more difficult for the
learner, in contrast, had to actually produce sentences from a     AP-learner to retain the embedded clause auxiliary in CPQs.
bag-of-words and not merely classify them. Statistical infor-
mation sufficient for classification might not be suitable for                                Results
production. Chang et al. (2008) argued that bag-of-word gen-
                                                                   Experiment 1
eration is an adequate task to assess and compare statistical
learners across languages.                                         The first experiment tested whether the AP-learner was able
   The remainder of this paper is organized as follows. First,     to produce correct CPQs when trained only on simple declar-
we demonstrate that the AP-learner can learn the syntax of         atives, simple polar questions and declaratives with relative
complex questions in the absence of positive evidence and          clauses. The learner was trained on 20.000 sentences ran-
that overgeneralization does not occur. Then we compare the        domly generated from the artificial language. 50% of these
AP-learner with n-gram models and show that it performs su-        were simple sentences, the others were complex. 50% of
perior. Finally, we identify conditions under which the AP-        the simple sentences were questions, the others were declar-
learner does make structure-independent errors. Such con-          atives. Crucially, the training corpus did not contain any in-
ditions arguably do not obtain in child-language acquisition.      stance of a CPQ or any other question with a relative clause.
We conclude with a discussion of the results.                      Thus, it was tested whether the statistical information con-
                                                                   tained in the trained structures was sufficient for the AP-
                           Method                                  learner to generalize to the syntax of the novel CPQs. If so,
                                                                   this would support the idea that indirect evidence from fre-
Language input                                                     quent structures which are attested in child-directed speech
The AP-learner was trained on an artificial English-like lan-      might be sufficient to learn the correct subject-auxiliary in-
guage with transitives and intransitives as basic construction     version rule for complex polar questions.
types. From these constructions, simple declaratives, sim-            The test set contained 40 CPQs randomly generated by the
ple polar questions, complex declaratives, and polar ques-         artificial grammar. All CPQs had an intransitive main clause.
tions with relative clauses could be generated (see Table 2).      20 had a center-embedded intransitive relative-clause (II), and
The language had number and noun-verb agreement, tense             20 had a transitive relative-clause. Half of the transitive em-
(past/present) and aspect (progressive/simple). Nouns could        beddings were subject-relativized (ITS), the other half were
be animate and inanimate, or substituted by pronouns. Over         object-relativized (ITO). All tested CPQs were ambiguous in
a lexicon of 104 words and inflectional morphemes the lan-         that the main clause auxiliary was identical with the embed-
guage generated approximately 2.8 × 109 distinct sentences.        ded clause auxiliary. Auxiliaries could be singular or plural,
It was suggested by Ambridge et al. (2008) that structure-in-      past or present tense. Three actual test questions are listed
                                                               2694

in Table 3. In contrast to the study of Reali and Christiansen              we added either ambiguous CPQs or CPQs with mixed num-
                                                                            ber, tense and aspect (or both) to the training set, the learner’s
                             Table 3: Sample test questions.                performance did not improve on any of the tested question
                                                                            types. These results suggest that the distributional informa-
   Type                Example                                              tion contained in simple polar questions and complex declar-
   II                  Were the boy -s that were dirty play -ing ?          atives support the learning of structure-dependent general-
   ITS                 Was a brother that was push -ing them hungry ?       izations even if the learner does not explicitly represent the
   ITO                 Is a cat that a boy is chase -ing jump -ing ?        hierarchical organization of CPQs into clauses and phrasal
                                                                            units. Since both these structures—simple questions and rel-
(2005), the set of tested CPQs was structurally diverse (in-                ative clause constructions—typically occur in child-directed
transitive and transitive embeddings, subject- and object-rel-              speech, children might be exposed to sufficient indirect evi-
ativized) and not limited to the auxiliary “is”.                            dence to induce the syntax of auxiliary fronting in the absence
   When evaluating the learner’s output for ITS and ITO ques-               of positive examples.
tions, only those grammatical alternatives were considered
                                                                            Experiment 2
which preserved clause type and the grammatical role of the
relativized constituent. For instance, when tested on ITOs,                 In the previous experiment, the AP-learner showed differ-
the learner’s utterance had to have an intransitive main clause,            ences in production accuracy between II, ITS and ITO ques-
and the transitive embedding had to have an object gap in or-               tions. To trace the origin of differential performance, it was
der to count as an accurate production. The results of this                 helpful to compare the AP-learner with Bi- and Trigram mod-
experiment are shown in Figure 2.3 The mean sentence pre-                   els of statistical learning. Both these models were trained,
                                                                            tested and evaluated in exactly the same way as the AP-learn-
                       100
                                                                            er. Figure 3 shows the prediction accuracy of the different
                                                           Correct
                                                           Incorrect
                                                                            models by CPQ type. All models displayed the same qualita-
                       80
                                                                                                                        100
                                                                                                                                                   AP
          Percentage
                       60                                                                                                                          Trigram
                                                                                     Sentence prediction accuracy (%)
                                                                                                                                                   Bigram
                                                                                                                        80
                       40
                                                                                                                        60
                       20
                                                                                                                        40
                       0
                                  II          ITS         ITO
                                                                                                                        20
                                          Question Type
                                                                                                                        0
     Figure 2: AP-learner tested on three kinds of CPQs.
                                                                                                                              II       ITS         ITO
diction accuracy was 91.25% versus 8.75% incorrect produc-                                                                         Question Type
tions. On CPQs of type II, the AP-learner reached 100% ac-
curacy. Slightly lower was the accuracy on ITS (94%) and                        Figure 3: AP-learner in comparison with n-gram models.
ITO structures (71%). This difference between subject- and
object-relativized transitives is consistent with developmental             tive behavior in that II questions were easier to produce than
data on relative-clause acquisition in English-speaking chil-               ITS, which were easier than ITO. Both n-gram models per-
dren (Diessel & Tomasello, 2005). The AP-learner made                       formed similar to the AP-learner on II questions. These CPQs
mistakes on this task, it did not produce all test questions cor-           were shorter than the other question types and thus had fewer
rectly. Importantly, however, none of the learner’s incorrect               choice points for prediction error. Moreover, ungrammatical
productions matched an ungrammatical CPQ which would re-                    II questions frequently contained word sequences which were
flect structure-independent generalization. Although the AP-                not supported by the training corpus (e.g., “that happy”). The
learner did not experience any instance of a CPQ in training,               models followed a principle of non-monotonic learning to
it correctly generalized the syntax of subject-auxiliary inver-             produce grammatical II questions: in the absence of evidence
sion from simple polar questions and declaratives with rel-                 to the contrary, embedded clause auxiliaries should not be
ative clauses to the formation of complex questions. When                   omitted. The Trigram model came close to the AP-learner on
    3 All modelling data reported here are averaged over 10 randomly        ITS questions (82%), whereas the Bigram model dropped be-
generated training sets to ensure that results were robust with regard      low 40% accuracy. Errors made by the Bigram model mostly
to the artificial language used to create input environments.               occurred sentence-initially (e.g., “quest Is chase”), whereas
                                                                         2695

Trigram model errors mostly occurred in the relative clause            in complex declaratives from the training set. The auxiliary
(e.g., wrong verb type). The AP-learner was less vulnerable            are2 resembled the main clause auxiliary in complex declar-
to these kinds of errors because it did not rely exclusively           atives. These similarities were picked up by the adjacency-
on co-occurrence frequencies. In addition to adjacency, the            prominence statistics, as shown in Figure 4. Now the AP-
model could also use the prominence statistic which informed
it that a subject should precede a verb form in the main clause
                                                                                              Correct CPQ
and that a transitive verb should be produced in the relative                                 Structure−independent generalization
clause (instead of an intransitive) when there was a direct ob-
ject (e.g., a pronoun) left to sequence in the bag-of-words.                        70
Neither n-gram model produced any correct ITO question,
                                                                                    60
whereas the AP-learner produced 71% correct ITOs. The Bi-
gram model made the same errors as in ITS questions and                             50
sequenced a verb form after the initial auxiliary. The Trigram                      40
model often converted ITOs into grammatical ITS questions.
The AP-learner also made such conversion errors, but less                           30
frequently. Again, the prominence statistic helped the model                        20
to place subject noun phrases before the verb form in transi-
tive embeddings and this information was not available to the                       10
other models.                                                                       0
    Kam et al. (2008) argued that Bigram models are not suf-
                                                                                             II           ITS            ITO
ficient to learn the syntax of complex questions from noisy,
realistic corpora. Our results support their findings for ide-         Figure 4: Structure-independent errors occurred when multi-
alized input environments. The AP-learner was superior to              ple auxiliaries were distinguished in the corpus.
both n-gram models when tested CPQs could not reliably be
generated from a bag-of-words based on forward probabili-
                                                                       learner produced only 13.75% correct CPQs. Out of the to-
ties alone.
                                                                       tal incorrect CPQs, 65.5% were structure-independent errors
Experiment 3                                                           in which the question-initial auxiliary was omitted from the
                                                                       relative clause rather than the main clause. Hence, the AP-
As mentioned in the introduction, Chomsky’s argument for               learner could be forced to generalize erroneously when con-
the innateness of structure-dependent constraints on language          stituents were forward marked. Children, however, learn the
learning has two prongs. Children have no basis in experience          syntax of questions from input which is not marked in this
to infer the correct rule for auxiliary fronting, and they should      way. It is therefore not self-evident, as Chomsky suggests,
overgeneralize by displacing the linearly-first auxiliary, as          that children should adopt the wrong auxiliary fronting hy-
witnessed in simple polar questions in their language input.           pothesis in the absence of innate constraints. In order to sub-
In Experiment 1, we found no evidence for either claim. The            stantiate this claim, one would have to argue that children
AP-learner could produce more than 90% grammatical CPQs                perceptually distinguish and track multiple auxiliary tokens
without having experienced such structures in training. Al-            in a way similar to the AP-learner in the above experiment.
though the model made some mistakes, it never produced                 Unless this can be done convincingly, there is no reason to be-
ungrammatical CPQs in which the embedded clause auxil-                 lieve that children should overgeneralize. As a consequence,
iary was omitted. In a third experiment we attempted to elicit         it is no longer puzzling that they in fact rarely do (Crain &
overgeneralizations by creating input conditions which mis-            Nakayama, 1987; Ambridge et al., 2008). Moreover, there is
lead the AP-learner into producing structure-independent er-           no need to posit innate constraints on learning as the best ex-
rors. To do this, multiple word tokens were distinguished with         planation of why they do not. One crucial premiss of the pov-
markers in forward order of their occurrence within one sen-           erty-of-the-stimulus argument breaks away. Experiments 1 &
tence. Question (3), for instance, was now represented as              3, we believe, jointly shift the burden of proof back to those
                                                                       who claim that a biological endowment for structure-depen-
(4)    are1 the1 boy1 -s1 that1 run1 -ing1 are2 eat1 -ing2 ?
                                                                       dent processing is necessary to block overgeneralization.
After the model had produced a CPQ from a marked bag-of-
words, the markers were removed and the output was com-                              Discussion and conclusions
pared with the equally unmarked target questions (grammati-            Using a statistical model of syntactic development adapted
cal and ungrammatical versions).                                       from Chang et al. (2008), we demonstrated that the syntax
   Distinguishing constituents in this way created clause-spe-         of complex polar questions was learnable to a high degree
cific similarities between auxiliaries in different structures.        of accuracy even when these structures were not present in
The auxiliary are1 in test item (4) resembled the auxiliary            the language input to the model. The tested questions were
in simple polar questions and the embedded clause auxiliary            more diverse, both lexically (auxiliaries) and structurally (rel-
                                                                    2696

ative clause types), than the items used in Reali and Chris-         distributional information might be sufficient for a statistical
tiansen (2005) which may answer to some of the criticism             learner to resolve the Chomskyan challenge.
posed by Kam et al. (2008). Our learner, however, was col-
lecting more than n-gram statistics to accomplish this task.                             Acknowledgments
In addition to adjacency, it used a prominence ordering over         Thanks to Franklin Chang for helpful discussions.
words that were left to sequence. Words which were more
prominent in sentences of the learner’s experience were more                                  References
accessible for production. Thus, the AP-learner was not re-          Ambridge, B., Rowland, C. E., & Pine, J. M. (2008). Is
lying on the presence (or absence) of particular bigrams to            structure dependence an innate constraint? New experi-
produce grammatical questions and it outperformed several              mental evidence from children’s complex-question produc-
n-gram models. Importantly, it was also shown that errors the          tion. Cognitive Science, 32, 222–255.
learner made did not reflect structure-independent generaliza-       Bod, R. (2009). From exemplar to grammar: A probabilis-
tions. To elicit these errors, the learning environment had to         tic analogy-based model of language learning. Cognitive
be manipulated such that it no longer resembled natural lan-           Science, 33, 752–793.
guage input to children. This casts some doubt on the claim          Chang, F., Lieven, E., & Tomasello, M. (2008). Automatic
that children should overgeneralize in the absence of innate           evaluation of syntactic learners in typologically-different
constraints.                                                           languages. Cognitive Systems Research, 9, 198–213.
                                                                     Chomsky, N. (1980). Language and learning: The debate
   On the downside, the AP-learner was trained on an arti-
                                                                       between Jean Piaget and Noam Chomsky (M. Piattelli-Pal-
ficial English-like language which did not exhibit the noisi-
                                                                       marini, Ed.). Cambridge, MA: Harvard University Press.
ness, diversity and distributional properties of child-directed
                                                                     Clark, A., & Eyraud, R. (2006). Learning auxiliary fronting
speech. Our results should therefore be interpreted as a proof-
                                                                       with grammatical inference. In Proceedings of the 28th
of-concept that under idealized conditions a statistical learner
                                                                       Annual Conference of the Cognitive Science Society. Van-
which draws on sequential and semantic information can
                                                                       couver, BC.
learn the syntax of complex polar questions from simpler and
                                                                     Crain, S., & Nakayama, M. (1987). Structure dependence in
similar structures in the input. It remains to be tested whether
                                                                       grammar formation. Language, 63(3), 522–543.
this approach scales to real corpora and in particular whether
                                                                     Dabrowska, E., & Lieven, E. (2005). Towards a lexically spe-
it works for different languages which permit complex polar
                                                                       cific grammar of children’s question constructions. Cogni-
questions other than auxiliary-initial ones (Kam et al., 2008).
                                                                       tive Linguistics, 16, 437–474.
   We do not suggest here that the adjacency-prominence              Diessel, H., & Tomasello, M. (2005). A new look at the
statistic is all that is required to learn the syntax of complex       acquisition of relative clauses. Language, 81(4), 882–906.
questions. For one thing, the learner made mistakes where            Estigarribia, B. (2009). Facilitation by variation: Right-to-
adults do not. The inclusion of meaning constraints (bag-              left learning of English yes/no questions. Cognitive Sci-
of-words) into a statistical learning model was not sufficient         ence, 34, 68–93.
to guarantee error-free learning or rule out the production of       Fitz, H. (2009). Neural syntax. ILLC dissertation series,
grammatical alternatives. Tighter semantic constraints and             University of Amsterdam.
additional sources of information might be necessary.                Kam, X., Stoyneshka, I., Tornyova, L., Fodor, J., & Sakas, W.
   Compared with other models which have previously been               (2008). Bigrams and the richness of the stimulus. Cognitive
proposed to show the data-driven learnability of auxiliary             Science, 32, 771–787.
fronting, the AP-learner did not make assumptions about              Lewis, J., & Elman, J. (2001). Learnability and the statistical
the nature of syntactic representations in children, or the            structure of language: Poverty of stimulus arguments revis-
operations performed on such representations. The model                ited. In Proceedings of the 26th Annual Boston University
learned from untagged raw text by means of simple, domain-             Conference on Language Development. Somerville, MA.
general mechanisms and did not incorporate language-specif-          Perfors, A., Tenenbaum, J., & Regier, T. (2006). Poverty of
ic knowledge or biases. The model’s task to produce rather             the stimulus? A rational approach. In Proceedings of the
than classify sentences is closer to experimental paradigms            28th Annual Conference of the Cognitive Science Society.
in developmental psychology than grammaticality judgement              Vancouver, BC.
and incremental word prediction is consistent with current           Pickering, M. J., & Garrod, S. (2007). Do people use lan-
theories of language processing (Pickering & Garrod, 2007).            guage production to make predictions during comprehen-
Furthermore, the evaluation standard did not depend on lan-            sion? Trends in Cognitive Sciences, 11, 105–110.
guage-specific assumptions about syntactic categories or on          Pullum, G. K., & Scholz, B. C. (2002). Empirical assessment
sentence probabilities which are difficult to interpret. Even          of stimulus poverty arguments. The Linguistic Review, 19,
though the AP-learner did not explicitly represent the hierar-         9–50.
chical structure of complex questions or syntactic rules oper-       Reali, F., & Christiansen, M. H. (2005). Uncovering the
ating on such representations, it performed as if it respected         richness of the stimulus: Structure dependence and indirect
the structure-dependence of auxiliary fronting. Thus, surface          statistical evidence. Cognitive Science, 29, 1007–1028.
                                                                 2697

