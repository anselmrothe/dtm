UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Toward a Functional Model of Human Language Processing
Permalink
https://escholarship.org/uc/item/8kr28525
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Ball, Jerry
Freiman, Mary
Rodgers, Stuart
et al.
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                     Toward a Functional Model of Human Language Processing
                           Jerry Ball1, Mary Freiman2, Stuart Rodgers3 & Christopher Myers1
                                 Air Force Research Laboratory1, L3 Communications2, AGS TechNet3
                       Jerry.Ball@mesa.afmc.af.mil, Mary.Freiman@mesa.afmc.af.mil, Stu@agstechnet.com,
                                                  Christopher.Myers@mesa.afmc.af.mil
                              Abstract                                 facilitate the development of functional models by pushing
   This paper describes a computational cognitive model of             development in directions that are more likely to be
   human language processing under development in the ACT-             successful. There are short-term costs associated with
   R cognitive architecture. The paper begins with the context         adherence to cognitive constraints; however, we have
   for the research, followed by a discussion of the primary           already realized longer-term benefits. For example, the
   theoretical and modeling commitments. The main theoretical          integration of a word recognition capability with ACT-R‟s
   commitment is to develop a language model which is at once          perceptual system and higher-level linguistic processing has
   functional and cognitively plausible. The paper continues
   with a description of the word recognition subcomponent of
                                                                       facilitated the recognition and processing of multi-word
   the language model which uses a perceptual span and ACT-            expressions and multi-unit words in ways that are not
   R‟s spreading activation mechanism to activate and select           available to systems with separate word tokenizing and part
   the lexical unit that most closely matches the perceptual           of speech tagging processes. Using an available tokenizer
   input. Next we present a description of the linguistic              and part of speech tagger would have initially facilitated
   structure building component of the model which combines            development, but the cognitive implausibility of using
   parallel, probabilistic processing with serial, pseudo-             staged tokenizing and part of speech tagging led us to reject
   deterministic processing, including a non-monotonic context         this approach. The benefits that we have realized as a result
   accommodation mechanism. A description of the mapping of
   linguistic representations into a situation model, follows. The     of this decision are described below.
   paper concludes with a summary and conclusions.
                                                                             Theoretical & Modeling Commitments
   Keywords: human language processing (HLP); functional;              There is extensive psycholinguistic evidence that human
   cognitively plausible; pseudo-deterministic.
                                                                       language processing is incremental and interactive (Gibson
                          Introduction                                 & Pearlmutter, 1998; Altmann, 1998; Tanenhaus et al.,
                                                                       1995; Altmann & Steedman, 1988). Garden-path effects,
The capability to model the cognitive processes associated             although infrequent, strongly suggest that processing is
with language is a long sought-after goal of cognitive                 essentially serial at the level of phrasal and clausal analysis
science. Computational cognitive process models help                   (Bever, 1970). Lower level processes of word recognition
researchers to not only understand language processes in               suggest parallel, activation-based processing mechanisms
their own right, but to determine how they affect and                  (McClelland & Rumelhart, 1981; Paap et al., 1982).
interact with other cognitive processes (e.g., reasoning,              Summarizing the psycholinguistic evidence, Altmann &
decision-making, situation assessment, etc.). Scaled-up                Mirkovic (2009, p. 605) claim “The view we are left with is
versions of these models also support the development of               a comprehension system that is „maximally incremental‟; it
cognitive agents with communicative capabilities based on              develops the fullest interpretation of a sentence fragment at
human linguistic processes (Ball et al., 2009; Douglass, Ball          each moment of the fragment‟s unfolding”.
& Rodgers, 2009). In this paper we present a “snapshot” of                These cognitive constraints legislate against staged
a functional language comprehension model under                        analysis models. All levels of analysis must at least be
development within the ACT-R architecture (Anderson,                   highly pipelined together, if not, in addition, allowing
2007). The model implements a referential and relational               feedback from higher to lower levels. They also suggest the
theory of human language processing (Ball, 2007; Ball,                 need for hybrid systems which incorporate a mixture of
Heiberg & Silber, 2007) within ACT-R1.                                 parallel and serial mechanisms, with lower levels of
   A key commitment of the language comprehension                      processing being primarily parallel, probabilistic and
research is development of a model which is at once                    interactive, while higher levels of analysis are primarily
cognitively plausible and functional. We believe that                  serial, deterministic and incremental.
adherence to well-established cognitive constraints will                  To adhere to and take advantage of these cognitive
                                                                       constraints, we have developed a pseudo-deterministic
1
  At the time of publication the model contained 6,395 declarative     human language processing model—i.e. a model that
memory elements and 548 production rules which cover a broad           presents the appearance and efficiency of serial,
range of grammatical constructions.                                    deterministic processing, but uses a non-monotonic context
                                                                   1583

accommodation mechanism and relies on lower level                       In the model‟s DM, word chunks have slots for letters,
parallel mechanisms to deal with the ambiguity that makes            word-length, and trigrams. Multi-unit words and multi-word
true deterministic processing impossible. This model makes           expressions have this information for all of the constituent
use of the architectural mechanisms in ACT-R that are most           units. Text input is distilled into this information by the
compatible with incremental and interactive processing. For          model and put into buffers to spread activation to words in
example, parallel, probabilistic processing taps into ACT-           DM containing matching information. The activation
R‟s declarative memory (DM) and parallel spreading                   mechanism allows the model to retrieve words from DM
activation mechanism, with ACT-R‟s DM retrieval                      that are not an exact match to the input. Letters and trigrams
mechanism supporting probabilistic selection—without                 in the text input increase the activation of word chunks
inhibition between competing alternatives as is typical of           containing those letters and trigrams in the mental lexicon.
connectionist models (cf. Vosse & Kempen, 2000). Serial,             The most highly activated word chunk, which need not be
incremental processing is based on ACT-R‟s procedural                an exact match to the input, is retrieved. These processes
memory which is instantiated as a production system. ACT-            and encodings are based on the Interactive Activation
R at once constrains the computational implementation and            model of word recognition (McClelland and Rumelhart
provides the basic mechanisms on which the model relies.             1981), with the addition of trigrams based on “letter triples”
Other than adding a collection of buffers to support                 (Seidenberg and McClelland, 1989).
language processing by retaining the partial products of                Besides breaking words into letters and trigrams, we
retrieval and structure building, and improving the                  modified the ACT-R architecture to better interpret multi-
perceptual processing in ACT-R, the computational                    unit words and multi-word expressions. By default, ACT-R
implementation does not add any language-specific                    splits input text into perceptual units based on spaces and
mechanisms. In the following sections we discuss important           punctuation—even word internal punctuation, where “ACT-
subcomponents of the model, such as how the model                    R” becomes “ACT” “-” “R”—and processes each
recognizes words, builds linguistic representations, and             perceptual unit separately. We replaced this behavior with a
maps linguistic representations to a situation representation.       perceptual span that is based on human reading span data
                                                                     and a multi-level splitting of the input within the perceptual
            Reading & Word Recognition                               span into larger and smaller perceptual units which spread
   A functional language model must deal with the linguistic         activation in parallel. We also added multi-word expression
input as is. In an experiment involving human subjects               chunks and multi-unit lexical chunks to DM. The overall
communicating via text chat (cf. Ball, et al., 2009), we             effect is a significant reduction in the number of DM
collected a text chat corpus that is riddled with variability in     retrievals per space and punctuation delimited input. Words
word forms—e.g., misspellings like “altitde”, abbreviations          with internal punctuation and multi-word expressions can
like “alt.”, and concatenations like “speedrestriction” and          now be retrieved as a single perceptual unit despite their
“speed=200-500”. For competent readers, misspelled words             internal structure (Freiman & Ball, submitted).
activate the intended lexical items because they contain                The new perceptual span is considerably larger than
many of the same letters and trigrams (Perea & Lupker,               ACT-R‟s punctuation and space delimited span. There is a
2003). Further, all the letters of a word can be transposed,         great deal of evidence that the perceptual span of adult
yet still prime the intended word (Guerrera 2004). Key               readers is about 14-15 letters to the right of fixation
requirements of a functional language model are the ability          (McConkie & Rayner, 1975; Rayner, 1986). We
to handle variability and misspellings in input forms, the           implemented a span of up to twelve letters, with the greatest
ability to separate perceptually conjoined units (e.g.               amount of activation spreading from the first few letters of
separating punctuation from words as in “He went.”, but not          the span and decreasing toward the end of the span. Just as
“etc.”); separating concatenated words, and the ability to           for adult readers, information to the right of fixation is
recognize multi-word expressions (e.g. “speed up”) and               obtained when the next word is predictable from the
multi-unit words (e.g. “ACT-R”, “a priori”).                         preceding text (see Rayner 1975; and Binder, Pollatsek, &
   To satisfy these requirements, the model includes a word          Rayner, 1999).
recognition subcomponent that uses ACT-R‟s spreading                    Within the context of a functional language model—i.e.
activation mechanism combined with a multi-word                      one that must interpret and act on the linguistic input, we
perceptual span to influence lexical item retrieval. It is           are also attempting to model adult human reading rates
assumed that word recognition involves mapping                       (Freiman & Ball, submitted). Adult humans read at a
orthographic input directly into DM representations without          phenomenal rate of 200-300 (space delimited) words per
recourse to phonetic processing (although a phonetic                 minute (Carver, 1973a; 1973b). The ACT-R architecture
mapping is not precluded). The model does not treat each             supports the timing of cognitive processes down to the msec
word as a sum of its parts, ignoring the complete form               level. The real-time it takes for a model to run can also be
altogether. Rather, if the text input as a whole does not            measured. Although we have not yet succeeded in achieving
match, and thereby activate an item in the lexicon, the              adult reading rates, we have improved the reading rate of
closest match can be retrieved based on the cues that do             the model significantly in both cognitive and real-time: 143
match, such as letters, word-length, and trigrams.                   words per minute in ACT-R cognitive time (important for
                                                                 1584

cognitive plausibility); and 249 words per minute in real-             There are two basic ways of building structure: 1)
time on a single-core, 2.1 GHz Windows Vista machine                integration of the current linguistic unit into an existing
with 2 gigabytes of RAM (important for a functional                 representation which contains an expectation for the
model). Ultimately, we believe that achieving adult reading         linguistic unit (i.e. substitution), and 2) projection or
rates hinges on minimizing the amount of structure building         construction of a novel representation coupled with
and maximizing the average size of linguistic units which           integration of the current linguistic unit into the novel
are retrieved. We are pursuing mechanisms and                       representation. For example, the processing of the word
representations that will make this possible.                       “pilots” recognized as a plural noun by the word recognition
                                                                    component can lead to projection of a nominal and
        Building Linguistic Representations                         integration of “pilots” as the head of the nominal. On the
The word recognition subcomponent typically delivers a              other hand, if “the” has already projected a nominal and set
lexical item categorized for part of speech to the higher           up the expectation for a head to occur, the processing of
level component that builds linguistic representations of           “pilots” can lead to its integration as the head of the
referential and relational meaning. For example, consider           nominal projected by “the”.
the processing of “the pilot”. The processing of “the” leads           The structure building mechanism is incremental in that it
to its identification as a determiner via retrieval from DM.        executes a sequence of productions that determine how to
Selection of this lexical item is based on the probabilistic,       integrate the current linguistic unit into an existing
context-sensitive mechanism discussed in the previous               representation and/or which kind of higher level linguistic
section. The subsequent processing of the determiner “the”          unit to project. These productions execute one at a time
leads to the projection or construction of a nominal                within the ACT-R architecture which incorporates a serial
construction. The processing of the word “pilot” in the             bottleneck for production execution. Although supported by
context of the preceding word “the” and the projected               extensive empirical evidence, the serial production
nominal leads to retrieval of a DM chunk identifying “pilot”        execution bottleneck is a characteristic of ACT-R that
as a noun. The noun “pilot” is then integrated as the head of       distinguishes it from other production system architectures
the nominal projected during the processing of “the”.               which support parallel production execution.
   Similar parallel, probabilistic mechanisms operate at the           The structure building mechanism uses all available
phrasal and clausal level, selecting between competing              information in deciding how to integrate the current
phrasal and clausal alternatives, and potentially interacting       linguistic input into the evolving representation. Although
with lower level probabilistic mechanisms. As an example            the parallel, probabilistic mechanism considers multiple
of this potential interaction, consider the processing of           alternatives in parallel, the output of this parallel mechanism
personal pronouns like “he” and “it”. At the lexical level,         is a single linguistic unit and the result of structure building
these words are categorized as pronouns, but they are also          is also a single representation. The structure building
closely associated with the nominal phrasal category since          mechanism operates in a pseudo-deterministic manner. It is
they typically function as the head of a complete nominal.          deterministic in that it builds a single representation which
Processing personal pronouns may involve their recognition          is assumed to be correct, but it relies on the parallel,
as pronouns followed by projection of a nominal phrase              probabilistic mechanism to provide the inputs to this
from the pronoun, but it may also be that the perceptual            structure building mechanism. In addition, structure
form can directly lead to retrieval of a nominal phrase,            building is subject to a mechanism of context
without the intermediate step of identifying the word as a          accommodation capable of making modest adjustments to
pronoun. The word recognition component, which prefers              the evolving representation (Ball, 2010a). Although context
larger and higher level units, may deliver a pre-compiled           accommodation does not involve backtracking or
nominal unit corresponding to the pronoun, rather than a            reanalysis, it is not, strictly speaking, deterministic, since it
lexical unit to the higher level construction process, blurring     can modify an existing representation and is therefore non-
the distinction between lexical and phrasal units. The              monotonic. For example, in the processing of the expression
determiner “the” may behave similarly, resulting in direct          “the altitude restriction”, when the word “altitude” is
retrieval of a nominal with an empty head, without the              processed, it can be integrated as the head of the nominal
intermediate step of identifying “the” as a determiner.             projected by “the”. But when “restriction” is subsequently
   The parallel, probabilistic mechanism which is capable of        processed, the context accommodation mechanism can
retrieving existing phrasal and clausal representations as          adjust the representation, shifting “altitude” into a
well as lexical units, competes with a mechanism which              modifying function so that “restriction” can function as the
builds novel representations. DM retrieval has priority over        head. This context accommodation capability can apply
this alternative construction mechanism. However, lexical           iteratively as in the processing of “the pressure valve
units are more likely to be available for retrieval than            adjustment screw” where “screw” is the ultimate head of the
phrasal and clausal representations. Further, the parallel,         nominal, but “pressure”, “valve” and “adjustment” are all
probabilistic mechanism is not capable of building any              incrementally integrated as the head prior to the processing
structure—building structure is the function of the serial          of “screw”. Note that at the end of processing it appears that
construction mechanism.                                             “pressure”, “valve” and “adjustment” were treated as
                                                                1585

modifiers all along, giving the appearance that these                   The situation model is implemented in three main
alternatives were carried along in parallel with their               subcomponents: the ACT-R module definition, a set of
treatment as heads.                                                  domain general production rules, and a set of domain
   Context accommodation uses the full available context to          specific production rules. The module is instantiated like
make modest adjustments to the evolving representation or            other ACT-R modules (Anderson, 2007), and includes the
to construe the current input in a way that allows for its           module buffers and handlers for module requests and
integration into the representation. As an example of                queries.
construal, the verb “kick” is construed as an object and                The main situation buffers are: sm-subject-context, sm-
functions as the head of a nominal when it occurs in the             related-object-context, sm-sit-context, sm-action-context,
context of “the”, as in “the kick”. Function overriding and          sm-event-context, and sm-prior-attention. They are named
function shifting are two additional mechanisms of context           and designed to reflect the semantics of the represented
accommodation. We have already seen an example of                    situations. The buffers will contain chunks representing the
function shifting (e.g. “the altitude restriction”). In the          objects, actions, events, and relationships discussed or
processing of “no altitude or airspeed restrictions”, the            encountered in the task environment. The top level chunk
conjoined head “altitude or airspeed” can override the               types were based upon the Suggested Upper Merged
initial treatment of “altitude” as the head of the nominal,          Ontology (SUMO) (Niles and Pease, 2001) and are: Action,
with the subsequent shifting of “altitude and airspeed” into a       Attribute, Concept, Event, Object, Relation, and Situation.
modifying function during the processing of “restrictions”.          All entities represented in the situation model will be sub-
At a lower level, there are accommodation mechanisms for             typed from one of these top level chunk types. Because the
handling conflicts in the grammatical features associated            situations being represented in our model may span multiple
with various lexical items. For example, the grammatical             sentences, the contents of the sm-subject-context buffer will
feature definite is associated with “the” and the grammatical        frequently not equate to the subject of an individually
feature indefinite is associated with “pilots”. In “the pilots”,     processed sentence. Rather, the contents of the sm-subject-
the definite feature of “the” blocks the indefinite feature of       context buffer should be thought of as the central topic or
“pilots” from projecting to the nominal. See Ball (2010b)            theme of the discourse at an individual moment. The
for more details.                                                    situation chunk-type and its sub-types can be thought of as
   Context accommodation need not be computationally                 instances of schemata or structures for mental models of
expensive—a single production may effect the                         stereotypical      situations   (Alba,     1983).    In     our
accommodation, just as a single production may effect                implementation, the situation chunk contains the relevant
integration without accommodation. In this respect, context          gist of the situation, where the "gist" can be thought of as an
accommodation is not a reanalysis mechanism that disrupts            index to a specific category of situation.
normal processing—it is part and parcel of normal                       It is the responsibility of the modeler to define any
processing. Reanalysis mechanisms need only kick in when             needed specific chunk subtypes. Because ACT-R's chunk
context accommodation fails and larger adjustment is                 inheritance mechanism does not permit inheritance from
needed. The mechanism of context accommodation is most               multiple supertypes, it is expected that there will be some
closely related to the limited repair parsing of Lewis (1998).       redundancy in the definitions of the chunk subtype
Context accommodation may be viewed as a very modest                 hierarchy. While this redundancy will create some
form of repair. According to Lewis (1998, p. 262) “The               inefficiency in the type hierarchy design, it should not
putative theoretical advantage of repair parsers depends in          preclude the modeling of necessary elements.
large part on finding simple candidate repair operations”.              The domain general productions manage the relationships
The mechanism of context accommodation provides                      between elements within each individual situation. For
evidence for this theoretical advantage.                             instance, in a situation involving an uninhabited air vehicle
   Overall, the highly interactive, parallel, probabilistic          altitude restriction for a reconnaissance waypoint, a
mechanism for selecting between competing alternatives               situation chunk would contain a subject slot and a related
combines with the incremental, serial construction and               object slot. The subject slot value would refer to the
context accommodation mechanisms to provide an efficient,            reconnaissance waypoint and the related object slot value
pseudo-deterministic language processing capability.                 would refer to the waypoint‟s altitude restriction. The
                                                                     domain general productions provide the mechanisms that
          Mapping into the Situation Model                           manage the references between the situation elements.
Although we borrow the term (cf. Zwann & Radvansky,                     The domain specific productions primarily consist of task
1998), we define situation model as a domain-specific                knowledge and responses to the situations, events, actions,
mental representation of a set of objects, actions, events,          and objects that are learned from interacting with a specific
and relationships related to a task, sufficient for reasoning        task environment. It is the modeler's responsibility to define
about a set of actions within that task. The situation model         the needed domain specific productions. A central goal of
is separate from the model‟s world knowledge but is related          current research is to discover regularities and useful
to and affected by world knowledge.                                  abstractions within the domain specific production rules that
                                                                     can be generalized.
                                                                 1586

   The situation model represents the domain specific
objects and situations to which the linguistic representations                   Summary and Conclusions
refer. The linguistic comprehension system interfaces to the       This paper describes a model of human language processing
non-linguistic situation model via the identification of           which is intended to be both functional and cognitively
referring expressions in the linguistic input. For example,        plausible. It includes a linguistic structure building
recognition of a nominal, or object referring expression,          mechanism which combines a serial, deterministic
results in the mapping to a corresponding object in the            processing mechanism with a non-monotonic mechanism of
situation model. There are two basic cases: 1) recognition of      context accommodation, and a lower level parallel,
a definite object referring expression typically results in        probabilistic mechanism for selecting between competing
identification of an existing object in the situation model or     alternatives. Overall, the model is pseudo-deterministic—it
surrounding context, and 2) recognition of an indefinite           presents the appearance and efficiency of deterministic
object referring expression typically results in the               processing, and can handle much of the more mundane
introduction of a new object into the situation model.             ambiguity evident in human language via the parallel,
Extensions to these basic cases are considered in Ball             probabilistic and non-monotonic context accommodation
(2010c) which expands the ontology of referential types to         mechanisms. The model adheres to well-established
include types, collections, exemplars, prototypes and even         cognitive constraints on human language processing
negative instances. The extended ontology has the important        including incremental and interactive processing. This
benefit of simplifying the mapping from referring                  commitment led to the integration of a cognitively plausible
expressions to situation model entities.                           word recognition subcomponent, rather than adopting an
   An object referring expression from the comprehension           off-the-shelf tokenizer and part of speech tagger that lacked
system is mapped to the situation model when the head of           cognitive plausibility.
the object referring expression is identified. For example, if        A key attribute of the language comprehension model is
the input is “the altitude”, then recognition of “altitude” as     the capability to handle variability and mismatch at all
the head triggers the mapping to the situation model. Note         levels of analysis from word recognition, through the
that if the input is actually “the altitude restriction”, an       generation of linguistic representations and the mapping
altitude object will still be mapped to at the processing of       into the situation model, to the determination of the
“altitude”. At the processing of “restriction” an “altitude        conversational implicatures not literally described in the
restriction” object will be mapped. Further, if a post-head        linguistic input (although the capability to handle
modifier occurs as in “for Waypoint-A” in “the altitude            conversational implicatures is not yet implemented). There
restriction for Waypoint-A”, the mapping may need to be            is no level of analysis at which variability and mismatch can
modified following processing of the post-head modifier.           be ignored.
The model does not currently attempt to map to an object              The language comprehension model is a key component
on the basis of pre-head modifiers as in “the red…”                of a larger synthetic teammate model which is capable of
although there is evidence that humans may do so in Visual         functioning as the pilot in a three-person simulation of an
World Paradigm tasks (Tanenhaus et al., 1995). It should be        uninhabited air vehicle reconnaissance mission task (Ball,
noted that object referring expressions contain ambiguous          et. al, 2009). The main objective of the synthetic teammate
words, not word senses or abstract concepts. It is the             project is to develop cognitive agents capable of being
mapping to objects in the situation model which                    integrated into team training simulations while maintaining
disambiguates the words in the linguistic representation.          training efficacy. To achieve this goal, synthetic teammates
   Other challenges include anaphora and co-reference              must be capable of closely matching human behavior. To
resolution. We currently use grammatical features to               this end, we have developed and integrated models of
constrain the possible co-referents of a pronoun (e.g. “it” is     several important cognitive capacities into a composite
inanimate and singular). We plan to adhere to the                  synthetic teammate model. In addition to language
constraints of binding theory with respect to binding              comprehension and situation modeling, these capacities
pronouns and anaphors (Chomsky, 1981) and to adopt                 include the ability to perform the UAV piloting task, and
mechanisms of Centering Theory (Grosz, Joshi &                     language generation and dialog modeling capabilities.
Weinstein, 1995) in a more complete implementation. We                Although we do not report a direct comparison of model
are not proposing a general solution in our research               results to human data, Cassimatis, Bello & Langley (2009)
program; however, we expect to implement an initial                argue that models of higher-level cognitive processes, such
capability for co-reference resolution by relying on ACT-R's       as language comprehension, may be better evaluated on
chunk merging feature. So long as the specific context for a       model breadth, parsimony, and functionality. Ball (2008)
chunk is the same for newly introduced references to               provides similar arguments for a functional approach, but
previously referenced knowledge elements, some amount of           makes a stronger commitment to cognitive plausibility. The
the new references automatically merge with previously             synthetic teammate is capable of receiving text
constructed chunks in DM. For a more general solution,             communications from a teammate, reading the text,
existing approaches to co-reference resolution are being           producing linguistic representations of the text, and
investigated for inclusion in our design.                          mapping the representations into a situation model. Based
                                                               1587

on the contents of the situation model, the synthetic                      Cassimatis, N., Bello, P. & Langley, P. (2008). Ability, breadth,
teammate then interacts with its task environment, or                         and parsimony in computational models of higher-order
responds to communications with its own text messages. We                     cognition. Cognitive Science, 32, 1304-1322.
believe that this demonstrates the functionality and                       Chomsky, N. (1981). Lectures on Government and Binding.
                                                                              Dordrecht, Holland: Foris.
capability of the presented language comprehension model.
                                                                           Culicover, P. (2009). Natural Language Syntax. NY: Oxford
                                                                              University Press.
                           References                                      Douglass, S., Ball, J. & Rodgers, S. (2009). Large declarative
Alba, J. W., & Hasher, L. (1983). Is memory schematic?                        memories in ACT-R. Proceedings of the 9th International
   Psychological Bulletin, 93, 203-231.                                       Conference on Cognitive Modeling 2009, Manchester, UK.
Altmann, G. (1998). Ambiguity in sentence processing. Trends in            Freiman, M., & Ball, J. (2008). Computational cognitive modeling
   Cognitive Sciences, 2(4), 146-152.                                         of reading comprehension at the word level. Proceedings of the
Altmann, G. & Mirkovic, J. (2009). Incrementality and prediction              38th Western Conference on Linguistics, 34-45. Davis, CA:
   in human sentence processing. Cognitive Science, 222, 583-                 University of California, Davis.
   609.                                                                    Freiman, M. & Ball, J. (submitted). Improving the reading rate of
Altmann, G., & Steedman, M. (1988). Interaction with context                  Double-R-Language.
   during human sentence processing. Cognition, 30, 191-238.               Grosz, B., Joshi, A. & S. Weinstein (1995). Centering: A
Anderson, J. (2007). How Can the Human Mind Occur in the                      framework for modelling the local coherence of discourse.
   Physical Universe? NY: Oxford University Press.                            University of Pennsylvania: IRCS Technical Report Series.
Anderson, J. R., Bothell, D., Byrne, M.D., Douglass, S., Lebiere,          Guerrera, C. (2004). Flexibility and constraint in lexical access:
   C., & Qin, Y. (2004). An integrated theory of the mind.                    Explorations in transposed-letter priming. Unpublished
   Psychological Review, 111, 1036-1060.                                      dissertation, Department of Psychology, University of Arizona.
Ball, J. (2007). A bi-polar theory of nominal and clause structure         Gibson, E., & Pearlmutter, N. (1998). Constraints on sentence
   and function. Annual Review of Cognitive Linguistics, 5, 27-54.            comprehension. Trends in Cognitive Sciences, 2(7), 262-268.
Ball, J. (2008). A naturalistic, functional approach to modeling           Lewis, R. L. (1998). Leaping off the garden path: Reanalysis and
   language comprehension. Papers from the AAAI Fall 2008                     limited repair parsing. In J. D. Fodor, & F. Ferreira (Eds.),
   Symposium, Naturally Inspired Artificial Intelligence. Menlo               Reanalysis in Sentence Processing. Boston: Kluwer Academic.
   Park, CA: AAAI Press.                                                   McClelland, J., & Rumelhart, D. (1981). An interactive activation
Ball, J. (2010a). Context Accommodation in Human Language                     model of context effects in letter perception: I. An account of
   Processing. Proceedings of the Natural Language Processing                 basic findings. Psychological Review, 88(5), 375-407.
   and Cognitive Science Workshop. Lisbon: INSTICC Press.                  McConkie, G. W., & Rayner, K. (1975). The span of the effective
Ball, J. (2010b). Projecting grammatical features in nominals:                stimulus during a fixation in reading. Perception &
   Cognitive     Processing        Theory     and     Computational           Psychophysics, 17. 578-586.
   Implementation. Proceedings of the 19th Behavior                        Paap, K., Newsome, S., McDonald, J. & Schvaneveldt, R. (1982).
   Representation in Modeling and Simulation Conference.                      An activation-verification model of letter and word recognition:
Ball, J. (2010c). Simplifying the mapping from referring                      the word-superiority effect. Psychological Review, 89, 573-594.
   expression to referent in a conceptual semantics of reference.          Perea, M., & Lupker, S. J. (2003). “Does jugde activate COURT?
   Proceedings of the 32nd Annual Meeting of the Cognitive                    Transposed-letter similarity effects in masked associative
   Science Society.                                                           priming”. Memory and Cognition, 31, 829- 841.
Ball, J., Heiberg, A. & Silber, R. (2007). Toward a large-scale            Rayner, K. (1975). The perceptual span and peripheral cues in
   model of language comprehension in ACT-R 6. In R. Lewis, T.                reading. Cognitive Psychology, 7, 65-81.
   Polk & J. Laird (Eds.) Proceedings of the 8th International             Rayner, K. (1986). Eye movements and the perceptual span in
   Conference on Cognitive Modeling (pp. 173-179). NY:                        beginning and skilled readers. Journal of Experimental Child
   Psychology Press.                                                          Psychology, 41, 211-236.
Ball, J., Myers, C. W., Heiberg, A., Cooke, N. J., Matessa, M., &          Seidenberg, Mark S., & McClelland, J. L. (1989). A distributed,
   Freiman, M. (2009). The Synthetic Teammate Project.                        developmental model of word recognition and naming.
   Proceedings of the 18th Annual Conference on Behavior                      Psychological Review, 96(4), 523-568.
   Representation in Modeling and Simulation. Sundance, UT.                Tanenhaus, M., Spivey-Knowlton, M., Eberhard, K., & Sedivy, J.
Bever, T. (1970). The cognitive basis for linguistic structures. In J.        (1995). Integration of visual and linguistic information in
   Hayes (Ed.), Cognition and the development of language, 279-               spoken language comprehension. Science, 268(5217),1632-
   362. NY: Wiley.                                                            1634.
Binder, K., Pollatsek, A., & Rayner, K. (1999 ). Extraction of             Vosse, T. & Kempen, G. (2000). Syntactic structure assembly in
   information to the left of the fixated word in reading. Journal of         human parsing: A computational model based on competitive
   Experimental     Psychology:        Human      Perception      and         inhibition and a lexicalist grammar. Cognition, 75, 105-143.
   Performance, 25, 1162-1172.                                             Zwann, R. & Radvansky, G. (1998). Situation models in language
Carver, R. (1973). Understanding, information processing and                  comprehension and memory. Psychological Bulletin, 123(2),
   learning from prose materials. Journal of Educational                      162-185.
   Psychology, 64, 76-84.
Carver, R. (1973). Effect of increasing the rate of speech
   presentation upon comprehension. Journal of Educational
   Psychology, 65, 118-126.
                                                                       1588

