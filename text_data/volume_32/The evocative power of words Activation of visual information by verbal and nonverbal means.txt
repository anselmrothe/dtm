UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The evocative power of words: Activation of visual information by verbal and nonverbal
means
Permalink
https://escholarship.org/uc/item/3j83x56h
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Lupyan, Gary
Thompson-Schill, Sharon
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

                  The evocative power of words: Activation of visual information
                                           by verbal and nonverbal means.
        Gary Lupyan (lupyan@sas.upenn.edu), Sharon L. Thompson-Schill (sschill@psych.upenn.edu)
                           Institute for Research in Cognitive Science, Center for Cognitive Neuroscience
                                                        University of Pennsylvania
                                                      Philadelphia, PA 19104 USA
                             Abstract                                   such as visual recognition memory (e.g., Lupyan, 2008a)
                                                                        and even visual processing (Gilbert, Regier, Kay, & Ivry,
  A major part of learning a language is learning to map spoken         2006; Lupyan, 2008b; Winawer et al., 2007). For example,
  words onto objects in the environment. An open question               hearing a verbal label such as “chair” facilitates the visual
  concerns the consequence this learning has for cognition and
                                                                        processing of the named category compared to trials on
  perception. We show that hearing common words (e.g., dog)
  activates visual information more than equally informative            which participants know the relevant object category but do
  non-linguistic information (e.g., a dog bark). The main results       not actually hear its name (Lupyan, 2007, 2008b; Lupyan &
  show that (1) pictures were verified more quickly after hear-         Spivey, 2010). Hearing a label can even make an invisible
  ing a word than after hearing a nonverbal sound, even after           object, visible (Lupyan & Spivey, 2008). One way to think
  hundreds of trials of practice. (2) Verbal labels activated vis-      about such results is that processing a verbal label preacti-
  ual information more effectively than nonverbal sounds as             vates the sensory and higher-level representations of objects
  tested by a simple visual discrimination task that required mi-       denoted by the label—over and above activation caused by
  nimal semantic processing. (3) The advantage of the verbal            just thinking about the object category.
  modality did not arise simply due to greater familiarity of
                                                                           The present work addresses the question of how special
  verbal labels: when experience with novel labels and sounds
  was equated, verbal labels continued to activate the associated       words are in evoking visual information. Is a highly familiar
  visual information more reliably than the equally well-learned        concept accessed equivalently through verbal and nonverbal
  nonverbal sounds. These results inform the understanding of           means with words being a merely convenient way to acti-
  how human cognition is shaped by language and hint at ef-             vate conceptual information? Or, do words evoke concep-
  fects that different patterns of naming can have on individu-         tual representations in a special way? We focus here on vis-
  als’ conceptual structure.                                            ual representations and compare the power of verbal and
                                                                        nonverbal cues to evoke visual information of both familiar
                         Introduction                                   and novel categories.
  Two hallmarks of human development are the develop-                      It has been long known that a response to a visual stimu-
ment of conceptual categories—learning that things with                 lus can be altered by a cue presented prior to the target sti-
feathers tend to fly, that animals possessing certain features          mulus. These cues can be nonverbal (Egly, Driver, & Rafal,
are dogs, and that foods of a certain color and shape are               1994; Eriksen & Hoffman, 1972; Posner, Snyder, & David-
edible (Carey, 1987; Keil, 1992; Rogers & McClelland,                   son, 1980) as well as verbal. For example, verbal cues in the
2004), and learning names for those categories. The latter              form of words like “left” and “right” produce automatic
achievement is unique to humans. While many have com-                   shifts of attention just as reliably as nonverbal cues such as
mented on the transformative power of names (Clark, 1998;               directional arrows even when the words are entirely non-
Dennett, 1994; Harnad, 2005; James, 1890; Vygotsky,                     predictive of the target’s location (e.g., Hommel, Pratt, Col-
1962), it is only recently that the interplay between verbal            zato, & Godijn, 2001). Words related to motion, e.g.,
labels and concepts has become a subject of rigorous em-                “float,” have been shown to affect visual motion processing
pirical study.                                                          (Meteyard, Bahrami, & Vigliocco, 2007). Several studies
  The learning of categories is, in principle, separable from           have also shown visual object processing can be altered by
the learning of language. A child can have a conceptual cat-            verbal cues (Puri & Wojciulik, 2008; Vickery, King, & Ji-
egory of “dog” without having a verbal label associated                 ang, 2005). Such effects of cues on visual processing have
with the category. However, in practice the two processes               been linked to increases in category-specific cortical activ-
are intimately linked. Not only does conceptual develop-                ity. For example, after seeing the word “face,” participants
ment shape linguistic development (Snedeker & Gleitman,                 are not only better at making a gender judgment of faces
2004), but linguistic development—specifically learning                 embedded in visual noise, but this enhanced discrimination
words—impacts conceptual development (Casasola, 2005;                   correlates with activity in the fusiform face area (Esterman
Lupyan, Rakison, & McClelland, 2007; Gentner & Goldin-                  & Yantis, 2009). These experiments have typically used
Meadow, 2003; Spelke, 2003; Spelke & Tsivkin, 2001;                     verbal labels as cues, as language is a natural way to convey
Waxman & Markow, 1995; Yoshida & Smith, 2005). The                      information about objects. It is at present unknown whether
effects of words on nonverbal cognition only begin at word-             a verbal label should be thought of as merely a convenient
learning. The learned associations between words and their              method of cuing—it is a primary function of language to
referents appear to continue to influence cognitive processes           convey information not presently in view—or whether there
                                                                    883

is something special in the way language activates visual           their response, auditory feedback in the form of a buzz or
information. In other words, is the type of visual activation       bleep indicated whether the response was correct. Exps. 1a
produced by hearing the word “cow” somehow special or               and 1b differed in one respect: in Exp. 1a the delay between
can it be achieved by nonverbal cues similarly associated           cue offset and picture onset was 400 ms. In Exp. 1b this was
with the concept of cows, e.g., a mooing sound. Although            increased to 1 s—a common delay used in verification tasks
both “cow” and the sound of a cow mooing are associated             (Stadthagen-Gonzalez, et. al.2009). The rationale for this
with cows, only the former is treated (in the normal course         long delay is that it gives plenty of time for the word or
of things) as referring to a cow.                                   sound to be encoded thoroughly by the time the picture ap-
   We present six experiments comparing the powers of ver-          pears. Thus, the verification RTs will be largely determined
bal and nonverbal cues to evoke visual information. Ex-             by the time it takes to recognize the picture rather than re-
periments 1a-1c contrast verbal and nonverbal cues in a             flecting residual processing of the label or sound cue.
series of picture-verification tasks. Experiments 2a-2b con-           All factors were within-subjects and each participants
trast verbal and nonverbal cues in a visual discrimination          completed 400 verification trials: 10 categories × 5 category
task that requires minimal semantic processing. Experiment          exemplars × 2 levels of congruence × 2 cue-types (sound vs.
3 tests whether the verbal advantage arises due to partici-         label) × 2 repeats.
pants’ greater familiarity with the verbal cues or whether the
verbal advantage in evoking visual information is due spe-          Results and Discussion
cifically to the referential status of words.                       The data were analyzed using a mixed-effects ANOVA with
                                                                    all factors as within-subject effects. Only correct RTs were
                     Experiments 1a-1b                              included. RTs less than 200 ms or greater than 1500 ms
Experiments 1a-b comprised picture verification tasks in            were excluded (1.9% of all trials). An analysis of RTs re-
which participants heard an auditory cue (a label or a non-         vealed a highly reliable validity advantage, Mvalid=552 ms,
verbal sound), and then saw a matching or mismatching               Minvalid=600 ms, F(1,18)=35.72, p<.0005 and a strong ad-
picture. If verbal labels activate visual information more          vantage for label trials, Mlabel=563 ms, Msound=588 ms,
reliably than do nonverbal cues, participants should be able        F(1,18)=24.77, p<.0005 (Figure 1A). This advantage was
to respond more quickly after hearing a label than a nonver-        also observed in accuracy, Mlabel=96.2%, Msound=95.2%,
bal sound.                                                          F(1,18)=6.38, p=.02. There were no reliable cue-type × va-
                                                                    lidity interactions.
Participants                                                           Experiment 1b likewise revealed a validity advantage for
A total of 116 University of Pennsylvania undergraduates            RTs, F(1,14)=20.80, p<.0005, and a strong label advantage,
volunteered in the experiments in exchange for course cre-          Mlabel=583 ms, Msound=620 ms, F(1,14)=26.80, p<.0005
dit: 18 in Exp. 1a, 15 in Exp. 1b, 20 in Exp. 1c, 18 in Exp.        (Figure 1B). The label advantage was also observed in accu-
2a, 25 in Exp. 2b, and 20 in Exp. 3.                                racy, Mlabel=97.8%, Msound=96.0%, F(1,14)=13.11, p=.003.
                                                                    There was no significant prime-type × item interaction, F<1.
Materials                                                           A replication of Exp. 1b with a 1.5 s delay yielded virtually
                                                                    identical results.
We selected 10 objects that were easily nameable and that
                                                                       It is conceivable that the advantage of labels is short-
had characteristic sounds (cat, car, dog, frog, gun, motorcy-
                                                                    lived, owing its existence to the initial unfamiliarity of the
cle, rooster, train, cow, whistle). Each category was instanti-
                                                                    sound cues. If so, the advantage should vanish or be dimin-
ated by 5 images: a normed color drawing (Rossion & Pour-
                                                                    ished with practice. We divided each participant’s data into
tois, 2004), 3 photographs obtained from online image
                                                                    four equal blocks and ran an ANCOVA with block as a co-
collections, and 1 “cartoon” image (e.g., a drawing of a car-
                                                                    variate. Although participants became faster, and more ac-
toon dog). We used several instances of each category to
                                                                    curate over time (Fs >10), there were no hints of an interac-
introduce some visual heterogeneity. Spoken labels com-
                                                                    tion between block and cue-type for either RT or accuracy
prised basic-level names (listed above). Nonverbal sounds
                                                                    in either experiment, Fs<1.
were obtained from online environmental sound libraries
                                                                       Experiments 1a-1b show that hearing a verbal label com-
and judged to be unambiguously related to the target catego-
                                                                    pared to a nonverbal sound affords a quicker identification
ries through piloting. All sounds were volume and length-
                                                                    of a subsequent picture most likely by pre-activating visual
normalized.
                                                                    information associated with the label allowing for quicker
                                                                    and more accurate acceptance of a congruent picture and a
Procedure
                                                                    quicker rejection of an incongruent picture.
On each trial participants heard a label or nonverbal sound
followed by a picture, which, with equal probability, either                              Experiment 1c
matched the cue or did not. In the latter case, the picture was
randomly selected from among the non-matching category              The results from Exps. 1a-1b suggest that labels may play a
images. Participants responded by pressing a “match” or             special role in evoking visual representations owing to their
“does not match” key on a keyboard. Immediately following           referential nature (see below for discussion). Alternatively,
                                                                884

participants may have simply been more familiar with ver-                              Experiments 2a-2b
bal labels than the sounds we used. This latter account pre-
                                                                    A limitation of Experiments 1a-1c is that the response re-
dicts that, in a verification context, participants should, on
                                                                    quires the participants to semantically classify the image. It
seeing an image, be faster to activate a label than its non-
                                                                    is thus unclear whether the label advantage derives from a
verbal sound. Experiment 1c tested this possibility by re-
                                                                    faster activation of associated visual information (which
versing the order of the label/sound and picture. Participants
                                                                    facilitates subsequent recognition) or if it arises from faster
now saw a picture first and had to judge a subsequently pre-
                                                                    activation of a semantic category itself. To tease apart these
sented auditory label or nonverbal sound as either matching
                                                                    accounts, we use a task with a response that depends on
the picture or not. A finding of a continued advantage of
                                                                    visual processing, but only minimally dependent on seman-
labels would support the familiarity account (but would not
                                                                    tic processing: discriminating an upright image from an up-
necessarily contradict the reference-based account). A dis-
                                                                    side-down one. The task is similar to one used by Puri and
appearance of the label advantage would provide evidence
                                                                    Wojciulik (2008) to examine effects of general and specific
against the familiarity-based account.
                                                                    cues on visual processing.
Materials and Procedure                                             Materials
Materials were identical to Experiments 1a-1b. The proce-           The verbal and nonverbal sounds were identical to Experi-
dure was identical except for the reversal of cue and target        ments 1a-1c. In addition, a non-informative cue was created
identities. On each trial, participants saw a picture for 1 s.      consisting of white noise of the same length and volume as
One additional second after it disappeared, a verbal label or       the other auditory cues. For the pictures, only the standard-
nonverbal sound was played and the participants task was,           ized and normed instances of each category were used
as quickly as possible, to press the appropriate key indicat-       (Rossion & Pourtois, 2004).
ing whether the sound matched the picture (valid trial) or
not (invalid trial). Participants could start responding at any     Procedure
time after the onset of the target label or sound, although         On each trial, participants saw two pictures for 200 ms. pre-
responses generally occurred after the offset of the label or       sented simultaneously to the left and right of a fixation
sound. Accuracy feedback was provided immediately after             cross. These pictures were identical except one was upside-
the response.                                                       down (flipped about the x-axis). Participants’ task was sim-
                                                                    ply to indicate which side of the screen contained the up-
Results and Discussion                                              right picture by pressing the ‘z’ key with their left index
  The data were analyzed identically to Exps. 1a-1b. There          finger if it was the picture on the left, and the ‘/’ key with
was a significant validity advantage in RTs, F(1,19)=17.45,         their right index finger if it was the picture on the right. It
p=.001: Mvalid=575 ms, Minvalid=614ms. There was no sig-            was stressed that it did not matter what object was shown in
nificant difference between label and sound trials,                 the picture. The pictures were preceded by an auditory cue.
F(1,19)=2.62, p=.12, with a trend for slower responses              The trials were evenly divided into label cues, sound cues,
times to label trials than to sound trials, Mlabel=502 ms,          and uninformative noise cues. The label and sound cues
Msound=587 ms. An analysis of accuracy also failed to find a        validly cued the upcoming picture on 80% of the trials. On
difference between label and sound cues, Mlabel=94.6%,              the remaining 20% the cue was invalid, for example, par-
Msound=94.9%, F<1, further demonstrating that the nonver-           ticipants would hear “cow” (or hear a mooing sound) but
bal sounds were as recognizable as the labels. Comparing            then see a car. This allowed us to measure both the advan-
Exps. 1b and 1c revealed a highly reliable experiment ×             tage of a valid cue relative to a noise cue (are people faster
cue-condition interaction, F(1,33)=24.19, p<.0005.                  to locate the upright cow after hearing “cow”/a moo sound?)
  If the label advantage observed in experiments 1a-1b was          and the cost of an invalid cue relative to a noise cue baseline
a simple consequence of participants’ greater familiarity           (are people slower to identify the upright cow after hearing
with labels, it was expected that a label advantage would be        “car”/a car-starting sound?) And, critically, we can compare
observed in the present study because viewing the picture           these benefits and costs for label and sound cues.
would activate the stronger associate—the label—more                Exps. 2a and 2b differed in only one respect: in Exp. 2a the
quickly than the weaker associate—the nonverbal sound.              delay between the offset of the cue and onset of the pictures
However, that is not what we observed. Rather, the label            was 400 ms. In Exp. 2b it was lengthened to 1 s to deter-
advantage appears to be asymmetric, occurring when visual           mine whether the results observed in Exp. 2a were due to
information is to be activated by a label cue, but not when a       insufficient time to process the nonverbal sound. There were
label needs to be activated by a visual cue. An alternative         20 practice and 300 real trials.
explanation is that lexical items are more complex than en-
vironmental sounds and thus require additional processing           Results and Discussion
time. On this account, however, it is unclear why, if labels        RTs were analyzed by mixed-effects ANOVAs followed by
required greater processing time, we found a reliable verifi-       directed t-tests. The first analysis included validity and cue-
cation advantage in Exps. 1a-1b.                                    type (sound vs. label) as fixed factors (validity is undefined
                                                                885

for noise cue trials) and subject as a random factor. Results                            Experiment 3
are shown in Figure 1. We found a highly reliable effect of
                                                                   The studies thus far examined effects of words/sounds on
validity, with valid trials being reliably faster than invalid
                                                                   visual processing of objects with which participants have
trials, F(1,17)=39.72, p<0005. There was a significant valid-
                                                                   had extensive prior experience. We had no way of knowing
ity × cue-type interaction with label cues showing a larger
                                                                   whether and what types of differences in experience may
cuing effect than sound cues, F(1,17)=8.23, p=.011. Rela-
                                                                   have produced the label advantage observed in Exps 1-2.
tive to the no-cue baseline, valid sound cues improved per-
                                                                   The label advantage is unlikely to be a product of a simple
formance by a significant amount, t(17)=2.84, p=.03). Label
                                                                   familiarity difference between labels and sounds (see Exp.
cues also improved performance, t(17)=5.01, p<.0005, and
                                                                   1c), but it is possible that labels have a greater power to
this improvement was significantly greater than the im-
                                                                   evoke visual information because they have been more fre-
provement due to sounds, t(17)=2.93, p=.009. Conversely,
                                                                   quently encountered in the context of the visual referent. In
relative to the no-cue baseline, invalid label cues signifi-
                                                                   Exp. 3, we exerted complete control over by training differ-
cantly slowed responses, t(17)=4.38, p<.0005; sounds cues
                                                                   ent groups of participants to associate either novel labels or
did not, t(17)=1.19, p>.2. There was a significant difference
                                                                   nonverbal sounds with novel stimuli. A finding of a label
between the cost of invalid labels and the cost of invalid
                                                                   advantage in this context would lend support to the idea that
sounds, t(17)=2.12, p=.048. Accuracy was very high
                                                                   words have a special power to evoke visual information.
(M=97.8%) and did not vary between conditions.
                                                                   Materials
                                                                   The learning set consisted of 6 novel 3D objects (Figure 2).
                                                                   There were 3 variants of each object to increase visual het-
                                                                   erogeneity. These variants involved changes in viewpoint
                                                                   and slight changes in feature configuration. Each category
                                                                   was paired with a novel label (shonk, whelph, scaif, crelch,
                                                                   foove, and streil). Each of these nonce words was designed
                                                                   to have approximately equal bigram and trigram statistics
                                                                   and similar real-world lexical neighborhoods. We also cre-
                                                                   ated 6 nonverbal sounds: one for each category. These were
                                                                   created by modifying and combining environmental and
                                                                   animal sounds to create 6 sounds that were not readily
                                                                   nameable, as judged by pilot testing.
     Figure 1: Results of Exps 2a-2b. Error bars show ±1 SE
 of the difference between noise cues and the condition clos-
  est to its mean. The mean of the noise cue trials is plotted
                 twice for ease of comparison.
   Did the label advantage result from a lack of time to proc-
ess the sound cue? This was unlikely given the results of
Exp. 1c, but nevertheless, we conducted a replication of
Exp. 2a with a longer (1 s) delay between cue offset and
picture onset. As shown in Figure 2b, valid labels helped
                                                                   Figure 2: Materials used in the learning task for Exp 3.
relative to baseline, t(24)=2.45, p=.022, while sounds did
not, t(24)=1.13, p>.2, though the interaction was not signifi-     Procedure
cant. Invalid sounds now hurt performance relative to base-        Participants were randomly assigned into label and sound
line (although not as much as labels). In sum, labels contin-      groups. There were 3 parts to the experiment presented in
ued to function as more effective cues than sounds.                immediate succession. In the first part, participants pas-
    With a longer time to process the cue, the nonverbal cues      sively viewed 12 trials during which all three exemplars of
start to act more like verbal cues, quite possibly because         each category were presented together with a recording,
participants may explicitly label the nonverbal sounds.            e.g., “These are all shonks” (for the label condition), or
                                                                   These all make the sound___” (for the sound condition).
                                                               886

Part 2 consisted of a verification task. Participants saw two                             racy, Mlabel = 96.4%, Msound = 95.7%, F<1.
exemplars from different categories followed by a prompt,                                    In Experiment 3 we had complete control over partici-
e.g., “Which one’s the streil?” or “Which one makes the                                   pants’ exposure to the pictorial stimuli, labels, and sounds.
sound___”) and had to select whether the left or right stimu-                             We could thereby ensure that they were equally familiar
lus matched. There were 180 training trials.                                              with the labels and nonverbal sounds. Participants were
   The last part was a replication of Experiment 2b with the                              equally proficient in learning to associate the novel catego-
novel stimuli. That is, participants judged whether the left or                           ries with labels or sounds. After only about 10 minutes of
right picture was upright (i.e., in the familiar orientation)                             training, hearing a label or sound activated the correspond-
after hearing a sound or label cue (now without a sentential                              ing visual form, as revealed by an RT advantage on valid
context). The images were presented for 200 ms after a 1 s                                trials and an RT cost on invalid trials. This in itself is quite
delay which was timed to the offset of the auditory cue.                                  remarkable. Critically for our thesis, the label cues were
                                                                                          more reliable in activating the corresponding visual form
Results and Discussion                                                                    than the sound cues, confirming that even when familiarity
Participants were remarkably adept at learning the 6 catego-                              and experience with verbal and nonverbal associates is fully
ries. After Part I—just two exposures to each category—                                   equated, verbal cues activate visual information more relia-
participants could correctly perform the 2AFC task of Part                                bly than nonverbal cues.
II with ~95% accuracy. The label group was slightly less
accurate and slower than the sound group, ps=.08, and there                                                  General Discussion
were no reliable condition × block interactions. By block 5                                  Humans learn an elaborate system of sounds (or gestures
both groups were performing at 99%, demonstrating that                                    in case of sign language) that refer, in a largely arbitrary
learning names for novel categories is no more or less diffi-                             way, to objects, actions, and relations. Beyond enabling
cult than learning what sounds they make.                                                 linguistic communication, does the acquisition and use of
                                                                                          the system confer certain cognitive and perceptual abilities?
                         60                                                                  In this work, we have investigated whether information
 Difference Score (ms)
                                               Label
                         50                                                               communicated verbally (through words denoting concrete
                                               Sound
                                                                                          objects) and nonverbally (through sounds associated with
                         40
                                                                                          those objects) activates visual information in the same way.
                         30                                                               We found that it does not. Cuing categories by using words
                         20                                                               is more effective than cuing them using nonverbal cues.
                         10                                                               Verbal cues, more than nonverbal cues appear to preactivate
                          0
                                                                                          a visual representation of the cued category, helping when
                                                                                          the cue is valid and hurting performance when the cue is
                              Benefit of Valid Benefit of Valid   Cost of Invalid
                              Cue relative to Cue relative to     Cue relative to         invalid. This phenomenon is robust, being observed in vir-
                               Invalid Cue        No Cue            No Cue                tually every subject. A number of control experiments rule
                                                                                          out the possibility that this effect is due to different levels of
       Figure 3: Cuing effects in Experiment 3. Left: RTinvalid-                          familiarity with verbal versus nonverbal cues.
      RTvalid. Middle: RTno-cue-RTvalid. Right: RTinvalid-RTno-cue.                          These findings contradict the popular view that language
        Error bars show ±SE of the mean difference score.                                 simply activates nonverbal concepts (Gleitman & Papafra-
                                                                                          gou, 2005; e.g., Li, Dunham, & Carey, 2009; Snedeker &
   The critical part of the experiment was subsequent orien-                              Gleitman, 2004) because presumably such concepts should
tation judgment task. Having ruled out entirely differences                               have been activated in the same way by equally well-learned
in familiarity and association strength between labels and                                nonverbal information (as in Exp. 3), but they were not. The
sounds, would labels continue to evoke visual activations in                              finding that representations of very familiar categories (e.g.,
a more robust way than sounds? Indeed, that is what we                                    dogs, cats, and cars) can be evoked more reliably by labels
found. As shown in Figure 3, there was a significant validity                             than by sounds, even a full second after cue offset hints at
advantage, F(1,18)=49.55, p<.0005, but this advantage was                                 the powerful effects of language on visual activation.
significantly larger for label than sound cues, F(1,14)=6.14,                                How do words come to have such evocative powers? We
p=.023. The valid cue also benefited RTs relative to the                                  believe it is unlikely that there is innately privileged access
uninformative noise cue, F(1,18)=38.34, p<.0005, and this                                 to vision from the verbal modality (indeed, it is unclear
benefit was larger for the label than sound trials,                                       what an innate verbal modality would entail). Rather, the
F(1,18)=10.73, p=.004. Finally, there was a significant cost                              special status of words may derive from accumulated ex-
of hearing an invalid cue relative to no cue, F(1,18)=8.08,                               perience of treating them in a referential way (Waxman,
p=.011, but this cost was not reliably different for the two                              1999), although what exactly this entails vis-à-vis a neural
groups, F<1. An identical pattern of results was found when                               mechanism remains unknown. The present results show that
we used proportions instead of RT differences.                                            verbal labels serve as powerful cues (Elman, 2004; Rumel-
   The two groups did not differ in overall response times,                               hart, 1979), invoking associated concepts and percepts in a
Mlabel=433 ms, Msound=389 ms, F(1,18)=1.70, p>.2, or accu-                                unique way, even when the concept in question is a highly
                                                                                    887

familiar one such as [dog]. The finding that after only ~10                     Li, P., Dunham, Y., & Carey, S. (2009). Of substance: The nature of
minutes of experience, labels affect representations of new                        language effects on entity construal. Cognitive Psychology, 58(4),
                                                                                   487-524. doi:10.1016/j.cogpsych.2008.12.001
concepts (Exp. 3), hints that long-term differences in lin-                     Lupyan, G. (2007). The Label Feedback Hypothesis: Linguistic Influ-
guistic experience can have significant effects on the ease of                     ences on Visual Processing. PhD. Thesis. Carnegie Mellon Univer-
activating specific mental states. Rather than being simple                        sity.
constituent feature of the concept with which it is associ-                     Lupyan, G. (2008a). From chair to "chair:" A representational shift
ated, a name appears to offer a particularly efficient route to                    account of object labeling effects on memory. Journal of Experi-
the activation of visual and perhaps other information. Al-                        mental Psychology: General, 137(2), 348-369.
                                                                                Lupyan, G. (2008b). The Conceptual Grouping effect: Categories mat-
though the verbal activation of conceptual information can                         ter (and named categories matter more). Cognition, 108, 566-577.
be deemed a human universal—perhaps the defining feature                        Lupyan, G., Rakison, D., & McClelland, J. (2007). Language is not
of language—the present results hint that the substantial                          just for talking: labels facilitate learning of novel categories. Psy-
differences in lexicalization patterns between languages                           chological Science, 18(12), 1077-1082.
may translate to cross-linguistic differences in how particu-                   Lupyan, G., & Spivey, M. (2008). Now You See It, Now You Don't:
                                                                                   Verbal but not visual cues facilitate visual object detection. In Pro-
lar mental states can be evoked.
                                                                                   ceedings of the 30th Annual Conference of the Cognitive Science
                                                                                   Society (pp. 963-968). Austin, TX.
                        Acknowledgments                                         Lupyan, G., & Spivey, M. (2010). Redundant spoken labels facilitate
This work was supported by an IGERT training grant to G.L. and NIH                 perception of multiple items. under review.
R01DC009209 and R01MH70850 to S.T-S. We thank Nina Hsu for                      Meteyard, L., Bahrami, B., & Vigliocco, G. (2007). Motion detection
designing the stimuli used in Exp. 3, and thank Joyce Shin, Ali                    and motion verbs - Language affects low-level visual perception.
Shapiro, and Arber Tasimi for help with data collection.                           Psychological Science, 18(11), 1007-1013.
                                                                                Posner, M., Snyder, C., & Davidson, B. (1980). Attention and the
                                                                                   Detection of Signals. Journal of Experimental Psychology-General,
                             References                                            109(2), 160-174.
Carey, S. (1987). Conceptual Change in Childhood (First paperback               Puri, A., & Wojciulik, E. (2008). Expectation both helps and hinders
   edition.). The MIT Press.                                                       object perception. Vision Research, 48(4), 589-597.
Casasola, M. (2005). Can language do the driving? The effect of lin-            Rogers, T., & McClelland, J. (2004). Semantic Cognition: A Parallel
   guistic input on infants' categorization of support spatial relations.          Distributed Processing Approach. Cambridge, MA: Bradford Book.
   Developmental Psychology, 41(1), 183-192. doi:10.1037/0012-                  Rossion, B., & Pourtois, G. (2004). Revisiting Snodgrass and Vander-
   1649.41.1.188                                                                   wart's object pictorial set: The role of surface detail in basic-level
Clark, A. (1998). Magic Words: How Language Augments Human                         object recognition. Perception, 33(2), 217-236.
   Computation. In Language and Thought: Interdisciplinary themes               Rumelhart, D. (1979). Some problems with the notion that words
   (pp. 162-183). Cambridge University Press.                                      have literal meanings. In A. Ortony (Ed.), Metaphor and Thought
Dennett, D. (1994). The Role of Language in Intelligence. In What is               (pp. 71-82). Cambridge University Press.
   Intelligence? The Darwin College Lectures. Cambridge University              Snedeker, J., & Gleitman, L. (2004). Why is it hard to label our con-
   Press.                                                                          cepts? In D. G. Hall & S. R. Waxman (Eds.), Weaving a Lexicon (il-
Egly, R., Driver, J., & Rafal, R. (1994). Shifting Visual-Attention                lustrated edition., pp. 257-294). The MIT Press.
   Between Objects and Locations - Evidence from Normal and Parie-              Spelke, E. (2003). What Makes Us Smart? Core knowledge and natural
   tal Lesion Subjects. Journal of Experimental Psychology-General,                language. In Language in Mind: Advances in the Study of Language
   123(2), 161-177.                                                                and Thought (pp. 277 -311). Cambridge, MA.: MIT Press.
Elman, J. L. (2004). An alternative view of the mental lexicon. Trends          Spelke, E., & Tsivkin, S. (2001). Initial knowledge and conceptual
   in Cognitive Sciences, 8(7), 301-306. doi:10.1016/j.tics.2004.05.003            change: Space and number. In Language acquisition and conceptual
Eriksen, C., & Hoffman, J. (1972). Temporal and Spatial Characteris-               development (pp. 475-511). Cambridge, UK: Cambridge University
   tics of Selective Encoding from Visual Displays. Perception & Psy-              Press.
   chophysics, 12(2B), 201-&.                                                   Stadthagen-Gonzalez, H., Damian, M. F., Pérez, M. A., Bowers, J. S.,
Esterman, M., & Yantis, S. (2009). Perceptual Expectation Evokes                   & Marín, J. (2009). Name–picture verification as a control measure
   Category-Selective Cortical Activity. Cereb. Cortex, bhp188.                    for object naming: A task analysis and norms for a large set of pic-
   doi:10.1093/cercor/bhp188                                                       tures. The Quarterly Journal of Experimental Psychology, 62(8),
Gentner, D., & Goldin-Meadow, S. (2003). Language in Mind: Ad-                     1581. doi:10.1080/17470210802511139
   vances in the Study of Language and Thought. Cambridge, MA.:                 Vickery, T. J., King, L., & Jiang, Y. (2005). Setting up the target tem-
   MIT Press.                                                                      plate in visual search. Journal of Vision, 5(1), 81-92.
Gilbert, A., Regier, T., Kay, P., & Ivry, R. (2006). Whorf hypothesis is           doi:10:1167/5.1.8
   supported in the right visual field but not the left. Proceedings of the     Vygotsky, L. (1962). Thought and Language. Cambridge, MA: MIT
   National Academy of Sciences of the United States of America,                   Press.
   103(2), 489-494.                                                             Waxman, S. (1999). The dubbing ceremony revisited: Object naming
Gleitman, L., & Papafragou, A. (2005). Language and thought. In                    and categorization in infancy and early childhood. In Folkbiology
   Cambridge Handbook of thinking and Reasoning (pp. 633-661).                     (pp. 233 -284). Cambridge, MA: MIT Press.
   Cambridge: Cambridge University Press.                                       Waxman, S., & Markow, D. (1995). Words as invitations to form cate-
Harnad, S. (2005). Cognition is categorization. In H. Cohen & C. Le-               gories: Evidence from 12- to 13-month-old infants. Cognitive Psy-
   febvre (Eds.), Handbook of Categorization in Cognitive Science (pp.             chology, 29(3), 257-302.
   20-45). Elsevier.                                                            Winawer, J., Witthoft, N., Frank, M., Wu, L., Wade, A., & Boroditsky,
Hommel, B., Pratt, J., Colzato, L., & Godijn, R. (2001). Symbolic                  L. (2007). Russian blues reveal effects of language on color dis-
   control of visual attention. Psychological Science, 12(5), 360-365.             crimination. Proceedings of the National Academy of Sciences of the
James, W. (1890). Principles of Psychology. Vol. 1. New York: Holt.                United States of America, 104(19), 7780-7785.
Keil, F. C. (1992). Concepts, Kinds, and Cognitive Development. The             Yoshida, H., & Smith, L. (2005). Linguistic cues enhance the learning
   MIT Press.                                                                      of perceptual cues. Psychological Science, 16(2), 90-95.
                                                                            888

