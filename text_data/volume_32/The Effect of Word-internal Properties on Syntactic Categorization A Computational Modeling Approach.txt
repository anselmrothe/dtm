Algorithm 1: Incremental word clustering                                Cues used for categorization
 1: initialize set of clusters K = 0/
 2: for every frame f do                                                As previously mentioned, children are known to group words
 3:     CM = argmaxC∈K Sim(f,C)                                         into syntactic categories by drawing on a number of different
 4:     if Sim(f,CM ) ≥ θ then                                          information sources. In our work, we include three different
 5:        Add frame f to cluster CM
 6:     else                                                            sources of information, and five types of cues (features) in
 7:        Construct a new cluster for frame f                          total, as explained below:1
 8:     end if
 9: end for                                                             • Distributional information about word co-occurrences:
(This algorithm is a modification of the one proposed by Alishahi &         This kind of information has been reported to be reliable
Chrupała, 2009).                                                            and very important in syntactic categorization (Schütze,
                                                                            1993; Redington et al., 1998; Mintz, 2003; Clark, 2000;
                                                                            Parisien et al., 2008; Alishahi & Chrupała, 2009). We take
Modeling the acquisition of syntactic categories                            one word from each side of a target head-word as its co-
Our goal is to build a computational model of syntactic cate-               occurrence features, because in many of the above stud-
gorization that is cognitively plausible, i.e., we make as few              ies words closer to a word have been shown to be more
assumptions as possible about the type of cues accessible to                informative about its category. For example, considering
young children, and about the mechanisms children might                     sentences, such as “There is a cat in the basket”, and “We
use for categorization. We thus use an adaptation of a sim-                 need a table in our kitchen”, “A cat is in the basket”, and
ple incremental algorithm proposed by Alishahi and Chrupała                 “A table is in the kitchen.” provides a clue to the model to
(2009), which forms categories simply by drawing on the                     group cat and table together since they share similar co-
similarity among words to be categorized. Here, we present                  occurrence features. In our framework, each co-occurring
an overview of our adaptation of the algorithm, and a descrip-              word is considered as an independent feature when deter-
tion of three types of cues we use for categorization.                      mining similarity between a word (frame) and a cluster (as
                                                                            in many previous studies, and in contrast to representations
The categorization algorithm                                                such as “frequent frames” of Mintz, 2003). For example,
                                                                            even if the two tokens cat and table did not share the prepo-
The unsupervised clustering algorithm proposed by Alishahi                  sition in, they would still be considered as similar because
and Chrupała (2009) works based on contextual similarities                  of the preceding determiner a they have in common.
among words. The algorithm is incremental in that it pro-
cesses words one by one, discarding each word after clus-               • Phonological information: Words belonging to the same
tering. For each newly-observed frame (a target head-word                   syntactic category tend to have common phonological
along with its neighboring words from left and right), if the               properties. For example, looking at child-directed utter-
similarity to all of the already-shaped clusters is less than a             ances, (Monaghan et al., 2007) show that verbs and nouns
predefined threshold, a new cluster is constructed. Otherwise,              are different with respect to several phonological features,
the word is assigned to the most similar cluster. We mod-                   including the number of syllables. The study of Monaghan
ify this algorithm in two ways: (i) the original algorithm of               et al. focuses on the relevance of syntactic categories and a
Alishahi and Chrupała includes a phase in which clusters are                large number of word-level, syllable-level, and phoneme-
merged if they are sufficiently similar. To keep the algorithm              level phonological properties. We focus here on two of
simple, we removed this step; (ii) our frames are composed of               the simplest word-level phonological properties that we
three different types of features (five features in total besides           assume are readily accessible by young children, namely
the head-word content; see next subsection for details). We                 the length of a word in terms of number of syllables and
thus need to slightly modify the similarity score calculation in            phonemes (we use the number of letters to approximate
order to accommodate for more than one set of features. The                 the number of phonemes in a word).
similarity between a frame and a cluster (a group of frames)
                                                                        • Morphological information: It has been shown that English
is calculated as in:
                                                                            affixes, such as -ing in verbs, can provide strong clues to
                                                                            the identification of syntactic categories, and that such in-
                Sim(f,C) =     ∑ ωi ∗ Simi (f,C) ∈ F            (1)         formation is abundant in child-directed speech (Onnis &
                              i∈F                                           Christiansen, 2008). Nonetheless, it is not clear whether
                                                                            we can assume that children have access to such accu-
where f is a frame, C is a cluster, i is a feature, F is the                rate morphological knowledge about words and categories
set of all features, Simi (f,C) is the similarity of frame f to             prior to syntactic category learning. Inspired by the work
cluster C with respect to the ith feature, and ωi determines
the weight of the contribution of feature i in determining the               1 In this study, we do not consider one other important source of
overall similarity. Weights for all features need to sum to             information for learning of syntactic categories, namely, semantic
                                                                        information about words. This type of information requires making
1, i.e., ∑i ωi = 1. The modified version of the algorithm is            assumptions about what meaning is and how children may represent
shown in Algorithm 1 .                                                  it, and hence is outside the scope of this study.
                                                                    1530

   of Onnis and Christiansen (2008), here we use the last                                 Head:    table        Cooc:       a, in
   phoneme (ending) of the words as an approximation of the                               Phon:    2, 5         Morph:      l
   morphological affixes.2
                                                                           Figure 1: Sample frame extracted for the target word table from
   Overall, we include six different features (cues) in our cat-           the utterance “We need a table in the kitchen”.
egorization: two Cooc features, Head word, two Phon fea-
tures, and one Morph feature. The Cooc cues are consid-
                                                                           each frame contains a head word (the target word to be cat-
ered as properties external to the word (properties of the con-
                                                                           egorized), as well as several other features (two Cooc, two
text the word appears in), whereas the rest are related to the
                                                                           Phon, and one Morph features). A sample frame is shown in
word itself and hence are considered as word-internal cues.
                                                                           Figure 1. The head word and the Cooc features can be directly
In our experiments, we examine the effect of each different
                                                                           extracted from the utterance. If any of the Cooc features are
type of cue on categorization, and also consider the role of
                                                                           missing (i.e., the target word is the first or the last word of the
word-internal cues versus external ones.
                                                                           utterance), that feature is set to “Unknown”. For the two other
                                                                           types of features (Phon and Morph) we need to have access
                    Experimental Setup                                     to a phonemic representation of words and other phonolog-
Corpus                                                                     ical features. We extract two of these features (the ending
                                                                           phoneme, and the number of syllables) from the MRC Psy-
We extract our input data (both for training and testing) from
                                                                           cholinguistic Database, a publicly available resource built for
the Manchester corpus (Theakston et al., 2001), one of the
                                                                           use in studies on child language (Wilson, 1988).4 If a word
English subsets in the CHILDES database (MacWhinney,
                                                                           is not found in MRC, we set the values of the above features
2000). The Manchester corpus contains conversations of par-
                                                                           manually. For the third feature, the number of phonemes in a
ents/caregivers with 12 British children between the ages of
                                                                           word, we use the number of letters as an approximation.
1;8 (years;months) and 3;0.3 For training, we choose around
10000 child-directed utterances from the conversations of all              Evaluation
12 children, such that the chronological order of the utter-
                                                                           To examine the contribution of different types of cues on syn-
ances is maintained, and the utterances contain only words
                                                                           tactic categorization, we evaluate the effectiveness of clusters
selected from a limited vocabulary of 500 words. When se-
                                                                           resulting from one or a combination of features in two tasks.
lecting the 500 words, we make sure that their distribution in
                                                                           Specifically, we train our model (on the training corpus) in
the corpus matches a Zipfian distribution, so that our results
                                                                           three different conditions, that is, using one of the follow-
are not biased towards words from certain frequency ranges.
                                                                           ing feature combinations: Head+Cooc, Head+Cooc+Morph,
We limit the size of vocabulary because some feature values
                                                                           Head+Cooc+Phon. We then determine the effectiveness of
need to be determined manually. In addition, in one exper-
                                                                           the resulting clusters in each condition by examining the per-
imental task, we need access to actual novel words not pre-
                                                                           formance of the model on inferring the category of a number
viously seen in the training corpus, as opposed to made-up
                                                                           of test words. Note that the model does not create any new
novel words used in many psychological experiments.
                                                                           clusters during the test phase, but assigns each word to one of
   We use two different test corpora, one for each experimen-              the clusters formed in the training phase.
tal task (as explained in the Evaluation subsection below).                   We evaluate our model using two experimental tasks: one
The first set of test data (used in the Word Category Prediction           is to predict the syntactic category of a word whose identity
Task) is selected exactly as the training data, though from a              is known to the model/learner; the other one is to infer the
non-overlapping portion of the original (Manchester) corpus.               syntactic category of a novel (previously-unseen) word. In
The other test data (used in the Novel Word Categorization                 the word category prediction task (Experiment 1) the Head of
Task) is selected such that the target words to be categorized             a frame is considered as a feature, whereas it is not included in
are a novel word not in the vocabulary of 500 words. This                  the task of novel word categorization (Experiment 2). More
second test set is similar to the training data in all other as-           details on each of these tasks is given in the following section.
pects. Each test corpus contains 2000 word usages (tokens to                  Note that the resulting categories do not necessarily need to
be categorized).                                                           match the conventional adult-like categories put forth by lin-
                                                                           guists. Nonetheless, as a first-line evaluation, here we com-
Feature Extraction
                                                                           pare the categories learned by our model to a gold-standard
From each utterance (in the training or test data), we extract             set of syntactic categories. To measure test performance, we
a number of frames to be clustered. As explained previously,               must compare the ‘true’ syntactic category of each test word
                                                                           (according to the gold-standard) to the label of its associated
    2 We also included the first phoneme (beginning) of a word as
                                                                           cluster. We thus need to label each cluster with a syntactic
also done by Onnis and Christiansen (2008). However, in our initial
evaluations we found that the inclusion of this feature did not affect     category. Words in the Manchester corpus are tagged with
the results, and hence removed it from our set of features.                their parts of speech according to a fine-grained tag set. For
    3 Thanks to Chris Parisien for providing us with a preprocessed
version of this corpus.                                                        4 http://www.psych.rl.ac.uk/
                                                                       1531

our evaluation, we use a coarse-grained version of this origi-
nal tagging (also used by Parisien et al., 2008), including 11
tags, namely: Noun, Verb, Adjective, Adverb, Determiner,                    100
                                                                             90
Negation, Infinitive, Auxiliary, Conjunction, Preposition, and
                                                                             80
Others. Each cluster is assigned the majority label among all                70
its members. E.g., a cluster containing 30 nouns, 90 verbs,                  60
                                                                                                                           Cooc
and 20 adjectives is labeled as Verb.                                        50
                                                                                                                           Cooc + Morph
                                                                             40
   Test performance is measured using Accuracy: the propor-                  30
                                                                                                                           Cooc + Phon
tion of test words assigned to their correct category. We also               20
look into the accuracy for different groups of words, such                   10
as Verbs and Nouns, as well as open-class and closed-class                    0
                                                                                     Overall    Open-Class    Closed-class
words.
Model Parameters                                                     Figure 2: %Accuracy of known-word category prediction in
                                                                     three conditions; the total number of clusters constructed dur-
Our model contains two sets of parameters: the weights ωi            ing training phase is in the range 258–288.
used for measuring the similarity of a frame to a cluster (in
Eqn 1), and a similarity threshold θ used for deciding whether
to create a new cluster for a given frame. We set the weights        between 258–288).5 This way we maintain one factor (num-
ωi uniformly, giving equal weights to all features. The value        ber of clusters) constant, allowing us to focus on the effect of
of θ affects the number of generated clusters: a low value in-       different features involved in categorization.
creases the likelihood of grouping words, hence decreasing              Results are presented in Figure 2. In each condition, we
the total number of clusters. We set this parameter to differ-       measure accuracy on all 2000 words (displayed in the figure
ent values for different experimental conditions (i.e., different    as the Overall accuracy), as well as for open-class and for
combinations of features), so that we maintain the total num-        closed-class words separately. Since Head is used as a feature
ber of clusters generated in each condition within a desired         in all conditions, for the ease of exposition, the figure refers
range.                                                               to the conditions as Cooc, Cooc+Phon, and Cooc+Morph.
   We use two different ways of measuring Simi (f,C) in Eqn 1           Figure 2 shows that the overall categorization accuracy of
depending on feature i. For categorical features (Head, Cooc,        the model is improved by adding morphological or phonolog-
Morph) we use the cosine of the vectors (widely used for sim-        ical information, reinforcing that word-internal features are
ilar clustering algorithms). A vector representing a categor-        indeed informative about a word’s syntactic category. The
ical feature such as Head is of the size of word types in the        best performance is achieved by combining Cooc and Morph
corpus. E.g., for a sample frame f this vector includes 0 in all     features, suggesting that our morphological feature might be
elements except where the value of Head in that frame is pre-        more indicative of syntactic category than the phonological
sented. For numerical features (Phon) we use the Euclidean           features.
distance.                                                               Comparing the accuracy on open-class words and on
                                                                     Closed-class words, we can see that in two out of the three
                  Experimental Results                               conditions (i.e., Cooc and Cooc+Phon), open-class words
Experiment 1: Word Category Prediction                               are better categorized in comparison to closed-class words.
                                                                     This is expected because it is more likely that the word co-
Recall that to determine the effect of different types of cues
                                                                     occurrence information (which is the main source of informa-
(Head, Cooc, Phon, Morph) in the acquisition of syntac-
                                                                     tion in all conditions) reveals the similarity among open-class
tic categories, we train our model in three conditions (i.e.,
                                                                     (content) words more easily than for closed-class (function)
using three combinations of features, namely Head+Cooc,
                                                                     words. As an example, we expect nouns to often appear after
Head+Cooc+Phon, and Head+Cooc+Morph). In Experiment
                                                                     a small set of determiner types (e.g., a, an, the), whereas de-
1, we measure the accuracy of category prediction over a test
                                                                     terminers may precede many different nouns, sharing fewer
data containing 2000 known words. Comparing the accuracy
                                                                     context features.
of the categorization model across these conditions is fair and
meaningful only if the number of clusters are relatively close          Previous studies have shown a strong effect for the Head
for all conditions. Generally, allowing a larger number of           feature in determining a word’s syntactic category (e.g.,
clusters makes the categorization more conservative (i.e., by        Chang et al., 2006). It is thus reasonable to compare the over-
forming too many small clusters each containing one or a few             5 We have performed experiments with different ranges of cluster
word types that are highly similar). Based on our observa-           numbers, and found that the general patterns in results are similar.
tion, this implicitly affects the test accuracy. Hence, in the       As noted before, we prefer fewer clusters (fewer than our vocabu-
training phase for each of the three above-mentioned condi-          lary size) to allow for generalization. Indeed, we observe that even
                                                                     with 258–288 clusters, the generalization of the model is reasonably
tions, we use different values for the similarity threshold θ        good. Since more than 55% of the clusters contain three or more
to obtain approximately similar number of final clusters (i.e.,      word types.
                                                                 1532

all performance of our model in the three conditions with that
of a simple category learner that uses only the Head feature,
which we refer to as the lex-stat learner following Chang et                90
                                                                            80
al. (2006). For the performance of our model and that of the                70
lex-stat learner to be comparable, we must set the similar-                 60
ity threshold so that we end up with around 500 clusters for                50
                                                                            40
all conditions (since the lex-stat learner constructs a separate            30
                                                                                                                          Cooc
                                                                                                                          Cooc + Morph
cluster for each word type in the vocabulary). Indeed, we find              20
                                                                            10                                            Cooc + Phon
that the overall performance of lex-stat (92%; not shown in                  0
Figure 2) is better than for Cooc (89%), and is comparable to
the other two categorization conditions, Cooc+Phon (92%),
and Cooc+Morph (92%). This raises an important question:
whether the positive effect we observe here for the addition
of Phon and Morph features is a true effect. In other words,         Figure 3: %Accuracy of novel word categorization in three
since both Phon and Morph features are word-internal, it is          conditions; the total number of clusters constructed during
possible that their inclusion in categorization increases the        training phase is in the range 258–288.
contribution of the Head feature in calculating similarity, im-
plicitly giving more weight to the Head feature.
   Note that the lex-stat learner is a very conservative model       (beginning and) ending phonemes results in good categoriza-
with no generalization abilities (since each word type is in         tion. Their approach differs from ours in that they perform a
its own cluster). Such a model thus fails to properly catego-        batch processing over child-directed utterances, which allows
rize novel (previously unseen) words. In contrast to such a          their model to more easily learn the correspondences between
learner, children have the ability to categorize novel words         a certain category, e.g., verbs, and endings shared by words
(even meaningless artificial words made up for experimental          from this category, such as -ing in finishing, playing, reading.
purposes), by the help of the context, or based on their mor-        Our model has to learn such correspondences incrementally,
phological properties (Brown, 1957). We thus argue that for a        and hence is prone to making errors when calculating similar-
categorization model to reveal the true effect of features such      ity between a word form such as “finishing” (a verb ending in
as morphology or phonology, it should be able to general-            the suffix -ing) and one such as “string” (a noun with a sim-
ize well on unseen words. In the second Experiment, we use           ilar ending which is not a suffix but part of the word itself).
our three categorization models to determine the category of         Such errors in early stages may cause the algorithm to form
novel words. We consider actual novel words in this task be-         incoherent clusters in later stages.
cause we want to draw on word-internal features, e.g., phono-           Figure 3 also includes the performance of our model (in
logical and morphological properties of words.                       all three conditions) separately shown for Nouns and Verbs.
                                                                     Although the use of Morph features does not help the overall
Experiment 2: Novel Word Categorization                              categorization accuracy, it does seem to be particularly help-
In this task, we use our model (in the three conditions) to cat-     ful in identifying Verbs. Interestingly, using Cooc features
egorize 2000 novel words. In such cases, the Head feature is         alone results in a better detection of novel nouns, whereas for
not informative (since test words have not been seen during          verbs, other types of information (Morph and Phon) are help-
training), and hence the model has to utilize other sources of       ful. Hence, even among open-class words, discovering differ-
information to determine the category of a word. Results are         ent categories seems to rely on different types of information.
presented in Figure 3. Comparing performance on this Ex-             This is supported by the observation that, typically, context
periment with those on Experiment 1 (Figure 2) shows a sub-          words such as determiners mark the appearance of nouns; in
stantial decrease in the overall categorization accuracy (note       contrast, verbs particularly share morphological and phono-
that here Head feature is taken out of consideration). We es-        logical properties. Related statistical analysis, such as that of
pecially observe a significant drop in performance for closed-       (Monaghan et al., 2007; Clark, 2003) suggest such a comple-
class words. This decrease in performance emphasizes the             mentary contribution of different cues; and moreover, some
importance of the Head feature for word categorization, par-         psychological studies implicitly take this into account when
ticularly in determining the category of closed-class words.         designing their experiments on children (Brown, 1957).
This is again an expected result, given our discussion pre-
sented in the previous subsection about the weakness of co-                                   Conclusions
occurrence features in categorizing closed-class words.              We have used an adaptation of a categorization algorithm pro-
   Comparing results for the conditions shown in Figure 3 re-        posed by Alishahi and Chrupała (2009) to model the acqui-
veals that, as in Experiment 1, the use of Morph features does       sition of syntactic categories (e.g., verbs and nouns) in chil-
not improve the overall accuracy of categorization. These re-        dren, and to examine the effect of different types of cues on
sults are in contrast to the findings of Onnis and Christiansen      this task.
(2008), who claim that featuring words solely based on their            Our novel word categorization task provides a suitable
                                                                 1533

framework to evaluate the helpfulness of word-external (e.g.,        Brown, R. W. (1957). Linguistic determinism and the part of
context) as well as word-internal features (e.g., morpholog-           speech. , 55(1).
ical and phonological properties), independently from the            Cartwright, T. A., & Brent, M. R. (1997). Syntactic catego-
identity of the word being categorized (head word). For                rization in early language acquisition: formalizing the role
example, our results indicate that categorizing closed-class           of distributional analysis. Cognition, 63(2), 121–170.
words strongly relies on the head word. Specifically, these          Chang, F., Lieven, E., & Tomasell, M. (2006). Using child
classes of words do not share intra-category similarities (nei-        utterances to evaluate syntax acquisition algorithms. In In
ther contextual nor morpho/phonological similarities), and             Proceedings of the 28th Annual Conference of the Cogni-
hence cannot be categorized well only by drawing on such               tive Science Society.
properties. In contrast, open-class words can be success-            Clark, A. (2000). Inducing syntactic categories by context
fully categorized based on a combination of word-internal              distribution clustering. In Proceedings of the CoNLL-2000
and word-external properties, even without considering the             and LLL-2000 (pp. 91–94).
head word.                                                           Clark, A. (2003). Combining distributional and morpholog-
   In a more detailed investigation of the roles of word-              ical information for part of speech induction. In Proceed-
external versus word-internal features, we find that verbs             ings of the 10th annual meeting of the European Associa-
are better recognized when phonological and morphological              tion for Computational Linguistics (pp. 59–66).
properties are taken into account in addition to the context         Gelman, S., & Taylor, M. (1984). How two-year-old children
(co-occurring words). Note that we do not assume a full                interpret proper and common names for unfamiliar objects.
knowledge of morphology, but instead use word endingd as               Child Development, 55(4), 1535–1540.
an approximation to word suffixes (as suggested by Onnis             Gerken, L., Wilson, R., & Lewis, W. (2005). Infants can use
& Christiansen, 2008). Interestingly, for nouns, considering           distributional cues to form syntactic categories. Journal of
only the information about the co-occurring words results in           Child Language, 32(2), 249–268.
a more accurate categorization. This finding is in contrast to       MacWhinney, B. (2000). The CHILDES project: Tools for
that of Onnis and Christiansen (2008). We argue this differ-           analyzing talk (3rd ed., Vol. 2: The Database). MahWah,
ence to be due to the incremental nature of our model.                 NJ: Lawrence Erlbaum Associates.
                                                                     Mintz, T. H. (2003). Frequent frames as a cue for grammat-
   Evaluating the effect of different cues in word catego-
                                                                       ical categories in child directed speech. Cognition, 90(1),
rization models needs much care. Studies such as those of
                                                                       91–117.
Parisien et al. (2008) and Alishahi and Chrupała (2009) have
                                                                     Monaghan, P., Christiansen, M. H., & Chater, N. (2007). The
reported the capability of co-occurrence information in cate-
                                                                       phonological-distributional coherence hypothesis: Cross-
gorizing words. They include, however, the head word itself
                                                                       linguistic evidence in language acquisition. Cognitive Psy-
as part of their features used for categorization. These studies
                                                                       chology, 55(4), 259–305.
evaluated the performance of their models on various tasks,
                                                                     Onnis, L., & Christiansen, M. H. (2008). Lexical categories at
such as noun/verb disambiguation, and semantic feature pre-
                                                                       the edge of the word. Cognitive Science, 32(1), 184–221.
diction. But they did not provide a comparison between their
                                                                     Parisien, C., Fazly, A., & Stevenson, S. (2008). An incre-
models and a categorization model that only uses the head
                                                                       mental bayesian model for learning syntactic categories. In
word. As shown in our experiments, it is possible to achieve
                                                                       Proceedings of the CoNLL-2008.
a high accuracy on a task by using such a simple conservative
                                                                     Pearl, L. (2009). Using computational modeling in language
model. The task of novel word categorization that we propose
                                                                       acquisition research. To appear in Experimental Methods
is appropriate for evaluating the ability of a set of categories
                                                                       in Language Acquisition Research.
generated by a model to make generalizations.
                                                                     Redington, M., Chater, N., Finch, S., & Technology, T.
   In this study, we have shown that different types of cues,          (1998). Distributional information: A powerful cue for ac-
e.g., contextual or word-internal properties, provide children         quiring syntactic categories. Cognitive Science, 22, 425–
with complementary information, each helping with the cat-             469.
egorization of a particular group of words. However, our             Samuelson, L. K., & Smith, L. B. (1999). Early noun vocab-
framework is general and can be extended to incorporate                ularies: Do ontology, category structure and syntax corre-
other similar features (e.g., other morphological or phonolog-         spond? Cognition, 73(1), 1–33.
ical cues), as well as information about the semantic proper-        Schütze, H. (1993). Part of speech induction from scratch. In
ties of words.                                                         Proceedings of the ACL-1993 (pp. 252–258).
                                                                     Theakston, A. L., Lieven, E. V., Pine, J. M., & Rowland, C. F.
                          References                                   (2001). The role of performance limitations in the acqui-
                                                                       sition of verb–argument structure: An alternative account.
Alishahi, A., & Chrupała, G. (2009). Lexical category ac-              Journal of Child Language, 28, 127–152.
   quisition as an incremental process. In Proceedings of the        Wilson, M. (1988). MRC Psycholinguistic Database:
   CogSci-2009 workshop on psychocomputational models of               Machine-usable Dictionary, version 2.00.
   human language acquisition. Amsterdam.
                                                                 1534

