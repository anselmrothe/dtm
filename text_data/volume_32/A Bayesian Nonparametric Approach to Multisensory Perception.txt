UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Bayesian Nonparametric Approach to Multisensory Perception
Permalink
https://escholarship.org/uc/item/0dw9z101
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Yildirim, Ilker
Jacobs, Robert
Publication Date
2010-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

               A Bayesian Nonparametric Approach to Multisensory Perception
                                             İlker Yıldırım (iyildirim@bcs.rochester.edu)
                                            Robert A. Jacobs (robbie@bcs.rochester.edu)
                 Department of Brain & Cognitive Sciences, University of Rochester, Rochester, NY 14627, USA
                               Abstract                                  lateral occipital complex (LOC) shows similar patterns of ac-
                                                                         tivation regardless of whether an object is seen or touched.
   We propose a Bayesian nonparametric model of multisensory
   perception based upon the Indian buffet process. The model in-           Third, researchers have speculated that representations based
   cludes a set of latent variables that learn multisensory features     on different modalities are associated with each other. Sup-
   from unisensory data. The model is highly flexible because it
   makes few statistical assumptions. In particular, the number          pose that an observer sees, but does not hear, an object. A
   of latent multisensory features is not fixed a priori. Instead,       visual representation of that object will be active in the ob-
   this number is estimated from the observed data. We applied           server’s brain, and this representation will often predict or
   the model to a real-world visual-auditory data set obtained
   when people spoke English digits. Our results are consistent          activate an auditory representation of the object even though
   with several hypotheses about multisensory perception from            the object is not heard. Evidence consistent with this hypoth-
   the cognitive neuroscience literature. We found that the model        esis was obtained by Calvert et al. (1997). They found that
   obtained the statistical advantages provided by sensory inte-
   gration. We also found that the model acquired multisensory           viewing facial movements associated with speech (lipread-
   representations that were relatively sensory invariant. Lastly,       ing) leads to activation of auditory cortex in the absence of
   we found that the model was able to associate unisensory rep-         auditory speech sounds.
   resentations based on different modalities.
   Keywords: multisensory perception; Bayesian modeling; ra-                Here, we propose a model of multisensory perception that
   tional analysis; Indian buffet process                                learns about its multisensory environment in an unsupervised
                                                                         manner. In unsupervised learning, the data provided to a
                           Introduction                                  learner are unlabeled. The goal of the learner is to discover
We learn about our environments from many different senses.              patterns and structure within the data set. There is a dichotomy
Objects can be seen, heard, touched, tasted, and smelled.                in the cognitive science and machine learning literatures be-
How are our mental representations based on these different              tween parametric and nonparametric unsupervised learning
sensory modalities structured, combined, and coordinated?                methods. A parametric method uses a fixed representation
   Cognitive neuroscientists have recently studied three im-             that does not grow structurally as more data are observed.
portant hypotheses about multisensory perception. First, re-             Examples include factor analysis, where the number of la-
searchers have conjectured that multisensory representations             tent variables is fixed a priori, and cluster analysis, where the
are advantageous because sensory integration ameliorates the             number of clusters is fixed a priori. In contrast, a nonpara-
effects of bias and noise contained in representations based             metric method uses representations that are allowed to grow
on single modalities. Multisensory representations are, there-           structurally as more data are observed. These methods are
fore, able to convey more accurate and reliable information              often used when the goal is to impose as few assumptions as
than the unisensory representations from which they are de-              possible and to “let the data speak for themselves” (Blei, Grif-
rived. Consider an observer that sees and touches a surface              fiths, & Jordan, 2010). Examples include Dirichlet process
slanted in depth. Suppose that the observer’s slant estimates            mixture models (or Chinese restaurant processes) and Indian
based on the visual cue and on the haptic cue are each cor-              buffet processes.
rupted by sensory noise with some variance. It is easily shown              The proposed model of multisensory perception is an in-
that the maximum likelihood estimate of surface slant ob-                stance of a Bayesian nonparametric model. It “explains” the
tained by combining information from both cues has a lower               unisensory representations arising from different modalities
variance, and is thus more reliable, than estimates based on             through the use of a set of latent or hidden variables that learn
either cue alone. Evidence that the brain is able to combine             multisensory representations. The number of latent variables
sensory information in such a manner was obtained by Ernst               is not fixed. Instead, this number is treated as a random vari-
and Banks (2002), for example, who found that people’s esti-             able whose probability distribution is estimated based on the
mates of object height based on both visual and haptic infor-            unisensory data. Because the size of the latent multisensory
mation was more reliable than their estimates based on either            representations are estimated from the observed unisensory
visual or haptic information alone.                                      data, nonparametric statistical methods are required for in-
   Second, researchers have hypothesized that our neural rep-            ference. We use a Bayesian nonparametric framework de-
resentations of objects are often sensory invariant, meaning             veloped by Griffiths and Ghahramani (2005, 2006) known
they are the same (or at least similar) regardless of the sen-           as the Indian buffet process. Due to its Bayesian founda-
sory modalities through which we perceive those objects. Ev-             tions, the proposed model can be regarded as an ideal ob-
idence consistent with this hypothesis was obtained by Amedi             server model inferring optimal features of its multisensory
et al. (2001). They showed that a neural region known as the             environment (Austerweil & Griffiths, 2009).
                                                                     2633

   We applied the proposed model to a visual-auditory data                                                            multisensory
                                                                                                                        features
set obtained when people spoke different digits. Our results
are consistent with the three hypotheses from the cognitive
neuroscience literature described above. It was found that
the model obtained the statistical advantages provided by sen-
sory integration: categorization of objects was more accurate
based on its latent multisensory representations than on the                           visual                          auditory
latent features of unisensory models. In addition, the model’s                        features                         features
latent or multisensory representations were relatively sensory
invariant. That is, similar representations of an object were              Figure 1: A coarse schematic of the multisensory perception
formed regardless of whether an object was seen or heard.                  model.
Lastly, the model was able to associate representations based
on different modalities. In other words, it could use one
type of unisensory representation to predict or activate an-               and their distributions can be inferred. Similarly, the values
other type of unisensory representation.                                   of the auditory features are observed when an object is heard.
                                                                           Otherwise, the auditory features are latent, and their distribu-
                 Visual-Auditory Data Set                                  tions can be inferred. The multisensory features are always
                                                                           latent variables. Whereas the numbers of visual and auditory
The multisensory perception model was applied to a visual-
                                                                           features are fixed, the number of multisensory features is not.
auditory data set known as the Tulips1 data set (Movellan,
                                                                           Consistent with the nonparametric approach, this number is a
1995). Twelve people (9 adult males, 3 adult females) were
                                                                           random variable whose distribution is inferred from the data.
videotaped while uttering the first four digits of English twice.
                                                                              Formally, the model is a straightforward extension of the
   In each video frame, the image of a speaker’s mouth was
                                                                           Indian buffet process (Griffiths & Ghahramani, 2005, 2006).
processed to extract 6 visual features: the width and height
                                                                           A detailed graphical representation of the model is shown in
of the outer corners of the mouth, the width and height of
                                                                           Figure 2. An important goal of the model is to find a set of
the inner corners of the mouth, and the heights of the up-
                                                                           latent multisensory features, denoted Z, “explaining” a set of
per and lower lips. The auditory signal corresponding to a
                                                                           observed visual and auditory features, denoted XV and XA ,
frame was processed to extract 26 features: 12 cepstral coef-
                                                                           respectively. Assume that a learner both sees and hears a
ficients1 , 1 log-power, 12 cepstral coefficient derivatives, and
                                                                           number of objects. Let Z be a binary multisensory feature
1 log-power derivative. Because speech utterances had differ-
                                                                           ownership matrix, where Zi j = 1 indicates that object i pos-
ent durations, we sampled 6 frames for each utterance span-
                                                                           sesses multisensory feature j. Let XV and XA be real-valued
ning the entire duration of the utterance in a uniform manner.
                                                                           visual and auditory feature matrices, respectively (e.g., XVi j
In summary, each data item contained values for 36 visual
                                                                           is the value of visual feature j for object i). The problem of
features (6 frames × 6 visual features per frame) and 156 au-
                                                                           inferring Z from XV and XA can be solved via Bayes’ rule:
ditory features (6 frames × 26 auditory features per frame).
   Training and test sets were created as follows. For the first                                       p(XV |Z) p(XA |Z) p(Z)
eight speakers, one utterance of each digit was used for train-                     p(Z|XV , XA ) =
                                                                                                    ∑Z ′ p(XV |Z ′ ) p(XA |Z ′ ) p(Z ′ )
ing and the other utterance was used for testing. For the
remaining speakers, both utterances were used for training.                where p(Z) is the prior probability of the multisensory feature
Thus, the training set contained 16 data items for each digit,             ownership matrix, and p(XV |Z) and p(XA |Z) are the likeli-
and the test set contained 8 data items for each digit.                    hoods of the observed visual and auditory feature matrices,
                                                                           respectively, given the multisensory features. We now de-
             Multisensory Perception Model                                 scribe the prior and likelihood distributions.
We describe the proposed model in the context of a visual-                    The multisensory feature ownership matrix is assigned a
auditory environment, though we note that the model is equally             Bayesian nonparametric prior distribution known as the In-
applicable to other sensory modalities and to any number of                dian buffet process (Griffiths & Ghahramani, 2005, 2006). It
modalities. A coarse schematic of the model is illustrated in              can be interpreted as a probability distribution over feature
Figure 1. It contains three sets of nodes or variables corre-              ownership matrices with an unbounded (infinite) number of
sponding to visual features, auditory features, and multisen-              features. The distribution is written as:
sory features. The visual and auditory features are statisti-
                                                                                           αK                     K
                                                                                                                      (N − mk )!(mk − 1)!
cally dependent. However, they are conditionally indepen-                      p(Z) =    2N −1
                                                                                                  exp{−αHN } ∏
dent given values for the multisensory features. The values                            ∏h=1 kh !                k=1              N!
of the visual features are observed when an object is viewed.
                                                                           where N is the number of objects, K is the number of mul-
When an object is not viewed, the visual features are latent,
                                                                           tisensory features, Kh is the number of features with history
    1 Cepstral coefficients are the coefficients of the Fourier trans-     h (the history of a feature is the matrix column for that fea-
form representation of the log magnitude spectrum.                         ture interpreted as a binary number), HN is the N th harmonic
                                                                       2634

                                   α                                first 3000 iterations were discarded as burn-in. To reduce cor-
                                                                    relations among variables at nearby iterations, the remaining
                                                                    iterations were thinned to every 10th iteration (i.e., only vari-
                                                                    able values at every 10th iteration were retained). Thus, the
                                   Z                                results below are based on 200 iterations.
       σW                                               σW
          V
                  WV                            WA         A
                                                                    Posterior distributions over latent features
                                                                    Recall that the number of latent features in each model is not
             σX                                    σX               fixed a priori. Instead, it is a random variable whose distri-
                V
                                          XA          A             bution is inferred from the training data. The three graphs
                         XV
                                                                    in Figure 3 show the distributions of the numbers of latent
                                                                    features in the visual-only, auditory-only, and multisensory
Figure 2: A Bayesian network representation of the multisen-        models. The visual-only model used relatively few latent fea-
sory perception model.                                              tures, the auditory-only model used more latent features, and
                                                                    the multisensory model used the most latent features. This
                                                                    result confirms that the models are highly flexible. Their non-
number, mk is the number of objects with feature k, and α is        parametric nature allows them to adapt their representational
a variable influencing the number of features.                      capacities based on the complexities of their data sets.
   The visual and auditory likelihoods are each based on a
linear-Gaussian model. Let zi be the multisensory feature val-      Categorization performances
ues for object i, and let xiβ be the feature values for object i    We evaluated each model’s ability to categorize the speech ut-
where β is set to either V or A depending on whether we are         terances as instances of one of the first four digits in English
referring to visual or auditory features. Then xiβ is drawn         based upon its latent feature representations. At each itera-
from a Gaussian distribution whose mean is a linear func-           tion of an MCMC chain, a model sampled a latent feature
tion of the multisensory features, ziWβ , and whose covariance      representation for each data item in the training set. Using
matrix equals σ2Xβ I, where Wβ is a weight matrix (the weight       these representations, we performed k-means clustering with
matrices themselves are drawn from zero-mean Gaussian dis-          four cluster centers. We then performed an exhaustive search
tributions with covariance σW     2 I). Given these assumptions,    of assignments of clusters to English digits (e.g., cluster A
                                   β
                                                                    → digit 3, cluster B → digit 1, etc.) to find the assignment
the likelihood for a feature matrix is:
                                                                    producing the best categorization performance. Performances
                                        1                           were averaged across iterations of a chain.
      p(Xβ |Z,Wβ , σ2Xβ ) =                       ×
                                  (2πσ2Xβ )NDβ /2                      The results are shown in the leftmost graph of Figure 4.
                                                                    The horizontal axis gives the model, and the vertical axis
                          1                                         plots the percent of data items in the training set that were cor-
                exp{−          tr((Xβ − ZWβ )T (Xβ − ZWβ ))}
                         2σ2Xβ                                      rectly classified (error bars indicate the standard deviations of
                                                                    these percents across iterations of an MCMC chain). As ex-
where Dβ is the dimensionality of Xβ , and tr(·) denotes the        pected, the vision-only model showed the worst performance,
trace operator.                                                     the auditory-only model showed better performance, and the
                                                                    multisensory model showed the best performance.
                    Simulation Results                                 Its possible that the multisensory model showed the best
The multisensory perception model was applied to the visual-        performance solely due to the fact that it received both visual
auditory data set. To better understand its performances, we        and auditory features and, thus, received a richer set of inputs
also consider the performances of two other models. The             than either the visual-only or auditory-only models. To eval-
vision-only model is identical to the multisensory model ex-        uate this possibility, we simulated a model, referred to as a
cept that it contains only two sets of variables correspond-        ‘mixed’ model, that resembled the multisensory model in the
ing to visual and latent features. When applied to the visual-      sense that it received both visual and auditory features. How-
auditory data set, it received only the visual features. Simi-      ever, for the mixed model, these features were not segregated
larly, the auditory-only model contains only two sets of vari-      into separate input streams. Instead, the mixed model con-
ables corresponding to auditory and latent features. It re-         tained a set of latent features that received inputs from a set
ceived only the auditory features from the data set.                of undifferentiated perceptual features, namely a concatena-
   Because exact inference in the models is computationally         tion of the visual and auditory features. The results for the
intractable, approximate inference using Markov chain Monte         mixed model on the training set are also shown in the left-
Carlo (MCMC) sampling methods (e.g., Gelman et al., 1995)           most graph of Figure 4. The mixed model showed signifi-
was performed based upon the training data following Grif-          cantly poorer performance than the multisensory model, thus
fiths and Ghahramani (2005). A single chain of each model           suggesting the statistical advantages of segregating percep-
was simulated. Each chain was run for 5000 iterations. The          tual inputs into separate streams.
                                                                2635

                                        Vision only                                   Auditory only                                   Multisensory
                              0.2                                            0.2                                            0.2
                Probability                                    Probability                                    Probability
                              0.1                                            0.1                                            0.1
                               0                                              0                                              0
                                20    30    40      50    60                   20    30    40      50    60                   20    30    40      50    60
                                     Number of features                             Number of features                             Number of features
Figure 3: The distributions of the numbers of latent features in the visual-only (left), auditory-only (middle), and multisensory
(right) perception models.
   This analysis was repeated using the data items in the test                               Sensory invariance
set. Performing the analysis on test items presents unique
challenges. Although it is reasonable to sample variables’                                   As discussed above, neural representations of objects are of-
values, and thus estimate variables’ distributions, on the ba-                               ten sensory invariant. That is, the same (or at least simi-
sis of training items, models are not meant to learn from test                               lar) neural representations arise regardless of the modality
items. Consequently, we could not run our MCMC sampler                                       through which an object is sensed. Does the multisensory
on a model using the test items to evaluate the model’s cate-                                perception model show this same property?
gorization performance. Doing so would erase the distinction                                    We investigated this question as follows. As above, let
between training and test data items.                                                        L i denote the set of multisensory feature representations ob-
                                                                                             tained on iteration i of the MCMC sampler when the model
   Instead, we proceeded as follows. For a given model, con-
                                                                                             was trained on the training data. Recall that these are the
sider the latent feature representations obtained on iteration i
                                                                                             latent or multisensory representations with non-zero proba-
of the MCMC sampler when the model was trained on the
                                                                                             bility based solely on iteration i. For each data item in the
training data. There is one such representation for each train-
                                                                                             training set, we calculated the probability distribution of the
ing item. These are the latent representations with non-zero
                                                                                             multisensory representation given an item’s visual features,
probability based solely on iteration i. Let L i denote this
                                                                                             and the distribution of the multisensory representation given
set of representations. For each data item in the test set, we
                                                                                             an item’s auditory features where L i was the set of possi-
searched L i to find a latent representation that was most prob-
                                                                                             ble multisensory representations. When all training items are
able given the item. This was repeated for every item in the
                                                                                             taken into account, these distributions are denoted p(Z|XV )
test set. Using these representations, the analysis of the test
                                                                                             and p(Z|XA ), respectively. We then calculated the Battacharyya
set is identical to the analysis of the training set described
                                                                                             distance between p(Z|XV ) and p(Z|XA ).2 On every iteration,
above: latent representations were clustered using k-means
                                                                                             this distance was zero.
clustering, and an exhaustive search of assignments of clus-
ters to digits was performed to find the assignment producing                                   We repeated this analysis using the data items in the test
the best categorization performance. Performances were av-                                   set. Again, we computed p(Z|XV ) and p(Z|XA ) where XV
eraged across iterations.                                                                    and XA refer to the visual and auditory features of test items,
                                                                                             and where L i is the set of possible multisensory represen-
  The results are shown in the rightmost graph of Figure 4.                                  tations. The Battacharyya distances between p(Z|XV ) and
Again, the multisensory model showed the best performance.                                   p(Z|XA ) are always small values—the distribution of these
   In summary, the multisensory perception model showed                                      distances has values of 1.51, 1.55, and 1.68 as its 25th , 50th ,
the best categorization performance on both training and test                                and 75th percentiles, respectively. By way of comparison, we
data sets. We conclude that its superior performance is due to                               also computed the distance between p(Z|XA ) and a uniform
both its rich set of inputs (it receives both visual and auditory                            distribution over multisensory representations. The distribu-
features) and due to its internal structure (visual and audi-
tory features are segregated perceptual streams). Clearly, this                                2 We also considered the Kullback-Leibler distance but use of this
model received the statistical benefits of sensory integration.                              metric led to numerical instabilities.
                                                                                        2636

                                               Training                                                                     Test
                          100                                                                     100
                          80                                                                      80
        Percent correct                                                         Percent correct
                          60                                                                      60
                          40                                                                      40
                          20                                                                      20
                           0                                                                       0
                                Vision   Auditory   Multisensory Mixed                                  Vision   Auditory      Multisensory Mixed
Figure 4: Categorization performances of the vision-only, auditory-only, multisensory, and mixed models on the training set
(left) and on the test set (right). The horizontal axis of each graph gives the model, and the vertical axis plots the percent of data
items correctly classified (error bars indicate the standard deviations of these percents across iterations of an MCMC chain).
tion of these distances has values of 3.49, 7.83, and 19.04 as              The four graphs in the top row of the figure show the distribu-
its 25th , 50th , and 75th percentiles.                                     tions of the visual representations given the auditory features
   In summary, both training and test sets suggest that the                 of the test items. More precisely, the graphs show that when
multisensory perception model did indeed acquire sensory in-                presented with the auditory features corresponding to one of
variant representations. Its latent multisensory features had               the digits, the model’s distribution of visual representations
the same or similar distributions regardless of whether a speech            was tightly peaked at a representation corresponding to the
utterance was seen or heard.                                                same digit. The four graphs in the bottom row show analo-
                                                                            gous results for distributions of auditory representations given
Predicting sensory representations in missing                               test items’ visual features.
modalities                                                                     In summary, the multisensory perception model learns to
Above, we reviewed evidence of activity in people’s audi-                   associate unisensory representations from different modali-
tory cortices when they viewed speech utterances but did not                ties. It successfully predicts representations from missing
hear those utterances (Calvert et al., 1997). This result is                modalities based on features from observed modalities.
consistent with the hypothesis that sensory representations in
one modality can predict or activate representations in other                                                     Conclusions
modalities. Does the multisensory perception model show                     Bayesian nonparametric approaches to modeling are becom-
this behavior?                                                              ing increasingly popular in the cognitive science and machine
   This question was studied using the data items in the test               learning literatures. We regard this approach as an important
set. Let V and A denote the sets of visual and auditory                     advance over conventional parametric approaches in which a
feature representations for the data items in the training set.             researcher sets the number of latent variables by hand, often
Once again, let L i denote the set of multisensory representa-              in an ad hoc or unprincipled manner. How can a researcher be
tions obtained on iteration i of the MCMC sampler when the                  sure that the number of latent features should, for example, be
model was trained on the training data. For each test item,                 exactly 10? Shouldn’t the number of latent features be deter-
we computed the probability distribution of an auditory rep-                mined by the structure of the task or data set? The Bayesian
resentation given a test item’s visual features. This was ac-               nonparametric approach is also an advance over modeling ap-
complished by first calculating a conditional joint distribution            proaches that define a set of models, each with a different
over both multisensory and auditory representations, and then               number of latent features, and perform “model comparison”
by marginalizing over the multisensory representations where                to select the best model. Typical model comparison tech-
the set of possible auditory and multisensory representations               niques are computationally expensive and, thus, only prac-
were given by A and L i . Analogous computations were car-                  tical for comparing small numbers of models. How should a
ried out to compute the distibution of a visual representation              researcher pick a small number of models to consider? The
given an item’s auditory features.                                          Bayesian nonparametric approach eliminates (or at least ame-
   Representative results are shown in Figure 6. Four test                  liorates) the problems associated with model comparison.
items (items 1, 12, 24, and 28) were selected at random with                   We have proposed a Bayesian nonparametric model of mul-
the constraint that one item corresponded to each spoken digit.             tisensory perception. The model includes a set of latent vari-
                                                                         2637

                       P(V|A=1)                 P(V|A=12)                   P(V|A=24)                  P(V|A=28)
                1                          1                        1                          1
                0                          0                        0                          0
                    1    2    3    4          1    2    3    4           1     2   3    4           1    2    3    4
                       P(A|V=1)                 P(A|V=12)                   P(A|V=24)                  P(A|V=28)
                1                          1                        1                          1
                0                          0                        0                          0
                    1    2    3    4          1    2    3    4           1     2   3    4           1    2    3    4
Figure 5: Graphs in the top row demonstrate that when presented with auditory features of a test item corresponding to one
of the digits, the multisensory perception model’s distribution of visual representations was tightly peaked at a representation
corresponding to the same digit. Graphs in the bottom row show analogous results for distributions of auditory representations
given test items’ visual features.
ables that learn multisensory features from unisensory data.                                References
The model is highly flexible because it makes few statistical      Amedi, A., Malach, R., Hendler, T., Peled, S., & Zohary, E.
assumptions. In particular, the number of multisensory fea-           (2001). Visuo-haptic object-related activation in the ven-
tures is not fixed a priori. Instead, this number is estimated        tral visual pathway. Nature Neuroscience, 4, 324-330.
from the data.                                                     Austerweil, J. L. & Griffiths, T. L. (2009). Analyzing human
   We applied the model to a real-world visual-auditory data          feature learning as nonparametric Bayesian inference. In
set obtained when people spoke English digits. Our results            D. Koller, Y. Bengio, D. Schuurmans, & L. Bottou (Eds.),
are consistent with several hypotheses about multisensory per-        Advances in Neural Information Processing Systems 21.
ception from the cognitive neuroscience literature. We found          Cambridge, MA: MIT Press.
that the model obtained the statistical advantages provided by     Blei, D. M., Griffiths, T. L., & Jordan, M. I. (2010). The
sensory integration. We also found that the model acquired            nested Chinese restaurant process and Bayesian inference
multisensory representations that were relatively sensory in-         of topic hierarchies. Journal of the ACM, 57, 1-30.
variant. Lastly, we found that the model was able to associate     Calvert, G. A., Bullmore, E. T., Brammer, M. J., Campbell,
unisensory representations based on different modalities.             R., Williams, S. C. R., McGuire, P. K., Woodruff, P. W.
   Because the multisensory perception model is based on              R., Iversen, S. D., & David, A. S. (1997). Activation of
Bayesian statistics, it can be regarded as an ideal observer          auditory cortex during silent lipreading. Science, 276, 593-
inferring optimal multisensory features from unisensory data          596.
(Austerweil & Griffiths, 2009). As such, it provides a basis       Ernst, M. O. & Banks, M. S. (2002). Humans integrate visual
for a rational analysis of multisensory perception. This anal-        and haptic information in a statistically optimal fashion.
ysis suggests that the acquisition of latent multisensory rep-        Nature, 415, 429-433.
                                                                   Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (1995).
resentations that are sensory invariant and more statistically
                                                                      Bayesian Data Analysis. London, UK: Chapman & Hall.
robust than latent features from unisensory models is a ratio-
                                                                   Griffiths, T. L. & Ghahramani, Z. (2005). Infinite latent fea-
nal response of an agent attempting to learn the structure of
                                                                      ture models and the Indian buffet process. Gatsby Unit
its multisensory environment. It also suggests the rationality
                                                                      Technical Report GCNU-TR-2005-001.
of acquiring associations among unisensory representations.
                                                                   Griffiths, T. L. & Ghahramani, Z. (2006). Infinite latent fea-
                                                                      ture models and the Indian buffet process. In B. Schölkopf,
                    Acknowledgments                                   J. Platt, & T. Hofmann (Eds.), Advances in Neural Infor-
                                                                      mation Processing Systems 18. Cambridge, MA: MIT Press.
We thank J. Drugowitsch, A. E. Orhan, and C. Sims for many         Movellan J. R. (1995). Visual speech recognition with stochas-
helpful discussions, and J. Movellan for making the Tulips1           tic networks. In G. Tesauro, D. Toruetzky, & T. Leen (Eds.),
data set available on the web. This work was supported by a           Advances in Neural Information Processing Systems 7. Cam-
research grant from the National Science Foundation (DRL-             bridge, MA: MIT Press.
0817250).
                                                               2638

