UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Beyond Boolean logic: exploring representation languages for learning complex concepts
Permalink
https://escholarship.org/uc/item/69c705cp
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Piantadosi, Steven
Tenenbaum, Joshua
Goodman, Noah
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                               Beyond Boolean logic: exploring representation
                                     languages for learning complex concepts
                              Steven T. Piantadosi, Joshua B. Tenenbaum, Noah D. Goodman
                                                    { piantado, jbt, ndg } @ mit.edu
                                            MIT Department of Brain and Cognitive Sciences
                                  43 Vassar Street, Building 46, Room 3037G, Cambridge, MA 02139
                             Abstract                                     In this paper we extend the probabilistic approach to con-
                                                                       cept induction to representation languages which manipu-
   We study concept learning for semantically-motivated, set-
   theoretic concepts. We first present an experiment in which we      late sets of objects. We first describe an experiment that
   show that subjects learn concepts which cannot be represented       explores the difficulty of learning concepts that involve set-
   by a simple Boolean logic. We then present a computational          manipulation and quantification. Second, we compare human
   model which is similarly capable of learning these concepts,
   and show that it provides a good fit to human learning curves.      difficulty to the predictions of models with varying RLs. Our
   Additionally, we compare the performance of several potential       modeling work has two goals: the first is to test different RLs
   representation languages which are richer than Boolean logic        to see which provide the best account of people’s learning
   in predicting human response distributions.
   Keywords: Rule-based concept learning; probabilistic model;         behavior. Each possible RL differs in representational power
   semantics.                                                          and the way in which it assigns probability to potential con-
                                                                       cepts. This means that different RLs make different predic-
                          Introduction                                 tions about people’s learning trajectories and we can therefore
Every cognitive theory requires a hypothesis about mental              compare RLs by determining how well they match subjects’
representation—what structures and operations form the ba-             empirical response distributions. The second goal of the mod-
sis for complex ideas. High-level, symbolic theories often             eling work is to provide an explicit learning theory for these
characterize this representation using a representation lan-           concepts. Work on boolean concept learning has provided a
guage (RL) (Fodor 1975) that specifies primitive elements and          probabilistic model which accounts for subjects’ behavior in
composition laws which can be used to form complex cogni-              acquiring boolean concepts, but there is no comparable for-
tive structures. This approach has been extensively studied            mal theory for concepts which require a richer representation
within two traditions in cognitive science: concept learning           language. Such a theory would importantly extend rule-based
and linguistic semantics. In models of rule-based concept              concept learning in cognitive science to richer, linguistically-
learning (Bruner, Goodnow, & Austin 1956, Shepard, Hov-                interesting semantic representations.
land, & Jenkins 1961, Feldman 2000) the representation lan-
guage has typically been simple Boolean logic, which rep-                                Behavioral experiment
resents concepts—stable mental representations—using con-              The experiment we present aims to extend the rule-based con-
junctions, disjunctions, and negation of simple perceptual             cept learning paradigm to concepts which refer to sets and
primitives. Goodman et al. (2008) presented a model of prob-           properties of sets of objects. To do this, we used a learning
abilistic learning for rule-based concepts that represents con-        paradigm where subjects see a set of objects, guess at a label-
cepts in a simple propositional language and achieves state-           ing of the objects according to the unknown target concept,
of-the-art fits to experimentally-measured difficulty of learn-        and receive feedback on their responses. Subjects used this
ing Boolean concepts. The logical complexity of concepts               feedback to infer the target concept.
appears to play a crucial role in determining how these con-
cepts are learned: learners are biased to preferentially learn         Procedure
concepts with simpler representations.                                 Amazon’s Mechanical Turk was used to run 381 subjects.
   However, simple Boolean concepts can capture only a very            Each subject was told that they had to learn the meaning of
limited range of the human conceptual repertoire. People               a novel word, wudsy, from an alien language. Subjects were
readily conceptualize context-dependent meanings such as               told that aliens use wudsy, to refer to some objects in a col-
“happiest,” and can form more complex and abstract rela-               lection of objects, and that they have to figure out what makes
tional concepts like “everyone with two or more siblings.”             an object wudsy. Subjects were informed that what makes an
Semantic theories capture such meanings using primitive op-            object wudsy may depend on which other objects are present.
erations which manipulate and quantify over sets of objects,              During the experiment subjects were shown a set of four
rather than simply features and propositional connectives.             objects which varied in size, color, shape, and background
The denotation of a quantifier like “some,” for instance, is a         color. An example set of items is shown below:
function which takes two sets, A and B, and is true only when
the intersection of A ∩ B is nonempty1 (Montague 1974).                be A and the set of things which smiled would be set B. “Some
                                                                       boy smiled” is true if and only if the intersection of boys and things
    1 In a sentence like “Some boy smiled,” the set of boys would      which smiled is nonempty.
                                                                   859

                                                                       largest. As such, they require answering NA when there is not
                                                                       a unique largest element3 .
                                                                       Results
                   Figure 1: An example set.                           The plots in Figure 2 show subjects’ accuracy at labeling
                                                                       which objects are wudsy (y-axis), as a function of the amount
   After seeing a set of objects, subjects were told to guess          of labeled data they received (x-axis). Subjects who were
which objects were the wudsy ones. For each object in the              more than 3 standard deviations below the mean accuracy
set, they were required to response “Yes,” “No,” or “NA,” and          for each concept were removed in order to exclude subjects
were told to respond “NA” when it is unspecified whether an            who were not performing the task. The vertical error bars
object is wudsy. For this example, subjects might entertain            show binomial 95% confidence intervals, and the red lines
the concept is “red objects,” in which case they should re-            show the best fitting model, which is discussed in the next
spond that the second and fourth objects are in the concept            section. These results reveal several interesting qualitative
and the first and third are not. However, subjects might also          trends. First, subjects accuracies increase for almost all of
entertain that the meaning of wudsy is context-dependent, as           the concepts. Importantly, even though the subjects receive
in, for instance “unique smallest.” Similarly, the concept may         labeled data, they are never explicitly instructed on the con-
also be complex, such as “same shape as the object with the            cept. This means that high accuracy can only be achieved by
darkest background.” The shape with the darkest background             generalizing from the observed data, which requires inferring
is a circle, so subjects should say all the circles are in the         abstract rules for these concepts.
concept; if all backgrounds are the same color, subjects may              Two interesting exceptions to subjects’ general ability to
respond “NA.” After responding, subjects were told what the            learn these concepts are “Everything iff there is a triangle”
correct answer was according to the target concept, but never          and “Everything iff there is a single blue object.” Subject per-
given explicit instruction on the target concept. Subjects who         formance on these concepts does not substantially improve,
responded incorrectly to any element of the set were penal-            and these are intuitively somewhat unnatural concepts which
ized with a 5 second delay, during which they saw the set and          require all elements of a set to be selected based on what
the correct responses for each object.                                 the set contains. Words do exist with similar denotations in
Materials & Concepts                                                   English—for instance, a set is contaminated if one element
                                                                       of the set is bad—but subjects find these types of concepts
The meanings subjects were required to learn consisted of the          unusually hard to learn.
concepts shown in Figure 2. These concepts include simple                 Figure 2 reveals a number of places where subject perfor-
boolean rule-based concepts (e.g. “circles” and “circles or            mance drops temporarily for a single set–for instance at item
blue objects”), as well as more complex concepts which can-            32 of “there exists a smaller object.” Post-hoc analysis re-
not be expressed in boolean logic (“larger than all the other          vealed that many of these dips are caused when subjects see
objects”), and concepts which require several bound variables          one of the first exceptions to a plausible alternative concept:
to express (“Same shape as the largest blue object”).                  item 32 is the first time that all objects in the set are the same
   Several of the concepts we studied focus on size predicates.        size. Subjects responded true to objects in this item, consis-
This is because size predicates, such as “largest” and “small-         tent with a concept such as “not smaller than the rest,” but
est,” are salient properties of objects in sets. They are per-         incorrect according to the target concept.
haps the simplest words whose meaning is context-sensitive,
and therefore not expressible with only conjunctions, disjunc-         Analysis
tions, and negation of object features. We included three sim-         We first used a regression to analyze how subjects’ learning
ple size relations, “there exists a smaller object,” “larger than      rate varied across the 12 concepts studied4 . In each logistic
all other objects,” and “one of the largest objects.” Note that        regression the outcome was whether the subject’s response to
the latter two differ with respect to uniqueness: if there are         each object in a set was correct, and the independent vari-
two objects of the maximal size, then neither is larger than all       able was the number of items each subject had seen so far
other objects, but both are one of the largest2 .                      (0 . . . 70). The key prediction we tested is whether slopes
   Because we included these simple size predicates, it is nat-        (regression coefficients)—which quantify the effect of addi-
ural to include complex concepts which are also based on               tional data on accuracy—differed between concepts.
size, such as “same shape as the largest object,” “same shape
                                                                           3 This makes it difficult to compare these concepts with the sim-
as the largest blue object,” and “unique largest blue object.”
                                                                       ple size-predicate concepts since the latter never require NA, which
All three of these concepts require finding the largest object         may be a difficult response for subjects to learn, independent of the
and selecting other elements based on the properties of the            concept.
                                                                           4 Because subjects typically were only run on one concept, sub-
    2 These concepts are interesting in part because it is unclear     ject effects are confounded with concept. We therefore performed
which of these meanings corresponds to the denotation of “largest”     a separate mixed-effect logistic regression (Gelman & Hill 2007)
in English, and also what role pragmatics plays in understanding       within each concept including slopes and intercepts by subjects. Re-
“largest” in normal conversation.                                      gression coefficients across concepts were compared using t-tests.
                                                                   860

Figure 2: Subject accuracy (y-axis) in labeling the wudsy objects as a function of trial number (x-axis). Black lines show subject
mean percent correct. Error bars are 95% confidence intervals. The red line shows the best-fitting model, although note that the
model is fit based on agreement with the full distribution of responses, not the accuracies shown here. Numbers in the lower
right show the correlation between the model and human accuracies.
   Our results replicate basic effects in Boolean concept learn-     the one that was used to generate the data—high accuracy
ing (Bruner, Goodnow, & Austin 1956, Shepard, Hovland, &             on some concepts can be achieved by inferring related con-
Jenkins 1961). As is clear from Figure 2, simple concepts            cepts. To address this, we compared how well closely-related
(“blue objects”) are easier to learn than complex concepts           concepts predicted subjects’ responses in the last half of the
(“circles and blue objects”) (t = 2.70, p < 0.01). In addition,      experiment. For each target concept in Table 1, we looked
our results replicate that conjunctions (“circles and blue ob-       at data points for which the target concept made a differ-
jects”) are easier to learn than disjunctions (“circles or blue      ent prediction from the specified alternative hypothesis. For
objects”) (t = 3.10, p < 0.01). These replications provide val-      instance, we looked at sets for which “Largest blue object”
idation for our experimental paradigm.                               and “blue objects” made different predictions—that is, when
   These effects of complexity also generalized to more com-         there are multiple blue objects, so not all of them are the
plex functions than those expressible in Boolean logic. For          largest. We then computed the percent of subjects who re-
instance, “The unique largest blue object” was easier to learn       sponded more than half the time in agreement with the target
than “The same shape as the unique largest blue object” (t =         concept, as well as the overall proportion of time subjects
2.71, p < 0.01). This effect is interesting because it shows         responded with the target concept. These results show that
the additional difficulty associated with more complex set-          for most of the concepts, subjects typically responded in ac-
theoretic concepts. The latter concept requires an additional        cord with the target concept and not a close alternative. The
bound variable to express in first-order logic, or a lambda ab-      only exception to this is the comparison between “One of the
straction to express in lambda calculus, and the effect of this      largest objects” and “size = 5” (In the items, “5” is the max-
complexity is reflected in subjects’ learning rates. The re-         imal size for objects), which showed that subjects may have
gression revealed no difference between uniqueness presup-           been learning to identify objects based on comparing them
positions for concepts involving the largest element of a set:       to absolute size, rather than a context-sensitive measure of
“larger than all other objects” was no more difficult than “one      “largest”. In general, though, these results show that subject’s
of the largest objects” in either slopes (t < 1.26, p > 0.20) or     pattern of learning cannot be explained by simpler theories
intercepts (t < 1.83, p > 0.05).                                     which make reference only to only individual objects’ prop-
   Importantly, subjects may infer a different concept from          erties. This is especially striking given that many of the alter-
                                                                 861

            Target                  Alternative       Subject pct. Response Pct.
                                                                                   This expression represents a function which checks whether
                                                                                   its argument, x, is red and a square, and returns true or false
                                One of the largest    1.00         0.94
  Larger than all other objects objects                                            for any object (since red and square are assumed to return
                                Size = 5              0.81         0.67
                                Size ≥ 4              1.00         0.82            true or false). We might also have concepts which take two
                                Size ≥ 3              1.00         0.88            arguments, a contextually-relevant set S and an element x:
                                One of the largest    0.79         0.66                           λS λx.(equal x (unique-smallest S))
  Unique largest blue object    objects
                                Blue Objects          0.79         0.74
                                Larger than all other 0.79         0.63            This function checks if x is the object which is the unique,
                                objects
                                                                                   smallest element of S.
  Same shape as the unique      Same shape as the     0.52         0.58               We use a probabilistic context-free grammar (PCFG)
  largest blue object           largest object                                     which assigns probability to every possible composition of
                                                                                   primitive elements. This PCFG functions as a prior over con-
                                Size = 5              0.36         0.46
  One of the largest objects    Size ≥ 4              1.00         0.67            cepts and for simplicity, we assume that all PCFG expansions
                                Size ≥ 3              1.00         0.67            are equally probable6 . In general, the PCFG assigns high
                                                                                   probability to short or “simple” compositions of L ’s primi-
                                Size = 5              1.00         0.67
  There exists a smaller object Size ≥ 4              1.00         0.73            tives, and lower probability to complex rules. For instance,
                                Size ≥ 3              1.00         0.91
                                                                                   a function λx.(red x) will be higher probability a-priori than
Table 1: Comparison of subject agreement with target concepts                      λx.(red x) ∧ ((square x) ∨ (circle x)). This captures the no-
compared to alternative concepts. Subject pct. shows the propor-                   tion that people should be biased to prefer simple explana-
tion of subjects agreeing more than 50% with the target concept,                   tions of the labeled data they observe.
Response Pct. shows the overall percent agreement with the target.
                                                                                      The second part of the model is a likelihood function which
native hypotheses are much simpler than the target concepts,                       provides the probability of labels according to a hypothesized
and provides strong evidence that subjects are attending to                        RL expression. Specifically, for any composition E of prim-
more than simple object properties.                                                itives in L , the correct label is generated with probability α,
                                                                                   and a label is chosen uniformly at random with probability
                        Computational model                                        1 − α. However, it is also likely that memory factors come
The behavioral experiment shows that generally subjects are                        into play in remembering past labeled examples. We include
able to induce these types of set-theoretic concepts from the                      this in the model by weighting the log likelihood for the n’th
labeled data. Although it is important that subjects can even-                     data point back in time by n−β , where β > 0. As β → 0, the
tually learn most of these concepts, we are also interested in                     model has perfect memory, and as β → ∞ the model quickly
whether their learning trajectory—their guesses and hypoth-                        forgets past data points. This leaves us with two unknown
esized concepts at each point in time—follow sensibly from                         free parameters: α, which controls how reliably set elements
the observed data. It may be rational to initially learn simpler                   are labeled, and β which controls how much more recent set
related concepts which give approximately correct answers.                         elements matter than past ones.
There is no guarantee, for instance, that 70 items are enough                         Together, the prior and likelihood specify a complete prob-
to justify learning the correct form of the target concepts. We                    abilistic model for any RL. Formally, we can score the prob-
next present a computational model which can learn these                           ability of a hypothesized concept expression E conditioned
types of set-theoretic concepts.                                                   on a collection of example sets S with corresponding labels L
    Our computational model aims to extend the rational rules                      according to Bayes rule:
model of Goodman et al. (2008) to a richer hypothesis
space—one which is capable of representing these types of                                         P(E | S, L, L ) ∝ P(L | S, E)P(E | L ).              (1)
set-theoretic concepts. The probabilistic structure of the
model and inference algorithm we use are neutral with respect                      Here, P(E | L ) is the probability of E according to the PCFG
to the RL, meaning that any potential RL can be incorporated                       for L and P(L | S, E) scores the likelihood of the labels L
and tested to see what distribution of responses it predicts for                   under the observed sets of objects S and hypothesized expres-
each object in each concept.                                                       sion E. While Equation 1 scores the probability of any given
    Each potential RL L defines a hypothesis space of potential                    expression E, it is a complex inference problem to actually
concepts corresponding to the set of all ways to compose the                       determine what expressions are likely given the data. This
RL’s primitive functions in order to create functions which                        problem is difficult because the space of possible expressions
map objects to labels (true, false, NA). For instance one con-                     E is in principle infinite and difficult to search. We solved this
cept might be5                                                                     problem using a Markov-Chain Monte-Carlo (MCMC) sim-
                                                                                   ilar to Goodman et al (2008)’s method, which takes samples
                           λx.(red x) ∧ (square x)
                                                                                       6 Unlike the rational-rules model, we do not integrate out the
     5 We   write functions as lambda expressions, meaning that the                PCFG production probabilities. This is because primitives which
name for the argument is preceded by λ. We also use prefix no-                     introduce new bound variables, such as quantifiers, make this inte-
tation: a function f applied to an argument x is written (f x).                    gration difficult and potentially not analytically tractable in general.
                                                                               862

from the posterior distribution P(E | S, L, L ). This method             as a baseline, and way to test if subjects are really per-
takes a biased random walk around the space of hypotheses                forming the task. The SIMPLE - BOOLEAN language is one
by making local changes to hypothesized expressions E, and               which include basic logical operations and object proper-
can be shown to, in the limit, draw samples from the poste-              ties, and implements the representational system studied most
rior distribution. We ran the MCMC algorithm for a range                 in previous rule-based concept learning experiments. The
of α and β values for each amount of data, in each sequence,             SET- FUNCTIONS language extends the SIMPLE - BOOLEAN
conditioning on the correct, observed labels for all previous            language by including primitive operation for testing if sets
sets in the sequence. This gives a distribution P(E | S, L, L )          contain elements, extracting sets or elements with maximal
on expressions E in the RL L at each point during learning.              or minimal properties along the size-dimension and filtering
These expressions can be evaluated on the next item in or-               sets by elements. The QUANTIFIERS language extends the
der to provide a model prediction of subject’s distribution of           SET- FUNCTIONS language by incorporating quantification.
responses, conditioned on the observed labeled data. Thus,
the model was run conditioned on the same labeled data hu-               Results & Discussion
man subjects were given, and—just like human subjects—                   Table 2 shows the performance of these models in predicting
was asked to make predictions about the correct labels for the           the human distribution of responses across the 12 concepts
next data point. Ideally, subjects’ distribution of responses at         studied. This shows the average log-likelihood of the human
each point in time during learning should correspond to the              responses for the best-fitting values of α and β within each
predictions of the model, conditioned on the exact same se-              concept8 . This table illustrates several key properties of the
quence of training data.                                                 RLs. First, the RESPONSE - BIASED model is overall the worst
    One goal of the model is to test different representational          predictor of human responses. This is important because it
languages to see which provide the best theory of people’s               shows that subjects are performing the task, and performing
inductive biases in learning these concepts. We computed the             nontrivial inferences about the target concepts.
posterior predictive distribution of responses for each repre-              In addition, this figure shows that while SIMPLE - BOOLEAN
sentation language L and saw which assigned the human re-                is a good predictor for the simple Boolean concepts, SET-
sponses highest likelihood7 . We compared four different RLs             FUNCTIONS and QUANTIFIERS provide a better account for
with differing primitives and representational power:                    the set-theoretic concepts that subjects are able to learn.
                                                                         SIMPLE - BOOLEAN provides the worst account for “same
   Language                      Primitive Operations
                                                                         shape as the largest object” and “same shape as the unique
   R ESPONSE - BIASED            true, false, undefined
                                                                         largest blue object.” While subjects do not learn these con-
   S IMPLE - BOOLEAN             and, or, not, shape, size, color,
                                                                         cepts especially well, these results show that the SIMPLE -
                                 background-color, equal
                                                                         BOOLEAN does not account well for subject responses.
   S ET- FUNCTIONS               contains, filter, only, unique-
                                                                            Overall, the best RL is QUANTIFIERS; however, the differ-
                                 largest, unique-smallest, set-of-
                                                                         ences between QUANTIFIERS and SET- FUNCTIONS is small.
                                 largest, set-of-smallest, same-
                                                                         Richer representation languages not only have the formal
                                 object
                                                                         power to represent the types of set-theoretic and logical con-
   Q UANTIFIERS                  exists, forall
                                                                         cepts required by human conceptual systems, but also provide
    Each RL is a superset of the preceding languages, ex-                a better account of human inductive leaning than the other
cept that none other than RESPONSE - BIASED contain true,                RLs considered here.
false, and na as primitives. Here, shape, color, and                        As discussed above, the black line in Figure 2 shows learn-
background-color are functions which extract the corre-                  ing curves showing percent accuracy over time for human
sponding properties of objects. equal tests if two properties            subjects. This figure also shows a red line, corresponding to
are equal. contains returns true if a set contains an element,           the performance of the RL QUANTIFIERS for the best-fitting
filter removes all elements in a set not satisfying a predicate,         α and β within each concept. We chose the best-fitting model
and only return the only element of a set and NA if the set              parameters based on which parameter values assigned highest
has more than one element. The primitives unique-largest                 likelihood to the observed distribution of human responses,
and set-of-largest return the unique largest element in a set            “true,” “false,” and “NA.” Doing this does not necessarily pro-
(and NA otherwise), and the set of elements for which none               vide the best fit to the human learning curves in Figure 2 since
are larger, respectively. same-object tests if two objects are           the model is not fit to human accuracy (correct/incorrect).
identical on all dimensions. exists and forall are first-order           This means that Figure 2 shows a conservative view of the
existential and universal quantifiers.                                   agreement between human accuracies and model accuracies.
    Intuitively, the R ESPONSE - BIASED language allows learn-           For the concepts “circles or blue objects” and “unique largest
ers only to infer a distribution on responses, but not give re-          blue object” the model’s learning trajectory would increase
sponses which depend on the current objects. This serves
                                                                             8 That is, these numbers are the total log likelihood assigned to
     7 Model predictive distributions were smoothed to give each re-     human responses, divided by the number of responses. This was
sponse a minimum possible probability of 0.01, to prevent diver-         necessary for cross-concept comparison since concepts may have
gence.                                                                   differing numbers of subject responses.
                                                                     863

       Concept                                      R ESPONSE - BIASED     S IMPLE - BOOLEAN      S ET- FUNCTIONS   Q UANTIFIERS
       Blue objects                                        -0.66                  -0.18                  -0.19           -0.18
       Circles                                             -0.73                  -0.17                  -0.17           -0.17
       Circles or blue objects                             -0.81                  -0.73                  -0.74           -0.74
       Circles and blue objects                            -0.30                  -0.27                  -0.27           -0.27
       Everything iff there is a triangle                  -0.80                  -0.73                  -0.73           -0.73
       There exists a smaller object                       -0.81                  -0.51                  -0.40           -0.41
       Larger than all other objects                       -0.58                  -0.48                  -0.46           -0.36
       One of the largest objects                          -0.80                  -0.63                  -0.28           -0.28
       Everything iff there is a single blue object        -0.85                  -0.78                  -0.78           -0.78
       Same shape as the largest object                    -1.10                  -1.75                  -0.99           -0.99
       Unique largest blue object                          -1.05                  -1.54                  -1.06           -1.06
       Same shape as the unique largest blue object        -1.08                  -1.34                  -1.06           -1.04
       Mean                                               -0.797                 -0.760                 -0.594          -0.584
Table 2: Model log likelihoods per response for each concept. These represent the model log likelihood assigned to human
responses, divided by the total number of responses in each concept to allow comparisons across concepts.
more for other values of α and β, and thus look more like              mantic meaning for words like “most.” Our work provides
subject’s accuracies, but provide a less-good fit to subjects’         a complementary approach to the same problem—instead of
overall response distribution.                                         measuring response times, we studied what RLs provide a
   This figure shows good fit between the probabilistic model          good account of human inductive biases during learning. This
and human learning. This fit appears especially remarkable             method may be broadly applicable to discovering the form of
for concepts which subjects have a difficult time learning,            semantic representations in natural language.
such as “Everything iff there is a triangle.” Because subjects            Of course, the RLs we study here are still incomplete with
do not learn this concept well, the best-fitting α is low and          respect to the full richness of human conceptual systems;
β is highly negative, meaning that the model is not penal-             however, this work suggests that rule-based concept-learning
ized much for incorrect answers and down-weights old data.             can be extended to complex concepts which can begin to ap-
The model therefore responds with in simple ways, such as              proach the complexity and context-dependence observed in
always responding true, or responding true to only the trian-          human linguistic systems. Furthermore, the model provides
gles; subjects appear to use similar strategies, and thus both         one potential acquisition theory for semantic concepts. Chil-
show similar patterns of response accuracies9                          dren may learn semantic meanings like adults in our exper-
   The model also shows more subtle agreement patterns with            iment did—by inducing concepts in a sufficiently-powerful
human subjects. First, it is capable of learning simple boolean        compositional RL.
concepts in a way similar to humans, quickly arriving at the
correct meaning given the training data. This is also true for                                   References
concepts like “there exists a smaller object” and the other            Bruner, J. S., Goodnow, J., & Austin, G. (1956). A study of
size-predicates. The model also matches local dips and peaks             thinking. New York: Wiley.
in reasonably well. This is because the model, like people,            Feldman, J. (2000). Minimization of boolean complexity in
may temporarily be led to a concept which is not the target              human concept learning. Nature, 407, 630-633.
concept, just as subjects (e.g. at item 32 of “there exists a          Fodor, J. (1975). The language of thought. Cambridge, MA:
smaller object”). This provides evidence that people make                Harvard University Press.
the same rational, statistical inferences given the same data.         Goodman, N., Tenenbaum, J., Feldman, J., & Griffiths, T.
                                                                         (2008). A rational analysis of rule-based concept learning.
                            Conclusion                                   Cognitive Science, 32, 108-154.
While the SIMPLE - BOOLEAN RL provided a good fit to hu-               Hackl, M. (2009). On the grammar and processing of pro-
man response data in some cases, it is insufficient to represent         portional quantifiers: most versus more than half. Natural
some of the complex concepts that subjects learned. Subjects’            Language Semantics, 17, 63-98.
ability to learn these concepts was demonstrated by their              Montague, R. (2002). The proper treatment of quantification
learning curves for several context-dependent concepts. The              in english. In P. Portner & B. H. Partee (Eds.), Formal
comparison of different RLs suggests a potentially fruitful ap-          semantics: The essential readings. Oxford: Blackwell.
proach to discovering the precise form of semantic represen-           Pietroski, P., Lidz, J., Hunter, T., & Halberda, J. (2009). The
tations. Recently, Pietroski et al. (2009) and Hackl (2009)              meaning of most: semantics, numerosity, and psychology.
have used psychophysical measures to make inferences about               Mind and Language, 24, 554-585.
plausible representations and computations that underlie se-           Shepard, R., Hovland, C., & Jenkins, H. (1961). Learn-
                                                                         ing and memorization of classifications. Psychol. Monogr.
    9 The  best fitting β also shows a modest negative correlation       Gen. Appl., 75, 1-42.
(R = −0.55, p = 0.06, N = 12) with response accuracies over the
12 concepts, suggesting an interaction between the target concept
and the attentional or memory resources people allocate.
                                                                   864

