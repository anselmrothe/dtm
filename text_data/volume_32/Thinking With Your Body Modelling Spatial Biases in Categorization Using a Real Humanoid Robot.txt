UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Thinking With Your Body: Modelling Spatial Biases in Categorization Using a Real Humanoid
Robot
Permalink
https://escholarship.org/uc/item/7tt5556n
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Morse, Anthony
Belpaeme, Tony
Cangelosi, Angelo
et al.
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

           Thinking With Your Body: Modelling Spatial Biases in Categorization
                                             Using a Real Humanoid Robot
                                    Anthony F. Morse (anthony.morse@plymouth.ac.uk)
                                      Tony Belpaeme (tony.belpaeme@plymouth.ac.uk)
                                   Angelo Cangelosi (angelo.cangelosi@plymouth.ac.uk)
                                  Centre for Robotics and Neural Systems, University of Plymouth
                                                     Plymouth, Devon, PL4 8AA, UK
                                              Linda B. Smith (smith4@indiana.edu)
                              Cognitive Development Lab, Indiana University, 1101 East Tenth Street
                                                       Bloomington, IN 47405-7007
                             Abstract                                   way or a little that way. This rather sparse account supposes
  This paper presents a model of sensorimotor learning
                                                                        that such profiles can be constructed and recognized,
  grounded in the sensory streams of a real humanoid robot (the         leading to the recognition of objects in the world in terms of
  iCub robot). The robot participates in a replication of two           their Gibsonian affordances (Gibson, 1979).                 This
  developmental psychology experiments, in which it is shown            construction of profiles of interaction is crucial to the ability
  how spatial cues are sufficient for associating linguistic labels     of sensorimotor theories to account for high-level cognitive
  with objects. The robot, using auto-associated self-organizing        and mental phenomena such as perception, but is also the
  maps connecting is perceptual input and motor control,                least detailed and most challenging aspect of these theories.
  produces similar performance and results to human
  participants. This model confirms the validity of a body              Few sensorimotor theories do more than just suppose an
  centric account of the linking of words to objects as sufficient      ability to do this. Nevertheless such embodiment centric
  to account for the spatial biases in learning that these              accounts of perception are supported by a large number of
  experiments expose.                                                   psychology experiments and neuroscientific evidence
  Keywords: Developmental Robotics; Neural Networks;                    exposing various bodily biases in categorization
  Sensorimotor; Learning; Spatial Bias; Category Learning.              (Richardson & Kirkham, 2004; Smith, 2005; Smith &
                                                                        Samuelson, 2010). For example, for Gallese and Lakoff
                         Introduction                                   (2005) the biological sensorimotor system is not merely
                                                                        foundational to our mental conceptual abilities but
At the heart of all sensorimotor theories of cognition is the
                                                                        constitutes action and perception which are inseparably
claim that perception is to a large degree based upon the use
                                                                        interwoven in those sensorimotor systems. In addition, the
of sensorimotor knowledge in predicting the future sensory
                                                                        re-activation of visual and motor areas during imagined
consequences of an action, either overtly executed or
                                                                        actions (Jeannerod, 1994; Kosslyn & Press, 1994) “shows
covertly simulated (Gallese & Lakoff, 2005; Morse, Lowe,
                                                                        that typical human cognitive activities such as visual and
& Ziemke, 2008; Noë, 2004, 2009; O'Regan & Noë, 2001).
                                                                        motor imagery, far from being of a disembodied, modality-
As such our perception of continuous contact with a rich
                                                                        free, and symbolic nature, make use of the activation of
visual world laid out in front of us is somewhat misleading,
                                                                        sensory-motor brain regions.” (Gallese & Lakoff, 2005, p.
as sensory input is highly impoverished by comparison to
                                                                        465). Similarly while paralysis and neuromuscular
perception; for example visual acuity is focused on an area
                                                                        blockades do not disrupt conscious thought processes
the size of a thumb nail at arm’s length. From a
                                                                        (Topulos, Lansing, & Banzett, 1994), the current activity of
sensorimotor perspective, our perception of things outside
                                                                        the motor cortex is highly influential on both perception and
the fovea is largely constructed from predictions of what
                                                                        thought. Barsalou et al. (2003) highlight some of the ways
you would see were you to look in this or that direction
                                                                        in which body posture and action affect perception and
(Noë, 2004). Clearly such perception is supported by
                                                                        cognition; for example, subjects rated cartoons differently
processing of the sparse input from the periphery of our
                                                                        when holding a pen between their lips than when holding it
visual field, and mechanisms drawing attention to
                                                                        between their teeth. The latter triggered the same
movement, flashes, and other such changes, yet there
                                                                        musculature as smiling, which made the subjects rate the
remains a large disparity between sensory input and
                                                                        cartoons as funnier, whereas holding the pen between the
perception.
                                                                        lips activated the same muscles as frowning and
  In taking a sensorimotor perspective, the recognition and
                                                                        consequently had the opposite effect (Strack, Martin, &
categorization of objects in our perceptual field can be
                                                                        Stepper, 1988). Moreover, bodily postures influence the
achieved through the identification of profiles of interaction
                                                                        subjects’ affective state; e.g., subjects in an upright position
unique to each object category. As an example we can
                                                                        experience more pride than subjects in a slumped position.
perceive a plate as round, not because it projects a round
                                                                        Further compatibility between bodily and cognitive states
image onto our retina, but rather because we can predict
                                                                        enhances performance. For instance, several motor
how our sensory contact will change as we move a little this
                                                                    1362

performance compatibility effects have been reported in             space playing a role in binding objects to names through the
experiments in which subjects responded faster to ‘positive’        expected location of that object.
words (e.g. ‘love’) than ‘negative’ words (e.g. ‘hate’) when           While several other variations of this experiment have
asked to pull a lever towards them (Chen & Bargh, 1999).            been conducted with children, it is these two versions of the
   In the remainder of this paper we describe a                     experiment that we have replicated with our robot model.
developmental robotics (Cangelosi & Riga 2006; Weng et
al. 2002) model of a simple sensorimotor system grounded                            The Robot Experiments
in the sensors and actions of iCub, a child-like humanoid           The ‘modi’ experiments, though not conclusive, strongly
robot.      The robot then participates in a psychology             suggest that body posture is central to the linking of
experiment highlighting the role of body posture and spatial        linguistic and visual information, especially as large
locations in learning the names of objects. Finally we              changes in posture such as from sitting to standing disrupt
compare the results of the robot experiments to the data            the effect reducing performance in the first experiment to
from human child psychology experiments conducted by                chance levels. In our model this suggestion is taken quite
Smith and Samuelson (Smith & Samuelson, 2010).                      literally, using body posture information as a ‘hub’
                                                                    connecting information from other sensory streams in
The ‘Modi’ Experiment	                                             ongoing experience. Connecting information via a ‘hub’
In a series of experiments related to Piaget’s famous A-not-        allows for the spreading of activation via this hub to prime
B error (1963), and derived from experiments by Baldwin             information in one modality from information in another.
(1993), Linda Smith and Larissa Samuelson (Smith &                  Furthermore using the body posture as a ‘hub’ also makes a
Samuelson, 2010) repeatedly showed children between 18              strong connection to the sensorimotor literature reviewed in
and 24 months of age two different objects in turn, one             the introduction; as actions, here interpreted as changes in
consistently presented on the left, and the other consistently      body posture, also have the ability to directly prime all the
presented on the right. Following two presentations of each         information associated with that new position and hence
object, the child’s attention is drawn to one of the now            indicate what the agent would expect to see were it to
empty presentation locations and the linguistic label “modi”        overtly move to that posture. Such predictive abilities are
is presented. Finally the children are presented with both          the foundation of sensorimotor theories.
objects in a new location and asked; “can you find me the              In this experiment we use the humanoid robotic platform
modi”. Not surprisingly the majority (71%) of the children          iCub, an open source platform which has been recently
select the spatially correlated object despite the fact that the    developed as a benchmark platform for cognitive robotics
name was presented in the absence of either object. Varying         experiments (Metta et al., 2008). It has 53 degrees of
the experiment to draw the child’s attention to the left or         freedom, allowing experiments on visual, tactile and
right rather than to the specific location that the object,         proprioceptive perception, manipulation and crawling.
when saying “modi”, resulted in a similar performance               Initial iCub experiments were carried out in simulation
where 68% of the children selected the spatially linked             through the open source iCub simulator (Tikhanoff et al.
object. The results of this experiment challenge the popular        2008), and then adapted and tested on the physical robot
hypothesis that names are linked to the thing being attended        platform.
to at the time the name is encountered.
   In a follow up experiment, using the same basic                  Grounding information in sensory streams
procedure, one group of children were presented with only a         The information linked via the body-posture hub is the
single object labeled while in sight; a second group were           result of processing visual input from the iCub robots
repeatedly presented with a consistent spatial relationship         cameras, taking the average RGB color of the foveal area
until finally an object is labeled while in sight but in the        and using this as an input to a 2D self-organizing map
spatial location where the other object was normally                (SOM) (Kohonen, 1998) described in Equation 1, Equation
presented. In the control group, where a single object is           2, and Equation 3 below. The SOM provides pattern
presented and labeled, 80% correctly picked the labeled             recognition over the input space preserving input topology
object over the previously unencountered object; in the             while capturing the variance of the data. The body-posture
second group (spatial competition) a majority of 60%                ‘hub’ similarly used the joint angles of the robot as input to
selected the spatially linked object rather than the object that    another SOM. Though the iCub robot has 53 degrees of
was actually being attended while labeled. In both                  freedom, for simplicity in the experiments detailed herein
experiments changes in posture from sitting to standing             only 2 degrees from the head (up/down and left/right), and 2
disrupted the children’s ability to link the absent object to       degrees from the eyes (up/down and left/right) were actually
the name through space, while other visual or auditory              used, thus the body map of the iCub robot has 4 inputs, each
distracters did not. This is strong evidence challenging the        being the angle of a single joint. Further experiments are
simple hypothesis that names are associated to the thing            underway using a more complex body posture map
being attended at the time the name is heard, and strong            involving all the degrees of freedom of the iCub robot.
evidence for the role of the body’s momentary disposition in        Finally, auditory input is abstracted as a collection of
                                                                    explicitly represented ‘words’, each active only while
                                                                1363

hearing that word. In the experiments herein these ‘words’           to the body map in real time using positive Hebbian
are artificially activated, though in related work we are            connectivity following Equation 4 below.
using the open source CMU Sphinx library
(http://cmusphinx.org/) to provide voice processing,                 Equation 4 Positive Hebbian learning
achieving the same result from genuine auditory input.                                       Δw ij = α .x i.x j
   Both the color map and the body posture map are
                                                                     Where wij is the weight between node j and node i, αis the
initialized using random values in the appropriate sensory
                                                                     learning rate (0.01), xi is the activity of the winning node in
ranges with an increased probably of values in the extremes
                                                                     one map, and xj is the winning node in the posture map.
of each range until the SOM’s have stabilized. Increasing
the probability of extreme values ensures that the resulting                    €
                                                                     These Hebbian associative connections were then only
stable map fully covers the range of possible input values,
                                                                     modified from the current active body posture node.
without this step mid range values would tend to pull in the
                                                                     Inhibitory competition between any simultaneously active
extremities of the map resulting in poor coverage.
                                                                     nodes in the same map provides arbitration between
                                                                     multiple associated nodes resulting in dynamics similar to
Equation 1: Initial activation of SOM units
                                                                     those expressed in Interactive Activation and Competition
                                                  2
                             i=n                                     (IAC) models which have a long history of use in modeling
                   Aj =     ∑ (v     i −w ij    )                    psychological phenomena (Burton, Bruce, & Hancock,
                             i=0                                     1999; McClelland & Rumelhart, 1981; Morse, 2003).
Where Aj is the resulting activity of each node in the map              As the maps are linked together in real time based on the
following a forward pass, vi is an input, and wij is the weight      experiences of the robot (see Figure 1), strong connections
between that input and the current node. The winning node            between objects typically encountered in particular spatial
is the €
       node with the smallest value for Ai                           locations, and hence in similar body postures build up.
                                                                     Similarly, when the word ‘modi’ is heard, it is also
Equation 2: Final activation of SOM units                            associated with the active body posture node at that time.
                                   ⎛ − β i ⎞                       The relative infrequency of activity in the word nodes
                                   ⎜       ⎟
                                   ⎝ 2 n ⎠
                            y i= e
Where yi is the final activation of the ith node in the map, ß is
the distance from node i to the winning unit, and n is the
total number of nodes in the map. Note: units not within the
neighborhood€ size are set to zero activation, the
neighborhood size and learning rate are monotonically
decreased and the map is taken to be stable when the
neighborhood size is zero.
Equation 3: Weight changes
                                 (
                    Δw ij = α v i−w ij y i     )
Where wij is the weight between input j and unit i, and αis
the learning rate.
The neural model forms the upper tier of a 2 layer                         Figure 1: The general architecture of the model.
        €
subsumption architecture (Brooks, 1986) where the lower                    SOMs are used to map the color space, the body
tier continuously scans whole images for connected regions                 posture, and the word space. These maps are
of change between temporally contiguous images. The                        then linked using Hebbian learning with the body
robot is directed to orient with fast eye saccades and slower              posture map acting as a central ‘hub’. The
head turns to position the largest region of change (above a               model can easily be extended to include other
threshold) in the centre of the image. This motion saliency                features such as visual and touch information in
mechanism operates independently from the neural model,                    additional SOMs.
generating a motion saliency image driving the motor
                                                                     compared with continuous activity in the color map is not a
system. This motion saliency image can be replaced with a
                                                                     problem as competition is between nodes within each map
color-filtered image to provoke orientation to regions of the
                                                                     and not between the maps themselves. Finally at the end of
image best matching the color primed by the neural model.
                                                                     the experiment, when the robot is asked to ‘find the modi’,
   Using the model described we then replicated the
                                                                     activity in the ‘modi’ word node spreads to the associated
experimental setup used by Smith and Samuelson (2010),
                                                                     posture and on to the color map node(s) associated with that
linking the activity of the color map and the auditory words
                                                                     posture. The result is to prime particular nodes in the color
                                                                     map, the primed color is then used to filter the whole input
                                                                 1364

image and the robot adjusts its posture to center its vision on            spoken,
the region of the image most closely matching this color.            5.    Steps 1 and 2 are repeated again,
This is achieved using the same mechanism that detects and           6.    Object A and object B are presented in a new
moves to look at regions of change in the image, replacing                 location and the robot is asked ‘where is the modi’ –
the motion saliency image with a color-filtered image. Here                the robot then looks at one of the objects.
the robot moves to look at the brightest region of the color-
filtered image, circled in Figure 2 below.                         This experiment was repeated 18 times resetting the model
                                                                   between each run and starting with a different random seed
                                                                   thereby simulating 18 different individuals. The position of
                                                                   object A and object B (to the left and right) was swapped
                                                                   between each trial and the location that the robots attention
                                                                   was drawn to in step 4 was changed between the first 9 and
                                                                   the remaining trials thereby removing any bias favoring one
                                                                   object or one location over the other. The whole experiment
                                                                   was videoed and stills from steps 1, 2, 4 & 6 are shown in
                                                                   Figure 3. The results recorded which object was centered in
      Figure 2 left: Image from the iCub robot’s left              the robots visual field following step 6.
      camera. Right: the same image color-filtered
      with the primed blue color of the toy truck. The             Experiment 1 Switch Condition
      brightest area (circled) indicates the closest               In the switch condition the location of presentation of
      match to the primed color.                                   objects A and B was swapped for the first presentation only
Given that the number of associations constructed will grow        of each object (step 1). Subsequent presentations of each
over time in the absence of negative Hebbian learning and          object in steps 2 and 5 remained consistent with the original
in a changing environment, large changes in body posture           locations in the no switch condition. Again the experiment
are used to trigger a removal of these associative                 was repeated, this time 20 times, with the same variations as
connections consistent with the eradication of spatial biases      used in the no switch condition and the results recoded
in the psychology experiment following changes from                which object if any is centered in the robots visual field
sitting to standing. Additionally, external confirmation that      following step 6.
the correct object has been selected leads to more permanent
connections being constructed either directly between word         Experiment 2 Labeling while in sight – Control
and color maps or via a second pattern recognition based           Condition
‘hub’.     As these mechanisms are superfluous to the              Experiment 2 provides a variation on experiment 1 in which
experiments modeled herein their details have been omitted.        objects are labeled while in sight. In the control condition a
   The model as described is then used to replicate each           single object is presented either to the left or to the right and
condition of the two psychology experiments described in           labeled ‘modi’ while being attended, the object is then
the previous section as detailed below.                            presented in a new location with a second object and the
                                                                   robot is asked to ‘find the modi’.
Experiment 1 No Switch Condition
   1.   Object A is presented to the robot’s left – the robot      Experiment 2 Labeling while in sight – Switch
        then looks at object A,                                    Condition
   2.   Object B is presented to the robot’s right – the robot          1.   Object A is presented to the robots left – the robot then
        then looks at object B,                                              looks at object A
   3.   Steps 1 and 2 are repeated,                                     2.   Object B is presented to the robots right – the robot then
   4.   The robot’s attention is drawn to its left in the                    looks at object B
        absence of objects A and B and the word ‘modi’ is               3.   Steps 1 and 2 are repeated
                                    Figure 3: The experiment sequence with the iCub robot.
                                                               1365

     4.   Steps 1 and 2 are repeated again                                 selected the other object. These results are compared to the
     5.   Steps 1 and 2 are repeated yet again                             reported human child data in Figure 4.
     6.   Object A is presented to the robots right (i.e. in the wrong
          location) and the word ‘modi’ is spoken                                          Discussion and Conclusion
     7.   Steps 1 and 2 are repeated again                                 The close match between the results from the robot
     8.   Object A and object B are presented in a new location and        experiments and the human child results reported by Smith
          the robot is asked ‘where is the modi’ – the robot then          and Samuelson (Smith & Samuelson, 2010) suggests that
          looks at one of the objects                                      the hypothesis that body posture is central to early linking of
                                                                           names and object, and can account for the spatial biases
Experiment 2 was repeated 20 times in each condition with
                                                                           exposed by these experiments. What is of relevance here is
differently seeded networks where the identity of object A
                                                                           that the relations between the conditions of each experiment
and object B was swapped on each consecutive trial and the
                                                                           are consistent between the human and robot data, rather than
locations (left and right) were reversed following 10 trials to
                                                                           the absolute values achieved. As can be seen from Figure 4
remove any object or location specific bias.
                                                                           the robot data consistently produced a slightly stronger bias
   This model represents preliminary work investigating
                                                                           toward the spatially linked objects than the human data.
spatial biases in object categorization. Further work
                                                                              That the priming effect did not cause the robot to always
developing and extending this model as a model of
                                                                           select the spatially linked object in every variation of the
sensorimotor learning is currently underway.
                                                                           experiments was due to a variety of factors including; noise
                                                                           in the input sensors, varying lighting and reflectance
Results
                                                                           properties as objects are rotated slightly, inaccuracies in the
In each condition of each experiment, the results recorded                 orienting mechanism and so on. In combination these
which object, if any, was centered in the robots view                      factors produced variations in which a node in the color map
following the final step of each experiment where the robot                was activated as one particular object is being observed, this
was asked to ‘find the modi’. In the no-switch condition of                can lead to weak connections between several similar nodes
experiment 1, 83% (15/18) of the trials resulted in the robot              rather than a single strong connection to one node. In the
selecting the spatially linked object, while the remaining                 switch condition of experiment 1, this situation more
trials resulted in the robot selecting the non-spatially linked            frequently resulted in object B having a stronger connection
object. This is comparable to the reported result that 71% of              to the body posture in which object A was more frequently
children selected the spatially linked object in the human                 observed, thus object B was more strongly primed and
experiment in the same condition (Smith & Samuelson,                       selected. In these cases increasing the consistency in which
2010).                                                                     an object is seen in the labeled location promotes the
                                                                           strengthening of connections leading to that object being
                                                                           selected, as is seen in the no-switch condition of exp. 1.
                                                                              It is anticipated that the inclusion of other visual features,
                                                                           though likely to be subject to similar variance, would
                                                                           increase the discrepancy between the data from this model
                                                                           and the human data. This would be due to activation
                                                                           spreading between maps, influencing the priming in much
                                                                           the same way a localist IAC model (Burton et al., 1999;
                                                                           McClelland & Rumelhart, 1981; Morse, 2003). Despite this
                                                                           the relative effects of the various conditions across each
    Figure 4: The percentage of spatially linked objects                   experiment should remain relatively consistent. We suggest
    selected in each experimental condition for both                       that the close fit to human data could be misleading, as by
    robot data and for the human child data.                               comparison in the human case spatial priming would be in
                                                                           competition with far more complex factors influencing the
Reducing the consistency of the object-location correlation                saliency of the objects, factors we have not attempted to
in the switch condition resulted in a significant reduction in             model here. Conversely such competition may in fact
the spatial priming effect with a close to chance                          reduce the models tendency to over perform thereby more
performance of 55% (11/20) of the trials finishing with the                closely matching the human data.
spatially correlated object being centered in the view of the                 As indicated in the introduction our model is consistent
robot. The remaining 9 trials resulted in the other object                 with the sensorimotor approach to understanding cognition
being selected. In experiment 2 objects were labeled while                 as the model is able to predict the sensory input it would
being attended, the control group resulted in 95% (19/20) of               receive were it to move to different body-postures. This
the trials selecting the labeled object while in the switch                information is accessed simply by a spread of activation
condition only 45% (9/20) of the trials resulted in the                    from primed body-posture nodes in the ‘hub’. The model is
labeled object being selected. The remaining trials all                    also easily scaled up to include additional information
                                                                           presented in additional maps retaining the current IAC-like
                                                                       1366

architecture. Such models are also suitable for use in            McClelland, J. L., & Rumelhart, D. E. (1981). An
hierarchies providing a better fit to the underlying biology.       interactive activation model of context effects in letter
   In conclusion our model accurately reproduces the human          perception: Part 1. An account of basic findings.
data from Smith and Samuelson’s (2010) experiments, in an           Psychological Review, 88, 375407.
ongoing embodied human robot interaction. In fact, the            Metta G., Sandini G., Vernon D., Natale L., & Nori F.
close fit between our data and the reported human data is in         (2008). The iCub humanoid robot: an open platform for
part due to the difficulties and inaccuracies inherent in            research in embodied cognition. In R. Madhavan & E.R.
conducting experiments with complex real robots rather               Messina (Eds.), Proceedings of IEEE Workshop on
than simulations. In future work we are developing and               Performance Metrics for Intelligent Systems Workshop
demonstrating this architecture in a variety of related              (PerMIS’08). Washington, D.C.
sensorimotor and psychological tasks involving object             Morse, A. F. (2003). Autonomous Generation of Burton's
manipulations. The goal is close empirical studies of robots        IAC Cognitive Models. Paper presented at the
and children – in which robot models generate new                   EuroCogSci03, The European Cognitive Science
predictions tested in children. Such joint studies should           Conference.
advance robotics, our understanding of human cognitive            Morse, A. F., Lowe, R., & Ziemke, T. (2008). Towards an
development, and the nature of embodied intelligence more           Enactive Cognitive Architecture. Paper presented at the
generally.                                                          International Conference on Cognitive Systems,
                                                                    Karlsruhe, Germany.
                   Acknowledgements                               Noë, A. (2004). Action in Perception. Cambridge, Mass:
This work has been supported by the EU FP7 ITALK                    MIT Press.
project (no. 214668).                                             Noë, A. (2009). Out of our heads. New York: Hill & Wang.
                                                                  O'Regan, K., & Noë, A. (2001). A sensorimotor account of
                        References                                  visual perception and consciousness. Behavioral and
                                                                    Brain Sciences, 24, 939-1011.
Baldwin, D.A. (1993) Early referential understanding:             Piaget, J. (1963). The origins of intelligence in children.
  infants’ ability to recognize referential acts for what the       New York: Norton.
  are. Developmental Psychology, 29, 832-43.                      Richardson, D. C., & Kirkham, N. Z. (2004). Multimodal
Barsalou, L. W., Niedenthal, P. M., Barbey, A. K., &                events and moving locations: Eye movements of adults
  Ruppert, J. A. (2003). Social embodiment. Psychology of           and 6-month-olds reveal dynamic spatial indexing.
  Learning and Motivation: Advances in Research and                 Journal of Experimental Psychology: General, 133, 46-62.
  Theory, 43, 43-92.                                              Smith, L. B. (2005). Cognition as a dynamic system:
Brooks, R. A. (1986). A robust layered control system for a         Principles from embodiment. Developmental Review,
  mobile robot. IEEE Journal of Robotics and Automation,            25(3-4), 278-298.
  2(1), 14-23.                                                    Smith, L. B., & Samuelson, L. (2010). Objects in Space and
Burton, A. M., Bruce, V., & Hancock, P. J. B. (1999). From          Mind: From Reaching to Words. In K. Mix, L. B. Smith &
  pixels to people: A model of familiar face recognition.           M. Gasser (Eds.), Thinking Through Space: Spatial
  Cognitive Science, 23(1), 1-31.                                   Foundations of Language and Cognition. Oxford, UK.:
Cangelosi A., & Riga, T. (2006). An embodied model for              Oxford University Press.
  sensorimotor grounding and grounding transfer:                  Strack, F., Martin, L. L., & Stepper, S. (1988). Inhibiting
  Experiments with epigenetic robots, Cognitive Science,            and facilitating conditions of the human smile: A
  30(4), 673-689.                                                   nonobtrusive test of the facial feedback hypothesis.
Chen, M., & Bargh, J. A. (1999). Consequences of                    Journal of Personality and Social Psych, 54(5), 768-777.
  automatic       evaluation:       Immediate       behavioral    Tikhanoff V, Cangelosi A., Fitzpatrick P., Metta G., Natale
  predispositions to approach or avoid the stimulus.                 L., Nori F. (2008). An open-source simulator for
  Personality and Social Psychology Bulletin, 25(2), 215.            cognitive robotics research: The prototype of the iCub
Gallese, V., & Lakoff, G. (2005). The brain’s concepts: The          humanoid robot simulator. In R. Madhavan & E.R.
  role of the sensory-motor system in reason and language.           Messina (Rds.), Proceedings of IEEE Workshop on
  Cognitive Neuropsychology, 22, 455-479.                            Performance Metrics for Intelligent Systems Workshop
Gibson, J. J. (1979). The Ecological Approach to Visual              (PerMIS’08). Washington, D.C.
  Perception. Boston: Houghton Mifflin.                           Topulos, G. P., Lansing, R. W., & Banzett, R. B. (1994).
Jeannerod, M. (1994). The representing brain: Neural                The Experience of Complete Neuromuscular Blockade in
  correlates of motor intention and imagery. Behavioral and         Awake Humans. Survey of Anesthesiology, 38(03), 133.
  Brain Sciences, 17(2), 187-201.                                 Weng J., McClelland J., Pentland A., Sporns O., Stockman
Kohonen, T. (1998). The self-organizing map.                        I., Sur M, Thelen E. (2001). Autonomous mental
  Neurocomputing, 21(1-3), 1-6.                                     development by robots and animals. Science, 291, 599–
Kosslyn, S. M., & Press, M. I. T. (1994). Image and brain:          600.
  The resolution of the imagery debate: MIT Press.
                                                              1367

