UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Interleaving Worked Examples and Cognitive Tutor Support for Algebraic Modeling of
Problem Situations

Permalink
https://escholarship.org/uc/item/8rh5w6cp

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Corbett, Albert
Reed, Stephen
Hoffman, Bob
et al.

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Interleaving Worked Examples and Cognitive Tutor Support for Algebraic
Modeling of Problem Situations.
Albert Corbett (corbett@cmu.edu)
Human-Computer Interaction Institute, Carnegie Mellon University
Pittsburgh, PA 15213 USA

Stephen K. Reed (sreed@sunstroke.sdsu.edu)
Department of Psychology, San Diego State University
San Diego, CA 92182 USA

Bob Hoffmann (bob.hoffman@sdsu.edu)
Department of Educational Technology, San Diego State University
San Diego, CA 92182 USA

Ben MacLaren (maclaren@andrew.cmu.edu)
Human-Computer Interaction Institute, Carnegie Mellon University
Pittsburgh, PA 15213 USA

Angela Wagner (awagner@cmu.edu)
Human-Computer Interaction Institute, Carnegie Mellon University
Pittsburgh, PA 15213 USA

Abstract
Integrating worked examples with problem solving yields
more effective and efficient learning, as does intelligent
tutoring support for problem solving. This study examines the
impact of integrating worked examples and intelligent tutor
support for algebra modeling problems. Students in three
conditions alternately studied worked examples (either static
graphics, interactive graphics or static tables) and solved
Algebra Cognitive Tutor problems. A control group solved all
the problems with the Cognitive Tutor. Students in the four
groups developed equivalent problem-solving skills, but
students learned more efficiently in the interleaved worked
example conditions, requiring 26% less time to complete the
problem set. There were no differences among the four
groups in two measures of robust learning – a retention test
and a transfer test. But students in the static table condition
could more accurately describe what algebraic model
components represent in problem situations than could
students in the other three conditions.
Keywords: Education; Problem solving; Learning;
Classroom Study; Intelligent Tutors; Worked Examples.

Introduction
Extensive research has documented the beneficial impact on
learning of interleaving worked examples with problem
solving (Kalyuga, et al 2001; Pashler, et al, 2007; Sweller
& Cooper, 1985; von Gog, Paas, & Van Merrienboer,
2004). Novices learn more quickly and deeply from a
sequence of problems if they are asked to alternate between
explaining worked-out examples of problem solutions and
solving problems than if they are asked to solve all the
problems in the sequence.

Typically in this research problem solving is supported by
whole-answer feedback. After students complete a problem
solution, whether successfully or not, they are given an
example of a correct solution. This comparison condition is
relatively weak, since step-by-step assistance in problem
solving has been shown to be both more effective (improved
learning outcomes) and more efficient (less learning time to
achieve the same learning outcome) than whole answer
feedback. For instance, Corbett & Anderson (2001)
compared step-by-step feedback and whole-answer
feedback in the Lisp Programming Cognitive Tutor and
found that students in the former condition finished a fixed
set of problems in one-third the time required by those in
the latter condition, and made 40% fewer errors on posttests.
As a result, the question arises whether interleaving
worked examples with problem solving scaffolded by
intelligent tutoring systems might also yield improved
learning outcomes and/or improved learning efficiency.
McLaren, Lim and Koedinger (2008) examined this
question in an intelligent tutor for chemistry problem
solving and found that interleaving worked examples with
problem solving yielded the same learning outcome as the
baseline problem-solving condition, but in less time, thereby
increasing learning efficiency.
Several studies have examined the impact of
incorporating “faded” worked examples into Geometry
Cognitive Tutor (GCT) modules in which students solve
geometry problems and justify each step with a problemsolving principle (Aleven & Koedinger, 2002). In example
fading (Renkl & Atkinson, 2003) the first problem is
presented as a complete worked example, and in successive

2882

problems students complete progressively more steps
themselves until students are finally solving complete
problems. When faded worked examples were incorporated
into GCT, learning was more efficient (students spent less
time to reach the same level of skill) and some evidence was
obtained that the worked-example condition yielded deeper
understanding (Salden, et al, 2008; Schwonke, et al, 2009).
The present study examines the impact of interleaved
worked examples in a Cognitive Tutor (CT) module for
Algebra problem solving. The study has two purposes. First,
the study examines the impact of interleaving worked
examples on students’ learning time, their problem-solving
skill and their depth of understanding. Second, the study
evaluates three alternative types of worked examples: (1)
Static Graphics in which problem components are
represented graphically; (2) Interactive Graphics in which
students participate in constructing the graphical problem
representation; and (3) Static Tables in which problem
components are represented symbolically in a table,
analogous to the problem-solving interface.
This study compares four learning conditions; three
conditions in which each type of worked example is
interleaved with Cognitive Tutor problem solving and a
fourth, Cognitive Tutor problem-solving baseline condition.
The following sections describe the problem solving
domain, the Cognitive Tutor problem-solving environment
and the three types of worked examples.

The Domain: Algebraic Modeling
In this study students are asked to solve “mixture
problems,” for example:
You have an American Express credit card with a
balance of $715 at an 11% interest rate and a Visa
credit card with a 15% interest rate. If you pay a total of
$165 in annual interest, what is the balance on your
Visa card?

[Arithmetic Type 1] You have a MasterCard with a
balance of $532 at a 21% interest rate. You also have a
Visa credit card with a balance of $841 at a 16% interest
rate. How much money are you paying in total interest?
(.21 x $532) + (.16 x $841)=T
[Arithmetic Type 2] Shelly owed $475 in total interest on
her MasterCard and Visa accounts. Her MasterCard
charges 19% interest and her Visa Card charges 22%
interest. She paid the interest on her Visa Card debt of
$1100. How much interest does she still owe on her
MasterCard?
$475 - (.22 x $1100) = M
[Algebra Type 1] You have an American Express credit
card with a balance of $715 at an 11% interest rate and a
Visa credit card with a 15% interest rate. If you pay a
total of $165 in annual interest, what is the balance on
your Visa card?
(.11 x $715) + (.15 x V) = $165
[Algebra Type 2] You have a total balance of $1405 on
two different credit cards— an American Express credit
card with a 12% interest rate and a Discover credit card
with a 24% interest rate. If you owe a total of $224 in
annual interest, what is your balance on the Discover
card?
(.24 x D)+(.12 x [$1405 - D]) = $224
Figure 1: An example problem situation and symbolic
model for each of the four problem types.
table, the student enters an equation to model the situation in
the text cell at the bottom of the screen. The activities were
created with the Cognitive Tutors Authoring Tools (CTAT)
environment (Aleven, et al, in 2009). As in all cognitive
tutors, students received accuracy feedback on each step,
could request advice on any step, and were required to
complete a correct solution to each problem.

The problem-solving goal is to construct a symbolic model
of the situation that can be used to solve the problem, e.g.:
(.11 x $715) + (.15 x V) = $165
The problem-solving curriculum consists of four problem
types: Two types of “arithmetic problems,” in which the
unknown value is naturally represented as an isolated
variable on one side of the equation, and two types of
“algebra problems” in which the unknown quantity is more
naturally represented as a variable that is embedded in one
or in two expressions in the equation. See Figure 1 for an
example of each type.

Cognitive Tutor Problem Solving
Figure 2 displays the interface for the Cognitive Tutor at the
end of a problem. Each problem describes a mixture
scenario and provides a table to scaffold the relationship
between the scenario components and the mathematical
representations of the components. Students enter a number,
variable or operation into each cell. After completing the

Figure 2: The Cognitive Tutor interface at the completion of
a problem.

Worked Examples
Three types of worked examples were developed, in the
Animation Tutor environment (Reed, 2005), each consisting

2883

of multiple successive screens. In each case the first screen
presented a problem statement alone. Successive screens
developed an analysis of the problem’s component structure
in graphical or tabular form.
(1) Static Graphics (SG). Figure 3 shows the final screen
of a static graphics worked example. The first screen
displayed just the problem statement at the top. Students
successively press the Continue arrow to see (1) the first
stack of money which represents an account balance and
interest owed, (2) the second stack of money which
represents the second account balance and interest owed,
and (3) both the third stack, which represents the total
interest, and the symbolic model at the bottom of the screen.

either the bars or the table in the worked examples. The
second principle, minimize cognitive load, is achieved by
presenting the solution in successive segments.

Figure 4: A static table worked example at the completion
of the example.

Predictions

Figure 3: A static graphics worked example at the
completion of the example.
(2) Interactive Graphics (IG). Interactive graphics
worked examples are the same as the SG worked examples,
except that students construct the total interest stack.
Students click on the interest component at the bottom of
each of the other two stacks and drag that component over
to the total interest stack to add up the total interest.
Interactive worked examples were developed for all the
algebra problems and introduced with a single arithmetic
problem. Students in the IG condition viewed static graphic
examples for the other arithmetic problems.
(3) Static Table (ST). Figure 4 displays the final screen of
a static table worked example. As with the graphics
examples, the first screen displays the problem statement
alone. Students successively click the Continue arrow to see
(1) the column labels and first row of the table, which
represents an account balance and interest owed, (2) the
second row of the table which represents the second account
balance and interest owed, and (3) the symbolic model of
the situation beneath the table.
Design Principles. The three types of worked examples
all follow two principles of multimedia design (Sweller,
2003; Mayer 2001; Moreno & Mayer, 2007). The first is the
proximity principle that different media be closely
integrated in space. Verbal explanations are therefore placed
immediately above, and the equation immediately below,

Time and Learning efficiency. Time-on-task in learning is
expected to be less in the worked example conditions than
in the problem-solving condition. Students typically study
worked examples in less time than they can generate
problem solutions, even with intelligent tutoring support
(McLaren, et al, 2008; Salden, et al, 2008; Schwonke, et al,
2009). However, interleaved worked examples are only
more efficient if students in those conditions acquire as
good, or better, problem-solving skills as students in the
problem solving condition.
Robust Learning. There are several reasons to expect that
students may acquire a deeper understanding of problem
solving in the interleaved worked example conditions.
Cognitive Load theory (Sweller, 2003) suggests that worked
examples can eliminate the cognitive load associated with
generating problem solutions, and free up capacity that
students can devote to understanding the solutions. In this
study, all the worked-example conditions describe the
mapping between the mathematical representations and the
problem situations, so students may acquire a better
understanding of the underlying semantics, an
understanding that should support better retention and
transfer to novel problem situations. In addition, the two
graphics conditions may promote better retention than the
other two conditions, since they encourage visual thinking
(Reed, 2010), thereby creating multiple memory codes, both
graphical and symbolic (Mayer, 2001; Paivio, 1986).
Finally, interactive graphics may foster still better retention
than static graphics, since interactively constructing key
quantities in the graphics representation, (Moreno & Meyer,
2007), creates a third, motor code (Engelkamp, 1998;
Glenberg, et al, 2004; Reed, 2006, 2008).

2884

Robust Learning Measures
A problem-solving pretest and posttest were employed to
measure gains in students’ algebra problem-solving skills.
In addition, three “robust learning” tests were employed to
measures students’ depth of understanding.
(1) Retention. A retention test examined students’ arithmetic
and algebra problem solving skills after a one-week interval.
(2) Transfer. A transfer test described “mixture” situations
with novel quantitative structures and asked students to
generate mathematical models of the situations, which also
had novel structures.
(3) Model Description. The Cognitive Tutor Model Analysis
Tool (Corbett, et al, 2000, 2007; Corbett, Wagner & Raspat,
2003) was employed to ask students to explain the structure
of arithmetic and algebraic models. As displayed in Figure
5, each problem presents a problem description and a
mathematical model of the situation. Students select entries
from menus to describe what each hierarchical component
of the symbolic model represents in the problem situation.
As in all Cognitive Tutors, students receive feedback on
each problem step, can request advice on each step, and are
required to complete a correct solution to the problem.

problems and solved the even numbered problems with the
Cognitive Tutor each day. Students in the fourth condition
solved all the problems each day with the Cognitive Tutor.

Learning Materials
Four types of mixture problems were developed, two
“arithmetic” types and two “algebraic” types, as displayed
in Figure 1. Four problems of each type were developed, for
a total of 16 problems. Two problems of each type involved
interest payments on two credit cards, as displayed in the
figures. The other two were mining problems, about
extracting metals from two ores of different quality. The
four problems of each kind were presented in succession,
with the two equivalent interest problems first, followed by
the two equivalent ore problems.

Test Materials

Participants

Four test measures of student learning were developed.
Day-2 Problem-Solving Test. Paper-and-pencil tests were
developed consisting of two problems, equivalent to the two
types of algebra problems students solved with the online
tutor that day. Each problem presented a mixture problem
situation and students were asked to generate an equation to
model the situation. Two test forms were developed and
within each condition, each form served as the pretest for
half the students, who then switched to the other form for
the posttest, so that the pretests and posttests were matched
across the full set of students, but for each student the
pretest and posttest were different.
Day-3 Retention Test. This test consisted of four
problems, equivalent to the four types of problems students
had solved with the online tutor. Again, each problem
presented a mixture problem situation and students were
asked to generate an equation to model the situation.
Day-3 Transfer Test. The Day-3 transfer test consisted of
an arithmetic problem and an algebra problem in which
students were asked to generate symbolic models of
situations with novel structures.
Day-3 Model Component Descriptions. Four Model
Analysis problems were developed. Each problem
corresponded to one of the four problem types students had
solved on the prior two days of the study. Each problem
presented a mixture scenario and presented a symbolic
model of the scenario. Students were asked to describe what
each hierarchical component of the equation represents in
the real-world situation, by selecting entries from menus.

128 students enrolled in Cognitive Tutor Algebra courses in
three Pittsburgh-area high schools participated in the study.

Procedure

Figure 5: The Model Analysis tool partway through a
problem.

Method

Design
The study was completed over the course of three computer
sessions in the students’ Algebra Cognitive Tutor courses.
In the first two sessions, students completed 16 mixture
problems, eight problems per day. The students in each of
the three courses were randomly assigned to one of four
learning conditions. Students in the three worked example
conditions studied example solutions for the odd numbered

In the first session, the online problem solving and worked
example activities were introduced, then students worked
through the eight arithmetic mixture problems. In the second
session, students completed a two-problem paper pretest,
worked through eight algebraic problems, then completed a
two-problem paper posttest. In the third session, which
followed a week later, students completed the four-problem
paper retention test, followed by the two-problem paper
transfer test and finally the four Model Analysis problems.

2885

Results and Discussion
Four students were excluded from the analyses because they
missed the second session and seven others were excluded
for talking to others as they worked on the problems.

Robust Learning

Day-2 Pretest-Posttest Learning Gains
As displayed in Table 1, there were substantial pretestposttest learning gains in all four learning conditions,
averaging 26 percentage points. In an analysis of variance,
this main effect of test type was significant F(1,105) =
52.14, p < .001. There was no significant difference of
learning condition F(3,105) < 1, and no significant
interaction of test type and learning condition F(3,105) < 1.
Table 1: Learning Time per problem for Day 1 and Day 2
(minutes) and Day-2 pretest and posttest accuracy (percent
correct).
Learning
Conditions
CT
IG
SG
ST
Mean

Day 1
Time
2.30
1.52
1.68
1.75
1.81

Day 2
Time
2.15
1.68
1.52
1.72
1.77

Pretest
%correct
7
7
4
8
6

the worked examples and the CT group averaged 2.82 min.
solving those problems. The WE group averaged 2.67 min.
solving the subsequent problems and the CT group averaged
1.50 min. on those problems.

Posttest
% correct
37
28
34
28
32

Learning Efficiency
Table 1 displays average learning time per problem for the
first two sessions. Elapsed time was not measured for the
first worked example in each session (since the environment
did not directly record time), so the first pair of equivalent
problems in each session is excluded from this analysis for
all four groups. In addition, 13 students were excluded from
the Day-1 analysis and 16 students from the Day-2 analysis
because of missing data. While there were no differences in
skill acquisition outcomes among the four conditions,
students in the three interleaved worked example conditions
spent less time in learning, and so learned more efficiently.
Students in the three worked example conditions averaged
28% less time per problem on Day 1 than students in the
problem solving condition (1.65 vs 2.30) and 24% less time
per problem on Day 2 (1.64 vs. 2.15). The main effect of
condition is significant for Day 1, F(3,100) = 6.88, p < .001
and for Day 2, F(3,97) = 6.33, p < .001. Bonferroni
comparisons revealed that the CT group differed from each
one of the three worked example groups both on Day 1 and
on Day 2, p < .02 in each case. The three worked example
groups did not differ from each other.
These average times mask a highly significant Group x
Problem interaction on Day 1, F(3,100) = 93.12, p < .001,
and on Day 2, F(3,97) = 90.19, p < .001. On Day 1 the three
worked example (WE) groups averaged 0.78 min. on the
worked examples, while the CT group averaged 2.98 min.
solving the corresponding problems. The WE groups
averaged 2.53 min. on solving the subsequent equivalent
problems, while the CT group averaged 1.63 min. on those
problems. On Day 2, the WE groups averaged 0.62 min. on

Of the 117 students included in the study, 102 completed
the day 3 robust learning activities. Table 2 displays results
of the three robust learning measures included in the study:
(1) retention of problem-solving skill; (2) transfer of
problem-solving skill; and (3) explanations of symbolic
model components.
Retention Test. Table 2 displays students’ test accuracy on
the one-week retention test of problem-solving skill.
Retention test accuracy did not vary significantly across the
four learning conditions, F(3,90) < 1.
Transfer Test. As can be seen in Table 2, students in the
four learning conditions averaged 17% correct on the
transfer test of problem-solving skill. The main effect of
learning condition was not significant F(3,90) < 1.
Model Component Descriptions. The model analysis task
required students to describe what a total of 31 hierarchical
equation components represented in the four real-world
problem situations. Table 2 displays the average percentage
of these 31 descriptions on which students’ first menu
selection was correct. There was no significant difference
among the groups in an ANOVA, F(3,97) < 1. But the ST
group performed consistently best in describing the model
components, achieving the highest accuracy for 18 of the 31
components (vs. 5 for the IG and SG groups and 3 for the
CT group). This difference is significant in a Friedman twoway ANOVA of rank ordering, χ2(3) = 20.00, p < .001.
Table 2: Day-3 Robust learning measures: Retention,
transfer and model analysis accuracy (percent correct).
Learning
Conditions

Retention
%correct

Transfer
% correct

CT
IG
SG
ST
Mean

32
29
29
26
29

15
18
21
13
17

Model
Analysis
% correct
52
52
53
58
54

Conclusion
The main results confirm earlier conclusions in chemistry
and geometry that incorporating worked examples into
intelligent tutor-supported problem solving can improve
learning efficiency. While students developed similar
problem-solving skills across the four conditions, students
spent 26% less time completing the sixteen problems in the
three interleaved worked-example conditions than in the
problem-solving comparison condition.

2886

However, there is relatively thin evidence that
incorporating worked examples yielded a deeper
understanding of problems solving, as expected by
Cognitive Load theory. Students in the static table worked
example condition demonstrated a better understanding of
the referential semantics that link the mathematical
representations and real-world problem situations than
students in the problem solving condition. However, this
deeper knowledge did not support greater problem solving
accuracy, retention or transfer. Students in the two graphics
worked example conditions also did not show more robust
learning than students in the problem solving condition.

Acknowledgments
This project was funded by the Pittsburgh Science of
Learning Center, NSF awards SBE-0354420 and SBE0836012.

References
Aleven, V. & Koedinger, K.R. (2002). An effective
metacognitive strategy: Learning by doing and explaining
with a computer-based Cognitive Tutor. Cognitive
Science, 26, 147-179.
Aleven, V., McLaren, B.M., Sewall, J., & Koedinger, K.R.
(2009). A new paradigm for intelligent tutoring systems:
Example-tracing tutors. International Journal of Artificial
Intelligence in Education, 19, 105-154.
Corbett, A.T. & Anderson, J.R. (2001). Locus of feedback
control in computer-based tutoring: Impact on learning
rate, achievement and attitudes. In J. Jacko, A. Sears, M.
Beaudouin-Lafon & R. Jacob (Eds.), Proceedings of ACM
CHI’2001 Conference on Human Factors in Computing
Systems (pp, 245-252).
Corbett, A., McLaughlin, M., Scarpinatto, K. C., & Hadley,
W. (2000). Analyzing and generating mathematical
models: An Algebra II Cognitive Tutor design study. In
G. Gauthier, C. Frasson & K. VanLehn (Eds.), Intelligent
tutoring systems: 5th international conference (pp. 314323). New York: Springer.
Corbett, A.T., Wagner, A., Lesgold, S., Ulrich, H. &
Stevens, S. (2007). Modeling students’ natural language
explanations. UM2007 User Modeling: Proceedings of
the Eleventh International Conference, 117-126.
Corbett, A., Wagner, A., & Raspat, J. (2003). The impact of
analysing example solutions on problelm solving in a prealgebra tutor. Paper presented at the AIED 2003: The
11th International Conference on Artificial Intelligence
and Education.
Engelkamp, J. (1998). Memory for actions. Hove, England:
Psychology Press.
Glenberg, A. M., Gutierrez, T., Levin, J. R., Japuntich, S., &
Kaschak, M. P. (2004). Activity and imagined activity
can enhance young children's reading comprehension.
Journal of Educational Psychology, 96, 424-436.
Kalyuga, S., Chandler, P., Tuovinen, J., & Sweller, J.
(2001). When problem solving is superior to studying

worked examples. Journal of Educational Psychology,
93, 579-588.
Mayer, R. E. (2001). Multimedia learning. Cambridge:
Cambridge University Press.
McLaren, B. M., Lim, S.-J., & Koedinger, K. R. (2008).
When is assistance helpful to learning? Results in
combining worked examples and intelligent tutoring.
Paper presented at the Proceedings of the 9th
International Conference on Intelligent Tutoring Systems.
Moreno, R., & Mayer, R. E. (2007). Interactive multimodal
learning environments. Educational Psychology Review,
19, 309-326.
Paivio, A. (1986). Mental representations: A dual coding
approach. New York: Oxford University Press.
Pashler, H., Bain, P., Bottge, B., Graesser, A., Koedinger,
K., McDaniel, M., & Metcalfe, J. (2007). Organizing
Instruction and Study to Improve Student Learning.
Washington, DC: National Center for Education
Research, Institute of Education Sciences, U.S.
Department of Education.
Reed, S. K. (2005). From research to practice and back: The
Animation Tutor project. Educational Psychology
Review, 17, 55-82.
Reed, S. K. (2006). Cognitive architectures for multimedia
learning. Educational Psychologist, 41, 87-98.
Reed, S. K. (2008). Manipulating multimedia materials. In
R. Zheng (Ed.), Cognitive effects of multimedia learning
(pp. 51-66). New York: IGI Global.
Reed, S. K. (2010). Thinking Visually. New York: Taylor &
Francis.
Renkl, A. & Atkinson, R. K. (2003). Structuring the
transition from example study to problem solving in
cognitive skills acquisition: A cognitive load perspective.
Educational Psychologist, 38, 15-22.
Salden, R., Aleven, V., Renkl, A., & Schwonke, R. (2008).
Worked examples and tutored problem solving:
Redundant or synergistic forms of support? In B. C. Love,
K. McRae, & V. M. Sloutsky (Eds.), Proceedings of the
30th Annual Conference of the Cognitive Science Society
(pp. 589-594).
Schwonke, R., Renkl, A., Krieg, C., Wittwe, J., Aleven, V.,
& Salden, R. (2009). The worked-example effect: Not an
artifact of lousy control conditions. Computers in Human
Behavior, 25, 258-266.
Sweller, J. (2003). Evolution of human cognitive
architecture. In B. Ross (Ed.), The psychology of learning
and motivation (Vol. 43, pp. 215-266). San Diego:
Academic Press.
Sweller, J. & Cooper, G.A. (1985). The use of worked
examples as a substitute for problem solving in learning
algebra. Cognition and Instruction, 1985, 2, 59-89.
von Gog, T., Paas, F., & Van Merrienboer, J. J. G. (2004).
Process-oriented worked examples: Improving transfer
performance
through
enhanced
understanding.
Instructional Science, 32, 83-98.

2887

