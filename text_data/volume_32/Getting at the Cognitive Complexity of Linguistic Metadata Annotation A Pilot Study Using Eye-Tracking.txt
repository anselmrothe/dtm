UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Getting at the Cognitive Complexity of Linguistic Metadata Annotation: A Pilot Study
Using Eye-Tracking
Permalink
https://escholarship.org/uc/item/88h8d92b
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Lohman, Steffen
Tomanek, Katrin
Ziegler, Jurgen
et al.
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                Powered by the California Digital Library
                                                                 University of California

         Getting at the Cognitive Complexity of Linguistic Metadata Annotation –
                                             A Pilot Study Using Eye-Tracking
       Steffen Lohmann                         Katrin Tomanek                    Jürgen Ziegler                      Udo Hahn
Dept. of Computer Science &              Language & Information          Dept. of Computer Science &          Language & Information
 Applied Cognitive Science               Engineering (JULIE) Lab           Applied Cognitive Science          Engineering (JULIE) Lab
 Universität Duisburg-Essen                   Universität Jena          Universität Duisburg-Essen              Universität Jena
      Duisburg, Germany                        Jena, Germany                   Duisburg, Germany                    Jena, Germany
                               Abstract                                   with each other, or with standard random sampling, in terms
                                                                          of annotation efficiency the AL community, up until now, as-
   We report on an experiment where the decision behavior of
   annotators issuing linguistic metadata is observed with an eye-        sumed uniform annotation costs for each linguistic unit, e.g.,
   tracking device. As experimental conditions we consider the            words (Ringger et al., 2008 ; Settles, Craven, & Friedland,
   role of textual context and linguistic complexity classes. Still       2008 ; Arora, Nyberg, & Rosé, 2009). This claim, however,
   preliminary in nature, our data suggests that semantic com-
   plexity is much harder to deal with than syntactic one, and            has been shown to be invalid in several studies (Hachey, Alex,
   that full-scale textual context is negligible for annotation, with     & Becker, 2005 ; Settles et al., 2008 ; Tomanek & Hahn,
   the exception of semantic high-complexity cases. We claim              2010). If uniformity does not hold and, hence, the number of
   that such observational data might lay the foundation for em-
   pirically grounded annotation cost models and the design of            annotated units does not indicate the true annotation efforts
   cognitively adequate annotation user interfaces.                       required for a specific sample, empirically more adequate cost
   Keywords: Natural Language Metadata Annotation; Annota-                models have to be developed. Accordingly, we here consider
   tion Behavior; Eye-Tracking; Syntactic Complexity; Semantic            different classes of syntactic and semantic complexity that
   Complexity; Cognitive Cost Modeling
                                                                          might affect the cognitive load during the annotation process,
                                                                          with the overall goal to find empirically more adequate vari-
                           Introduction                                   ables for cost modeling.
Supervised approaches to machine learning (ML) are cur-
                                                                             The complexity of linguistic utterances can be judged ei-
rently very popular in the natural language processing (NLP)
                                                                          ther by structural or by behavioral criteria. Structural com-
community. While linguistic regularities are no longer hand-
                                                                          plexity emerges, e.g., from the static topology of phrase
crafted by human experts in this paradigm, human interven-
                                                                          structure trees and procedural graph traversals exploiting the
tion is still required to produce sufficient amounts of reliably
                                                                          topology of parse trees (see Szmrecsányi (2004) or Cheung
annotated training material from which ML classifiers may
                                                                          et Kemper (1992) for a survey of metrics of this type). How-
learn or, considered as empirically valid ground truth, against
                                                                          ever, structural complexity criteria do not translate directly
which NLP systems can be evaluated.
                                                                          into empirically justified cost measures.
   The assignment of linguistic metadata (e.g., related to parts
of speech, syntactic parses, or semantic interpretations) to                 The behavioral approach accounts for this problem as it
plain natural language corpus data, a process called annota-              renders observational data of the annotators’ eye movements.
tion, is a complex cognitive task. It requires a sound compe-             The technical vehicle to gather such data are eye-trackers
tence of the natural language in the corpus, as well as a decent          which have already been used in psycholinguistics (Rayner,
level of domain and even text genre expertise.                            1998). Eye-trackers were able to reveal, e.g., how subjects
   Meanwhile lots of annotated corpora have been built which              deal with ambiguities (Frazier & Rayner, 1987 ; Rayner,
contain these precious human judgments (e.g., PennTreeBank                Cook, Juhas, & Frazier, 2006 ; Traxler & Frazier, 2008) or
(Marcus, Santorini, & Marcinkiewicz, 1993), PennPropBank                  with sentences requiring re-analysis, so-called garden path
(Palmer, Gildea, & Kingsbury, 2005) or OntoNotes (Pradhan                 sentences (Altmann, Garnham, & Dennis, 2007 ; Sturt, 2007).
et al., 2007)). Almost all of these annotated corpora were                   The rationale behind the use of eye-tracking devices for
assembled by collecting the documents to be annotated on a                the observation of the annotation behavior is that the length
random sampling basis (once the original document set had                 of gaze durations and the behavioral patterns underlying gaze
been restricted thematically or chronologically).                         movements are considered to be indicative for the hardness
   Only recently, more sophisticated approaches to select the             of the linguistic analysis and the expenditures for the search
annotation material are being investigated in the NLP com-                of clarifying linguistic evidence (e.g., anchor words) to solve
munity. One of the most promising approaches is known as                  hard decision tasks such as phrasal attachments or word sense
Active Learning (AL) (Cohn, Ghahramani, & Jordan, 1996 ;                  disambiguation. Gaze duration and search time are then taken
Tomanek, Wermter, & Hahn, 2007) where an intentional se-                  as empirical correlates of processing complexity and, hence,
lection bias is enforced and only those linguistic samples are            unveil the real costs. We therefore consider eye-tracking as a
selected from the entire document collection which are con-               promising means to get a better understanding of the nature
sidered to be most informative to learn an effective classifica-          of linguistic annotation processes with the ultimate goal of
tion model. When different approaches to AL are compared                  identifying predictive factors for annotation cost models.
                                                                      2146

    [Federal Aviation Adminstration]ORG investigators were to examine the aircraft, said spokeswoman [Arlene]PER . She said
    [Martinair Holland]ORG is certified to fly large jet aircraft into the [US]LOC as a scheduled passenger service.
    When the [Cessna]ORG took off in rain and snow from the 6,900-foot runway at [Cheyenne Municipal Airport]LOC in
    [Wyoming]LOC , [Reid]PER was seated at one control panel, [Jessica]PER was seated at another and her father was in a
    passenger seat in a four-seat [Cessna]ORG 177B, a 21-year-old single-engine plane owned by [Reid]PER .
    Figure 1: Text snippets taken from M UC 7 documents annotated by LOCation, PERson, and ORGanization entity types.
                   Experimental Design                                orthographic signals are by no means a sufficient condition
                                                                      for the presence of a named entity mention within a CNP.
The focus of our study is on semantic annotation, the anno-
                                                                         The choice of CNPs as stimulus phrases is motivated by
tation of named entity mentions in particular. In this task, a
                                                                      the fact that named entities are usually fully encoded by this
human annotator has to decide for each word in a sentence
                                                                      kind of linguistic structure. The chosen stimulus – an anno-
whether it belongs to one of the entity types of interest or
                                                                      tation example with one phrase highlighted for annotation –
not. For the first time ever to the best of our knowledge, we
                                                                      allows for an exact localization of the cognitive processes and
applied eye-tracking to study the cognitive processes under-
                                                                      annotation actions performed relative to that specific phrase.
lying the annotation of linguistic metadata.
   We used the English part of the M UC 7 corpus (Linguistic          Independent Variables
Data Consortium, 2001) for our study, which contains New              We defined two measures for the complexity of the annotation
York Times articles from the year 1996 reporting on plane             examples: The syntactic complexity was given by the num-
crashes. These articles come already annotated with three             ber of nodes in the parse tree dominated by the annotation
types of named entities considered important in the newspa-           phrase (Szmrecsányi, 2004).1 According to a threshold on
per domain, viz. “persons”, “locations”, and “organizations”.         the number of nodes in such a parse tree, we classified CNPs
Figure 1 depicts typical text snippets from these articles along      as having either high or low syntactic complexity.
with the available annotations.                                          The semantic complexity of an annotation example is
   Annotation of these entity types in newspaper articles is          based on the inverse document frequency df (wi ) of each word
admittedly fairly easy. We chose this rather simple setting           wi of the respective CNP according to a reference corpus.2
because the participants in the experiment had no previous                                                                            1
                                                                      We calculated the semantic complexity score as maxi df (w         i)
                                                                                                                                           ,
experience with document annotation and no serious linguis-           where wi is the i-th word of the annotation phrase. Again,
tic education background. Moreover, the limited number of             we determined a threshold classifying CNPs as having either
entity types reduced the amount of participants’ training prior       high or low semantic complexity. This automatically gener-
to the actual experiment, and positively affected the design          ated classification was then manually checked and, if neces-
and handling of the experimental apparatus (see below).               sary, revised by two annotation experts. For instance, if an
   We triggered the annotation processes by giving our par-           annotation phrase contained a strong trigger (e.g., a social
ticipants specific annotation examples. An example con-               role or job title as with “spokeswoman” in “spokeswoman
sists of a text document having one single annotation phrase          Arlene”; cf. Figure 1), it was classified as a low-semantic-
highlighted which then had to be semantically annotated for           complexity item even though it was assigned a high inverse
named entity mentions. The annotation task was defined such           document frequency due to the infrequent word “Arlene”.
that the correct entity type had to be assigned to each word in          Two experimental groups were formed to study different
the annotation phrase. If a word belongs to none of the three         kinds of textual context. In the document context condition
entity types a fourth class, “no entity”, had to be assigned.         the whole newspaper article was shown as annotation exam-
   The phrases highlighted for annotation were complex noun           ple, while in the sentence context condition only the sentence
phrases (CNPs), each a sequence of words where a noun (or             containing the annotation phrase was presented. The partic-
an equivalent nominal expression) constitutes the syntactic           ipants3 were randomly assigned to one of these groups. We
head and thus dominates dependent words such as determin-
                                                                          1 Constituency parse trees were generated using the OpenNLP
ers, adjectives, or other nouns or nominal expressions (includ-
                                                                      TreeBank parser (http://opennlp.sourceforge.net/).
ing noun phrases and prepositional phrases). CNPs with even               2 We chose the English part of the Reuters RCV2 corpus, a col-
more elaborate internal syntactic structures, such as coordina-       lection of over 400,000 news stories from 1996 and 1997, as the
tions, appositions, or relative clauses, were isolated from their     reference corpus for our experiments.
                                                                          3 20 subjects (12 female) with an average age of 24 years (mean
syntactic host structure and the intervening linguistic mate-
                                                                      = 24, standard deviation (SD) = 2.8) and normal or corrected-to-
rial containing these structures was deleted to simplify overly       normal vision capabilities took part in the study. All participants
long sentences. We also discarded all CNPs that did not con-          were students with a computing-related study background, with
tain at least one entity-critical word, i.e., one which might be      good to very good English language skills (mean = 7.9, SD = 1.2,
                                                                      on a ten-point scale with 1 = “poor” and 10 = “excellent”, self-
a named entity given its orthographic appearance (e.g., start-        assessed), but without any prior experience in annotation practice
ing with an upper-case letter). It should be noted that such          and without previous exposure to academic linguistic education.
                                                                  2147

                         text        above            before           phrase            after         below
     Figure 2: Subareas for the eyetracking analysis. Annotation example is of low semantic and low syntactic complexity.
decided for this between-subjects design to avoid any irrita-        Stimulus Material
tion of the participants caused by constantly changing con-          According to the above definition of complexity, we auto-
texts. Accordingly, the participants were assigned to one of         matically preselected annotation examples characterized by
the experimental groups and corresponding context condition          either a low or a high degree of semantic and syntactic com-
already in the second training phase that took place shortly         plexity. After manual fine-tuning of the example set assuring
before the experiment started (see below).                           an even distribution of entity types and syntactic correctness
Hypotheses and Dependent Variables                                   of the automatically derived annotation phrases, we finally
                                                                     selected 80 annotation examples for the experiment. These
We tested the following two hypotheses:                              were divided into four subsets of 20 examples each falling
Hypothesis H1: Annotators perform differently in the two             into one of the following complexity classes:
   context conditions.                                                   sem-syn        low semantic – low syntactic complexity
   H1 is based on the linguistically plausible assumption that           SEM-syn        high semantic – low syntactic complexity
   annotators are expected to make heavy use of the sur-                 sem-SYN        low semantic – high syntactic complexity
   rounding context because such context could be helpful for            SEM-SYN        high semantic – high syntactic complexity
   the correct disambiguation or de-anaphorization of entity         Experimental Apparatus and Procedure
   classes. Accordingly, lacking context, an annotator is ex-
   pected to annotate worse than under the condition of full         The annotation examples were presented in a custom-built
   context. As an adverse effect, the availability of (too much)     tool and its user interface was kept as simple as possible not
   context might overload and so distract annotators, with a         to distract the eye movements of the participants. It merely
   potentially negative effect on annotation performance.            contained one frame showing the text of the annotation exam-
                                                                     ple, with the annotation phrase highlighted (as with “Mark
Hypothesis H2: Annotators’ performance is different for              Munhall” in Figure 2). A blank screen was shown after
   varying levels of syntactic and semantic complexity.              each annotation example to reset the eyes and to allow for
   The assumption is that high syntactic or semantic complex-        a break, if needed. The time the blank screen was shown was
   ity, in contrast to low complexity, for both complexity types     not counted as annotation time. The 80 annotation examples
   significantly lowers the annotation performance.                  were presented to all participants in the same randomized or-
                                                                     der, with a balanced distribution of the complexity classes. A
In order to test these hypotheses we collected data for the fol-     variation of the order was hardly possible for technical and
lowing dependent variables: (a) the annotation accuracy – we         analytical reasons but is not considered as a drawback due to
identified erroneous entities by comparison with the original        extensive, pre-experimental training (see below). The limita-
gold annotations in the M UC 7 corpus, (b) the time needed per       tion to 80 annotation examples reduced the chances of errors
annotation example, and (c) the distribution and duration of         due to fatigue or lack of attention that can be observed in
the participants’ eye gazes.                                         long-lasting annotation sessions.
                                                                 2148

               subareas                                                         above     left   phrase    right    below
               percentage of participants looking at a subarea                   35        32     100       34         16
               average number of fixations in a subarea per participant          2.2               14.1               1.3
                      Table 1: Distribution of annotators’ attention among sub-areas per annotation example.
   Five introductory examples (not considered in the final           Testing Context Conditions
evaluation) were given to get the subjects used to the exper-
imental environment. All annotation examples were chosen             To test hypothesis H1 we compared the number of annota-
in a way that they completely fitted on the screen (text length      tion errors on entity-critical words made by the annotators in
was limited) to avoid the need for scrolling and thus eye dis-       the two contextual conditions (complete document vs. sen-
traction. The contextual position of the CNP was randomly            tence). Surprisingly, on the total of 174 entity-critical words
distributed, excluding the first and last sentence.                  within the 80 annotation examples, we found exactly the same
   The participants used a standard keyboard to assign the en-       mean value of 30.8 errors per participant in both conditions.
tity types for each word of the annotation example. All but          There were also no significant differences on the average time
5 keys were removed from the keyboard to avoid extra eye             needed to annotate an example in both conditions (means
movements for finger coordination (three keys for the posi-          of 9.2 and 8.6 seconds, respectively, with F(1, 18) = 0.116,
tive entity classes, one for the “no entity” class, and one to       p = 0.74).4 These results seem to suggest that it makes no dif-
confirm the annotation). Pre-tests had shown that the partici-       ference (neither for annotation accuracy nor for time) whether
pants could easily issue the annotations without looking down        or not annotators are shown textual context that contains the
at the keyboard.                                                     annotation phrase beyond the sentence.
   We recorded each participant’s eye movements on a Tobii              To further investigate this finding we analyzed the eye-
T60 eyetracking device which is invisibly embedded in a 17”          tracking data of the participants gathered for the document
TFT monitor and comparatively tolerant to head movements.            context condition. We divided the whole text area into sev-
The participants were seated in a comfortable position with          eral subareas as shown in Figure 2. We then determined the
their head in a distance of 60-70 cm from the monitor. Screen        average proportion of participants that directed their gaze at
resolution was set to 1280 x 1024 px and the annotation ex-          least once at these subareas. We considered all fixations with
amples were presented in the middle of the screen in a font          a minimum duration of 100 ms, using a fixation radius (i.e.,
size of 16 px and a line spacing of 5 px. The presentation           the smallest distance that separates fixations) of 30 px and ex-
area had no fixed height and varied depending on the context         cluded the first second as it was mainly used for orientation
condition and length of the newspaper article. The text was          and identification of the annotation phrase.
always vertically centered on the screen.                               Table 1 reveals that on average only 35% of the participants
   All participants were familiarized with the annotation task       looked in the textual context above the annotation phrase em-
and the guidelines in a pre-experimental workshop where              bedding sentence, and even less perceived the context below
about 60 minutes were spent on annotation exercises. During          (16%). The sentence parts before and after the annotation
the next two days, the actual experiments were conducted,            phrase were, on the average, visited by one third (32% and
each one lasting between 15 and 30 minutes, including cali-          34%, respectively) of the participants. The uneven distribu-
bration of the eye-tracking device. Another 20-30 minutes of         tion of the annotators’ attention becomes even more apparent
training time directly preceded each individual experiment.          in a comparison of the total number of fixations on the dif-
After the experiment, the participants were interviewed and          ferent text parts (see Table 1): 14 out of an average of 18
asked to fill out a questionnaire. Overall, the experiment took      fixations per example were directed at the annotation phrase
about two hours for each participant for which they were fi-         and the surrounding sentence, the text context above the an-
nancially compensated. The participants were also instructed         notation chunk received only 2.2 fixations on the average and
to focus more on annotation accuracy than on annotation time         the text context below only 1.3.
as we wanted to avoid random guessing. Accordingly, as an               Thus, eye-tracking data indicates that the textual context is
extra incentive, we rewarded the three participants with the         not as important as might have been expected for quick and
highest annotation accuracy with cinema vouchers. None               accurate annotation. This result can be explained by the fact
of the participants reported serious difficulties with either        that participants of the document-context condition used the
the newspaper articles or the annotation tool and all subjects       context whenever they thought it might help, whereas par-
agreed that they understood the annotation task very well.           ticipants of the sentence-context condition spent more time
                                                                     thinking about a correct answer, overall with the same result.
                            Results
We used a mixed-design analysis of variance (ANOVA)                      4 In general, we observed a high variance in the number of errors
model to test the hypotheses, with the context condition as          and time values between the subjects. While, e.g., the fastest partic-
                                                                     ipant handled an example in 3.6 seconds on the average, the slowest
between-subjects factor and the two complexity classes as            one needed 18.9 seconds; concerning the annotation errors on the
within-subject factors.                                              174 entity-critical words, these ranged between 21 and 46 errors.
                                                                2149

                               experimental      complexity       e.-c.        time                errors
                                 condition           class       words     mean SD          mean     SD      rate
                                                   sem-syn         36       4.0s 2.0         2.7     2.1    .075
                                 document          SEM-syn         25       9.2s 6.7         5.1     1.4    .204
                                 condition        sem-SYN          51       9.6s 4.0         9.1     2.9    .178
                                                  SEM-SYN          62      14.2s 9.5        13.9     4.5    .224
                                                   sem-syn         36       3.9s 1.3         1.1     1.4    .031
                                  sentence         SEM-syn         25       7.5s 2.8         6.2     1.9    .248
                                 condition        sem-SYN          51       9.6s 2.8         9.0     3.9    .176
                                                  SEM-SYN          62      13.5s 5.0        14.5     3.4    .234
Table 2: Average performance values for the 10 subjects of each experimental condition and 20 annotation examples of each
complexity class: number of entity-critical (e.-c.) words, mean annotation time and standard deviations (SD), mean annotation
errors, standard deviations, and error rates (number of errors divided by number of entity-critical words).
Testing Complexity Classes                                                    complexity      fixation on phrase     fixation on context
To test hypothesis H2 we also compared the average anno-                          class       mean         SD        mean         SD
tation time and the number of errors on entity-critical words                   sem-syn          4.9       4.0         1.0        2.9
for the complexity subsets (see Table 2). The ANOVA results                    SEM-syn           8.1       5.4         5.6        5.6
show highly significant differences for both annotation time                   sem-SYN         18.1        7.7         1.5        2.0
and errors.5 A pairwise comparison of all subsets in both                     SEM-SYN          25.4        9.3         5.0        4.1
conditions with repeated t-test measurements showed non-
significant results only between the SEM-syn and syn-SEM                  Table 3: Average number of fixations on the annotation
subsets.6 Thus, the empirical data generally supports hypoth-             phrase and context for the document condition and 20 anno-
esis H2 in that the annotation performance seems to correlate             tation examples of each complexity class.
with the complexity of the annotation phrase, on the average.
                                                                          a plot for one participant which illustrates a scanning-
Context and Complexity                                                    for-coreference behavior we observed for many annotation
We also examined whether the need for inspecting the con-                 phrases with high semantic complexity. Words were searched
text increases with the complexity of the annotation phrase.              in the upper context, which according to their orthographic
So we analyzed the eye-tracking data in terms of the average              appearance might refer to a named entity, but which could not
number of fixations on the annotation phrase and on its em-               fully be resolved only relying on the information given by the
bedding contexts for each complexity class (see Table 3). The             annotation phrase itself and its embedding sentence. This is
values illustrate that while the number of fixations on the an-           the case for “Roselawn” in the annotation phrase “Roselawn
notation phrase rises generally with both the semantic and the            accident”. The context reveals that Roselawn, which also oc-
syntactic complexity, the number of fixations on the context              curs in the first sentence, is a location. A similar procedure is
rises only with semantic complexity. The number of fixations              also performed for acronyms and abbreviations which cannot
on the context is nearly the same for the two subsets with low            be resolved from the immediate local context. As indicated
semantic complexity (sem-syn and sem-SYN, with 1.0 and                    by the gaze movements, it also became apparent that texts
1.5), while it is significantly higher for the two subsets with           were rather scanned for hints instead of being deeply read.
high semantic complexity (5.6 and 5.0), independent of the
                                                                                         Summary and Conclusions
syntactic complexity.7 These results suggest that the need for
context mainly depends on the semantic complexity of the                  We explored the use of eye-tracking technology to investi-
annotation phrase, while it is less influenced by its syntactic           gate the behavior of human annotators during the assignment
complexity.                                                               of three types of named entities – persons, organizations and
   This finding is qualitatively supported by gaze plots we               locations – based on the eye-mind assumption. We tested two
generated from the eye-tracking data. Figure 3 shows such                 main hypotheses: one relating to the amount of contextual in-
                                                                          formation being used for annotation decisions, the other relat-
    5 Annotation time results: F(1, 18) = 25, p < 0.01 for semantic
                                                                          ing to different degrees of syntactic and semantic complexity
complexity and F(1, 18) = 76.5, p < 0.01 for syntactic complexity;        of expressions that had to be annotated. We found experimen-
Annotation complexity results: F(1, 18) = 48.7, p < 0.01 for se-
mantic complexity and F(1, 18) = 184, p < 0.01 for syntactic com-         tal evidence that the textual context is searched for decision
plexity.                                                                  making on assigning semantic meta-data at a surprisingly low
    6 t(9) = 0.27, p = 0.79 for the annotation errors in the document
                                                                          rate (with the exception of tackling high-complexity semantic
context condition, and t(9) = 1.97, p = 0.08 for the annotation time
in the sentence context condition.                                        cases and resolving co-references) and that annotation perfor-
    7 ANOVA result of F(1, 19) = 19.7, p < 0, 01 and significant dif-     mance highly correlates with semantic complexity and to a
ferences also in all pairwise comparisons.                                lesser degree with syntactic complexity.
                                                                      2150

Figure 3: Annotation example with annotation phrase and the antecedent for “Roselawn” in the text (left), and gaze plot of one
participant showing a scanning-for-coreference behavior (right).
   The results of these experiments can be taken as a heuris-        Rayner, K. (1998). Eye movements in reading and informa-
tic clue to focus on cognitively plausible features of learning             tion processing: 20 years of research. Psychological
empirically rooted cost models for annotation (see Tomanek,                 Bulletin, 126, 372–422.
Lohmann, Ziegler, et Hahn (2010) for more details).                  Rayner, K., Cook, A., Juhas, z. B., & Frazier, L. (2006). Im-
                                                                            mediate disambiguation of lexically ambiguous words
                          Références                                      during reading: Evidence from eye movements. British
Altmann, G., Garnham, A., & Dennis, Y. (2007). Avoiding                     Journal of Psychology, 97, 467–482.
       the garden path: Eye movements in context. Journal of         Ringger, E., Carmen, M., Haertel, R., Seppi, K., Lonsdale,
       Memory and Language, 31(2), 685–712.                                 D., McClanahan, P., et al. (2008). Assessing the costs
Arora, S., Nyberg, E., & Rosé, C. (2009). Estimating annota-               of machine-assisted corpus annotation through a user
       tion cost for active learning in a multi-annotator envi-             study. In LREC 2008 – 6th International Conference on
       ronment. In NAACL HLT Workshop on Active Learning                    Language Resources and Evaluation (pp. 3318–3324).
       for Natural Language Processing (pp. 18–26).                  Settles, B., Craven, M., & Friedland, L. (2008). Active learn-
Cheung, H., & Kemper, S. (1992). Competing complexity                       ing with real annotation costs. In NIPS 2008 Workshop
       metrics and adults’ production of complex sentences.                 on Cost-Sensitive Machine Learning (pp. 1–10).
       Applied Psycholinguistics, 13, 53–76.                         Sturt, P. (2007). Semantic re-interpretation and garden path
Cohn, D., Ghahramani, Z., & Jordan, M. (1996). Active                       recovery. Cognition, 105, 477–488.
       learning with statistical models. Journal of Artificial       Szmrecsányi, B. M. (2004). On operationalizing syntactic
       Intelligence Research, 4, 129–145.                                   complexity. In JADT 2004 – 7th International Conf. on
Frazier, L., & Rayner, K. (1987). Resolution of syntactic                   Textual Data Statistical Analysis (pp. 1032–1039).
       category ambiguities: Eye movements in parsing lex-           Tomanek, K., & Hahn, U. (2010). Annotation time stamps:
       ically ambiguous sentences. Journal of Memory and                    Temporal metadata from the linguistic annotation pro-
       Language, 26, 505–526.                                               cess. In LREC 2010 – 7th International Conference on
Hachey, B., Alex, B., & Becker, M. (2005). Investigating                    Language Resources and Evaluation.
       the effects of selective sampling on the annotation task.     Tomanek, K., Lohmann, S., Ziegler, J., & Hahn, U. (2010).
       In CoNLL 2005 – 9th Conference on Computational                      A cognitive cost model of annotations based on eye-
       Natural Language Learning (pp. 144–151).                             tracking data. In ACL 2010 – 48th Annual Meeting of
Linguistic Data Consortium. (2001). Message Understanding                   the Association for Computational Linguistics.
       Conference (MUC) 7.                                           Tomanek, K., Wermter, J., & Hahn, U. (2007). An Approach
Marcus, M., Santorini, B., & Marcinkiewicz, M. (1993).                      to Text Corpus Construction which Cuts Annotation
       Building a large annotated corpus of English: P ENN                  Costs and Maintains Reusability of Annotated Data. In
       T REEBANK. Computational Linguistics, 19, 313–330.                   EMNLP/CoNLL 2007 – Joint Conference on Empirical
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The Propo-                  Methods in Natural Language Processing and Compu-
       sition Bank: An annotated corpus of semantic roles.                  tational Natural Language Learning (pp. 486–495).
       Computational Linguistics, 31(1), 71–106.                     Traxler, M., & Frazier, L. (2008). The role of pragmatic prin-
Pradhan, S., Hovy, E., Marcus, M., Palmer, M., Ramshaw, L.,                 ciples in resolving attachment ambiguities: Evidence
       & Weischedel, R. (2007). Ontonotes: A unified rela-                  from eye movements. Memory & Cognition, 36, 314–
       tional semantic representation. In ICSC 2007 – Inter-                328.
       national Conf. on Semantic Computing (pp. 517–526).
                                                                 2151

