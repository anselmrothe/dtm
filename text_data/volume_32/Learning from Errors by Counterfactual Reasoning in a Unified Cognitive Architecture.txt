UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning from Errors by Counterfactual Reasoning in a Unified Cognitive Architecture
Permalink
https://escholarship.org/uc/item/7288c0sz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Danielescu, Andreea
Stracuzzi, David J.
Li, Nan
et al.
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                   Learning from Errors by Counterfactual Reasoning
                                 in a Unified Cognitive Architecture
                                  Andreea Danielescu (lavinia.danielescu@asu.edu)
                                  David J. Stracuzzi (david.stracuzzi@gmail.com)
                                            Nan Li (nanan9177@gmail.com)
                                             Pat Langley (langley@asu.edu)
                                             Computing Science and Engineering
                                     Arizona State University, Tempe, AZ 85287 USA
                          Abstract                               using and acquiring them, provide Icarus with basic
                                                                 support for benefiting from undesirable outcomes. Our
   A key characteristic of human cognition is the ability to
   learn from undesirable outcomes. This paper presents a        approach to learning from errors responds to a single
   computational account of learning from errors based on        negative experience, which distinguishes it from connec-
   counterfactual reasoning, which we embed in Icarus,           tionist, reinforcement-based, and Bayesian techniques,
   a unified theory of the cognitive architecture. Our ap-
   proach acquires new skills from single experiences that       which typically require many experiences.
   improve upon and mask those that initially produced              We begin our discussion with a motivating task do-
   the undesirable behavior. We demonstrate the opera-           main and a review of the Icarus architecture. After
   tion of this model in a simulated urban driving environ-
   ment. We also relate our approach to other research on        this, we present our approach to learning from errors via
   error-driven learning and discuss possible improvements       counterfactual reasoning, including methods for deter-
   to the framework.                                             mining the source of the error, acquiring new concepts
   Keywords: cognitive architecture, learning from error,        and skills in response, and utilizing these structures in
   counterfactual reasoning, problem solving
                                                                 future behavior. We then describe the extended architec-
                                                                 ture’s operation in the task domain, discuss connections
           Background and Motivation                             to other work on error-driven learning, and consider di-
The ability to acquire knowledge from experience is a            rections for further research in this important area.
fundamental component of human intelligence. There
exist many accounts of learning from positive experi-              An Illustrative Domain: Urban Driving
ences, most often based on successful problem-solving            In modern society, the task of operating a vehicle in an
attempts (Anzai & Simon, 1979; Laird, Rosenbloom, &              urban setting is both common and cognitively challeng-
Newell, 1986). In this paper, we focus instead on learn-         ing. People perform a variety of tasks in this context,
ing from undesirable outcomes, an ability that plays an          such as navigation, obstacle avoidance, and signal re-
important role in human cognition by providing a mech-           sponse, along with higher-level tasks such as package
anism for avoiding past failures in the future. We pro-          delivery. Successful performance relies on substantial do-
vide a computational model for one type of error-driven          main expertise, making urban driving a useful domain in
learning that uses counterfactual reasoning to determine         which to study embedded cognition and learning.
both the error’s cause and the correct behavior.                    For this reason, we have developed a three-dimensional
   Counterfactual reasoning is a strategy that considers         urban driving environment based on the Torque Game
what might have occurred if causal events were changed           Engine produced by Garage Games.1 The driving simu-
in some way. Psychological studies suggest that people           lator provides the driver with control over the gas pedal,
employ counterfactual reasoning in a variety of situations       brake and steering, with objects obeying realistic laws of
(Roese, Hur, & Pennington, 1999). Byrne and McEleney             physics. The simulator also generates detailed percep-
(2000) also show that they tend to employ counterfactual         tual information, in egocentric polar coordinates, about
reasoning mainly in response to negative outcomes, such          nearby entities, including road segments, intersections,
as failure to achieve or maintain goals. Finally, Epstude        lane lines, buildings, pedestrians, and other vehicles.
and Roese (2008) make the connection to learning, based             The driving task we examine here requires the agent
on their theory that the primary motivation for counter-         to overtake a stalled vehicle, which it decides to passs on
factual reasoning is to improve future performance.              the left. However, in taking this step, the agent crosses a
   The work described here offers a computational ac-            double yellow line, thereby violating the rules of driving
count of the role of counterfactual reasoning in learning        and risking collision with oncoming traffic. The problem
from failures. We embed this account within Icarus               is not that the agent lacks knowledge about this con-
(Langley & Choi, 2006), a unified theory of the human            straint; the error occurs because it was focusing on a
cognitive architecture that makes a commitment to hi-            different goal that interacts with the one it violates. We
erarchical, composable knowledge structures. We claim
                                                                    1
that these structures, along with the mechanisms for                  http://www.garagegames.com
                                                             2566

maintain that learning to avoid such errors relies on a          with each belief being an instance of some defined con-
form of counterfactual reasoning, which we describe in           cept. For example, the belief (aligned-in-lane me
detail later. Our analysis is not limited to urban driving,      line23 line24), that the agent is aligned in a lane
nor do we intend it to model how humans learn to oper-           bounded by line23 and line24, is an instance of the
ate a vehicle, but it provides a useful setting to illustrate    aligned-in-lane concept above. A recent extension of
our ideas. However, we must embed our account within             Icarus includes time stamps on beliefs to indicate when
some theoretical framework, to which we now turn.                the architecture inferred them and when they became
                                                                 false, with the symbol now indicating that a belief holds
    A Review of the Icarus Architecture                          on the current cycle. These time-annotated beliefs serve
We have explored these issues within Icarus, a theory            as a simple episodic trace to which we will return later.
of the cognitive architecture (Newell, 1990) that makes             Icarus includes a separate long-term memory for
commitments to the representations, performance mech-            skills. These are similar in form to concepts but spec-
anisms, and learning processes that underlie intelligence.       ify methods for achieving goals rather than conditions
Like Soar (Laird et al., 1986) and ACT-R (Anderson,              for recognizing their achievement. Each skill includes a
1993), Icarus encodes content as symbolic list struc-            generalized goal (which must refer to a defined concept),
tures, matches long-term structures against short-term           a set of perceptual and conceptual conditions that must
ones in a recognize-act cycle, combines goal-driven with         hold for the skill to match, and ordered subgoals that,
data-driven processing, and interleaves an incremental           once satisfied, should achieve the parent goal. For in-
form of learning with performance. Key differences in-           stance, the skill clause
clude separate memories for concepts and skills, the hi-          ((driving-well-in-lane ?agent ?line1 ?line2)
erarchical organization of knowledge, and cascaded in-             :percepts ((self ?agent) (lane-line ?line1)
tegration in which problem solving builds on skill exe-                         (lane-line ?line2))
cution, which in turn relies on conceptual inference. In           :start      ((in-lane ?agent ?line1 ?line2))
this section, we review Icarus’ structures and processes,          :subgoals ((at-speed ?agent)
drawing brief examples from urban driving. Langley and                          (centered-in-lane ?agent ?line1 ?line2)
Choi (2006) describe the framework in more detail.                              (aligned-in-lane ?agent ?line1 ?line2)))
   The most basic process in Icarus is inference, which
matches the agent’s perceptions against long-term con-           specifies three subgoals the agent should achieve, once it
ceptual structures to produce beliefs about the environ-         is in a lane, to be driving well in that lane. Primitive
ment. On each cycle, the environment deposits descrip-           skills have the same structure but replace subgoals with
tions of perceived objects into the perceptual buffer, with      a set of executable actions the agent should carry out.
each percept giving the object type, a unique identifier,           The Icarus execution process uses the beliefs pro-
and (typically numeric) attribute-value pairs. Icarus            duced during conceptual inference to determine which
links these perceptions to long-term structures in its con-      skills to select. This process begins by choosing an un-
ceptual memory. Each concept describes a class of situa-         satisfied goal – a concept instance the agent wants to
tions in logical form that includes the concept’s name, its      be true – stored in a separate goal memory. The ar-
arguments, and the conditions under which the concept            chitecture retrieves all skills indexed by this goal, then
applies. Conditions may refer to percepts or to simpler          attempts to find an applicable path downward through
concepts, thus creating a hierarchical organization on           the skill hierarchy. A skill path is applicable if, for each
memory. For instance, the conceptual structure                   skill on the path, the associated goal is not satisfied and
                                                                 the associated conditions are met. Such a path must ter-
 ((aligned-in-lane ?agent ?line1 ?line2)                         minate in a primitive skill with executable actions that
  :percepts ((self ?agent) (lane-line ?line1))                   affect the environment. On each cycle, Icarus selects
               (lane-line ?line2 angle ?angle)                   the first such path through the skill hierarchy that it
  :relations ((steering-straight ?agent)                         finds, incorporating a preference for continuing an activ-
               (in-lane ?agent ?line1 ?line2))                   ity it has initiated over starting new ones.
  :tests      ((≥ ?angle -3) (≤ ?angle 3)))                         When Icarus can find no applicable skill in memory
states when one should infer that ?agent is driving par-         to achieve its current goal, it resorts to a form of means-
allel to the lane lines on either side, with the conditions      ends problem solving (Newell & Simon, 1961; Carbonell,
referring to percepts, numeric tests among perceived at-         Knoblock, & Minton, 1990) that attempts to dynami-
tributes, and other conceptual relations.                        cally compose known skills, which it executes as they
   On each cognitive cycle, Icarus matches these con-            become applicable. The process begins when the archi-
cepts against elements in its perceptual buffer to pro-          tecture cannot find an applicable path through the skill
duce inferences that it deposits in a belief memory, which       hierarchy, in which case it attempts to retrieve a skill
in turn produce higher-level inferences. These typically         that would achieve the goal, then creates subgoals for
describe relations among objects in the environment,             its unsatisfied preconditions. If Icarus cannot find such
                                                             2567

a skill, it instead retrieves the definition for the goal        previously satisfied, higher-priority goal in the course of
concept and creates subgoals for each of its unsatisfied         pursuing its current goal. For example, suppose a driv-
conditions. This process continues recursively, chaining         ing agent (me) has two goals, (on-right-side me) and
backward off subgoals, until it retrieves a relevant ap-         (at-speed me). The agent first maneuvers the vehicle
plicable skill, which it then executes in the environment.       onto the right side of the road and then begins accelerat-
Upon achieving the current subgoal, the system turns its         ing toward its desired speed. If another, slower-moving
attention to others, continuing this activity until achiev-      vehicle then enters the road in front of the agent, it may
ing the top-level goal that initiated problem solving.           execute a skill for passing on the left. This violates the
   This mechanism lets Icarus overcome situations in             agent’s first goal, and causes it to abandon the second
which it lacks skills to achieve its goals, but it often re-     goal in an effort to restore the first.
quires substantial search. However, the architecture also           If the agent prefers to pass on the left when ap-
includes a learning process that generalizes and stores so-      proaching a slow vehicle, it will never satisfy both
lutions for future use in similar situations. Briefly, this      goals in this situation. To address this stalemate, the
creates a skill whenever problem solving achieves a goal         architecture constructs a new goal by conjoining the
or subgoal, with the new structure being indexed by that         two concepts that supported the original goals, then
goal, including subgoals that it satisfied along the way,        replacing the goals with one based on the new con-
and having conditions that were present when it began            cept. Returning to the driving example above, Icarus
working on the problem. The details differ depending             first creates a concept (on-right-side-and-at-speed
on whether the problem solver chained off skills or con-         ?agent) with relations (on-right-side ?agent) and
cept definitions, but the results are similar for both cases.    (at-speed ?agent). Note that the new concept ex-
Icarus can then apply the new skills in the same manner          tends the current hierarchy by building on existing con-
as the other, older skills during subsequent execution.          cepts. Icarus then replaces the two original goals with
                                                                 (on-right-side-and-at-speed me), giving the agent
    Learning from Undesirable Outcomes                           a new goal for which it can learn more specific skills.
We have used Icarus to develop cognitive models in
a number of complex domains, including urban driv-               Assigning Blame and Finding Alternatives
ing (Langley & Choi, 2006) and American football (Li,            After identifying which goals conflicted and creating a
Stracuzzi, Cleveland, et al., 2009). Nevertheless, the ar-       revised top-level goal, Icarus attempts to understand
chitecture lacks some important functionalities, includ-         the reasons for its failure and how it might have been
ing the ability to learn from undesirable outcomes. The          avoided. To this end, it attempts to identify the most
work we report here has started to address this limi-            recently executed primitive skill that, if replaced by a
tation by adding a mechanism for learning from errors.           better choice, would avoid violating the high-priority
More specifically, we consider the case in which the agent       goal and achieve the newly constructed one. This con-
manages to achieve a given goal, but in which this causes        stitutes a form of counterfactual reasoning. The system
it to inadvertently violate another, higher-priority, goal,      begins by considering each primitive skill executed in the
which it must then attempt to repair.                            episode in reverse chronological order. For a selected
   As we discuss in detail below, the extended Icarus            skill, it rolls back episodic belief memory to the cycle on
responds to such situations in three steps. First, it deter-     which it first selected and executed the skill.2 Icarus
mines which goals conflicted and constructs a new con-           does not consider non-primitive skills, as they could lead
cept that it uses to encode the combined goal. Next, the         it to backtrack farther than necessary.
architecture employs counterfactual reasoning to identify           After rolling back the episodic trace, Icarus invokes
the primitive skill that produced the error and to de-           problem solving with the new, conjoined goal created
termine another sequence that achieves this goal. Note           earlier. Recall that the architecture normally interleaves
that the skill which led to the conflict may have been           problem solving with execution, but here the agent can
executed many cycles prior to the actual goal violation.         only suppose what might have happened if it had taken
Finally, Icarus learns new skills that mask the origi-           another path. For this reason, we introduced a more tra-
nal structures and achieve the joint goal in similar situa-      ditional version of problem solving that uses mental sim-
tions. These mechanisms are not completely new, in that          ulation based on skills’ effects. Each time the problem
they build upon many existing Icarus processes, mak-             solver selects a skill for imaginary execution, it updates
ing them more elaborations of the architectural frame-           belief memory with the expected changes, then triggers
work than separate modules.                                      inference, which updates belief memory as though it had
                                                                 received new percepts.
Combining Interacting Goals
                                                                     2
The first step toward learning from errors is to deter-                Primitive skills may be durative, which means they may
                                                                 require several cycles to achieve their intended goals. Back-
mine which goals conflicted to cause the failure. A con-         tracking over such a skill may therefore involve jumping back-
flict here refers to a case in which the system violates a       ward in time by several cognitive cycles.
                                                             2568

   If the problem solver fails to achieve the combined          show that the new system exhibits an important capac-
goal, then the system rolls back another step, past the         ity of human cognition that its predecessor lacked. In
preceding primitive skill in the original episodic trace.       this run, we placed the Icarus driving agent3 in the
The process outlined above then begins again with a             leftmost lane on the right side of the road heading east.
new round of problem solving. Note that this approach           Another vehicle in the same lane was stalled in the road
to error localization and counterfactual reasoning is well      ahead. The agent’s initial goals were (on-right-side
integrated with existing facets of the architecture, and        me) and (avoid-obstacle me). As the agent drives, it
it depends critically on results produced by the modules        realizes that it is approaching the car ahead too rapidly
for inference and problem solving. This provides further        and avoids colliding with it by swerving left. The agent
evidence that Icarus offers a unified theory of cognition.      swerves left rather than right simply because the associ-
                                                                ated skill happens to prefer that option, but this causes it
Learning and Selecting New Skills                               to violate the high-priority goal, (on-right-side me),
Having determined a sequence of primitive skills that           by crossing to the left side.
would have achieved both of its original goals, the ex-            At this point the agent realizes that it had ignored
tended architecture must generalize these results and           this high-priority goal while focusing on another one.
store them in skill memory for use in guiding future be-        Drawing its counterfactual reasoning abilities, the sys-
havior. We want the acquired skills to apply in more            tem creates a concept,
constrained situations than those which produced the
                                                                 ((on-right-side-and-avoid-obstacles ?self)
undesirable outcome, so they should mask the old skills
                                                                  :percepts ((self ?self))
to prevent their selection in these cases. For our driving
                                                                  :relations ((on-right-side ?self)(avoid-obstacles ?self)))
example, the system should select the new skill only if
it wants to achieve both (on-right-side ?agent) and             that can serve as a new conjoined goal to direct its anal-
(at-speed ?agent).                                              ysis. Next, the reasoning system backtracks through
   To learn skills from the results of counterfactual rea-      the episodic trace to the previously executed primi-
soning, Icarus uses the same mechanism as during nor-           tive skill, (throttle-special-value me). Using time
mal problem solving. As the agent works toward its top-         stamps on beliefs to reconstruct its mental state at
level goal, it acquires a new skill as means-ends analysis      that point, it invokes problem solving and mental sim-
achieves each subgoal. The use of counterfactual reason-        ulation to search for another sequence of skills that
ing and mental simulation, rather than execution in the         achieves the goal (avoid-obstacle me) without vio-
environment, has no effect on this process. However, to         lating the other one. In this case, the problem solver
make effective use of this learned knowledge, we must           cannot find a solution that begins with this state, so
modify Icarus’ execution module. The standard mech-             the reasoner continues to backtrack through the earlier
anism selects the unsatisfied goal with highest priority,       skill, (crossing-into-left-lane me), which also fails
then finds a path through the skill hierarchy that should       to solve the problem. Eventually, after returning men-
achieve it. This scheme works well when there are no            tally to the state before (wheels-straight me), prob-
goal interactions, but, as we have seen, it can lead to         lem solving finds a sequence of primitive skill instances,
problems when they exist.
                                                                  (crossing-into-right-lane2 me), (wheels-straight me),
   In response, we modified the execution module to pre-
                                                                  (on-right-side-lane2 me), (lane-aligned me),
fer skills that, other things being equal, would let the
                                                                  (wheels-straight me)
agent achieve multiple goals. For example, it selects a
skill that addresses two goals with first and second prior-     that, if executed, would have achieved the conjoined goal
ity over one that would achieve only the first goal. How-       (on-right-side-and-avoid-obstacles me). Analysis
ever, priority still plays a key role; the system prefers a     of this solution using the adapted means-end problem
skill that tackles a first priority goal over one that would    solver leads to creation of a single new skill
achieve goals with second and third priority. This ap-           ((on-right-side-and-avoid-obstacles ?self)
proach lets more specific skills acquired through counter-        :percepts ((self ?self))
factual reasoning mask the original, more general skills          :start      ((on-right-side-lane1 ?self)
that caused the undesired behavior, while letting the                          (drone-ahead ?self ?drone ?dist ?angle))
older skills remain available for situations in which only        :subgoals ((avoid-obstacles-by-right ?self)))
they apply.
                                                                which is indexed by the new conjoined concept that, if
       Demonstration on Urban Driving                           satisfied, ensures that its component concepts are met.
We tested this extension to Icarus in the urban driving            On a subsequent run after learning with the same ini-
domain described earlier. Here we consider a single run         tial situation, the agent makes a different choice when
at length to clarify the architecture’s operation. Our              3
                                                                      The Icarus agent for this task included 23 skill clauses
aim is not to match human behavior in detail, but to            and 48 conceptual clauses, both organized hierarchically.
                                                            2569

it approaches the stalled vehicle, swerving into the right         errors that stem from overly general rules by compar-
lane rather than crossing over to the left side. The reason        ing the actual and intended outcomes of selected actions,
is that the architecture prefers skills that are indexed by        while Holland et al. (1986) describe a mechanism for spe-
more specific goals, thus masking the original preference          cializing rules using counterexamples. Langley (1987)
for veering left over right. The result is that the agent          also reports an approach which compares similar situ-
still avoids hitting the stalled car ahead of it, satisfying       ations that produce positive and negative outcomes to
the goal (avoid-obstacle me), without violating the                improve upon overly general rules. Work on analytical
even higher-priority goal, (on-right-side me).                     learning typically focuses on learning from success, but
                                                                   a few efforts (Carbonell et al., 1990; Laird et al., 1986)
                       Discussion                                  address learning from failure. These share our concern
Counterfactual reasoning has been implicated in humans             with explaining reasons for errors, but they produce con-
as a mechanism for establishing the cause of particular            trol rules that specify what to avoid, while our approach
events (Roese, 1997), for identifying errors of both omis-         instead acquires skills that mask the undesired behavior.
sion and commission (Byrne & McEleney, 2000), and for                 In addition to testing the architectural extensions in
learning from errors (Roese & Olson, 1995; Wells & Ga-             other domains that involve goal interactions, we should
vanski, 1989). Our work with Icarus has focused on                 also improve our account of counterfactual reasoning
using counterfactuals to establish the cause of negative           along other dimensions. One involves increasing its psy-
events (violations of maintenance goals) and to replace            chological plausibility by placing realistic limits on the
incorrect actions with proper ones. Although we have               contents of Icarus’ perceptual buffer and its episodic
not yet shown that it can recover from errors of omis-             memory, which currently contain far more than their
sion, we believe that the same mechanisms will support             human analogs. We should also expand the generality
such learning.4 Our account of counterfactual reasoning            of our counterfactual reasoning framework to learn from
makes clear contact with psychological literature on the           other types of errors, such as Ohlsson’s constraint viola-
topic and, although our model makes some implausible               tions. In addition, we should extend the architecture’s
assumptions (e.g., about memory), its main features are            representation and its inferential abilities to let it reason
consistent with key theories and empirical findings.               about the goals and beliefs of other agents, since many
   Few computational models have made use of coun-                 of the most interesting errors that humans exhibit, and
terfactuals in the context of learning. One example,               from which they are driven to learn, occur during their
Mueller and Dyer’s (1985) Daydreamer, uses them                    interpersonal interactions.
more broadly than Icarus but in a less directed man-                  The main contribution of our work has been a compu-
ner. The system learns from both positive and nega-                tational account of skill learning through counterfactual
tive experiences by postulating alternative actions and            reasoning. This involves three major steps: detecting
considering their consequences, but it proposes scenarios          that pursuit of one goal has violated another, reasoning
based on control goals, episodic memory contents, and              backwards from the conflict to identify the choice that
emotional state. This strategy can produce a variety of            caused it and finding an alternative path that would have
outcomes, some substantially removed from reality, while           avoided it, and storing a specialized skill that produces
Icarus pursues a single goal until achieving it. Pearson’s         the desired actions and masks the original behavior. Al-
(1996) Improv also makes use of counterfactuals to im-             though we have embedded our account within Icarus,
prove procedural knowledge. Like Icarus, it considers              one could also incorporate it into other architectures,
alternative action sequences starting from the last state          although some details would certainly differ. And al-
before the error occurred, then working backward until             though we have illustrated these mechanisms in the con-
it finds a solution. However, Improv focuses on revising           text of urban driving, they seem relevant to any domain
skills that fail to achieve intended goals, while Icarus           in which goal conflicts can arise. We will not claim that
specializes skills that violate other goals it achieved pre-       our account covers all forms of learning through coun-
viously. In addition, Improv revises its knowledge by              terfactual reasoning, which may also support revision of
modifying skill preconditions, rather than learning new            incorrect concepts, skills, and beliefs, but we believe it
skills that achieve more specific goals.                           advances our understanding of this complex ability, and
   Other research on learning from errors has also focused         thus our grasp of human cognition.
on detecting and resolving errors, most on ones that stem
from overly general rules. For example, early versions of                         Concluding Remarks
the Swale system (Schank, 1986) adapt explanations to
unanticipated situations when its expectations are vio-            In this paper, we presented a set of interacting compu-
lated. Similarly, Ohlsson (1996) shows how to correct              tational mechanisms that support learning from unde-
    4                                                              sirable outcomes via counterfactual reasoning. We em-
      Ginsberg (1986) discusses the use of counterfactual rea-
soning in identifying subgoals during problem solving, which       bedded this account within Icarus, a theory of the cog-
we have not addressed here.                                        nitive architecture that placed strong constraints on our
                                                               2570

approach to the problem. After reviewing the structures          Laird, J. E., Rosenbloom, P. S., & Newell, A. (1986).
and processes that Icarus assumes, we presented new                Chunking in Soar: The anatomy of a general learning
mechanisms that identify the violation of previously sat-          mechanism. Machine Learning, 1, 11–46.
isfied goals, localize the cause of this event by inspecting     Langley, P. (1987). A general theory of discrimination
an episodic trace, invoke problem solving to find alterna-         learning. In D. Klahr, P. Langley, & R. Neches (Eds.),
tive steps that would have avoided the error, and learn            Production system models of learning and development
specialized skills from this analysis that generate the de-        (pp. 99–161). Cambridge, MA: MIT Press.
sired behavior in the future.                                    Langley, P., & Choi, D. (2006). A unified cognitive ar-
   We demonstrated these interacting mechanisms in the             chitecture for physical agents. In Proceedings of the
context of a simulated urban driving environment, show-            Twenty-First National Conference on Artificial Intel-
ing that they behave as intended in a complex scenario             ligence (pp. 1469–1474). Boston: AAAI Press.
that requires multi-step reasoning. We also considered           Li, N., Stracuzzi, D. J., Cleveland, G., Könik, T.,
earlier work on error-driven learning that bears similar-          Shapiro, D., Molineaux, M., et al. (2009). Construct-
ities to our own, but that has addressed different issues,         ing game agents from video of human behavior. In
and promising directions for extending our approach. It            Proceedings of the Fifth AAAI Conference on Artifi-
seems clear that counterfactual reasoning plays an im-             cial Intelligence and Interactive Digital Entertainment
portant role in human learning and, although our current           (pp. 64–69). Stanford, CA: AAAI Press.
model explains only certain forms of such cognitive be-          Mueller, E. T., & Dyer, M. G. (1985). Towards a compu-
havior, it nevertheless provides a novel account of the            tational theory of human daydreaming. In Proceedings
mechanisms that underlie support this complex ability.             of the Seventh Annual Conference of the Cognitive Sci-
                                                                   ence Society (pp. 120–129). Irvine, CA.
                 Acknowledgements                                Newell, A. (1990). Unified theories of cognition. Cam-
This material is based on research sponsored by ONR un-            bridge, MA: Harvard University Press.
der agreement N00014-09-1-0123. The U. S. Government             Newell, A., & Simon, H. A. (1961). GPS: A program
is authorized to reproduce and distribute reprints for             that simulates human thought. In H. Billing (Ed.),
Governmental purposes notwithstanding any copyright                Lernende automaten. Munich: Oldenbourg KG. (Re-
notation thereon. The views and conclusions contained              printed in E. A. Feigenbaum & J. Feldman (Eds.),
herein are the authors’ and should not be interpreted as           Computers and thought. New York: McGraw-Hill.)
representing the official policies or endorsements, either       Ohlsson, S. (1996). Learning from performance errors.
expressed on implied, of ONR or the U. S. Government.              Psychological Review, 103, 241–262.
                                                                 Pearson, D. J. (1996). Learning procedural planning
                        References                                 knowledge in complex environments. Unpublished doc-
                                                                   toral dissertation, Computer Science and Engineering
Anderson, J. R. (1983). The architecture of cognition.
                                                                   Division, University of Michigan, Ann Arbor, MI.
   Cambridge, MA: Harvard University Press.
                                                                 Roese, N. (1997). Counterfactual thinking. Psychological
Anderson, J. R. (1993). Rules of the mind. Hillsdale,
                                                                   Bulletin, 121, 133–148.
   NJ: Lawrence Erlbaum.
                                                                 Roese, N., Hur, T., & Pennington, G. (1999). Coun-
Anzai, Y., & Simon, H. A. (1979). The theory of learning
                                                                   terfactual thinking and regulatory focus: Implications
   by doing. Psychological Review, 86, 124–140.
                                                                   for action versus inaction and sufficiency versus neces-
Byrne, R., & McEleney, A. (2000). Counterfactual think-
                                                                   sity. Journal of Personality and Social Psychology, 77,
   ing about actions and failures to act. Journal of Ex-
                                                                   1109–1120.
   perimental Psychology: Learning, Memory, and Cog-
                                                                 Roese, N., & Olson, J. (1995). What might have been:
   nition, 26, 1318–1331.
                                                                   The social psychology of counterfactual thinking. Hills-
Carbonell, J., Knoblock, C., & Minton, S. (1990).
                                                                   dale, NJ: Erlbaum.
   Prodigy: An integrated architecture for planning and
                                                                 Schank, R. (1986). Explanation patterns: Understanding
   learning. In K. VanLehn (Ed.), Architectures for in-
                                                                   mechanically and creatively. Hillsdale, NJ: Erlbaum.
   telligence. Hillsdale, NJ: Lawrence Erlbaum.
                                                                 Stracuzzi, D. J., Li, N., Cleveland, G., & Langley, P.
Epstude, K., & Roese, N. J. (2008). The functional the-
                                                                   (2009). Representing and reasoning over time in a sym-
   ory of counterfactual thinking. Personality and Social
                                                                   bolic cognitive architecture. In Proceedings of the 31st
   Psychology Review, 12, 168–192.
                                                                   Annual Meeting of the Cognitive Science Society (pp.
Ginsberg, M. (1986). Counterfactuals. Artificial Intelli-
                                                                   2986–2991). Amsterdam.
   gence, 30, 35–79.
                                                                 Wells, G., & Gavanski, I. (1989). Mental simulation of
Holland, J., Holyoak, K., Nisbett, R., & Thagard, P.
                                                                   causality. Journal of Personality and Social Psychol-
   (1986). Induction: The process of inference, learning,
                                                                   ogy, 55, 161–169.
   and discovery. Cambridge, MA: MIT Press.
                                                             2571

