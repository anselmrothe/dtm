UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Self-directed speech alters visual processing
Permalink
https://escholarship.org/uc/item/9fn7n33r
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Lupyan, Gary
Swingley, Daniel
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                   Self-directed speech alters visual processing
                                              Gary Lupyan (lupyan@sas.upenn.edu)
                           Institute for Research in Cognitive Science, Center for Cognitive Neuroscience
                                                        University of Pennsylvania
                                                       Philadelphia, PA 19104 USA
                                         Daniel Swingley (swingley@psych.upenn.edu)
                               Department of Psychology, Institute for Research in Cognitive Science
                                                        University of Pennsylvania
                                                       Philadelphia, PA 19104 USA
                              Abstract                                  1994; Goldstein, 1948; Rumelhart, Smolensky, McClelland,
                                                                        & Hinton, 1986; Vygotsky, 1962). This is not to say that
   A major part of learning a language is learning connections          different languages necessarily place strong constraints on
   between spoken words and their referents in the world. An            the speaker’s ability to entertain certain concepts. Rather, it
   open question concerns the consequence this learning has for
                                                                        is a claim that language richly interacts with putatively
   cognition and perception. According to the label feedback hy-
   pothesis (Lupyan, 2007), processing a verbal label can change        nonlinguistic processes such as visual perception.
   ongoing perceptual processing, e.g., actually hearing “chair”           Insofar as performance on putatively nonverbal tasks
   compared to simply thinking about a chair temporarily makes          draws on language, interfering with language should inter-
   the visual system a better chair detector. Here, we test wheth-      fere with performance on those tasks (Goldstein, 1948).
   er engaging in a non-communicative verbal act—speaking to            Indeed, verbal interference impairs certain types of catego-
   oneself—also affects visual processing. Participants searched        rization in a way strikingly similar to impairments observed
   for common objects, sometimes being asked to speak the tar-          in aphasic patients (Lupyan, 2009). Interfering with lan-
   get’s name aloud. Speaking facilitated search, but only when         guage can also affect perception. A number of studies have
   there was a strong association between the name and the vis-
                                                                        shown that interfering with language impairs categorical
   ual target. Speaking appeared to hurt performance when there
   was even a slight discrepancy between the name and the tar-          color perception (e.g., Gilbert, Regier, Kay, & Ivry, 2006;
   get. Together these results speak to the power of words to           Roberson & Davidoff, 2000; Roberson, Pak, & Hanley,
   evoke associated visual information.                                 2008; Winawer et al., 2007), suggesting that language ac-
                                                                        tively modulates visual processing.
                         Introduction                                      An additional way to study affects of language on percep-
                                                                        tion is by attempting to increase rather than decrease its
Learning a language involves, among other things, learning
                                                                        putative effect. A surprising finding is that when asked to
to map words onto categories of objects in the environment.
                                                                        find a certain visual item among distractors actually hearing
In addition to learning that chairs are good for sitting on,
                                                                        its name immediately prior to performing the search—even
one learns that this class of objects has the name “chair.”
                                                                        when the label is entirely redundant—improves speed and
Clearly, this learning is critical for linguistic communica-
                                                                        efficiency of searching for the named object (or searching
tion. But beyond communication, what consequences does
                                                                        among the named objects). For example, when participants
naming things—hearing and producing verbal labels—have
                                                                        search for the numeral 2 among 5’s (for hundreds of trials),
on perception and nonverbal cognition?
                                                                        actually hearing the word “two” (or hearing “ignore fives”)
   On one account language is a “transparent medium
                                                                        immediately prior to doing the search, improves search RTs
through which thoughts flow” (H. Gleitman, Fridlund, &
                                                                        and reduces search slopes (Lupyan, 2007a, 2008a). Indeed,
Reisberg, 2004, p. 363). Therefore, words are mapped onto
                                                                        hearing an object name can even make an otherwise invisi-
concepts, but do not affect them (e.g., L. Gleitman & Papa-
                                                                        ble object visible (Lupyan & Spivey, 2008; under review).
fragou, 2005; Gopnik, 2001). Thus, while word-learning is
                                                                           One way to understand such findings is in terms of an in-
significantly constrained by nonverbal cognition, nonverbal
                                                                        teractive activation framework (Rumelhart & McClelland,
cognition is not significantly influenced by learning or using
                                                                        1982; Spivey, 2008) in which recognition involves the com-
words (Snedeker & L. Gleitman, 2004).
                                                                        bination of bottom-up perceptual information, with higher-
   The alternative is that words are not simply mapped on to
                                                                        level top-down (conceptual) information. As one learns a
concepts, but actually change them, affecting nonverbal
                                                                        verbal label, it becomes associated with features that are
cognition, and even perception. The idea that words can
                                                                        most diagnostic (or typical) of the named category. With
affect the concepts to which they refer is not new: William
                                                                        such associations in place, hearing the label provides top-
James, for example, remarked on the power of labels to
                                                                        down activation of visual properties associated with the la-
make distinctions more concrete (James, 1890, p. 333), and
                                                                        bel. In effect, the object name makes an object a “better”
it has been argued that words stabilize abstract ideas in
                                                                        object by augmenting the idiosyncratic perceptual features
working memory and make them available for inspection
                                                                        of a given object with features typical to the named category
(Clark, 1997; Clark & A Karmiloff-Smith, 1993; Dennett,
                                                                        (Lupyan, 2007b, 2008b).
                                                                   1210

Aims and Hypotheses                                                 of measures for these pictures, which we included for item-
   In the present work, we investigate whether non-                 analyses. Most relevant to the present work are: RT to name
communicative (self-directed) speech can affect visual              the picture, familiarity, subjective visual complexity, and
processing in the context of a visual search task. Does pro-        imagery-concordance. The latter measure was derived by
ducing the name of a pre-defined target object enable sub-          presenting participants with a picture name (e.g., butterfly),
jects to find it faster? Participants were asked to find an ob-     asking them to form a mental image of the object, and then,
ject among distractors while speaking its name or not. We           on seeing the actual picture, providing a rating of imagery
predicted that actually speaking the object’s name would            agreement. For the lexical items themselves, we obtained
facilitate visual search—even though such speaking can be           log frequency from the British National Corpus, word
seen to constitute a form of distraction. We also predicted         length in phonemes and syllables, actual age-of-acquisition
that the effect of speaking would be largest for items most         (AoA) norms (Morrison, Chappell, & Ellis, 1997), and sev-
strongly associated with the label, and speaking might actu-        eral measures from the MRC Psycholinguistic Database
ally be detrimental when searching for objects having               (www.psych.rl.ac.uk/): imageability, concreteness, and
weaker associations with the label, e.g., objects judged as         word familiarity.
being less typical of their categories.
                                                                    Procedure
                        Experiment 1                                   Each trial began with a prompt informing the participant
                                                                    what object they would need to find. The prompt also in-
The participants’ task was to find and click on a target ob-
                                                                    formed them whether they should repeat the object’s name
ject among 35 distractors, positioned randomly in a 6×6 grid
                                                                    as they searched for it, or not. For example, immediately
on a computer screen (Figure 1). For half the trials, partici-
                                                                    prior to a no-speaking trial, participants saw a prompt such
pants were asked to speak the name of the target as they
                                                                    as “Please search for a butterfly. Do not say anything as
searched for it.
                                                                    you search for the target” For a speaking trial, the second
Participants                                                        sentence was replaced by “Keep repeating this word con-
                                                                    tinuously into the microphone until you find the target.” The
Twelve University of Pennsylvania undergraduates partici-           speech/no-speech trials were intermixed, as were the target
pated for course credit.                                            identities. Participants completed 320 trials: 20 targets ×
                                                                    speech condition (speaking vs. not speaking) × 8 blocks. A
Materials                                                           block included all target × speech condition combinations.
   The targets and distractors were drawn from a set of 260         Participants used a computer mouse to click on the target
colored images of common objects (Rossion & Pourtois,               object.
2004). For the targets, we selected 20 images having 100%
picture-name agreement, as assessed by Rossion and Pour-             Results and Discussion
tois (2004) (airplane, banana, barn, butterfly, cake, carrot,       Participants showed excellent compliance with the instruc-
elephant, giraffe, chicken, ladder, lamp, leaf, truck, motor-       tion to speak the name of the target on the label trials and to
cycle, mouse, mushroom, rabbit, tie, umbrella, windmill).           remain silent on the no-speaking trials. We focus on accu-
                                                                    racy and median RTs to find the target as the main depend-
                                                                    ent measures. Comparisons between conditions were made
                                                                    using a mixed-effects ANCOVA with speech condition as a
                                                                    fixed effect, subject as a random effect, and block as a co-
                                                                    variate. For reasons described in Thomas et al., (2009),
                                                                    separate tests were run to assess fixed factor main effects
                                                                    and those of the covariate × factor interaction.
                                                                       Accuracy was extremely high, M=98.8%, revealing that
                                                                    (1) subjects had no trouble remembering which item they
                                                                    were supposed to find, and (2) the word cues were suffi-
                                                                    ciently informative to locate the correct object. Despite this
                                                                    very high accuracy, saying the object’s name during search
                                                                    resulted in significantly higher accuracy, M=99.2% than not
                                                                    repeating the name, M=98.4%, F(1,11)=12.19, p=.005. Par-
                                                                    ticipants’ accuracy increased over the course of the experi-
                                                                    ment, F(1,11)=10.90, p=.001, but there was no reliable
                                                                    speech-condition × accuracy interaction, F(1,11)=1.49,
            Figure 1: A sample search trial from Exp. 1             p>.2.
                                                                        The analysis of median RTs included correct responses
For a given trial, any of the 259 non-target images could           only. Unsurprisingly, participants’ speed improved over the
serve as distractors. Rossion and Pourtois provide a number         course of the experiment, F(1,11)=22.85, p<.0005. There
                                                                1211

                                                                            variables did not predict overall search performance, though
                                                                            there were marginal correlations of search times with word
                                                                            frequency, r(18)=-.38, p=.10, and of accuracy with age-of-
                                                                            acquisition (AoA) provided by adults, r(18)=-40, p=.08.
                                                                                Finally, we examined which items were most affected by
                                                                            self-directed speech by subtracting performance on speech
                                                                            trials from performance on no-speech trials. Overall, speak-
                                                                            ing improved RTs most for the items which took, on aver-
                                                                            age, the least time to find, r(18)=-.57, p=.009, and ones for
                                                                            which accuracy was, on average, the highest, r(18)=.47,
                                                                            p=.037. Recall that familiarity was not related to overall
                                                                            accuracy. However, separating accuracy into speech and no-
        Figure 2: RTs in Exp. 1: Speaking significantly de-
     creased RTs for the second half of the task. Error bars                speech trials revealed a very different pattern. Familiarity
    show ±1SE of the mean condition difference. Accuracy                    was unrelated to performance on no-speech trials, p>.3, but
       was significantly higher for the speaking condition                  was highly correlated with performance on speaking trials,
                   throughout the task; see text.                           r(18)=.55, p=.01. The interaction was significant: speaking
                                                                            improved accuracy most for the more familiar items,
  was no main effect of the speech-condition on RTs, F<1, but
                                                                            r(18)=.51, p=.02 (Figure 3). Finally, RTs were improved
  there was a highly reliable speech-condition × block interac-
                                                                            marginally more for the items with the highest imagery-
  tion, F(1,11)=8.1, p=.004. As shown in Figure 2, perform-
                                                                            concordance, r(18)=.39, p=.08.
  ance on the speech trials tended to be slower than on no-
                                                                                We also observed a relationship between AoA and self-
  speech trials for the initial blocks, but this pattern reversed
                                                                            directed speech. This relationship changed over the course
  for the latter part of the experiment. Collapsing the last three
                                                                            of the experiment: for the first half of the task, AoA (both
  blocks, participants were faster on speech trials than no-
                                                                            subjective and objective), correlated with the effect of
  speech trials, t(11)=2.91, p=.01 (two-tailed). This finding
                                                                            speaking on search times, robjective AoA(28)=-.54, p=.02, rsubjec-
  suggests that although the target objects were very familiar,
                                                                            tive AoA=-.62, p=.003: performance was impaired by saying
  speaking the name decreased RTs only when participants
                                                                            words having higher AoA. By the second half of the task,
  had several opportunities to associate the picture name with
                                                                            these correlations disappeared entirely, rs<.1.
  the target picture, which presumably strengthened the pic-
                                                                                For interpretive ease, we performed a median split on the
  ture-name association.
                                                                            familiarity and imagery-concordance values. The label ad-
      We next turn to the item analysis. A number of item fac-
                                                                            vantage (RTwithout-speaking-RTspeaking) was larger for items hav-
  tors predicted overall search performance. Search was
                                                                            ing imagery-concordance scores above than below the me-
  faster, r(18)=.55, p=.01, and more accurate, r(18)=-.54,
                                                                            dian, F(1,18)=6.32, p=.022. Search items below the median
  p=.02, for pictures that were visually simpler according to
                                                                            were actually slowed by speaking, t(10)=2.24, p=.049 (two-
  Rossion and Pourtois’s (2004) norms. Search was faster,
                                                                            tailed). The label advantage in accuracy trended in the same
  r(18)=-.55, p=.01, and slightly more accurate, r(18)=.34,
                                                                            direction, being (marginally) larger for items with above-
  p=.15 for pictures with higher imagery-concordance. Fa-
                                                                            median familiarity ratings, F(1,18)=4.19, p=.056.
  miliarity did not predict search times or accuracy. Lexical
                                                                                To summarize: speaking facilitated search for pictures
                                                                            judged in a separate norming study to be most familiar, and
Speaking
                  0.05                                                      targets having the highest concordance between the actual
                                                                            image and the mental image formed by reading the name.
                  0.04
                                                                                One way in which self-directed speech may help visual
Improves Acc.     0.03                                                      search is through verbal rehearsal: saying the name of the
                  0.02
                                                                            target might have helped participants remember what it was
                                                                            they were looking for. This account is not supported for two
                  0.01                                                      reasons. First, accuracy was extremely high, making it
Speaking Lowers
                  0.00                                                      unlikely that difficulties in remembering the target played a
                                                                            significant role. Second, a memory-based account would
                  -0.01
Accuracy
                                                                            predict that speech should help most for items that were
                  -0.02                                                     most difficult to find. We found exactly the opposite pat-
                          2.0      2.5   3.0   3.5       4.0       4.5
                                                                            tern.
                                                                                The item effects presented above place some constraints
                          Lower                      Greater
                                                                            on the mechanisms by which labels affect visual search.
                          Familiarity                Familiarity
                                                                            One possibility is that saying the target name helps to find
              Figure 3: Relationship between item familiarity and effects   the target by activating and/or keeping active the visual fea-
              of speaking on accuracy. Y-axis shows % correct when          tures typical to that object (e.g., saying “cherry” makes it
                      speaking - % correct when not speaking.               easier to attend to red things). Alternatively (or addition-
                                                                         1212

ally), repeating a label helps to reject distractors. If speaking     disappear, thus marking it as being selected. Once satisfied
facilitated search only by improving rejection of distractors,        with their choices, participants clicked on a large “Done”
one would not predict correlations between the magnitude              button that signaled the end of the trial. To make the task
of the speaking advantage and properties of the target. The           more challenging, some of the distractors were categorically
presence of these correlations supports the hypothesis that           related to the target, e.g., whenever searching for “Diet
speaking the target’s name facilitates deployment of atten-           Coke,” some distractors were of other sodas, e.g., “Ginger
tion to the target item over and above seeing the printed             Ale.” There were a total of 240 trials (30 targets by × 8
name of the target.                                                   blocks). Within each block, half the items were presented in
   The present results can be viewed as an extension of find-         a speech trial and half in a no-speech trial. Speech and no-
ings showing that hearing a label, even when it is entirely           speech trials alternated. Across the 8 blocks, each item was
redundant, facilitates visual search, and this facilitation is        presented an equal number of times in speech trial and no-
greatest for the stimuli most strongly associated with the            speech trials.
label (Lupyan, 2007a, 2007b, 2008a). When visual quality                 Prior to beginning the search task, participants rated each
of the item is reduced, or the item is made more ambiguous,           item on typicality (“How typical is this box of Cheerios
hearing a label can impair performance (Lupyan, 2007b).               relative to boxes of Cheerios in general?”), and visual qual-
Thus, compared to just being told what to find, speaking a            ity (“How well does this picture depict a box of Cheer-
target name—just like hearing it—affects visual search.               ios?”). For each item category (i.e., all three images of
                                                                      Cheerios), participants rated its familiarity (“Overall, how
                       Experiment 2                                   familiar to you are the objects depicted in these pictures?”)
The goal of Experiment 2 was to test whether self-directed            and visual similarity (“Considering only the visual appear-
speech affects performance on a more difficult and ecologi-           ance of these picture, how different are they from each
cally valid “virtual shopping” task in which participants             other?”). In addition to providing us with item information,
search for supermarket products in a visually complex dis-            this task served to pre-expose participants to all the targets.
play and were required to find several instances of a cate-           We also obtained an imageability measure from a separate
gory.                                                                 group of participants (N=28) who were shown the written
                                                                      product names, e.g., “Cheerios” and asked to rate how well
Participants                                                          they could visualize its appearance on a supermarket shelf.
Twenty-two University of Pennsylvania undergraduates (14              Results and Discussion
women) participated for course credit.
                                                                      The data were analyzed in the same way as in Exp. 1. Over-
 Materials                                                            all, participants were very accurate, averaging 1.5% false
                                                                      alarms and 97.7% hits (2.93 out of 3 targets). Overall per-
We photographed products on supermarket shelves in the                formance (RTs, hits, and false alarms) correlated with all
Philadelphia area and selected 30 to serve as targets, e.g,
                                                                      four item variables (visual similarity, visual quality, famili-
apples, Pop-Tarts, Raisin Bran, Tylenol, Jell-O. For each             arity, and typicality). Correlation coefficients ranged from
product, we obtained three pictures depicting instances of
                                                                      .35 to .65 (ps between .035 and <.0005). Items that were
the product in various sizes and orientations. Some pictures          familiar, typical, of higher quality, and having least within-
depicted multiple instances of the product, e.g., a shelf con-
                                                                      category similarity were found faster and with higher accu-
taining multiple cartons of orange juice. See Figure 4 for            racy. Of course, the item variables were not all independent,
some examples.
                                                                      e.g., familiar items and those of higher quality tended to be
                                                                      rated as more typical. The typicality and familiarity meas-
Procedure                                                             ures clustered together and were not independently predic-
As in Exp. 1, par-                                                    tive of performance (familiarity was the stronger of the two
ticipants were in-                                                    predictors). Within category visual similarity predicted per-
structed that they                                                    formance independently of familiarity; multiple regression:
would need to                                                         F(2,27)=9.15, p=.001.
search for various                                                       There was a reliable difference in hits between the two
items while being                                                     speech conditions: Mspeech=97.9%, Mno-speech=99.1%,
asked to some-                                                        F(1,21)=11.19, p=.003. While speaking the product name,
times speak the                                                       participants were more likely to miss one or more of the
items’       names.                                                   targets. As reported below, however, this effect was modu-
Each trial included                                                   lated strongly by the different targets in predictable ways.
all three instances                                                   Speech-condition was not a reliable predictor of false-
of the product and                                                    alarms, F(1,21)<1. There were no differences in total or per-
13       distractors.                                                 click RTs between the speech and no-speech conditions,
Clicking on an                                                        F<1. The speech-condition × block interaction was not reli-
                          Figure 4: Samples of 2 search catego-
object made it                    ries used in Exp. 2.                able, F<1.
                                                                  1213

  The item analyses in Exp. 1. suggested that effects of self-      magnitude of the female RT advantage and the measure of
directed speech may be modulated by the relationship be-            visual similarity: r(26)=.38, p=.049. The advantage was
tween the item and its name. Indeed, the cost in the hit rate       greatest for the most visually similar items (two items were
incurred by speaking (Hitsno-speech-Hitsspeech) was correlated      excluded, as statistical outliers). There were no other reli-
with within-category similarity, r(28)=-.34, p=.04: the cate-       able correlations.
gories having the most dissimilar items incurred the highest           Using a larger, more perceptually varied and true-to-life
cost when their names were repeated during search. The              item set, the item analyses of Exp. 2 reinforced the conclu-
effect of self-directed speech (RTno-speech-RTspeech) was also      sions of Exp. 1. As in Exp. 1, speaking aided search for the
mediated by familiarity, r(28)=-.51, p=.004: labels tended to       more familiar items. In contrast to Exp. 1, accuracy (hit rate)
hurt performance for the less familiar items, but improve           was actually decreased by speaking, though this decrease
                                                                    was limited to the items having low within-category similar-
                            40
                                       Most familiar                ity. This finding is consistent with the idea that speaking an
  Speaking advantage (ms)
                            30         Least Familiar               object name activates a (proto)typical representation of the
                                                                    category. When the task requires finding items that diverge
                            20
                                                                    from this prototype (as when participants need to find visu-
                            10                                      ally heterogeneous items from the same category), speaking
                             0
                                                                    can impair performance.
                                  1
                            -10                                                           General Discussion
                            -20                                        Can language affect ongoing perceptual processing? A
                            -30
                                                                    growing body of literature argues that it can. The present
                                                                    work is the first to examine effects of non-communicative
      Figure 5: Speaking advantage (no-speech – speech trials)      (self-directed) speech on a visual task.
     as a function of familiarity (median split). RTs were de-         The findings show that speaking the name of the object
    creased by speaking the names of the more familiar items        that one is searching for improves search performance, pro-
    and increased by speaking the names of the least familiar       vided that the object’s name is strongly associated with the
    items. Errors bars indicate 1±SE of the mean difference.        visual depiction of the object.
                                                                       The present results are somewhat less reliable than those
performance for the more familiar items (Figure 5). The             of hearing labels on visual search (Lupyan, 2007a, 2008a).
label advantage also correlated positively with product im-         Subsequent work has shown that the effects of speech on
ageability, r(28)=.44, p=.01. As an added confirmation of           visual processing have a characteristic timecourse, peaking
this finding, we divided the targets into those having charac-      about 0.5-1.5 seconds after the presentation of the label, and
teristic colors (N=11), e.g., bananas, grapes, cheerios, raisin     declining afterwards (Lupyan & Spivey, 2010, under re-
bran and those with weaker color associations, e.g., Jell-O,        view). In the present studies we did not have precise control
Pop-Tarts. The speaking advantage was greater for color-            over the timing of the label. Recordings of participants’
diagnostic items (for which speaking significantly improved         speech from the present work revealed a wide variability in
RTs) than for non color-diagnostic items (for which speak-          the onset, speed, and duration of self-directed speech. Thus,
ing marginally increased RTs), F(1,28)=7.35, p=.01.                 more reliable effects may be obtained with finer control
   Exp. 2 revealed a striking gender difference in perform-         over speaking onset and rate.
ance. Men had a significantly lower hit rate, F(1,20)=5.02,            Our results join work arguing for cognitive functions of
p=.037, and were significantly slower, F(1,20)=6.37, p=.02          self-directed speech. For example, even mild forms of ar-
to find the targets. The gender effect on RTs was substan-          ticulatory suppression impair adults’ ability to switch from
tial: men took on average 350 ms longer per trial. This ef-         one task to another (Baddeley, Chincotta, & Adlam, 2001;
fect was replicated in an item analysis, F2(1,29)=43.40,            Emerson & Miyake, 2003; Miyake, Emerson, Padilla, &
p<.0005 (the only item on which men were faster than                Ahn, 2004). The present results are consistent with Vygot-
women was “Degree Deodorant”). There was a marginal                 sky’s claim that the function of self-directed speech extends
gender × speech-condition interaction for hit rates,                far beyond verbal rehearsal (Carlson, 1997; Vygotsky,
F(1,20)=3.79, p=.066: labels hurt performance slightly more         1962)—itself a learned strategy (Flavell, Beach, & Chinsky,
for men than women. An examination of item ratings re-              1966).1
vealed that there were no gender differences in subjective             The present work comprises a first step in understanding
ratings of familiarity, visual-quality, or visual-similarity,       effects of self-directed speech on visual processing. One
Fs<1, and only a marginal difference in typicality: women           unanswered question is whether effects of speaking on vis-
believed our items to be slightly more typical than did men,        ual search arise from the act of production itself, or from
F(1, 20)=2.66, p=.12. In an effort to better understand the
origin of this gender difference, we correlated the magnitude           1
                                                                         It is worth noting that these articulatory suppression effects on puta-
of the female advantage with various ratings of the stimuli.        tively nonverbal task-switching were compelling enough for Baddeley to
We observed a mildly reliable relationship between the              concur with Vygotsky’s claim (Baddeley et al., 2001, p. 655).
                                                                 1214

hearing one’s speech. Although this distinction is of little                ries matter (and named categories matter more). Cognition,
practical importance—one almost always hears oneself                        108, 566-577.
speak—a full understanding of the mechanism by which                     Lupyan, G. (2008b). From chair to "chair:" A representational
speech and visual processing interact requires the two ex-                  shift account of object labeling effects on memory. Journal
planations to be teased apart. Despite these unknowns, the                  of Experimental Psychology: General, 137(2), 348-369.
present results show that in the context of searching for a              Lupyan, G. (2009). Extracommunicative Functions of Lan-
familiar object, knowing what an object is called is not the                guage: Verbal Interference Causes Selective Categorization
same as actually saying its name.                                           Impairments. Psychonomic Bulletin & Review, 16(4), 711-
                                                                            718. doi:10.3758/PBR.16.4.711
                                                                         Lupyan, G., & Spivey, M. (2008). Now You See It, Now You
                    Acknowledgments                                         Don't: Verbal but not visual cues facilitate visual object de-
We thank Ali Shapiro, Joyce Shin, for their help with data                  tection. In Proceedings of the 30th Annual Conference of
collection and for assembling the stimulus materials.                       the Cognitive Science Society (pp. 963-968). Austin, TX.
                                                                         Lupyan, G., & Spivey, M. (2010). Redundant spoken labels
                         References                                         facilitate perception of multiple items. under review.
Baddeley, A. D., Chincotta, D., & Adlam, A. (2001). Working              Miyake, A., Emerson, M., Padilla, F., & Ahn, J. (2004). Inner
   memory and the control of action: Evidence from task                     speech as a retrieval aid for task goals: the effects of cue
   switching. Journal of Experimental Psychology: General,                  type and articulatory suppression in the random task cuing
   130(4), 641-657. doi:doi:10.1037/0096-3445.130.4.641                     paradigm. Acta Psychologica, 115(2-3), 123-142.
Carlson, R. A. (1997). Experienced Cognition (1st ed.). Psy-                doi:10.1016/j.actpsy.2003.12.004
   chology Press.                                                        Morrison, C. M., Chappell, T. D., & Ellis, A. W. (1997). Age
Clark, A. (1997). Being There: Putting brain, body, and world               of Acquisition Norms for a Large Set of Object Names and
   together again. Cambridge, MA: MIT Press.                                Their Relation to Adult Estimates and Other Variables. The
Clark, A., & Karmiloff-Smith, A. (1993). The Cognizer's In-                 Quarterly Journal of Experimental Psychology A, 50, 528-
   nards: A Psychological and Philosophical Perspective on the              559. doi:10.1080/027249897392017
   Development of Thought. Mind & Language, 8(4), 487-519.               Roberson, D., & Davidoff, J. (2000). The categorical percep-
Dennett, D. (1994). The Role of Language in Intelligence. In                tion of colors and facial expressions: The effect of verbal in-
   What is Intelligence? The Darwin College Lectures. Cam-                  terference. Memory & Cognition, 28(6), 977-986.
   bridge University Press.                                              Roberson, D., Pak, H., & Hanley, J. R. (2008). Categorical
Emerson, M., & Miyake, A. (2003). The role of inner speech in               perception of colour in the left and right visual field is ver-
   task switching: A dual-task investigation. Journal of Mem-               bally mediated: Evidence from Korean. Cognition, 107(2),
   ory and Language, 48(1), 148-168.                                        752-762. doi:10.1016/j.cognition.2007.09.001
Flavell, J. H., Beach, D. R., & Chinsky, J. M. (1966). Sponta-           Rossion, B., & Pourtois, G. (2004). Revisiting Snodgrass and
   neous Verbal Rehearsal in a Memory Task as a Function of                 Vanderwart's object pictorial set: The role of surface detail
                                                                            in basic-level object recognition. Perception, 33(2), 217-
   Age. Child Development, 37(2), 283-299.
Gilbert, A., Regier, T., Kay, P., & Ivry, R. (2006). Whorf hy-              236.
   pothesis is supported in the right visual field but not the left.     Rumelhart, D., & McClelland, J. (1982). An Interactive Activa-
   Proceedings of the National Academy of Sciences of the                   tion Model of Context Effects in Letter Perception .2. the
   United States of America, 103(2), 489-494.                               Contextual Enhancement Effect and Some Tests and Exten-
Gleitman, H., Fridlund, A., & Reisberg, D. (2004). Psychology               sions of the Model. Psychological Review, 89(1), 60-94.
   (6th ed.). New York: Norton & Company.                                Rumelhart, D., Smolensky, D., McClelland, J., & Hinton, G.
Gleitman, L., & Papafragou, A. (2005). Language and thought.                (1986). Parallel Distributed Processing Models of Schemata
   In Cambridge Handbook of thinking and Reasoning (pp.                     and Sequential Thought Processes. In Parallel Distributed
   633-661). Cambridge: Cambridge University Press.                         Processing Vol II (pp. 7-57). Cambridge, MA: MIT Press.
Goldstein, K. (1948). Language and language disturbances.                Snedeker, J., & Gleitman, L. (2004). Why is it hard to label our
   New York: Grune & Stratton.                                              concepts? In D. G. Hall & S. R. Waxman (Eds.), Weaving a
Gopnik, A. (2001). Theories, language, and culture: Whorf                   Lexicon (illustrated edition., pp. 257-294). The MIT Press.
                                                                         Spivey, M. (2008). The Continuity of Mind. Oxford University
   without wincing. In Language acquisition and conceptual
   development (pp. 45-69). Cambridge, UK: Cambridge Uni-                   Press.
   versity Press.                                                        Thomas, M. S. C., Annaz, D., Ansari, D., Scerif, G., Jarrold,
James, W. (1890). Principles of Psychology. Vol. 1. New York:               C., & Karmiloff-Smith, A. (2009). Using developmental tra-
   Holt.                                                                    jectories to understand developmental disorders. Journal of
Lupyan, G. (2007a). Reuniting categories, language, and per-                Speech, Language, and Hearing Research: JSLHR, 52(2),
   ception. In D. McNamara & J. Trafton (Eds.), Twenty-Ninth                336-358. doi:10.1044/1092-4388(2009/07-0144)
   Annual Meeting of the Cognitive Science Society (pp. 1247-            Vygotsky, L. (1962). Thought and Language. Cambridge, MA:
   1252). Austin, TX: Cognitive Science Society.                            MIT Press.
Lupyan, G. (2007b). The Label Feedback Hypothesis: Linguis-              Winawer, J., Witthoft, N., Frank, M., Wu, L., Wade, A., &
   tic Influences on Visual Processing. PhD. Thesis. Carnegie               Boroditsky, L. (2007). Russian blues reveal effects of lan-
   Mellon University.                                                       guage on color discrimination. Proceedings of the National
Lupyan, G. (2008a). The Conceptual Grouping effect: Catego-                 Academy of Sciences of the United States of America,
                                                                            104(19), 7780-7785.
                                                                     1215

