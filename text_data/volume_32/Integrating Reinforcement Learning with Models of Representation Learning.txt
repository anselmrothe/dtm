UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Integrating Reinforcement Learning with Models of Representation Learning
Permalink
https://escholarship.org/uc/item/88x6f84q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Jones, Matt
Canas, Fabian
Publication Date
2010-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

       Integrating Reinforcement Learning with Models of Representation Learning
                       Matt Jones (mcj@colorado.edu) & Fabián Cañas (canas@colorado.edu)
                               University of Colorado, Department of Psychology & Neuroscience
                                                       Boulder, CO 80309 USA
                            Abstract                                  based on principles of attention learning from the
   Reinforcement learning (RL) shows great promise as a model
                                                                      categorization literature (Kruschke, 1992; Nosofsky, 1986).
   of learning in complex, dynamic tasks, for both humans and         Two sets of simulation studies are reported, which
   artificial systems. However, the effectiveness of RL models        demonstrate both the power and the psychological validity
   depends strongly on the choice of state representation,            of this approach.
   because this determines how knowledge is generalized among
   states.      We introduce a framework for integrating                               Reinforcement Learning
   psychological mechanisms of representation learning that
   allows RL to autonomously adapt its representation to suit its     RL comprises a family of algorithms for learning optimal
   needs and thereby speed learning. One such model is                action in dynamic environments. RL models characterize a
   formalized, based on learned selective attention among             task as a Markov Decision Process, in which the
   stimulus dimensions. The model significantly outperforms           environment at any moment exists in one of a set of states,
   standard RL models and provides a good fit to human data.          each associated with a set of actions available to the learner.
   Keywords: Reinforcement learning; attention; generalization        The chosen action determines both the immediate reward
                                                                      received, if any, and the state of the environment on the next
                         Introduction                                 time step. This general framework encompasses most tasks
Most challenging tasks people face are inherently dynamic             of psychological interest (Sutton & Barto, 1998).
and interactive. Choices affect not just immediate outcomes              The key insight behind most RL algorithms is to learn a
but also future events, and hence subsequent decisions that           value for each possible state or action. This value represents
must be made. Normative and descriptive theories of                   the total future reward that can be expected starting from
learning in dynamic environments have advanced                        that point. Formally, given any state s and action a, the
dramatically in recent years with the development of                  state-action value is defined as
Reinforcement Learning (RL), a mathematical and                                                                       
                                                                              Q(s, a) = E ∑ γ k rt+k s t = s, at = a,        (1)
computational theory drawing on machine learning,                                           k≥0                       
psychology, and neuroscience (e.g., Schultz, Dayan, &
Montague, 1997; Sutton & Barto, 1998).                                where t is the current timestep; s, a, and r are the state,
   However, RL currently faces a fundamental challenge                action, and received reward on each step; and γ ∈ [0, 1] is a
relating to the issue of knowledge representation. Dynamic         € discount factor representing the relative value of immediate
tasks tend to be highly complex, with an enormous number              versus delayed rewards. This approach allows action
of possible states (situations) that can arise. Therefore,            selection to be based directly on the Q-values. Here we use
efficient learning must rely on generalization from past              a Luce-choice or Gibbs-sampling rule, with inverse-
states that are similar to the current one. Similarity, in turn,      temperature parameter θ.
depends on how states are represented, including the                          P(a t = a) ∝ e θ ⋅Q(s t ,a)                      (2)
features by which they are encoded and the relative attention
allocated to those features (Medin, Goldstone, & Gentner,                Once an action is selected, its value is updated according
1993). Thus representation is critical to the effectiveness of        to the immediate reward and the values associated with the
RL algorithms, because representation determines the               € state that follows. One of the best-studied algorithms for
pattern of generalization by which past experience is used to         learning action values, Q-learning (Watkins & Dayan,
make new decisions.                                                   1992), uses the update rule
   Although there has been little research on representation                  ΔQ(st , a t ) = ε val ⋅ δ ,                      (3)
in the context of RL, representation and representation
learning have long been topics of psychological study.                where εval ∈ (0, 1] is a learning rate, and δ is the temporal-
Empirical research in a number of domains, including                  difference (TD) error, defined as
perceptual learning, attention, categorization, object             €          δ = rt + γ ⋅ max{Q(s t+1 , a)} − Q(s t , a t ) . (4)
recognition, and analogy, has uncovered principles and                                      a
mechanisms by which people learn to modify how they                      The TD error represents the difference between the
encode objects and situations in the service of learning,             original action-value estimate, Q(st, at), and a new estimate
inference, and decision making. Here we describe a                 € based on the immediate reward and ensuing state. The
framework for a natural synthesis of these ideas with RL              expression maxa{Q(st+1, a)} represents an estimate of the
algorithms, which leads to models that learn representations          value of the new state, st+1, based on the best action that
for dynamic tasks. A specific model is presented that is              could be performed in that state.
                                                                  1258

   The simplest implementations of Q-learning and other RL          previously encountered exemplars (s'), weighting each
algorithms use a tabular (i.e., lookup-table) representation,       exemplar by its association to c.
in which a different set of action values is learned separately             E(s,c) = ∑ sim(s, s')⋅ w(s',c)                   (5)
for every possible state that can occur. Tabular                                        s'
representations are impractical for most realistic tasks,
because the number of states grows exponentially with the              The property of exemplar models most relevant to the
number of state variables. Therefore, most implementations          current  investigation is the similarity function. Rather than
of RL utilize some form of generalization, whereby €                being   fixed,   a large body of evidence indicates that
knowledge about one state is extended to other, similar             similarity  changes       during the course of learning as a
states. This approach dramatically speeds learning by               consequence     of     shifts   of attention among the stimulus
reducing the amount of information that must be retained            dimensions    (e.g,     Nosofsky,      1986). This flexibility is
and updated, and by allowing the learner to draw on a richer        modeled by expressing similarity as a decreasing function of
set of past experiences when making each new decision.              distance in psychological space, with each stimulus
   Central to the success of generalization in all learning         dimension, i, scaled by an attention weight, αi (Nosofsky,
tasks (not just RL) is the choice of representation. In order       1986). Here we assume an exponential similarity-distance
for generalization to be effective, states (or stimuli in           function, in accord with empirical evidence and normative
general) must be encoded so that stimuli that are perceived         Bayesian analysis (Shepard, 1987).
or treated as similar tend to be associated with similar                                     −∑ α s − s ′
                                                                            sim(s, s ′) = e i i i i                          (6)
outcomes or appropriate actions (Shepard, 1987). Such a
representation facilitates generalization, and hence learning,         The effect of attention on similarity is to alter the pattern
because it leads the learner to draw on precisely those past        of generalization between stimuli so as to fall off more
experiences that are most relevant to the current situation.     € rapidly with differences along dimensions with greater
   Unfortunately, the choice of representation is a                 attention weights. When stimuli differ only on unattended
notoriously difficult problem, and the field of machine             dimensions, their differences are unnoticed and hence
learning is far from having automated algorithms that               generalization between them is strong. This adaptation of
discover useful representations for learning novel tasks.           generalization leads to improved performance when
Successful applications of RL have instead tended to rely on        attention is shifted to task-relevant dimensions, because the
hand-coded human knowledge for encoding states. For                 learner generalizes between stimuli that have common
example, the state representation in Tesauro’s (1995)               outcomes while discriminating between stimuli that are
celebrated backgammon program, TD-gammon, was based                 meaningfully different.
on complex features (configurations of pieces) suggested by            The influence of attention on generalization has extensive
expert human players. Likewise, psychological research in           support, both theoretically (Medin et al., 1993) and
RL generally avoids the problem of representation by using          empirically (Jones, Maddox, & Love, 2005; Nosofsky,
small sets of stimuli with clearly defined features, so that        1986). An important question suggested by this research is
the subject’s representation can be confidently assumed by          how attention can be learned. One proposal is that attention
the modeler and is unlikely to change during the course of          weights are updated in response to prediction error
learning (e.g., Fu & Anderson, 2006).                Arguably,      (Kruschke, 1992). In a classification task, prediction error
representation is where the real challenge often lies, and          (δ) is simply the difference between the category evidence,
therefore starting a model with a hand-coded representation,        E(s, c), and the actual category membership given as
or using experimental stimuli with unambiguous features,            feedback to the learner (e.g., +1 if s ∈ c and -1 otherwise).
presupposes the most difficult and interesting aspects of           The updating rule for attention is then based on gradient
learning (Schyns, Goldstone, & Thibaut, 1998).                      descent on this error, squared and summed over categories.
     Selective Attention in Category Learning                                                 ∂ 1        2
                                                                                                            
                                                                            Δα i = −ε att ⋅        2 ∑δ c  .               (7)
                                                                                            ∂α i  c 
One behavioral domain in which generalization and
representation have been extensively studied is category               This mechanism for attention learning has been
learning. Much of the research on category learning has             implemented in ALCOVE, a highly successful model of
aimed to understand the internal representations that humans € human category learning (Kruschke, 1992). ALCOVE
develop to facilitate classification of objects and inference       learns to shift attention to stimulus dimensions that are most
of unobserved features. All of these models serve, in one           relevant to predicting category membership and away from
way or another, to allow generalization of category                 dimensions that are non-diagnostic. This leads to adaptation
knowledge from previously encountered to novel stimuli.             of generalization, which in turn speeds learning.
   The most direct mechanism for generalization in
categorization is embodied by exemplar models (Medin &                             Attention Learning in RL
Schaffer, 1978).      In these models, the psychological
                                                                       Because of the strong empirical support for attention
evidence (E) in favor of classifying a stimulus (s) into a
                                                                    learning in the categorization literature, we believe it is a
given category (c) is given by summing its similarity to all
                                                                    potentially fruitful topic for study in the context of RL.
                                                                1259

  Selective attention may be especially relevant in this domain         analogously to both Q-learning (Eq. 3) and ALCOVE, using
  because most interesting RL tasks have complex state                  the same TD error signal as in Q-learning (Eq. 4).
  spaces of high dimensionality, and learning to distinguish                    Δw(s t , at ) = εval ⋅ δ                       (9)
  relevant from irrelevant dimensions should be expected to
  greatly speed learning in such tasks.                                    Similarity between states in Q-ALCOVE is defined
     The present investigation addresses two questions                  identically to stimulus similarity in ALCOVE (Eq. 6),
  regarding the relationship between attention learning and € except that a normalization term is included that fixes the
  RL. The first question is a psychological one, of whether             total similarity (i.e., the integral of the generalization
  attention learning as observed in supervised tasks such as            gradient) to 1. We have found that attention learning in
  categorization also operates in the dynamic tasks modeled             tasks requiring continuous prediction only functions well
  by RL. This extrapolation is not trivial, because RL relies           when normalization is included.
  on TD error, which is an internally generated signal based in            Learning of attention weights follows the same rule as in
  part on the learner’s own value estimate of the ensuing state         ALCOVE, except for the critical substitution of
  (see Eq. 4). It is an empirical question whether this internal        classification error with TD error. In addition, we only
  signal can drive attention shifts and other forms of                  differentiate δ with respect to Q(st, at) and not Q(st+1, a),
  representation learning in the same way that external                 because the motivation behind Q-learning is to use Q(st+1, ⋅)
  feedback does. A companion paper (Cañas & Jones, 2010)                to adjust Q(st, ⋅). Nevertheless, changing α also affects
  reports a behavioral experiment that supports an affirmative          Q(st+1, ⋅), and further analytical work is needed to
  answer to this question, and the data from that experiment            understand the impacts of this fact on model behavior and
  are modeled below.                                                    predictions. The resulting rule for attention learning is
     The second question is a computational one, of whether
                                                                                                   ∂
  the formal equations that describe RL and attention learning                  Δα i = εatt ⋅ δ ⋅      Q(s t ,at )            (10)
  can be coherently integrated, and whether the resulting                                         ∂α i
  model will exhibit efficient learning. This normative                    The intuition behind attention learning in Q-ALCOVE is
  question is important psychologically because computation-            that, after feedback, the model adjusts attention weights to
  al power constitutes a significant motivation for expecting           reduce generalization from states that contributed to error
  attention learning to play a role in human RL. If the two are € and to increase generalization from states that suggested
  computationally compatible, then the potential significance           more correct predictions. Over the course of experience,
  of RL is greatly increased, in that RL is capable of auto-            attention should shift to those dimensions that are most
  nomously adapting the representations on which it operates.           diagnostic of correct actions and their values.
     Comparison of the equations describing Q-learning and
  attention learning reveals there is indeed a natural, highly                                Simulation Studies
  complementary integration. The strength of Q-learning, and
  RL algorithms in general, is in the sophisticated updating               Two sets of simulations were carried out to evaluate the
  signals they compute, which take into account both external           behavior    of Q-ALCOVE. The first set was based on
  reward and internal consistency of value estimates (Eq. 4).           Gridworld, a common benchmark task for RL models.
  The updating itself is fairly trivial, consisting of adjusting        These simulations aimed to test whether the attention-
  the existing estimate by a proportion of the error (Eq. 3).           learning mechanism in Q-ALCOVE operates as predicted,
  Attention learning, and models of category learning more              to shift attention toward relevant dimensions and away from
  generally, have the opposite character. Their updating                irrelevant dimensions. If so, a second question was whether
  signals are fairly trivial (prediction error relative to external     selective attention leads to significant improvements in
  feedback), but the updates themselves are complex, driving            learning speed, and how such a potential advantage depends
  adaptation of sophisticated internal representations. This            on the dimensionality of the task. The second set of
  complementary relationship suggests the solution of using             simulations was based on a human behavioral experiment
  the TD error signal from RL to drive representation                   (Cañas & Jones, 2010) designed to test whether humans can
  learning, and in particular to update attention weights.              learn selective attention using internal value estimates (i.e.,
     We refer to the model resulting from this integration as Q-        TD error) as feedback, as proposed here. These simulations
  ALCOVE.         Q-ALCOVE estimates action values via                  aimed to evaluate Q-ALCOVE’s viability as a psychological
  similarity-based generalization among states, directly                model.
  analogous to ALCOVE (Eq. 5).
                                                                        Directional Gridworld
          Q(s,a) = ∑ sim(s, s')⋅ w(s',a)                    (8)
                    s'
                                                                        Gridworld is a class of tasks with a long tradition as a
                                                                        benchmark for RL algorithms (e.g., Sutton & Barto, 1998).
  The Q-values are used to generate action probabilities                The states of a Gridworld task form a rectangular lattice of
  according to the response-selection rule used by both Q-              dimensionality D. We call the present task Directional
€ learning and ALCOVE (Eq. 2). The w parameters, which                  Gridworld, because it was set up in such a way that one
  act as pre-generalization action values, are updated                  dimension was relevant and the others were irrelevant.
                                                                    1260

            Before                          After                                       2
                                                                                       1.5                                            Dimensions
                                                                                        1
                                                                                                                                             3
                                                                                                                                             4
                                                                         Reward Rate
                                                                                       0.5                                                   5
                                                                                                                                             6
                                                                                        0
                                                                                                                                             7
                                                                                   −0.5
                                                                                                                      Model                  8
                                                                                       −1
                                                                                                                             Q-ALCOVE
                                                                                                                             Fixed Generalization
Figure 1.       State space for 3-dimensional Directional
                                                                                   −1.5
Gridworld task. Grey states are goal states. Black cloud                                     0   500   1000   1500   2000   2500   3000
depicts Q-ALCOVE’s generalization gradient, at the start of                               Step
learning (left) and after 300 time steps (right).                    Figure 2. Learning curves in Directional Gridworld for Q-
                                                                     ALCOVE and version with fixed generalization.
   Figure 1 illustrates the Directional Gridworld task for the
case of D = 3 (the generalization gradients in the figure are           Two models were simulated in addition to Q-ALCOVE.
discussed below). Each dimension has 7 levels, for a total           The first was tabular Q-learning, which learns actions values
of 7D states. In each state, the learner has 2D available            independently for all states. The second was a fixed-
actions, corresponding to motion in either direction along           generalization model obtained from Q-ALCOVE by setting
any dimension. For simplicity, we assume that actions are            the attention-learning rate, εatt, to 0. Q-ALCOVE was run
deterministic and move the learner by 1 step in the chosen           using εatt = .01. All models were run with value-learning
direction. Actions on the boundaries that would take the             rate εval = 1 and choice parameter θ = .5. The models’ value
learner outside the space have no effect.                            estimates (w or Q) were initialized at 0 at the start of each
   States are encoded as vectors corresponding to their              run. The initial values for attention weights were set to .4
values on the D dimensions. Other than this, the model has           for both Q-ALCOVE and the fixed-generalization model.
no prior knowledge of the topology of the environment or of          This value was chosen so as to maximize performance of
the meanings or effects of actions. The spatial interpretation       the fixed-generalization model on 3 dimensions.
is only a convenient metaphor, and the task is not meant as a           Figure 2 shows average learning curves for Q-ALCOVE
model of spatial navigation that might involve specialized           and the fixed-generalization model for Directional
psychological mechanisms. The stricter interpretation is as          Gridworlds of 3 to 8 dimensions. Performance for tabular
an abstract problem space (e.g., Newell & Simon, 1972).              Q-learning was poor enough, especially at higher
   The highlighted states (Fig. 1) spanning the center of the        dimensionalities, that it is omitted. Each curve indicates
space are goal states. Whenever the learner reaches a goal           reward rate, smoothed with a rectangular window of 100
state, a reward of 10 is provided. On the next step, the             time steps, and averaged over 4 separate runs of the model.
learner is taken to a random state maximally distant from            As can be seen, Q-ALCOVE learns more quickly with
the goal region. All actions that do not lead to a goal              attention learning than without, and the magnitude of this
produce a reward of -1. The learner’s task is to choose              advantage grows rapidly with the number of dimensions.
actions so as to maximize total temporally discounted                This result suggests that attention plays an indispensable
reward (Eq. 1, with γ set to .5). Thus, optimal behavior             role in natural tasks of much higher dimensionality.
consists of repeatedly moving in a straight line from the               Figures 1 and 3 illustrate how Q-ALCOVE’s attention-
boundary to the nearest goal state.                                  learning mechanism facilitates learning, in the case of three
   For all values of the dimensionality D, the goal states           dimensions. Figure 3 shows the attention weights for a
form a hyperplane through the center of the space. The               single run, which increase for the relevant dimension and
dimension perpendicular to the goal region is relevant to            decrease toward 0 for the irrelevant dimensions. This shift
optimal action selection, as the learner needs to move in            of attention leads to the change in the generalization
opposite directions depending on which side of the goal              gradient depicted in Figure 1. The initial gradient (left) is
region it is on. All other dimensions are irrelevant. Indeed,        spherical, reflecting the model’s lack of knowledge of the
it can easily be shown that the optimal Q-values for any             dimensions’ predictive validities. After 300 time steps
state depend only on the state’s position on the relevant            (right), the gradient has been reshaped so that there is strong
dimension. Therefore, the most efficient generalization              generalization between states as long as they match on the
strategy for learning Q-values is to average over all states at      relevant dimension and very little generalization otherwise.
each level of the relevant dimension but to learn separate           Thus the model has learned the anisotropy of the task, which
values for each level. This strategy can be achieved by              allows it essentially to estimate a common set of Q-values
strong attention to the relevant dimension and zero attention        for all the states at each stratum (as an average over all the
to all other dimensions. A primary question was whether Q-           ws), while keeping the values for different strata separate.
ALCOVE would learn such an attention distribution.
                                                                  1261

                                                                    Tabular Q-Learning        Fixed Gen.          Q-ALCOVE
  Attention weights (α)
                                           Relevant
                                           Irrelevant
                             Step                                   Figure 4. Q-values obtained from all three models after 300
                                                                    steps in 3-dimensional Directional Gridworld. Shown is a
Figure 3. Dynamics of attention weights for one run of Q-
                                                                    2-dimensional slice through the center of the state space.
ALCOVE in 3-dimensional Directional Gridworld.
                                                                    Arrows in each state correspond to the four actions within
   The consequences of all three models’ patterns of                the plane. Darker arrows indicate greater Q-values.
generalization are illustrated in Figure 4, which shows a
                                                                    allow subjects to learn to choose Action 1 so as to maximize
two-dimensional slice through the center of the three-
                                                                    the probability of obtaining Stimulus 2a (the more valuable
dimensional state space. Within each state, the four arrows
                                                                    mushroom). The key additional prediction of Q-ALCOVE
indicate the model’s estimated Q-values for the four actions
                                                                    is that TD error will also drive learning of attention to the
within the plane. These values are from a single run of each
                                                                    more relevant dimension of Stimulus 1, to improve the
model, after 300 time steps. Darker arrows indicate greater
                                                                    effectiveness of generalization among stimuli.
Q-values. The values for tabular Q-learning are irregular,
                                                                       Results revealed that subjects who learned the first step of
reflecting the fact that they were learned separately for each
                                                                    the task also learned to selectively attend to the more
state. In most states there has not been enough experience
                                                                    relevant dimension (see Cañas & Jones, 2010, for details).
to obtain reliable estimates. The Q-values estimated by the
                                                                    Simulations of Q-ALCOVE corroborated this conclusion.
fixed-generalization model are more accurate, because each
                                                                    Q-ALCOVE and the fixed-generalization version of the
draws on knowledge from neighboring states, so experience
                                                                    model were fit to the data of each subject using maximum
is used more efficiently. However, there is still irregularity
                                                                    likelihood. Aggregating over all 150 subjects, Q-ALCOVE
among states at a given stratum (insufficient generalization
                                                                    fit reliably better, χ2(150) = 1913.8, p ≈ 0. The difference
across the irrelevant dimension) and too much smoothing
(excess generalization) along the relevant dimension. Q-            between fits of the models was significant at the .05 level
ALCOVE’s estimated Q-values are much more accurate,                 for 55 individual subjects. These results support the central
allowing the model to select correct actions more reliably.         hypothesis that attention learning, as embodied by Q-
                                                                    ALCOVE, was involved in learning the task.
The Spores Task
Psychologically, the core assumption of Q-ALCOVE is that
attention learning can be driven by internally generated TD-
error signals, not just overt feedback. A behavioral experi-
ment, reported by Cañas and Jones (2010), tested this
hypothesis using a two-step task, in which Action 1 deter-
                                                                               Figure 5. Structure of the Spores task.
mined Stimulus 2, but reward was not received until after
Action 2. The basic question was whether selective atten-
tion to the dimensions of Stimulus 1 could be learned, when
                                                                                           Conclusions
the only immediate feedback was the identity of Stimulus 2.         Despite its computational power and neurological support,
   Figure 5 illustrates the design of the task. Stimulus 1 (a       the basic principles behind RL are inherently limited by the
cartoon mushroom spore) varied along two dimensions and             representations it operates on. We argue here for a tight
was sampled from a circular set. This set was probabil-             linkage between RL and mechanisms of representation
istically divided into two regions, which had different             learning established in other domains of psychology. Speci-
consequences for the outcome of Action 1 (two options for           fically, TD error, the engine behind nearly all RL models,
how to grow the spore). The border between regions was              can also drive updating of state representations. Representa-
oriented so that one dimension was more relevant than the           tions thereby adapt so the pattern of generalization among
other. The second step was designed so that the two                 states is tuned to the structure of the task, which in turn
possibilities for Stimulus 2 (two colors of mushrooms) each         facilitates learning of optimal actions. RL’s capacity to auto-
had a different optimal choice for Action 2 (selling the            nomously drive construction of representations that serve its
mushrooms to a troll or a goblin). Under these optimal              needs greatly increases its power and flexibility, and hence
actions, Stimulus 2a led to more reward than Stimulus 2b.           its potential as a model of complex human learning.
   RL models in general predict subjects will learn internal           The specific model proposed here draws on principles of
values for Stimuli 2a and 2b (or their pairings with choices        selective attention from category learning and related
of Action 2), and these values will be used to generate             domains (Nosofsky, 1986; Sutherland & Mackintosh, 1971).
internal feedback (TD error) for Action 1. This will in turn        Shifting attention away from irrelevant dimensions allows
                                                                 1262

the learner to aggregate knowledge over states with similar                                  References
outcomes, while attention toward relevant dimensions
                                                                     Aha DW & Goldstone RL (1992). Concept learning and
maintains discrimination of meaningful differences.
                                                                       flexible weighting. Proceedings of the 14th Annual
Generalization in any learning task raises a bias-variance
                                                                       Conference of the Cognitive Science Society, 534-539.
dilemma, in that more generalization reduces variance in
                                                                     Cañas F & Jones M (2010). Attention and reinforcement
parameter estimates but increases their bias. Selective
                                                                       learning: Constructing representations from indirect
generalization as modeled by attention learning is an elegant
                                                                       feedback. Proceedings of the 32nd Annual Conference of
way of sidestepping this dilemma.
                                                                       the Cognitive Science Society.
  In a companion paper, we report empirical evidence
                                                                     Cristianini N & Shawe-Taylor J (2000). An Introduction to
supporting attention learning via TD error as a
                                                                       Support Vector Machines and Other Kernel-based
psychological mechanism (Cañas & Jones, 2010). Here we
                                                                       Learning Methods. Cambridge University Press.
show how such a mechanism can be formalized in a
                                                                     Fu W-T & Anderson JR (2006). From recurrent choice to
mathematical model. Attention learning and RL in this
                                                                       skill learning: A reinforcement-learning model. Journal of
model bootstrap off of each other, in that the internal value
                                                                       Experimental Psychology: General, 135, 184-206.
estimates generated by RL drive shifts of attention, and
                                                                     Jäkel F, Schölkopf B & Wichmann FA (2007). A tutorial on
selective attention in turn improves RL’s value estimates.
                                                                       kernel methods for categorization. Journal of
This synergistic relationship, together with the elegance of
                                                                       Mathematical Psychology, 51, 343-358.
the integration between the equations of Q-learning
                                                                     Jones M, Maddox WT & Love BC (2005). Stimulus
(Watkins & Dayan, 1992) and ALCOVE (Kruschke, 1992),
                                                                       generalization in category learning. 27th Annual Meeting
suggests that RL and attention learning are similarly tightly
                                                                       of the Cognitive Science Society, 1066-1071.
coupled in the brain. The simulation studies reported here
                                                                     Kruschke JK (1992). ALCOVE: An exemplar-based connec-
show that the unified model, Q-ALCOVE, is both
                                                                       tionist model of category learning. Psychological Review,
computationally powerful and psychologically plausible.
                                                                       99, 22-44.
  Investigating attention is a useful first step because it acts
                                                                     Love B, Medin D & Gureckis T (2004). SUSTAIN: A net-
to modify similarity directly, so that its effects on
                                                                       work model of category learning. Psychological Review,
generalization are transparent. In further work, we plan to
                                                                       111, 309-332.
explore more complex psychologically supported
                                                                     Markman AB & Gentner D (1993). Structural alignment
mechanisms, such as stimulus-dependent attention (Aha &
                                                                       during similarity comparisons. Cognitive Psychology, 25,
Goldstone, 1992), construction of new conjunctive features
                                                                       431-467.
(Love, Medin, & Gureckis, 2004), and analogical mapping
                                                                     Medin DL, Goldstone RL & Gentner D (1993). Respects for
between structured stimulus representations (Markman &
                                                                       similarity. Psychological Review, 100, 254-278.
Gentner, 1993).
                                                                     Medin DL & Schaffer MM (1978). Context theory of class-
  Psychological models that generalize knowledge based on
                                                                       ification learning. Psychological Review, 85, 207-238.
pairwise similarity are closely related to kernel methods
                                                                     Micchelli CA & Pontil M (2007). Feature space
developed in statistics (Jäkel, Schölkopf, & Wichmann,
                                                                       perspectives for learning the kernel. Machine Learning,
2007). Kernel methods add considerable flexibility to many
                                                                       66, 297-319.
learning algorithms, by allowing them to be recast from the
                                                                     Newell A & Simon HA (1972). Human problem solving.
raw stimulus space to a mathematical (Hilbert) space of
                                                                       Englewood Cliffs, NJ: Prentice Hall.
functions (e.g., Cristianini & Shawe-Taylor, 2000). Q-
                                                                     Nosofsky RM (1986).          Attention, similarity, and the
ALCOVE can be viewed as a kernel method applied to RL.
                                                                       identification-categorization relationship. Journal of
Viewed from the perspective of kernel methods, an
                                                                       Experimental Psychology: General, 115, 39-57.
important contribution of the present research is the
                                                                     Schultz W, Dayan P & Montague P (1997, March). Neural
proposal for adaptively modifying the kernel (i.e.,
                                                                       substrate of prediction and reward. Science, 275, 1593-
generalization gradient) to improve learning. Learning the
                                                                       1599.
kernel has been a focus of recent research in machine
                                                                     Schyns PG, Goldstone RL & Thibaut J-P (1998). Develop-
learning (e.g., Micchelli & Pontil, 2007), but results thus far
                                                                       ment of features in object concepts. Behavioral and Brain
have been largely limited to existence theorems and global-
                                                                       Sciences, 21, 1-54.
search algorithms that seem psychologically implausible.
                                                                     Shepard RN (1987). Toward a universal law of generalization
Here we propose a simpler mechanism based on
                                                                       for psychological science. Science, 237, 1317-1323.
psychological principles. The mathematical results on
                                                                     Sutherland NS & Mackintosh NJ (1971). Mechanisms of
kernel learning have been influential in guiding our design
                                                                       Animal Discrimination Learning. NY: Academic Press.
of well-behaved models and in inspiring more sophisticated
                                                                     Sutton R & Barto A (1998). Reinforcement Learning: An
mechanisms. Continuing to exploit this link to statistical
                                                                       Introduction. The MIT Press.
and machine-learning techniques, while maintaining
                                                                     Tesauro G (1995). Temporal difference learning and TD-
grounding in established psychological phenomena, seems
                                                                       gammon. Communications of the ACM, 38(3), 58-68.
promising for advancing the power and flexibility of
                                                                     Watkins CJCH & Dayan P (1992). Q-Learning. Machine
psychological models.
                                                                       Learning, 8, 279-292.
                                                                 1263

