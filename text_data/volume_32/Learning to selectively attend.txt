UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning to selectively attend

Permalink
https://escholarship.org/uc/item/7bn072ds

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Gershman, Samuel
Cohen, Jonathan
Niv, Yael

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Learning to Selectively Attend
Samuel J. Gershman (sjgershm@princeton.edu)
Jonathan D. Cohen (jdc@princeton.edu)
Yael Niv (yael@princeton.edu)
Department of Psychology and Neuroscience Institute, Princeton University
Princeton, NJ 08540 USA
Abstract

bias: Restricting attention to only a few dimensions is akin to
the belief that only those dimensions are relevant for earning
reward. This has the effect of reducing the space of possible
value functions to a much smaller subspace.
While Bayesian probability theory stipulates the ideal
learner, in general it may not be computationally tractable to
perform the necessary calculations exactly (Kruschke, 2006;
Daw & Courville, 2008). We therefore consider a “hybrid”
model that combines the computational efficiency of modelfree RL with the statistical efficiency of Bayesian inference.
We compare the Bayesian and hybrid models to naive RL (no
sparsity bias) and show that models that exploit structured
knowledge better capture choice behavior in our experiment.

How is reinforcement learning possible in a high-dimensional
world? Without making any assumptions about the structure of the state space, the amount of data required to effectively learn a value function grows exponentially with the state
space’s dimensionality. However, humans learn to solve highdimensional problems much more rapidly than would be expected under this scenario. This suggests that humans employ inductive biases to guide (and accelerate) their learning.
Here we propose one particular bias—sparsity—that ameliorates the computational challenges posed by high-dimensional
state spaces, and present experimental evidence that humans
can exploit sparsity information when it is available.
Keywords: reinforcement learning; attention; Bayes.

Introduction
Reinforcement learning (RL) in high-dimensional state
spaces is a notoriously difficult problem in machine learning (Sutton & Barto, 1998), primarily because of the curse
of dimensionality: The number of states grows exponentially
with dimensionality (Bellman, 1957), and thus if one were
naively to represent a separate value (expected reward) for
each state, one would require astronomical amounts of data to
effectively learn the value function (and thereby behave adaptively). Nonetheless, humans appear to learn rapidly from
small amounts of data. Thus, while substantial evidence has
accumulated that human behavior follows the predictions of
RL models (Dayan & Niv, 2008), these models may fundamentally underestimate the learning capabilities of humans.
Following work in other areas of cognition (Braun,
Mehring, & Wolpert, 2009; Kemp & Tenenbaum, 2009), we
suggest that rapid learning arises from the exploitation of
structured knowledge in the form of inductive biases. In particular, our proposal is that humans employ a sparsity bias:
the assumption that only one (or a small number) of dimensions (input features) is relevant at any given time for predicting reward. For example, when you are at a stoplight, only the
color of the light matters, not its shape, size, etc. In other domains (such as ordering food in a restaurant), you may know
that dimensional relevance is sparse, but not which particular dimensions are relevant (does it matter which restaurant
it is? which table I am sitting at? who the chef is? who
the waiter is?); for this purpose, one requires a learning algorithm that can exploit sparsity. We formalize this idea in
terms of rational statistical inference, and present new experimental evidence that human behavior is consistent with such
a model.
Central to our analysis is the idea that selective attention
is a direct consequence of Bayesian inference with a sparsity

The Computational Problem
For concreteness, we consider one particular example of the
general class of reinforcement learning problems for which
the sparsity assumption holds. This example is meant to capture the abstract structure of many problems facing humans in
the real world, where they must make choices between several multidimensional stimuli under conditions where most
dimensions are unpredictive of reward. This example will
also serve as a formal description of the task that we asked
human subjects to perform, the results of which we report in
a later section.
The subject plays N trials, and observes M stimuli simultaneously on each trial. The ith stimulus on trial n is denoted by
a D-dimensional vector xni , where each integer-valued component xni j indicates the property of the jth stimulus dimension (for instance, [color = green, shape = triangle, texture
= dots]). Each set of trials has a target dimension d (e.g.,
‘shape’) and target property f on that dimension (e.g., ‘circle’). The subject chooses a stimulus cn on each trial and
observes a binary reward rn . The probability of reward given
choice and target is

θ1 if xncn d = f
P(rn = 1|cn , d, f , Xn ) =
(1)
θ0 otherwise,
In other words, the subject receives a reward with probability
θ1 if the chosen stimulus posseses the target property on the
target dimension, and with probability θ0 otherwise.

Bayesian Model
Given uncertainty about the target dimension and property, a
Bayesian model would use Bayes’ rule to infer the posterior
over the target dimensions and property and then calculate the

1270

Function Approximation Models

value of the stimulus by taking the expectation of reward with
respect to the posterior:
Vn (c) = ∑ ∑ P(rn = 1|cn = c, d, f , Xn )P(d, f |Dn−1 , r1:n−1 ),
d

f

(2)
where Dn−1 = {X1:n−1 , c1:n−1 }. The intuition behind the
Bayesian model is that the model weights the expected reward in each possible state of the world (i.e., target dimension and property) by the probability of the world being in
that state given past observations. A key characteristic of this
model is that a complete posterior distribution is maintained
over states of the world, rather than a point estimate. The
posterior distribution used by the Bayesian model is given by
Bayes’ rule:
P(d, f |Dn−1 , r1:n−1 ) ∝ P(r1:n−1 |Dn−1 , d, f )P(d, f ),

D

(3)

Vn (c) =

∑ wn (d, xncd )φd ,

(7)

d=1

where the prior is assumed to be uniform and the likelihood
is given by:
n−1

P(r1:n−1 |Dn−1 , d, f ) = ∏ P(rt |ct , d, f , Xt ).

One reason why the naive RL model may be ineffective in this
task is that it lacks the ability to generalize across different
configurations of features. Intuitively, if you knew the target
dimension and property, then the value of a stimulus should
be independent of the properties on the non-target dimension.
However, the naive RL model yokes these together, such that
learning operates on configurations of properties and hence
fails to exploit this invariance. For example, the naive RL
model learns a different value for green triangles with dots
and for green triangles with waves, although the texture dimension may be completely incidental and not predictive of
reward.
A more structured RL model that generalizes across configurations can be obtained by constructing the value function
as a linear combination of D basis functions φ:

(4)

t=1

Note that although this model describes the optimal learning
rule, we shall assume that subjects are “weakly” rational in
their decision rule (see the softmax choice function described
below).

where the weight matrix Wn determines how the basis functions are combined, with one weight for each dimensionproperty pair. This type of model is known as a function approximation architecture (Sutton & Barto, 1998). RL is used
to update the weights according to:
wn+1 (d, xncd ) = wn (d, xncd ) + α∆n φd ,

(8)

where the prediction error ∆n is computed the same way as
in the naive RL model (Eq. 6). This update can be understood as performing gradient ascent on the value function by
optimizing the weight parameters (Williams, 1992).
We will consider a family of basis functions parameterized
by η:

Reinforcement Learning Models
We now consider several alternative models based on RL. The
intuition behind these models is that what ultimately matters
for the choice value is the expectation under the posterior; so
incrementally updating an estimate of this expectation from
experience will eventually converge to the optimal choice values, even though these updates do not make optimal use of
information on each trial. The various RL models differ principally in their construction of the value function.

η

φd =

Pd
η,
∑ j Pj

(9)

(6)

where Pd = ∑ f P(d, f |Dn−1 , r1:n−1 ). The basis function can
be thought of as an “attentional focus” that encodes the
subject’s beliefs about what dimension is currently relevant.
Thus, rather than maintaining the full posterior over target
dimension and property (which may be quite computationally expensive), with the function approximation model the
subject maintains the marginal posterior over target dimension (i.e., the probability of a dimension being the target, averaging over different target properties), which is then used
to weight separate value functions, one for each dimension.
When reward feedback is received, credit (or blame) is assigned to each value function in proportion to its posterior
probability.1 We refer to this model as the hybrid model, because it combines properties of RL and Bayesian inference.
Different settings of η lead to several special cases of interest:

Although the optimal solution is learnable by this model, its
learning will be very slow, as we describe in the next section.

1 Note that the subject need not maintain and update the full posterior; any procedure that estimates the marginal posterior directly
is consistent with this formulation.

Naive RL Model
The naive RL model represents a separate value for every
possible stimulus-dimension-property configuration. Specifically, the choice value estimate is given by Vn (c) = vn (xnc ).
This estimate is updated according to the learning rule:
vn+1 (xncn ) = vn (xncn ) + α∆n ,

(5)

where α is a learning rate and ∆n is the prediction error:
∆n = rn −Vn (c).

1271

• η = 0: uniform weighting of dimensions (diffuse focus).
• η = 1: exact posterior weighting of dimensions (optimal
focus).
• η → ∞: maximum a posterior (MAP) weighting (myopic
focus).
Other intermediate scenarios are also possible. Thus, the
value of η tells us how much information about the posterior distribution the subject is using to focus attention, with
η = 1 being optimal focus and η = 0 completely ignoring
information from the posterior and attending equally to all
dimensions.2 When η is larger than 1, the subject discards
posterior uncertainty by focusing on the mode of the distribution, and is therefore overconfident in her beliefs about the
relevant dimension.
One way to interpret the function approximation model is
as a neural network in which the basis functions represent
attentional units focusing on different sensory channels, and
the weights represent synaptic connections between the attentional units and a reward prediction unit (Figure 1). The
synaptic weights are updated using RL (Eq. 8). This interpretation resonates with ideas in computational neuroscience
that view the dorsolateral prefrontal cortex as encoding attentional or task-related biases that interact with a striatal reward
prediction system (Braver, Barch, & Cohen, 1999; Rougier,
Noelle, Braver, Cohen, & O’Reilly, 2005; Todd, Niv, & Cohen, 2009). The prediction error ∆ driving the weight updates
is thought to be signaled by midbrain dopaminergic afferents
to the striatum (Schultz, Dayan, & Montague, 1997)
reward

Midbrain

Striatum

Reward prediction
(value) unit

w

Δ

Prefrontal cortex

ϕ

Attentional units

by the intra-dimensional/extra-dimensional set-shifting task
(Dias, Robbins, & Roberts, 1996; Owen, Roberts, Polkey,
Sahakian, & Robbins, 1991), in which subjects are asked to
discriminate between visual stimuli on the basis of a particular (but unknown) dimension which they must learn from
feedback, as well as the Wisconsin card-sorting task (Milner,
1963). We have adapted this task to a multi-armed bandit
setting, such as has been used in previous studies of reinforcement learning (e.g., Daw, O’Doherty, Dayan, Seymour,
& Dolan, 2006; Schonberg, Daw, Joel, & O’Doherty, 2007).

Participants and Stimuli
Sixteen participants received 12 dollars reimbursement for
performing 1800 trials of the task. The stimuli were triplets
of stimuli varying along three dimensions: color (red, yellow,
green), shape (circle, triangle, square), and texture (waves,
dots, lattice). An example triplet is shown in Figure 2.

Figure 2: Example experimental stimuli.

Procedure
For each game, the target dimension and property were chosen randomly and with equal probability. On each trial, the
subject was presented with a random triplet and asked to
choose one of the stimuli. The stimuli on each trial were generated by a random permutation of the property assignments.
After making the choice, the subject received feedback about
whether or not her choice resulted in a reward. If the subject chose the stimulus with the target dimension/property
pair, she received a reward with probability 0.75. Otherwise, reward was delivered with probability 0.25. The targets changed on each game (lasting 20-30 trials), and subjects
were informed when a new game was beginning.

Choice Probabilities
Sensory units

Extrastriate cortex

To map from values to choices, we define a policy πn that
specifies the probability πn (c) of making choice c on trial n.
Here we use the “softmax” policy defined by

x
Figure 1: Neural network interpretation of the hybrid
model.

Method

eβVn (c)
,
∑a eβVn (a)

(10)

where β is an inverse temperature parameter that allows us to
model stochasticity in the subject’s responses.

We now describe a behavioral experiment designed to quantitatively evaluate these models. Our experiment was inspired
2 It

πn (c) =

Parameter Estimation and Model Comparison

is important to note that diffuse focus is not the same as the
naive RL model. For all values of η, the function approximation
model is still able to generalize across different configurations, unlike the naive RL model.

We fit the parameters of each model with MAP estimation
using gradient descent and calculated the Laplace approximation (Kass & Raftery, 1995) to the log marginal likelihood

1272

(evidence) for each model m according to:

too high later in a game. No single value of the inverse temperature would be able to capture this pattern.

Z

Em = ln

P(ωm )P(c|ωm )dωm
ωm

1
1
≈ ln P(ω̂m ) + ln P(c|ω̂m ) + Gm ln 2π − ln |Hm |, (11)
2
2
where ωm is the set of parameters for model m, P(c|ω̂m ) =
∏Nn=1 πn (cn |ω̂m ), ω̂m is the MAP estimate of the parameters,
Gm is the number of parameters (length of ωm ), and Hm is
the Hessian matrix of second derivatives of the negative logposterior evaluated at the MAP estimate. We then calculated
the log Bayes Factor relative to chance (where all choices are
equiprobable) according to Em − N ln(1/3). A larger Bayes
Factor indicates greater support for a model. Note that the
chance (null) model has no parameters. In addition to comparing models based on Bayes Factors, we also calculated
predictive log-likelihood on a held-out game using a leaveone-out cross-validation procedure.
For all the models, we fit an inverse temperature β, placing on it a Gamma(2,2) prior. This served to ameliorate a
well-known degeneracy in models with both a temperature
and learning rate, such that these parameters tend to tradeoff against each other (inverse temperature becoming very
large and learning rate very small). For the RL models, we
fit a learning rate α, placing on it a Beta(1.2,1.2) prior, which
slightly biases the fits away from the parameter boundaries.
We also allowed θ1 and θ0 to vary across subjects, since we
only told subjects that the target would be rewarding more
often than non-targets, placing on θ1 a Beta(12,4) prior and
on θ0 a Beta(4,12) prior; these priors were chosen to have
as their expected value the true—but unknown—values of θ1
and θ0 . Finally we placed a Uniform(0,10) prior on η.

Results
Figure 3 presents the log Bayes Factors for each model,
summed across subjects, along with the cross-validation resutls. Zero represents the null (chance) model in both cases.
Clearly all the models do better than chance, but the naive RL
model appears to perform substantially worse than the others.
Overall, the hybrid model appears to best match behavior on
this task. Figure 4 displays the distribution of log Bayes Factors for the Bayesian and hybrid models, showing that there
are also individual differences in which model is favored for
each subject.
Additional insight into these models can be gained by inspecting aggregate learning curves (the probability of choosing the optimal stimulus as a function of trials within a game).
As shown in Figure 5, the naive RL model appears to consistently underestimate the speed of learning exhibited by
subjects, whereas both the Bayesian and hybrid models hew
closely to the empirical learning curve. One peculiarity of
the learning curve is that subjects appear to learn faster than
the Bayesian model. We believe that this is an artifact of the
softmax choice probability function: the inverse temperature
parameter appears to be too low early in a game and slightly

Bayesian
Hybrid
Naive

Log Bayes Factor
5425
5892
3307

Held-out log-likelihood
5620
6208
3312

Figure 3: Model comparison results. Highest scores are
shown in bold.

Figure 4: Comparison of Log Bayes Factors for Bayesian
and hybrid models. Points above the diagonal are favored
by the hybrid model. The red shaded region indicates the
confidence interval outside of which one model is more likely
than the other with p < 0.05.
Another question we can ask is whether subjects who behave more in accordance with the Bayesian model or hybrid
model earn more reward overall. Figure 6 does indeed show
this relationship (measured as the correlation between reward
earned and the log Bayes Factors for the Bayesian model relative to the hybrid model), suggesting that subjects who more
effectively exploit the structure of the task tend to perform
better. The correlation is significant at p < 0.01. The hybrid
model log Bayes Factor relative to the null model also correlates with total reward (p < 0.02).
Figure 7 shows the parameter estimates for η on a logscale, demonstrating that subjects cluster around 0, corresponding to exact posterior weighting (optimal focus). This
was supported by a sign test which failed to reject (p=0.45)
the null hypothesis that ln(η) was drawn from a distribution
with 0 median. Thus, within the family of possible basis functions, posterior attentional weighting best describes human
behavior on this task.

Discussion
In this paper we have posed a problem that humans face in everyday life: how to learn value functions in high-dimensional
state spaces. The crucial assumption that makes this possible

1273

MAP

Exact

Uniform

Figure 5: Learning curves. Probability of choosing the optimal stimulus as a function of trial within a game. The circles represent the empirical choice probability. Error bars are
standard errors of the mean.

Figure 7: Boxplot of ln(η) estimates.

550

Total reward

500

450

400

350

300

250
−150

−100

−50

0

50

100

Log Bayes Factor (Bayesian − Hybrid)
Figure 6: Individual differences in earned reward. On the
x-axis is plotted the log Bayes Factors of the Bayesian model
relative to the hybrid model, and on the y-axis is plotted the
total reward earned. A least-squares line is superimposed on
the scatter plot.
is that only one or a few dimensions is relevant at any given
time. By employing this sparsity bias in the machinery of
Bayesian inference, the effective dimensionality of the problem is reduced. This can be understood as a kind of selective
attention that is learned through experience.
Our experimental results demonstrate that humans can exploit sparsity information when it is available. We compared a
Bayesian model and a family of sophisticated RL algorithms
against a naive RL model that ignores sparsity information
and hence does not generalize across stimulus configurations,
the key ingredient to efficient learning. Our computational
analysis of behavior on this task suggests that humans combine reinforcement learning with Bayesian inference, rather
than using a purely Bayesian strategy. This makes sense if
the brain’s learning algorithms are designed to deal with highdimensional problems for which exact Bayesian inference is

intractable. The hybrid model represents a compromise between the computational efficiency of model-free RL and the
statistical efficiency of Bayesian inference.
The idea that selective attention can be framed as the outcome of Bayesian inference has been explored by several authors (Dayan, 2009; Rao, 2005; Yu, Dayan, & Cohen, 2009).
Most relevant to our work is the competitive combination
model of Dayan, Kakade, and Montague (2000), in which
stimuli are assumed to vary in how reliably they predict reward. Dayan et al. (2000) showed that selective attention
to particular stimuli falls naturally out of inference over the
causal relationships between stimuli and reward in such a
model. Our work is conceptually similar, with the main difference that we model inference over dimensions, rather than
just stimuli. As emphasized by Dayan et al. (2000), the selectivity of attention in our model is based on proceses of
statistical inference, rather than resource constraints. This
point is particularly important to explaining how attention is
learned; resource-limitation models, without further elaboration, do not speak to this issue.
The central role of selective attention has been extensively explored in the category learning literature, notably
by Nosofsky (1986) and Kruschke (1992). The basic idea
is that learned attentional weights amplify or attenuate specific stimulus dimensions in a way that facilitates category
discrimination. Recently, Kruschke (2006) has attempted to
connect these ideas to a form of approximate Bayesian inference he dubs “locally Bayesian learning” (LBL). Much as in
our work, attention arises in LBL as a consequence of weighting different hypotheses about the currently relevant stimulus
dimension in response to new evidence. In this sense, LBL
is form of hybrid model; here we have attempted to identify
a continuum through which Bayesian knowledge can influence RL, and fit this to data to identify where human learners fall on this continuum. At the same time, although our
model shares some characteristics with categorization models such as LBL (see also Heller, Sanborn, & Chater, 2009),
it is important to note that the problem it is designed to solve

1274

is conceptually different: it does not receive feedback indicating the correct response (as in supervised category learning),
but must instead learn from probabilistic reward feedback.
While our work was partly inspired by earlier neural network models (Braver et al., 1999; Rougier et al., 2005), our
goal in this paper was to step away from implementational
details and interrogate computational- and algorithmic-level
concerns. Future work will need to examine more systematically how the algorithmic ideas presented here map onto
neural mechanisms. We are currently investigating this question with functional magnetic resonance imaging.
In conclusion, the main theoretical and experimental contribution of this paper is showing that the human RL system is
more sophisticated than previous computational models have
given it credit for. This may not, after all, be that surprising;
many years of machine learning research have shown that the
naive assumptions of previous models simply do not scale
up to high-dimensional real world problems. It remains to
be seen what other hidden sophistications in the RL system
await discovery.

Acknowledgments
We thank Michael Todd for invaluable discussion. SJG
was supported by a Quantitative Computational Neuroscience
training grant from the National Institute of Mental Health.

References
Bellman, R. (1957). Dynamic Programming. Princeton, NJ:
Princeton University Press.
Braun, D., Mehring, C., & Wolpert, D. (2009). Structure
learning in action. Behavioural Brain Research.
Braver, T., Barch, D., & Cohen, J. (1999). Cognition and control in schizophrenia: A computational model of dopamine
and prefrontal function. Biological Psychiatry, 46(3), 312–
328.
Daw, N., & Courville, A. (2008). The pigeon as particle
filter. Advances in neural information processing systems,
20, 369–376.
Daw, N., O’Doherty, J., Dayan, P., Seymour, B., & Dolan,
R. (2006). Cortical substrates for exploratory decisions in
humans. Nature, 441(7095), 876.
Dayan, P. (2009). Load and attentional bayes. In D. Koller,
D. Schuurmans, Y. Bengio, & L. Bottou (Eds.), Advances
in neural information processing systems 21 (pp. 369–
376).
Dayan, P., Kakade, S., & Montague, P. (2000). Learning and
selective attention. Nature Neuroscience, 3, 1218–1223.
Dayan, P., & Niv, Y. (2008). Reinforcement learning: the
good, the bad and the ugly. Current opinion in neurobiology, 18(2), 185–196.
Dias, R., Robbins, T., & Roberts, A. (1996). Dissociation in
prefrontal cortex of affective and attentional shifts.
Heller, K., Sanborn, A., & Chater, N. (2009). Hierarchical
learning of dimensional biases in human categorization. In
Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams,

& A. Culotta (Eds.), Advances in neural information processing systems 22 (pp. 727–735).
Kass, R., & Raftery, A. (1995). Bayes factors. Journal of the
American Statistical Association, 90(430).
Kemp, C., Perfors, A., & Tenenbaum, J. (2007). Learning
overhypotheses with hierarchical Bayesian models. Developmental Science, 10(3), 307–321.
Kemp, C., & Tenenbaum, J. (2009). Structured statistical models of inductive reasoning. Psychological review,
116(1), 20–58.
Kruschke, J. (1992). ALCOVE: An exemplar-based connectionist model of category learning. Psychological Review,
99(1), 22–44.
Kruschke, J. (2006). Locally Bayesian learning with applications to retrospective revaluation and highlighting. Psychological Review, 113(4), 677–698.
Milner, B. (1963). Effects of different brain lesions on card
sorting: The role of the frontal lobes. Archives of Neurology, 9(1), 90.
Nosofsky, R.
(1986).
Attention, similarity, and the
identification–categorization relationship. Journal of Experimental Psychology: General, 115(1), 39–57.
Owen, A., Roberts, A., Polkey, C., Sahakian, B., & Robbins,
T. (1991). Extra-dimensional versus intra-dimensional
set shifting performance following frontal lobe excisions,
temporal lobe excisions or amygdalo-hippocampectomy in
man. Neuropsychologia, 29(10), 993–1006.
Rao, R. (2005). Bayesian inference and attentional modulation in the visual cortex. Neuroreport, 16(16), 1843.
Rougier, N., Noelle, D., Braver, T., Cohen, J., & O’Reilly,
R. (2005). Prefrontal cortex and flexible cognitive control: Rules without symbols. Proceedings of the National Academy of Sciences of the United States of America,
102(20), 7338.
Schonberg, T., Daw, N., Joel, D., & O’Doherty, J. (2007).
Reinforcement learning signals in the human striatum distinguish learners from nonlearners during reward-based decision making. Journal of Neuroscience, 27(47), 12860.
Schultz, W., Dayan, P., & Montague, P. (1997). A neural substrate of prediction and reward. Science, 275(5306), 1593.
Sutton, R., & Barto, A. (1998). Reinforcement Learning: An
Introduction. Cambridge, MA: MIT Press.
Todd, M., Niv, Y., & Cohen, J. (2009). Learning to use Working Memory in Partially Observable Environments through
Dopaminergic Reinforcement. In Neural information processing systems (pp. 1689–1696).
Williams, R. (1992). Simple statistical gradient-following
algorithms for connectionist reinforcement learning. Machine Learning, 8(3), 229–256.
Yu, A., Dayan, P., & Cohen, J. (2009). Dynamics of attentional selection under conflict: Toward a rational Bayesian
account. Journal of Experimental Psychology: Human Perception and Performance, 35(3), 700–717.

1275

