UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning to selectively attend
Permalink
https://escholarship.org/uc/item/7bn072ds
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Gershman, Samuel
Cohen, Jonathan
Niv, Yael
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                             Learning to Selectively Attend
                                        Samuel J. Gershman (sjgershm@princeton.edu)
                                             Jonathan D. Cohen (jdc@princeton.edu)
                                                    Yael Niv (yael@princeton.edu)
                              Department of Psychology and Neuroscience Institute, Princeton University
                                                          Princeton, NJ 08540 USA
                               Abstract                                  bias: Restricting attention to only a few dimensions is akin to
                                                                         the belief that only those dimensions are relevant for earning
   How is reinforcement learning possible in a high-dimensional
   world? Without making any assumptions about the struc-                reward. This has the effect of reducing the space of possible
   ture of the state space, the amount of data required to effec-        value functions to a much smaller subspace.
   tively learn a value function grows exponentially with the state         While Bayesian probability theory stipulates the ideal
   space’s dimensionality. However, humans learn to solve high-
   dimensional problems much more rapidly than would be ex-              learner, in general it may not be computationally tractable to
   pected under this scenario. This suggests that humans em-             perform the necessary calculations exactly (Kruschke, 2006;
   ploy inductive biases to guide (and accelerate) their learning.       Daw & Courville, 2008). We therefore consider a “hybrid”
   Here we propose one particular bias—sparsity—that amelio-
   rates the computational challenges posed by high-dimensional          model that combines the computational efficiency of model-
   state spaces, and present experimental evidence that humans           free RL with the statistical efficiency of Bayesian inference.
   can exploit sparsity information when it is available.                We compare the Bayesian and hybrid models to naive RL (no
   Keywords: reinforcement learning; attention; Bayes.                   sparsity bias) and show that models that exploit structured
                                                                         knowledge better capture choice behavior in our experiment.
                           Introduction
Reinforcement learning (RL) in high-dimensional state                                  The Computational Problem
spaces is a notoriously difficult problem in machine learn-              For concreteness, we consider one particular example of the
ing (Sutton & Barto, 1998), primarily because of the curse               general class of reinforcement learning problems for which
of dimensionality: The number of states grows exponentially              the sparsity assumption holds. This example is meant to cap-
with dimensionality (Bellman, 1957), and thus if one were                ture the abstract structure of many problems facing humans in
naively to represent a separate value (expected reward) for              the real world, where they must make choices between sev-
each state, one would require astronomical amounts of data to            eral multidimensional stimuli under conditions where most
effectively learn the value function (and thereby behave adap-           dimensions are unpredictive of reward. This example will
tively). Nonetheless, humans appear to learn rapidly from                also serve as a formal description of the task that we asked
small amounts of data. Thus, while substantial evidence has              human subjects to perform, the results of which we report in
accumulated that human behavior follows the predictions of               a later section.
RL models (Dayan & Niv, 2008), these models may funda-                      The subject plays N trials, and observes M stimuli simulta-
mentally underestimate the learning capabilities of humans.              neously on each trial. The ith stimulus on trial n is denoted by
   Following work in other areas of cognition (Braun,                    a D-dimensional vector xni , where each integer-valued com-
Mehring, & Wolpert, 2009; Kemp & Tenenbaum, 2009), we                    ponent xni j indicates the property of the jth stimulus dimen-
suggest that rapid learning arises from the exploitation of              sion (for instance, [color = green, shape = triangle, texture
structured knowledge in the form of inductive biases. In par-            = dots]). Each set of trials has a target dimension d (e.g.,
ticular, our proposal is that humans employ a sparsity bias:             ‘shape’) and target property f on that dimension (e.g., ‘cir-
the assumption that only one (or a small number) of dimen-               cle’). The subject chooses a stimulus cn on each trial and
sions (input features) is relevant at any given time for predict-        observes a binary reward rn . The probability of reward given
ing reward. For example, when you are at a stoplight, only the           choice and target is
color of the light matters, not its shape, size, etc. In other do-                                            
mains (such as ordering food in a restaurant), you may know                                                     θ1 if xncn d = f
                                                                                  P(rn = 1|cn , d, f , Xn ) =                         (1)
that dimensional relevance is sparse, but not which particu-                                                    θ0 otherwise,
lar dimensions are relevant (does it matter which restaurant
it is? which table I am sitting at? who the chef is? who                 In other words, the subject receives a reward with probability
the waiter is?); for this purpose, one requires a learning al-           θ1 if the chosen stimulus posseses the target property on the
gorithm that can exploit sparsity. We formalize this idea in             target dimension, and with probability θ0 otherwise.
terms of rational statistical inference, and present new exper-
imental evidence that human behavior is consistent with such                                   Bayesian Model
a model.                                                                 Given uncertainty about the target dimension and property, a
   Central to our analysis is the idea that selective attention          Bayesian model would use Bayes’ rule to infer the posterior
is a direct consequence of Bayesian inference with a sparsity            over the target dimensions and property and then calculate the
                                                                     1270

value of the stimulus by taking the expectation of reward with        Function Approximation Models
respect to the posterior:                                             One reason why the naive RL model may be ineffective in this
                                                                      task is that it lacks the ability to generalize across different
Vn (c) = ∑ ∑ P(rn = 1|cn = c, d, f , Xn )P(d, f |Dn−1 , r1:n−1 ),     configurations of features. Intuitively, if you knew the target
           d  f                                                       dimension and property, then the value of a stimulus should
                                                               (2)    be independent of the properties on the non-target dimension.
                                                                      However, the naive RL model yokes these together, such that
where Dn−1 = {X1:n−1 , c1:n−1 }. The intuition behind the             learning operates on configurations of properties and hence
Bayesian model is that the model weights the expected re-             fails to exploit this invariance. For example, the naive RL
ward in each possible state of the world (i.e., target dimen-         model learns a different value for green triangles with dots
sion and property) by the probability of the world being in           and for green triangles with waves, although the texture di-
that state given past observations. A key characteristic of this      mension may be completely incidental and not predictive of
model is that a complete posterior distribution is maintained         reward.
over states of the world, rather than a point estimate. The              A more structured RL model that generalizes across con-
posterior distribution used by the Bayesian model is given by         figurations can be obtained by constructing the value function
Bayes’ rule:                                                          as a linear combination of D basis functions φ:
                                                                                                      D
    P(d, f |Dn−1 , r1:n−1 ) ∝ P(r1:n−1 |Dn−1 , d, f )P(d, f ), (3)
                                                                                           Vn (c) =  ∑ wn (d, xncd )φd ,                 (7)
                                                                                                     d=1
where the prior is assumed to be uniform and the likelihood
is given by:                                                          where the weight matrix Wn determines how the basis func-
                                                                      tions are combined, with one weight for each dimension-
                                    n−1                               property pair. This type of model is known as a function ap-
          P(r1:n−1 |Dn−1 , d, f ) = ∏ P(rt |ct , d, f , Xt ).  (4)    proximation architecture (Sutton & Barto, 1998). RL is used
                                     t=1                              to update the weights according to:
Note that although this model describes the optimal learning                        wn+1 (d, xncd ) = wn (d, xncd ) + α∆n φd ,           (8)
rule, we shall assume that subjects are “weakly” rational in
their decision rule (see the softmax choice function described        where the prediction error ∆n is computed the same way as
below).                                                               in the naive RL model (Eq. 6). This update can be under-
                                                                      stood as performing gradient ascent on the value function by
           Reinforcement Learning Models                              optimizing the weight parameters (Williams, 1992).
                                                                         We will consider a family of basis functions parameterized
We now consider several alternative models based on RL. The           by η:
intuition behind these models is that what ultimately matters
                                                                                                             η
for the choice value is the expectation under the posterior; so                                            Pd
                                                                                                   φd =        η,                        (9)
incrementally updating an estimate of this expectation from                                              ∑ j Pj
experience will eventually converge to the optimal choice val-
ues, even though these updates do not make optimal use of             where Pd = ∑ f P(d, f |Dn−1 , r1:n−1 ). The basis function can
information on each trial. The various RL models differ prin-         be thought of as an “attentional focus” that encodes the
cipally in their construction of the value function.                  subject’s beliefs about what dimension is currently relevant.
                                                                      Thus, rather than maintaining the full posterior over target
Naive RL Model                                                        dimension and property (which may be quite computation-
                                                                      ally expensive), with the function approximation model the
The naive RL model represents a separate value for every              subject maintains the marginal posterior over target dimen-
possible stimulus-dimension-property configuration. Specif-           sion (i.e., the probability of a dimension being the target, av-
ically, the choice value estimate is given by Vn (c) = vn (xnc ).     eraging over different target properties), which is then used
This estimate is updated according to the learning rule:              to weight separate value functions, one for each dimension.
                                                                      When reward feedback is received, credit (or blame) is as-
                  vn+1 (xncn ) = vn (xncn ) + α∆n ,            (5)    signed to each value function in proportion to its posterior
                                                                      probability.1 We refer to this model as the hybrid model, be-
where α is a learning rate and ∆n is the prediction error:            cause it combines properties of RL and Bayesian inference.
                                                                         Different settings of η lead to several special cases of in-
                         ∆n = rn −Vn (c).                      (6)    terest:
                                                                          1 Note that the subject need not maintain and update the full pos-
Although the optimal solution is learnable by this model, its         terior; any procedure that estimates the marginal posterior directly
learning will be very slow, as we describe in the next section.       is consistent with this formulation.
                                                                  1271

• η = 0: uniform weighting of dimensions (diffuse focus).                   by the intra-dimensional/extra-dimensional set-shifting task
                                                                            (Dias, Robbins, & Roberts, 1996; Owen, Roberts, Polkey,
• η = 1: exact posterior weighting of dimensions (optimal                   Sahakian, & Robbins, 1991), in which subjects are asked to
   focus).                                                                  discriminate between visual stimuli on the basis of a partic-
                                                                            ular (but unknown) dimension which they must learn from
• η → ∞: maximum a posterior (MAP) weighting (myopic
                                                                            feedback, as well as the Wisconsin card-sorting task (Milner,
   focus).
                                                                            1963). We have adapted this task to a multi-armed bandit
Other intermediate scenarios are also possible. Thus, the                   setting, such as has been used in previous studies of rein-
value of η tells us how much information about the poste-                   forcement learning (e.g., Daw, O’Doherty, Dayan, Seymour,
rior distribution the subject is using to focus attention, with             & Dolan, 2006; Schonberg, Daw, Joel, & O’Doherty, 2007).
η = 1 being optimal focus and η = 0 completely ignoring
information from the posterior and attending equally to all
                                                                            Participants and Stimuli
dimensions.2 When η is larger than 1, the subject discards                  Sixteen participants received 12 dollars reimbursement for
posterior uncertainty by focusing on the mode of the distri-                performing 1800 trials of the task. The stimuli were triplets
bution, and is therefore overconfident in her beliefs about the             of stimuli varying along three dimensions: color (red, yellow,
relevant dimension.                                                         green), shape (circle, triangle, square), and texture (waves,
   One way to interpret the function approximation model is                 dots, lattice). An example triplet is shown in Figure 2.
as a neural network in which the basis functions represent
attentional units focusing on different sensory channels, and
the weights represent synaptic connections between the at-
tentional units and a reward prediction unit (Figure 1). The
synaptic weights are updated using RL (Eq. 8). This inter-
pretation resonates with ideas in computational neuroscience
that view the dorsolateral prefrontal cortex as encoding atten-                        Figure 2: Example experimental stimuli.
tional or task-related biases that interact with a striatal reward
prediction system (Braver, Barch, & Cohen, 1999; Rougier,
Noelle, Braver, Cohen, & O’Reilly, 2005; Todd, Niv, & Co-                   Procedure
hen, 2009). The prediction error ∆ driving the weight updates
                                                                            For each game, the target dimension and property were cho-
is thought to be signaled by midbrain dopaminergic afferents
                                                                            sen randomly and with equal probability. On each trial, the
to the striatum (Schultz, Dayan, & Montague, 1997)
                                                                            subject was presented with a random triplet and asked to
                                                                            choose one of the stimuli. The stimuli on each trial were gen-
            reward           Striatum     Reward prediction                 erated by a random permutation of the property assignments.
                                          (value) unit                      After making the choice, the subject received feedback about
                                                                            whether or not her choice resulted in a reward. If the sub-
                                            w
                             Δ                                              ject chose the stimulus with the target dimension/property
            Midbrain
                                                                            pair, she received a reward with probability 0.75. Other-
             Prefrontal cortex                   Attentional units          wise, reward was delivered with probability 0.25. The tar-
                                     ϕ                                      gets changed on each game (lasting 20-30 trials), and subjects
                                                                            were informed when a new game was beginning.
                                                                            Choice Probabilities
           Extrastriate cortex                   Sensory units
                                     x                                      To map from values to choices, we define a policy πn that
                                                                            specifies the probability πn (c) of making choice c on trial n.
                                                                            Here we use the “softmax” policy defined by
Figure 1: Neural network interpretation of the hybrid
model.                                                                                                       eβVn (c)
                                                                                                  πn (c) =             ,              (10)
                                                                                                           ∑a eβVn (a)
                                   Method                                   where β is an inverse temperature parameter that allows us to
We now describe a behavioral experiment designed to quanti-                 model stochasticity in the subject’s responses.
tatively evaluate these models. Our experiment was inspired
                                                                            Parameter Estimation and Model Comparison
    2 It is important to note that diffuse focus is not the same as the     We fit the parameters of each model with MAP estimation
naive RL model. For all values of η, the function approximation
model is still able to generalize across different configurations, un-      using gradient descent and calculated the Laplace approxi-
like the naive RL model.                                                    mation (Kass & Raftery, 1995) to the log marginal likelihood
                                                                        1272

(evidence) for each model m according to:                             too high later in a game. No single value of the inverse tem-
            Z                                                         perature would be able to capture this pattern.
  Em = ln       P(ωm )P(c|ωm )dωm
             ωm                                                                       Log Bayes Factor     Held-out log-likelihood
                                    1            1                      Bayesian            5425                              5620
       ≈ ln P(ω̂m ) + ln P(c|ω̂m ) + Gm ln 2π − ln |Hm |, (11)
                                    2            2                      Hybrid              5892                              6208
                                                                        Naive               3307                              3312
where ωm is the set of parameters for model m, P(c|ω̂m ) =
∏Nn=1 πn (cn |ω̂m ), ω̂m is the MAP estimate of the parameters,
Gm is the number of parameters (length of ωm ), and Hm is             Figure 3: Model comparison results. Highest scores are
the Hessian matrix of second derivatives of the negative log-         shown in bold.
posterior evaluated at the MAP estimate. We then calculated
the log Bayes Factor relative to chance (where all choices are
equiprobable) according to Em − N ln(1/3). A larger Bayes
Factor indicates greater support for a model. Note that the
chance (null) model has no parameters. In addition to com-
paring models based on Bayes Factors, we also calculated
predictive log-likelihood on a held-out game using a leave-
one-out cross-validation procedure.
    For all the models, we fit an inverse temperature β, plac-
ing on it a Gamma(2,2) prior. This served to ameliorate a
well-known degeneracy in models with both a temperature
and learning rate, such that these parameters tend to trade-
off against each other (inverse temperature becoming very
large and learning rate very small). For the RL models, we
fit a learning rate α, placing on it a Beta(1.2,1.2) prior, which
slightly biases the fits away from the parameter boundaries.
We also allowed θ1 and θ0 to vary across subjects, since we           Figure 4: Comparison of Log Bayes Factors for Bayesian
only told subjects that the target would be rewarding more            and hybrid models. Points above the diagonal are favored
often than non-targets, placing on θ1 a Beta(12,4) prior and          by the hybrid model. The red shaded region indicates the
on θ0 a Beta(4,12) prior; these priors were chosen to have            confidence interval outside of which one model is more likely
as their expected value the true—but unknown—values of θ1             than the other with p < 0.05.
and θ0 . Finally we placed a Uniform(0,10) prior on η.
                                                                         Another question we can ask is whether subjects who be-
                              Results                                 have more in accordance with the Bayesian model or hybrid
Figure 3 presents the log Bayes Factors for each model,               model earn more reward overall. Figure 6 does indeed show
summed across subjects, along with the cross-validation re-           this relationship (measured as the correlation between reward
sutls. Zero represents the null (chance) model in both cases.         earned and the log Bayes Factors for the Bayesian model rel-
Clearly all the models do better than chance, but the naive RL        ative to the hybrid model), suggesting that subjects who more
model appears to perform substantially worse than the others.         effectively exploit the structure of the task tend to perform
Overall, the hybrid model appears to best match behavior on           better. The correlation is significant at p < 0.01. The hybrid
this task. Figure 4 displays the distribution of log Bayes Fac-       model log Bayes Factor relative to the null model also corre-
tors for the Bayesian and hybrid models, showing that there           lates with total reward (p < 0.02).
are also individual differences in which model is favored for            Figure 7 shows the parameter estimates for η on a log-
each subject.                                                         scale, demonstrating that subjects cluster around 0, corre-
    Additional insight into these models can be gained by in-         sponding to exact posterior weighting (optimal focus). This
specting aggregate learning curves (the probability of choos-         was supported by a sign test which failed to reject (p=0.45)
ing the optimal stimulus as a function of trials within a game).      the null hypothesis that ln(η) was drawn from a distribution
As shown in Figure 5, the naive RL model appears to con-              with 0 median. Thus, within the family of possible basis func-
sistently underestimate the speed of learning exhibited by            tions, posterior attentional weighting best describes human
subjects, whereas both the Bayesian and hybrid models hew             behavior on this task.
closely to the empirical learning curve. One peculiarity of
the learning curve is that subjects appear to learn faster than                                Discussion
the Bayesian model. We believe that this is an artifact of the        In this paper we have posed a problem that humans face in ev-
softmax choice probability function: the inverse temperature          eryday life: how to learn value functions in high-dimensional
parameter appears to be too low early in a game and slightly          state spaces. The crucial assumption that makes this possible
                                                                  1273

                                                                            MAP
                                                                           Exact
                                                                         Uniform
Figure 5: Learning curves. Probability of choosing the op-                       Figure 7: Boxplot of ln(η) estimates.
timal stimulus as a function of trial within a game. The cir-
cles represent the empirical choice probability. Error bars are
standard errors of the mean.                                         intractable. The hybrid model represents a compromise be-
                                                                     tween the computational efficiency of model-free RL and the
                    550                                              statistical efficiency of Bayesian inference.
                                                                         The idea that selective attention can be framed as the out-
                    500
                                                                     come of Bayesian inference has been explored by several au-
                                                                     thors (Dayan, 2009; Rao, 2005; Yu, Dayan, & Cohen, 2009).
     Total reward
                    450
                                                                     Most relevant to our work is the competitive combination
                    400                                              model of Dayan, Kakade, and Montague (2000), in which
                                                                     stimuli are assumed to vary in how reliably they predict re-
                    350                                              ward. Dayan et al. (2000) showed that selective attention
                                                                     to particular stimuli falls naturally out of inference over the
                    300
                                                                     causal relationships between stimuli and reward in such a
                    250
                                                                     model. Our work is conceptually similar, with the main dif-
                     −150   −100   −50     0     50     100
                                                                     ference that we model inference over dimensions, rather than
                      Log Bayes Factor (Bayesian − Hybrid)
                                                                     just stimuli. As emphasized by Dayan et al. (2000), the se-
                                                                     lectivity of attention in our model is based on proceses of
Figure 6: Individual differences in earned reward. On the
                                                                     statistical inference, rather than resource constraints. This
x-axis is plotted the log Bayes Factors of the Bayesian model
                                                                     point is particularly important to explaining how attention is
relative to the hybrid model, and on the y-axis is plotted the
                                                                     learned; resource-limitation models, without further elabora-
total reward earned. A least-squares line is superimposed on
                                                                     tion, do not speak to this issue.
the scatter plot.
                                                                         The central role of selective attention has been exten-
                                                                     sively explored in the category learning literature, notably
is that only one or a few dimensions is relevant at any given        by Nosofsky (1986) and Kruschke (1992). The basic idea
time. By employing this sparsity bias in the machinery of            is that learned attentional weights amplify or attenuate spe-
Bayesian inference, the effective dimensionality of the prob-        cific stimulus dimensions in a way that facilitates category
lem is reduced. This can be understood as a kind of selective        discrimination. Recently, Kruschke (2006) has attempted to
attention that is learned through experience.                        connect these ideas to a form of approximate Bayesian infer-
   Our experimental results demonstrate that humans can ex-          ence he dubs “locally Bayesian learning” (LBL). Much as in
ploit sparsity information when it is available. We compared a       our work, attention arises in LBL as a consequence of weight-
Bayesian model and a family of sophisticated RL algorithms           ing different hypotheses about the currently relevant stimulus
against a naive RL model that ignores sparsity information           dimension in response to new evidence. In this sense, LBL
and hence does not generalize across stimulus configurations,        is form of hybrid model; here we have attempted to identify
the key ingredient to efficient learning. Our computational          a continuum through which Bayesian knowledge can influ-
analysis of behavior on this task suggests that humans com-          ence RL, and fit this to data to identify where human learn-
bine reinforcement learning with Bayesian inference, rather          ers fall on this continuum. At the same time, although our
than using a purely Bayesian strategy. This makes sense if           model shares some characteristics with categorization mod-
the brain’s learning algorithms are designed to deal with high-      els such as LBL (see also Heller, Sanborn, & Chater, 2009),
dimensional problems for which exact Bayesian inference is           it is important to note that the problem it is designed to solve
                                                                  1274

is conceptually different: it does not receive feedback indicat-        & A. Culotta (Eds.), Advances in neural information pro-
ing the correct response (as in supervised category learning),          cessing systems 22 (pp. 727–735).
but must instead learn from probabilistic reward feedback.            Kass, R., & Raftery, A. (1995). Bayes factors. Journal of the
   While our work was partly inspired by earlier neural net-            American Statistical Association, 90(430).
work models (Braver et al., 1999; Rougier et al., 2005), our          Kemp, C., Perfors, A., & Tenenbaum, J. (2007). Learning
goal in this paper was to step away from implementational               overhypotheses with hierarchical Bayesian models. Devel-
details and interrogate computational- and algorithmic-level            opmental Science, 10(3), 307–321.
concerns. Future work will need to examine more system-               Kemp, C., & Tenenbaum, J. (2009). Structured statisti-
atically how the algorithmic ideas presented here map onto              cal models of inductive reasoning. Psychological review,
neural mechanisms. We are currently investigating this ques-            116(1), 20–58.
tion with functional magnetic resonance imaging.                      Kruschke, J. (1992). ALCOVE: An exemplar-based connec-
   In conclusion, the main theoretical and experimental con-            tionist model of category learning. Psychological Review,
tribution of this paper is showing that the human RL system is          99(1), 22–44.
more sophisticated than previous computational models have            Kruschke, J. (2006). Locally Bayesian learning with appli-
given it credit for. This may not, after all, be that surprising;       cations to retrospective revaluation and highlighting. Psy-
many years of machine learning research have shown that the             chological Review, 113(4), 677–698.
naive assumptions of previous models simply do not scale              Milner, B. (1963). Effects of different brain lesions on card
up to high-dimensional real world problems. It remains to               sorting: The role of the frontal lobes. Archives of Neurol-
be seen what other hidden sophistications in the RL system              ogy, 9(1), 90.
await discovery.                                                      Nosofsky, R.       (1986).    Attention, similarity, and the
                                                                        identification–categorization relationship. Journal of Ex-
                     Acknowledgments                                    perimental Psychology: General, 115(1), 39–57.
                                                                      Owen, A., Roberts, A., Polkey, C., Sahakian, B., & Robbins,
We thank Michael Todd for invaluable discussion. SJG
                                                                        T. (1991). Extra-dimensional versus intra-dimensional
was supported by a Quantitative Computational Neuroscience
                                                                        set shifting performance following frontal lobe excisions,
training grant from the National Institute of Mental Health.
                                                                        temporal lobe excisions or amygdalo-hippocampectomy in
                          References                                    man. Neuropsychologia, 29(10), 993–1006.
                                                                      Rao, R. (2005). Bayesian inference and attentional modula-
Bellman, R. (1957). Dynamic Programming. Princeton, NJ:                 tion in the visual cortex. Neuroreport, 16(16), 1843.
   Princeton University Press.                                        Rougier, N., Noelle, D., Braver, T., Cohen, J., & O’Reilly,
Braun, D., Mehring, C., & Wolpert, D. (2009). Structure                 R. (2005). Prefrontal cortex and flexible cognitive con-
   learning in action. Behavioural Brain Research.                      trol: Rules without symbols. Proceedings of the Na-
Braver, T., Barch, D., & Cohen, J. (1999). Cognition and con-           tional Academy of Sciences of the United States of America,
   trol in schizophrenia: A computational model of dopamine             102(20), 7338.
   and prefrontal function. Biological Psychiatry, 46(3), 312–        Schonberg, T., Daw, N., Joel, D., & O’Doherty, J. (2007).
   328.                                                                 Reinforcement learning signals in the human striatum dis-
Daw, N., & Courville, A. (2008). The pigeon as particle                 tinguish learners from nonlearners during reward-based de-
   filter. Advances in neural information processing systems,           cision making. Journal of Neuroscience, 27(47), 12860.
   20, 369–376.                                                       Schultz, W., Dayan, P., & Montague, P. (1997). A neural sub-
Daw, N., O’Doherty, J., Dayan, P., Seymour, B., & Dolan,                strate of prediction and reward. Science, 275(5306), 1593.
   R. (2006). Cortical substrates for exploratory decisions in        Sutton, R., & Barto, A. (1998). Reinforcement Learning: An
   humans. Nature, 441(7095), 876.                                      Introduction. Cambridge, MA: MIT Press.
Dayan, P. (2009). Load and attentional bayes. In D. Koller,           Todd, M., Niv, Y., & Cohen, J. (2009). Learning to use Work-
   D. Schuurmans, Y. Bengio, & L. Bottou (Eds.), Advances               ing Memory in Partially Observable Environments through
   in neural information processing systems 21 (pp. 369–                Dopaminergic Reinforcement. In Neural information pro-
   376).                                                                cessing systems (pp. 1689–1696).
Dayan, P., Kakade, S., & Montague, P. (2000). Learning and            Williams, R. (1992). Simple statistical gradient-following
   selective attention. Nature Neuroscience, 3, 1218–1223.              algorithms for connectionist reinforcement learning. Ma-
Dayan, P., & Niv, Y. (2008). Reinforcement learning: the                chine Learning, 8(3), 229–256.
   good, the bad and the ugly. Current opinion in neurobiol-          Yu, A., Dayan, P., & Cohen, J. (2009). Dynamics of atten-
   ogy, 18(2), 185–196.                                                 tional selection under conflict: Toward a rational Bayesian
Dias, R., Robbins, T., & Roberts, A. (1996). Dissociation in            account. Journal of Experimental Psychology: Human Per-
   prefrontal cortex of affective and attentional shifts.               ception and Performance, 35(3), 700–717.
Heller, K., Sanborn, A., & Chater, N. (2009). Hierarchical
   learning of dimensional biases in human categorization. In
   Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams,
                                                                  1275

