UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Visual and Task characteristics may explain hemispheric asymmetry in visual word
recognition

Permalink
https://escholarship.org/uc/item/0tm6w1z6

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Cheung, Kloser Chee Fung
Hsiao, Janet Hui-wen

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Visual and Task characteristics may explain
hemispheric asymmetry in visual word recognition
Kloser Chee Fung Cheung (kloser@hku.hk)
Janet Hui-wen Hsiao (jhsiao@hku.hk)
Department of Psychology, University of Hong Kong
604 Knowles Building, Pokfulam Road, Hong Kong SAR
Abstract
Previous studies proposed that the left hemisphere (LH)
lateralization in English word recognition is because of the
LH superiority in language processing. Nevertheless, Chinese
character recognition has been shown to be more bilateral or
right hemisphere (RH) lateralized and thus is a counter
example of this claim. Through computational modeling, here
we show that at least two factors other than language
lateralization may influence hemispheric asymmetry in visual
word recognition: (1) Visual similarity among words, which
can be influenced by the ratio between the alphabet size and
the lexicon size and the visual similarity among letters: We
show that the more similar the words are in the lexicon, the
more high spatial frequency (HSF) information is required to
distinguish them, and this leads to more LH lateralization (2)
The requirement to decompose a word into letters in order to
map them to corresponding phonemes in pronunciation: We
show that letter identity mapping requires more HSF
information than word identity mapping, and alphabetic
reading requires more HSF information than logographic
reading; this leads to more LH lateralization in alphabetic
languages. These two visual and task characteristic factors
alone may explain differences in lateralization between
English word and Chinese character recognition, without
assuming the influence from language lateralization.
Keywords: visual word recognition, hemispheric asymmetry,
computational modeling

Introduction
Lateralization in visual word recognition
Words, which surround us ever since our childhood, have
been extensively studied in the research on visual
recognition. Previous studies have consistently shown a left
hemisphere (LH) lateralization effect in visual word
recognition in alphabetic languages such as English. A
classical right visual field (RVF)/LH advantage in reading
English words (or words in alphabetic languages) has been
demonstrated first in tachistoscopic recognition (e.g.,
Bryden & Rainey, 1963) and consistently reported in other
word recognition tasks, such as word naming (Brysbaert &
d‟Ydewalle, 1990) and lexical decision tasks (Faust,
Babkoff, & Kravetz, 1995). Data from fMRI studies have
shown a region inside the left fusiform area (Visual Word
Form Area, VWFA) responding selectively to words (e.g.,
McCandliss, Cohen, & Dehaene, 2003). ERP studies also
show that words elicit a larger N170 in the LH than strings
of symbols (e.g., Maurer, Brandeis, & Dehaene, 2005). This
RVF/LH advantage in visual word recognition in alphabetic
languages has been argued to be because of the LH
lateralization in language processing (e.g., Voyer, 1996).

Nevertheless, this claim has been challenged by at least
one counter example, that is, the recognition of Chinese
characters. In contrast to the RVF/LH advantage in the
recognition of English words, the recognition of Chinese
characters, a logographic writing system, has been shown to
have a left visual field/right hemisphere (LVF/RH)
advantage in orthographic processing, demonstrated in
tachistoscopic recognition tasks (e.g., Tzeng et al., 1979;
Cheng & Yang, 1989). In addition, Hsiao and Cottrell (2009)
showed a left side bias effect in Chinese character
perception in Chinese readers (experts), but not in nonChinese readers (novices). This left side bias effect also
suggests the RH involvement in Chinese character
processing.
As for phonological processing in Chinese character
recognition, Weekes and Zhang (1999) reported
phonological priming effects on the recognition of phonetic
compounds (i.e. characters with a phonetic radical that has
information about character pronunciation) when the
characters were presented in the RVF/LH but not in the
LVF/RH; this effect was not observed in integrated
characters (i.e. characters that do not have a phonetic radical;
Weekes, Chen, & Lin, 1998). Thus, research on Chinese
character recognition has exhibited a LVF/RH advantage for
orthographic processing, and a RVF/LH advantage for
phonological processing, especially for phonetic compounds.
ERP and fMRI studies of Chinese character recognition
have also shown a more bilateral or RH-lateralized
activation in the visual system than those of English word
recognition (e.g., Tan et al., 2000; Liu & Perfetti, 2003),
which is consistent with the behavioral data.
The RH advantage in Chinese character recognition has
been argued to reflect the RH superiority in handling
holistic pattern recognition (Tzeng et al., 1979).
Nevertheless, findings in later studies do not support this
claim. For example, Cheng and Yang (1989) showed no
laterality effect in the recognition of non-characters and
pseudo-characters, suggesting that this RH advantage may
be related to lexical knowledge of Chinese characters or
learning experience. Also, in contrast to Tzeng et al.‟s claim,
Hsiao and Cottrell (2009) showed a reduced holistic
processing effect in Chinese readers compared with nonChinese readers. Thus, it remains unclear why Chinese
character recognition and English word recognition involve
different hemisphere lateralization.

Hemispheric processing model
In order to investigate why Chinese character and English
word
recognition
involve
different
hemispheric

1441

lateralization, here we adopt a computational approach,
aiming to examine potential factors that may influence
hemispheric asymmetry in visual word recognition, since
computational modeling approaches enable us to have better
control over variables.
Anatomical evidence shows that our visual field is
initially split along the vertical midline, and the two visual
hemifields are initially contralaterally projected to different
hemispheres. In order to examine at which processing stage
this split information converges, Hsiao, Shieh, and Cottrell
(2008) conducted a hemispheric modeling study of face
recognition, aiming to account for the left side bias effect in
face perception. They proposed three models with different
timing of convergence: early, intermediate and late
convergence models (Figure 1). They showed that both the
intermediate and late convergence models are able to
account for the left side bias effect in face perception,
whereas the early convergence model fails to show the
effect.
Hsiao et al.‟s hemispheric processing model (2008)
incorporates several known observations about visual
anatomy and neural computation: Gabor responses are used
over the input images to simulate neural responses of cells
in the early visual system (Lades et al., 1993); Principal
Component Analysis (PCA), a biologically plausible linear
compression technique (Sanger, 1989), is used to simulate
possible information extraction processes beyond the early
visual system. This PCA representation then is used as the
input to a two-layer neural network (Figure 2).
In addition, the model implements a theory of
hemispheric asymmetry in perception, Double Filtering by
Frequency theory (DFF, Ivry & Robertson, 1998). The DFF
theory argues that information coming into the brain goes
through two frequency filtering stages: The first stage
involves attentional selection of a task-relevant frequency
range. At the second stage, the LH amplifies high frequency
information, while the RH amplifies low frequency
information. This differential frequency bias in the two
hemispheres is implemented in the model by using two
sigmoid weighting functions to assign different weights to
the Gabor responses in the two hemispheres (Figure 2).
Here we apply Hsiao et al.‟s hemispheric processing
model (2008) to the modeling of visual word recognition, in
order to examine whether visual and task characteristics
alone are able to account for the differences in hemispheric
lateralization in different languages, without assuming the
influence of language processing being LH-lateralized. We
introduce our hypothesis below.

Figure 1: Hemispheric models with different
timing of convergence (Hsiao et al., 2008)

Figure 2: Hsiao et al.‟s hemispheric processing
model (2008)

Visual and task characteristics of a writing system
Here we test the hypothesis that differences in visual and
task characteristics of a writing system alone are able to
account for differences in hemispheric lateralization in
visual word recognition in different languages. We
hypothesize that at least two factors other than language
lateralization may influence hemispheric lateralization in
visual word recognition:

(1) Visual similarity among words in the lexicon:
The more similar the words look visually in the lexicon, the
more high spatial frequency (HSF) information is required
to recognize them; this leads to more LH lateralization. We
hypothesize that at least two factors may influence visual
similarity among words in the lexicon:
(i) Number of letters shared among words in the lexicon:
The more letters are shared among words in the lexicon, the
more similar the words look visually in the lexicon. This
factor is influenced by the ratio between the alphabet size
(i.e. the number of letters in the alphabet) and the lexicon
size (i.e. the number of words in the lexicon); that is, given a
fixed lexicon size, the smaller the alphabet size is, the more
number of letters may be shared among the words in the
lexicon, and thus the more similar the words look visually in
the lexicon.
(ii) Similarity among letters in the alphabet: The more
similar the letters in the alphabet look visually, the more
similar the words look visually in the lexicon. This factor
may be influenced by the number of letters in the alphabet;
that is, given a fixed representational space for all possible
letters, when we gradually increase the number of letters in
the alphabet, it becomes more likely that some letters will
look similar to each other (i.e. closer to each other in the
space).
According to these two factors, we predict that with a
fixed lexicon size, when we gradually increase the alphabet
size, the model will first exhibit more and more low spatial
frequency (LSF) reliance since the words will share fewer
and fewer common letters (factor (i)); when the letters in the
alphabet start to look visually similar to each other because
of the alphabet size increase, the model will start to exhibit
reduced LSF reliance (factor (ii)). In other words, we expect

1442

that there will be an inverted-U-shaped curve in LSF
reliance/RH lateralization in the model when we gradually
increase the alphabet size given a fixed lexicon size.

condition.

(2) The requirement to decompose a word into
letters in order to map them into corresponding
phonemes in pronunciation
Maurer and McCandliss (2007) proposed the phonological
mapping hypothesis to account for the difference in ERP
N170 lateralization between faces and words: N170 has
been found to be larger in the RH compared with the LH in
face recognition, whereas in the recognition of English
words, it has been found to be larger in the LH compared
with the RH. They argued that given phonological processes
are typically left-lateralized (e.g., Price et al., 1997; Rumsey
et al., 1997), specialized processing of visual words in
visual brain areas also becomes left-lateralized. Accordingly,
the LH lateralization of N170 may be specifically related to
the influence of grapheme-phoneme conversion established
during learning to read. According to this hypothesis, this
phonological modulation should be less pronounced in
logographic scripts such Chinese (Maurer & McCandliss,
2007).
In contrast to the phonological mapping hypothesis, here
we hypothesize that the LH lateralization in English word
recognition is due to the requirement to decompose a word
into letters, without assuming phonological processes being
left-lateralized. We test this hypothesis through two
simulations. In the first simulation, we contrast two
mapping tasks using the same stimuli: word identity
mapping and letter identity mapping. In the word identity
mapping task, the model learns to distinguish different
words, whereas in the letter identity mapping task, the
model learns to identify the constituent letter in each letter
position of an input word. We expect that the letter identity
mapping task will require more HSF information (i.e. LH
lateralization) compared with the word identity mapping
task1.
In the second simulation, instead of mapping word image
input to either word or letter identities, we model visual
word recognition more realistically by mapping them to
pronunciations. We use an artificial lexicon with Koreancharacter-like pseudo-characters as the orthography. Two
pronunciation conditions are created: in the alphabetic
reading condition, each component (letter) of a character
maps to a consonant or vowel in pronunciation
systematically, whereas in the logographic reading condition,
each character maps to a pronunciation randomly without a
systematic
relationship
between its orthographic
components (letters) and the phonemes in pronunciation.
We expect that the alphabetic reading condition will require
more HSF information (i.e. more LH lateralization)
compared
with
the
logographic
reading
1
Note that we reported some pilot data in Hsiao & Cottrell
(2009b). Compared with Hsiao & Cottrell (2009b), here we have
revised the hypotheses and modeling methods, and presented
brand-new and more complete simulations.

Figure 3: Images used in the current study: (a)
palindrome English pseudo-words; (b) Korean
pseudo-characters (from left to right, vertical
structure, top heavy structure, and bottom heavy
structure); (c) & (d) Left and right damaged
images of the English pseudo-words and the
Korean pseudo-characters

Modeling Method and Results
To test our hypotheses, we applied the intermediate
convergence model proposed by Hsiao et al. (2008) to
visual word recognition. In the model, the input word
images were first filtered with a rigid grid of overlapping
2D Gabor filters (Daugman, 1985) to obtain Gabor
responses. At each grid, we used Gabor filters of eight
orientations and a fixed number of scales. The number of
scales used depended on the task-relevant frequency range,
which was determined according to the smaller dimension
of the images; the highest frequency scale did not exceed
the smaller dimension of the images (following Hsiao et al.,
2008). In the current simulation, the dimensions of the two
types of images used were 35 x 100 for the English pseudowords and 70 x 80 for the Korean pseudo-characters (see
Figure 3); thus the number of scales for English pseudoword images was five (25 = 32 < 35, and 26 = 64 > 35) and
that for Korean pseudo-character images was six (26 = 64 <
70, and 27 = 128 > 70). We applied the Gabor filters to a
5x18 grid of points on each English pseudo-word image,
and to a 12x14 grid of points on each Korean pseudocharacter image. So each English pseudo-word image was
transformed into a vector of size 3600 (5x18 sample points
x 8 orientation x 5 scales) while each Korean pseudocharacter image was transformed into a vectors of size 8064
(12x14 sample points x 8 orientations x 6 scales).
After obtaining the Gabor magnitudes, two conditions
were created: the baseline condition and the biased
condition. In the baseline condition (the control condition),
Gabor responses in different scales were given equal
weights (i.e. no frequency bias), while in the biased
condition, we implemented the second stage of the DFF
theory by using a sigmoidal weighting function to bias the
Gabor responses on the left half word (RH) to LSFs, and
those on the right half word (LH) to HSFs (Figure 2). The
perceptual representation of each of the left and right half

1443

words was compressed by PCA into a 50-element
representation each (100 elements in total, following Hsiao
et al., 2008) 2. This PCA representation then was used as the
input to a two layer neural network, as shown in Figure 2
(see Hsiao et al., 2008, for more simulation details).
We trained our neural network model to recognize the
input images until the performance on the training set
reached 100% accuracy. The training algorithm was
gradient descent with an adaptive learning rate. To test
hemispheric asymmetry effects, in contrast to the previous
hemispheric models of face and word recognition (e.g.,
Hsiao et al., 2008, Hsiao & Cottrell, 2009b), here we did not
use “chimeric images” (Figure 3(a) & (b)) as a way to give
noise to one side of the stimulus in order to test the model‟s
reliance on either the left or right half of the representation.
A potential problem in using this kind of chimeric images
for words is some letters may have a similar shape as their
mirror images (such as „o‟ and „m‟ in the English alphabet),
while others do not; thus these letters will give non-uniform
noise distribution over the mirror-image sides of the
chimeric words. Here we avoided this problem by using
damaged images (Figure 3(c) & (d).) It was made by setting
one half of the PCA representation to zero, so that when
mapping these damaged images to their identities, only one
of the visual hemifields was used for recognition. The left
side bias effect thus was measured as the difference between
the accuracy of recognizing a right-side-damaged word
(carrying LSF/RH information only) as the original word
and the accuracy of recognizing a left-side-damaged word
(carrying HSF/LH information only) as the original word.

Visual similarity among words in the lexicon:
We first used images of six-letter English pseudo-words to
examine how visual similarity among words in the lexicon
influences lateralization in visual word recognition. To
counterbalance the information carried in the two visual
fields, we used palindrome pseudo-words as the stimuli (e.g.,
Figure 3(a)). We created artificial lexicons with an
increasing alphabet size (a-c, a-e, a-g…), and trained the
model to learn each lexicon 50 times. In each of the 50
simulations, 26 palindrome words were chosen randomly
from all possible combinations of letters in the alphabet to
form the artificial lexicon. In the model, each output node
corresponded to a word identity.
In the first lexicon with letters from „a‟ to „c‟, there were
27 possible combinations: aaaaaa, aabbaa, aaccaa, abaaba,
abbbba… The randomly chosen 26 words thus looked very
similar to one another. When we increased the alphabet size
to include „a‟ to „e‟, the number of combinations became
125, and the randomly chosen 26 words became more
dissimilar visually to one another (i.e. the similarity among
words decreased). In other words, the larger the alphabet
size was, the lower the visual similarities among words in
the lexicon were. Here we examined how the model‟s
2

In a separate simulation, we found that using 100 components
each made the representation noisier and deteriorated the model‟s
performance.

lateralization changed when we gradually increased the
alphabet size.
In the datasets, we used 8 different fonts for each word,
with 4 of them used as the training data, and the other 4
used as the testing data (counterbalanced across the
simulations). Thus, in both the training and testing datasets,
each word had 4 images of different fonts.

Figure 4: RH/LSF preference in the models trained
with lexicons with different alphabet sizes in the
word identity mapping task (*p<0.01; **p<0.001;
***p<<0.001).
The results are shown in Figure 4. The RH/LSF
preference was defined as the difference in the left side bias
effect between the biased condition and the baseline
condition; it reflected how much the model preferred the
RH/LSF-biased representation over the LH/HSF-biased
representation compared with the control condition when no
frequency bias was applied (Hsiao et al., 2008). As shown in
Figure 4, when the alphabet size was small (e.g., „a‟ to „c‟),
the model had low RH/LSF preference. When we increased
the alphabetic size, the RH/LSF preferences became
stronger, and then decreased after the peak at around „a-g‟
(i.e., an inverted-U shape in Figure 4).
Thus, the results showed that, when gradually increasing
the alphabetic size of the lexicon, the visual similarity
among words decreased, and the model relied more on LSFs
to distinguish the words. But when the alphabetic size kept
increasing, more and more letters with similar shapes were
used in the alphabet (e.g., „c‟ and „o‟, „b‟ and „h‟, „m‟ and
„n‟), and the visual similarity among words in the lexicon
increased; as the result, the model required more HSFs to
distinguish the words.

The requirement to decompose a word into letters
When reading words in alphabetic languages, the readers
have to decompose the visual input of a word into its
constituent letters/graphemes and map them to the
corresponding phonemes. This decomposition may require
details of the word image and thus rely more on the HSF
information. Here we examined lateralization effects in a
letter identity mapping task using the English pseudo-words.
Instead of learning to map word images to word identities,
the model was trained to map a word image to its
constituent letter identities. The output layer of the model

1444

was divided into 3 parts corresponding to the first 3 letter
positions in a word (the end 3 letters were the same as the
first 3 since they were palindrome words). The number of
nodes in each part was equal to the alphabetic size (see
Figure 5).

Figure 7 shows the results. As shown in the figure, the
RH/LSF preference in the logographic reading condition
was always stronger than that in the alphabetic reading
condition. This result suggests logographic reading requires
more LSF information compared with alphabetic reading,
and is consistent with the visual word recognition literature
showing a more RH lateralization in reading logographic
languages such as Chinese compared with alphabetic
languages such as English.

Figure 5: Output layers of the letter-position identity
mapping task (Hsiao & Cottrell, 2009b).

Figure 7: RH/LSF preference in the Korean
pseudo-character
reading
task
(*p<0.01;
**p<0.001; ***p<<0.001).
Figure 6: RH/LSF preference in the letter identity
mapping task (in red) in the models trained with
lexicons of different alphabet sizes, compared with
the word identity mapping task (in blue; *p<0.01;
**p<0.001; ***p<<0.001).

Conclusion and Discussion

Figure 6 shows the results. The results showed that
compared with the word identity mapping task, the letter
identity mapping task required more LH/ HSF information.
In addition, in the letter identity task, as the alphabet size
increased, the model relied more on LH/HSF information.
In another simulation, we used artificial lexicons with
Korean-character-like pseudo-characters to examine
hemispheric asymmetry effects in recognizing square-shape
characters, and more importantly, to examine hemispheric
processing difference between logographic and alphabetic
language reading. In this examination, we modeled visual
word recognition more realistically by mapping each word
input into its pronunciation with a consonant-vowelconsonant structure.
In the datasets, there were also 8 different fonts for each
Korean-character-like pseudo-character. Each character
consisted of three Korean-alphabet-like letters, arranging in
three different structures: vertical, top-heavy, and bottomheavy (Figure 3(b)). The frequency of each letter appearing
in either side of the characters in the lexicon was balanced.
In the alphabetic reading condition, each letter
systematically mapped to either a vowel or a consonant in
pronunciation, whereas in the logographic reading condition,
each character mapped to a randomly assigned
pronunciation without a systematic letter-phoneme mapping.

Visual word recognition in alphabetic languages such as
English has been reported to be LH lateralized, and argued
to be due to the LH lateralization of language processes.
Nevertheless, a RH/LVF advantage has been reported in
orthographic processing of Chinese character recognition. In
this study, by applying the hemispheric processing model
(Hsiao et al., 2008) to visual word recognition, we examined
whether visual and task characteristics alone are able to
account for differences in hemispheric lateralization in
different languages without assuming the influence from
language processing being LH-lateralized.
We first showed that visual similarity among words in the
lexicon can influence lateralization in visual word
recognition. We used artificial lexicons with the same
number of words and word length, but with different
alphabetic sizes, and trained the model to map word image
input to their word identities. The results showed an
inverted- U -pattern (Figure 4): When the alphabet size
increases, the model initially relies more and more on the
RH/LSF information, because words in the lexicon share
fewer and fewer common letters and the visual similarity
among words in the lexicon decreases. Nevertheless, with
further increase of the alphabet size, the model‟s RH/LSF
reliance starts to decrease, because of the increase of visual
similarity among letters in the alphabet.
We then showed that the requirement to decompose a
word in to its constituent letters can also influence
lateralization in visual word recognition. We used the same
artificial lexicons but trained the model to perform a letteridentity mapping task instead of the word identity mapping

1445

task. The results showed that decomposition of words into
letters requires more HSF information and thus results in
more LH lateralization. In addition, we used Korean
pseudo-characters to examine lateralization differences
between logographical reading and alphabetic reading. The
results showed that logographical reading requires more
LSF information compared with alphabetic reading, and
thus results in more RH-lateralization.
The two factors related to visual and task characteristics
of a writing system we proposed here are able to account for
the lateralization differences between English word and
Chinese character recognition. Compared with Chinese,
words in the English lexicon may look more similar to one
other, because of the smaller alphabet size (only 26 letters)
and a much larger lexicon size (more than 20,000 words). In
contrast, Chinese has a smaller lexicon size (about 4500
characters for a native speaker), but a much larger
“alphabet” (i.e., more than 1000 stroke patterns). In addition,
English is an alphabetic language whereas Chinese is a
logographic language. Chinese logographic reading may
require more LSF information that leads to more RHlateralization compared with English alphabetic reading,
since logographic reading does not require a decomposition
of words into letters in order to map them to corresponding
phonemes.
In summary, here we show that visual and task
characteristics of a writing system alone may account for
lateralization differences in visual word recognition in
different languages. Specifically, they are (1) visual
similarity among words in the lexicon, and (2) the
requirement to decompose a word into letters for performing
grapheme-phoneme conversion during learning to read.

Acknowledgement
We are grateful to the HKU Seed Funding Program for
Basic Research(project #10400471 to J.H. Hsiao) and the
Research Grant Council of Hong Kong (project code: HKU
744509H to J.H. Hsiao).

References
Bryden, M. P. & Rainey, C. A. (1963). Left-right
differences in tachistoscopic recognition. Journal of
Experimental Psychology, 66, 568-571.
Brysbaert, M. & d‟Ydewalle, G. (1990). Tachistoscopic
presentation of verbal stimuli for assessing cerebral
dominance: Reliability data and some practical
recommendation. Neuropsychologia, 28, 443-455.
Cheng, C. M. & Yang, M. J. (1989). Lateralization in the
visual perception of Chinese characters and words.
Brain and Language, 36, 669-689.
Daugman, J. G. (1985). Uncertainty relation for resolution
in space, spatial frequency, and orientation optimized
by two dimensional visual cortical filters. Journal of
the Optical Society of America A, 2, 1160-1169.
Faust, M., Babkoff, H., & Kravetz, S. (1995). Linguistic
process in the two cerebral hemispheres: Implications
for modularity vs. interactionism. Journal of Clinical
and Experimental Neuropsychology, 17, 171-192.

Hsiao, J. H. & Cottrell, G. W. (2009). Not all expertise is
holistic, but it may be leftist: The case of Chinese
character recognition. Psychological Science.
Hsiao, J. H. & Cottrell, G. W. (2009b). What is the cause of
left hemisphere lateralization of English visual word
recognition? Pre-existing language lateralization, or
task characteristics? Proceedings of the Thirty-First
Annual Conference of the Cognitive Science Society.
Hsiao, J. H., Shieh, D., & Cottrell, G. W. (2008).
Convergence of the visual filed split: hemispheric
modeling of face and object recognition. Journal of
Cognitive Neuroscience, 20(12), 2298-2307.
Ivry, R. & Robertson, L. C. (1998). The Two Sides of
Perception. Cambridge: MIT Press.
Lades, M., Vorbruggen, J. C., Buhmann, J., Lange, J., von
der Malsburg, C., Wurtz, R. P., & Konen, W. (1993).
Distortion invariant object recognition in the dynamic
link architecture. IEEE Transaction on Computers, 42,
300-311.
Liu, Y. & Perfetti, C. A. (2003). The Time course of brain
activity in reading English and Chinese: An ERP study
of Chinese bilinguals. Human Brain Mapping, 18, 167175.
Maurer U., Brandeis, D., & McCandliss, B. (2005). Fast,
visual specialization for reading in English revealed by
the topography of the N170 ERP response. Behavioral
& Brain Functions, 1(1), 13.
Maurer U., & McCandliss, D. (2007). The Development of
visual experticse for words: the contribution of
electrophysiology. In E.L. Grigorenke & A. Naples
(Eds.). Single-Word Reading: Cognitive, behavioral
and biological perspectives. Mahwah, NJ: Lawrence
Erlbaum Associates.
McCandliss, B. D., Cohen, L., & Dehaene, S. (2003). The
visual word form area: expertise for reading in the
fusiform gyrus. Trends in Cognitive Sciences, 7, 293299.
Sanger, T. (1989). An optimality principle for unsupervised
learning. In Touretzky, D. (ed) Advances in Neural
Information Processing Systems, vol. 1, pp. 11-19, San
Mateo: Morgan Kaufmann.
Tan, L. H., Spinks, J. A., Gao, J. H., Liu, H. L., Perfetti, C.
A., Xiong, J., et al. (2000). Brain activation in the
processing of Chinese characters and words: A
functional MRI study. Human Brain Mapping, 10, 1627.
Tzeng, O. J. L., Hung, D. L., Cotton, B., & Wang, S. Y.
(1979). Visual lateralization effect in reading Chinese
characters. Nature (London), 282, 499-501.
Voyer, D. (1996). On the magnitude of laterality effects and
sex differences in functional literalities. Laterality, 1,
51-83.
Weekes, B. S. & Zhang, B. Y. (1999). Chinese character
recognition in the left and right visual fields. Brain &
Cognition, 40, 269-272.
Weekes, B. S., Chen, M. J., & Lin, Y. B., (1998).
Orthographic Phonological and semantic priming of
Chinese character recognition. Reading and Writing,
10 (3-5), 201-222.

1446

