UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Visual Similarity Effects in Categorical Search
Permalink
https://escholarship.org/uc/item/90d626tz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Alexander, Robert
Zhang, Wei
Zelinsky, Gregory
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                 Visual Similarity Effects in Categorical Search
         Robert G. Alexander1 (rgalexander@notes.cc.sunysb.edu), Wei Zhang (weiz@microsoft.com)2,3
                                 Gregory J. Zelinsky1,2 (Gregory.Zelinsky@stonybrook.edu)
                                           1
                                            Department of Psychology, Stony Brook University
                                       2
                                        Department of Computer Science, Stony Brook University
                                                           3
                                                             Microsoft Corporation
                              Abstract                                  often incomplete and lacking in visual detail; exact
                                                                        knowledge of a target’s appearance is an artificial situation
   The factors affecting search guidance to categorical targets
   are largely unknown. We asked how visual similarity                  that typically exists only in the laboratory. Particularly
   relationships between random-category distractors and two            interesting are cases in which a target is defined
   target classes, teddy bears and butterflies, affects search          categorically, as from a text label or an instruction (i.e., no
   guidance. Experiment 1 used a web-based task to collect              picture preview of the target). Given the high degree of
   visual similarity rankings between these target classes and          variability inherent in most categories of common objects,
   random objects, from which we created search displays                search under these conditions would have few visual
   having either high-similarity distractors, low-similarity
   distractors, or “mixed” displays with high, medium, and low-
                                                                        features of the target that could be confidently compared to
   similarity distractors. Subjects made faster manual responses        a scene to generate a guidance signal. Indeed, a debate
   and fixated fewer distractors on low-similarity displays             exists over whether categorical search is guided at all, with
   compared to high. On mixed trials, first fixations were more         some labs finding that it is (Schmidt & Zelinsky, 2009;
   frequent on high-similarity distractors (bear=49%;                   Yang & Zelinsky, 2009) and others suggesting that it is not
   butterfly=58%) than low-similarity distractors (bear=9%;             (e.g., Castelhano et al., 2008; Wolfe et al., 2004).
   butterfly=12%). Experiment 2 used the same high/low/mixed               The present study enters this debate on the existence of
   conditions, but now these conditions were created using
   similarity estimates from a computer-vision model that ranked        categorical guidance, focusing it on the relationship between
   objects in terms of color, texture, and shape similarity. The        target-distractor visual similarity and guidance to
   same patterns were found, suggesting that categorical search         categorically-defined realistic targets. Guidance from a
   is indeed guided by visual similarity.                               pictorial preview is known to decrease with increasing
   Keywords: Visual search; eye movements; categorical                  visual similarity between a target and distractors; does this
   guidance; visual similarity; object class detection                  same relationship hold for categorically-defined targets?
                                                                        Given that the representation of categorical targets is largely
                           Introduction                                 unknown, it may be the case that target descriptions are
                                                                        dominated by non-visual features, such as semantic or
You have probably had the experience of searching for your
                                                                        functional properties of the target category. If this is the
car in a parking lot and finding several other vehicles of the
                                                                        case, guidance to the target may be weak or even
same color or model before finally finding your car. This is
                                                                        nonexistent, potentially explaining the discrepant findings.
an example of visual similarity affecting search; the
                                                                        To the extent that categorical search does use non-visual
presence of these target-similar distractors made it harder to
                                                                        features, effects of target-distractor visual similarity would
find the actual target of your search.
                                                                        therefore not be expected. However, if target categories are
   Such visual similarity effects have been extensively
                                                                        represented visually, one might expect the same target-
studied in the context of search, with the main finding from
                                                                        distractor similarity relationships demonstrated for target-
this effort being that search is slower when distractors are
                                                                        specific search to extend to categorical search.
similar to the target (e.g., Duncan & Humphreys, 1989;
                                                                           It is unclear how best to manipulate visual similarity in
Treisman, 1991). Models of search have also relied
                                                                        the context of categorical search. Traditional methods of
extensively on these visual similarity relationships (e.g.,
                                                                        manipulating target-distractor similarity by varying only a
Pomplun, 2006; Treisman & Sato, 1990; Wolfe, 1994;
                                                                        single target feature are clearly suboptimal, as realistic
Zelinsky, 2008). Despite their many differences, all of these
                                                                        objects are composed of many features and it is impossible
models posit a very similar process for how similarity
                                                                        to know a priori which are the most important. This
relationships are computed and used; the target and scene
                                                                        problem is compounded by the categorical nature of the
are represented by visual features (color, orientation, etc.),
                                                                        task; the relevance of a particular target feature would
which are compared to generate a signal used to guide
                                                                        almost certainly depend on the specific category of
search to the target and to target-like distractors in a display.
                                                                        distractor to which it is compared. It is not even known how
In general, the more similar an object is to the target, the
                                                                        best to derive specific target features for such a comparison;
more likely that object will be fixated.
                                                                        should an average be obtained from many target exemplars
   All of these models, however, assume knowledge of the
                                                                        or should features be extracted from a particular exemplar
target’s specific appearance in the creation of this guidance
                                                                        that is representative of the target class?
signal. This assumption is problematic, as it is often
                                                                           In light of the difficulties associated with directly
violated in the real world. Descriptions of search targets are
                                                                        manipulating the specific features underlying visual
                                                                   1222

similarity, we opted for a more holistic approach—to use                similarity distractors. However, if categorical guidance is
ratings of visual similarity obtained from subjects.                    mediated by non-visual factors, or if the visual similarity
Specifically, we obtained ratings from Zhang et al. (2008),             estimates obtained from an explicit task cannot be extended
who used a web experiment to collect visual similarity                  to search, we would expect no effect of our similarity
estimates between random objects and categorical targets                manipulations on guidance or manual search efficiency.
for the purpose of comparing these estimates to the behavior
of a computational model of object class detection. Subjects                                   Experiment 1
were randomly assigned to either a butterfly target class or a
teddy bear target class, and their task was to rate real-world          Method
objects (from the Hemera collection) to these target
categories. They did this by rank-ordering groups of five               Participants Twenty-four students from Stony Brook
                                                                        University participated in exchange for course credit. All
objects; each trial showed five random objects, and the
                                                                        subjects reported normal or corrected to normal vision.
subjects’ task was to give each a 1-5 ranking, where “1”
indicated low target similarity and 5 indicated high target
similarity (objects given the 2-4 rankings and objects with             Stimuli and Apparatus Targets and distractors were
                                                                        selected from the objects used by Zhang et al. (2008). The
low inter-subject ranking agreement were considered
                                                                        target categories were teddy bears, obtained from Cockrill
medium similarity). There were 142 subjects, yielding a
                                                                        (2001), and butterflies, obtained from the Hemera
total of 71,000 butterfly and teddy bear similarity estimates
for 2,000 different objects. Importantly, subjects were                 collection. The distractors were also Hemera objects. Each
                                                                        object was sized to subtend ~2.8º of visual angle.
instructed to use only visual similarity and to disregard
                                                                           Gaze position was recorded using an SR Research
categorical or associative relationships between the objects
                                                                        EyeLink® II eye tracking system. This eye tracker is video-
and the target category when making their judgments.
Consult Zhang et al. (2008) for additional details regarding            based and has a sampling rate of 500 Hz and a spatial
                                                                        resolution of ~0.2º. Target present/absent search decisions
this web-based collection of visual similarity estimates.
                                                                        were made using a GamePad controller connected to a USB
   Using these estimates of visual similarity, Experiment 1
                                                                        port. Head position and viewing distance were fixed at 72
asked whether the visual similarity relationships known to
affect search for specific targets also extends to categorical          cm from the screen with a chin rest. Trials were displayed
                                                                        on a flat-screen monitor at a resolution of 1024 × 768 pixels
search. Previous arguments for the existence of categorical
                                                                        (subtending 28º × 21º) and a refresh rate of 85 Hz.
search guidance relied on evidence showing the preferential
direction of initial saccades to targets (Schmidt & Zelinsky,
2009; Yang & Zelinsky, 2009). Although there is good                    Design and procedure Half of the subjects searched for a
                                                                        teddy bear target, the other half searched for a butterfly
reason to believe that these initial saccades are dominated
                                                                        target. This search was categorical; subjects were not
by visual features, and occur too early in search to be
                                                                        shown a specific bear or butterfly target preview prior to
influenced by semantic relationships between targets and
distractors, it is still possible that the preferential fixation of     each search trial. Rather, subjects were told the target
                                                                        category at the start of the experiment. They were also
categorical targets might have been influenced by non-
                                                                        shown examples of the target category, none of which were
visual factors. More compelling would be a demonstrated
                                                                        used as actual targets in the experimental trials.
relationship between categorical guidance and a
manipulation of target-distractor visual similarity; providing             Each trial began with the subject fixating a central dot and
                                                                        pressing a button on the controller to initiate the search
this evidence was the primary goal of this experiment.
                                                                        display. The search display consisted of six evenly-spaced
   We were also interested in determining whether explicit
                                                                        objects arranged on an imaginary circle with a radius of 300
visual similarity judgments are predictive of effects of
target-distractor visual similarity on categorical search.              pixels (8.4º) relative to the center of the screen. On target
                                                                        present trials (50%), one object was either a bear or a
Search guidance is a largely implicit process, and as
                                                                        butterfly, depending on the condition, and the other five
discussed can be expressed in even the first search saccade
                                                                        objects were randomly selected distractors. On target absent
(Chen & Zelinsky, 2006); the task of assigning rankings to
objects in a web experiment is comparatively slow and far               trials (50%), distractors were selected based on the
                                                                        similarity rankings from the Zhang et al. (2008) web task.
more explicit. Do these two tasks use fundamentally
                                                                           There were three target absent conditions: high-similarity
different sources of information, or can visual similarity
                                                                        trials (all distractors were similar to the target category),
estimates obtained from explicit judgments be useful in
describing guidance during search? Answering this question              low-similarity trials (all distractors were dissimilar to the
                                                                        target category), and “mixed” trials, where two distracters
was a secondary goal of this experiment.
                                                                        were selected from the high-similarity category, two from
   If categorical search is guided by target-distractor visual
                                                                        the low-similarity category, and two from the medium
similarity, and if this relationship can be captured by
explicit similarity judgments, we would expect a relatively             similarity category (see Figure 1). The high and low
                                                                        similarity conditions were included to determine whether
high proportion of initial saccades to high-similarity
                                                                        visual similarity affects search accuracy and manual
distractors, and relatively few initial saccades to low-
                                                                        reaction times (RTs). The mixed condition allowed us to
                                                                    1223

directly examine which distracters were preferentially              A                        B                  C
fixated (i.e., search guidance) as a function of target-
distractor similarity.
   Target presence/absence and similarity condition were
within-subjects variables, and both were randomly inter-
leaved throughout the experiment. Subjects were asked to            Figure 1: Objects from a typical mixed trial. (A) low-
make their present/absent judgments as quickly as possible          similarity, (B) medium-similarity, and (C) high-similarity
while maintaining accuracy. Accuracy feedback was                   distractors, as ranked to the teddy bear target category.
provided following each response.
                                                                    had been the topic of debate (Castelhano et al., 2008, and
Results and Discussion                                              Wolfe et al., 2004). Not only is categorical search guided, it
As only the target absent trials contained the similarity           is guided by matching visual features to a visual
manipulation, analyses were restricted to these data.               representation of the target category.
   Errors were less than 6% in all conditions, and were               The second conclusion following from our data is that
excluded from all subsequent analyses. This low false               explicit visual similarity rankings from a web task are
positive rate means that subjects were not confusing the            highly predictive of categorical search. Given the dramatic
high-similarity distractors for targets (e.g., a stuffed bunny      differences between these tasks, this finding is surprising.
distractor was not recognized as a teddy bear).                     Judgments in the web task were highly deliberative. In
   RTs were longest in the high-similarity condition and            piloting, a subject was observed agonizing over whether a
shortest in the low-similarity condition, with the mixed            wooden box or a backpack was visually more similar to a
condition yielding intermediate RTs (Table 1). These                teddy bear. These highly explicit similarity judgments can
differences were significant for both butterfly targets             be contrasted with the largely implicit visual similarity
(F(2,22) = 46.87, p < .001) and for bear targets (F(2,22) =         computations that drove search guidance. Whereas the web-
53.85, p < .001). The number of distractors fixated during          based judgments could be measured in seconds, effects of
search also differed between the similarity conditions, and         similarity on search guidance appeared almost immediately,
this again occurred for both butterfly (F(2,22) = 30.41, p <        at least within the first 199 ms following search display
.001) and bear targets (F(2,22) = 59.55, p < .001).                 onset (the average latency of initial saccades in this
Distractors were fixated most frequently on the high-               experiment). Our data suggest a common thread between
similarity trials (3.16±0.23 for bears; 2.50±0.36 for               these two processes. Regardless of whether a visual
butterflies), followed by the medium-similarity trials              similarity relationship has to be completed in time for an
(2.53±0.24 for bears; 1.83±0.31 for butterflies), and finally       initial eye movement, or the opportunity exists to deliberate
the low-similarity trials (1.51±0.23 for bears; 1.29±0.26 for       on this relationship for an extended period, the same
butterflies); as distractor similarity to the target increased,     features seem to be represented and compared.
so did the number of fixations on these distractors. All of
these patterns are consistent with the suggestion that visual       Table 1: Manual RTs by similarity condition, in seconds
similarity rankings are predictive of search efficiency.
   One of the most conservative measures of search                              Experiment 1                Experiment 2
guidance is the first fixated object—the object looked at                       Butterfly Bear              Butterfly Bear
first following search display onset. Consistent with the RT        High        1.17 (.06) 1.48 (.14)       1.59 (.13) 1.24 (.15)
analyses we found that distractor similarity to the target          Medium 0.97 (.06) 1.15 (.11)            1.25 (.10) 1.07 (.15)
determined which objects were fixated first on mixed                Low         0.82 (.05) 0.84 (.08)       0.92 (.09) 0.74 (.09)
condition trials (Figure 2A). High-similarity distractors           Note. Values in parentheses indicate one standard error.
were more often fixated first compared to medium-
similarity distractors, which were more often fixated first                                Experiment 2
compared to low-similarity distractors, and this pattern was        Were subjects from Experiment 1 confining their similarity
found for both butterflies (F(2,22) = 10.13, p < .01) and for       judgments to purely visual dimensions? The fact that this
bears (F(2,22) = 30.15, p < .001).                                  was the instructed task does not guarantee that non-visual
   Two conclusions follow from our data. First, categorical         factors were not creeping into the similarity judgments,
search guidance is affected by target-distractor visual             raising the possibility that these factors, and not visual
similarity. As the visual similarity between a distractor and       similarity, were responsible for the observed categorical
a target category increases, search efficiency decreases.           guidance. Experiment 2 addressed this possibility.
This decreased efficiency is due to distractors becoming              It is unclear how best to separate visual from non-visual
more distracting, as evidenced by an increase in the number         factors in estimates of similarity. Even when stimuli are
of first fixations on the high similarity distractors. More         oriented bars with no compelling semantic properties,
generally, this finding adds to the growing body of evidence        semantic features might still influence perceptual decisions
suggesting that categorical search is indeed guided (Schmidt        (Wolfe et al., 1992). The task of separating these factors
& Zelinsky, 2009; Yang & Zelinsky, 2009), a question that
                                                                1224

          A                                                        B
                      Target-distractor similarity                               Target-distractor similarity
Figure 2: Percentage of mixed condition trials in which the first object fixated had a low, medium, or high target-distractor
similarity for (A) Experiment 1 and (B) Experiment 2. Error bars show one standard error. Dashed lines indicate chance.
using purely behavioral methods is even more daunting in            identical target and distractor objects used in the present
the present study, as our stimuli are realistic objects having      study. Specifically, it successfully classified the high-
an untold number of visual and semantic dimensions.                 similarity and low-similarity objects from the above-
   In Experiment 2 we take a different approach to this             described web task, regardless of whether the target
problem—turning to the computer vision literature to obtain         category was a teddy bear or a butterfly. This makes the
similarity estimates. Recent years have seen considerable           Zhang et al. model an obvious choice for our goal of
success in the development of automated methods for the             collecting computer-vision-based similarity estimates; not
detection of object categories in realistic scenes, a task with     only was this model able to learn classifiers to discriminate
obvious relevance to categorical visual search. At the core         our target categories from random objects, these classifiers
of these methods is the computation of visual similarity            were also shown to be partially successful in capturing
relationships between visual images and features extracted          human visual similarity relationships between these random
from a target class. These similarity relationships are             objects and the bear and butterfly target classes.1
potentially useful for our current purpose, as they provide            To the extent that the Zhang et al. model is successful in
estimates of purely visual similarity between distractors and       capturing human visual similarity relationships, and to the
a categorically-defined target, free from any contamination         extent that these similarity estimates extend to a search task
by semantic properties. Whereas the similarity estimates            (as we found in the previous experiment), then displays
used in Experiment 1 may have been based on some mix of             constructed of high-similarity or low-similarity distractors,
visual and non-visual information, the similarity estimates         as rated by the model, should produce the same patterns of
obtained from a computer vision method are                          guidance found in Experiment 1. Initial saccades should be
incontrovertibly exclusively visual.                                preferentially guided to high-similarity distractors, and
   To obtain these purely visual similarity estimates we used       preferentially guided away from low-similarity distractors,
the computer vision method described in Zhang et al.                with guidance to medium similarity distractors falling
(2008). We chose this method for two reasons. First, it             between these two levels. Replicating these patterns in the
works by having multiple visual features contribute flexibly        context of new search displays, assembled using the purely
to target classification (see also Zhang et al., 2005).             visual similarity estimates from a computer vision model,
Specifically, it combines state-of-the-art color histogram          would offer converging evidence for our claim that visual
features (Swain & Ballard, 1991), texture features (the Scale       similarity affects categorical search. Of course failing to
Invariant Feature Transform, or SIFT; Lowe, 2004), and              replicate these patterns would weaken this claim, and would
global shape context features (Belongie et al., 2002) with a        raise concerns that the evidence for guidance reported in
well-studied machine learning technique (AdaBoost; Freund
& Schapire, 1997) to create classifiers having features                1
                                                                         Note that this agreement to human behavior does not mean that
tailored for the detection of specific target categories. The       the features and learning method used by this model accurately
advantage of this method over other automated object                describes how humans arrive at their visual similarity estimates.
classification techniques is that similarity estimates can be       Making this correspondence is a goal to which we aspire, but one
based on the contribution of multiple features, not just one.       that we believe is still out of reach. However, this modest level of
   Our second reason for choosing the Zhang et al. (2008)           agreement does suggest that the model’s multi-feature approach
model is that it has already been successfully applied to the       has the potential to generate visual similarity estimates having
                                                                    behavioral significance, which makes it relatively unique with
                                                                    respect to purely automated computational approaches.
                                                                1225

Experiment 1 might have been due to semantic, associative,         condition relative to the mixed condition (t(11) = 5.15, p <
or other non-visual sources of information.                        .01 for teddy bears and t(11) = 6.22, p < .01 for butterflies).
                                                                      Analysis of the number of distractors fixated during
Method                                                             search revealed the same patterns. Fixated distractors varied
Participants Twenty-four Stony Brook University students           with visual similarity for both butterfly targets (F(2,22) =
participated in exchange for course credit, none of whom           74.55, p < .001) and bear targets (F(2,22) = 93.55, p < .001).
participated in Experiment 1. All subjects reported normal         More distractors were fixated on high-similarity trials
or corrected to normal vision. Half searched for a teddy           (2.42±0.20 for bears; 3.66±0.24 for butterflies) compared to
bear target, the other half searched for a butterfly target.       either mixed trials (2.10±0.17 for bears; 2.88±0.23 for
                                                                   butterflies) or low-similarity trials (1.01±0.19 for bears;
Stimuli and Apparatus Experiment 2 was conducted using             1.94±0.24 for butterflies).
the same equipment as in Experiment 1. The stimuli were               The availability of high-, medium-, and low-similarity
also objects selected from the same image set, although the        distractors in mixed condition displays again enabled us to
new selection criteria (described below) required the              look for direct oculomotor evidence for categorical search
potential placement of these objects into different                guidance. Analyses of these trials showed a relationship
conditions. The search displays were therefore different,          between visual similarity and the probability of first fixation
but were assembled from the same set of objects.                   on an object (F(2,22) = 19.42, p < .001 for butterflies;
                                                                   F(2,22) = 36.60, p < .001 for bears – see Figure2B).
Design and procedure Experiments 1 and 2 had the same              Moreover, first fixations on high-similarity distractors were
conditions and followed the same procedure, with the only          well above chance (t(11) = 5.89, p < .01 for bears; t(11) =
difference being the distractor composition of target absent       10.01, p < .01 for butterflies), and first fixations on low-
trials; distractors were now selected based on visual              similarity distractors were well below chance (t(11) = 25.47,
similarity estimates obtained from the Zhang et al. (2008)         p< .01 for bears; t(11) = 8.32 for butterflies), indicating that
model rather than from similarity rankings from the web            initial saccades were guided towards target-similar
task. To derive these similarity estimates we again trained        distractors and away from target-dissimilar distractors.
an AdaBoost-based classifier for each target class using              We also analyzed initial saccade latencies to see whether
color, shape, and texture features, then evaluated these same      these patterns could be attributed to speed-accuracy
features for the distractors to compute target-distractor          tradeoffs, but none were found; initial saccade latencies did
similarity. This resulted in the creation of two rank ordered      not reliably differ between the similarity conditions for
lists, one indicating visual similarity to teddy bears and the     either butterfly (F(2,22) = 1.29, p = 0.30) or bear targets
other to butterflies. High-similarity trials for each target       (F(2,22) = 0.76, p = 0.48). The observed effects of visual
category were then constructed from distractors ranked in          similarity reflect actual changes in search guidance.
the top third of each list, and low-similarity trials were            The conclusion from this experiment is clear. While the
constructed from distractors ranked in the bottom third.           results of Experiment 1 could have been confounded by the
Mixed trials consisted of high-similarity distractors from the     unintentional inclusion of non-visual features in the
top third, low-similarity distractors from the bottom third        behavioral similarity rankings, the same cannot be said for
and medium-similarity distractors from the middle third.           the similarity estimates used in Experiment 2. Even when
                                                                   estimates reflected purely visual features, target-distractor
Results and Discussion                                             similarity still predicted categorical search performance.
                                                                   This strongly suggests that categorical guidance not only
Errors were less than 3% in all conditions and were again
                                                                   exists, but that it may operate in much the same way as
excluded from subsequent analyses. These infrequent errors
                                                                   search guidance from a pictorial target preview. The visual
were likely just motor confusions rather than cases of
                                                                   features used to represent a categorical target may be
confusing teddy bears or butterflies with random objects.
                                                                   different and come from a different source (learned and
   If categorical search is affected by the visual similarity
                                                                   recalled from memory rather than extracted from a target
between our target categories and random distractors, and if
                                                                   preview), but the underlying process of comparing these
the Zhang et al. (2008) model is able to capture these
                                                                   visual features to the search scene and using this signal to
relationships, then RTs should be the slowest on high-
                                                                   guide search may be the same. A goal of future work will
similarity trials, faster on mixed trials, and the fastest on
                                                                   be to determine what these categorical features are for a
low-similarity trials. These predictions were confirmed
                                                                   variety of real-world target classes. The present work
(Table 1). Search efficiency varied with target-distractor
                                                                   constrains this goal by requiring that these features capture
visual similarity for both teddy bears (F(2,22) = 35.84, p<
                                                                   target-distractor visual similarity relationships.
.001) and butterflies (F(2,22) = 60.95, p < .001); post-hoc t-
tests with Bonferroni correction showed slower RTs in the
high-similarity condition relative to the mixed condition                                  Conclusions
(t(11) = 5.77, p < .01 for teddy bears and t(11) = 6.50, p <       Previous research had suggested that search is unguided to
.01 for butterflies) and faster RTs in the low-similarity          categorical targets (e.g., Castelhano et al., 2008; Wolfe et
                                                                   al., 2004). In light of recent evidence, this suggestion
                                                               1226

should be revisited. Multiple studies have now shown                  Research, 46, 4118-4133.
guidance in the very first saccades made to categorical             Cockrill, P. (2001). The teddy bear encyclopedia. New
targets (Schmidt & Zelinsky, 2009; Yang & Zelinsky,                   York: DK Publishing Inc.
2009). The present work extends this finding to non-target          Duncan, J., & Humphreys, G. (1989). Visual search and
objects from categories that are visually similar to the target       stimulus similarity. Psychological Review, 96 (3), 433-
class. Specifically, in the absence of a target our subjects          458.
preferentially directed their initial saccades to distractors       Freund, Y., & Schapire, R. (1997). A decision-theoretic
that were target-similar, and away from distractors that were         generalization of on-line learning and an application to
target-dissimilar (mixed condition; Figure 2). This pattern,          boosting. Journal of Computer and System Sciences,
when combined with the patterns of manual search                      55(1), 119-139.
efficiency found in the high-similarity and low-similarity          Lowe, D. (2004, November). Distinctive image features
distractor conditions (Table 1), provides strong converging           from scale-invariant keypoints. International Journal of
evidence for categorical search guidance in our task. The             Computer Vision, 60(2), 91-110.
fact that these results were obtained despite the highly non-       Pomplun, M. (2006). Saccadic selectivity in complex visual
obvious similarity relationships between random objects and           search displays. Vision Research, 46, 1886-1900.
teddy bears / butterflies, makes the clear expression of            Schmidt, J., & Zelinsky, G. J. (2009). Search guidance is
guidance reported here all the more striking.                         proportional to the categorical specificity of a target cue.
   We can also conclude that these effects of similarity on           Quarterly Journal of Experimental Psychology, 62 (10),
categorical search guidance are visual, and can be well               1904-1914.
described by explicit similarity estimates regardless of            Swain, M., & Ballard, D. (1991, November). Color
whether these estimates were obtained from behavioral                 indexing. International Journal of Computer Vision,
rankings using a web task (Experiment 1) or generated by a            7(1), 11-32.
computer vision model of object category detection                  Treisman, A. M. (1991). Search, similarity, and integration
(Experiment 2). This too is a striking finding. The lengthy           of features between and within dimensions. Journal of
deliberations that accompanied the behavioral judgments,              Experimental Psychology: Human Perception and
and certainly the simplistic visual features underlying the           Performance. 17 (3), 652-676.
model’s estimates, might have easily resulted in no success         Treisman, A. M., & Sato, S. (1990). Conjunction search
whatsoever in predicting categorical search behavior. The             revisited. Journal of Experimental Psychology: Human
fact that these radically different methods both successfully         Perception and Performance, 16, 459-478.
predicted patterns of search guidance is informative,               Wolfe, J. M. (1994). Guided search 2.0: A revised model
suggesting that the computation of visual similarity is not           of visual search. Psychonomic Bulletin & Review 1(2),
only a core cognitive operation, but one that is remarkably           202-238.
stable across method. We speculate that visual similarity is        Wolfe, J. M., Friedman-Hill, S., Stewart, M., & O'Connell,
computed early and automatically during perception, and               K. (1992). The role of categorization in visual search for
once derived is used to mediate a variety of perceptual (e.g.,        orientation.     Journal of Experimental Psychology:
search guidance) and cognitive (similarity judgments)                 Human Perception and Performance, 18, 34-49.
behaviors. To the extent that this is true, it bodes well for       Wolfe, J. M., Horowitz, T. S., Kenner, N., Hyle, M., &
the diversity of researchers in cognitive psychology, human-          Vasan, N. (2004). How fast can you change your mind?
computer interaction, and vision science, all attempting to           The speed of top-down guidance in visual search. Vision
better understand human visual similarity relationships.              Research, 44, 1411-1426.
                                                                    Yang, H., & Zelinsky, G. J. (2009). Visual search is guided
                    Acknowledgments                                   to categorically-defined targets. Vision Research, 49,
We thank Ryan Moore, Jonathan Ryan, and Arunesh Mittal                2095-2103.
for their help with data collection. This work was supported        Zelinsky, G. J. (2008). A theory of eye movements during
by NIH grant R01MH063748-07 to GJZ.                                   target acquisition. Psychological Review 115(4), 787-835.
                                                                    Zhang, W., Samaras, D., & Zelinsky, G. J. (2008).
                                                                      Classifying objects based on their visual similarity to
                         References
                                                                      target categories. Proceedings of the 30th Annual
Belongie, S., Malik, J., & Puzicha, J. (2002, April). Shape           Conference of the Cognitive Science Society (pp. 1856-
   matching and object recognition using shape contexts.              1861).
   Pattern Analysis and Machine Intelligence, 24(4), 509-           Zhang, W., Yu, B., Zelinsky, G. J., & Samaras, D. (2005).
   522.                                                               Object class recognition using multiple layer boosting
Castelhano, M. S., Pollatsek, A., & Cave, K.R. (2008).                with heterogeneous features. Proceedings of the IEEE
   Typicality aids search for an unspecified target, but only         Computer Society Conference on Computer Vision and
   in identification and not in attentional guidance.                 Pattern Recognition (CVPR), 2, 323-330.
   Psychonomic Bulletin & Review, 15(4), 795-801.
Chen, X., & Zelinsky, G. J. (2006). Real-world visual
   search is dominated by top-down guidance. Vision
                                                                1227

