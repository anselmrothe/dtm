UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Building a Model of Infant Social Interaction
Permalink
https://escholarship.org/uc/item/91r3p2h4
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Lewis, Joshua
Deak, Gedeon
Jasso, Hector
et al.
Publication Date
2010-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                                  Building a Model of Infant Social Interaction
                                 Joshua M. Lewis                                          Gedeon O. Deák
                        Department of Cognitive Science                          Department of Cognitive Science
                      University of California, San Diego                       University of California, San Diego
                              San Diego, CA 92093                                       San Diego, CA 92093
                          josh@cogsci.ucsd.edu                                     deak@cogsci.ucsd.edu
                                    Hector Jasso                                           Jochen Triesch
                            hmjasso@gmail.com                                Frankfurt Institute for Advanced Studies
                                                                                Johann Wolfgang Goethe University
                                                                                60438 Frankfurt am Main, Germany
                                                                            triesch@fias.uni-frankfurt.de
                               Abstract                                 simulation environment, creating rich and realistic stimuli for
    Naturalistic observations of infant/caregiver social attention
                                                                        our learning agent.
    have yielded rich information about human social develop-              In the following subsections we will review the theoretical
    ment. However, observational data are expensive, laborious,         issues relevant to this work.
    and reliant on fallible human coders. We model interactions
    between caregivers and infants using a three dimensional sim-          Embodied Modeling The goal of developmental model-
    ulation environment in order to gain greater insight into the       ing is to test theories of learning processes as they take place
    development of infant attention sharing, specifically gaze fol-     within organisms undergoing gross changes. Valid tests of
    lowing. Most models of infant cognition have been only ab-
    stractly linked to the detail of a real life environment and to     these theories require additional theories as to the information
    the perception-and-action physicality of human infants. Our         patterns found in realistically structured environments [3].
    simulation uses human data from videotaped infant/caregiver         Currently, however, we do not possess the computational re-
    interactions and a rich 3D environment to model the develop-
    ment of gaze following. Initial tests suggest that infant gaze      sources to model human perceptual and neural systems, and
    following can be learned in our simulation using parameters         our technological ability to simulate real, multi-modal envi-
    derived from behavioral data.                                       ronments is still primitive. The key, then, is to gradually con-
    Keywords: embodiment; infancy; joint attention; simulation;         verge on a set of biological traits that capture key properties
    social learning.
                                                                        of learning, as well as some key ecological patterns that can
    Human communication is a dauntingly complex system to               be simulated at a level of detail that is appropriate for the
model. Consider a seemingly simple system like an infant                theoretical question at hand. This typically requires consider-
and caregiver playing together: even with language pared                ation of the physicality of the organism and the environment.
away, infant/caregiver social interactions feature a wide range         That is, to test our theories with greater validity we must in-
of behaviors. These take place across many time scales in a             corporate the embodiment of our models [4]. To the degree
complex environment. Moreover, the infant is a moving tar-              that we can embody simulations, we improve our tests of the
get; its brain and behavior change rapidly, and this requires           motivating theory of development and learning.
caregivers to adapt to the infant’s changing skills. Thus it is            Robotic studies are one way to achieve embodied sim-
difficult to generate a powerful model of infant social behav-          ulations, and there are a growing number of good exam-
ior and learning.                                                       ples [5, 6, 7]. Robots can be placed in the same environments
    Developing such a model is important because there is am-           as infants and presented with identical stimuli. Unfortunately
ple evidence that early social development has long term ef-            robotic studies are expensive, and they introduce tangential
fects on (and likely serves as a foundation for) later social           methodological issues—they require solving mechanical and
cognition, language, and even cognitive style and exploratory           computational problems simply to begin testing learning the-
behavior [1]. In this paper we describe a modeling approach             ories. Solving these problems is certainly important for some
that is unique in two key areas, extending the approach intro-          theoretical questions, but it is not currently necessary to ad-
duced in [2]. First, we model both the learning agent (in this          dress basic questions about infant social development. Addi-
case the infant) and the agent’s environment. Many models               tionally, robotic models cannot be run faster than real time,
of infant learning use an abstract symbolic environment with            and they require active supervision. In many cases, current
little relation to the dynamic world infants experience. Ide-           theories can realize faster progress by using simulations that
ally, simulations are comprised of both a biologically plau-            retain elements of embodiment while greatly simplifying im-
sible learning model, and a physically and socially realistic           plementation and reducing cost.
environment [3]. The latter requirement is problematic be-                 Gaze Following We have been investigating the develop-
cause detailed data on the structure of infants’ learning envi-         ment of attention sharing behaviors in human infants. Atten-
ronment only exist in bits and pieces. Our second innovation            tion sharing is a behavioral cornerstone of all social learning.
is to directly tie behavioral data collected by our lab into our        In general it means one or more agents changing their fo-
                                                                    278

cus of attention because they have observed another individ-          that there is no information about caregivers’ meaningful
ual attending to some stimulus or area. A common example              moment-by-moment action patterns. In most experimental
is following the line-of-gaze of another person. There is an          studies of infant social responses, the social input from the
extensive literature on the development of infants’ attention         adult is controlled and extremely artificial (e.g. [9]). Al-
sharing skills. This literature has focused on the development        though these experimental studies are critical for establishing
of gaze following, which is defined as reorienting one’s di-          developmental “benchmarks” that a simulated infant should
rection of gaze to intersect with that of another person, based       replicate, they do not provide information about real infant
on encoding the other’s head pose and/or eye direction.               learning environments, which can be abstracted for simula-
   Infants begin following other people’s gaze between 6 and          tion.
12 months of age, and their ability to follow more and more              Our approach to solving this problems starts by generating
subtle cues, to a wider range of their environment, increases         a dense, rich video dataset of minimally directed interactions
significantly between 9 and 18 months of age [8, 9]. It is            between infants and caregivers. Figure 1 shows one frame of
unknown by what mechanism infants develop more powerful               these interactions from two separate viewpoints. By coding
gaze-following skills.                                                these interactions at 30fps in the manner described below, we
   We have hypothesized [10] that infants’ gaze following             generate a temporally detailed dataset that opens a new win-
skills might emerge as the byproduct of a “basic set” of per-         dow into infant/caregiver interaction in a natural setting.
ceptual, learning, and affective traits that are in place within         In the following sections we will explain our methodolog-
the first 2 to 3 months of age, well before fully developed           ical workflow, describe the machine learning and computer
gaze following can be observed. The basic set theory states           vision techniques driving our simulated infant, present results
that the following elements are sufficient (though not neces-         from the simulation environment, and finally discuss the im-
sary) for joint attention:                                            pact this work has on the modeling of infant social interac-
                                                                      tion.
• A set of motivational biases, in particular a preference for
   social stimuli such as human faces.
                                                                                                 Workflow
• Habituation as a basic reward attenuation mechanism.
                                                                      Our lab takes an end-to-end approach to infant social model-
• A learning mechanism such as temporal difference learn-             ing (see Figure 2)—we start in the lab and in the homes of
   ing [11], to learn the temporal structure of predictable, con-     our subjects by collecting hours of audiovisual data from in-
   tingent interactions between infant and caregiver.                 fant/caregiver interactions. These data consist of both semi-
                                                                      naturalistic free play sessions and scripted lab sessions. In
• Early emerging perceptual traits such as attention shift-           the free play sessions caregivers are instructed to play with
   ing, face processing and sensitivity to motion, contrast, and      their infants using a supplied set of toys while the infant is
   color.                                                             seated in a tray chair. In lab sessions an experimenter per-
                                                                      forms a series of gaze and point maneuvers to salient ob-
• A structured environment providing strong correlation be-
                                                                      jects in the room while holding the infant’s attention. In both
   tween where caregivers look and where interesting things
                                                                      cases the interactions are recorded with audio from multi-
   are.
                                                                      ple camera angles. The lab has amassed many terabytes of
   This basic set of infant traits might be sufficient to gener-      this audiovisual data, which is passed off to a team of under-
ate new attention sharing skills. However, this requires that         graduate research assistants who perform a detailed frame by
the infant learn on a regular regimen of well structured social       frame coding of relevant events (e.g. gaze shifts, manual ac-
input, as provided by an organized caregiver [10]. Our mod-           tions, environmental and toy-generated noise). These codes
eling efforts are meant to prove the plausibility of this theory.     are stored in a database in order to facilitate an automated
If they are unsuccessful, then perhaps additional mechanisms,         analysis of infant/caregiver behavior using custom software
such as special-purpose modules, are necessary for an agent           written in C# and Python. The automated analysis derives
to learn gaze following skills during the first 6-9 months of         information from the coding such as the probability of the in-
human social experience. The question, then, is how to gen-           fant or caregiver to transition from one state to another (e.g.
erate valid simulations of this social learning process. We           from looking at a toy to looking at a social partner), the du-
must imbue the simulated infant with biologically plausible           ration of their actions, and extended events where the infant
perceptual, learning, and motivational traits, and we must im-        and caregiver move through a specified series of states within
bue its environment with a reasonable facsimile of a natural          a restricted time window [12].
social environment.                                                      Our simulation environment can operate in two modes. In
   Naturalistic Social Coding The fine-grained structure of           the first, it simply replicates caregiver behavior from a partic-
infant social environments is difficult to quantify. Although         ular experimental session using the codes in the database. If
it is possible to derive gross patterns from previous obser-          the real-life caregiver started off looking at the infant and then
vational and ethnographic behavioral studies, these tend to           switched to looking at a toy after 2.3 seconds, the simulated
be sparse in details, and coded at such a low sampling rate           caregiver will do the same. In the second mode, the care-
                                                                  279

              Figure 1: Still picture from naturalistic study, from which the simulated caregiver behavior is derived.
giver behaves probabilistically based on the transition proba-
bilities and timings derived from the automated analysis. In
this way, the caregiver behaves realistically without replicat-
ing the steps of any particular subject; the simulation can run
indefinitely. For example, if our data indicate that caregivers
transition from holding an object to holding and moving an
object 20% of the time that they change what they are doing,
then our simulation likewise will make that transition 20% of
the time. In addition, this mode allows our caregiver to (in
principle) respond contingently to previous actions of the in-
fant. Our simulation environment is implemented in C++ and
we use hardware-accelerated OpenGL for the 3D rendering.
Unfortunately, to our knowledge there is no open software
for human simulation, so we use Boston Dynamics’ DI-Guy
platform for rendering and animating our caregiver and props.
Finally, at the end of the chain, our infant learning agent pro-
cesses rendered frames of the simulation using the OpenCV             Figure 2: A flow chart depiction of the data collection, anal-
computer vision library [13]. At each time step of the simu-          ysis and modeling work in our lab, annotated with relevant
lation the only information the infant agent receives about its       technologies.
environment are these rendered frames—it extracts a reward
signal and high level information about the environment using
the computer vision techniques described in the next section.         and action states. It can be: waving or not waving its arm,
                                                                      looking at the infant or the toy, and holding the toy or not.
                            Methods                                   These states correspond to codes for caregiver motion, care-
There are three primary components to our simulation, the             giver gaze target, and caregiver held object status in our em-
caregiver and environment, the infant agent’s visual process-         pirical data. Because our caregiver is simulated as an actual
ing system, and its learning system. In this section we will          body, these discrete behavior states manifest to the infant as
detail the three components, starting with the caregiver and          a wide range of visual stimuli. For example while waving
environment.                                                          an object the caregiver’s arm can be in many positions. Sim-
   Caregiver and Environment Our simulation environment               ilarly, when looking to an object the caregiver’s head pose
is set in the interior of a room containing a table and a chair.      varies over time as the motion is undertaken and the final head
The caregiver is seated at the chair and interacts with toys          pose is based on the actual position of the object in the room.
placed on the table (see Figure 3, top). The caregiver is capa-          From these data we also estimate the probability of tran-
ble of interacting with more than one toy, but for our initial        sitioning between any of the states, and the simulated care-
simulations we used just one toy, a red bus, for simplicity.          giver chooses its actions probabilistically based on these es-
The simulated caregiver occupies several different attention          timates (the caregiver is operating in the second mode de-
                                                                  280

scribed above, not off a script). The caregiver uses two tran-
sition matrices: the first governs behavior with respect to the
toy (holding and waving) and the second governs looking tar-
get. The interval between state transitions is based on the ob-
served interval between separate caregiver behaviors (every
2.18 seconds) plus some uniform noise (+/- 1 second).
   The infant also has a body in the environment (unseen
from its perspective), with its head at about high-chair height.
Changes in infant gaze target are accomplished by tying the
position and orientation of a camera to the position and ori-
entation of this body’s head.
   The objects in the environment are part of the DI-Guy
package, which has a nice variety of (mostly military themed)
props. A text configuration file specifies the props to load at
the start of the simulation as well as their location, orienta-
tion and scale. Similarly, the text file specifies the initial lo-
cation, orientation and appearance of human agents. In this
way we can quickly modify the appearance of the simulation,
add agents, and rearrange props.
   Visual Processing In order for the infant agent to learn
from its raw visual input , it needs to extract high level in-
formation about its environmental state as well as determine
the reward value of the state that it is in. Since we are inter-
ested in gaze following, we extract the caregiver head posi-
tion from the raw image, estimate head pose and use the dis-
cretized pose state as the infant agent’s environmental state.
To do this we first localize the caregiver’s head by calculating
the probability that each pixel in the raw image came from
the known distribution of pixel properties in caregiver head
pixels, running a Gaussian blur over that probability map,
and then centering a head position rectangle over the maxi-
mum probability point on the blurred map. Technically, this
is an application of cvCalcBackProject (to calculate the
back projection of our face color histogram), cvSmooth (the
Gaussian blur) and cvMinMaxLoc (to find the location of
maximum probability in the image) from the OpenCV library.
Pragmatically, we’re only assuming the infant knows broadly
what its caregiver’s face looks like.
   To calculate the head pose, we break the detected head
region up into a left and a right segment then perform a
color histogram comparison between the observed segments
and model segments of left and right facing heads (using
cvCompareHist). From the histogram distances we can
calculate the probability that the caregiver is looking left or        Figure 3: From top to bottom: the raw visual input to
right by seeing how close the observed segments are to the             the infant agent, head detection and pose estimation output,
models. If the segments are distant from both models we can            salience and reward visualization.
infer that the caregiver’s head pose is center. Again, the only
assumption is that the infant knows generally what left and
right facing heads look like. Finally we discretize the head           salience map has three components: motion, contrast, and
pose probability into three states: left, center, and right. A         saturation, and it is similar to salience-based visual pro-
visualization of this head position and pose detection can be          cessing approaches such as the one in [6]. The compo-
seen in Figure 3, middle. The box represents the head posi-            nents are summed to represent overall saliency. Motion
tion and the handles represent the pose probability.                   is calculated by comparison with the previous input frame
   To compute the reward for the current frame of input, we            (cvAbsDiff), contrast is derived from an edge detection
first calculate a salience map over the entire frame. The              routine (cvSobel), and saturation is extracted naturally
                                                                   281

from the color values of the pixels. Reward is then calculated        reward and the advantage in expected reward generally in-
by averaging the saliency values within the agent’s center of         creases over time.
vision (see Figure 3, bottom—the reward area is inside the
rectangle). Since the agent only chooses looking direction on                                   Discussion
the horizontal axis, the center of vision is defined to be taller
                                                                      After a fairly short period of training, the agent expects more
than it is wide.
                                                                      reward when its looking direction is congruent with the care-
   Learning The agent uses a reinforcement learning [11]
                                                                      giver’s head pose than when its looking direction is incongru-
paradigm to choose its actions and learn from the conse-
                                                                      ent. For example, if the caregiver is looking to its left, then
quences. Its state-action space is a cross of the discretized
                                                                      if the infant looks to the right it expects more reward (the in-
caregiver head poses and a set of five looking directions: left,
                                                                      fant and caregiver are facing each other and thus their looking
near left, center, near right, and right. Every time the agent
                                                                      directions to the same location are opposite). Both the near
shifts gaze position, it updates its expected reward for the pre-
                                                                      and far looking directions show this effect. Looking right in
vious state-action pair using the following formula
                                                                      general is privileged because the caregiver is left handed (it
                                                                      only picks up objects with its left hand), and thus during time
        er(i, j)new = er(i, j)old − η(er(i, j)old − ar)               periods where the caregiver is holding the toy it is more likely
                                                                      to be near or far right than near or far left (from the infant’s
where er(i, j) is the expected reward given caregiver head
                                                                      perspective).
pose i and gaze action j, η is a learning rate parameter (set
                                                                         Looking center is always very rewarding since the care-
to 0.1 in our simulation) and ar is the average reward ob-
                                                                      giver is at center. When the caregiver holds an object it will
tained since the last action j in state i. The agent changes
                                                                      often be at center, and when it moves the object it generally
gaze pose after a period of time derived from observed infant
                                                                      is at center or near right. Motion is highly rewarding, and
behavior (every 2.43 seconds) plus some uniform noise (+/- 1
                                                                      the caregiver is normally looking at center during motion, so
second—a more complex but more realistic approach would
                                                                      the center/center expected reward is quite high. The caregiver
be to draw fixation duration from an estimate of the fixation
                                                                      also has a naturally higher contrast than other parts of the en-
duration probability density function from actual infants).
                                                                      vironment.
   It would be straightforward to increase the number of states
and actions (e.g. by giving caregiver and infant looking direc-          This general pattern of results fits other recent findings. It
tion a vertical degree of freedom) and add bells and whistles         seems that infants in everyday setting are highly attentive to
to the reward estimation process, but the purpose of this work        caregivers’ manual actions [12], and this might bootstrap in-
is not to showcase machine learning techniques. Rather, we            fants’ learning of caregivers’ head pose (because adults often
are investigating whether gaze following can be learned given         look at what they are manipulating). It is also known that
a simple learning mechanism, data-driven caregiver behavior,          infants are attracted to faces, and the simulation results are
and a complex simulated environment. The results of this en-          consistent with that. Since the head pose and position estima-
deavor are summarized and discussed in the next section.              tion are not used in calculating reward, the infant agent learns
                                                                      that looking center (where the caregiver’s head is) is valuable
                            Results                                   independent of the general knowledge about head appearance
We ran our simulation for approximately 500 seconds                   that it has.
(enough time for the infant to shift gaze about 200 times) with          These first results show that with a limited set of assump-
the infant agent watching a simulated caregiver interact with         tions, a simple learning model, and a complex data-driven en-
a single toy. The agent’s expected reward over its state-action       vironment, gaze following can be learned. More importantly,
space is detailed in the table below. Looking at a location in        this work sets the stage for even more detailed simulation
the room with background (i.e. smallest) saliency results in          of infant/caregiver interaction—such as interaction between
a reward around 6.0, so that quantity is subtracted from the          more than two agents (a sibling agent, perhaps), reaching and
below numbers.                                                        grasping capability for the infants, and realistic audio cues.
                                                                      Further, since the infant agent no longer receives knowledge
                              Looking Direction                       about its environmental state other than through visual pro-
  CG Pose       left    near left center near right       right       cessing, its input will degrade meaningfully and realistically.
         left   1.30      1.54       3.58       2.62      1.79        For example, if the infant picks up an object that occludes the
      center    1.09      2.62       8.50       3.20      1.97        caregiver, its head position and pose estimates will degrade
       right    1.56      1.72       1.71       1.43      0.76        realistically.
                                                                         In the greater context of understanding infant social de-
Table 1: The final state/action reward space of the infant            velopment, from modeling to robotics to experimental work,
learning agent.                                                       we see this as occupying a productive niche between disem-
                                                                      bodied and discretized 2D models and robotic agents. We
   The course of learning over time is shown in Figure 4. The         open computer simulations up to state and action space com-
agent quickly learns that congruent gaze shifts result in higher      plexities that mirror those in the real world, but our learning
                                                                  282

Figure 4: The sum reward expected from highly congruent gaze shifts (red, top right and bottom left of Table 1) and incongruent
gaze shifts (blue, top left and bottom right) over the training period.
simulations are more convenient and we can have complete                   [7] N. Butko, I. Fasel, and J. R. Movellan. Learning about humans
control over the agent and environment. Moreover, our sim-                     during the first 6 minutes of life. Proceedings of the Interna-
                                                                               tional Conference on Development and Learning, 2006.
ulations do not require the expensive and complicated hard-
ware of robotic simulations; nor do they force us to address               [8] G. Butterworth and N. Jarrett. What minds have in common
interesting but difficult and peripheral questions about motor                 is space: Spatial mechanisms serving joint visual attention in
                                                                               infancy. British Journal of Developmental Psychology, 9:5572,
control.                                                                       1991.
                     Acknowledgments                                       [9] G. O. Deák, R. A. Flom, and A. D. Pick. Effects of gesture
                                                                               and target on 12- and 18-month-olds’ joint visual attention to
The authors would like to thank Ricky Ng for his valuable                      objects in front of or behind them. Developmental Psychology,
assistance on this project. This work was supported by re-                     36:157–192, 2000.
search grants from the M.I.N.D. Institute and the National
Alliance for Autism Research to G. Deák and J. Triesch, a                [10] J. Triesch, C. Teuscher, G. O. Deák, and E. Carlson. Gaze-
National Science Foundation (SES-0527756) to G. Deák, and                     following: why (not) learn it? Developmental Science, 9:125–
NSF IGERT Grant #DGE-0333451 to G. Cottrell and V de                           147, 2006.
Sa.
                                                                          [11] R. S. Sutton and A. G. Barto. Reinforcement Learning: An
                                                                               Introduction. MIT Press, Cambridge, MA, 1998.
                          References
 [1] L. A. Sroufe, B. Egeland, E. Carlson, and W.A. Collins. The          [12] A. Krasno, G. Deák, J. Triesch, and H. Jasso. Watch the hands:
     Development of the Person: The Minnesota Study of Risk and                Do infants learn gaze-following from parents’ object manipu-
     Adaptation from Birth to Adulthood. Guilford, New York,                   lation? In Biennial Meeting of the Society for Research in
     2005.                                                                     Child Development, 2007.
 [2] H. Jasso and J. Triesch. A virtual reality platform for modeling     [13] OpenCV Wiki. http://opencv.willowgarage.com/
     cognitive development. In Biomimetic Neural Learning for In-              wiki/.
     telligent Robots, volume 3575/2005, pages 211–224. Springer
     Berlin / Heidelberg, 2005.
 [3] G. O. Deák, M.S. Bartlett, and T. Jebara. How social agents
     develop: New trends in integrative theory-building. Neuro-
     computing, 70:2139–2147, 2007.
 [4] M. Wilson. Six views of embodied cognition. Psychonomic
     Bulletin and Review, 9:625636, 2002.
 [5] G. Metta, G. Sandini, G. S., and L. Natale. Sensorimotor inter-
     action in a developing robot. In First International Workshop
     on Epigenetic Robotics: Modeling Cognitive Development in
     Robotic Systems, pages 18–19. Lund University Press, 2001.
 [6] Y. Nagai, K. Hosoda, A. Morita, and M. Asada. A construc-
     tive model for the development of joint attention. Connection
     Science, 20:211–229, 2003.
                                                                      283

