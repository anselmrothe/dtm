UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Discovering Structure by Learning Sparse Graphs
Permalink
https://escholarship.org/uc/item/1ww3443p
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Lake, Brenden
Tenenbaum, Joshua
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                               Discovering Structure by Learning Sparse Graphs
                                            Brenden M. Lake and Joshua B. Tenenbaum
                                               Department of Brain and Cognitive Sciences
                                                   Massachusetts Institute of Technology
                                                           {brenden, jbt}@mit.edu
                                                                           a)                           b)
                                 Abstract
    Systems of concepts such as colors, animals, cities, and arti-
    facts are richly structured, and people discover the structure
    of these domains throughout a lifetime of experience. Dis-
    covering structure can be formalized as probabilistic inference
    about the organization of entities, and previous work has op-
    erationalized learning as selection amongst specific candidate         c)                           d)
    hypotheses such as rings, trees, chains, grids, etc. defined
    by graph grammars (Kemp & Tenenbaum, 2008). While this
    model makes discrete choices from a limited set, humans ap-
    pear to entertain an unlimited range of hypotheses, many with-
    out an obvious grammatical description. In this paper, we
    approach structure discovery as optimization in a continuous
    space of all possible structures, while encouraging structures
    to be sparsely connected. When reasoning about animals and
    cities, the sparse model achieves performance equivalent to
    more structured approaches. We also explore a large domain
    of 1000 concepts with broad semantic coverage and no simple
    structure.
    Keywords: structure discovery, semantic cognition, unsuper-
    vised learning, inductive reasoning, sparse representation           Figure 1: Structure learned by the structural forms model for
                                                                         colors (a) and mammals (c), compared to the sparse model (b,
    The act of learning is not just memorizing a list of facts;          d). Shorter edges correspond to stronger connections. Graphs
instead people seem to learn specific organizing structures              in this paper, except cities, were drawn with Cytoscape.
for different classes of entities. The color circle captures the
structure of pure-wavelength hues, a tree captures the biolog-
ical structure of mammals, and a 2D space captures the geo-              shared habitat, role as predator versus prey, and size. While
graphical structure of cities (Fig. 1a, 1c, 5a). How does the            these factors cannot be perfectly explained by a single tree,
mind discover which type of structure fits which domain?                 other domains are interestingly structured and are even fur-
    Discovering structure can be understood computationally              ther removed from a clean form, such as artifacts and social
as probabilistic inference about the organization of entities.           networks. Since humans learn and reason about all of these
Past work has tackled this problem by considering rings,                 domains, they must entertain structural hypotheses without
trees, chains, grids, etc. as mutually exclusive hypotheses              obvious grammatical descriptions.
called structural forms (Kemp & Tenenbaum, 2008). Forms                     These considerations have motivated models without an
are defined by grammatical constraints on the connections                explicit representation of structure. Rogers and McClelland
between entities; for example the ring form constrains each              (2004) demonstrated how structure can emerge in a connec-
color to have two neighbors (Fig. 1a). After considering all             tionist network mapping animals (like canary) and relations
of the candidate forms, the structural forms model selects the           (can) to output attributes that a canary can do (grow, move,
best fitting form and instance of that form. This can be a pow-          fly, and sing). Without being constrained to follow a tree,
erful approach; the model selects a ring for colors, a tree for          their network learns a distributed representation that approxi-
mammals, and a globe-like structure for world cities. These              mates a tree. But Kemp and Tenenbaum (2009) suggest some
structures can then predict human inductive reasoning about              advantages of explicit representation: for incorporating ob-
novel properties of objects (Kemp & Tenenbaum, 2009).                    servations that have direct structural implications (“Indiana is
    Despite its power, the structural forms approach is not              next to Illinois”) and for learning higher-level knowledge (a
clearly appropriate when structures stray from the prede-                tree helps learn the word “primate”, Fig 1c). It also remains
fined forms, and such exceptions are common in real world                to be seen if this model can predict human inductive infer-
domains. While the genetic similarity of animals is cap-                 ences about animal properties, as past researchers have found
tured by an evolutionary tree,1 everyday reasoning about ani-            this difficult (Kemp, Perfors, & Tenenbaum, 2004).
mals draws on factors that span divergent branches, including               Here, we present an approach to structure discovery that in-
     1 Even this structure has exceptions; for example, Rivera and       corporates some of the best features of previous probabilistic
Lake (2004) provide evidence that at the deepest levels “the tree of     and connectionist models. Rather than selecting between dis-
life is actually a ring of life” where genomes fused.                    crete structural hypotheses defined by grammars, the model
                                                                     778

learns structure in an unrestricted space of all possible graphs.          Generative model of features. Following the formula-
In order to achieve good inductive generalization, there must           tion in Zhu, Lafferty, and Ghahramani (2003), a particu-
be a method for promoting simple graphs. While Kemp                     lar property vector f (k) , observed for all n objects f (k) =
and Tenenbaum (2008) used grammars, here we use sparsity,                  (k)         (k)
                                                                        ( f1 , ..., fn ), is modeled as
meaning only a small number of edges are active. This struc-
                                                                                                    1                (k)    (k)      1
tural freedom can approximate cleaner structural forms, such              p( f (k) |W ) ∝ exp(−             wi j ( fi − f j )2 − 2 f (k)T f (k) ).
as the ring-like graph for colors in Fig. 1b learned from simi-                                     4∑ i, j                         2σ
larity data printed in Shepard (1980), and on other datasets it
                                                                        This defines a notion of feature smoothness, and it is equiva-
deviates, such as mammals (Fig. 1d). Often these deviations
                                                                        lent to the n-dimensional Gaussian distribution
capture additional information; while the tree suggests squir-
rels and mice are equidistant from chimps, the sparse struc-                                     p( f (k) |W ) ∼ N(0, ∆˜ −1 ),
ture suggests squirrels and chimps share additional similarity,
                                                                        where ∆˜ = Q −W + I/σ2 is the precision (inverse covariance)
like their association with trees.
                                                                        matrix, Q = diag(qi ) is a diagonal matrix with entries qi =
   The sparse model achieves performance equivalent to
                                                                        ∑ j wi j , and I is the identity matrix. We also restrict wi j ≥ 0,
more structured approaches when predicting human inductive
                                                                        so the model represents only positive correlations. The model
judgements. We show this for biological properties of ani-
                                                                        assumes the feature mean is zero, and raw data is scaled such
mals and geographical properties of cities (Kemp & Tenen-
                                                                        that the mean value in D is zero and the maximum value in
baum, 2009). Due to the model’s computational efficiency, it
                                                                        covariance m1 DDT is one. The parameter σ2 can be thought
can learn on datasets too large for most previous approaches.
                                                                        of as the a priori feature variance (Zhu et al., 2003), and we
We demonstrate learning a structure for 1000 concepts with
                                                                        choose the value that maximizes the objective function.
broad semantic coverage, resembling classical proposals for
                                                                           Sparsity penalty. To complete the model, we need a prior
semantic networks (Collins & Loftus, 1975).
                                                                        distribution on graph structures, p(W ). To learn a simple
                                                                        graph representation with a minimal number of edges, we
                      The Sparse Model
                                                                        assume each weight p(wi j ) is independently drawn from a
In the structural forms and sparse models, a structure defines          distribution p(wi j ) ∼ Exponential(β), meaning
how objects covary with regard to their features. Objects are
nodes in a weighted graph, where the strength of connectivity                                    p(W ) =          ∏      βe−βwi j .
between two objects is related to the strength of covariation                                                1≤i< j≤n
with regard to their features. The weights of the graph, de-            This prior encourages small weights, and in practice it pro-
noted as the symmetric matrix W , are learned from data by              duces sparse graph structures by forcing most weights to zero.
optimizing an objective function that trades off the fit to the            Structure Learning. Finding argmax log p(W |D) is equiv-
data with the sparsity of the graph.                                                                                      W
                                                                        alent to the following convex optimization problem:
   The data D is an n x m matrix with n objects and m features.
The columns of D, denoted as features { f (1) , ..., f (m) }, are                                     ˜ − trace(∆˜ 1 DDT ) − β ||W ||1
                                                                                   maximize log |∆|
assumed to be independent and identically distributed draws                          ˜
                                                                                     ∆0,W,σ 2                           m          m
from p( f (k) |W ). If the graph structure fits the data well, fea-
                                                                                    subject to
tures should vary smoothly across the graph. For example,
if two objects i and j are connected by a large weight wi j                                    ∆˜ = diag(∑ wi j ) −W + I/σ2
                                                                                                                j
(like seal and dolphin), they often share similar property val-
ues (“is active” or “lives in water”). As a result of sparsity,                                wii = 0, i = 1, . . . , n
most objects are not directly connected in the learned graph                                   wi j ≥ 0, i = 1, . . . , n; j = 1, . . . , n
(wi j = 0, like dolphin and chimp), meaning they are condi-
                                                                                               σ2 > 0.
tionally independent when all the other objects are observed.
   Formally, the undirected graph W defines a Gaussian dis-             The first term in the objective, log |∆|            ˜ − trace(∆˜ 1 DDT ), is
                                                                                                                                           m
tribution p( f (k) |W ), known as a Gaussian Markov Random              proportional to the log-likelihood from Kemp and Tenenbaum
Field (GMRF), where the n objects are the n-dimensions of               (2008) after dropping unnecessary constants, and mβ ||W ||1 ,
the Gaussian. Learning GMRFs with sparse connectivity has               where ||W ||1 = ∑ni=1, j=1 |wi j |, comes from the log-prior. ∆˜ 
a long history (Dempster, 1972), and recent work has formu-             0 denotes a symmetric positive definite matrix. The only free
lated this as a convex optimization problem that can be solved          parameter, β, controls the tradeoff between the log-likelihood
very efficiently, in O(n3 ), for the globally optimal structure         of the data and the sparsity penalty (||W ||1 ). A larger β en-
(e.g., Duchi, Gould, & Koller, 2008). Following Kemp and                courages sparser graphs. As more features are observed (m
Tenenbaum (2008), we assume people learn a single set of                increases), the likelihood is further emphasized in the trade-
parameters that fits the observed data well. Thus, we find              off. For all simulations, we set β = 14. The solution was
the maximum a posteriori (MAP) estimate of the parameters               found using CVX, a package for solving convex programs
argmax log p(W |D) = argmax log p(W ) + ∑m                   (i)
                                                 i=1 log p( f |W ).     (Grant & Boyd, n.d.).
   W                         W
                                                                    779

Figure 2: The (a) tree and (b) sparse graphs learned for mammals. Shorter edges in the tree correspond to stronger weights.
The sparse graph is overlaid by node position, and thus edge length does not indicate strength. Strong edges w > .2 are in bold.
                     Taxonomic reasoning                                  tree. For instance, hippo is connected to the blue whale and
Kemp and Tenenbaum (2009) learned a tree structure from a                 walrus; although distant in the taxonomy, they are large and
dataset of 50 mammals and 85 biological properties collected              live in/around water. Similarly spider monkey and squirrel
by Osherson, Stern, Wilkie, Stob, and Smith (1991). Prop-                 have a new link, perhaps due to agility and living in trees.
erties were various kinds of biological and anatomical fea-                  Property induction. A learned structure defines a prior
tures, including “is smart,” “is active,” and “lives in water,”           distribution on properties of animals, which can be used for
and participants rated the strength of the association between            induction about new properties. Learning often involves gen-
each mammal and feature. The learned tree achieves high cor-              eralizing new properties to familiar animals; when a child
relations when predicting human inductive judgments about                 first hears about the property “eats plankton,” the child makes
novel biological properties. This predictive success may be               decisions about which mammals this property extends to.
due to the origin of these properties in the natural world; bio-          To test the sparse and tree model, we apply them to two
logical relatedness is determined by an evolutionary process              classic datasets of human inductive judgments collected by
where species split and branch off. But there are reasons to              Osherson, Smith, Wilkie, Lopez, and Shafir (1990), which
suspect humans learn more complicated cognitive structures                were also used in Kemp and Tenenbaum (2009). Judgments
due to shared similarity across divergent branches, as dis-               concerned 10 species: horse, cow, chimp, gorilla, mouse,
cussed in the introduction. Rather than constraining structure            squirrel, dolphin, seal, and rhino (Fig 1). Participants were
to be a tree, perhaps optimization with a sparsity constraint             shown arguments of the form “Cows and chimps require bi-
can learn appropriate structure for taxonomic reasoning.                  otin for hemoglobin synthesis. Therefore, horses require bi-
   Learning structure. Fig. 2 compares a tree learned by                  otin for hemoglobin synthesis.” The Osherson horse set con-
the structural forms model and a graph learned by the sparse              tains 36 two-premise arguments with the conclusion “horse,”
model for the mammals dataset.2 The sparse model has                      and the mammals set contains 45 three-premise arguments
19% of possible edges active (w > .01), and stronger edges                with the conclusion “all mammals.” Participants ranked each
are highlighted in the figure. While the sparse model does                set of arguments in increasing strength by sorting cards.
not learn a tree, it captures some important aspects of the                  We compare the inductive strength of each argument for
tree-based model. Major branches of the tree correspond to                both the models and the participants (averaged rank across
densely connected regions of the sparse model. The sparse                 participants). Following Kemp and Tenenbaum (2009), to
graph captures some additional detail not represented by the              compute inductive strength in the models, we calculated the
                                                                          posterior probability that all categories in a set Y have the
    2 The antelope had four missing color features which were filled
                                                                          novel feature (in the above example Y = {horses})
in from giraffe. They were left missing in the structural forms work.
When learning any model from Kemp and Tenenbaum (2009), the
best fitting σ2 variance parameter was found, as in the sparse model.                                    ∑ f : fY =1, fX =lX p( f )
                                                                                        p( fY = 1|lX ) =                            .  (1)
In the original work this parameter was fixed at σ = 5.                                                     ∑ f : fX =lX p( f )
                                                                      780

A binary label vector lx is a partial specification of a full bi-             by adding four different mammals, where each addition was
nary feature vector f that we want to infer. In the above ex-                 replicated 30 times with different random sets of observed
ample, X = {cows, chimps} and lX = [1, 1] indicating both                     properties. For each missing property, its expected value was
cows and chimps have biotin. Intuitively, Equation 1 states                   calculated by performing inference in the Gaussian defined
that the posterior probability p( fY = 1|lX ) is equal to the pro-            by the structure. Compared to the raw covariance matrix, the
portion of possible feature vectors consistent with lX that also              sparse model provided significantly better predictions of the
set fY = 1, where each feature vector is weighted by a prior                  missing features for each mammal tested (all 8 comparisons
probability p( f ) defined by the structure. We compute p( f )                t(29), p < .01, Fig. 4). Since running all combinations is
by drawing 106 feature samples from the Gaussian defined by                   slow in the tree model, each model was also compared on an
that structure, converted to binary by thresholding at zero.                  “informative feature set” ( *’s in Fig. 4), defined as the fea-
   Performance of the sparse model is shown in column 1                       ture set the raw covariance performed best on. For learning a
of Fig. 3. The sparse model and tree-based model (column                      new object with these features, the sparse model performs at
2) perform equivalently and predict the participant data well.                least as well as the tree model.
Both models outperform a spatial model (column 3, see Eq.
                                                                                                  Added with 10 features             Added with 20 features
2) which embeds the animals in a 2D space, with particular                                       1                                  1                   informative
                                                                                                              sparse         raw           tree
advantage on the mammals dataset. The sparse, tree, and spa-                                                                                            feature set
tial models can be viewed as “cleaning up” the raw covariance                                   0.9                                0.9
         1
matrix 85  DDT , approximating it as closely as possible while
                                                                                  correlation
satisfying certain constraints (sparsity, tree grammar, or 2D                                   0.8                                0.8
embedding). When compared to the raw covariance (column
4), the sparse and tree model show better performance.                                          0.7                                0.7
               r=0.91     r=0.95        r=0.84        r=0.81                                    0.6                                0.6
    Osherson
                                                                                                0.5                                0.5
     horse
                                                                                                      horse dolphin lion chimp           horse dolphin lion chimp
               r=0.91     r=0.88        r=0.73         r=0.8
                                                                              Figure 4: Each model adds a new object (seeing only 10 or
                                                                              20 features), and the missing features are predicted. Bars are
    Osherson                                                                  mean performance over 30 random feature picks, and stars
    mammals
                                                                              (*) show performance from a single informative feature set.
               Sparse      Tree         Spatial        Raw
                                                                                                                 Spatial reasoning
Figure 3: Model performance on taxonomic reasoning. Hu-                       Geographical knowledge seems to require different structural
man ratings of argument strength (y-axis) are plotted against                 representations than animals. Following the tradition of using
the model ratings (x-axis) for each argument.                                 Euclidean spaces to build semantic representations such as
                                                                              multidimensional scaling (Shepard, 1980), Kemp and Tenen-
   Learning about new objects. In addition to learning                        baum (2009) proposed learning a 2D space to represent the
about new properties, people constantly encounter new ob-                     relationship between cities. This 2D space defines a Gaus-
jects. How do the models learn about a new mammal, ob-                        sian distribution with zero mean and covariance matrix K
served for just a few features? The tree-based model pro-
                                                                                                       1       1
vides strong grammatical guidance, but it might be difficult                                   Ki j =    exp(− ||yi − y j ||2 ),           (2)
                                                                                                      2π       σ
to make discrete placement decisions with only a few ob-
served features. By contrast, the sparse model has no gram-                   where yi is the location of the city i in 2D space. Kemp and
matical guidance, so this provides an interesting comparison.                 Tenenbaum (2009) found a double dissociation between the
Adding a new concept to the sparse model involves solving                     tree model and the spatial model, which only perform well on
two convex programs. First, the model was trained on all                      taxonomic and spatial reasoning respectively. Can the sparse
but one mammal (49) and all properties (85). Second, the                      model learn structures applicable to both domains?
learned connections and variance were frozen, and the new                        Learning structure. Structures were learned from partici-
concept was added while observing only a few features (10                     pant drawings of nine cities on a piece of paper, and similarity
or 20).3 Performance was evaluated on predictive ability for                  was calculated from the pairwise distances (Kemp & Tenen-
the missing properties (75 or 65). The models were tested                     baum, 2009). This similarity matrix was treated as the raw
                                                                              covariance input to all the models. The learned spatial repre-
   3 Since  many data entries are missing, simply skipping miss-              sentation is compared to the learned sparse graph in Fig. 5.
ing entries results in a covariance matrix that is not positive semi-
definite. Instead we use a maximum likelihood estimate of the co-             All the models require an assumed number of features, set to
variance matrix found by Expectation-Maximization.                            m = 85, preserving the β/m sparsity ratio from before.
                                                                        781

a)                                          b)
Seattle    Minneapolis           Boston     Seattle        Minneapolis     Boston
                                 Durham
            Denver                                      Denver            Durham
San Francisco                               San Francisco
                                 Orlando
San Diego Houston                           San Diego         Houston    Orlando
Figure 5: The (a) spatial and (b) sparse models learned from
the city dataset. Graphs nodes are overlaid on the 2D space.
                        r=0.83     r=0.44        r=0.8         r=0.89
          Minneapolis
                        r=0.73     r=0.38        r=0.75        r=0.81
                                                                                         Figure 7: Structure learned for 1000 concepts. This small
                                                                                         subset shows the significant neighbors of the bold nodes (w >
          Houston
                                                                                         .2 except dotted edge w = .09). Shorter edges are stronger.
                        r=0.76     r=0.54        r=0.83        r=0.86
                                                                                         cities by 2D spaces, what type of structure organizes concepts
          All cities
                                                                                         as diverse as fruit, vegetable, fish, penguin, building, and col-
                                                                                         lege? Human semantic reasoning operates in a huge semantic
                        Sparse      Tree         Spatial        Raw                      space, and here we learned a sparse model on an expansive
                                                                                         domain of 1000 entities and 218 properties. A dataset of this
                                                                                         size is prohibitive for the structural forms model as well as
Figure 6: Model performance on spatial reasoning. Human
                                                                                         the connectionist model of Rogers and McClelland (2004).
ratings of argument strength (y-axis) are plotted against the
model rating (x-axis) for each argument.                                                    Dataset and Algorithm. The dataset was collected by In-
                                                                                         tel Labs (Palatucci, Pomerleau, Hinton, & Mitchell, 2009).
                                                                                         Semantic features were questions such as “Is it manmade?”
   Property induction. As in the taxonomic reasoning sec-                                and “Can you hold it?” Answers were on a 5 point scale
tion, the models were compared to human data regarding                                   from definitely no to definitely yes, conducted on Amazon
property generalization. In an experiment by Kemp and                                    Mechanical Turk. To learn the optimal structure, we use a
Tenenbaum (2009), participants were presented a scenario                                 faster algorithm from Duchi et al. (2008) instead of a generic
where Native American artifacts can be found under most                                  convex solver. For now, this requires two small changes to the
large cities, and some kinds of artifacts are found under just                           model: wi j can be positive or negative and a separate variance
one city while other are under a handful of cities. An ex-                               term σ2i is fit to each object instead of one for all objects.
ample inductive argument is: “Artifacts of type X are found                                 Results. The structure learned from the entire data is very
under Seattle and Boston. Therefore, artifacts of type X are                             sparse with approximately 2.4% of edges active (|w| > .01).
found under Minneapolis.” There were 28 two-premise argu-                                Fig. 7 shows snapshots of the network, consisting of nodes
ments with Minneapolis as the conclusion, 28 with Houston                                that are strong direct neighbors of either fruit, vegetable, fish,
as the conclusion, and 30 three-premise arguments with “all                              penguin, building, and college (w > .2). Fruit and vegetable
large American cities” as the conclusion. These arguments                                are linked to subordinate examples, and connect to fish via a
were ranked for strength, and mean rank was correlated with                              path through food. Interestingly, the network connects pen-
the model inductive predictions. The sparse model (column 1                              guin to both sea animals (like fish and seal) and birds, high-
of Fig. 6) provides good predictions, as does the 2D spa-                                lighting its role as an aquatic bird. Building and college
tial model and the raw covariance matrix, which performs                                 are connected via several paths, including building–hotel–
best (columns 3 and 4). The tree performs poorly (column                                 university–college and building–hotel–hospital–college.
2). While there is a double dissociation between the tree and                               To evaluate the sparse model’s predictive capacity for novel
spatial model for taxonomic and spatial reasoning, the sparse                            questions, we performed 4-fold cross validation, training on
model can predict human reasoning in both contexts.                                      3/4 of the properties and predicting the rest. The average
                                                                                         test log-likelihood is −3.50 · 104 for the sparse model and
     Discovering structure for 1000 concepts                                             −3.84 · 106 for the raw covariance. The raw covariance per-
Learning sparse graphs can also be applied to domains with                               forms worse than in the past experiments since there are many
no simple structure. While animals may be fit by trees and                               more objects than features, and performance can be improved
                                                                                   782

by other regularization techniques such as Tikhonov (com-            Dempster, A. P. (1972). Covariance selection. Biometrics,
puted as m1 DDT +vI for identity matrix I (Duchi et al., 2008)),       28, 157-175.
which achieves a test log-likelihood of−3.63 · 104 . Tikhonov        Duchi, J., Gould, S., & Koller, D. (2008). Projected subgradi-
regularization does not significantly improve the raw covari-          ent methods for learning sparse gaussians. In Proceedings
ance on the previous property induction tasks. Even though             of the twenty-fourth conference on uncertainty in AI (UAI).
we fine-tuned the Tikhonov parameter v = .17 to the test             Grant, M., & Boyd, S. (n.d.). CVX: Matlab software for
sets, the sparse model still performs better with its parame-          disciplined convex programming. Retrieved 2009, from
ter β = 14 fixed across all experiments in this paper.                 http://stanford.edu/∼boyd/cvx
                                                                     Kemp, C., Perfors, A., & Tenenbaum, J. B. (2004). Learn-
                   General Discussion                                  ing domain structures. In Proceedings of the twenty-sixth
Here we applied the sparse model to taxonomic and spatial              annual conference of the cognitive science society.
reasoning. Past work has found a double dissociation be-             Kemp, C., & Tenenbaum, J. B. (2008). The discovery of
tween these inductive contexts (Kemp & Tenenbaum, 2009),               structural form. Proceedings of the National Academy of
where a tree model and a spatial model provide good fits to            Sciences, 105(31), 10687-10692.
only one context. However the sparse model is able to predict        Kemp, C., & Tenenbaum, J. B. (2009). Structured statisti-
human inductive judgments in both contexts, by emphasiz-               cal models of inductive reasoning. Psychological Review,
ing sparsity in structural representation. In addition to these        116(1), 20-58.
inductive tasks, we applied the sparse model to a dataset of         Osherson, D. N., Smith, E. E., Wilkie, O., Lopez, A., &
1000 concepts with broad semantic coverage and no simple               Shafir, E. (1990). Category-based induction. Psychological
structure. The sparse model learned reasonable structure and           Review, 97(2), 185-200.
outperforms simple regularization on novel features.                 Osherson, D. N., Stern, J., Wilkie, O., Stob, M., & Smith,
   The sparse model also provides a probabilistic foundation           E. E. (1991). Default probability. Cognitive Science, 15,
for classic models of semantic memory such as semantic net-            251-269.
works (Collins & Loftus, 1975). Semantic networks stipulate          Palatucci, M., Pomerleau, D., Hinton, G., & Mitchell, T.
that concept nodes are connected to related concepts by vary-          (2009). Zero-shot learning with semantic output codes. In
ing degrees of strength. These networks resemble the large             Y. Bengio, D. Schuurmans, & J. Lafferty (Eds.), Advances
structure learned for 1000 concepts (Fig. 7), suggesting the           in neural information processing systems (NIPS).
sparse model can be used to learn semantic networks from             Rivera, M., & Lake, J. (2004). The ring of life provides
data. The sparse model is also related to Pathfinder networks          evidence for a genome fusion origin of eukaryotes. Nature,
(Schvaneveldt, Durso, & Dearholt, 1989) that find the mini-            431, 152-155.
mal graph that maintains all pairwise sum-over-path distances        Rogers, T. T., & McClelland, J. L. (2004). Semantic cog-
between objects. While highlighting important structure, it            nition: A parallel distributed processing approach. Cam-
retains the same similarity matrix from input to output, lack-         bridge, MA: MIT Press.
ing the regularization that is important in our simulations.         Schvaneveldt, R. W., Durso, F. T., & Dearholt, D. W. (1989).
   While the sparse model is an important first step, it leaves        Network structures in proximity data. In G. Bower (Ed.),
out desirable features of previous connectionist and proba-            The psychology of learning and motivation: Advances in
bilistic models. The Rogers and McClelland (2004) model                research and theory (Vol. 24). Academic Press.
accounts for a rich array of phenomena from development              Shepard, R. N. (1980). Multidimensional scaling, tree-fitting,
and semantic dementia, yet to be explored with the sparse ap-          and clustering. Science, 210, 390-398.
proach. Compared to structural forms, the sparse model does          Zhu, X., Lafferty, J., & Ghahramani, Z. (2003). Semi-
not learn latent nodes (compare Fig. 1c,d), which increase             supervised learning: From gaussian fields to gaussian pro-
sparsity and could be important for learning higher-level con-         cesses (Tech. Rep. No. CMU-CS-03-175). Carnegie Mel-
cepts such as “mammal” or “primate” (Kemp & Tenenbaum,                 lon University.
2009). Future work will use the sparse approach to explore
learning deeper conceptual structure with latent variables.
                   Acknowledgements
We thank Intel Labs for providing their 1000 objects dataset,
Charles Kemp for providing code and datasets for learn-
ing structural forms, and Venkat Chandrasekaran, Ruslan
Salakhutdinov, and Frank Jäkel for helpful discussions.
                         References
Collins, A. M., & Loftus, E. F. (1975). A spreading activation
   theory of semantic processing. Psychological Review, 82,
   407-428.
                                                                 783

