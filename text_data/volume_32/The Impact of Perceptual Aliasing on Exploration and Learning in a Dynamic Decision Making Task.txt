UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Impact of Perceptual Aliasing on Exploration and Learning in a Dynamic Decision
Making Task
Permalink
https://escholarship.org/uc/item/5169k6m2
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Zaval, Lisa
Gureckis, Todd M
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

   The Impact of Perceptual Aliasing on Exploration and Learning in a
                                     Dynamic Decision Making Task
                                         Lisa Zaval (lz2261@columbia.edu)
                                      Columbia University, Department of Psychology
                           416 Schermerhorn, 1190 Amsterdam Ave., New York, NY 10027 USA
                                   Todd M. Gureckis (todd.gureckis@nyu.edu)
                                      New York University, Department of Psychology
                                      6 Washington Place, New York, NY 10003 USA
                           Abstract                               any moment. Across these cases, the decision-making
                                                                  ability of the learner is expected to improve as the po-
   Perceptual aliasing arises in situations where multiple,       tential confusion is reduced, and relevant states in the
   distinct states of the world give rise to the same per-
   cept. In this study, we examine how the degree of per-         world become mapped to diﬀerentiated percepts.
   ceptual aliasing in a task impacts the ability of human           In this paper, we examine how the degree of percep-
   agents to learn reward-maximizing decision strategies.         tual aliasing in a task environment impacts the ability
   Previous work has shown that the presence of percep-
   tual cues that help signal distinct states of the envi-        of humans to learn eﬀective decision strategies in a dy-
   ronment can improve the ability of learners to adopt           namic task environment. A growing body of work sug-
   an optimal decision strategy in sequential decision mak-       gests that human trial-and-error learning shares a similar
   ing tasks (Gureckis & Love, 2009). In our experiments,
   we parametrically manipulated the degree of perceptual         computational foundation with algorithms developed in
   aliasing aﬀorded by certain perceptual cues in a sim-          the reinforcement learning (RL) literature (see Dayan
   ilar task. Our empirical results and simulations show          & Daw, 2008 for a review). However, less work has ex-
   how the ability of the learner improves as relevant states
   in the world uniquely map to diﬀerentiated percepts.           amined how the identification and categorization of dis-
   The results provide further support for the model of se-       tinct task states might interact with these learning and
   quential decision making proposed by Gureckis & Love           decision-making processes to determine human perfor-
   (2009) and highlight the important role that state rep-
   resentations may have on behavior in dynamic decision          mance.
   making and learning tasks. Keywords: perceptual
   aliasing, dynamic decision making, reinforcement learn-        Previous Work
   ing
                                                                  Our work builds upon previous studies of behavior in
                                                                  the “Farming on Mars” task (Gureckis & Love, 2009b,
                       Introduction                               2009a; Otto, Gureckis, Love, & Markman, 2009). In
A crucial problem facing both human and artificial learn-         this task, participants make repeated selections between
ers is correctly perceiving and interpreting the current          two “robots” presented on a computer screen. Selec-
state of the environment. For instance, imagine a trav-           tion of each robot results in a certain number of “oxy-
eler staying in an unfamiliar hotel, with each floor and          gen” points. Participants’ goal is to maximize the to-
exit decorated identically. Based on perceptual cues              tal amount of oxygen generated over the entire experi-
alone, this guest may experience diﬃculty navigating              ment. One robot (the “Short-term” option) always re-
towards his room, since each floor is eﬀectively indistin-        turns more points than the other (the “Long-term” op-
guishable. In order for navigation to be successful, the          tion). However, unknown to participants at the start
traveler must overcome the problem of perceptual alias-           of the task, the experienced reward structure (i.e., pay-
ing, in which relevant “states” or situations in the world        oﬀ for selecting either robot) continually changes in re-
map to a single percept (Whitehead & Ballard, 1991;               sponse to the recent choice history of the participant.
McCallum, 1993). In this example, that current state              In particular, a dynamic is set up so that when the
is the location of the traveler in the building, and the          immediately attractive alternative is selected (i.e., the
percept is the various cues available that might indicate         Short-term option), the long-term expected value of both
this location. Note that environments may be aliased              robots is generally lowered on the following trial (Fig-
along a continuum from the perspective of any individ-            ure 1 illustrates the payout function used in previous
ual. For example, suppose that only every other floor             Farming on Mars task experiments). Conversely, selec-
in the building is decorated identically. In this case,           tions of the immediately worse option (the Long-term
the guest will be able to diﬀerentiate at least half the          option) cause the expected value of both options to in-
floors, and his ability to navigate might be somewhat             crease (in particular, the payoﬀ for each option depends
improved. This example can be extended to cases where             on the number of selections of the Long-term option over
each floor of the hotel is uniquely decorated, such that          the last nine trials). As a result, the optimal reward-
salient perceptual cues indicate the traveler’s location at       harvesting strategy is to learn to choose the option that
                                                              162

                                                                                                         Highly Aliased	  (10:1	  in	  a	  10-state	  task)	  
                                                                                                                     Short-Term                       Long-Term
                                                                                                                       Robot                            Robot
                                                                                                                      Choices                          Choices
                                                                                Decreasing	  Aliasing
                                                                                                         Less Aliased (2:1	  in	  a	  10-state	  task)
                                                                                                                              Long-Term	  Robot	  Choices
                                                                                                                              Short-Term	  Robot	  Choices
                                                                                                         Distinct States (1:1	  in	  a	  10-state	  task)
                                                                                                                              Long-Term	  Robot	  Choices
Figure 1: Illustrative payout function of the Farming on
Mars Task. The horizontal axis in the figure represents the                                                                   Short-Term	  Robot	  Choices
number of selections out of the last nine in which the Long-
Term robot was chosen. The upper diagonal line measures
the reward earned from choosing the Short-Term robot as a              Figure 2: Degrees of perceptual aliasing. At the top is an
function of recent choice history, while the lower line illus-         example of a highly aliased environment where multiple dis-
trates the reward produced from Long-Term selections.                  tinct states maps onto a single percept (many-to-one). At the
                                                                       other extreme, distinct perceptual information disambiguates
                                                                       all states (one-to-one). Intermediate levels maps a subset of
                                                                       states to a single percept.
appears worse on each individual trial, since this strategy
                                                                       The Present Studies
leads to the greatest cumulative reward.
                                                                       The present studies were designed to test a key predic-
   Critically, performance in the task requires an appro-              tion of Gureckis & Love’s RL model. As anticipated by
priate balance of exploration (in order to discover the                our example of the traveler in an unfamiliar hotel, the
hidden contingencies) as well as exploitation of choice                perceptual aliasing of states in the environment to dis-
options known to be rewarding. In addition, a key ob-                  tinct percepts can vary along a continuum (see Figure 2).
servation about this task is that there are multiple dis-              At one extreme, every state in the world could map to
tinct “states” of the environment (which correspond to                 the same percept (a many-to-one relationship). At the
the number of Long-term robot selections over the pre-                 other extreme, each state in the world could map to a dis-
vious trials). When participants fail to recognize this                tinct percept (a one-to-one relationship). Intermediate
structure, and the fact that the state of the system is                cases exist where only a subset of distinct environmen-
changing as a function of their past response history, it              tal states are perceptually aliased. One possibility is that
becomes diﬃcult to learn the reward-maximizing strat-                  any time distinct states are poorly diﬀerentiated, perfor-
egy. Consistent with this, Gureckis & Love (2009a,b)                   mance in situations such as the Farming on Mars task
found that providing participants with simple percep-                  should suﬀer. Alternatively, it is possible that learners
tual cues that readily aligned with the state structure                may still be able to acquire eﬀective decision strategies
of the task improved their ability to learn the reward                 when the representation of the task suggested by per-
maximizing strategy. In their experiment, participants’                ceptual cues and the true structure of the task misalign,
display screen was augmented with a horizontal row of                  given that this misalignment takes a particular form. In
ten indicator lights which served as a cue indicative of               other words, learners may not need to have a completely
the current state of the system. Participants who were                 accurate representation of the task environment in order
given cues that correlated with the underlying task state              to still acquire a near-optimal reward-maximizing strat-
performed better than participants attempting to learn                 egy. Indeed, this later hypothesis is what is predicted by
without these cues. Further, results revealed that cues                Gureckis & Love’s RL model which can still find opti-
which supported generalization from one situation to the               mal policies in some cases given misleading or inaccurate
next had a more beneficial eﬀect on performance rela-                  cues about the structure of the task. In the following ex-
tive to cues that eﬀectively limited such generalization               periments, we explore how various types of misalignment
(see also Otto, et al., 2009). Gureckis & Love suggested               between perceptual information and task state informa-
that associating separate perceptual cues with each task               tion influences human learning. In particular, we are
“state” could reduce perceptual aliasing and facilitate                interested in how misalignments between perception of
more eﬀective learning in the same way that appropri-                  the world and the actual structure of contingencies influ-
ate state representations help artificial learning agents              ence learning and exploration behavior. Understanding
based on Q-learning (Sutton & Barto, 1998; Watkins,                    the nature of this process is important since it is unlikely
1989).                                                                 that human learners have completely accurate informa-
                                                                 163

tion about the state structure of the environment at all
times.
                     Experiment 1
In Experiment 1, each subject was randomly assigned
to one of four conditions in the Farming on Mars task.
Participants in each condition were given diﬀerent types
of perceptual cues which suggested a diﬀerent interpre-
tation of the nature of the task. Besides the type of cues
displayed, each condition was identical with respect to
the payoﬀ function and task dynamics. The overall ma-
nipulation (providing diﬀerent types of perceptual cues
to learners in the task) parallels the approach in Gureckis
                                                                  Figure 3: Example of the task interface used in the exper-
& Love (2009).                                                    iment. The display shows the indicator lights used in the
   In one condition (the no-cue condition), participants          five-cue condition. Additionally, the screen illustrates how
were given no additional cues as part of the display, and         rewards were conveyed to participants.
thus had to rely on memory and non-perceptual cues in
order to uncover the optimal task strategy (c.f., Bogacz,
McClure, Li, Cohen, & Montague, 2007). In the second              total of 12 participants were dropped from the analysis
condition (the two-cue condition), the interface screen           for responding with the same button on more than 95%
was augmented with a simple cue consisting of two lights.         of the trials. The remaining participants were randomly
At any point in time, only one of these lights was active,        assigned to one of four conditions: the no-cue condition
and a shift between the two cues indicated a change in            (N = 44), the two-cue condition (N = 45), the five-cue
the underlying task system. The position of the acti-             condition (N = 45), and the ten-cue condition (N = 46).
vated light was determined by the number of times the             Materials and design The experiment was admin-
Long-term robot was selected over the previous nine tri-          istered on standard Macintosh computers using an in-
als of the experiment (this condition reflects a many-to-         house data collection system written in Python1 . Par-
one situation with 5 states mapping to each percept). In          ticipants were tested individually over a single one-hour
the third condition (the five-cue condition), a circle of         session. Extraneous display variables, such as which
five lights (see Figure 3) was presented on the interface.        robot corresponded to the left or right choice option,
The indicator lights were organized in a consistent array         the position of the lights, and which direction the ac-
along the circle, such that the active light moved one            tive light moved (clockwise or counter-clockwise), were
position either clockwise or counterclockwise as the task         counterbalanced across participants. On each trial, the
state was updated. The five lights were mapped onto the           payoﬀ for selecting the Long-term robot was 40+70∗h/9,
underlying task system using a “modulus” rule, result-            where h is the number of times the Long-term robot was
ing in two distinct task states mapping to each percept.          selected in the last 9 trials. In contrast, the payoﬀ on
In the final condition, a display of ten lights was em-           each trial for the Short-term robot was 30 + 70 ∗ h/9.
ployed, such that each light corresponded exactly to a            The final values were scaled by 110 and displayed as a
distinct numerical state in the underlying task system            percentage on the sliding oxygen meter.
(one-to-one mapping).                                             Procedure Participants were tested in the basic
   Consistent with Gureckis & Love (2009a), we pre-               Farming on Mars task as described above. At the be-
dicted that providing participants with light cue arrays          ginning of the experiment, subjects were presented in-
which readily align with the underlying state of the sys-         structions on the screen which conveyed the basic cover
tem will limit the aliasing of functionally distinct states,      story for the task. The instructions were identical for
and improve subjects’ ability to learn the reward maxi-           all conditions, and there was no explicit reference to the
mizing strategy. Thus, we predict that conditions where           function or purpose of the indicator lights/cues. On each
perceptual cues limit this aliasing (i.e., the ten-state con-     trial, participants were shown a display with two large
dition) will result in better overall performance. In addi-       response buttons. Between these buttons was a video
tion, we expect that participants’ induced representation         display which presented trial-relevant feedback. After a
of the task will strongly influence the strategies they use       robot selection was made, the quantity of oxygen pro-
to balance exploration and exploitation in the task.              duced for that trial was presented on the video display.
Methods                                                           The amount of oxygen points earned was presented vi-
                                                                  sually with a vertical, sliding bar which filled green to
Participants One hundred and ninety-two New York
University undergraduates participated for course credit
and a small cash bonus based on task performance. A                   1
                                                                        http://www.pypsyexp.org
                                                              164

varying levels. The oxygen level display was shown for             example, participants in the diﬀerent conditions might
800 ms, after which the the screen was reset to indicate           adopt alternative strategies for exploring the task. One
the start of a new trial. No information regarding cumu-           way to quantify these diﬀerences is to plot the percent-
lative oxygen generation was presented, but instructions           age of total trials participants spent in each true (la-
did emphasize that participants should try to “maximize            tent) state in the task. Remember that “states” in this
the number of oxygen points generated over the entire              dynamic task are defined by the percent allocation of
experiment.” In the two-light, five-light, and ten-light           choices to the Long-term option over the last nine tri-
conditions (but not in the no-cue condition), the screen           als. Figure 1 plots this distribution for each of the four
was augmented with an array of indicator lights as de-             conditions. Interestingly, the structure of the cues in
scribed above and shown in Figure 3. The experiment                the task has a strong impact on the way participants
consisted of 500 separate trials divided into five blocks          explored the task dynamic. In particular, participants
of 100 trials. In order to maintain motivation, partici-           in the two-cue condition spent a much larger percent-
pants were informed that they would receive a small cash           age of time in intermediate states (indicated roughly
bonus of $2-5 dollars based on total oxygen generated by           equal allocation to both choices for extended periods
the end of the task.                                               of time). For example, a one-way ANOVA on propor-
                                                                   tion of time spent in states 3-7 revealed an eﬀect of
Results                                                            condition, F (3, 132) = 4.57, p < .005. Specifically,
The primary dependent measure in our experiment                    participants in the two-cue condition spent more total
was the proportion of Long-term robot selections (i.e.,            time in these intermediate states than in the no-cue,
reward-maximizing responses) made by the participant.              t(64) = 2.95, p < .005, five-cue, t(66) = 2.31, p < .02,
Total mean proportions by condition are presented in               and ten-cue, t(66) = 3.43, p = .001, conditions (since
Figure 4. Overall, the proportion of Long-term choices             these are post-hoc analyses significance should be inter-
were significantly higher than chance in all conditions,           preted using a conservative α = .05/3 = .016). On the
except for the five-cue condition (all p < .05). Given the         other hand, there was also a significant eﬀect of condition
binary outcome choice data, we conducted a series of               on how long participants spend in the end point states
binomial regressions using the χ2 distributed deviance-            (i.e., state 1 & 2 and 9 & 10), F (3, 132) = 3.25, p < .025.
based test as our measure of model selection2 . There was          Post-hoc test revealed this was driven primarily by the
an overall significant eﬀect of condition χ2 (3) = 15.6,           lower percentage of total time spent in these states in
p = .001. In addition, the pattern of results across con-          the two-cue condition condition compared to the 10-cue
ditions was best predicted as a quadratic function of the          condition, t(66) = 3.17, p < .003.
number of perceptually distinct task states compared
to a linear relationship (χ2 (1) = 11.32, p < .001, the            Discussion
quadratic term was reliably above zero, βcond2 = .02,              The results of Experiment 1 show that participant’s con-
p < .001). Pairwise contrasts (using an Bonferroni-                ceptualization of the state structure of the task can in-
adjusted α = .05/4 = .012) between the individual con-             fluence both their exploration strategies as well as their
ditions revealed a significantly higher proportion of max-         ability to identify a reward maximizing strategy. In par-
imizing responses in the ten-cue condition compared to             ticular, when cues about the underlying state of the
both the five-cue condition, χ2 (1) = 13.46, p < .001, and         states were more highly aliased (the two-cue and five-
the two-cue condition, χ2 (1) = 11.62, p < .001. Surpris-          cue conditions) participant’s overall task performance
ingly, there was a relatively small diﬀerence between the          suﬀered. Closer examination of the way in which par-
ten-cue and no-cue conditions which did not reach sig-             ticipants explored the task revealed that the alignment
nificance, χ2 (1) = 3.59, p = .06. Note, however, that in          of the cues in the task had a dramatic eﬀect on behav-
a similar task, Gureckis & Love (2009b) and Otto, et al.           ior, even when overall performance diﬀerences appeared
(2009) found an advantage for one-to-one percept-state             smaller. In particular, relative to the other conditions,
representations. Also, note that when given only two               participants in the two-cue condition spent a consider-
state cues, performance was not significantly better than          ably longer time in intermediate states, consistent with a
when participants are given five state cues, χ2 (1) = 1.04,        choice strategy involving alternations between the short-
p = .3.                                                            term and long-term options.
   In order to better understand the genesis of the alias-
ing eﬀect, we examined the dynamics of exploration in                                   Experiment 2
the task. In particular, even if the marginal propor-              In Experiment 1 we found that reward-maximizing per-
tion of maximizing choices is constant, it is possible that        formance was worst when a circle of five indicator lights
the distribution of those choices in time could vary. For          was presented on the interface, such that two diﬀerent
    2                                                              task states mapped to the same perceptual display. How-
      We also analyzed these data through a one-way ANOVA
and a series of t-tests which revealed an identical pattern of     ever, it is as yet unclear if the performance diﬀerence
significant results.                                               for highly aliased environments results from the num-
                                                               165

                                     1.0                                                             0.25
       Percent Maximizing Response
                                                                                                                                   no-cue
                                                                                                                                   2-cue
                                                                           Percent of Total Trials
                                     0.8                                                             0.20
                                                                                                                                   5-cue
                                                                                                                                   10-cue
                                     0.6                                                             0.15
             (i.e., Long-Term)
                                                                            Spent in Each State
                                     0.4                                                             0.10
                                     0.2                                                             0.05
                                     0.0
                                           no-cue 2-cue   5-cue   10-cue                                     1      2     3    4      5      6   7   8    9    10
                                                                                                                                     State
Figure 4: Panel A: Average proportion of Long-Term (maximizing) responses made throughout the experiment as
a function of condition. The horizontal line at 0.5 shows chance performance. Error bars are standard errors of the
mean. Panel B : Average percentage of total experiment spent in each state. State 1 corresponds to 0 of the last
nine choices being to the Long-term option. State 10 corresponds to 9 of the last 9 choices being to the Long-term
option.
ber of implied states (5) or how those states “blend to-                                                          the perception of a correlation between the movement of
gether” by the dynamics of the focal cue (i.e., the active                                                        the light and the magnitude of the reward is supported
light). For example, in the five-cue condition of Experi-                                                         by previous studies showing that participants use such
ment 1, the active cue moved one position either to the                                                           information even when it is against their best interest in
left or right as the state of the underlying system was                                                           the task (Otto et al., 2009).
updated. Thus, a participant who steadily progressed
from states 1-10 would experience the active light loop-                                                          Methods
ing twice around the circle of indicator lights. An al-                                                           Participants Forty New York University undergradu-
ternative display which maintains the same level of per-                                                          ates participated for course credit and a small cash bonus
ceptual aliasing (two true states for every one distinct                                                          based on task performance. Participants were randomly
percept) would be to have the active light remain in the                                                          assigned to either the twice-looped condition (N=21) or
same position across two consecutive state updates. In                                                            the single-looped condition (N = 19).
this design, a participant who steadily progressed from
                                                                                                                  Materials and design All aspect of the materials and
states 1-10 would observe the active light making a sin-
                                                                                                                  design were identical to Experiment 1, except for the
gle loop around the five indicator lights, ’doubling-up’ at
                                                                                                                  changes to the five-cue display described above.
each individual light position. In other words, if the let-
ter A-E represent the five locations for the state cue, then                                                      Procedure The general procedure was the same as in
the mapping from the 10 latent task states to the display                                                         Experiment 1.
would be 1,2,3,4,5,6,7,8,9,10→A,A,B,B,C,C,D,D,E,E. In
Experiment 2, we compare task performance in this                                                                 Results
single-looped condition with performance in the twice-                                                            As before, the primary dependent measure in our ex-
looped condition (which is identical to the ‘five-cue’ con-                                                       periment was the proportion of Long-term robot selec-
dition of Experiment 1).                                                                                          tions (i.e., reward-maximizing responses) made by the
   Our prediction was that performance in the twice-                                                              participant. However, there was no overall eﬀect of con-
looped condition would be lower than in the single-                                                               dition χ2 (1) = 0.26, p = .61, M =0.52 in the twice-
looped condition. The rationale was that participants                                                             looped condition and M =0.54 in the single-looped con-
in the single-looped condition would be better able to                                                            dition. Closer examination of the distribution of overall
recognize that the “gradient” of reward was rising as the                                                         performance scores indicated that the distribution was
light moved in a particular direction. In contrast, the                                                           strongly bimodal in the twice-looped condition, while it
twice-looped condition would be more likely to be con-                                                            was uni-model in the single-looped condition. As shown
fused as a state that they had previously experienced to                                                          in Figure 5, this bi-modality arose from the way that
have low reward (e.g., state cue position A) might later                                                          participants explored the latent task states. In partic-
also be associated with high reward. The prediction that                                                          ular, a 2-way repeated measures ANOVA on condition
                                                                                                            166

and time spent in each state found a significant eﬀect of                                      0.25
state, F (9, 342) = 4.12, p < .001, and a significant state                                                           one-looped
                                                                     Percent of Total Trials
by condition interaction, F (9, 342) = 3.17, p < .001. At                                      0.20
                                                                                                                      twice-looped
least a subset of participants in the twice-looped con-
dition appeared to have spent a disproportion amount                                           0.15
of time in state 6 which is the point where the display
                                                                      Spent in Each State
looped back on itself suggesting that they were attempt-
                                                                                               0.10
ing to keep the state cue from crossing back around to
the state associated with the lowest reward. In contrast,
participants in the single-looped condition spent more
                                                                                               0.05
time in the lower states (1-4) indicating that they had
an overall bias towards the short-term option that a sub-                                             1   2   3   4       5      6   7   8   9   10
set of participants eventually overcame.                                                                                 State
               General Discussion                                   Figure 5: Average percentage of total experiment spent
                                                                    in each state in Experiment 2.
Across a set of two experiments we explored how per-
ceptual cues concerning the underlying state structure
of a dynamic decision making task influenced learning.                    bias in human reinforcement learning. Brain Re-
Consistent with previous work (Gureckis & Love, 2009b,                    search, 1153 , 111-121.
2009a), we find that when task states are aliased, par-             Dayan, P., & Daw, N. (2008). Decision theory, reinforce-
ticipants’ ability to identify an optimal task strategy is                ment learning, and the brain. Cognitive, Aﬀective,
impaired. It is important to point out that the eﬀects we                 and Behavioral Neuroscience, 8 , 429-453.
see here are unlikely to be a simple consequence of partic-         Gureckis, T., & Love, B. C. (2009a). Learning in noise:
ipants ignoring the primary task (to earn oxygen points)                  Dynamic decision-making in a variable environ-
and instead exploring aspects of the display. First, par-                 ment. Journal of Mathematical Psychology, 53 ,
ticipants were clearly instructed that the primary goal                   180-193.
was to control the system to earn as many points as pos-            Gureckis, T., & Love, B. C. (2009b). Short term gains,
sible. In addition, participants were paid a small cash                   long term pains: How cues about state aid learning
bonus tied to their performance in the task which in-                     in dynamic environments. Cognition, 113 (3), 293-
creased the relevance of the primary task. Finally, our                   313.
analysis of the dynamics of exploration (i.e., the percent          McCallum, R. (1993). Overcoming incomplete percep-
of time spent in each state) reveal systematic diﬀerences                 tion with utile distinction memory. In The pro-
related to the structure of the cues we provided.                         ceedings of the tenth international machine learn-
   One possibility is that the structure of the perceptual                ing conference (ml’93). Amherst, MA.
cues provide a kind of strategy “aﬀordance” in the task,            Otto, A., Gureckis, T., Love, B., & Markman, A. (2009).
limiting the space of exploration/response policies that                  Navigating through abstract decision spaces: Eval-
participants considered. Note that in a separate study,                   uating the role of state knowledge in a dynamic
we recently found that motivational manipulations can                     decision making task. Psychonomic Bulletin and
also impact participant’s exploration behavior in a simi-                 Review , 16 (5), 957-963.
lar task (Otto, Markman, Gureckis, & Love, in review).              Otto, A., Markman, A., Gureckis, T., & Love, B. (in re-
A theoretical analysis of these results and evaluation of                 view). Regulatory fit in a dyanmic decision-making
their implication for the Gureckis & Love (2009) model                    environment. Journal of Experimental Psychology:
are currently underway. However, preliminary simula-                      Learning, Memory, and Cognition.
tions show a close correspondence between the results               Sutton, R., & Barto, A. (1998). Reinforcement learning:
reported here and the behavior of the model. Future                       An introduction. Cambridge, MA: MIT Press.
work will continue to evaluate how RL models can be                 Watkins, C. (1989). Learning from delayed rewards. Un-
used to understand the motivational and cognitive influ-                  published doctoral dissertation, Cambridge Uni-
ences underlying dynamic decision-making.                                 versity, Cambridge, England.
Acknowledgements We thank Louis Tur and Nathaniel                   Whitehead, S., & Ballard, D. (1991). Learning to per-
Blanco for programming assistance and discussion in the de-               ceive and act by trial and error. Machine Learning,
velopment of this project.
                                                                          7 (1), 45-83.
                     References
Bogacz, R., McClure, S., Li, J., Cohen, J., & Montague,
     P. (2007). Short-term memory traces for action
                                                              167

