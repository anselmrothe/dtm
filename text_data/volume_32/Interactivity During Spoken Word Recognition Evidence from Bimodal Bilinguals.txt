UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Interactivity During Spoken Word Recognition: Evidence from Bimodal Bilinguals

Permalink
https://escholarship.org/uc/item/5kw0n2g7

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Shook, Anthony
Marian, Viorica

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Interactivity During Spoken Word Recognition: Evidence from Bimodal Bilinguals
Anthony Shook (a-shook@northwestern.edu)
Department of Communication Sciences and Disorders, 2240 Campus Drive
Evanston, IL 60208 USA

Viorica Marian (v-marian@northwestern.edu)
Department of Communication Sciences and Disorders, 2240 Campus Drive
Evanston, IL 60208 USA

simultaneously activated. This effect appears to be bottomup in nature – as auditory input enters the language system,
it non-selectively activates lexical items in both languages
based on structural overlap. Critically, a dual-language
bottom-up pathway cannot exist in bimodal bilinguals, as
their languages do not utilize the same structural input. The
cross-modal nature of bimodal bilingualism therefore allows
for the direct investigation of top-down mechanisms in
isolation.
If bimodal bilinguals co-activate their two languages in
the absence of form overlap, it would suggest that language
co-activation can be driven by top-down information, and
would require a system capable of activating the non-target
language via top-down or lateral connections. Models that
consider exclusively bottom-up information for lexical
activation or selection (such as the Shortlist Model, Norris,
1994) are unlikely to be able to explain this result, as they
limit activation to items that exist in the same modality as
the target. Therefore, a bimodal bilingual, when faced with
single-modality input (e.g., spoken English), should not
activate the signed language.
However, recent research indicates that bimodal
bilinguals do co-activate their languages. For example,
hearing ASL-English bilinguals produce speech and signs
simultaneously (Emmorey, Borinstein, Thompson, &
Gollan, 2008), deaf ASL-English bilinguals show
interference from sign-language while processing writtenEnglish (Villwock, Wilkinson, Bailey, Kroll, Morford, &
Piñar, 2009), and late-learning Dutch-Sign Language of the
Netherlands bilinguals show interference from English
while processing SLN signs (Van Hell, Ormel, van der
Loop, & Hermans, 2009). In addition to clarifying the role
of top-down mechanisms in language processing, language
co-activation in bimodal bilinguals would suggest that
linguistic information is readily transmitted across
modalities, such that two unrelated languages can be
activated simultaneously, even when phonological
information from one of the two languages is absent.
The current study used an adapted visual world paradigm
to examine parallel language processing in normal-hearing,
ASL-English bilinguals. Investigating whether languages
that do not share modality are co-activated in bimodal
bilinguals can provide insight into the influence of top-down
mechanisms on language processing and the architecture of
the language system in general, as well as reveal the extent
to which linguistic information is modality independent.

Abstract
We explore the role of top-down information in language
processing by investigating parallel language activation in
bimodal bilinguals, who are fluent users of a spoken and a
signed language. In an eye-tracking study, bimodal bilinguals
showed activation of their signed language while receiving
input in English only. Since spoken and signed languages do
not share structure, the results suggest that linguistic
information can be readily transmitted across modalities, and
that parallel language activation can be driven by top-down
processes.
Keywords: bilingualism; ASL; language co-activation; topdown processing; eye-tracking; visual world paradigm

Introduction
The architecture of the language system is determined by
the way that information flows among levels of processing.
Language processing may involve both bottom-up/feedforward and top-down/feed-back mechanisms (Rapp &
Goldrick, 2000; Navarette & Costa; 2009). However,
exclusively feed-forward systems may also be capable of
explaining language processing without the aid of feed-back
mechanisms (Hagoort & Levelt, 2009; Levelt, Roelofs, &
Meyers, 1999; McQueen, Jesse, & Norris, 2009; Norris,
1994). Proponents of language systems that recruit topdown mechanisms face the difficulty of separating the
impact of top-down information from that of bottom-up
information. When both forms of information are present, it
is difficult to disentangle the unique contributions each may
make to language processing. To understand the role of topdown mechanisms during language processing, the
influence of top-down pathways must be measured in
isolation. One possible way of limiting the impact of
bottom-up information is by investigating language
processing in bimodal bilinguals.
Unlike unimodal bilinguals, who use two spoken
languages, bimodal bilinguals are fluent in a spoken and a
signed language. Research on unimodal bilinguals has
revealed non-selective language effects, wherein unimodal
bilinguals activate both of their languages in parallel
(Blumenfeld & Marian, 2007; Marian & Spivey, 2003;
Weber & Cutler, 2004). For example, when a RussianEnglish bilingual hears the English word “marker,” she will
also make eye movements to items that are phonologically
similar in the non-target language (e.g., Russian) such as
“marka” (stamp), suggesting that her Russian is

790

Method
Participants
Twenty-six participants were tested (thirteen ASL-English
bilinguals, Mage=33.2, SD=11.8 and thirteen English
monolinguals, Mage=23.9, SD=9.8). An additional five
participants were not included in the analysis – three due to
failure to display sufficient proficiency in ASL, and two due
to technical error with the eye-tracker. All participants
completed the Peabody Picture Vocabulary Test (PPVT-III;
Dunn & Dunn, 1997) to assess their English vocabulary
skill. No differences were found between bilinguals
(M=108.2, SD=9.9) and monolinguals (M=111.8, SD=9.4;
t(24)=0.96, p=.35).
Information on the participants’
language background was obtained via the Language
Experience and Proficiency Questionnaire (LEAP-Q;
Marian, Blumenfeld, & Kaushanskaya, 2007). On a scale of
1-10, where 10 means “fluent,” bilinguals rated their ASL
abilities at 8.5 for production, and 8.8 for comprehension,
indicating a high degree of ASL proficiency.
All
participants reported normal hearing and vision.
Figure 1: Example of a Competitor Trial.
Participants eye-movements were recorded while
they were instructed in English to “click on the
cheese.” At the same time, a phonologically related
competitor in ASL (“paper”) was present in the
display.

Materials
Twenty-two minimal sign pairs were developed by choosing
two signs that matched on three of four ASL-phonological
parameters – handshape, location in space, motion, and
orientation of the palm (Brentari, 1998). These sign pairs
represented the target and competitor items in our
competitor condition. For example, the signs for “cheese”
and “paper” overlapped in handshape, location, and palm
orientation, but differed in the motion of the sign. Target
and competitor signs did not differ significantly in English
word frequency [t(38)=-1.654, p=.106] (obtained from the
SubtLexus database; Brysbaert & New, 2009). In addition,
twenty-two control items and 110 filler items were chosen
based on their lack of phonological overlap to the target in
both ASL and English. Control items were used in place of
competitor items in the control condition. Control signs
also did not differ from target signs in English word
frequency [t(38)=-1.027, p=.311]. In the experiment, each
item was represented by a black and white line drawing. In
each condition, four black and white drawings were
displayed on a computer screen in the corners of a 3x3 grid.
The words were recorded at 44.1 Khz, 32 bits by a female,
monolingual speaker of English, in sentence context as the
final word in the phrase “click on the _____.” Recordings
were normalized such that the carrier phrase was of equal
length for all target sentences, and the onset of the target
word always occurred at 600 ms post onset of the sentence.
Recordings were amplitude-normalized.

for a look, 0 for no look) and duration of looks (percent of
time per trial spent looking at an item). There were twentytwo competitor trials, containing a target, a competitor item
that overlapped with the target in ASL phonology, and two
fillers (Fig. 1). Every competitor trial had a corresponding
control trial, in which the content and location of the target
item and two filler items were identical, but where the
phonologically-overlapping competitor item found in the
competitor trial was replaced with an unrelated control item.
This allowed for the comparison of looks to a specific
location in the display as a function of the presence or
absence of a phonological competitor. There were also
forty-four filler trials, containing a target and three
phonologically unrelated items.

Procedure
After informed consent was obtained, participants viewed a
video clip displaying the experimental instructions in ASL
performed by a native signer of ASL. Following the
instructions, participants were fitted with an ISCAN eyetracker to measure the location of their gaze during the eyetracking portion of the experiment. Instructions were again
provided, in both written and spoken English, followed by
five practice trials meant to familiarize participants with the
task. Auditory stimuli were presented over headphones and
appeared synchronously with picture stimuli. Participants
were told that they would hear instructions to choose a
specific object in the visual display, and should click on the
object that best represents the target word. Participants’ eye-

Design
The current study used a 2x2 Mixed design, with group
(bilingual, monolingual) as a between-subjects factor, and
condition (competitor, control) as a within-subjects factor.
The dependent variables include the proportion of looks (1

791

movements were recorded. After the eye-tracking portion of
the experiment, all participants completed the PPVT and the
LEAP-Q. In addition, bilingual participants were presented
with a list of words and asked to provide the American Sign
Language translations.
Bilinguals provided correct
translations for 95.2% of the words (M=62.8/66, SD=2.5).

measures ANOVAs were performed on each individual
window, with time (1, 2, 3) and condition (competitor,
control) as within-subjects factors. Significant effects of
condition were found in each of the four 100 ms time
windows between 0 ms (word onset) and 400 ms, between
1000 and 1100 ms, and between 1300 and 1400 ms (all
ps<0.05); in all cases, bilinguals showed more looks to
competitor items than control items. Similar analyses
performed on the monolingual activation curves revealed no

Results
Frequency of Looks
We measured both the proportion and duration of looks to
competitor and control items. Bilinguals looked more at
competitor items than at control items, and looked more at
competitor items than monolingual participants. Repeated
measures Analyses of Variance (ANOVAs) revealed a
significant Group x Condition interaction for both the
proportion [F(1,24)=27.284, p<0.001; Fig. 2] and duration
[F(1,24)= 23.285, p<0.001; Fig. 3] of looks. Bilinguals
looked at competitor items more than at control items
[t(12)=7.62, p<0.001] and for a longer period of time
[t(12)=5.925, p<0.001], signifying that bilinguals activated
phonologically
related
competitors
more
than
phonologically unrelated controls. No differences were
found in the monolingual group for either the proportion
[t(12)=-0.95, p=0.362] or duration [t(12)=-0.16, p=0.87] of
looks. Bilinguals also looked at competitor items more than
monolinguals [t(24)=5.58, p<0.001] and for a longer period
of time [t(24)=3.512, p<.01], while both groups looked at
control items equally [proportion=t(24)=1.18, p=0.248;
duration=t(24)=-.73, p=0.47], verifying that bilinguals
activated phonologically related items more than
monolinguals. Means and standard errors are illustrated in
Table 1.

1

12.9
(0.8)

7.7
(0.6)

Monolingual

42.9
(3.2)

45.5
(3.8)

8.5
(0.9)

8.6
(1.0)

Proportion of Looks (%)

*

0.6
0.5
0.4
0.3
0.2

Bilingual

Monolingual

Figure 2. Proportion of Looks (%)
0.2

Percent Time Spent Looking (%)

Bilingual

51.6
(3.5)

0.7

Control

0

Duration (%)
Comp. Control

66.9
(2.8)

*

0.8

0.1

Table 1: Means and Standard Errors of the Proportion and
Duration of Looks (%).
Proportion (%)
Comp. Control

Competitor

0.9

Time Course
Analysis of the bilingual time-course was consistent with
the overall looks analysis, with bilinguals looking at
competitors more than at control items. In contrast,
monolinguals looked at competitor and control items
equally across time. The activation curves were divided
into 100 ms windows, beginning with the time window
between -600 and -500 ms (which signified the first 100 ms
after the onset of the picture), and ending with the window
between 1900 and 2000 ms. Three-by-two repeated-

Competitor

*
0.15

Control

*

0.1

0.05

0
Bilingual

Monolingual

Figure 3. Duration of Looks (%)

792

Proportion of Looks

Monolinguals
Target
Competitor
Control
Target Word Onset

2000
1900
1800
1700
1600
1500
1400
1300
1200
1100
1000
900
800
700
600
500
400
300
200
100
0
-100
-200
-300
-400
-500
-600

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Proportion of Looks

Bilinguals

*

*

*

Target
Competitor
Control

Target Word Onset

2000
1900
1800
1700
1600
1500
1400
1300
1200
1100
1000
900
800
700
600
500
400
300
200
100
0
-100
-200
-300
-400
-500
-600

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Time (ms)
Figure 4. Time-course data for Monolingual (top) and Bilingual (bottom) participants, showing activation
curves for proportion of looks to target, competitor, and control items across time. The negative 600 time
point represents onset of the picture stimulus, and the 0 time point represents onset of the target word.
Shaded areas indicate windows where looks to competitor and control items differed at p<0.05
significant effects of Condition for the monolingual group in
any time window (all ps > 0.1), suggesting that
monolinguals did not look more at competitor items than
controls (see Figure 4).
It is possible that the effect seen in the late windows
(1000-1100 ms and 1300-1400 ms) is a product of residual
activation from the early window. To ensure that the latewindow effects were not due to lingering activation from the
early window, the proportion of late-window looks to
competitors with a look to targets or competitors in the
early-window was compared to late-window looks without

earlier target or competitor fixations. If there was a higher
proportion of late-window looks when the early window
contained a look to either the target or competitor than when
it did not, it would suggest that late-window activation was
due to residual activation from the early window. However,
both instances showed the same proportion of looks,
t(12)=1.04, p=.377, suggesting that the results seen in the
late-windows are not due to previous activation in the earlywindow.

793

is still a product of top-down processes – the linguistic input
should activate English only. However, bimodal bilinguals
clearly activate their ASL during the task.
Coactivation may also occur via lateral links between
translation equivalents. As an English word is presented, it
may activate its ASL translation via direct excitatory
connections at the lexical level, which may in turn activate
phonologically similar ASL items. While this account does
not involve top-down processes, it is also not exclusively
bottom-up, and requires a system capable of interaction
across languages, within a single level of processing.
However, the strength of within-level translational
connections in bimodal bilinguals is unclear. For instance,
bimodal bilinguals do not show enhanced performance on
executive control tasks, which has been found in unimodal
bilinguals. Emmorey, Luk, Pyers, and Bialystok (2008)
suggest that since a bimodal bilingual’s two languages
utilize separate modalities, they do not compete to the same
extent as two spoken languages. Therefore there is less
need for executive control of the non-target language in
bimodal bilinguals. One could argue that the lack of
competition between a spoken and a signed language may
indicate that bimodal bilinguals do not develop crosslinguistic connections in the same manner as unimodal
bilinguals. It is not yet clear whether the connections
between translation equivalents, or the way in which they
are processed, are similar in unimodal and bimodal
bilinguals.
Regardless of whether parallel activation in bimodal
bilinguals is due to top-down effects or lateral connections
at the lexical level, it is clear that the processes that underlie
language coactivation in bimodal bilinguals differ from
those of unimodal bilinguals.
While coactivation in
unimodal bilinguals relies more on phonological overlap
across two languages, no such overlap exists within the
processing architecture of bimodal bilinguals. Therefore,
the finding that bimodal bilinguals coactivate their
languages implies a system where top-down or lateral
processes are capable of governing cross- linguistic
activation.
Our results also indicate that language
information may be readily transmitted across modalities,
such that two highly unrelated languages can be activated
simultaneously. Thus, the language system should be
considered modality-independent and able to process
linguistic information equally, regardless of whether it is
auditory or sensorimotor in nature.
Moreover, there is reason to believe that a system of this
nature is not unique to bimodal bilinguals, and may provide
a window into more general language processing.
Unimodal bilinguals and monolinguals have shown robust
cross-modal effects as well (for a review, see Marian, 2009),
and semantic-competition effects from eye-tracking provide
evidence for rapid and highly robust lexical-semantic
interaction (Huettig & Altmann, 2005; Yee & Sedivy,
2006). In addition, when bimodal bilinguals produce codeblends, they do so in the same fashion as unimodal
bilinguals would code-switch, with the added benefit of

Discussion
The results of the current experiment provide evidence for a
modality-independent language system that utilizes topdown pathways during processing by revealing parallel
language activation in bimodal bilinguals. Specifically,
bilinguals looked more to items with ASL translation
equivalents that overlapped phonologically with the target
item than to items with translation equivalents that did not
overlap, suggesting that phonologically overlapping
competitor items were more activated than unrelated
controls. In turn, monolinguals looked at competitor items
and unrelated control items equally. This pattern was found
in the overall looks analysis and in the duration of looks
analyses, as well as within specific time windows during
processing. The results suggest that even though the
bilingual participants received no ASL input, they
nevertheless activated their sign-language during the
experiment.
The finding that bimodal bilinguals coactivate their
languages indicates that lexical items from two distinct
languages do not require surface-level overlap in order to be
simultaneously activated. Previous studies on unimodal
bilinguals have relied on bottom-up information as the force
behind parallel language activation – words activate
phonologically similar words, regardless of language. If
parallel activation is driven purely by overlap at the
phonological level, then the bimodal participants should not
have shown cross-linguistic activation. Instead, the
connection between ASL and English likely exists at the
semantic level, since the two languages do not share
phonological or lexical items. Semantic representations,
once activated, appear to be able to feed back to the lexical
levels of both signed and spoken languages, resulting in
parallel activation.
However, the mechanisms that underlie parallel
processing in bimodal bilinguals are unclear. Examination
of the time-course showed that at the moment of the onset of
the target word, competitor items were activated more than
control items in the bilingual group. If the target word has
yet to be presented in full, how is it possible that bilinguals
would show increased activation of competitor items? Prior
to onset of the word, bilinguals view the display containing
all four images for 600 ms. Bilinguals may automatically
activate the corresponding semantic concepts due to visual
input. This activation can feed back ASL lexical levels and
activate phonologically similar lexical items.
The
phonologically related items may then continually activate
one another until target selection occurs.
The process of top-down activation of the non-target
language in bimodal bilinguals can also be initiated by
linguistic input. The initial semantic representation could be
activated by the incoming English target word, rather than
by visual stimuli. When the semantic representation is
activated, it feeds back to the lexical representations in both
English and ASL, thereby activating phonologically similar
ASL signs and their corresponding semantic representations.
It is important to note that in this account, parallel activation

794

being able to produce speech and signs in tandem,
suggesting similarities in the underlying mechanisms of
production for unimodal and bimodal bilinguals (Emmorey
et al., 2008). It is possible that the top-down pathways
utilized by bimodal bilinguals are present in unimodal
bilinguals and monolinguals as well, but are overshadowed
by more immediate bottom-up effects.
The results of the current study indicate that bimodal
bilinguals activate both of their languages simultaneously
via a cross-linguistic lexical-semantic loop where top-down
information from the conceptual level feeds back to lower
levels of processing in both languages, regardless of
modality. The results have further implications for the
architecture and processing dynamics of the language
system, bilingual and monolingual alike, suggesting that
language information can be freely accessed across
modalities, and that top-down mechanisms can have a
strong influence on language processing.

Huettig, F., & Altmann, G. T. M. (2005). Word meaning
and the control of eye fixation: semantic competitor
effects and the visual world paradigm. Cognition, 96,
B23–B32.
Levelt, W. J. M., Roelofs, A., & Meyer, A. S. (1999). A
theory of lexical access in speech production. Behavioral
and Brain Sciences, 22, 1-75.
Marian, V., Blumenfeld, H., & Kaushanskaya, M. (2007).
The language experience and proficiency questionnaire
(LEAP-Q): Assessing language profiles in bilinguals and
multilinguals. Journal of Speech, Language, and Hearing
Research, 50(4), 940-967.
Marian, V. & Spivey, M. (2003b). Competing activation in
bilingual language processing: Within- and betweenlanguage competition.
Bilingualism: Language and
Cognition, 6(2), 97-115.
McQueen, J. M., Jesse, A., & Norris, D. (2009). No lexicalprelexical feedback during speech perception or: Is it time
to stop playing those Christmas tapes? Journal of
Memory and Language, 61(1), 1-18.
Navarette, E., & Costa, A. (2009). The naming of gendermarked pronouns supports interactivity in models of
lexical access. Psicológica, 30, 301-321.
Norris, D. (1994). Shortlist: A connectionist model of
continuous speech recognition. Cognition, 52(3), 189234.
Rapp, B., & Goldrick, M. (2000). Discreteness and
interactivity in spoken word production. Psychological
Review, 107(3), 460-499.
Van Hell, J. G., Ormel, E., van der Loop, J., & Hermans, D.
(2009). Cross-language interaction in unimodal and
bimodal bilinguals. Paper presented at the 16th
Conference of the European Society for Cognitive
Psychology. Cracow, Poland, September 2-5.
Villwock, A., Wilkinson, E., Bailey, R., Kroll, J., Morford,
J., & Piñar, P. (2009). Cross-language lexical activation
in deaf bilinguals: Does English print activate ASL signs?
Presented at The International Symposium on
Bilingualism 7. Utrecht, NL.
Weber, A., & Cutler, A. (2004). Lexical competition in nonnative spoken- word recognition. Journal of Memory and
Language, 50, 1-25.
Yee, E. & Sedivy, J. (2006). Eye movements reveal
transient semantic activation during spoken word
recognition. Journal of Experimental Psychology:
Learning, Memory and Cognition, 32, 1-14.

Acknowledgments
This research was funded in part by grants NICHD
1R03HD046952 and NSF BCS-0418495 to the second
author. The authors would like to acknowledge Dr.
Margarita Kaushanskaya, Dr. Henrike Blumenfeld, Caroline
Engstler, Scott Schroeder, James Bartolotti, Michelle
Masbaum, and Rucha Mehta for their contributions to this
project.

References
Blumenfeld, H., & Marian, V. (2007). Constraints on
parallel activation in bilingual spoken language
processing: Examining proficiency and lexical status
using eye-tracking. Language and Cognitive Processes,
22(5), 633-660.
Brentari, D. (1998). A Prosodic Model of Sign Language
Phonology. MIT Press.
Brysbaert, M., & New, B. (2009). Moving beyond Kucera
and Francis: A critical evaluation of current word
frequency norms and the introduction of a new and
improved word frequency measure for American English.
Behavioral Research Methods, 41(4), 977-990.
Dunn, L. M. & Dunn, L. M. (1997). Peabody Picture
Vocabulary Test, Third Edition. Circle Pine, MN:
American Guidance Service.
Emmorey, K., Borinstein, H.B., Thompson, R., & Gollan,
T.H. (2008). Bimodal bilingualism.
Bilingualism:
Language and Cognition, 11(1), 43-61.
Emmorey, K., Luk, G., Pyers, J. E., & Bialystok, E. (2008).
The source of enhanced cognitive control in bilinguals:
Evidence from bimodal bilinguals.
Psychological
Science, 19(12), 1201-1206.
Hagoort, P., & Levelt, W. J. M. (2009). The speaking brain.
Science, 326(5951), 372-373.

795

