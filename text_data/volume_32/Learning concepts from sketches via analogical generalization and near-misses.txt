UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning concepts from sketches via analogical generalization and near-misses
Permalink
https://escholarship.org/uc/item/1vv6g525
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
McLure, Matthew D.
Friedman, Scott E.
Forbus, Kenneth D.
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

      Learning concepts from sketches via analogical generalization and near-misses
                                      Matthew D. McLure (mclure@u.northwestern.edu)
                                       Scott E. Friedman (friedman@northwestern.edu)
                                        Kenneth D. Forbus (forbus@northwestern.edu)
                      Qualitative Reasoning Group, Northwestern University, 2133 Sheridan Rd
                                                      Evanston, IL 60208 USA
                              Abstract                               concept in only one way. A near miss exemplar should be
                                                                     highly alignable with some instances of a concept 1.
   Modeling how concepts are learned from experience is an
   important challenge for cognitive science. In cognitive             This paper describes a model of concept learning that
   psychology, progressive alignment, i.e., comparing highly         combines analogical generalization and near-miss analysis
   similar examples, has been shown to lead to rapid learning.       to capture both similarity-based and analytic aspects of
   In AI, providing very similar negative examples (near-misses)     concepts. Its inputs are labeled positive or negative
   has been proposed as another way to accelerate learning. This     examples of concepts.           It uses SAGE to construct
   paper describes a model of concept learning that combines         generalizations for each concept, thus capturing similarity-
   these two ideas, using sketched input to automatically encode
   data and reduce tailorability.        SAGE, which models
                                                                     based aspects of concepts (and typicality, via probability).
   analogical generalization, is used to implement progressive       When a positive example is provided, the corresponding
   alignment. Near-miss analysis is modeled by using the             concept is updated. When a negative example is provided,
   Structure Mapping Engine to hypothesize classification            analogical retrieval is used to find the closest prior positive
   criteria based on differences. This is performed both on          example or generalization, and analogical matching is used
   labeled negative examples provided as input, and by using         to construct and update hypotheses about inclusion and
   analogical retrieval to find near-miss examples when positive
   examples are provided. We use a corpus of sketches to show
   that the model can learn concepts based on sketches and that
   incorporating near-miss analysis improves learning.
   Keywords: Concept learning; analogy; generalization.
                          Introduction
How concepts are learned from experience is a central
question in cognitive science. It is well-known that some
concepts can be viewed as analytic, having compact
necessary and sufficient defining criteria (e.g., grandparent
or triangle), whereas others are based on similarity or                 Figure 1: An example of the skeletal arm concept drawn
typicality (e.g., chair, bachelor). Prior work has explored                                    in CogSketch.
analogical generalization as an explanation for learning
similarity-based categories. The SAGE model of analogical            exclusion criteria for that concept. Near-miss analysis is
generalization, an evolutionary improvement over SEQL                also attempted when a positive example is provided, using
(Kuehne et al 2000a) has been used to model learning of              analogical retrieval over negative examples to look for a
perceptual stimuli (Kuehne et al 2000b), stories (Kuehne et          candidate near-miss. (Using analogical retrieval to find
al 2000a), spatial prepositions (Lockwood et al 2008) and            positive concepts and near-misses is a significant advance
causal models (Friedman & Forbus, 2008; Friedman &                   over Winston’s model, which used hand-coded
Forbus, 2009). SAGE’s ability to construct probabilistic             representations, a single abstract description for concepts
generalizations provides a model of typicality, i.e., high-          and required a teacher to supply all negative examples.) To
probability relationships and attributes are more typical.           test the model, we use sketches to describe concepts, which
SAGE has been used to model progressive alignment                    are automatically encoded by a sketch understanding
(Gentner et al 2007), where sequences of highly similar              system. We show that the model can indeed learn concepts
exemplars lead to more rapid learning (Kuehne et al 2000a).          from sketches, and that including near-miss analysis
Progressive alignment alone may suffice to generate rule-            improves learning. Our simulation is implemented using
like concepts (e.g., Gentner & Medina, 1998), but another            the Companions cognitive architecture (Forbus et al, 2009),
possibility is to use negative examples to sharpen criteria for      which integrates analogical processing and sketching.
concepts. Winston (1970) proposed the idea of a near-miss,
a labeled negative example that differs from the intended
                                                                       1
                                                                         For disjunctive concepts, some exemplars will not be similar.
                                                                 1726

   The next section summarizes the simulations of                   CogSketch
analogical processing and sketch understanding that our             CogSketch2 (Forbus et al, 2008) is an open-domain sketch
model is built upon. We describe our model next, followed           understanding system. The ink that a user draws to
by a description of our experiments. We close with related          represent an entity is called a glyph, which can be labeled
and future work.                                                    with concepts from an OpenCyc3-derived knowledge base.
                                                                    For example, in the sketch shown in Figure 1, each bone is
                Simulation Components                               labeled a Bone-BodyPart, which is stored as an attribute
                                                                    for each of the individual entities.
Analogical processing                                                  CogSketch automatically computes qualitative spatial
Our system uses three cognitive models as components to             relations (e.g., above, rightOf, touchesDirectly)
                                                                    between glyphs. In the knowledge representation that is
learn concepts and categorize examples. Similarity-based
                                                                    produced by CogSketch, these relations are automatically
retrieval is used to find similar examples across conceptual
                                                                    applied to the entities that the glyphs represent. CogSketch
boundaries. Analogical comparison is used to compare                also computes candidate visual/conceptual relations (again,
examples and generate classification hypotheses. Finally,           from the OpenCyc-derived knowledge base) for pairs of
analogical generalization is used to generalize examples.           sketched entities based on the visual relationships that hold
We use the Structure Mapping Engine (SME) (Falkenhainer             between them the conceptual labels they have been
et al, 1989) to model analogical matching, MAC/FAC                  assigned, and the genre and pose of the sketch. For
(Forbus et al, 1995) to model retrieval, and SAGE (Keuhne           example, the fact that the glyphs depicting the carpus and
et al, 2000) to model analogical generalization.                    metacarpus in Figure 1 touch suggests that the objects they
   SME is based on Gentner’s (1983) structure-mapping               depict might be touching or connected in some way. The
theory of analogy. Given two relational representations, a          list of candidate visual/conceptual relations for these objects
base and a target, SME computes mappings which represent            is further constrained by the Bone-BodyPart concept
how they can be aligned.             A mapping consists of          labels they have been assigned, as well as the Physical
correspondences which describe “what goes with what” in
the two representations and a numerical score indicating
their degree of similarity. SME also computes candidate                                           he                he       he
                                                                                Arch
inferences from the base to the target and from the target to                                     he
                                                                             context                               hi
the base. Candidate inferences suggest possible relations                                                                    hi
                                                                                                  hi
that can be transferred across representations, using the
correspondences in the mapping as support.
                                                                                 ...      generalizations      ungeneralized
   Given a probe case and case library, MAC/FAC
efficiently retrieves a case from the case library that is                                                          examples
similar to the probe. For scalability, its first stage estimates                                     he
                                                                            Triangle
similarity via dot products on vectors automatically                                                            he
produced from the structured, relational representations used                context
                                                                                                     hi
as cases. At most three descriptions are passed to the
second stage, which uses SME to compare their full
relational versions to the probe, in parallel, to find the best            Figure 2: SAGE generalization contexts for Arch and
case, or up to three cases if they are very close to the best.         Triangle concepts, with associated inclusion and exclusion
   Our model uses SAGE for generalization. Each concept                              hypotheses (hi and he, respectively).
has its own generalization context, which SAGE uses to
maintain a list of generalizations and ungeneralized                genre and from-side pose of the sketch. The user can
examples. Given a new example, it is first compared                 browse the candidate relationships and select those which
against each generalization in the context, using SME. If           are accurate. In our input stimuli, correct visual/conceptual
the SME similarity score is over the assimilation threshold,        relationship candidates were always included.
the example is merged to update the generalization.                    CogSketch is based on the observation that people talk
Otherwise, the new example is compared with the                     when they sketch, providing verbal labels for what they are
ungeneralized examples in the context. Again, if the score          drawing, and using language to express functional
is over threshold, the two examples are then combined to            relationships (e.g. that two parts can rotate, or that one
form a new generalization in the context. Otherwise, the            supports another) that the sketch alone cannot convey. The
example is added to the context’s list of ungeneralized             conceptual labels described above, which are applied by a
examples. Figure 2 depicts generalization contexts for              simple menu system, model the effect of verbal labeling.
concepts Arch and Triangle.                                         The possible visual/conceptual relationships described
                                                                    above, which are computed automatically and are available
                                                                       2
                                                                         http://www.qrg.northwestern.edu/software/cogsketch/
                                                                       3
                                                                         www.opencyc.org
                                                                1727

for the user to choose or not, model the effect of providing       Near-miss analysis. Winston argued for the importance of
functional information via language. This makes the input          near misses in learning concepts. A near miss consists of a
process much closer to what happens in human-to-human              positive example e1 (e.g. Figure 3, left) and a negative
sketching. The user draws ink, which CogSketch’s visual            example e2 (e.g. Figure 3, right) that differ only slightly.. In
system analyzes, producing visual and spatial relationships.       analogical reasoning terms, e1 and e2 are highly alignable,
The user-supplied conceptual labels plus the visual/spatial        enabling a learner to conjecture that differences between
analysis enables CogSketch to automatically compute                them could be useful criteria for classification. Two kinds
visual/conceptual relationship candidates, from which the          of hypotheses are computed to enhance concept
user can select, if they choose. (In the experiments reported      discrimination. Inclusion hypotheses represent potential
here, correct visual/conceptual relationships were always          necessary conditions for something to be an instance of the
chosen, thereby providing some functional information              concept. Exclusion hypotheses represent potential negative
about the concept.)                                                conditions that are sufficient to prevent something from
                                                                   being classified as an instance of that concept.
     Similarity & near-miss concept learning                          Near-miss analysis starts with a positive and a negative
                                                                   example. As noted above, one of these examples is a new
Our model takes as input a stream of labeled sketches.             learning example, while the other is a previous example
There are two kinds of labels: A positive label indicates that     retrieved via MAC/FAC. A similarity threshold of 0.75 is
the example is an instance of a concept, e.g., an arch. A          used for their comparison, to ensure high alignability.
negative label indicates that, whatever it is, it is not an           Figure 3 shows a near miss that was processed by our
example of that concept (e.g. not an arch). Currently the          simulation.      The positive example is used as the base
model assumes that concepts are mutually exclusive. When           whereas the negative example is used as the target, and they
the first positive example for a new concept is provided, a        are compared via SME. SME aligns a with e, b with f, c
generalization context is created for that concept. Positive       with g, and the grounds d with h. The conjunction of
examples are added to the appropriate generalization               positivenegative candidate inferences in the mapping
context, invoking SAGE on it. MAC/FAC is used to find a            becomes a new inclusion hypothesis (Figure 3, hi)
negative example similar to the positive example. If a             designating criteria that might be necessary for concept
sufficiently similar exemplar from a different concept is          membership.          Similarly, the conjunction of all
found, near-miss analysis is invoked. Similarly, when a
                                                                   negativepositive candidate inferences is becomes a new
negative example is provided, MAC/FAC is used to retrieve
                                                                   exclusion hypothesis (Figure 3, he) designating criteria that
the closest positive exemplar or generalization, which is
                                                                   might prevent concept membership. Here the attribute (isa
then used for near-miss analysis.
                                                                   a wedge) is the sole forward candidate inference, so it
  When given an example to categorize, the model uses
MAC/FAC to generate a reminding from each concept’s                becomes the inclusion hypothesis hi. Similarly, the block
context. The system tests the new example against the              attribute, touchesDirectly relations, and adjacentTo
classification criteria for each concept. Of the concepts          relations comprise the conjunctive exclusion hypothesis he.
whose criteria are satisfied, the one with the most similar           Inclusion and exclusion hypotheses are associated with
reminding is chosen as the category of the new example.            the positive example in the near miss, as shown in Figure 2.
  In explaining our model, we use as a running example             Consequently, when MAC/FAC retrieves more than one
learning the concept of an arch, which was first used by           near miss for a given positive example, the system
Winston (1970), who used hand-generated representations.           computes more than one inclusion and exclusion hypothesis
                                                                   about the example, and must combine them. Inclusion
                                                                   hypotheses pertaining to the same example are combined
       hi: (isa a wedge)     he: (and (isa a block)
                                      (adjacentTo b c)             via set union, since all necessary facts must hold for positive
                                      (adjacentTo c b)             classification.    Conversely, any exclusion hypothesis
                                      (touchesDirectly b c)
                                      (touchesDirectly c b)        suffices to rule out that concept, so they are kept separate.
                                                                      In Figure 3, the inclusion hypothesis hi generated by the
                                                                   system erroneously asserts that all arches have wedges as
                                                                   their top. This error reflects one learning bias of the model,
                                                                   which is the immediate assumption that all differences
                                                                   detected in the near miss of a concept are important to the
                                                                   definition of the concept. Such errors can be removed
                                                                   during analogical generalization, which we discuss next.
                                                                   Analogical generalization. During training, our learning
                                                                   system incrementally develops a disjunctive model of a
   Figure 3: A near miss of concept arch and the resulting         concept through the observation of positive and negative
     inclusion hypothesis hi and exclusion hypothesis he.          examples. As positive examples are observed, they are
                                                                   added to a SAGE generalization context for the concept,
                                                                   where they are generalized with sufficiently similar
                                                               1728

examples. When an example is generalized, resulting in              not hold on the new generalization. In Figure 4, the facts
new or larger generalizations (shown in Figure 2) the system        (isa a wedge) and (isa i block) are pruned from
revises the near-miss hypotheses associated with the                the inclusion hypotheses of the constituent examples
generalization constituents.                                        because they are not true of the resulting generalization, i.e.,
   Across generalizations, the near-miss hypotheses can be          the corresponding generalized entity gai is not known to be
considered disjunctive hypotheses about the concept. For            either wedge or block. After pruning, the facts of the two
example, suspension bridges may be different enough from            inclusion hypotheses are unioned to create a conjunctive
beam bridges that the classification hypotheses required of         hypothesis associated with the new generalization
them differ. We can capture this distinction if suspension             Next, the system uses the generalization operation to
bridge examples and beam bridge examples form separate              identify and discard erroneous exclusion hypotheses. In
generalizations when added to the generalization context for        Figure 4, exclusion hypothesis (isa i wedge) of the
the concept bridge. During classification, we may claim             middle example is erroneous because it shares a
that an example is a bridge if it is similar enough to the          generalization with the topmost example whose
suspension bridge generalization and satisfies the conditions       corresponding entity a is a wedge. Consequently, the
for suspension bridge, or if it is similar enough to the beam       exclusion hypothesis is discarded. Remaining exclusion
bridge generalization and satisfies the conditions for beam         hypotheses are mapped onto the resulting generalization.
bridge. The construction of disjunctive hypotheses based            Finally, the system discards exclusion hypotheses of the
on similarity introduces another learning bias of the model,        resulting generalization that are more specific than other
which assumes that similar examples of a concept are                associated hypotheses (i.e., for every exclusion hypothesis
subject to the same rules for membership.                           composed of fact set f, any hypothesis of fact set f’ such that
   After an observed positive example is generalized with an        f  f’ is eliminated). In Figure 4, hypothesis he of the
existing generalization or ungeneralized example, their             topmost example is discarded for this reason.
hypotheses are generalized. Figure 4 shows how a new
example (top) and a previously ungeneralized example                Classification
(middle) are merged into a new generalization with revised          Given a new testing example enew, our model decides
hypotheses (bottom).                                                whether it is an instance of one of its learned concepts. The
                                                                    model decides this using similarity-based retrieval and by
                             hi: (isa a wedge)
                             he: (and (isa a block)                 testing the hypotheses created during learning.
                                      (adjacentTo b c)                 For each learned concept, the system uses MAC/FAC to
                                      (adjacentTo c b)
                                      (touchesDirectly b c)
                                                                    retrieve the most similar generalization or ungeneralized
                                      (touchesDirectly c b)         example of the concept ec from the concept’s generalization
                                                                    context. The inclusion and exclusion hypotheses associated
                                                                    with ec (as shown in Figure 2) are used as criteria for
                                                                    classifying enew.
                             hi: (and (isa i block)
                                      (onPhysical i k)                 The inclusion and exclusion hypotheses associated with ec
                                      (touchesDirectly i k)         are represented in terms of the entities in ec, which typically
                             he1: (and (adjacentTo j k)             do not exist in enew. Consequently, structural alignment is
                                       (adjacentTo k j)
                                       (touchesDirectly j k)        used to perform the analogical equivalent of rule
                                       (touchesDirectly k j)        application. SME is used to find entity correspondences
                             he2: (isa i wedge)
                                                                    between ec and enew, and the entities of ec are substituted
                                                                    with the corresponding entities in enew in each hypothesis.
                                                                       Testing the classification criteria is the final step in
                             hi: (and (onPhysical gai gck)
                                      (touchesDirectly gai gck)     classification. If an inclusion hypothesis does not hold in
                             he: (and (adjacentTo gbj gck)          enew, or if an exclusion hypothesis does hold in enew, it is not
                                      (adjacentTo gck gbj)          an instance of the concept. Otherwise, enew is an instance of
                                      (touchesDirectly gbj gck)
                                      (touchesDirectly gck gbj)     the concept. If enew is a viable instance of multiple concepts,
                                                                    given the exclusion and inclusion criteria, the system
                                                                    chooses the concept whose MAC/FAC reminding similarity
                                                                    score was higher. Thus our model of concepts combines
  Figure 4: The generalization of two positive examples and         both rule-based and similarty-based aspects.
           their inclusion and exclusion hypotheses
                                                                                            Experiment
   The first step in generalizing inclusion hypotheses is           We created a series of 44 sketches representing six concepts
mapping the hypotheses from their respective generalized            for learning and categorization, summarized in Table 1. The
examples to the newly created generalization. This involves         false arches, false triangles, and false squares sketches are
replacing the names of entities with the names of                   all highly alignable with examples of their associated
corresponding entities in the generalization. Next, inclusion       concept, but are not positive examples themselves.
hypotheses are pruned by removing any assertions that do
                                                                1729

          Table 1: Sketched examples for simulation.               p < 0.001. The number of false positives decreased from
                                                                   eight to two but the number of false negatives increased
         Arches:             8      Triangles:           4         from one to four due to overly restrictive hypotheses. The
         False arches:       8      False triangles:     4         rightmost example in Figure 3 was among the negative
         Bridges:            4      Squares              4         examples correctly classified. Just as with similarity-only,
         Skeletal arms:      4      False squares:       4         the model determined that this example was sufficiently
         Skeletal legs:      4                                     similar to a generalization of the concept arch. However, it
                                                                   reported a failure to meet classification conditions due to a
   Our experiment follows a four-fold cross validation             satisfied exclusion hypothesis,
format covering all 44 sketches. The sketches were                         (TheSet (adjacentTo f g)
randomly assigned to four groups (folds) of 11 sketches                               (touchesDirectly g f))
each, with the constraint that all groups had the same             which expresses the justification “This is not an arch
distribution of sketches from the categories in Table 1 (two       because f is adjacent to g and g touches f directly.”
arches, two false arches, one bridge, one skeletal arm, etc).
The system trained on three 11-example groups, for a total                       Discussion & Future Work
of 33 examples for learning. The remaining group of 11             We have described a model that extends analogical
examples is used for classification testing. We repeat this        generalization with near-miss analysis to learn concepts
four times, so each group of 11 examples is used once for          from sketches. We have generalized the notion of near-miss
testing, resulting in 44 classifications total.                    that Winston (1970) used in two important ways. First,
   We tested our simulation under two conditions: The full         Winston assumed that near-misses were always provided by
condition uses the entire model, while in the similarity-only      a teacher. We have shown that near misses can also
condition, near-miss analysis is turned off. In similarity-        naturally arise from the process of similarity-based retrieval,
only, the system classifies a new example by using                 thereby providing more self-direction in learning. Second,
MAC/FAC to retrieve a similar representation from the              Winston’s system had one description of the target concept
concept context, and asserts concept membership if the             it was learning, and hence did not capture the possibility of
normalized SME similarity score is above a threshold of            disjunctive concepts and finding the appropriate conceptual
0.85. We expected that, based on prior experiments                 representation, which we do via a combination of SAGE
(Kuehne et al 2000b), similarity-only will learn quite well        and MAC/FAC. A version of the model without near-
with only a handful of examples. However, we also expect           misses, using similarity alone, performs well over chance.
that it will show false positives due to misleadingly similar      However, similarity alone leads to a pattern of
negative examples, which near-miss analysis should                 misclassification errors, which is partially corrected by near-
prevent.                                                           miss analysis. The incorporation of classification criteria
                                                                   enables the model to make more expressive justifications for
                                                                   its classification decisions, as in the case of the negative
                                                                   example from Figure 3. We also believe that near-miss
                                                                   analysis will allow the model to more readily benefit from a
                                                                   larger training set, as hypotheses from new near-misses will
                                                                   add potentially valuable criteria to reduce false positives and
                                                                   hypothesis generalization will alleviate over-restrictiveness,
                                                                   which accounted for all but one of the false negatives. We
                                                                   expect the similarity-only classifier to gain less from
                                                                   additional training, since the examples it misclassifies are
                                                                   mostly negative examples that bear high relational similarity
                                                                   to positive examples. Thus near-miss analysis provides an
                                                                   important extension to similarity-based concept learning.
 Figure 5: Effectiveness of using structural similarity alone         Our concept learning model learns several concepts
    for classification, as a function of similarity threshold.     simultaneously, with relatively few examples. It requires
                                                                   orders of magnitude fewer examples than existing
   In the similarity-only condition, 79% correct                   connectionist models of concept learning (e.g., Krushke,
classification is achieved with a similarity threshold of 0.75     1992; Regier 1996; Elman 1999), and unlike such models,
(Figure 5), well above chance (p < 0.001). Inspection of the       uses automatically encoded relational stimuli, to reduce
results revealed that almost all of the 20% error can be           tailorability. We believe our model makes more realistic
attributed to false positives. One such false positive is the      demands, although it could be argued that our model learns
rightmost example in Figure 3, which shares considerable           too quickly. One reason that we see such rapid learning in
relational structure with other arches.                            simulation experiments is that our system, unlike people,
   With near-miss analysis turned on, 86% correct                  has many fewer distracters. Everyday life does not always
classification was achieved, which is better than chance with      afford closely packed sequences of similar concept
                                                               1730

instances, and human perception may contain more                   Gentner, D. (1983). Structure-Mapping: A Theoretical
attributes and relations than CogSketch currently computes.          Framework for Analogy. Cognitive Science, 7: 155-170.
However studies such Gentner et al (2009) suggest that             Gentner, D., Levine, S., Dhillon, S. & Poltermann, A.
people can learn spatial concepts quickly with highly                (2009). Using structural alignment to facilitate learning of
alignable near-misses, which our model captures nicely.              spatial concepts in an informal setting. In Proceedings of
   Winston (1982, 1986) also explored learning rules from            the Second International Workshop on Analogy, Sofia,
analogies, using simplified English inputs. His system               Bulgaria, 2009.
generalized based on one example, rather than several, and         Gentner, D., Loewenstein, J., & Hung, B. (2007).
produced logical quantified rules, while ours uses analogical        Comparison facilitates children's learning of names for
matching to apply hypotheses to new examples. His if-then            parts. Journal of Cognition and Development, 8. 285-307.
rules and censors are functionally similar to our inclusion        Gentner, D. & Medina, J. (1998). Similarity and the
and exclusion hypotheses, respectively.                              development of rules. Cognition 65(2-3):263-97.
   There are several aspects of concept learning that our          Kruschke, JK (1992). ALCOVE: An exemplar-based
model does not currently capture. For example, our                   connectionist       model      of     category     learning.
sketched input does not include causal relationships or goals        Psychological Review 99, 22-44.
(Lombrozo, 2009; Rehder & Kim, 2006). Based on prior               Kuehne, S., Forbus, K., Gentner, D. and Quinn, B. (2000).
work (Falkenhainer, 1987; Friedman & Forbus, 2009) we                SAGE: Category learning as progressive abstraction using
believe our model will handle such information if it is              structure mapping. Proceedings of CogSci 2000.
included in the initial encoding, since it basically adds          Kuehne, S., Gentner, D. and Forbus, K. (2000). Modeling
relational structure that influences similarity judgments, and       infant learning via symbolic structural alignment.
hence classification, in our model. Other factors, such as           Proceedings of CogSci 2000.
ontological structure (Medin & Smith, 1984) and centrality         Lockwood, K., Lovett, A., and Forbus, K. (2008).
and mutability of properties (Sloman, Love, & Ahn, 1998)             Automatic Classification of Containment and Support
we believe can be handled by further exploiting the                  Spatial Relations in English and Dutch. In the
statistical information gathered via SAGE in cross-concept           Proceedings of Spatial Cognition 2008.
analyses. We plan to explore both of these issues in future        Lombrozo, T. (2009). Explanation and categorization: how
work.                                                                ''why?'' informs ''what?''. Cognition, 110, 248-253.
                                                                   Medin, D. and Smith, E. (1984). Concepts and concept
                      Acknowledgments                                formation. Annual Reviews of Psychology, 35, 113-138.
This work is supported by the Cognitive Science Program of         Regier, T. The human semantic potential: Spatial language
the Office of Naval Research.                                        and constrained connectionism, Cambridge Mass: MIT
                                                                     Press (1996).
                          References                               Rehder, B. & Kim, S. (2006). How causal knowledge
                                                                     affects classification: A generative theory of
Elman, J. (1999). Generalization, rules, and neural net-             categorization. Journal of Experimental Psychology:
   works: A simulation of Marcus et. al, (1999). Ms., Uni-           Learning, Memory, and Cognition, 32, 659-683.
   versity of California, San Diego.                               Rips, L. J., & Handte, J. (1984). Classification without
Falkenhainer, B., Forbus, K. and Gentner, D. (1989). The             similarity. Unpublished manuscript, Univ. Chicago.
   Structure Mapping Engine: Algorithm and examples.               Sloman, S., Love, B., Ahn, W. K. (1998). Feature centrality
   Artificial Intelligence, 41, 1-63.                                and conceptual coherence. Cognitive Science 22(2). 189-
Forbus, K., Klenk, M., and Hinrichs, T. (2009). Companion            228.
   Cognitive Systems: Design Goals and Lessons Learned             Winston, P.H. 1970. Learning structural descriptions by
   So Far. IEEE Intelligent Systems, vol. 24, no. 4, pp. 36-         examples. Ph.D. thesis, MIT.
   46, July/August.                                                Winston, P.H. 1982.          Learning new principles from
Forbus, K., Lovett, A., Lockwood, K., Wetzel, J., Matuk,             precedents and exercises. Artificial Intelligence 23(12).
   C., Jee, B., and Usher, J. (2008). CogSketch. Proceedings       Winston, P.H. 1986. Learning by augmenting rules and
   of AAAI 2008.                                                     accumulating censors. In Michalski, R., Carbonell, J. and
Forbus, K., Gentner, D. and Law, K. (1995). MAC/FAC: A               Mitchell, T. (Eds.) Machine Learning: An Artificial
   model of similarity-based retrieval. Cognitive Science,           Intelligence Approach, Volume 2. Pp. 45-62. Morgan-
   19(2), 141-205.                                                   Kaufman.
Friedman, S. & Forbus, K. (2008). Learning Causal Models
   via Progressive Alignment & Qualitative Modeling: A
   Simulation. In Proceedings of the 30th Annual
   Conference of the Cognitive Science Society.
Friedman, S. & Forbus, K. (2009). Learning Naïve Physics
   Models & Misconceptions. In Proceedings of the 31st
   Annual Conference of the Cognitive Science Society.
                                                               1731

