UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
What You Did and Didn't Mean: Noise, Context, and Human Skill
Permalink
https://escholarship.org/uc/item/74d497g2
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Ligorio, Tiziana
Epstein, Susan L.
Passonneau, Rebecca J.
et al.
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                What You Did and Didn’t Mean: Noise, Context, and Human Skill
                                              Tiziana Ligorio (tligorio@gc.cuny.edu)
                                       Susan L. Epstein (susan.epstein@hunter.cuny.edu)
        Department of Computer Science, Hunter College and The Graduate Center of The City University of New York
                                                        New York, NY 10016 USA
                                        Rebecca J. Passonneau (becky@cs.columbia.edu)
                                         Joshua B. Gordon (gordon@cs.columbia.edu)
                                   Center for Computational Learning Systems, Columbia University
                                                        New York, NY 10037 USA
                              Abstract                                  signed an interpretation. Under real-world telephone condi-
   In spoken dialogue between people and machines, the com-
                                                                        tions, however, even state-of-the-art ASR can exhibit a
   puter must understand not only what the speaker means but            word error rate (WER) as high as 68% (Raux et al., 2005).
   also what she does not. The computer begins with a consider-         High WER is common when the environment is noisy, the
   able disadvantage: even the best speech recognition technol-         language the system is expected to understand is flexible
   ogy can provide error-ridden transcriptions of human speech          and based on a large vocabulary, or the user population is
   under real-world telephone conditions. The work recounted            diverse in gender, age, and native language. These are all
   here examines how, and how well, people use context to in-           characteristic of our target domain: telephoned book re-
   terpret noisy transcribed utterances in a challenging domain.
   Models learned from this experiment highlight two aspects of         quests from patrons of the Andrew Heiskell Braille and
   this human skill: the ability to detect a context-supported          Talking Books Library.
   match, and the ability to know when the quality of attempted            Although people manage dialogue well in the presence of
   matches is so poor that it should be questioned. These models        noise, computers do not. Our subjects used ASR output
   can then be applied by a spoken dialogue system to find the          from a spoken title to query our copy of Heiskell’s book da-
   correct interpretation of users’ spoken requests, despite incor-     tabase. For example, for one book title, the ASR output
   rect speech recognition.
                                                                        string was “ROLL DWELL.” A database query on this string
   Keywords: spoken dialogue systems; natural language proc-            returned three likely matches (in real time): “CROMWELL,”
   essing; machine learning; Wizard of Oz studies, learning.            “ROBERT LOWELL,” and “ROAD TO WEALTH.”
                                                                           This is a difficult task. (The reader is invited to guess
                          Introduction                                  whether any of these actually matches the spoken title.) Our
A computer system intended to replicate a human skill faces             experiment studies how people manage this task. The resul-
two considerable limitations: it works from a different input           tant data is then used to train accurate models of human be-
modality and it is restricted to a preprogrammed set of alter-          havior, and to identify the features that make people profi-
native actions. The thesis of our work is that people should            cient. Such models are ultimately intended as an integral
be studied for their skill at the target task as if they were si-       part of an SDS, to make it more robust to noisy ASR in the
milarly restricted, that is, as if they had only the system’s           context of database queries. The next sections of this paper
input and alternatives. The domain of investigation here is a           describe related work, our target system, and the experimen-
spoken dialogue system (SDS). Subjects were given the                   tal design and results. These are followed by a description of
same data that would be available to the SDS: error-ridden              the models learned from the data collected during the ex-
strings representing transcribed speech plus a large database           periment, and a discussion of their import and application.
of possible matches (Passonneau et al., In Press). The resul-
tant data was then used to identify the best performers, and                                   Related Work
to learn models of them destined for the system. There were             The Wizard of Oz (WOz) paradigm is a well-known ap-
two principal results. First, subjects ably guessed what the            proach to iterative prototype design. It gathers information
speaker meant, that is, they could often identify the correct           about the characteristics of a successful system before the
item from the context provided by a database query on the               system’s development (Dix, et al. 2003). In a WOz experi-
error-ridden string. Second, the people most skilled at this            ment, only a user-system interface is provided. Users be-
task excelled because they could also identify what the                 lieve they are interacting with a computer system through
speaker had not meant, that is, they knew when no item re-              this interface, but instead a person (the wizard) is “behind
turned from the database was a correct match. Such recogni-             the curtain.” This permits the system designer to observe
tion is essential to move dialogue forward constructively               human responses to certain system functionalities, to study
   Ideally, an SDS offers people a natural way to communi-              user behavior and expectation, and to assess interface design
cate with a computer and benefit from its expertise. In an              features before the construction of an initial prototype.
SDS, automated speech recognition (ASR) transcribes hu-                    WOz can also be used to study the wizard, to provide data
man spoken input into a string of words, which is then as-              on how the system should behave. In particular, wizard ab-
                                                                    996

lation is a WOz study in which a wizard relies on system              patrons have with Heiskell’s librarians, we transcribed 82
input and output rather than her own communication re-                telephone calls from a larger set we had recorded. Forty four
sources (Levin and Passonneau 2006). Wizard ablation sup-             percent of the book requests were by catalogue number,
ports the collection of dialogues that illustrate the decisions       28% by title or a combination of title and author, and 28%
people make when confronted by the same input/output data             were more general. CheckItOut is therefore designed to ac-
and choices as an SDS. Data collected under wizard ablation           cept book requests by catalogue number, author, or title.
supports supervised learning to predict wizard actions. The              CheckItOut builds upon the Olympus/Ravenclaw archi-
resultant model can then be incorporated into a system to             tecture and dialogue management framework (Bohus, et al.
improve its behavior. WOz studies that directed their atten-          2007, Bohus and Rudniky 2003). Olympus/Ravenclaw has
tion to the wizard during full spoken dialogues include ef-           been the basis for approximately a dozen research SDSs in
forts to predict: the wizard’s response when the user is not          different domains. During a dialogue with CheckItOut, the
understood (Bohus 2004), the wizard’s use of multimodal               user first identifies herself as a patron of the library, and
clarification strategies (Rieser and Lemon 2006), and the             then requests at most four books. CheckItOut references two
wizard’s use of application-specific clarification strategies         databases: a sanitized version of Heiskell’s database of
(Skantze 2005). The experiment presented here is restricted           5,028 patrons, and its entire book database with 71,166 titles
to single utterances, rather than full dialogues. It also differs     and 28,031 authors. These force CheckItOut to manage a
in that it analyzes several wizards’ behavior. It recognizes          large vocabulary; titles and author names alone contribute
differences among wizards and identifies distinctive and              54,448 distinct words. Moreover, Heiskell’s patrons include
successful behavior, so that the system will ultimately bene-         many elderly and non-native speakers. The experiment de-
fit only from models of the most skilled wizards.                     scribed next observes how human wizards respond to the
   To limit communication errors incurred by faulty ASR, an           same challenges that CheckItOut confronts.
SDS may use enriched strategies to detect and respond to
incorrect recognition output (Bohus 2004). It may repeat-                                 Experimental Design
edly request user confirmation to avoid misunderstanding,             The experiment described here seeks to uncover how people
or ask for confirmation using language that elicits responses         marshal system resources (e.g., the ASR string and database
from the user that the system can handle (Raux and                    results), and which strategies achieve the best performance.
Eskenazi 2004). When the user adds new information in re-             Here the focus is on single turn interactions that request
sponse to a system prompt, two-pass recognition can con-              books by title, the CheckItOut request type most likely to
sider the extra information contained in such user responses          elicit problematic ASR output.
to restrict the language expected in the second pass and the-            In an offline pilot study, 3 native speakers of English read
reby achieve better recognition (Stoyanchev and Stent                 50 titles to generate 3 sets of ASR output strings
2009). In a highly interactive setting, an SDS might benefit          (Passonneau, et al. 2009). Each subject received a different
when it takes this approach one step further and uses con-            ASR set and was asked to find the corresponding title from
text-specific language for incremental understanding of the           a text file that listed all 71,166 titles. WER was 69% – 83%,
noisy input throughout the dialog (Aist, et al. 2007). This           depending on the speaker. Despite the high WER, these sub-
paper explores the use of system-internal resources, such as          jects identified the correct title 74% of the time.
a database search, to respond to faulty ASR. It embeds a wi-             Given this demonstration of human skill, we designed a
zard into a system, and then observes and models her ability          WOz study to identify which aspects of human performance
to use such context to respond appropriately.                         come into play when a wizard seeks to match noisy ASR
   Peripherally related are other approaches that increase            against a list of candidates (possible title matches) (Passon-
understanding between an SDS and the user through the ad-             neau et al. In Press). The experiment was designed to iden-
aptation of an SDS’s response based on a user model. In               tify what makes a good wizard, and to extract any additional
automated tutoring, for example, it is essential to validate          insights a wizard may offer when supported by database
the user when she is correct and to elicit more reasoning             search with the quality common in modern systems.
when she is not (Franceschetti, et al. 2003, Ohlsson, et al.             During the experiment, users and wizards were isolated
2003). In particular, affect-adaptive systems can improve             from one another in separate rooms. Each had her own
learning efficiency by responding to uncertainty in the tran-         graphical user interface (GUI) and microphone. In a title cy-
scribed speech (Forbes-Riley and Litman 2009).                        cle, the user read a book title into a speech recognizer
                                                                      through the microphone, and the corresponding ASR was
          Ordering Books with CheckItOut                              displayed on the wizard’s GUI. The wizard then formulated
CheckItOut is a research SDS for book requests from pa-               a query for the database. Once the search returned a list of
trons of the Andrew Heiskell Braille and Talking Books Li-            candidates, the wizard had four options: make a confident
brary, a branch of the New York Public Library and part of            choice among the candidates, make a tentative choice
the National Library System. Patrons of the library request           among the candidates, ask a question through her micro-
books by telephone and receive them by mail. Regular                  phone, or give up. (Wizards were also permitted to ask the
newsletters provide patrons with the titles and catalogue             user to repeat the title, but were discouraged from doing so.)
numbers of new books. To gauge the kinds of interactions              If the wizard chose a candidate, it then appeared on the us-
                                                                  997

user’s GUI, and the user scored it as correct or incorrect.          thoughtful decisions, no time limits were imposed upon ei-
That score was also displayed on the wizard’s GUI, so that           ther the wizard or the user. Finally, we devised a score that
the wizard knew if her most recent title choice was correct          subjects were asked to maximize throughout the experiment,
or incorrect. If the wizard asked a question instead, the user       with prizes to be awarded for the top two scorers. The wiz-
heard it through her headset and rated it on her GUI. The            ard scored +1 for a correctly identified title, +0.5 for a rele-
possible ratings with respect to the current book request            vant question and -1 for an incorrect title. To encourage co-
were “relevant and I can answer it,” “relevant but I cannot          operation between users and wizards, the user also scored
answer it,” irrelevant,” and “uncertain.” Question ratings           +0.5 for a successfully recognized title.
were not shared with the wizard. After the wizard saw the
user’s score or was notified that the user had judged the                                       Results
question, the wizard signaled the beginning of a new cycle.          The analysis in this section provides essential support for
   The speech recognizer continually transcribed the speech          automatically learning models of intelligent behavior wor-
signal from the user’s microphone, and the wizard’s GUI              thy of incorporation into an SDS. Given the permitted early
provided a live feed of the resultant ASR strings. For each          termination, there were 4172 title cycles (instead of 4200).
request, the wizard submitted a database query after very            In them, the average WER was 69%. Nonetheless, the dis-
limited editing of those strings (e.g., removing “um”). The          tribution of database returns was 46.7% Singleton, 53.26%
return from the database was displayed on the wizard’s GUI           AmbiguousList and 2.83% NoisyList. (Although in pilot
as a list of candidates in descending order of search confi-         tests 5% - 10% of the returns were empty, during the ex-
dence. This confidence was measured using Rat-                       periment itself none were.)
cliff/Obershelp pattern recognition (R/O) which evaluates               Figure 1 shows the overall distribution of wizard actions
the similarity of the ASR string to a book title from the da-        for our subjects, W1 through W7. Each of them saw a simi-
tabase (Ratcliff and Metzener 1988). Confidence scores               lar distribution of database returns: Singleton (µ = 278.57, σ
were not displayed on the wizard’s GUI.                              = 21.16), AmbiguousList (µ = 300.57, σ = 16.92), and Noi-
   Given an ASR query, the database produced one of the
                                                                     syList (µ = 16.86, σ = 4.78). The correct title was among
four following kinds of returns, based on the R/O scores:
                                                                     the candidates returned by the database 71.31% of the time.
   • Singleton: the single top-scoring candidate, if any were
                                                                     Singleton returns were the correct title 92.05% of the time.
      very good (R/O ≥ 0.85)
                                                                     AmbiguousList and NoisyList returns contained the correct
   • AmbiguousList: two to five moderately good candidates
                                                                     title 53.74% of the time.
      (0.85 > R/O ≥ 0.55)
                                                                        Ideally, a wizard should identify the correct title if it ap-
   • NoisyList: six to nine poor but non-random candidates           pears among the candidates, and otherwise ask a thoughtful
      (0.55 > R/O ≥ 0.40)                                            question that could constructively advance the dialogue. As
   • Empty: No candidates (max R/O < 0.40)                           one might expect from our pilot study, wizards knew what
Our focus here is not on the database search, but on the wiz-        the user meant when they saw it. If the correct title was
ard’s actions given noisy ASR and an adequate but imper-             among the candidates, wizards identified it confidently
fect database return. Words in each candidate that exactly           68.72% of the time and tentatively 26.53% of the time —
matched a word in the query appeared darkest on the GUI.             95.25% in all. Recall, however, that AmbiguousLists and
All other words appeared in grayscale in proportion to their         NoisyLists were sorted by search confidence. When the da-
degree of character overlap with the words in the query.             tabase returned multiple candidates, the top candidate was
   Two of the seven subjects were non-native speakers of             the correct title 41% of the time. It was second 5.81%, third
English (one Spanish, one Romanian). Each pair of students           2.61%, fourth 2.20%, and later (fifth through ninth) 1.67%
(a total of 21 possible pairs) met five times. In each meeting,      of the time. This did indeed help the wizards, who correctly
one student was the user and the other was the wizard in a           offered the first title 98.34% of the time (74.24% confi-
session of 20 title cycles. Then the pair immediately ex-            dently, and 24.10% tentatively). Of course, preference for
changed roles to run a second session of 20 title cycles.
Thus, each student was the wizard on 100 title cycles and
the user on 100 title cycles with every other student, for a
possible 4200 title cycles in all. Users were permitted to end
a session early after fewer than 20 title cycles if they experi-
enced severe system problems.
   Beyond the mechanics of this process, it was important to
create a dialogue-like environment and to encourage the
best possible performance from our subjects. To make her
speech more conversational and less like simply reading a
list, the user prepared immediately before each session. She
read brief synopses of the 20 titles (chosen at random from
the database) and then ordered them in some way (e.g., ge-
nre or theme) relevant to their content. To encourage
                                                                                   Figure 1: Distribution of wizard actions
                                                                 998

the top returned candidate is readily programmed into an           Table 1: Raw session score, accuracy, proportion of offered
SDS. Instead we focus here on what wizards did when the            titles listed first in the database search return, and frequency
title was not among the candidates.                                of correct non-offers for seven participants.
   Wizards were less skilled at recognizing what the user
had not meant. Indeed, their performance differed primarily                                Session              Chose      Correct
on their response when the correct title was not among the           Subject    Cycles      score   Accuracy      #1      non-offer
candidates — most wizards were less accurate then, and                 W4       600        0.7585   0.8550      70%         64%
their performance was less uniform. Despite careful instruc-           W5       600        0.7584   0.8133      76%         43%
tions to the subjects that had explained this option, wizards          W7       599        0.6971   0.7346      76%         14%
asked a question in only 22.32% of the cases where the cor-            W1       593        0.6936   0.7319      79%         16%
rect title was not among the candidates. Instead they made a           W2       599        0.6703   0.7212      74%         10%
tentative guess (67.71%), chose confidently (7.78%), or                W3       581        0.6648   0.6954      81%         20%
gave up (2.20%). Table 1 shows each wizard’s number of ti-             W6       600        0.6103   0.6950      86%          3%
tle cycles, session score, and accuracy, the proportion of ti-
tle cycles where she identified the correct title or correctly     derstanding. (Much of this system information was not
recognized that the title was not among the candidates (by         available to the wizard.) Other features described the session
asking a question or giving up). It also shows the frequen-        history (e.g., number of correctly identified titles so far), the
cies with which she offered the top candidate and correctly        database return (e.g., return type of Singleton, Ambigu-
recognized that the title was not among the candidates             ousList, NoisyList), or the similarity between the ASR
   Wizards are ranked in Table 1 in descending order of ses-       string and the candidates (e.g., number of matching words).
sion score and accuracy. Those values are highly correlated        Because the number of candidates differed across title cy-
(R = 0.91, p = 0.0041). W4 scored highest, primarily be-           cles, these features were averaged over multiple candidates.
cause of the frequency with which she asked a question                As a machine learning technique, we chose decision trees
when the candidates did not include the correct title (correct     to model wizard behavior because they are easy to interpret
non-offers = 64%). Table 2 shows the distribution of what          and compare, and relatively transparent. A decision tree
should have been the correct action across all 4172 title cy-      maps feature values to a target value (here, wizard action).
cles. The correct action was either to offer the title as the      A decision tree is a tree-like structure of nodes with directed
correct candidate at a given position (Return 1 through Re-        links between them. Each node is a branch test based on fea-
turn 9) or to ask a question or give up when the title was not     ture values. To simulate the modeled behavior, a program
among the candidates. Table 2 makes clear that the simple          traces a path from the root (the top node), following the
strategy “always guess the top candidate” (as our wizards          branch tests until it reaches a leaf, a non-branch node that
often did) would achieve about 65% accuracy. Note too that         provides a target value. With a version of C4.5 (Quinlan
those wizards who relied on it most (W3 and W6) were also          1993), we trained two kinds of decision-tree models: an
the least accurate overall, while the wizard who relied on it      overall model that used data from all the wizards to predict
least (W4) was the most accurate. Clearly, given a reason-         wizard action in general, and seven individual wizard mod-
able but fallible database search on noisy ASR, an SDS             els, one for each wizard.
should emulate W4, not simply choose the top candidate.               Cross-correlation over the features indicated that many of
                                                                   the initial 60 features were heavily correlated. We manually
            Learning to be Like a Wizard                           isolated groups of correlated features with R2 > 0.5, and re-
Wizards collaborate with the SDS — the system manages              tained only one representative feature from each group. We
input and output (except for the wizard’s questions), while        grouped features that described the similarity between ASR
the wizard exploits the available information (ASR string          string and candidates, features that described the database
and database return) to make a decision. Our experimental
design also captured data that described the system and the                  Table 2: Distribution of correct wizard actions
wizard’s session history. That data was then used to train
models of wizard actions selection. Such models could be                       Correct action           N             %
used to implement the best wizard behavior within an SDS.                   Return 1                     2722       65.2445
   The experiment collected data on 60 features available at                Return 2                      126        3.0201
run time, selected for their likely relevance to wizard action              Return 3                       56        1.3423
choices. They described the ASR (e.g., number of words in                   Return 4                       46        1.1026
the ASR string), the recognition process (e.g., recognizer’s                Return 5                       26        0.6232
confidence score when it produced the ASR string), the                      Return 6                         0       0.0000
speech signal (e.g., speech rate as number of 10ms speech                   Return 7                         7       0.1678
fragments per word), the ability of the SDS to interpret the                Return 8                         1       0.0002
ASR string (e.g., number of parses in the natural language                  Return 9                         2       0.0005
understanding component), and Olympus/ Ravenclaw con-                       Question | give up           1186        0.2843
fidence scores that combine recognition with language un-                   Total                        4172        1.0000
                                                               999

search returns, features that described confidence scores           F ≤ 0.87, but less consistently predicted tentative choices
from various system components, and features that de-               with 0.60 ≤ F ≤ 0.89, and could predict question only for
scribed the speech signal. This left 28 features. Before train-     W4, the top-scoring wizard who most often asked questions.
ing each model we also ran CfsSubsetEval, an attribute se-             The features that appeared most often in the individual
lection algorithm that evaluates subsets of features based on       models primarily described the database return, the ASR
both their individual predictive power and the degree of re-        string‘s similarity to the candidates, the wizard’s recent per-
dundancy among them (Hall 1999). This further reduced the           formance, and the quality of the speech recognition and lan-
number of features to between 8 and 12 per model. (Many             guage understanding. (Note that the last two were not avail-
of the same features survived into more than one model.) To         able to the wizard.) The five features that appeared most of-
reduce overfitting, we also activated pruning to remove sub-        ten at the root or top-level nodes were
trees likely to provide little additional power because they           • ReturnType (Singleton, AmbiguousList, NoisyList)
cover too few training instances.                                      • RecentSuccess, how often the wizard had chosen the
   To confirm the learnability and quality of the decision             correct title within the last three title cycles
trees, we also trained logistic regression and linear regres-          • ContiguousWordMatch, the maximum number of con-
sion models on the same data. Here, regression captures the            tiguous word matches between a candidate and the ASR
change in wizard action based on the changes in feature val-           string (averaged across candidates)
ues (Witten and Frank 2005). Linear regression fits data to a          • NumberOfCandidates, how many candidates were re-
linear function, and represents the wizard’s four actions nu-          turned by the database
merically in decreasing value: confident choice, tentative             • Confidence, an Olympus/Ravenclaw metric on confi-
choice, question, and give up. Logistic regression predicts            dence for recognition and language understanding
the probability of an action based on fit to a logistic curve.         Careful inspection of the model for the most accurate wi-
This generalizes the linear model to predict categorical data,      zard (W4) indicates that, if ReturnType was NoisyList, she
here, the wizard’s four actions. All models were produced           asked a question. If ReturnType was AmbiguousList, her
with the Weka data mining package (Hall, et al. 2009) under         decision involved the five features above, plus the acoustic
10-fold cross-validation.                                           model score (another internal system measure that indicates
   Ability to predict wizard action was uniform across learn-       the quality of the speech recognition), the length of the ASR
ing methods. On the overall model, logistic regression had          string in words, the number of times the wizard asked the
75.2% accuracy while the decision tree’s accuracy was               user to repeat, and the maximum size of the gap between
82.2%. The linear regression model had root mean squared            matching words in the ASR string and the candidates. To
error of 0.483, while the decision trees’ was 0.306. Predic-        further focus our analysis on W4’s distinctive behavior, we
tive ability for the individual wizard models was similarly         trained an additional decision tree to model how W4 chose
comparable. Thus the remainder of this discussion is re-            between selecting a title and asking a question. The
stricted to decision trees.                                         resulting model on 600 data points (each corresponds to a ti-
   Table 3 describes the learned models for individual wiz-         tle cycle) consisted of 37 nodes and 8 features, with F = .91
ards (ranked by wizard accuracy from Table 1). It shows             for selecting a title and F = 0.68 for asking a question. The
size in number of nodes, number of included features, accu-         root of this tree differs from all other wizard models — it is
racy, and the F measure on confident choice. Note that              the number of frames (10ms speech segments used to pro-
model accuracy does not correlate with wizard rank; model           duce the ASR string), a measure of the length of the ASR.
accuracy indicates only how well the tree predicts the wiz-         On short ASR strings (as measured both in number of
ard’s action from the training data. The simplest wizard            frames and number of words) with AmbiguousList or Noi-
strategies (e.g., always select the top candidate) are clearly      syList returns, W4 asked a question when RecentSuccess ≤
easier to predict, but not necessarily better. (Compare, for        1 or ContiguousWordMatch = 0, and the acoustic model
example, W4 and W6.)                                                score was low. (Short titles are more readily confused.) On
   Recall from Figure 1 that confident choice was more              long ASR strings, W4 asked a question only when Contigu-
common than tentative choice, which was in turn more                ousWordMatch ≤ 1, RecentSuccess ≤ 2, and either the re-
common than question or give up. As a result, the individual        turn was a NoisyList, or Confidence was low and there was
models consistently predicted a confident choice with 0.80 ≤        more than one candidate. In summary, the factors that drove
                                                                    W4 to ask a question include the length of the ASR string,
   Table 3: Learned decision trees model individual wizards.
                                                                    the quality of the ASR transcription, the database return
                                                                    type, the similarity between the ASR string and the candi-
  Tree       Rank       Size    Features   Accuracy     F conf
                                                                    dates, and how well she had performed on recent title cy-
   W4         1          55        12        75.67       0.85
                                                                    cles. These can all be captured by system-internal features.
   W5         2          21        10        76.17       0.85
   W1         3           7         8        80.44       0.87
                                                                                  Discussion and Future Work
   W7         4          45        11        73.62       0.83
   W3         5          33        10        77.42       0.84       As used here, wizard ablation embeds a wizard within an
   W2         6          35        10        78.49       0.85       SDS to study her choices when placed in the same environ-
   W6         7          23        10        85.19       0.80       ment as a machine. Given noisy ASR and the results of a
                                                                1000

database search, the best wizards do not always guess based           Industrial Research in Dialog Technology workshop at
on search return. Instead they sense that the knowledge they          HLT/NAACL 2007, 32-39.
have is a poor fit with what the recognizer “heard.” In that         Bohus, D. and Rudniky, A. I. (2003). RavenClaw: Dialog
case, a good wizard infers that the correct title is not among        Management Using Hierarchical Task Decomposition and
the returned candidates, and asks a thoughtful question to            an Expectation Agenda. Proceedings of Eurospeech 2003,
move the dialogue forward. (The mystery book at the be-               597-600.
ginning of this paper, by the way, was the third title listed.)      Dix, A., Finlay, J., Abowd, G. D. and Beale, R. (2003).
   Experiments like this provide insight into how people              Human-Computer Interaction, Prentice Hall.
match noisy input with returns from database search. The             Forbes-Riley, K. and Litman, D. (2009). Adapting to
experimental design led wizards to prefer the first candidate         Student Uncertainty Improves Tutoring Dialogues.
listed — they read it first, and it was typically correct if the      Proceedings of the 14th International Conference on
return included the correct title. Thus a wizard’s skill at           Artificial Intelligence in Education, AIED, 33-40.
finding the title when it is present is less noteworthy than          Brighton, UK.
W4’s ability to question the relevance of all the candidates.        Franceschetti, D. R., Adcock, A. B. and Graesser, A. C.
   The focus here has been on a single book request by title.         (2003). Analysis of strategies in expert tutoring dialog for
Current work extends this approach to full dialogue. Wiz-             use in Intelligent Tutoring System Development. CogSci
ards will see ASR and query results, and will have a prede-           2003, 1344. Boston, Massachusetts.
fined set of system-actions from which to choose. Dialogue           Hall, M. (1999). Correlation-based Feature Selection for
interactions will include greeting, user identification, and          Machine Learning. Ph.D. Thesis. Department of Computer
four book requests by author and catalogue number, as well            Science University of Waikato.
as by title. In full dialogue, context will have more rele-          Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann,
vance and can be measured more realistically by metrics in            P. and Witten, I. H. (2009). The WEKA Data Mining
addition to RecentSuccess, Analysis of wizards’ questions             Software: An Update. SIGKDD Explorations 11(1).
from this experiment will motivate a pre-defined set of              Levin, E. and Passonneau, R. (2006). A WOZ variant with
questions for wizards in the full dialogue study.                     contrastive conditions. Proceedings of the Interspeech
   This work successfully learned models that predict wizard          Satellite    Workshop,       Dialogue     on     Dialogues:
action primarily from system features. (The only prevalent            Multidisciplinary Evaluation of Speech-Based Interactive
wizard-specific feature was RecentSuccess, which is readily           Systems, 17-21.
replaced by the system’s recent success.) Similar learned            Ohlsson, S., Corrigan-Halpern, A., Di Eugenio, B., Lu, X.
models will be incorporated into CheckItOut. Our next ex-             and Glass, M. (2003). Explanatory Content and Multi-Turn
periment will train models to predict wizards’ actions during         Dialogues in Tutoring. CogSci 2003, 48. Boston,
full dialogue with our baseline version of CheckItOut, and            Massachusetts.
then refine the system with the learned models. We predict           Passonneau, R., Epstein, S. L. and Gordon, J. B. (2009).
that evaluation of the refined, wizard-informed CheckItOut            Help Me Understand You: Addressing the Speech
will provide better performance.                                      Recognition Bottleneck. AAAI Spring Symposium on
                                                                      Agents that Learn from Human Teachers, 119-126. Paolo
                   Acknowledgments                                    Alto, CA.
This research was supported in part by the National Science          Quinlan, J. R. (1993). C4.5: Programs for Machine
Foundation under IIS-084966, IIS-0745369, and IIS-                    Learning, Morgan Kaufmann.
0744904. We thank the staff of the Heiskell Library, the             Ratcliff, J. W. and Metzener, D. (1988). Pattern Matching:
Olympus/Ravenclaw developers at Carnegie Mellon, and                  The Gestalt Approach. Dr. Dobb's Journal 7, 46.
our tireless undergraduate research assistants.                      Raux, A. and Eskenazi, M. (2004). Non-Native Users in the
                                                                      Let's Go!! Spoken Dialogue Systems: Dealing with
                                                                      Linguistic Mismatch. HLT/NAACL, 217-224. Boston, MA.
                        References                                   Rieser, V. and Lemon, O. (2006). Using Machine Learning
Aist, G. S., Allen, J., Campana, E., Gomez Gallo, C.,                 to Explore Human Multimodal Clarification Strategies.
  Stoness, S., Swift, M. and Tanenhaus, M. K. (2007).                 COLING/ACL-06, 659-666. Sidney, Australia.
  Incremental dialogue system faster than and preferred to its       Skantze, G. (2005). Exploring human error recovery
  nonincremental counterpart. CogSci 2007, 779-774.                   strategies: Implications for spoken dialog systems. Speech
  Nashville, Tennessee.                                               Communication 45(3), 325-341.
Bohus, D. (2004). Error Awareness and Recovery in Task-              Stoyanchev, S. and Stent, A. (2009). Predicting Concept
  Oriented Spoken Dialog Systems. Ph.D. Thesis. Computer              Types in User Corrections in Dialog. EACL Workshop
  Science Carnegie Mellon University.                                 SRSL, 42-49.
Bohus, D., Raux, A., Harris, T. K., Eskenazi, M. and                 Witten, I. H. and Frank, E. (2005). Data Mining: Practical
  Rudniky, A. I. (2007). Olympus: an open-source                      Machine Learning Tools and Techniques. San Francisco,
  framework for conversational spoken language interface              Morgan Kaufmann.
  research. Proceedings of Bridging the Gap: Academic and
                                                                 1001

