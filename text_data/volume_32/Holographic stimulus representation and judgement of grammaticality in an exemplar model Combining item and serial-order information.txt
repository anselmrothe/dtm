UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Holographic stimulus representation and judgement of grammaticality in an exemplar
model: Combining item and serial-order information
Permalink
https://escholarship.org/uc/item/0k70v598
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Jamieson, Randall K.
Mewhort, D.J.K.
Publication Date
2010-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

        Holographic stimulus representation and judgement of grammaticality in an
                   exemplar model: Combining item and serial-order information
                                    Randall K. Jamieson (randy_jamieson@umanitoba.ca)
                                           Department of Psychology, University of Manitoba
                                                    Winnipeg, MB, R3T 2N2, Canada
                                            D. J. K. Mewhort (mewhortd@queensu.ca)
                                             Department of Psychology, Queen’s University
                                                    Kingston, ON, K7L 3N6, Canada
                              Abstract                                designed to sort the various predictors have confirmed a role
                                                                      for all of them (e.g., Johnstone & Shanks, 1999). Factorial
   We examine representation assumptions for learning in the
   artificial grammar task. Strings of letters can be represented     designs that have pitted predictors against one another have
   by first building vectors to represent individual letters and      been unable to identify a single dominant predictor (e.g.,
   then concatenating the letter vectors into a vector of larger      Kinder & Lotz, 2009; Vokey & Brooks, 1992).
   dimensionality. Although such a representation works well in         We think that many of the predictors (e.g., ACS, bigram
   selected examples of artificial-grammar learning, it fails in      over, etc) point to a common underlying factor, namely left-
   examples that depend on left-to-right serial information. We       to-right serial structure. If so, the problem is not to
   show that recursive convolution solves the problem by              determine which predictor dominates but, rather, to decide
   combining item and serial-order information in a stimulus          how subjects encode material so that they have access to the
   item into a distributed data structure. We import the
   representations into an established model of human memory.
                                                                      left-to-right serial structure in the exemplars.
   The new scheme succeeds not only in applications that were           In this paper, we explore an encoding mechanism that
   successful using concatenation but also in applications that       folds several orders of left-to-right serial structure in a string
   depend on left-to-right serial organization.                       into a coherent and distributed data structure (i.e., single
                                                                      letters, bi-grams, trigrams, and whole strings). To begin,we
   Keywords: Artificial grammar           learning;  Holographic
   representation; Exemplar model
                                                                      describe the representation scheme. After, we show that the
                                                                      new representations predict judgement of grammaticality
                         Introduction                                 when used in an established theory of retrieval (Jamieson &
                                                                      Mewhort, 2009a, 2010).
In an artificial-grammar learning (AGL) classification task,
participants study strings of symbols. Following study, the           Holographic representation in memory
participants are told that the studied items were constructed
                                                                      Many investigators have proposed that light holography
according to the rules of an artificial grammar and are
                                                                      provides a mathematical basis for memory representation
invited to sort novel rule-based (grammatical) exemplars
                                                                      (Borsellino & Poggio, 1973; Gabor, 1968; Khan, 1998;
from novel rule-violating (ungrammatical) ones. Even
                                                                      Longuet-Higgins, 1968; Poggio, 1973). Murdock’s (1982,
though the participants are unable to describe the rules, they
                                                                      1983, 1997) TODAM is probably the best-known use of the
can discriminate the two classes of stimuli.
                                                                      idea in experimental psychology. In TODAM, stimulus
  Initial accounts proposed that the participants abstracted
                                                                      associations are formed using linear convolution and
the grammar and used that knowledge to judge the status of
                                                                      associations are unpacked using correlation (deconvolution).
the exemplars (e.g., Reber, 1967, 1993). Later investigators
                                                                        More recently, Jones and Mewhort (2007) used recursive
argued that the participants judged grammaticality without
                                                                      circular convolution (Plate, 1995) to develop a self-
reference to the grammar. To support the latter position,
                                                                      organizing model of semantic memory (BEAGLE).
investigators identified several sources of information that
                                                                      BEAGLE captures judgements of semantic typicality,
discriminate the two classes of test strings. Brooks (1978)
                                                                      categorization, priming, and syntax from word order.
suggested that whole-item similarity between training and
                                                                      BEAGLE’s ability to handle so many phenomena of
test strings is used to infer grammaticality. Perruchet and
                                                                      semantic memory is in itself impressive. However, from our
Pacteau (1990) argued that bigram overlap is used to infer
                                                                      perspective, BEAGLE’s strength is that it shows how
grammaticality. Vokey and Brooks (1992) identified edit
                                                                      holographic representation can account for complex
distance as a predictor, and Brooks and Vokey (1991)
                                                                      decision behaviour without adding control structures (e.g.,
argued that patterns of repetition within a string are used to
                                                                      learning and the application of rules). BEAGLE’s success
infer grammaticality. Knowlton and Squire (1996) identified
                                                                      suggests that holographic stimulus representation should be
associative chunk strength (ACS), and Johnstone and
                                                                      explored in related models of learning and memory. The
Shanks (1999) identified chunk novelty. Finally, Jamieson             present work adapts BEAGLE’s representation scheme to
and Mewhort (2009a, 2010) showed that global similarity               represent strings in the artificial grammar classification task.
predicts performance in the task. Regression analyses
                                                                  1541

Recursive circular convolution                                       In models using vector representation, it is traditional to
Circular convolution is a mathematical operation that forms          compute the similarity between x and y, using a vector
an associative representation, z, for two input vectors, x and       cosine. Thus, with concatenated strings, similarity is
y,                                                                   computed by comparing information in corresponding serial
                                                                     positions of two strings (i.e., element-for-element). Because
                                                                     of the serial-position constraint, a model using the
                                        ,     [1]                    concatenated representation scheme treats the strings ABCD
                                                                     and CDAB as if they shared no overlapping features—a
                                                                     judgement that is at odds with data. In contrast, a
                                                                     holographic representation scheme distributes information
where i indexes the element in z and where n is the
                                                                     throughout the vector so that each part of it contains some
dimensionality of z, x, and y. Briefly, circular convolution
                                                                     information about the whole. Thus, in difference to the
forms the outer-product matrix—long used to represent
                                                                     concatenation scheme, the cosine calculation compares all
associations in neural networks (e.g., Anderson, 1995)—and
                                                                     parts of x (i.e., ABCD) and y (i.e., CDAB) simultaneously
then collapses it into a vector (see Figure 1). Circular
                                                                     and, thereby, acknowledges similarity between ABCD and
convolution is associative, commutative, and distributes
                                                                     CDAB. Because participants appreciate the similarity
over addition.
                                                                     between ABCD and CDAB, the holographic scheme is
                                                                     preferred.
                                                                       Critically, holographic stimulus representation finesses the
                                                                     problem of encoding serial structure. Importantly, it does so
                                                                     without requiring a change in the similarity calculation or
                                                                     other aspects of retrieval. This occurs because a
                                                                     representation of ABCD that is formed using recursive
                                                                     circular convolution superimposes overlapping orders of
                                                                     serial structure onto a single distributed structure. Because
                                                                     different orders of serial information about a string are
                                                                     superimposed in a single representation, a standard cosine of
    Figure 1. Collapsing an outer-product matrix with                two vectors supports parallel comparison of multiple orders
    circular convolution, where x and y are the                      of serial structure. The question we pose, then, is if we
    argument vectors and z represents the resulting                  import the holographic representations into an established
    compressed vector from collapsing the outer-                     model of retrieval, will the previously successful model still
    product matrix. The values i and j represent the                 work; that is, can we still explain peoples’ judgements in the
    row and column indices, respectively, for an                     artificial grammar task?
    element in the outer-product matrix.
                                                                                               Minerva 2
  In the work that follows, we apply circular convolution            Minerva 2 is an established model of retrieval (Hintzman,
recursively to encode a series, such as a sequence of letters.       1986, 1988). When a participant studies an item, an event is
Consider the string ABCD. To represent ABCD as a series,             encoded to memory as a unique trace.
first, generate a unique random vector for each of the                  In Minerva 2, a stimulus is represented by a vector of n
individual letters in the string {a, b, c, d}. Next, apply           elements; each element takes values: +1 or -1. To represent
circular convolution in a recursive fashion to bind the first        stimuli in the artificial grammar task, we first, generate a
letter to the second, the product of that binding to the third,      unique random vector for each of the letters in the English
and so on, until each letter has been folded into the                alphabet and then apply recursive circular convolution to
representation. At this point, using z to represent the string       those vectors to represent a string of letters. Thus, a string
ABCD, z = ABCD = ((a * b) * c) * d, where * denotes                  TXXV is represented by a trace: ((t * x) * x) * v.
circular convolution. No matter the length of the string, z             Memory is a matrix, M. Encoding an event involves
has the same dimensionality as the input (i.e., letter) vectors.     copying its corresponding vector representation to a new
                                                                     row in the memory matrix. Encoding is sometimes
Why holographic representation?                                      imperfect. Imperfect encoding is implemented by setting
In previous studies of AGL, we represented exemplars by              some vector elements to zero (indicating that the element is
concatenating letters. For example, a string ABCD was                indeterminate or unknown). A parameter, L, controls the
represented by concatenating the vectors for A, B, C, and D          probability with which an element is stored. As L increases,
to form a single vector a//b//c//d, where // denotes                 encoding quality improves.
concatenation. The scheme captured a swath of data from                 All retrieval is cued. When a retrieval cue is presented, it
the artificial grammar task and from serial reaction time            activates each trace in memory in proportion to its similarity
tasks (see Jamieson & Mewhort, 2009a, 2009b, 2010).                  to the cue. The activated traces are aggregated into a
Nevertheless, concatenated representations are problematic.
                                                                 1542

composite called the echo; the contribution of each trace to           we used concatenated stimulus vectors in that work. In the
the echo is based on its activation.                                   simulations that follow, we retest the model using the
   The similarity of trace, i, to the probe, P, is computed            holographic rather than concatenated stimulus vectors.
using a vector cosine, i.e.,                                              To simulate Reber’s (1967) task we began by representing
                                                                       his stimuli in our model.1 First, we constructed a unique
                                                                       100-element vector to represent each letter used to construct
                                                                       letter strings: {T, V, P, X, S}. Second, we generated a vector
                                     ,        [2]                      for each training and test string using recursive circular
                                                                       convolution. Third, we filled successive rows on the
                                                                       memory matrix with the training vectors. Fourth, we
                                                                       introduced moderate data-degradation to the items in
where Pj is the value of the jth feature in the probe, Mij is the      memory, i.e., L = 0.7. Finally, we calculated the mean echo
value of jth feature of the ith row in memory. Like the                intensity for each of the 24 grammatical and 24
Pearson r, the similarity of the ith item to the probe, Si, is         ungrammatical test strings.
scaled to the interval [-1, +1]. Similarity equals +1 when the            The new model successfully discriminated grammatical
trace is identical to the probe, 0 when the trace is orthogonal        from ungrammatical test items. The mean echo intensity for
to the probe, and -1 when the trace is opposite to the probe.          the 24 grammatical test strings was .57 (SD = .03); the
   The ith trace’s activation, Ai, is the cube of its similarity to    corresponding value for the 24 ungrammatical test strings
the probe, i.e.,                                                       was .49 (SD = .02), t(48) = 2.15, p < .05.
                                                                          In other simulations, we varied the integrity of data in
                                .             [3]                      memory (e.g., Jamieson, Holmes, & Mewhort, in press). As
                                                                       shown in Figure 2, the magnitude of the difference in mean
   The activation function exaggerates differences in                  echo intensity for grammatical and ungrammatical test
similarity between a probe and items in memory by                      strings (i.e., the model’s discrimination of grammatical and
attenuating activation of exemplars that are not highly                ungrammatical items) grew as a function of L.
similar to the probe. This allows traces most similar to the
probe to dominate the information retrieved. Note that the
exponent in the activation function preserves the sign of the
argument, Si.
   The information retrieved by a probe is a vector, c, called
the echo. The echo is computed by weighting each of the i =
1 ... m traces in memory by their activations and, then,
summing all m traces into a single vector,
                                  .           [4]
   The information in the echo is converted to decision
variable called echo intensity, I, by computing the cosine
similarity (see Equation 2) of the echo and probe. In the
context of the artificial grammar task (i.e., classification),
echo intensity is a proxy for judgement of grammaticality.                   Figure 2. Mean echo intensity for grammatical and
   In the remainder of this paper we apply the model to data                 ungrammatical test strings as a function of data
from the judgment of grammaticality task.                                    integrity in memory, L.
Evaluating the model                                                      The simulation illustrates several points. First, it shows
The judgement of grammaticality task was introduced by                 that the distributed stimulus representations generated using
Reber (1967). In his experiment, participants memorized                recursive circular convolution support discrimination of
grammatical exemplars. After, they judged the grammatical              grammatical from ungrammatical test items. Second,
status of novel test probes. Reber’s subjects discriminated            because the model discriminated the two classes of stimuli
novel grammatical from novel ungrammatical test probes,                without reference to grammatical rules, the simulation
but they could not articulate the rules of the grammar.                serves as an existence proof that grammatical strings can be
   We have shown previously that Minerva 2 captures
discrimination of grammatical from ungrammatical test                  1
                                                                         Reber did not list the specific study and test items that he used in
probes in Reber’s (1967) task, without reference to                    his original paper. He did, however, provide a list of representative
grammatical rules (Jamieson & Mewhort, 2009a, 2010). But               strings from the same grammar in another source (Reber, 1993, p.
                                                                       36). We took our strings from there.
                                                                   1543

discriminated from ungrammatical test strings without                items, indicating they were sensitive to whole-item
knowledge of the grammatical rules. Thirdly, the simulation          similarity between training and test strings.
shows that we can import holographic stimulus                          The pattern of results demonstrates that judgement of
representations into Minerva 2 without a deleterious impact          grammaticality is influenced concurrently by the positions
on the effects that the model captures using concatenated            of single letters in a string, by knowledge of small chunks
vectors (see Jamieson & Mewhort, 2009a, 2010).                       (i.e., knowledge of bigrams and trigrams), and by
   Next, we test the new representation scheme by applying           knowledge of larger chunks (i.e., whole training strings). To
it to data collected by Kinder and Lotz (2009). Their data           claim a model as a competent account of decision in the
provide a more detailed challenge.                                   judgement of grammaticality task, the model must
                                                                     accommodate concurrent sensitivity to the three sources of
Kinder and Lotz (2009)                                               information.
Kinder and Lotz (2009) engineered an artificial grammar to
distinguish stimulus properties thought to predict                   Simulation of Kinder and Lotz (2009; Exp 2)
judgements of grammaticality. They used the grammar to               Kinder and Lotz’s (2009) data provide a principled
construct a list of 12 training items and 48 test items. The         challenge to test the idea that holographic stimulus
test items were of four different types. Type 1 and Type 2           representation allows multiple orders of serial-structure to
items were ungrammatical; Type 3 and Type 4 items were               exert a concurrent influence on judgements of
grammatical. Type 1 test items violated both positional and          grammaticality. Hence, we tested our model using Kinder
sequential rules of the grammar; Type 2 items violated only          and Lotz’s (2009) materials.2 The simulation was otherwise
sequential rules (i.e., the strings included at least one illegal    the same as before.
bigram but all letters were in legal serial positions). Type 3         The results of the simulation are presented in Figure 4; the
and Type 4 items obeyed positional and sequential rules of           means were computed across 50 independent replications of
the grammar; but, Type 4 items had the additional property           the procedure. We treat mean echo intensity as a proxy for
of being very similar to a specific training exemplar.               mean judgement of grammaticality.
Accordingly, if participants endorse Type 2 over Type 1
items, they must appreciate the positional dependencies of
letters in the training set. If participants endorse Type 3 over
Type 2 items, they must appreciate the difference between
studied and unstudied chunks (i.e., bigrams and trigrams). If
they endorse Type 4 over Type 3 items, they must
appreciate whole-item similarity between training and test
strings.
                                                                             Figure 4. Simulation: Mean echo intensity for
                                                                             the four item types in Kinder and Lotz’s (2009)
                                                                             Experiment 2.
                                                                       As shown, the model reproduced the pattern of results
                                                                     from Kinder and Lotz’s (2009) experiment. Firstly, mean
                                                                     echo intensity for Type 2 items was greater than for Type 1
                                                                     items indicating that the model was sensitive to positional
        Figure 3. Empirical: Percentage of items
                                                                     dependencies of individual letters in the training strings.
        endorsed as grammatical in Kinder and
                                                                     Secondly, echo intensity for Type 3 items was greater than
        Lotz’s (2009) Experiment 2.
                                                                     for Type 2 items indicating that the model was sensitive to
                                                                     bigram and trigram structure in the stimuli. Finally, echo
  Kinder and Lotz’s (2009) results are reproduced in Figure
                                                                     intensity for Type 4 items was greater than for the Type 3
3. First, participants endorsed Type 2 over Type 1 items
indicating they were sensitive to the positions of individual
letters in the training strings. Second, participants’ endorsed
Type 3 over Type 2 items, indicating they were sensitive to          2
                                                                       A complete listing of Kinder and Lotz’s (2009) materials is
test strings’ inclusion/exclusion of studied and unstudied           presented in their Appendix B. The simulations were identical for
bigrams. Finally, participants endorsed Type 4 over Type 3           the two sets; a testament to their care at stimulus design.
                                                                 1544

items indicating that the model was sensitive to larger           a test string, it judges the test string as grammatical (see
chunks of letters, possibly whole strings.                        Reber, 2002, for an analysis of the approach; see Vokey &
  Importing a scheme for holographic stimulus                     Higham, 2004, for model comparison of the SRN and a
representation into a Minerva 2-based account of retrieval        related instance-based model). Cleeremans et al. (1989)
allows the model to capture additional details of                 showed the SRN develops a veridical representation of the
performance in the artificial grammar task. Minerva 2 now         grammar used to generate the training strings. By contrast,
captures trends that previously required a very different kind    our account treats judgement of grammaticality as an
of computational account (e.g., the SRN, see Elman, 1990).        episodic memory task. At study, the model encodes
                                                                  information about individual exemplars, including serial
                   General Discussion                             structure. At test, the model judges a test strings’
Judgements of grammaticality reflect a concurrent                 grammaticality by its global similarity to the exemplars in
consideration of discriminative cues (e.g., Johnstone &           memory. The two classes of model (Minerva 2 and the
Shanks, 1999). To accommodate that fact, we developed a           SRN) offer very different explanations of the cognitive
new kind of stimulus representation based on recursive            processes that underlie judgement of grammaticality. So,
circular convolution. The new representation folds                which approach is to be preferred? We think the answer
information about several cues into a distributed data            should be based on the nature of the experimental problem.
structure. More importantly, the holographic representation          In the training phase of a standard artificial grammar
scheme supports parallel comparison of features in a string,      experiment, participants are asked to memorize exemplars.
unconstrained by serial position alignment. Using the             At test, they are given the problem of inferring the
holographic representations in a model of human memory            grammaticality of test probes. Of course, people can learn
captures judgement of grammaticality.                             sequential structure in stimuli instructions. But they do not
  In previous work, Jamieson and Mewhort (2009a, 2010)            have to learn it: the task does not cue them to do so. In our
demonstrated that judgment of grammaticality can be               view, although learning sequential structure in a set of
understood using Minerva 2—an exemplar model of                   exemplars provides a possible mechanism, for the
memory. In that work, exemplars were represented by               judgement of grammaticality task, it implies compulsory
concatenating individual letter vectors. Judgement of             learning of sequential regularities even though that action is
grammaticality reflected a test probe’s global similarity to      neither implied by nor cued by the task. Unlike the SRN,
the studied exemplars. The representation scheme worked           Minerva 2 assumes people notice sequential characteristics
because it preserved the spatial structure of the stimulus        of each exemplar, but they do not learn the regularities in
(i.e., letters from left-to-right). However, the account          the set of exemplars. Moreover, because our account treats
neglected to include information about left-right sequential      judgement of grammaticality as a retrospective judgement,
properties of the exemplars—information that subjects             it is not necessary to justify or to describe prospective
notice during study. Because the model did not                    abstraction of structure in the training set.
acknowledge sequential structure in stimuli, it incorrectly         In developing our holographic representation scheme, we
computed similarity between two exemplars based on                have been careful to avoid altering our model’s assumptions
bigram overlap; a factor measured by associative chunk            about retrieval. In both our original and our present
strength.                                                         accounts, we assumed a perceptual system loads memory
  The holographic stimulus representations developed here         with what the subjects notice about each of the studied
finesse the problem associated with the earlier scheme by         exemplars. Judgment of grammaticality reflects the global
folding information about serial-structure into the               similarity of a test probe to training items. The difference in
representation of a string. By using the holographic              our new account is that the new model assumes that subjects
representations, the model now captures judgements that           notice more about the order of the symbols than the old
reflect serial structure (e.g., participants’ appreciation of     model assumed; a claim echoed in post-experimental
chunk overlap). Despite changes to the representation             interviews with our subjects. At a broader level, our solution
scheme in the model, we have not changed the model’s              honours an insight from Simon’s (1969) parable of the ant.
account of retrieval and so we retain our previous                Simon noted that an ant’s path on a beach may be complex
conclusion: Judgement of grammaticality can be captured           and difficult to describe. But, the complexity of the path
without an implicit rule-induction process that abstracts and     may be driven by complexity in the beach rather than
applies grammatical information.                                  complexity in the ant. Simon used the parable to goad
  Kinder (2000; Kinder & Lotz, 2009) and others (e.g.,            theorists into considering explanations for a behaviour based
Cleeremans, Servan-Schreiber, & McClelland, 1989) have            on the complexity of the environment before assuming that
argued for a Simple Recurrent Network (SRN) account of            the behaviour reflects complex psychological mechanisms.
artificial grammar learning. The SRN accomplishes                 Here, we have followed Simon’s advice. Peoples’ behaviour
judgement of grammaticality by learning the sequential            in the artificial grammar task appears complex and difficult
structure in a set of training sequences and then applying        to describe. However, the complexity is in the materials, not
that knowledge to predict sequential regularities in test         in the subjects. Judgement of grammaticality reflects the
items. When the SRN can predict the sequential structure of       storage and retrieval of studied exemplars.
                                                              1545

                   Acknowledgments                                 Experimental Psychology: Learning, Memory, and
                                                                   Cognition, 25, 524-531.
R. K. Jamieson, Psychology, University of Manitoba; D. J.
                                                                 Jones, M. N., & Mewhort, D. J. K. (2007). Representing
K. Mewhort, Psychology, Queen's University. The research
                                                                   word meaning and order information in a holographic
was supported by grants to both authors from the Natural
                                                                   lexicon. Psychological Review, 114, 1-37.
Sciences and Engineering Research Council of Canada.
                                                                 Khan, J. I. (1998). Characteristics of multidimensional
Mail correspondence to R. K. Jamieson, Department of
                                                                   holographic associative memory in retrieval with
Psychology, University of Manitoba, Winnipeg, MB,
                                                                   dynamically localizable attention. IEEE Transactions on
Canada,      R3T      2N2.          Electronic    mail    to:
                                                                   Neural Networks, 9, 389-406.
randy_jamieson@umanitoba.ca.
                                                                 Kinder, A. (2000). The knowledge acquired during artificial
                                                                   grammar learning: Testing the predictions of the two
                        References                                 connectionist models. Psychological Research, 63, 95-
Anderson, J. A. (1995). An introduction to neural networks.        105.
  Cambridge, MA: MIT Press.                                      Kinder, A., & Lotz, A. (2009). Connectionist models of
Borsellino, A., & Poggio, T. (1973). Convolution and               artificial grammar learning: What type of knowledge is
  correlation algebra. Kybernetik, 122, 113-122.                   acquired? Psychological Research, 73, 659-673.
Brooks, L. R., & Vokey, J. R. (1991). Abstract analogies         Knowlton, B. J. & Squire, L. R. (1996). Artificial grammar
  and abstracted grammars: Comments on Reber (1989) and            learning depends on implicit acquisition of both abstract
  Mathews et al. (1989). Journal of Experimental                   and exemplar-specific information. Journal of
  Psychology: General, 120, 316-320.                               Experimental Psychology: Learning, Memory, and
Cleeremans, A., Servan-Schreiber, D., & McClelland, J. L.          Cognition, 22, 169-181.
  (1989). Finite state automata and simple recurrent             Longuet-Higgins, H. C. (1968). Holographic model of
  networks. Neural Computation, 1, 372-381.                        temporal recall. Nature, 217, 104.
Dienes, Z. (1993). Computational models of implicit              Murdock, B. B. (1982). A theory for the storage and
  learning. In D. C. Berry & Z. Dienes (Eds.), Implicit            retrieval of item and associative information.
  learning: Theoretical and empirical issues (pp. 81-112).         Psychological Review, 89, 609-626.
  Hove, UK: Lawrence Erlbaum Associates.                         Murdock, B. B. (1983). A distributed model for serial-order
Elman, J. L. (1990). Finding structure in time. Cognitive          information. Psychological Review, 90, 316-338.
  Science, 14, 179-211.                                          Murdock, B. B. (1997). Context and mediators in a theory
Gabor, D. (1968). Improved holographic model of temporal           of distributed associative memory (TODAM2).
  recall. Nature, 217, 1288-1289.                                  Psychological Review, 104, 839-862.
Hintzman, D. L. (1986). “Schema abstraction” in a multiple-      Plate, T. A. (1995). Holographic reduced representations.
  trace memory model. Psychological Review, 93, 411-428.           IEEE Transactions on Neural Networks, 6, 623-641.
Hintzman, D. L. (1988). Judgements of frequency and              Perruchet, P., & Pacteau, C. (1990). Synthetic grammar
  recognition memory in a multiple-trace memory model.             learning: Implicit rule abstraction or explicit fragmentary
  Psychological Review, 95, 528-551.                               knowledge? Journal of Experimental Psychology:
Jamieson, R. K., Holmes, S., & Mewhort, D. J. K. (2010).           General, 119, 264-275.
  Global similarity predicts dissociation of classification      Poggio, T. (1973). On holographic models of memory.
  and      recognition:     Evidence      questioning    the       Kybernetik, 12, 237-238.
  implicit/explicit learning distinction in amnesia. Journal     Reber, A. S. (1967). Implicit learning of artificial grammars.
  of Experimental Psychology: Learning Memory and                  Journal of Verbal Learning & Verbal Behavior, 6, 855-
  Cognition, in press.                                             863.
Jamieson, R. K., & Mewhort, D. J. K. (2010). Applying an         Reber, P. J. (2002). Attempting to model dissociations of
  exemplar model to the artificial grammar task: String-           memory. Trends in Cognitive Sciences, 6, 192-194.
  completion and performance on individual items.                Reber, A. S. (1993). Implicit learning and tacit knowledge:
  Quarterly Journal of Experimental Psychology, 63, 1014-          An essay on the cognitive unconscious. New York:
  1039 .                                                           Oxford University Press.
Jamieson, R. K., & Mewhort, D. J. K. (2009a). Applying an        Simon, H. A. (1969). The psychology of thinking:
  exemplar model to the artificial-grammar task: Inferring         Embedding artifice in nature (Chapter 2). In Sciences of
  grammaticality from similarity. Quarterly Journal of             the artificial (pp. 23-26). MIT Press.
  Experimental Psychology, 62, 550-575.                          Vokey, J. R., & Brooks, L. R. (1992). Salience of item
Jamieson, R. J., & Mewhort, D. J. K. (2009b) Applying an           knowledge in learning artificial grammars. Journal of
  exemplar model to the serial reaction-time task:                 Experimental Psychology: Learning, Memory, and
  Anticipating from experience. Quarterly Journal of               Cognition, 20, 1504-1510.
  Experimental Psychology, 62, 1757-1783.                        Vokey, J. R., & Higham, P. A. (2004). Opposition logic and
Johnstone, T., & Shanks, D. R. (1999). Two mechanisms in           neural network models in artificial grammar learning.
  implicit artificial grammar learning?        Comment on          Consciousness and Cognition, 13, 565-578.
  Meulemans and van der Linden (1997). Journal of
                                                             1546

