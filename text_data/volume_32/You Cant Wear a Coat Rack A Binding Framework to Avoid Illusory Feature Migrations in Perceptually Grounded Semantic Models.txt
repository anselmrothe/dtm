UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
You Can’t Wear a Coat Rack: A Binding Framework to Avoid Illusory Feature Migrations in
Perceptually Grounded Semantic Models
Permalink
https://escholarship.org/uc/item/9k61f8m2
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Jones, Michael
Recchia, Gabriel
Publication Date
2010-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

       You Can’t Wear a Coat Rack: A Binding Framework to Avoid Illusory Feature
                         Migrations in Perceptually Grounded Semantic Models
        Michael N. Jones (jonesmn@indiana.edu)                                 Gabriel Recchia (grecchia@indiana.edu)
        Department of Psychological and Brain Sciences                           Cognitive Science Program, 1910 E 10th St.
         Indiana University, Bloomington, Indiana USA                          Indiana University, Bloomington, Indiana USA
                              Abstract                                  with, and position relative to, other words (e.g., Jones &
   Recent Semantic Space Models (SSMs) are now integrating
                                                                        Mewhort, 2007). Further, these models are able to retrieve
   perceptual information with linguistic statistics into a unified     plausible n-gram information (coarse grammaticality)
   mental space, offering a solution to the criticism that SSMs         directly from the blended space, without the need for
   are disembodied. However, these new models introduce the             explicit rules of grammaticality. The integration of word
   problem of illusory feature migrations. When the word dog is         order information has been shown to give a much better fit
   perceived, its perceptual features should migrate to hyena, so       to human data in a variety of semantic tasks.
   the system can infer the perceptual features for a non-                 Secondly, SSMs have been criticized as “disembodied” in
   perceived word (hyenas have fur). In doing so, however, the
   models are unable to avoid migrating the features for dog to         that they learn from only linguistic information but are not
   syntagmatically related words, such as bone. As a result, the        grounded in perception and action (see de Vega, Graesser,
   models incorrectly infer that bones have fur. We argue that          & Glenberg, 2008 for a workshop on the issue). The lack of
   the problems of perceptual grounding and word order are not          grounding in SSMs is in direct contrast to the recent
   independent—a model of word order information is needed to           literature on embodied cognition, demonstrating that a
   correctly infer how features should migrate in mental space.         word’s meaning is grounded in sensorimotor experience.
   We introduce a multiplicative binding framework that allows
                                                                        Sensorimotor information is an inherent part of the semantic
   all information sources to be stored in a composite mental
   space, but features will only migrate to words that share            organization of the human lexicon, but much of this
   sufficient order information with directly perceived words.          information cannot be learned from statistics in a text
                                                                        corpus—it must be learned from multisensory experience
   Keywords: semantic space models, symbol grounding
                                                                        (but see Riordan & Jones, in press). In addition, current
   problem, perceptual integration, embodied cognition.
                                                                        models have a symbol-reference problem: there is no way to
                                                                        link a word’s internal representation back to its referent in
                          Introduction
                                                                        the real world.
Semantic Space Models (SSMs) have seen remarkable                          We are now seeing the emergence of the first perceptually
success in recent years as models of how humans learn the               grounded SSMs. As a proxy for sensorimotor experience,
meanings of words from repeated episodic experience, and                these models use norms of human-generated features (such
for how lexical semantics are represented in mental space.              as the norms of McRae et al., 2005). These norms represent
Many types of SSMs now exist, with several modifications                aggregate human productions of the physical properties,
to better approximate human semantic cognition.1 In                     appearance, sounds, smells, functional properties, etc. for
general, these models all create semantic representations               concrete nouns and event verbs based on multisensory
from statistical regularities in large linguistic corpora,              experience. For example, the feature <has_4_legs> will
building on Harris’ (1970) distributional hypothesis of                 have a high probability for dog and cow, but a low
lexical semantics: the more similar the contexts in which               probability for centipede, and a zero probability for
words are experienced, the more similar their meanings.                 strawberry. However <is_red> is a highly salient feature of
SSMs have successfully accounted for a wide variety of                  strawberry and not for dog.
human semantic data, ranging from semantic priming and                     Most of the new grounded SSMs simultaneously consider
free association, up to high-level discourse processing by              the distribution of words across contexts in a text corpus and
applying compositional algorithms to SSM representations.               the distribution of words across perceptual features,
   Despite their successes, SSMs have been heavily                      allowing them to extract joint information between the two
criticized as implausible psychological models on a number              data sources. This allows the models to make implicit
of grounds. Firstly, most of these models have been                     inferences across the two information sources: if the model
criticized as “bag-of-words” models, in that they simply                learns from perceptual experience that sparrows have beaks,
consider the context in which the word occurs, but ignore               and from linguistic experience that sparrows and
the statistical information inherent in word transitions.               mockingbirds are used in a similar distributional fashion, it
Recent solutions to the word-order problem use binding                  naturally makes the inference that mockingbirds have beaks.
operations to learn a blended semantic space in which a                 The inference chain works in the opposite direction as well.
word’s representations reflects its history of co-occurrence            Most impressive, given a novel word most of these models
                                                                        can retrieve an accurate representation of the perceptual
   1
     For recent advances in SSMs, see the upcoming issue of             features of the novel word’s referent. Simulations have
Topics in Cognitive Science edited by Danielle McNamara.
                                                                    877

demonstrated that the blended linguistic/perceptual mental         be activated but the output feature for <made_of_metal>
space may yield a superior approximation of human data.            should be inhibited. After iterative training with backprop,
   However, a major issue common to all of these new               the model can infer the correct pattern of perceptual
grounded models is that they have no way to discriminate           properties for words that did not have a perceptual feature
between syntagmatic relationships (e.g., the relationship          vector. At its core, this technique simply maps similar
between bee and honey) and category-based paradigmatic             linguistic vectors to similar output vectors, as with other
relationships (e.g., bee and wasp). The linguistic abstraction     pattern generalization applications of feedforward networks.
phase of these models will learn to position the vectors for          Ad-hoc inference models typically begin with a raw word-
car, automobile, and road close in semantic space. This            by-document matrix of a text corpus and a word-by-feature
produces the problem that the model cannot distinguish             matrix of a feature database. During learning, the model
which regions of space may adopt features that migrate from        attempts to learn a word’s representation by simultaneously
a perceptually grounded word during the feature inference          considering inference across documents and features. An
phase. The result is that the model correctly infers that          excellent example of an ad-hoc model is presented in
automobile <has_wheels>, but it also incorrectly infers that       Andrews, Vigliocco, and Vinson (2009). Andrews et al., use
road <has_wheels>. We refer to these errors as illusory            a Bayesian framework to infer the joint distributional
feature migrations, and argue that errors of migration are         information for a word between linguistic and perceptual
much more common in semantic space than are correct                data. It is important to note that their technique is joint
migrations, which can severely pollute the resulting               inference: it squeezes more information out of the data than
semantic space relative to a human representation that             simply adding perception to linguistic experience. Andrews
would not contain this type of error.                              et al. convincingly demonstrate that their joint model gives
   One reason these models fail to discriminate between            better fits to word association data than a model that
context-based syntagmatic vs. category-based paradigmatic          considers only one data source, or the simple addition of the
relationships is that they ignore word order information,          two sources.
which is a powerful cue for category membership (Jones &
Mewhort, 2007). That is, words that are flanked by similar         Illusory Feature Migrations
n-grams tend to belong to the same conceptual categories.          A major problem with both post-hoc and ad-hoc inference
Extensive study in the field of category-based inference has       models is that they must exhibit illusory feature migrations
investigated the ways in which category structure constrains       as a consequence of their architecture. An illusory feature
feature generalization (for reviews, see Heit, 2000; Rips,         migration occurs when a non-perceived word adopts
2001). To ignore word information is to ignore a very              erroneous features from a linguistically related word simply
salient cue to category membership at an SSM’s disposal.           because they are proximal in semantic space. This is a
   To be clear at the outset, we strongly commend the              common issue in the aforementioned models because they
authors of these perceptually grounded models for taking a         do not have order information to discern between
huge step in the right direction towards our understanding of      syntagmatic and paradigmatic word relations. If the models
human semantic representation. However, a plausible model          are optimized on free-association data (which is strongly
must also be able to filter components of this representation      dominated by syntagmatic productions), then they must
so that perceptual information may generalize to                   position syntagmatically related words like bee and honey
paradigmatically but not syntagmatically similar words (i.e.,      close in space, as well as paradigmatically related words
from car to automobile but not road). Here we explore the          like bee and wasp. As a result, the inference mechanism
utility of a formal binding framework based on ideas from          simply sees both honey and wasp as similar patterns to bee,
signal processing and Jones and Mewhort’s (2007)                   and naturally makes the inference that honey can fly and has
BEAGLE model that has these desiderata.                            wings.
                                                                      Note that the “migration” described need not be a
Grounding Semantics in Perception and Action                       dichotomous on/off feature. It is simply the case that the
Recent attempts to ground SSMs in perception and action            inferred distribution over possible features for honey has
can be placed into one of two classes: post-hoc inference          some correlation with that of bee simply because their
models, and ad-hoc inference models. Both types can be             distributional structure in language has overlap. This
trained on the same text corpus and feature representations        overlap introduces error in the labeling of novel referents
(e.g., TASA and McRae et al., 2005).                               (e.g., a novel object that looks like an insect will activate
   Post-hoc inference models begin with the abstraction of a       words like honey as potential labels). Furthermore, this
text corpus into a reduced vector space (a traditional SSM),       inference error will introduce noise to the overall semantic
and then attempt to bind these linguistic vector                   organization, which will lead to a poorer account of human
representations to the feature norms. For example, Durda,          semantic data compared to a human who will not make
Buchanan, and Caron (2009) train a feedforward neural              these inference errors. The aforementioned models
network to associate linguistic vectors with their                 demonstrated examples of correct feature generalizations in
corresponding activation of features. Given the linguistic         their papers; what was not illustrated is the larger number of
representation for dog, the output feature <has_fur> should        incorrect feature generalizations.
                                                               878

   Presumably, humans use word-order information to                 that vector symbolic architectures employ to combine
constrain the inference of features in mental space. This           vectors in a neurally plausible manner (Levy & Gayler,
information allows a model to distinguish what types of             2009; Kanerva, 2009). CI, OI, and FI are indicator vectors—
words may adopt features given a perceived target word.             unchanging vectors that are bound with vectors representing
Rather than making this a terse rule-based model, we choose         context, order, and feature information, respectively. They
to adopt a graded feature migration framework—words                 serve to “tag” the source of the information signal (context,
adopt the aggregate features of proximal words that have            order, or perception). They may be initialized either as
features, weighted by their similarity in order space.              random vectors, or as binary vectors of ones and zeros
However, it is also important to keep the sources (context,         sharing little or no overlap with each other.
order, perception) blended to account for the wide range of            As in Jones & Mewhort (2007) and Sahlgren et al. (2008),
embodied semantic data. This requires a model that can              the context vector represents co-occurrence information: it is
create a blended semantic representation, but that can know         the sum of all environmental vectors of words occurring in
what part of the semantic signal to use in computing                the same sentence as w. The order vector is the sum of all n-
similarity for feature migration. We next describe a simple         grams surrounding w up to some fixed window size, where
framework towards this type of integrated model, test its           an n-gram is represented by binding the environmental
behavior on an artificial language paradigm, and then scale         vectors of all the words comprising the n-gram via
it up to a real language corpus to see how the properties are       elementwise multiplication. In the experiments presented
maintained at a large scale.                                        here, only bigrams directly to the left and right of w are
                                                                    considered. As in Sahlgren et al. (2008), words to the right
            A Feature-Binding Framework                             and left are distinguished by rotating the environmental
Our goal was to build an SSM with two key properties.               vectors by one unit in a positive or negative direction,
First, context, order, and feature information should be            respectively. Finally, the features vector represents
represented as patterns in high-dimensional vectors. Even           information about sensorimotor features of words. Each of
though these three sources of information should be blended         2,526 features taken from the feature norms of McRae, et al.
within a single vector, it should be possible to determine the      (2005) was assigned a unique random vector. If w is the
degree of similarity between two words in context space,            word for one of the 541 concepts for which feature norms
order space, or feature space alone. Because context, order,        were collected, featuresw is the sum of the five vectors that
and feature information is distributed, computing a vector          correspond to the five features that were attributed to w by
cosine between two vectors reflects their similarity when all       the greatest number of participants. If w is not among the
three sources of information are taken into account.                concepts in the McRae et al. feature norms, featuresw is
   Second, feature migration should occur, but features             initialized as a vector of zeroes (and only acquires nonzero
should only migrate to words with which they share order            values during training, when vectors are added to mw via the
information (i.e., words that are commonly flanked by               update rule). The fact that featuresw has a w subscript while
similar n-grams). For example, food and table will share            context and order do not reflects the fact that featuresw is
primarily context information, whereas table and countertop         derived from information about w in the feature norms,
will share primarily order information; therefore, features         while context and order represent information about the
should migrate from table to countertop, but not from table         sentence currently being processed.
to food.
                                                                    Retrieval. After training, the cosine between every pair of
Encoding. Our model is similar to other SSMs that represent         memory vectors is calculated to determine the model’s
both context and order information with fixed-length high-          estimate of the semantic similarity between words. These
dimensional vectors (Jones & Mewhort, 2007; Sahlgren et             similarity scores can be thought of as distances between
al., 2008). When a word w is encountered in the input text          points in a high-dimensional space, which we refer to as the
for the first time, it is assigned an initial “environmental”       composite space. In addition to having a lower
vector ew—a random vector whose elements are randomly               computational complexity than circular convolution, one
selected from a Gaussian distribution of mean 0 and                 benefit of using elementwise vector multiplication for
variance 1. Environmental vectors are intended to represent         binding the information source tag is that the operation
the static properties of a word’s surface form, such as its         serves as its own approximate inverse when vector elements
orthography and phonology, and are not updated during               are sampled from a z-distribution, hence:
processing. The new word is also assigned an initially                                 X ≈ (X ⊙ Y) ⊙ Y                     (1)
empty memory vector mw to represent its semantics. When
the model encounters a new sentence in the input corpus, mw         This allows vectors to be elementwise multiplied with the
is modified according to the update rule:                           aforementioned context indicator vector CI before
   mw = mw + (CI ⊙context) + (OI⊙order) + (FI⊙featuresw)            calculating their cosines. The operation serves to ‘unbind’
                                                                    the CI * context binding, yielding a context space in which
where the circumpunct “⊙” denotes elementwise vector                two words’ distance from each other reflects the amount of
multiplication, one of a class of multiplication-like operators     context information they share (but is not heavily influenced
                                                                879

by shared order or feature information).           Similarly,      retrained the model with the full update rule mw = mw + (CI
unbinding via elementwise multiplication with OI yields an         ⊙ context) + (OI ⊙ order) + (FI ⊙ featuresw), adding five
order space in which cosine similarity reflects the amount         vectors corresponding to five features for the word
of shared order information; unbinding with FI yields a            “strawberry” from the McRae et al. norms to the concept for
feature space where feature information is paramount.              the word A (a_fruit, grows_on_plants, grows_in_fields,
                                                                   grows_on_bushes, and has_green_leaves). We compared
                      Experiment 1                                 the model under three conditions: context, composite, and
The objective of Experiment 1 was to determine whether the         order. In each condition, feature migration proceeded by
binding model we outlined does in fact possess the desired         unbinding mw ⊙ FI to retrieve an approximation featuresw′
property of representing context, order, and feature               of featuresw, and adding this approximation to every other
information in a separable fashion, and whether it behaves         memory vector mi in proportion to the strength of their
appropriately with respect to feature migration.                   similarity in the relevant space (context €space, composite
Demonstrating this required training the model on a corpus         space, or order space, depending on condition). That is,
in which the amount of context, order, and feature                 features tend to be more likely to migrate in the order
information that words share is known, which is best               condition between two words that share a large amount of
accomplished using a corpus of an artificial language.             order information than between two words that do not.
Strictly controlling the input allows us to determine              Because we are interested in migrating features not merely
conclusively whether the model at least exhibits the desired       to words that are “close” to the perceived word but rather to
properties in the simplest case and lets us more clearly           words that are similar to w in terms of their relationships to
observe how the inclusion or exclusion of different types of       other words, the similarity between words w1 and w2 is
information affects the similarity space.                          obtained by correlating a vector of w1’s cosine with each
                                                                   word in the lexicon with a vector of w2’s cosine with each
Method                                                             word in the lexicon. However, using just the cosine of w1
Input corpus. The model was trained on a corpus of 1,000           and w2 yields largely similar results.
sentences from a simple artificial language. This language
was designed such that it would contain some word pairs            Simulation 1.1. Tables 1 and 2 illustrate the most similar
that shared context information but not order information,         words to A, B, Cs, D, E, Fs, can, and two in context and
some pairs that shared order information but not context           order space, respectively, after training using the update rule
information, and some words that shared context as well as         mw = mw + (CI ⊙ context) + (OI ⊙ order); no feature
order information. The language used is described by the           information was included in this simulation. In the absence
following context-free grammar (symbols in bold are                of feature information, context and order information are
terminal symbols):                                                 separable in this model, despite the fact that both
                                                                   information sources are fully distributed across vector
     S → A Aux B Num Cs | D Aux E Num Fs
                                                                   elements. Appropriately, the members of {A, B, Cs} cluster
     Aux → can | should | would | could | does
                                                                   together in context space, as do the members of {D, E, Fs}.
     Num → two | three | four | five | six
                                                                   Additionally, pairs {A, D}, {B, E}, and {Cs, Fs} cluster
  Sentences of the corpus were generated randomly, with            together in order space. Although they do not appear in the
each possible transition of equal probability. Thus, it            tables, auxiliaries and numbers also cluster together.
consisted of sentences such as “A can B three Cs”, “A
would B four Cs”, “D should E three Fs”, and so forth. In          Table 1. Z-scores of cosines of the most similar words to A,
this corpus, A, B, and Cs each share context information, as       B, Cs, and D in context space, Simulation 1.
they always co-occur, but they do not share order
information. If this were a real language, one could think of              A               B               Cs              D
                                                                     A      3.6     B        3.6   Cs         3.6   D        3.6
A, B, and Cs as fillers for three different grammatical roles.       B      .20     Cs       .20   B          .21   E        .18
Similarly, D, E, and Fs share context, but not order,                Cs     .16     A        .16   A          .13   Fs       .15
information. In contrast, the members of pairs {A, D}, {B,           five   ‐.08    two      .01   two        ‐.07  three    ‐.06
E}, and {Cs, Fs} each share order information, but                   two    ‐.09    five     ‐.01  five       ‐.09  could    ‐.09
significantly less context information. The auxiliary verbs
{can, should, would, could, does} and numbers {two, three,
four, five, six} share significant amounts of order                Table 2. Z-scores of cosines of the most similar words to A,
information with each other. They also share context               B, Cs, and D in order space, Simulation 1.
information: even though the grammar allows auxiliaries
and numbers to co-occur with any of A, B, Cs, D, E, or Fs,                A              B                Cs              D
                                                                     A      3.5     B        3.7   Cs         3.5   D        3.5
each auxiliary always co-occurs with some number.
                                                                     D      1.2     E        .32   Fs         1.1   A        1.2
                                                                     B      ‐.10    A        ‐.03  B          ‐.15  Fs       ‐.14
Procedure. Two simulations were conducted. In Simulation             Cs     ‐.13    Cs       ‐.04  A          ‐.17  E        ‐.17
1, no feature information was included. In Simulation 2, we          can    ‐.24    can      ‐.22  can        ‐.24  can      ‐.30
                                                               880

Simulation 1.2. Table 3 illustrates the standardized               of the pair (the target) were members of the McRae et al.
correlations of vector cosines of the four most similar words      feature norms2. For each pair, we determined the category
to A under each migration condition. Because the migration         membership of each word, using the categories employed by
rule transfers feature information in direct proportion to         Cree & McRae (2003, Appendix B): weapons, vehicles,
these values, the higher the value of a word, the more             foods, and so forth. Cree & McRae explicitly list which
feature information that word receives from A. The                 normed words belong in which categories, allowing us to
important pattern in Table 3 is the reversal of B and D: in        code whether the cue was a member of the same conceptual
context space, the syntagmatic relation between A and B is         category as the target. The 690 pairs in which both words
much more salient, but in the order space the paradigmatic         shared a category were interpreted as being paradigmatically
relation between A and D is emphasized. In the overall             related (e.g., apple-pear), while the 385 paired words not
composite space, these relations are mixed (our desired            sharing a category were interpreted as being syntagmatically
blending in full lexical space), but the information required      related (e.g., apple-crab). The fact that two words are
for correct feature migration is still implicitly represented.     associates and do not appear in the same category does not
                                                                   guarantee syntagmatic similarity nor does it preclude
Table 3. Standardized correlations of vector cosines of the        phrasal association, however, informal observation suggests
four most similar words to A under the context, composite          that many word pairs in the latter condition tend to appear in
and order conditions, Simulation 2.                                collocations or other classic syntagmatic relationships for
                                                                   which feature migration would be inappropriate. Indeed, the
            context         composite           order              cosine similarity scores from the McRae et al. (2005)
        A         3.5    A          3.2     A        3.4
                                                                   feature vectors for the word pairs were significantly higher
        B         .63    B          .06     D        1.2
        Cs        .55    D          .04     B        .05           for our paradigmatically related words than for
        does      ‐.17   Cs         .00     Cs       ‐.03          syntagmatically related ones, t(1073) = 24.66, p < .001.
                                                                     Motivated by the results of Experiment 1, we predicted
   Thus, it appears that only the order condition minimizes        that words sharing paradigmatic relationships would be
opportunity for illusory feature migrations while preserving       closer in order space than in context space. This pattern of
the appropriate migration to D, which is paradigmatically          results would suggest that attending to order information
similar to A in this corpus. Furthermore, when feature             facilitates more feature migrations among paradigmatically
information is added, the separability between context and         related words than among syntagmatically related ones,
order space is maintained, (allowing features to                   while attending to context information does just the
appropriately migrate from A to D) and individual features         opposite. For paradigmatically related words, the model’s
can be successfully retrieved.                                     cosine similarities were significantly higher in order space
                                                                   than in context space, t(689) = 2.96, p < .01. That is, words
                       Experiment 2                                in paradigmatically related pairs were gauged to be more
                                                                   similar to each other in order space than in context space. In
The objective of Experiment 2 was to explore whether the
                                                                   contrast, for syntagmatically related pairs, the model’s
proposed binding framework continues to yield distributions
                                                                   cosine similarities were significantly higher in context space
that inhibit illusory feature migrations (i.e., migrations to
                                                                   than in order space, t(384) = 4.371, p < .001.
syntagmatically similar words) while facilitating appropriate
feature migrations to paradigmatically similar words when
                                                                   Simulation 2.2. To briefly demonstrate how illusory feature
scaled up to a corpus of natural language. We therefore
                                                                   migrations may be corrected by incorporating order
designed a version of Experiment 1 trained on a real corpus,
                                                                   information, we selected 25 “triples” from Simulation 1,
the TASA corpus of high-school level English text. Two
                                                                   each consisting of a target T that existed in the McRae et al.
simulations were conducted: The first to examine the
                                                                   norms, a category coordinate CC of T, and a
similarity of the decoded context and order spaces to
                                                                   syntagmatically related word R that had an associative
paradigmatic and syntagmatic relations, and the second to
                                                                   relationship with T but was not a member of the same
demonstrate feature migrations to category co-ordinates vs.
                                                                   category. An example triple is <T:freezer, CC: refrigerator,
non-categorical associates of a target word. Both
                                                                   R:ice>. Freezer and refrigerator each share a common class
simulations were identical to Experiment 1’s Simulation 2
                                                                   (kitchen appliances); freezer and ice are certainly related as
in terms of the update rule, the conditions (context, order
                                                                   well, but not by virtue of a category relationship. Intuitively,
and composite), and the feature migration rule.
                                                                   one would like features to migrate more strongly from T to
                                                                   CC than from T to R, given that categories for concrete
Simulation 2.1. For each word, its feature vector featuresw
                                                                   words are defined at least partly on the basis of feature
was generated by summing the five vectors corresponding
                                                                   overlap. For example, the most popular features of freezer
to the five features from the McRae et al. norms attributed
                                                                   are used_for_storage, and has_an_inside, features that are
to w by the greatest number of participants. As test items,
we extracted 1075 word pairs from the word association               2
norms of Nelson, McEvoy, & Schreiber (1998) for which                  We excluded the 24 concept words that the McRae et al. norms
                                                                   explicitly identify as having ambiguous meanings, such as
both the first word of the pair (the cue) and the second word
                                                                   “mouse_(animal)” and “mouse_(computer).”
                                                               881

much more applicable to kitchen appliances than they are to                                      General Discussion
related non-category members (ice, frozen waffles, etc.). If
                                                                          Integration of sensorimotor information is an important next
a particular feature migrated more strongly from T to R than
                                                                          step in the development of SSMs. While human-generated
from T to CC, this was coded as an illusory feature
                                                                          feature norms are admittedly an intermediary step, it is
migration. Otherwise, it was coded as an appropriate feature
                                                                          important to understand the cognitive mechanisms that
migration.
                                                                          humans might use to integrate perception/action and
   The (incorrect) migration of feature information from T to
                                                                          linguistic structure to organize meaning in memory for
R was much stronger in the context condition than the order
                                                                          when perceptual models (e.g., computer vision) are
condition, and the (correct) migration of feature information
                                                                          sophisticated enough to directly represent environmental
from T to CC was stronger in the order condition than the
                                                                          information to integrate with linguistic distributional
context condition. By our coding scheme, 56% of the triples
                                                                          structure (see Roy, 2008 for a discussion).
exhibited at least one illusory feature migration in the
                                                                             While early attempts at integrating perception and
context condition (recall that this means the migration was
                                                                          language in SSMs have shown much promise, our work
stronger from T to R than it was from T to CC). In contrast,
                                                                          here indicates that a model must have a mechanism to
only 40% of the triples exhibited at least one illusory feature
                                                                          encode temporal linguistic information to know how
migration in the order condition. Most notable is that all
                                                                          perceptual information may be generalized in the mental
illusory feature migrations that took place in the order
                                                                          space. The binding framework presented here shows the
condition also took place in the composite condition, and all
                                                                          basic property of storing all information sources in a
illusory feature migrations taking place in the composite
                                                                          blended composite space (as is suggested by the literature in
condition also took place in the context condition. In other
                                                                          embodied cognition). However, the model is able to identify
words, some illusory feature migrations that took place in
                                                                          which components of the composite signal perceptual
the context and composite conditions were avoided in the
                                                                          information should be allowed to migrate to. While this
order space. Hence, emphasizing order information by
                                                                          scheme needs more testing at a large scale, we believe it has
unbinding with OI (order space) yielded equal or better
                                                                          promise for accounting for a wide range of semantic and
results for every triple when compared with emphasizing
                                                                          embodied data, and is a step toward addressing criticisms of
context information by unbinding with CI (context space) or
                                                                          SSMs being ungrounded.
not unbinding at all (composite space). Table 4 presents four
triples that differed by condition as to whether CC or R was                                     Acknowledgements
deemed a better candidate for feature migration from T by                 This research was supported in part by grants from Google
the model. In each case, a feature migration error was                    Inc. and IBM to MNJ.
committed in the context condition, but was avoided in the                                              References
order condition.                                                          Andrews, M., Vigliocco, G., & Vinson, D. P. (2009). Integrating
                                                                             experiential and distributional data to learn semantic representations.
Table 4. Example feature migration errors in context space                   Psychological Review, 116(3), 463-498.
                                                                          Cree, G. S. & McRae, K. (2003). Analyzing the factors underlying the
that were corrected in the order space. Cases in which the
                                                                             structure and computation of the meaning of chipmunk, cherry, chisel,
related word was the stronger attractor were considered                      cheese, and cello (and many other such concrete nouns). Journal of
illusory feature migrations. Target word is bold.                            Experimental Psychology: General, 132(2), 163-201.
                                                                          de Vega, M., Graesser, A., & Glenberg, A. (2008). Symbols and
                Features most strongly           Competitor that             Embodiment: Debates on Meaning and Cognition. New York: Oxford.
   Triple        attributed to target by featuresw’ Migrated More         Durda, K., Buchanan, L., & Caron, R. (2009). Grounding co-occurrence:
                participants in McRae    Strongly To, By Condition           Identifying features in a lexical co-occurrence model of semantic
                       et al. (2005)     context     comp      order         memory. Behavior Research Methods, 41, 1210-1223.
                                                                          Heit, E. (2000). Properties of inductive reasoning. Psychonomic Bulletin &
               used_for_holding_things
                                                                             Review, 7, 569-592.
   bottle            made_of_glass
                                                                          Jones, M. N. & Mewhort, D. J. K. (2007). Representing word meaning and
   CC: jar    used_for_holding_liquids      fill       jar        jar
                                                                             order information in a composite holographic lexicon. Psychological
    R: fill         made_of_plastic
                                                                             Review, 114, 1-37.
                        has_a_lid
                                                                          Kanerva, P. (2009). Hyperdimensional computing: An introduction to
                          has_fur
                                                                             computing in distributed representations with high-dimensional random
     cat                an_animal
                                                                             vectors. Cognitive Computation, 1, 139-159.
CC: mouse                  a_pet           tom        tom     mouse
                                                                          Levy, S. D., & Gayler, R. W. (2009). A distributed basis for analogical
   R: tom                   eats
                                                                             mapping. In B. Kokinov, K. Holyoak, & D. Gentner (Eds.), New
                      has_whiskers
                                                                             frontiers in analogy research. New Bulgarian University Press.
                     used_by_riding                                       McRae, K., Cree, G., Seidenberg, M. S., & McNorgan, C. (2005). Semantic
   horse                 is_large                                            feature production norms for a large set of living and nonliving things.
  CC: cow               an_animal        saddle       cow        cow         Behavior Research Methods, 37, 547-559.
 R: saddle            has_a_mane                                          Riordan, B., & Jones, M. N. (in press). Redundancy in perceptual and
                         has_legs                                            linguistic experience: Comparing feature-based and distributional
                       has_wheels                                            models of semantic representation. Topics in Cognitive Science.
motorcycle           has_2_wheels                                         Roy, D. (2008). A mechanistic model of three facets of meaning. In de
  CC: car             is_dangerous       wheels        car       car         Vega, Glenberg, and Graesser (Eds.) Symbols and Embodiment.
 R: wheels           has_an_engine                                        Sahlgren, M., Holst, A., & Kanerva, P. (2008). Permutations as a means to
                          is_fast                                            encode order in word space. Proceedings of Cognitive Science Society.
                                                                      882

