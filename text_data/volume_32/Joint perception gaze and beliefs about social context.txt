UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Joint perception: gaze and beliefs about social context

Permalink
https://escholarship.org/uc/item/9q2717xg

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Richardson, Daniel
Street, Chris
Tan, Joanne

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Joint Perception: Gaze and Beliefs about Social Context
Daniel C. Richardson (dcr@eyethink.org)
Chris N.H. Street (chris@eyethink.org)
Joanne Tan (tan.yin@ucl.ac.uk)

Cognitive, Perceptual & Brain sciences, University College London
Gower Street, London WC1E 6BT, UK
Abstract

symbols. In each set of images, there was one picture with a
negative valence (such as crying child), one with a positive
valence (a smiling couple) and two neutral images with no
strong valence. When participants believed that they were
the only ones currently looking at the images, they looked
more at the unpleasant ones. When they thought they were
looking jointly with another, they looked more at the
pleasant images.
Participants in this experiment could not see or interact
with each other. Yet their gaze was systematically shifted if
they imagined that another person was looking at the same
stimuli. There have previously been similar demonstrations
of the influence of social context on social or affective
responses, for example, that people will smile and laugh
more if they imagine that a friend elsewhere is currently
watching the same comedy clip as themselves (Fridlund,
1991). However, the joint perception result showed that, on
a trial-by-trial basis, social context can shape a low level
perceptual/cognitive process.
The original experiment was carried out at UC Santa Cruz
in the US. A replication was soon performed at University
College London in the UK (Richardson et al., 2009). The
same pervasive effect of social context was found. Gaze
patterns shifted in response to joint perception. However, in
this case, when participants believed that they were looking
together, they looked more at the negative images. The
contrasting US and UK data is shown in the top panel of
Figure 1. What is depicted is the total fixation duration for
the positive and negative images during joint and alone
looking. Each study found a significant interaction between
picture valence and social context, and between the two
experiments there was a significant three way interaction,
showing that the direction of the effect changed.
Though there were many differences between the
laboratories’ set up and the participant populations, we
hypothesised that an important determinant might be how
participants construed that task. One criticism of the first
study was that participants did not know why they were
looking at the images, and why the person next door was
(sometimes) doing the same thing. So, in subsequent
research in London (Richardson et al., 2009), we repeated
the experiments but told pairs of participants either that we
would be comparing their picture preferences (comparison
task), or that they would be collaborating on a memory task
afterwards (collaboration task). As Figure 2 shows, we
found a pattern of results that mimicked the US / UK
differences, and also produced a significant three way
interaction. People who thought they were being compared
to each other tended to look at the negative and positive
images equally in the joint condition, like the US
participants. People who thought that they were

The way that we look at images is influenced by social
context. Previously we demonstrated this phenomenon of
joint perception. If lone participants believed that an unseen
other person was also looking at the images they saw, it
shifted the balance of their gaze between negative and
positive images. The direction of this shift depended upon
whether participants thought that later they would be
compared against the other person or would be collaborating
with them. Here we examined whether the joint perception is
caused by beliefs about shared experience (looking at the
same images) or beliefs about joint action (being engaged in
the same task with the images). We place our results in the
context of the emerging field of joint action, and discuss their
connection to notions of group emotion and situated
cognition. Such findings reveal the persuasive and subtle
effect of social context upon cognitive and perceptual
processes.
Keywords: vision; joint action; eye movements; social
cognition, situated cognition

Introduction
Cognition is enveloped by social context. It is rare that we
use our cognitive or perceptual faculties outside of the world
of social influence, what Allport (1954/1979) described as
the real or imagined presence of other people. Yet in
cognitive and perceptual laboratories, we typically place
participants in experimental quarantine away from the
confounds of social interaction. The risk of this strategy is
that we overlook the ways in which cognitive and perceptual
processes interact with social context.
It is now well demonstrated that social cues such as eye
contact and gaze direction are attended to in fundamentally
different ways from non-social stimuli, both in terms of
higher-level attentional selection (e.g. Birmingham, Bischof
& Kingstone, 2008a, b, 2009; Frischen, Bayliss & Tipper,
2007; Senju & Johnson, 2009) and their different
neurological subsystems (e.g. Greene et al., 2009; Itier &
Batty, 2009; Ristic, Friesen & Kingstone, 2002). These
studies, and many others, show how perceptual processing
differs for social and non-social stimuli (Cacioppo, Visser &
Pickett, 2005).
In studies of joint perception, this relationship is turned
on its head; we keep the stimuli constant and examine how
different social cues exert an influence on perceptual
processing. The first demonstration (Richardson, Hoover &
Ghane, 2008) presented participants with a set of four
images on screen for eight seconds. On different trials,
participants either believed that in a cubicle next door
another participant was looking at the same images, or that
the person next door was looking at a set of unrelated

290

positive

/,-)0(1*),!

California

%$""#
%"""#
!$""#

'()*+#

%$""#
%"""#
!$""#

,-(*.#

!"#$#"#%&#'(/)--#0.#1(23,4.5#
Comparison task

&"""#
total looking time (ms)

total looking time (ms)

London

!"""#

!"""#

&"""#

/(*0(*!

&"""#
total looking time (ms)

&"""#
total looking time (ms)

negative

%$""#
%"""#
!$""#
!"""#

'()*+#

,-(*.#

'()*+#

,-(*.#

/.0(12#+.3+4#5!""#$!%#&'()
Collaboration Task

%$""#
%"""#
!$""#
!"""#

'()*+#

,-(*.#

Figure 1. Results from Richardson, Hoover and Ghane (2008) and Richardson et al. (2009).
collaborating looked more at the negative images in the joint
condition, like the London participants who did not get task
instructions. There could be other reasons, of course, why
the US and UK participants differed, but one plausible
reason appears to be that in the absence of instructions, they
interpreted the task in opposite ways. We can only speculate
as to the reason the US participants might felt that they were
being compared (they are academically evaluated more
frequently than UK students), or it might have been that the
physical setup of the lab (two adjoining cubicles, rather than
one big room) engendered a feeling of being contrasted.
These previous studies have shown that gaze patterns can
by systematically influenced by beliefs about social context,
and that the direction of this influence is sensitive to
differences in how participants construe their task. In the
current experiment, we zoom in to this concept of looking at
something ‘together’.
For the joint perception effect to occur, is it enough to
experience a set of stimuli at the same time as another
person? Or do participants have to believe that they are
engaged in the same task as the other person? In this
experiment, unlike those described above, the participants
always believed that they were looking at the same images
as each other. What changed, trial-by-trial, was the task that
they were doing, and the task that they believed their partner
was doing. Inspired by the seminal work on joint action
(Sebanz, Bekkering & Knoblich, 2006) that we discuss
below, we predicted that joint perception effects would be
strongest when participants believed that they were not just
passively sharing an experience, but acting jointly.

Methods
Participants
32 University College London students (9 male) participated
voluntarily or for course credit. Data from 4 participants
were unusable due to equipment calibration problems.
Note that although we actually ran pairs of participants
simultaneously in the lab, their experiments were run and
their data analysed independently from each other. This is
because participants could not see or interact with each
other during the experiment. In effect, they acted as a mute
social context for each other.

Procedure
Participants provided informed consent and then sat in
opposite corners of the laboratory with their backs to each
other, facing their display monitor. They could not see each
other or each other’s display. A brief 9-point calibration was
carried out for each, and then task instructions were
presented on screen. Two tasks were defined for the
subjects. In the memory task they had to remember as many
of the pictures as possible for a later test. In the search task,
they had to look for a translucent X superimposed on one
image, and press the mouse button that they held in one
hand if they detected it. They were informed that their task
could change from trial to trial, but that their partner would
always be looking at the same pictures as them.

291

time their gaze was tracked. There was a 1 second interval,
and then the instructions for the next trial began.
There were 40 trials. In half the participant was told that
they were to memorise the stimuli and in half they were told
that they were searching for an X. Similarly, they were told
that their partner performed the memory task half the time,
the search task the other half. These task conditions were
counterbalanced so that half the time the participant and
their partner were doing the same task, half a different task.
On eight trials (spread evenly across conditions), an X
appeared at a random location on one of the images.

You
will be searching.
Your partner will be
memorising

Stimuli

8000ms

Images were taken from the International Affective Picture
System (IAPS), a set of photographs that have been
extensively normed on a range of attributes (Lang, Bradley
& Cuthbert, 2005). We chose 40 negative items with
valence ratings from 1.6 to 2.4 and a mean of 2, 40 positive
items from 7.6 to 8.3 and a mean of 8, and 80 filler items
from 4.8 to 5.2 and a mean of 5. For each trial, stimuli were
chosen at random from these categories.

Figure 2. Trial schematic

Design
At the start of each trial, participants were told their task for
the upcoming presentation. A large icon at the top of the
screen showed their task (visual search or memory), and a
smaller icon below that showed their partner’s task (Figure
2). They also heard a voice say “You will be [memorising/
searching]. Your partner will be [memorising/searching]”.
Participants then saw one negative, one positive and two
filler images in random positions in a 2x2 grid (see Figure
2). They were presented for eight seconds, during which

The stimuli were presented on 19” LCD screen at a distance
of approximately 60cm. Beneath each display was a
Bobax3000 remote eye tracker that sampled fixations at 100
Hz. iMac computers behind a partition presented the stimuli,
calculated gaze position, and collected the data.

Own task

$%""#

total looking time (ms)

Apparatus

search

memory

$$""#
()*+,-)#

$"""#

./01,-)#

!'""#
!&""#
!%""#
!$""#
!"""#

same

different
different
Partnerʼs task

same

Figure 3. Looking times showed a significant interaction between valence and whether or not
the participant’s partner was believed to be doing the same or a different task

292

spatial codes) then reaction times increase (Simon, 1969).
Sebanz, Knoblich & Prinz (2003) divided such a task
between two people. The participants sat next to each other,
and each person responded to one colour: in effect, each
acting as one of the fingers of a participant in Simon’s
(1969) experiment. Though each person had only one
response to execute, they showed an incompatibility effect
when acting together. There was no incompatibility effect
when performing the same single response task alone. When
acting jointly, participants represented their partners’ actions
as if they were their own.
Joint action effects do not occur if the participant is
simply sat next to another person (Tsai et al., 2006), or if
that person’s button pressing actions are not intentional
(their finger is moved by a mechanical device). Also, if the
participant is acting jointly, but with a computer program
(Tsai et al., 2008) or a marionette’s wooden hand (Tsai &
Brass, 2007) there is not a stimulus-response incompatibility
effect. Therefore, participants only form representations of
another when that person’s genuine, intentional actions are
engaged in the same task.
Our results fill out this picture. We have shown that a
participant’s perceptual process is changed when they
believe that another person is co-acting with them: they do
not have to see the person (c.f. Tsai et al., 2008), and the
‘actions’ do not have to be overt behaviour. If the participant
thinks that the other person is memorising or scanning the
images together with them, then that mutual cognitive
process will shape their gaze patterns.

Results
Participants looked more towards the negative images when
they believed that their partner was doing the same task as
them, regardless of what the task was. We did not analyse
the 20% of trials when there was an X present, as X and
participants’ responses to it would interfere with how they
allocated their attention to each image. We calculated the
total amount of time spent looking at the critical negative
and positive images on trials where there was not X present.
A 2 (valence: negative/positive) x 2 (own task: memory/
search) x 2 (other’s task: same/different) ANOVA was
performed, and the means for each cell are displayed in
Figure 3. There was a significant two way interaction
between valence and other’s task (F(1,27)=10.08, p=.004).
Post hoc tests show that the difference between positive and
negative images was significant when the participants
believed they were doing the same task (using Tukey’s at
0.01), but did not reach significance when they were doing a
different task. There was also a main effect of valence (F
(1,27)=19.19, p<.0001), but all other main effects and
interactions were non significant (all Fs <1).

General Discussion
The effects of joint perception do not occur simply when
someone believes that another person is experiencing the
same stimuli as themselves. We have shown that it is
necessary that they believe that the other, unseen person is
engaged in the same task as themselves. This task could be
to memorise the pictures, which presumably would engage
processing something of the meaning of an image, or the
task could just be to search for a visual feature, which
requires only superficial processing: regardless, the effect of
joint perception arises whenever these tasks are believed to
be done together. In each case, the effect of this coengagement is to fixate the negative images more than the
positive. Below, we discuss other areas of research that
throw light on joint perception, and the direction of its
effects in this situation.

Focal Images
The term ‘focal image’ comes from Schelling (1960) who
found that people were very good at guessing what images
others would find salient. Schelling realised that everyday
cases of verbal reference are often ambiguous. We say,
‘Hand me the fork,’ in the presence of many such items, yet
listeners unproblematically infer the same referent. For
example, when presented with a page full of items, such as
watches from a catalog, participants agreed with each other
which one was most likely to be referred to as ‘the
watch’ (Clark, Schreuder & Buttrick, 1983).
When we enter into any joint activity, such coordination is
all important (Clark, 1996). When we talk, we implicitly
agree upon names for novel objects (Clark & Brennan,
1991), align our spatial reference frames (Schober, 1993),
use each others’ syntactic structures (Branigan, Pickering &
Cleland, 2000), sway our bodies in synchrony (Shockley,
Santana & Fowler, 2003; Condon & Ogston, 1971) and even
scratch our noses together (Chartrand & Bargh, 1999). We
also coordinate our gaze patterns with each other
(Richardson & Dale, 2005), taking into account the
knowledge (Richardson, Dale & Kirkham, 2007) and the
visual context (Richardson, Dale & Tomlinson, 2009) that
we share. Perhaps participants in our experiment,
anticipating a future discussion of the stimuli, attempted to
coordinate gaze patterns with their partner when they
believed they were acting jointly. In other words, they
looked at the pictures they thought another person would
look at: the focal image.

Joint Action
Though the standard cognitive model marginalises social
context, there have been notable exceptions. Studies of
situated cognition (Barsalou, Breazeal & Smith, 2007;
Robbins & Ayded, 2009) show that cognition ‘in the wild’ is
intimately linked not only to representations of the external
world, but also to the cognitive processes of others.
Hutchins (1995) observed the ways that navy navigators
distribute cognitive processes between themselves by using
external tools and representations, such as maps and
notations.
Recently, experimental methods are starting to reveal the
mechanisms involved in such joint action (Galantucci &
Sebanz, 2009; Sebanz, Knoblich & Bekkering, 2006).
Social context can modulate even the simplest of tasks. For
example, in a traditional stimulus-response compatibility
task, participants make a judgment about one stimulus
property (color) and ignore another stimulus property
(location). If there is an incompatibility between the
irrelevant property and the response (such as different

293

Responses to Negative Stimuli

Acknowledgments

Our discussion so far has not touched upon one question:
why is it that the effect of joint perception is sometimes to
increase looks to the negative pictures, and sometimes to the
positive images? It seems plausible that participants who
thought that they were being compared to each other might
want to look equally at the positive and negative images,
since they may feel that ogling a disturbing image might not
reflect well upon them. However why is it that in the
collaborative memory task and the joint visual search tasks,
the participants looking together tend to look at the negative
images?
We are generally very responsive to unpleasant or
threatening things. Negative images are considered more
potent than equivalently-valenced positive images, so much
so that when combinations of equivalent positively and
negatively valenced stimuli are presented simultaneously
participants rate the overall set as unpleasant (for reviews,
see Baumeister et al., 2001; Lewicka, Czapinski & Peeters,
1992; Rozin & Royzman, 2004; Skowronski & Charlston,
1989). Negative stimuli are likely to receive attention more
quickly (Norris et al., 2004, Smith et al., 2003) and for
longer (Hajcak & Olvet, 2008). But why might this bias
towards negative images be amplified during joint
perception?

We are grateful to Merrit Hoover, Arezou Ghane and
Natasha Eapen for help in designing the experiments,
running subjects and for many insightful discussions.

References
Allport, G.W. (1954/1979). The nature of prejudice.
Cambridge, MA: Perseus Books.
Barsade, S.G. (2002). The ripple effect: Emotional
contagion and its influence on group behaviour.
Administrative Science Quarterly, 47(4), 644-675.
Barsalou, L.W., Breazeal, C., & Smith, L.B. (2007).
Cognition as coordinated non-cognition. Cognitive
Processing, 8, 79-91.
Baumeister, R.F., Bratlavsky, E., Finkenauer, C., & Vohs,
K.D. (2001). Bad is stronger than good, Review of
General Psychology, 5(4), 323-370.
Birmingham, E., Bischof, W.F., & Kingstone, A. (2008a).
Gaze selection in complex social scenes. Visual
Cognition, 16(2/3), 341-355.
Birmingham, E., Bischof, W.F., & Kingstone, A. (2008b).
Social attention and real world scenes: The roles of
action, competition, and social content. Quarterly Journal
of Experimental Psychology, 61(7), 986-998.
Birmingham, E., Bischof, W.F., & Kingstone, A. (2009). Get
real! Resolving the debate about equivalent social stimuli.
Visual Cognition, 17(6), 904-924.
Branigan, H. P., Pickering, M. J., & Cleland, A. A. (2000).
Syntactic coordination in dialogue, Cognition, 75, B13B25.
Cacioppo, J.T., Visser, P.S.& Pickett C.L. (Eds.) (2005).
Social neuroscience: People thinking about thinking
people. Cambridge, MA: The MIT press.
Chartrand, T. L., & Bargh, J. A. (1999). The chameleon
effect: The perception-behavior link and social
interaction. Journal of Personality and Social Psychology,
76, 893-910.
Clark, H.H. (1996). Being there: Putting brain, body, and
the world together again. Cambridge: MIT Press.
Clark, H.H., & Brennan, S.E. (1991). Grounding in
communication. In L.B. Resnick, J.M. Levine, &
S.D.Teasley (Eds.), Perspectives on Socially Shared
Cognition. Washington, DC: American Psychological
Association
Clark, H.H., Schreuder, R. & Buttrick, S., (1983). Common
ground and the understanding of demonstrative reference.
Journal of Verbal Learning and Verbal Behavior, 22,
245-258.
Condon, W., & Ogston, W. (1971). Speech and body motion
synchrony of the speaker-hearer. In D. Horton & J.
Jenkins (Eds.), The Perception of Language. Columbus,
OH: Charles E. Merrill.
Fridlund, A.J., (1991). Sociality of Solitary Smiling:
Potentiation by an Implicit Audience. Journal of
Personality and Social Psychology, 60, 229-240.
Frischen, A., Bayliss, A.P., & Tipper, S.P. (2007). Gaze
cueing of attention: Visual attention, social cognition, and

Emotion and Social Interaction
When people collaborate in groups, they tend to align with
the group emotion (Barsade, 1998; Hatfield, Cacioppo &
Rapson, 1993; Wageman, 1995). That emotion arises from
the majority’s personal disposition for positive or negative
mood states (George, 1990). Since, as we’ve seen, negative
stimuli are usually attended to more by individuals, when
they cooperate together this would serve to amplify the
negativity bias (Taylor, 1991). Affect can influence
behaviour without necessarily having to personally
experience the emotion (Winkielman, Berridge & Wilbarger,
2005). In this light, our joint perception phenomenon could
be seen as a form of minimal, imagined cooperation that is
sufficient to produce an alignment of group emotional
biases.

Conclusion
How we move our eyes is swayed by a belief that others are
looking at the same scene and thinking the same thing.
These results broaden the notion of joint action to include
perceptual processes, unseen collaborators and mental
actions such as remembering and visual search. They also
suggest a possible experiment to perform at a poster session.
Sidle up to another conference attendee gazing over the
results of an experiment. If our results generalise, a slight
cough will alert them to your presence, engage their feeling
of joint perception and perhaps sway their gaze towards
more negative aspects of the poster, demonstrating that an
effect of social context can even be found at a cognitive
science conference.

294

individual differences. Psychological Bulletin, 133(4),
694-724.
Galantucci, B., & Sebanz, N. (2009). Joint action: Current
perspectives. Topics in Cognitive Science, 1, 255-259.
George, J.M. (1990). Personality, affect, and behavior in
groups. Journal of Applied Psychology, 75, 107-116.
Greene, D.J., Mooshagian, E., Kaplan, J.T., Zaidel, E., &
Iacoboni, M. (2009). The neural correlates of social
attention: Automatic orienting to social and nonsocial
cues. Psychological Research, 73, 499-511.
Hajcak, G., & Olvet, D.M. (2008). The persistence of
attention to emotion: Brain potentials during and after
picture presentation. Emotion, 8(2), 250-255.
Hatfield, E., Cacioppo, J.T., & Rapson, R.L. (1993).
Emotional contagion, Current Directions in
Psychological Science. 2(3), 96-99.
Itier, R.J, & Batty, M. (2009). Neural bases of eye and gaze
processing: The core of social cognition. Neuroscience
and Biobehavioral Reviews, 33, 843-863.
Knoblich, G., & Sebanz, N. (2006). The social nature of
perception and action. Current Directions in
Psychological Science, 15(3), 99-104.
Lang, P.J., Bradley, M.M., & Cuthbert, B.N. (2005).
International affective picture system (IAPS): Digitized
photographs, instruction manual, and affective ratings
(Tech. Rep. A-6). Gainesville: University of Florida,
Center for Research in Psychophysiology
Lewicka, M., Czapinsky, J., & Peeters, G. (1992). Positivenegative asymmetry or ‘When the heart needs a reason’.
European Journal of Social Psychology, 22, 425-434.
Norris, C.J., Chen, E.E., Zhu, D.C., Small, S.L., &
Cacioppo, J.T. (2004). The interaction of social and
emotional processes in the brain. Journal of Cognitive
Neuroscience, 16(10), 1818-1829.
Richardson, D.C & Dale, R. (2005). Looking to understand:
The coupling between speakers’ and listeners’ eye
movements and its relationship to discourse
comprehension. Cognitive Science, 29, 1045–1060.
Richardson, D.C, Hoover, M.A. & Ghane, A. (2008). Joint
perception: gaze and the presence of others. Proceedings
of the 30th Annual Conference of the Cognitive Science
Society (pp. 309-314). Austin, TX: Cognitive Science
Society.
Richardson, D.C., Dale, R., & Kirkham, N.Z. (2007). The
art of conversation is coordination: Common ground and
the coupling of eye movements during dialogue.
Psychological Science, 18(5), 407-413.
Richardson, D.C., Dale, R., & Tomlinson, J.M. (2009).
Conversation, gaze coordination, and beliefs about visual
context. Cognitive Science, 33(8), 1468-1482.
Richardson, D.C., Hoover, M.A. Ghane, A. Eapen, N. &
Tan, J. (2009). Joint perception across tasks: gaze and
social cognition. Proceedings of the 31st Annual
Conference of the Cognitive Science Society (pp. 66-72).
Austin, TX: Cognitive Science Society
Ristic, J., Friesen, C.K., & Kingstone, A. (2002). Are eyes
special? It depends on how you look at it. Psychonomic
Bulletin & Review, 9(3), 501-513.

Rozin, P., & Royzman, E.B. (2001). Negativity bias,
negativity dominance, and contagion. Personality and
Social Psychology Review, 5(4), 296-320.
Schelling, T. C. (1960). The Strategy of Conflict,
Cambridge, Mass.: Harvard University Press.
Schober, M.F. (1993). Spatial perspective-taking in
conversation. Cognition, 47(1), 1-24.
Sebanz, N., Bekkering, H., & Knoblich, G. (2006). Joint
action: Bodies and minds moving together. TRENDS in
Cognitive Sciences, 10(2), 70-76.
Sebanz, N., Knoblich, G., & Prinz, W. (2003). Representing
others’ actions: Just like one’s own? Cognition, 88(3),
B11-B21
Senju, A., & Johnson, M.H. (2009). Atypical eye contact in
autism: Models, mechanisms and development.
Neuroscience and Biobehavioral Reviews, 33(8),
1204-1214.
Shockley, K., Santana, M-V., & Fowler, C.A. (2003).
Mutual interpersonal postural constraints are involved in
cooperative conversation. Journal of Experimental
Psychology: Human Perception and Performance, 29(2),
326-332
Simon, J.R. (1969). Reactions toward the source of the
stimulation. Journal of Experimental Psychology, 81(1),
174-176.
Skowronski, J.J., & Carlston, D.E. (1989). Negativity and
extreme biases in impression formation: A review of
explanations. Psychological Bulletin, 105(1), 131-142.
Smith, N.K., Cacioppo, J.T., Larsen, J.T., & Chartrand, T.L.
(2003). May I have your attention, please: Electrocortical
responses to positive and negative stimuli.
Neuropsychologia, 41, 171-183.
Taylor, S.E. (1991). Asymmetrical effects of positive and
negative events: The mobilization-minimization
hypothesis. Psychological Bulletin, 110(1), 67-85.
Tsai, C-C., Kuo, W-J., Hung, D.L., & Tzeng, O. J-L. (2008).
Action co-representation is tuned to other humans.
Journal of Cognitive Neuroscience, 20(11), 2015-2024.
Tsai, C-C., Kuo, W-J., Jing, J-T., Hung, D.L., & Tzeng, O.
J-L. (2006). A common coding framework in self-other
interaction: Evidence from joint action task. Experimental
Brain Research, 175, 353-362.
Tsai, C.-C., & Brass, M. (2007). Does the human motor
system simulate Pinocchio’s actions? Co-acting with a
human hand versus a wooden hand in a dyadic
interaction. Psychological Science, 18(12), 1058-1062.
Wageman, R. (1995). Interdependence and group
effectiveness. Administrative Science Quarterly, 40,
145-180.
Winkielman, P., Berridge, K.C., & Wilbarger, J.L. (2005).
Unconscious affective reactions to masked happy versus
angry faces influence consumption behavior and
judgments of value. Personality and Social Psychology
Bulletin, 31(1), 121-135.
Robbins, P. & Aydede, M. (Eds.) (in press). The Cambridge
Handbook of Situated Cognition. Cambridge, UK:
Cambridge University Press.
Hutchins, E., (1995). Cognition in the Wild. Cambridge,
MA: MIT Press.

295

