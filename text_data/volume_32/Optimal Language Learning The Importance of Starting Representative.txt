UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Optimal Language Learning: The Importance of Starting Representative
Permalink
https://escholarship.org/uc/item/2mm1k99p
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Rafferty, Anna
Griffiths, Thomas
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

         Optimal Language Learning: The Importance of Starting Representative
                                             Anna N. Rafferty (rafferty@cs.berkeley.edu)
                             Computer Science Division, University of California, Berkeley, CA 94720 USA
                                          Thomas L. Griffiths (tom griffiths@berkeley.edu)
                             Department of Psychology, University of California, Berkeley, CA 94720 USA
                                Abstract                                    syntactic structures early in sentences. However, Newport
                                                                            et al. (1977) also found that many measures of acquisi-
   Child-directed speech has a distinctive structure and may have
   facilitatory effects on children’s language learning. We con-            tion were unaffected by characteristics of caregivers’ speech,
   sider these facilitatory effects from the perspective of Marr’s          and Huttenlocher, Vasilyeva, Cymerman, and Levine (2002)
   levels of analysis: could they arise at the computational level          found that exposing children to more complex speech resulted
   or must they be located at at the algorithmic or implementation
   levels? To determine if the effects could be due to computa-             in the children using more complex syntax.
   tional level benefits, we examine the question of what samples              Previous work has used specific computational models
   from a language should best facilitate learning by identifying           such as associative learning and artificial neural networks to
   the optimal linguistic input for an ideal Bayesian learner. Our
   analysis leads to a mathematical definition of the “represen-            explore the effects of simplified input on language learning
   tativeness” of linguistic data, which can be computed for any            (Goldowsky & Newport, 1993; Elman, 1993; Rohde & Plaut,
   probabilistic model of language learning. We use this measure            1999). Elman (1993) found that training a simple recurrent
   to re-examine the debate over whether language learning can
   be improved by “starting small” (i.e. learning from data that            neural network to predict the next word in a sequence using a
   have limited complexity). We compare the representativeness              corpus of limited complexity resulted in better generalization
   of corpora with differing levels of complexity, showing that             than beginning with the full corpus. However, the effects of
   while optimal corpora for a complex language are also com-
   plex, it is possible to construct relatively good corpora with           “starting small” are far from clear: Rohde and Plaut (1999)
   limited complexity. We discuss the implications of these re-             subsequently found a disadvantage for language learning that
   sults for the level of analysis at which a benefit of starting small     begins with data of limited complexity when using similar
   must be located.
   Keywords:        language learning; child-directed speech;               models and corpora.
   Bayesian models; representativeness; starting small                         Demonstrating an effect of starting small under specific as-
                                                                            sumptions about learning leaves open the question of the level
                            Introduction                                    of analysis at which there might be an advantage for child-
Child-directed speech is an important source of information                 directed speech. Marr (1982) defined three levels at which
for children’s language acquisition. Hoff and Naigles (2002)                information processing systems can be analyzed: the compu-
found that the amount of child-directed speech produced by                  tational level, where the analysis aims to identify the abstract
mothers was predictive of the vocabulary of their children,                 problem being solved and its ideal solution; the algorithmic
and Cameron-Faulkner, Lieven, and Tomasello (2003) found                    level, where the focus is on the representation and algorithm
correlations between the grammatical frames mothers used in                 being used to implement this solution; and the implementa-
speech to their children and the grammatical frames used by                 tion level, which emphasizes the physical hardware on which
the children. Child-directed speech also differs from adult-                the algorithm is executed. Facilitatory effects of the structure
directed speech in a number of ways. For example, Snow                      of child-directed speech could be caused by considerations at
(1972) found that speech to two year olds by caregivers has                 any of these levels. At the computational level, data of this
simplified structure and involves more repetitions than speech              kind could provide more statistical evidence for the struc-
to older children or adults, and Sherrod, Friedman, Crawley,                ture of the language. Alternatively, constraints at the algo-
Drake, and Devieux (1977) found that the mean length of ut-                 rithmic or implementation levels might limit the information-
terances spoken to a child changed in response to changes                   processing capacities of children, making simplified input
in the child’s understanding. Overall, child-directed speech                necessary despite the lack of a computational level benefit.
tends to be simplified, more grammatically correct, and more                   We try to identify the level of analysis at which a benefit
repetitive than adult-directed speech (Pine, 1994). This raises             from simplified input could be located by asking what char-
an important question: Does the structure of child-directed                 acteristics a sample of language should have in order to be
speech facilitate language acquisition?                                     most useful for an ideal learner. If simpler corpora are better
   There is some evidence for a facilitatory effect of child-               for this ideal learner, then we can provide a computational-
directed speech. Furrow, Nelson, and Benedict (1979)                        level account of the benefit of starting small. If not, such an
found that children’s language development was positively                   effect must be located at a lower level. It is necessary to con-
correlated with mothers’ use of simple constructions, and                   sider the performance of ideal learners in order to rule out the
Newport, Gleitman, and Gleitman (1977) found that acqui-                    possibility that starting small provides a computational-level
sition of certain syntactic features was facilitated by charac-             advantage. If this were the case, it would not be necessary
teristics of mothers’ speech, such as placement of particular               to assume algorithmic level constraints are the cause of an
                                                                        2069

advantage for starting small, as has been done in previous re-       of the “representativeness” of d relative to θ∗ , being an indi-
search.                                                              cator of the strength of evidence that d provides in favor of
   We identify the optimal input for an ideal Bayesian lan-          θ∗ relative to other values of θ. Intuitively, a sample is more
guage learner by asking what data maximize the posterior             representative if it is both very probable under the true model
probability such a learner ascribes to the target language. This     (the numerator of Equation 2) and not as probable under a
is a special case of the problem of defining a “representative”      model selected at random (the denominator of Equation 2).
sample analyzed by Tenenbaum and Griffiths (2001). Conse-
quently, we define a Bayesian measure of representativeness,         Representativeness for Discrete Distributions
and use this measure to give a mathematical characterization
                                                                     In general, we may not be able to solve the integral in the de-
of an optimal corpus. We present a general mathematical re-
                                                                     nominator of Equation 2 exactly. However, we can solve this
sult characterizing representativeness for discrete probability
                                                                     integral in the case where the model p(d|θ) is a discrete prob-
distributions, which are the basic component of any prob-
                                                                     ability distribution, as is often true with probabilistic models
abilistic model of language. This result provides the basis
                                                                     of language. For a multinomial with ordered outcomes, the
for a more detailed exploration of whether language of lim-
                                                                     likelihood is p(d|θ) = ∏ti=1 (θ∗i )ki , where t is the number of
ited complexity might be as good or better for learning than
                                                                     possible outcomes, θ∗i is the probability of outcome i, and
language of full complexity. We explore the implications of
                                                                     ki is the number of times the outcome i occurred. We place
this result by identifying the optimal input for four different
                                                                     a uniform Dirichlet prior on the distribution θ, reflecting no
learning scenarios, involving estimating probabilistic gram-
                                                                     strong expectations about the probabilities of different rules.
mars with varying degrees of knowledge about the structure
                                                                     Thus, the integral in Equation 2 is in this case:
of a language and estimating n-gram models.
                                                                                   t
                                                                                                   (∏t     ki !)        (∏ti=1 ki !)
                                                                              Z
             Identifying Optimal Corpora                                          ∏ θki i dθ = (t − 1 +i=1∑t         =               (3)
                                                                               ∆  i=1                      i=1 ki )!   (t − 1 + n)!
To understand the characteristics of an optimal sample of
language, we formalize the problem of language learning in
terms of Bayesian inference. Learning a probabilistic model          where ∆ is the simplex of values such that ∑ti=1 θi = 1, and n
of language requires estimating the value of a set of pa-            is the total number of observations. The representativeness of
rameters θ from observed linguistic data d. Assuming the             a corpus with respect to this model with a particular value of
learner has some initial beliefs about the value of θ, expressed     θ is then:
through a prior probability distribution p(θ), the beliefs of a
rational learner after observing d are given by the posterior                                      (t − 1 + n)! ∏ti=1 θki i
                                                                                       R(d, θ) =                            .        (4)
distribution p(θ|d) obtained by applying Bayes’ rule,                                                     (∏ti=1 ki !)
                                p(d|θ)p(θ)                           The optimal corpus is that which maximizes this quantity.
                   p(θ|d) = R                                (1)
                                p(d|θ)p(θ)dθ                            We can find an exact expression for the frequencies an opti-
                                                                     mal corpus would have by maximizing the quantity in Equa-
where the likelihood p(d|θ) indicates the probability of d un-       tion 4 with respect to ki . Since the logarithm is monotonic,
der the probabilistic model with parameters θ.                       the corpus that maximizes R(d, θ) is also the corpus that max-
                                                                     imizes log R(d, θ), so we perform our maximization with this
A Measure of Representativeness                                      transform. Additionally, this is a constrained optimized prob-
Formalizing language learning in this way now allows us to           lem since n must equal ∑ti=1 ki . We enforce this constraint
consider what corpora will most strongly support learning.           with a Lagrange multiplier, and replace the factorials using
Assume that the true structure of the language is character-         Stirling’s approximation to obtain the objective function:
ized by parameters θ∗ ; we consider a learner that is simply
learning this structure, although more complicated models                    L = (t − 1 + n) log(t − 1 + n) + 1 − t
that also learn other parts of the language, such as seman-                               t                                    t
tics, are possible. To maximize the probability of a learner                          + ∑ ki log(θi ) − ki log(ki ) + λ(n − ∑ ki )   (5)
inferring θ∗ over other values of θ, a teacher should provide                           i=1                                   i=1
data d that maximize p(θ∗ |d). Examination of the right hand
side of Equation 1 shows that this can be done by maximizing         where λ is the Lagrange multiplier. To determine the opti-
                                                                     mum of this objective function, we differentiate with respect
                                   p(d|θ∗ )                          to ki , set the derivative to zero, and solve for ki . This shows
                  R(d, θ∗ ) = R                              (2)     that the optimal value is ki = nθi . Rounding to the nearest in-
                                 p(d|θ)p(θ)dθ
                                                                     teger, this corresponds to what one might intuitively expect:
with respect to d, as the prior probability p(θ∗ ) is constant       The most representative corpus is that in which the relative
and thus unaffected by the choice of d. Tenenbaum and Grif-          frequencies of the outcomes match their probabilities under
fiths (2001) suggested that R(d, θ∗ ) be considered a measure        the target multinomial.
                                                                 2070

                           S       NP VP                                                        S          NP VP | VP NP
                          NP       PropN | Det1 N | Det1 N RC                                  NP          PropN | Det1 N | Det1 N RC
                          VP       VI | VT NP                                                  VP         VI | VT NP | NP VT
                          RC       Det2 VI | Det2 VT NP                                        RC         Det2 VI | Det2 VT NP | Det2 NP VT
                           N       boy | girl | cat | dog | boys | girls | cats | dogs          N          boy | girl | cat | dog | boys | girls | cats | dogs
                         PropN     John | Mary                                                PropN       John | Mary
                           VI      see | hear | walk | live | sees | hears | walks | lives      VI        see | hear | walk | live | sees | hears | walks | lives
                          VT       see | hear | hit | feed | sees | hears | hits | feeds       VT         see | hear | hit | feed | sees | hears | hits | feeds
                          Det1     the                                                         Det1       the
                          Det2     who                                                         Det2       who
Figure 1: The context-free grammars used in our simulations. On the left, the true grammar; on the right, the overly general
grammar with added rules, used in the third simulation. Bolded expansions are those present in the expanded grammar but not
in the true grammar. In addition to these rules, subject-verb agreement is enforced, resulting in a much larger PCFG.
Representativeness for Probabilistic Grammars                                                learner to the full language. A sentence is complex if it con-
The results for the representativeness of samples from multi-                                tains a recursive rule; for example, in both Elman (1993) and
nomial distributions can be used to characterize optimal cor-                                Rohde and Plaut (1999), complex sentences are those that
pora for any probabilistic language model with discrete ele-                                 contain relative clauses. Both Elman (1993) and Rohde and
ments, such as an n-gram model. These results also gener-                                    Plaut (1999) used neural networks that learned to predict the
alize naturally to a representativeness measure for more so-                                 next word in the sentence. Elman (1993) found that start-
phisticated probabilistic models of language, such as prob-                                  ing small was essential for his model; when a corpus of full
abilistic grammars. A probabilistic context-free grammar                                     complexity was used, the learner was never able to predict
(PCFG; Baker, 1979) defines a probability distribution over                                  the next word with satisfactory accuracy. Rohde and Plaut
sentences via a set of expansion rules for non-terminals (e.g. a                             (1999) found, in contrast, that in most cases starting small
noun phrase consists of a determiner followed by a noun)                                     resulted negative impacts on performance, and none of their
and distributions over those rules indicating the probability                                simulations showed any advantage to starting small.
of a given non-terminal being expanded to a particular se-
                                                                                                 We use the analysis of representativeness for an ideal lan-
quence (see Figure 1). The distributions over rules are in-
                                                                                             guage learner given in the previous section to explore the lo-
dependent multinomials, allowing us to build on the repre-
                                                                                             cus of a potential effect of starting small. Since our analysis
sentativeness analysis above. In this case, the parameters θ
                                                                                             focuses solely on the statistical evidence a corpus provides
describe the multinomial distributions associated with each
                                                                                             in favor of a particular language, we can examine whether a
expansion rule.
                                                                                             potential benefit of starting small could arise at the computa-
   When the structure of sentences (i.e. the sequence of ex-
                                                                                             tional level, or must be a consequence of specific information-
pansion rules used in generating each sentence) is known, the
                                                                                             processing constraints associated with human learning. Thus,
representativeness of a corpus follows directly from our re-
                                                                                             we consider two questions: First, does starting small result in
sult for multinomials. Since each rule is associated with an
                                                                                             particularly good corpora for language learning? And sec-
independent multinomial, the representativeness is the prod-
                                                                                             ond, can a corpus of limited complexity be as good as a cor-
uct of the representativeness for each multinomial. Thus, a
                                                                                             pus without limited complexity? Clearly, if a starting small
representative corpus is one in which the relative frequencies
                                                                                             corpus is optimal, then such a corpus is as or more represen-
with which expansion rules are used match the probabilities
                                                                                             tative than a more complex corpus. However, even if a limited
associated with those expansion rules in the grammar.
                                                                                             complexity corpus is non-optimal, it might be as representa-
   When the structure of sentences is unknown, p(d|θ) is ob-
                                                                                             tive as corpora generated by other means. In particular, we
tained by marginalizing over possible structures. For PCFGs,
                                                                                             compare corpora of different complexity generated by maxi-
this can be done efficiently using a dynamic program; in our
                                                                                             mizing representativeness and generated randomly.
simulations, we used Mark Johnson’s implementation of this
algorithm.1 However, since there is not a closed form for                                        As in the analysis in the previous section, we consider two
this marginalization, we cannot calculate the denominator of                                 types of corpora: those in which sentence structure is known
Equation 2 exactly. In this case, we can use a Monte Carlo                                   and those in which structure is unknown. In two simulations,
method to approximate the integral and obtain an estimate of                                 we assume that the learner knows the rules of the grammar,
the representativeness of a corpus.                                                          but does not know the frequencies with which they occur. Our
                                                                                             third simulation introduces ambiguity about the rules of the
                        Starting Small
                                                                                             grammar, and the fourth considers the possibility that chil-
As described in Elman (1993), “starting small” involves                                      dren are not learning a grammar but simply distributions over
showing a learner only a limited number of “complex” sen-                                    which words follow one another. We used a PCFG similar
tences from a language first, and gradually exposing the                                     to that in Elman (1991). The only instance of recursion was
    1 Version last updated 2 September 20007, and available at                               in the relative clause, which occurred in 75% of sentences
http://www.cog.brown.edu/∼mj/Software.htm                                                    generated from the grammar, and the grammar enforced the
                                                                                         2071

                                    100
                                           Corpora Representativeness with Known Strucure                       tive than randomly generated corpora of the same complex-
                                     80
                                                                                                                ity. However, the limitation on complexity greatly affects
                                     60
                                                                                                                representativeness. While limiting complexity significantly
                                                                                                                impacts the representativeness of only one rule, that which
           Log Representativeness
                                     40
                                     20
                                                                                                                allows the introduction of the relative clause, this impact is
                                      0
                                                                                                                severe enough to outweigh the representativeness of the other
                                    −20
                                                                                                                rules. Thus, an optimized sample of severely limited com-
                                    −40
                                                                                                                plexity is much less representative than a random sample in
                                    −60
                                                                                                                which complexity is not constrained. When the limit is not
                                                                                     Optimized Corpora
                                    −80
                                                                                     Random Corpora             as severe, though, optimized corpora with somewhat limited
                                       0     0.1   0.2   0.3   0.4   0.5   0.6   0.7     0.8   0.9       1
                                                         Percent of Corpus Complex                              complexity and random corpora with greater complexity have
Figure 2: Representativeness of corpora with known struc-                                                       equal representativeness, due to the fact that the severity of
ture. As the number of complex sentences increases, the rep-                                                    the complexity constraint has a non-linear effect on represen-
resentativeness of the corpora increases non-linearly.                                                          tativeness (Figure 2). For corpora of unconstrained complex-
agreement of subjects and verbs2 . Figure 1 shows the gram-                                                     ity, the results mirror the results for corpora with constrained
mar prior to integrating the constraint of verbal agreement;                                                    complexity equal to the true base rate of complex sentences
the final grammar consisted of 63 rules and 23 nonterminals.                                                    for the grammar (75%). The average representativeness of
                                                                                                                randomly selected corpora was 65.7 ± 4.9, with 76.3% ± 4.9
Representativeness with Known Structure                                                                         complex trees, while the average representativeness of cor-
We first considered the problem of learning from a corpus in                                                    pora selected for representativeness was 87.7 ± 8x10−5 , with
which the structures of the sentences are known, allowing us                                                    80.1% ± 10.3 complex trees.
to use the closed form given in Equation (4) to exactly com-                                                    Representativeness with Unknown Structure
pute the representativeness of the corpus. We sought to quan-
                                                                                                                The previous simulation assumed that our corpus consisted of
tify how representative a corpus could be given the constraint
                                                                                                                the structure of the sentences, from which we could directly
on complexity and discover how this compared to randomly
                                                                                                                compute the representativeness of a given corpus. However,
generated corpora as well as more complex corpora.
                                                                                                                one might alternatively assume that a language learner has
   To investigate this question, we generated several types
                                                                                                                only the sentences as data and must consider all possible
of corpora. All corpora were created by choosing a subset
                                                                                                                structures. We examine this possibility by using the same cor-
of sentences from a large corpus generated by the grammar.
                                                                                                                pora of sentences as in the previous simulation, but assuming
Random corpora were generated by selecting this subset ran-
                                                                                                                the structure of each sentence is unknown.
domly, subject to a constraint on the number of complex sen-
                                                                                                                   As mentioned in the previous section, when the structure of
tences. Optimized corpora were collections of sentences cho-
                                                                                                                sentences is unknown we need to resort to Monte Carlo ap-
sen to maximize representativeness. An ε-greedy perturba-
                                                                                                                proximation to compute representativeness. We used impor-
tion process was used to maximize representativeness. First,
                                                                                                                tance sampling (Neal, 1993); our proposal distribution was a
an initial corpus of the target complexity was randomly se-
                                                                                                                Dirichlet distribution with parameters equal to the true distri-
lected. This corpus was perturbed by adding additional sen-
                                                                                                                bution in the grammar multiplied by ten. Results are averages
tences, and then pruning sentences from the augmented cor-
                                                                                                                of 30 iterations of 10,000 samples each; in the case of random
pus. With small probability, a sentence was chosen randomly
                                                                                                                corpora, sampling was done for each of ten corpora with the
to add or prune. Otherwise, a sentence was chosen by check-
                                                                                                                same constraints on complexity. Given that variance for the
ing the effect of adding or pruning each possible sentence and
                                                                                                                optimized corpora was much smaller, sampling was done for
greedily adding or pruning the sentence that resulted in the
                                                                                                                only one optimized corpus of each level of complexity. We
corpus with the largest representativeness. Twenty perturba-
                                                                                                                consider the same four levels of complexity as Elman (1993)
tions of ten sentences each were performed; results were not
                                                                                                                and Rohde and Plaut (1999): 0%, 25%, 50%, or 75% of the
sensitive to small variations in these parameters.
                                                                                                                total corpus size. Additionally, we consider corpora of un-
   For both the random and optimized conditions, we created
                                                                                                                constrained complexity.
corpora with constrained complexity. Corpora were gener-
                                                                                                                   Figure 3 shows that the general trends from the previ-
ated with complexity ranging from 0% to 100% complex sen-
                                                                                                                ous simulation hold, with a few variations. The separation
tences, at 5% intervals. A complex sentence was any sentence
                                                                                                                between the optimized corpora and the random corpora is
containing a relative clause. Additionally, random and opti-
                                                                                                                smaller than when the structure is known. This is partially due
mized corpora were generated with no complexity constraint.
                                                                                                                to the way the corpora were created. Presumably, if it was fea-
Each corpus contained 100 sentences.
                                                                                                                sible to optimize over corpora with unknown structure, fur-
   As shown in Figure 2, this procedure succeeds in finding
                                                                                                                ther separation might be attained. However, these results do
subsets of sentences that are significantly more representa-
                                                                                                                suggest that optimizing the input sentences would not greatly
   2 Grammar creation was facilitated by the Simple Language Gen-                                               help a learner who must consider all possible structures of
erator (Rohde, 2003)                                                                                            sentences. Consistent with the previous simulations, repre-
                                                                                                             2072

                                               Corpora Representativeness with Unknown Structure                                                            Corpora Representativeness
                                                                                                                                               Given Unknown Structure and an Overly General Grammar
                                   150                                                                                               150
           Log Represenativeness
                                   100                                                                                               100
                                    50                                                                                                50
                                                                                                   Optimized Corpora                                                                                  Optimized Corpora
                                                                                                   Random Corpora                                                                                     Random Corpora
                                     0                                                                                                 0
                                             plex
                                                               ex              ex          pl  x      U n                                       ex               ex             ex               ex
                                                           pl              pl                                                                                                                            U
                                                                                                      C con
                                                                                              e                                                pl            pl                pl            pl            nc
                                         om              C               C               C             om st
                                         C                om              om              om
                                                                                                          pl rai
                                                                                                                                           om               om             om               om           om ons
                                   0%                %               %               %                      ex ne                          C            C                  C            C                   pl tra
                                                    25              50              75                        ity d                  0%              25                %             75                       ex in
                                                                                                                                                        %             50                %                       ity ed
                                                                                                                                                                                                           C
Figure 3: Representativeness of corpora with unknown struc-                                                               Figure 4: Representativeness of corpora with unknown struc-
ture. Limiting the complexity of a corpus limits its represen-                                                            ture using a grammar with extra rules.
tativeness, with the most extreme limitation having the great-
est effect.                                                                                                               pus of full complexity rather than a “starting small” corpus.
sentativeness increases non-linearly with complexity. Again,                                                              However, several concerns remain. The way in which we gen-
the least complex corpora are not as representative as those                                                              eralized the grammar was limited to switching the orders of
that match the base rate of complexity in the grammar.                                                                    verb phrases and noun phrases. This adds significant ambi-
                                                                                                                          guity to the grammar, but is not equivalent to considering any
Using an Overly General Grammar                                                                                           arbitrary grammar. For example, one could imagine a gram-
One might consider the above assumptions too strong: What                                                                 mar that had over-general rules for producing relative clauses.
if the exact structure of the grammar is not known? In this                                                               In that case, it is still unclear whether representativeness in a
variation, we instead assume the learner has an overly general                                                            corpus of severely limited complexity could equal the rep-
grammar that includes rules not present in the true grammar                                                               resentativeness of a more complex corpus. To fully explore
(see Figure 1). For example, rather than having only the op-                                                              the problem, we would need a tractable way to consider all
tion of expanding a transitive verb phrase to a verb followed                                                             (infinite) possible grammars that could produce the data.
by a noun phrase, the learner’s grammar also has the possi-
                                                                                                                          Representativeness with N-Grams
bility of expanding such a phrase to a noun phrase followed
by a verb phrase. This simulates learning a grammar with                                                                  The above simulations assume the learner learns a PCFG, but
unknown structure while maintaining a tractable hypothesis                                                                existing neural network models formulate language learning
space (in this case, not knowing the word order in the lan-                                                               as learning to predict the next word based on previous words.
guage, but knowing the relevant syntactic classes). The extra                                                             This corresponds to a model where the learner learns distri-
rules give the learner a larger hypothesis space to consider,                                                             butions over n-grams rather than rules, and thus we can apply
and our previous hypothesis space is the subset of the new                                                                the same mathematical tools to analyze the representativeness
space in which the probability of each of our newly added                                                                 of corpora according to an n-gram model.
expansions was zero. By using an overly general grammar,                                                                     To calculate representativeness, we can use exact counts
we introduce much more ambiguity as to the structure of                                                                   as in the first simulation. An n-gram is a sequence of two
any given sentence. Thus, one might expect different results                                                              (bigram) or three (trigram) words, and we assume a language
than in the previous simulation, where the number of possible                                                             model that estimates the probability of the next word given
derivations for any given sentence was relatively small.                                                                  the previous one or two words. Our target distribution is now
   The procedure for calculating representativeness in the                                                                the correct proportions for each n-gram, which we estimate
case of an overly general grammar was very similar to the                                                                 by computing the n-grams on the large corpus from which
previous case. We again are considering representativeness                                                                the other corpora were drawn.
for sentences with unknown structure, and thus used impor-                                                                   Despite the fact that the model of the language has changed
tance sampling to calculate the integral. The proposal dis-                                                               significantly, similar results hold in this case as in the other
tribution for sampling was modified so that expansions with                                                               cases. Figure 5 shows the representativeness of the same cor-
the added rules (not present in the true grammar) would be                                                                pora used in the other simulations with respect to n-grams:
considered. We again used a Dirichlet distribution, but the                                                               the random corpora of full complexity are still more repre-
parameters were equal to ten times the true parameters plus                                                               sentative than optimized corpora with limited complexity.
one. Thus, rules that had zero probability in the true grammar
had a parameter of one in the Dirichlet prior.                                                                            Summary
   As shown in Figure 4, even with a grammar with extra                                                                   In our analysis, we have shown that starting small limits the
rules, the results are very similar to the previous simula-                                                               degree of representativeness of the corpora and that the ef-
tion. Optimizing the corpora has the strongest effect when                                                                fect on representativeness is greatest when the limitations are
the complexity is somewhat limited, but for the greatest rep-                                                             particularly severe. These results hold regardless of the varia-
resentativeness, it is still best to use a corpus with greater                                                            tions we considered. While our task is not exactly the same as
complexity. This result suggests that even if a learner does                                                              in Elman (1993) or Rohde and Plaut (1999), it has bearing on
not know the true grammar, it is still better to provide a cor-                                                           this debate. From a computational level perspective, the only
                                                                                                                       2073

                                        Corpora Representativeness Using Bigrams                                                           Corpora Representativeness Using Trigrams
                       800                                                                                                   1200
                                                                                                                             1000
                       600
                                                                                                                             800
                       400                                                                                                   600
                                                                                                                             400
                       200
                                                                                                  Optimized Corpora          200                                                                Optimized Corpora
                                                                                                  Random Corpora                                                                                Random Corpora
                         0                             ex                                                                      0                           ex
                                       ex          pl
                                                                        ex               ex   nc                                           ex          pl                plex            plex   nc
                                   pl                               pl               pl          o                                       pl                                                        o
                                  om              om               om               om        C nst
                                                                                               om ra                                 C                om             om              om         C nst
                                                                                                                                                                                                 om ra
                                                                                                                                      om          C
                              C               C                C                C                  pl ine
                                                                                                     ex d
                                                                                                                                                                     C               C               pl ine
                                                                                                                                                                                                       ex d
                             0%             25              50               75                        ity                          0%          25               %               %                       ity
                                              %                %                %                                                                 %             50              75
                                                                                              U                                                                                                 U
                                             Figure 5: Representativeness of corpora using an n-gram model.
concern for such learning is whether the corpora are repre-                                                              for child-directed speech. This work provides a foundation
sentative, and we have shown that starting small (at least in                                                            for addressing these more advanced questions.
the extreme form) is not compatible with maximizing repre-
                                                                                                                         Acknowledgements. This work was supported by a Graduate Re-
sentativeness. However, if for mechanistic reasons one needs
                                                                                                                         search Fellowship and grant number SES-0631518 from the Na-
to start small, the above results suggest that starting “smaller”
                                                                                                                         tional Science Foundation.
can result in similar representativeness in an optimized corpus
to that of a random corpus of full complexity.                                                                                                                                   References
                                                                                                                         Baker, J. (1979). Trainable grammars for speech recognition. In
                             Discussion                                                                                    J. J. Wolf & D. H. Klatt (Eds.), Speech Communication Papers
                                                                                                                           presented at the 97th Meeting of the Acoustical Society of America
We have shown how the concept of Bayesian representative-                                                                  (pp. 547–550). MIT, Cambridge, Massachusetts.
ness can be applied to language in order to characterize an                                                              Cameron-Faulkner, T., Lieven, E., & Tomasello, M. (2003). A con-
                                                                                                                           struction based analysis of child directed speech. Cognitive Sci-
optimal sample and presented a case study of how represen-                                                                 ence, 27(6), 843–873.
tativeness changes with constraints on the sample. Mathemat-                                                             Elman, J. L. (1991). Distributed representations, simple recurrent
ically, the Bayesian representativeness of language structures                                                             networks, and grammatical structure. Machine Learning, 7, 195–
                                                                                                                           224.
matches our intuitive sense of representativeness: a sample of                                                           Elman, J. L. (1993). Learning and development in neural networks:
language is most representative if the actual number of occur-                                                             The importance of starting small. Cognition, 48(1), 71–99.
rences of each structure matches the expected number. While                                                              Furrow, D., Nelson, K., & Benedict, H. (1979). Mothers’ speech to
                                                                                                                           children and syntactic development: Some simple relationships.
we cannot give a closed form expression for the representa-                                                                Journal of Child Language, 6(3), 423–443.
tiveness of a corpus where the sentences structures are not                                                              Goldowsky, B. N., & Newport, E. L. (1993). Modeling the effects
given, simulations show that the trends concerning represen-                                                               of processing limitations on the acquisition of morphology: The
tativeness given constraints on complexity hold for these cor-                                                             less is more hypothesis. In J. M. Mead (Ed.), The Proc. of the
                                                                                                                           11th West Coast Conference on Formal Linguistics. Stanford,
pora as well. Finally, it is suggestive that given a grammar                                                               CA: CSLI.
with overly general rules, we still find a disadvantage for cor-                                                         Hoff, E., & Naigles, L. (2002). How children use input to acquire a
pora of limited complexity.                                                                                                lexicon. Child Development, 73(2), 418–433.
                                                                                                                         Huttenlocher, J., Vasilyeva, M., Cymerman, E., & Levine, S. (2002).
   Our results suggest that if there is a beneficial effect of                                                             Language input and child syntax. Cognitive Psychology, 45(3),
starting small, it is not located at the computational level: the                                                          337–374.
                                                                                                                         Marr, D. (1982). Vision. San Francisco, CA: W. H. Freeman.
statistical evidence a corpus provides in favor of the target                                                            Neal, R. M. (1993). Probabilistic inference using Markov chain
language falls off as its complexity deviates from the com-                                                                Monte Carlo methods (Tech. Rep. No. CRG-TR-93-1). Dept. of
plexity of the language. However, our results do show how                                                                  Computer Science, University of Toronto.
                                                                                                                         Newport, E. L., Gleitman, L. R., & Gleitman, H. (1977). Mother
it might be possible to start small in response to mechanistic                                                             I’d rather do it myself: Some effects and non-effects of maternal
information-processing constraints and still not impede learn-                                                             speech style. In C. Snow & C. Ferguson (Eds.), Talking to chil-
ing, as it is possible to construct limited-complexity corpora                                                             dren: Language input and acquisition (pp. 31–49). Cambridge,
                                                                                                                           England: Cambridge University Press.
that provide as much evidence as a random sample from the                                                                Pine, J. M. (1994). The language of primary caregivers. In C. Gall-
language. While suggestive, we note that these conclusions                                                                 away & B. J. Richards (Eds.), Input and interaction in language
are tempered by the models we considered, and in particular                                                                acquisition (pp. 15–37). Cambridge, England: Cambridge Uni-
                                                                                                                           versity Press.
the space of alternative hypotheses we allow the learner.                                                                Rohde, D. L. (2003). The simple language generator: Encoding
   Overall, our analysis provides insight into what optimal                                                                complex languages with simple grammars (Tech. Rep.). Depart-
linguistic input should look like in several interesting cases.                                                            ment of Brain and Cognitive Science, MIT.
                                                                                                                         Rohde, D. L., & Plaut, D. C. (1999). Language acquisition in the
A variety of next steps are possible. First, a more detailed ex-                                                           absence of explicit negative evidence: How important is starting
ploration of the nature of an optimal sample given unknown                                                                 small? Cognition, 72, 67–109.
rules would illuminate whether the preliminary results we                                                                Sherrod, K. B., Friedman, S., Crawley, S., Drake, D., & Devieux,
                                                                                                                           J. (1977). Maternal language to prelinguistic infants: Syntactic
have found hold given a larger space of possible grammars.                                                                 aspects. Child Development, 48(4), 1662–1665.
Additionally, comparing our theoretical results to actual cor-                                                           Snow, C. E. (1972). Mothers’ speech to children learning language.
pora of language acquisition would indicate whether child-                                                                 Child Development, 43(2), 549–565.
                                                                                                                         Tenenbaum, J. B., & Griffiths, T. L. (2001). The rational basis of
directed speech is more representative than randomly selected                                                              representativeness. In Proceedings of the 23rd Annual Conference
adult-directed speech. This would suggest a pedagogical role                                                               of the Cognitive Science Society (pp. 84–98).
                                                                                                                      2074

