UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Emergence of Adaptive Eye Movements in Reading

Permalink
https://escholarship.org/uc/item/8zm8h00d

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Liu, Yanping
Reichle, Erik

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

The Emergence of Adaptive Eye Movements in Reading
Yanping Liu (yanping@pitt.edu)
Department of Psychology, 3939 O’Hara St.
Pittsburgh, PA 15260 USA

Erik D. Reichle (reichle@pitt.edu)
Department of Psychology, 3939 O’Hara St.
Pittsburgh, PA 15260 USA
Abstract
Simulations were completed using artificial reading “agents”
that are subject to known physiological (e.g., limited visual
acuity) and psychological (e.g., limited attention) constraints
and capable of learning to move their eyes and allocate
attention to read as efficiently as possible. These simulations
indicate that agents learn when and where to move their eyes
to attain maximal reading efficiency, generalize this behavior
from training sentences to novel test sentences, and use word
length to predict word-identification times and thereby make
optimal decisions about when to initiate saccadic
programming—even if word length is only moderately
predictive of word-identification times. These results suggest
that humans may exploit even modestly informative cues in
learning to decide when to move their eyes during reading.
Keywords: Attention; Eye Movements; Genetic Algorithms;
Neural Networks; Reading; Reinforcement Learning

Introduction
One of the outstanding unanswered questions in the
psychology of reading (Rayner & Pollatsek, 1989) is: To
what extent are the moment-to-moment decisions about
when to move the eyes during reading determined by
cognition? Attempts to answer this question can be divided
into three theoretical “camps” (Reichle, 2006; Reichle,
Rayner, & Pollatsek, 2003).
The first maintains that when the eyes move is largely
determined by the constraints imposed by the visual and
oculomotor systems (e.g., limited visual acuity). Advocates
of this oculomotor-control account (Feng, 2006; McDonald,
Carpenter, & Shillcock, 2005; Reilly & O’Regan, 1998;
Suppes, 1990; Yang, 2006) argue against an eye-mind link
in reading, and maintain that individual fixation durations
provide only minimal information about ongoing lexical
and/or linguistic processing difficulty.
According to the second “camp,” most decisions about
when to move the eyes are determined by the activity of an
autonomous random timer that causes the eyes to move at a
rate that reflects a reader’s comprehension goals and overall
text difficulty, with cognition only intervening to inhibit
saccadic programming when processing difficulty is
encountered and thereby lengthening fixation durations.
Advocates of this autonomous-timer account (Engbert,
Longtin, & Kliegl, 2002; Engbert et al., 2005; Reilly &
Radach, 2006) argue for a weak eye-mind link, with
individual fixations occasionally reflecting ongoing lexical
or linguistic processing difficulty.

Finally, the third “camp” maintains that the eyes and
mind are tightly coupled, with the completion of some
cognitive process (e.g., lexical access) being the “trigger”
that normally causes the eyes to move from word to word
during reading. Advocates of this cognitive-control account
(Just & Carpenter, 1980; Reichle et al., 1998; Reichle,
Warren, & McConnell, 2009; Reilly, 1993; Salvucci, 2001)
argue for a strong eye-mind link, with individual fixation
durations usually reflecting local processing difficulty.
Perhaps not too surprisingly, all three theoretical
positions have been remarkably successful explaining the
basic patterns of eye movements that are observed during
reading; each position has provided one or more
computational models that formally instantiates the core
assumption of their respective positions and that simulate
many or all of the “benchmark” findings related to eyemovement behavior in reading (Reichle et al., 2003). This
makes it difficult to evaluate the models purely on the basis
of their ability to account for data, and because the models
make different a priori assumptions about the factors that
guide readers’ eye movements (e.g., how attention is
allocated), model evaluation is like the proverbial problem
of “comparing apples and oranges.” The present simulations
therefore adopt an entirely different approach to
understanding eye-movement control in reading.
Rather than developing a computational model around a
priori assumptions about the precise manner in which
perception, cognition, and motor control guide eye
movements in reading, the present approach is a direct
extension of the work reported by Reichle and Laurent
(2006). In this work, artificial reading “agents” that were
subject to known physiological (e.g., limited visual acuity)
and psychological (e.g., limited capacity attention)
constraints were given the task of learning how to move
their eyes and attention so as to “read” (i.e., identify
sequences of “words”) as efficiently as possible. The key
results of this work were that the agents learned: (1) to
direct their eyes towards the centers of words, the viewing
location that afforded the most rapid identification of the
words; (2) to use word length to predict when a given word
would be identified, and then initiate saccadic programming
to move its eyes from that word right as it was identified;
and (3) to incur local fixation duration costs by identifying
short, easy-to-identify words from peripheral vision, and
thereby avoiding more costly saccades to those words.

1136

The present simulations replicate and extend the
Reichle and Laurent (2006) results using artificial agents
that are capable of learning to move their eyes and attention
via reinforcement learning (Sutton & Barto, 1998).
However, in contrast to the Reichle and Laurent agents, the
present agents were implemented using artificial neural
networks (ANNs), and we demonstrated in two simulations
that the behavior of these agents: (1) generalizes to novel
sentences and words; (2) can be learned even in less than
optimal learning conditions; and (3) is generally congruent
with assumptions of cognitive-control theories. The
theoretical implications of these results will be discussed
after the simulation results are described.

spent processing a sentence. Learning continues until the
values of the states reach asymptote.
Table 1. State information (S) used by agents.
#
1
2
3
4
5
6
7
8
9

General Simulation Method
The artificial reading “agents” that were used in the
present simulations were given the task of learning how to
“read” (i.e., identify sequences of words in their canonical
order) as efficiently as possible. These words could vary in
terms of their length (1-8 letters) and/or the time required
for their identification (2-14 time steps). The agents learned
to perform this task (subject to various constraints,
discussed below) using trajectory sampling, a variant of the
value iteration reinforcement-learning algorithm (Sutton &
Barto, 1998) that is often used with large-scale problems.
This algorithm is specified by:
i=0
for all initial S:
Vi(S) = ANN(S)
repeat
i=i+1
if (random value < greed) then:
Vi(S) = Vi-1(S) + ε{maxaction∈M[reward(S, action)
+ γ Vi-1(S′)] – Vi-1(S)}
else random action
until learning has completed.
where i indexes the learning iteration, Vi(S) is the value
associated with state S at time i, and M is the set of
permissible actions from a given state. There are three
parameters: ε (= 0.5) controls the learning rate, greed (=
0.5) controls how often an agent exploits what it already
knows in selecting actions versus exploring the
consequences of randomly selected actions, and γ (= 0.9)
determines how much the agent weighs the reward that is
anticipated from the next state, S′, versus the immediate
reward that it receives from the action that it selects. Each
state, S, consists of information that is available to the agent
at any given point in time (see Table 1). The agents can
perform one of three actions: (1) continue attending (i.e.,
lexically processing) the current word; (2) shift attention to
the next word; and (3) request an eye movement of ±10
character spaces. An agent selects the actions that result in
the most (anticipated) reward, being “rewarded” +1 for
every identified word and “punished” -1 for every time step

Available Information
Attended word (i.e., wordn) identified? (Y/N)
# time steps processing wordn
# spaces between center of wordn and fixation
Length of wordn
Length of wordn+1
Length of wordn-1
Saccade being programmed? (Y/N)
Length (# spaces) of intended saccade
# time steps programming saccade

As mentioned, the agents are subject to several
constraints. First, visual acuity is limited, so that the rate of
lexical processing decreases as the spatial distance between
the agent’s center of vision and the center of the word being
processed increases (i.e., a word that takes N time steps to
identify when its middle letter is fixated will take 1
additional time step to identify for each character space of
disparity between the letter being fixated and the center of
the word). Saccades also require 3 time steps to program
and 1 time step to execute, and are subject to Gaussian (µ =
0; σ = 1) random error. Finally, because the perceptual span
is known to be of limited spatial extent (Rayner & Pollatsek,
1989; Rayner, 1998), the agents were only allowed to
process one word at a time, instantiating the assumption that
attention is allocated serially during reading (e.g., Reichle et
al., 1998) or approximating the assumption that attention is
allocated as a gradient—albeit a tightly focused one (e.g.,
Engbert et al., 2005). Although this assumption about
attention is quite controversial (e.g., see Reichle et al.,
2009), it was intended as a simplifying assumption to make
the simulations as tractable as possible.
In the Reichle and Lauent (2006) simulations, the value
of each state, Vi(S), was stored in a look-up table (i.e., one
value per combination of dimensions in Table 1). In the
present simulations, the values were learned and stored in
the connection weights of an ANN whose architecture and
principle weights were selected using NeuroEvolution of
Augmenting Topologies (NEAT) (Stanley & Miikkulainene,
2002) and whose weights were optimized via the
Covariance Matrix Adaptation Evolution Strategy (CMAES; Hansen, Müller, & Koumoustsakos, 2003) when trapped
in local maxima. Figure 1 is a schematic diagram of how the
NeuroEvolution and CMA-ES algorithms are used in
conjunction with task-specific training to select network
architectures that are well suited to solve the types of
problems explored in this article.
Each network was comprised of nine input units (one
per dimension in Table 1), one bias unit, one output unit
(representing the learned value of each state), and an
unspecified number of hidden units. In contrast to many
neural networks, the hidden units were not strictly layered,

1137

but could be configured in a variety of ways (e.g., as
additional bias units; see Fig. 1). The activation of input unit
i when given some value x of one of the dimensions in
Table 1 was scaled to the interval [-1, 1] using:
acti(x) = {x – [max(x) / 2]} / [max(x) / 2]
where the function “max” returns the maximum value of the
dimension. (Note that acti(x) = -1/1 when Dimensions 1 and
7 equal false/true.)

optimization stagnated. Each reported simulation is based
on populations of 100 individuals evaluated across 300
generations to solve the tasks of interest. Each individual
networks agent also learned to perform its task using value
iteration across five learning trials and using the specific
training regimens.

Simulation 1
The first simulation replicated and extended the Reichle
and Laurent (2006) results using agents implemented as
ANNs (as described above) and various novel test
sentences.
Method. Five agents were trained on a corpus of five 8word “sentences” comprised of random permutations of 1-,
3-, 5-, and 7-letter “words.” (These sentences were
randomly selected from 20 used by Reichle & Laurent,
2006.) The first and last words were always 1-letter in
length and required 2 time steps to identify, and always
excluded from our analyses because their processing
started/ended abruptly. The remaining 1-, 3-, 5-, and 7-letter
words respectively required 2, 6, 10, and 14 time steps to
identify when fixated from their central letters. After
training, agents were tested on: (1) the same five sentences;
(2) five novel 8-word sentences comprised of different
random permutations of 1-, 3-, 5-, and 7-letter words; and
(3) five 8-word sentences comprised of random
permutations of 2-, 4-, 6-, and 8-letter novel words.
Results. Figure 2 shows the simulated fixation landing-site
distributions on the words, as a function of their length and
whether the sentences being using used to evaluate the
agents were old (i.e., used during training), novel, or
comprised of novel words (i.e., 2-, 4-, 6-, and 8-letter
words). (In all of the figures shown below, the data points
indicate the condition means and the error bars indicate the
standard errors of the means.) As indicated, the agents
learned to direct their eyes towards the centers of the words
because this was the viewing position that afforded the most
rapid identification of the words. However, because of
saccadic error, the fixation landing sites are approximately
normally distributed, in line with what is observed with
human readers (McConkie et al., 1988, 1991; Rayner,
Sereno, & Raney, 1996). Finally, the behavior generalized
across both novel sentences and words.

Figure 1. Method use to evolve and train agents.
This NeuroEvolution algorithm was used to select
network architectures that were adapted to use the value
iteration reinforcement-learning algorithm (via using a
residual algorithm to implement back-propagation in the
ANNs) for the tasks that are the focus of this article—
learning to move the eyes and attention to read efficiently.
The CMA-ES algorithm was used to optimize weights when

1138

Figure 2. Simulated fixation landing-site distributions.

Figure 3 shows the mean probabilities of making a
single fixation, making two or more fixations, or skipping,
again as a function of word length and the nature of the test
sentences. As the figure shows, agents tended to either make
a single fixation on or skip the shorter words, and to make
two or more fixations on the longer words. These results are
consistent with what is observed with humans (Rayner et
al., 1996) and did not vary by testing condition.

“trigger” that initiates saccadic programming; Reichle et al.,
1998, 2003, 2009.)

Figure 4. Simulated time-based dependent measures.

Figure 3. Simulated fixation probabilities.
Figure 4 shows the mean simulated values of five
dependent measures (in time steps), again as a function of
word length: (1) first-fixation duration (FFD), or the
duration of the first fixation on a word during the first pass
through the sentence; (2) gaze duration (GD), or the sum of
all first-pass fixations; (3) total-viewing time (TT), or the
sum of all fixations, irrespective of whether they occur
during the first pass; (4) word-identification times (ID), or
the time spent processing the words; and (5) saccadicprogramming initiation times (SPIT), or the time spent
processing wordn prior to initiating the saccade that moved
the eyes to wordn+1. As Figure 4 indicates, the measures
increased with increasing word length (which is perfectly
correlated with identification time), but with the mean
processing time being longer than the minimal identification
time because saccadic error often caused words to be
processed from poor viewing locations, where lexical
processing was slower. Importantly, if an agent spent N time
steps processing wordn, then it on average spent
approximately N-3 time steps processing wordn before
initiating saccadic programming to move its eyes to wordn+1.
This is an optimal strategy for deciding when to move the
eyes because initiating saccadic programming any earlier
would cause the eyes to leave wordn prematurely, resulting
in it being processed more slowly from wordn+1 (because of
reduced visual acuity). Conversely, initiating saccadic
programming any later would cause the fixations to be
unnecessary long in duration. Thus, by initiating saccadic
programming at the observed times, an agent insures that, in
most cases, the eyes move from wordn right when it has
been identified. (It is also worth noting that this strategy is
similar to the “familiarity check” assumption of the E-Z
Reader model of eye-movement control during reading,
where a preliminary stage of lexical processing is the

These results of Simulation 1 replicate and extend the
key findings reported by Reichle and Laurent (2006) by
showing that the reading agents, implemented as ANNs, are
able to generalize from a small set of training sentences to
sentences containing novel configurations of words. This is
methodologically important because it shows how ANNs
might be used to solve large-scale reinforcement learning
problems that might otherwise be impossible to solve (e.g.,
a look-up table version of the agents would require the
storage and updating of the more than 6 million different
states listed in Table 1). This demonstration also makes it
possible to explore more complex contingencies between
eye-movement behavior and lexical variables, as described
next.

Simulation 2
The second simulation examined the consequences of
training on a more realistic sentence corpus—one in which
word length is only moderately predictive of the time
required to identify words.
Method. Five agents were trained and tested on five 8-word
sentences in which 1-, 3-, 5-, and 7-letter words required 4-9
time steps to identify, and where word length was only
moderately correlated to word-identification times across
the corpus (r = 0.31).
Results. The simulated landing-site distributions, fixation
probabilities, and time-based measures (grouped by both
word length and identification time) are shown in Figures 57, respectively. As indicated in the left panel of Figure 5, the
agents learned to direct their eyes towards the centers of
words because this location afforded the most rapid
identification of words. And as the left panel of Figure 6
shows, the agents were also more likely to make single
fixations on or skip the shorter words, and make two or
more fixations on the longer words. Both of these findings
are consistent with human readers (McConkie et al., 1988,
1991; Rayner, 1996) and the results of Simulation 1. The
right panels of Figures 5 and 6 indicate that similar wordtargeting behaviors were evident when the words are
grouped by their identification times, but that there are some
irregularities (e.g., bimodal landing-site distributions with

1139

the more-difficult-to-identify words) because these items
included a mixture of both short and long words.

Figure 5. Simulated fixation landing-site distributions, by
word length (left) and identification times (right).

Figure 6. Simulated fixation probabilities, by word length
(left) and identification times (right).

Figure 7. Simulated time-based measures, by word length
(left) and identification times (right).
Finally, the most striking result from Simulation 2 is
that the agents learn to use word length information to
predict the time required to identify words, and then used
this knowledge to program saccades so that the eyes would
leave a word right as it was identified. This “strategy” is
similar to the one that was adopted by the agents in
Simulation 1, even though the relation between word length
and identification times was much weaker in Simulation 2 (r
= 0.31) than Simulation 1 (r = 1). And as the left and right
panels of Figure 7 indicate, this strategy was evident
irrespective of whether the words are grouped by their
length or by their identification times.

General Discussion
The simulations reported in this article replicated the
Reichle and Laurent (2006) results by showing that

“intelligent” eye-movement behavior can emerge from
artificial reading agents that are subject to fairly
uncontroversial physiological and psychological constraints
and that are capable of learning to coordinate attention and
eye movements to support efficient reading. Simulation 1
extended the Reichle and Laurent results by implementing
the reading agents within an ANN and then showing that the
agents’ eye-movement behaviors generalize to novel
sentences and words. And importantly, the agents used
word-length cues to predict when words would be
identified, and then used this knowledge to learn when to
initiate saccadic programming. Simulation 2 indicated that
the agents learned the same eye-movement behaviors,
including learning to use word length to initiate saccadic
programming in an optimal manner—even though word
length was only moderately predictive of wordidentification time.
The simulation results have important theoretical
implications for our general understanding of eyemovement control in reading and the specific questions of
what determines when our eyes move during reading. First,
the simulations suggest how information that is predictive of
when a word will be identified can be used to initiate
saccadic programming in a manner that affords efficient
reading. In the absence of such predictive information, it
may be optimal to either simply wait until wordn has been
identified before initiating saccadic programming to move
the eyes to wordn+1, or to base the decision on the mean time
required to identify wordn. Although both of these strategies
prevent the eyes from moving prematurely (which would
then slow reading considerably because words would have
to be identified from poorer viewing locations), the
strategies are also conservative because they often produce
unnecessarily long fixations. This suggests that any strategy
that simply ignores lexical processing difficulty and uses
saccadic programming and visual acuity constraints to
decide when to move the eyes will not be optimal because it
ignores information (about the rate of lexical processing)
that can be used to inform those decisions. This conclusion
provides one argument against oculomotor-control theories
of eye-movement control in reading (Feng, 2006; McDonald
et al., 2005; Reilly & O’Regan, 1998; Suppes, 1990; Yang,
2006). And although our results admittedly say less about
the feasibility of autonomous-timer theories (Engbert et al.,
2002, 2005), such theories are not parsimonious if
decisions about when to move the eyes can be made using
information that is readily available to the reader (i.e.,
information about lexical processing difficulty). In other
words, it is not parsimonious to posit an autonomous timer
that is occasionally overridden by lexical processing
difficulty if this information is itself sufficient to decide
when to move the eyes in an optimal manner.
Second, the simulations suggest that humans may
exploit cues that may be only modestly predictive of lexical
processing difficulty in learning to decide when to initiate
saccadic programming. These cues probably include word
length, but also orthographic cues (e.g., prefixes and

1140

suffixes, unusual letter sequences, etc.), and possibly cues
that are generated via top-down processing (e.g., the
syntactic and/or semantic constraints imposed by a word’s
prior sentence context). It is an open question as to how
these different sources of information are combined in
making moment-to-moment decisions about when to move
the eyes during reading, but a vast experimental literature
(e.g., for a review, see Rayner, 1998) indicates that these
variables (and many others) do influence such decisions.
Future simulations using our artificial reading agents will
provide testable hypotheses regarding this question.

Acknowledgments
Address correspondence to: Erik Reichle, University of
Pittsburgh, 635 LRDC, 3939 O’Hara St., Pittsburgh, PA
15260 USA (or via email at: reichle@pitt.edu.) This work
was supported by a China Scholarship Council award to the
first author and an NIH grant (R01HD053639) awarded to
the second author.

References
Engbert, R., Longtin, A., & Kleigl, R. (2002). A dynamical
model of saccade generation in reading based on spatially
distributed lexical processing. Vision Research, 42, 621636.
Engbert, R., Nuthmann, A., Richter, E., & Kliegl, R. (2005).
SWIFT: A dynamical model of saccade generation during
reading. Psychological Review, 112, 777-813.
Feng, G. (2006). Eye movements as time-series random
variables: A stochastic model of eye movement control in
reading. Cognitive Systems Research, 7, 70-95.
Hansen, N., Müller, S. D., & Koumoutsakos, P. (2003).
Reducing the complexity of the Derandomized Evolution
Strategy with Covariance Matrix Adaptation (CMA-ES).
Evolutionary Computation, 11, 1-18.
Inhoff, A. W. & Rayner, K. (1986). Parafoveal word
processing during eye fixations in reading: Effects of
word frequency. Perception & Psychophysics, 40, 431439.
Just, M. A. & Carpenter, P. A. (1980). A theory of reading:
From eye fixations to comprehension. Psychological
Review, 87, 329-354.
McConkie, G. W., Kerr, P. W., Reddix, M. D., & Zola, D.
(1988). Eye movement control during reading: I. The
location of initial eye fixations in words. Vision Research,
28, 1107-1118.
McDonald, S. A., Carpenter, R. H. S., & Shillcock, R. C.
(2005). An anatomically constrained, stochastic model of
eye movement control in reading. Psychological Review,
112, 814-840.
Rayner, K. (1998). Eye movements in reading and
information processing: 20 years of research.
Psychological Bulletin, 124, 372-422.
Rayner, K. & Duffy, S. A. (1986). Lexical complexity and
fixation times in reading: Effects of word frequency, verb
complexity, and lexical ambiguity. Memory & Cognition,
14, 191-201.

Rayner, K. & Pollatsek, A. (1989). The Psychology of
Reading. Englewood Cliffs, NJ: Prentice Hall.
Rayner, K., Sereno, S. A., & Raney, G. E. (1996). Eye
movement control in reading: A comparison of two types
of models. Journal of Experimental Psychology: Human
Perception and Performance, 22, 1188-1200.
Reichle, E. D. (2006). Theories of the “eye-mind” link:
Computational models of eye-movement control during
reading. Cognitive Systems Research, 7, 2-3.
Reichle, E. D. & Laurent, P. A. (2006). Using reinforcement
learning to understand the emergence of “intelligent”
eye-movement behavior during reading. Psychological
Review, 113, 390-408.
Reichle, E. D., Liversedge, S. P., Pollatsek, A., & Rayner,
K. (2009). Encoding multiple words simultaneously in
reading is implausible. Trends in Cognitive Sciences, 13,
115-119.
Reichle, E. D., Pollatsek, A., Fisher, D. L., & Rayner, K.
(1998). Toward a model of eye movement control in
reading. Psychological Review, 105, 125-157.
Reichle, E. D., Rayner, K., & Pollatsek, A. (2003). The E-Z
Reader model of eye movement control in reading:
Comparisons to other models. Behavioral and Brain
Sciences, 26, 445-476.
Reichle, E. D., Warren, T., & McConnell, K. (2009). Using
E-Z Reader to model the effects of higher-level language
processing on eye movements during reading.
Psychonomic Bulletin & Review, 16, 1-21.
Reilly, R. (1993). A connectionist framework for modeling
eye movements in reading. In G. d’Ydewalle & J. Van
Rensbergen (Ed.), Perception and cognition: Advances in
eye movement research (pp. 193-212). Amsterdam:
North-Holland.
Reilly, R. & O’Regan, J. K. (1998). Eye movement control
in reading: A simulation of some word-targeting
strategies. Vision Research, 38, 303-317.
Reilly, R. G. & Radach, R. (2006). Some empirical tests of
an interactive activation model of eye movement control
in reading. Cognitive Systems Research, 7, 34-55.
Salvucci, D. D. (2001). An integrated model of eye
movements and visual encoding. Cognitive Systems
Research, 1, 201-220.
Stanley, K. O. & Miikkulainen, R. (2002). Evolving neural
networks through augmenting topologies. Evolutionary
Computation, 10, 99-127.
Suppes, P. (1990). Eye movement models for arithmetic and
reading performance. In E. Kowler (Ed.), Eye movements
and their role in visual and cognitive processes (pp. 395453). Amsterdam: Elsevier.
Sutton, R. S. & Barto, A. G. (1998). Reinforcement
learning: An introduction. Cambridge, MA: MIT Press.
Yang, S.-N. (2006). An oculomotor-based model of eye
movements in reading: The competitive/interaction
model. Cognitive Systems Research, 7, 56-69.

1141

