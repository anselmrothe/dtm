UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Attention and Reinforcement Learning: Constructing Representations from Indirect
Feedback
Permalink
https://escholarship.org/uc/item/1w83t8ct
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Canas, Fabian
Jones, Matt
Publication Date
2010-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                                       Attention and Reinforcement Learning:
                          Constructing Representations from Indirect Feedback
                         Fabián Cañas (canas@colorado.edu) & Matt Jones (mcj@colorado.edu)
                                   University of Colorado, Department of Psychology & Neuroscience
                                                           Boulder, CO 80309 USA
                               Abstract                                   the question of how the representation is learned is neglected
                                                                          (Schyns, Goldstone, & Thibaut, 1998).
   Reinforcement learning (RL) shows great promise as a theory
   of learning in complex, dynamic tasks. However, the learn-                A great deal of psychological research in domains other
   ing performance of RL models depends strongly on how stim-             than RL focuses on how people learn representations to fa-
   uli are represented, because this determines how knowledge             cilitate learning, inference, and decision-making. The aim of
   is generalized among stimuli. We propose a mechanism by
   which RL autonomously constructs representations that suit             our general research program is to explore how such mecha-
   its needs, using selective attention among stimulus dimensions         nisms might interact with RL, and in particular how RL can
   to bootstrap off of internal value estimates and improve those         build its own representations to bootstrap learning. In the
   same estimates, thereby speeding learning. Results of a behav-
   ioral experiment support this proposal, by showing people can          present paper we focus on selective attention, building on
   learn selective attention for actions that do not lead directly to     models from the literature on category learning (Kruschke,
   reward, through internally generated feedback. The results are         1992). In a companion paper (Jones & Cañas, 2010), we pro-
   cast in a larger framework for integrating RL with psychologi-
   cal mechanisms of representation learning.                             vide a formal framework for integrating representation learn-
   Keywords: Reinforcement Learning; attention; generalization            ing with RL and implement a specific computational model
                                                                          based on selective attention. Here, we present a behavioral
                           Introduction                                   experiment that support the thesis that RL can drive represen-
                                                                          tational learning. Our results show that the internally gener-
Humans have an incredible capacity to learn new and com-                  ated feedback signals at the core of RL can direct shifts of
plex tasks in dynamic environments. In recent years, Rein-                attention toward those stimulus dimensions that are most di-
forcement Learning (RL) has emerged as a theoretical frame-               agnostic of optimal action.
work that may explain how such powerful learning takes                       The remainder of this paper begins with background on
place (e.g., Sutton & Barto, 1998). Reinforcement learn-                  RL and modeling of attention learning in categorization. We
ing draws on a synthesis of machine learning and neuro-                   then outline our proposal for how RL and attention learning
science and offers a set of computational principles for de-              can bootstrap off of each other. We then report the results
scribing learning of dynamic tasks. RL has led to major ad-               of a sequential decision-making experiment designed to test
vances in the ability of machines to learn difficult tasks such           this specific proposal. Implications are discussed for the role
as backgammon and autonomous helicopter flight (Tesauro,                  of attention in more complex and temporally extended tasks,
1995; Bagnell & Schneider, 2001). RL has also received                    prescriptions for training in such tasks, and interactions be-
much interest in neuroscience, based on findings that phasic              tween representation learning and declarative memory.
dopamine signals have similar properties to the internal feed-
back computed by RL algorithms (Schultz, Dayan, & Mon-                                    Reinforcement Learning
tague, 1997). This correspondence suggests that RL offers a
useful model of biological learning.                                      RL is a computational framework for learning dynamic tasks
   Despite the promise of this framework, the learning perfor-            based on feedback from the environment. RL models rep-
mance of RL algorithms strongly depends on the representa-                resent a task as a set of environmental states together with
tions on which they operate. RL works by learning which                   a set of available actions in each state. The action selected
action to perform in each state of a task’s environment. In               at each step determines the immediate reward as well as the
realistically complex tasks with large state spaces, learning             ensuing state. This general framework accommodates nearly
about every state individually is impossible, and instead the             any psychological task, from simple conditioning to elaborate
learner must generalize knowledge among states. General-                  planning (Sutton & Barto, 1998).
ization is closely tied to similarity (Shepard, 1987), which                 RL works by estimating values of states and actions, which
in turn depends on how stimuli or situations are represented.             reflect predictions of total future reward. From any given
Therefore the efficacy of generalization depends on how a                 state, the action with the highest estimated value represents
task is internally represented. Most often in machine-learning            a best guess of the choice that will lead to the highest long-
applications, representations are pre-supplied by the modeler             term reward. The key to learning value estimates, which lies
based on features that are carefully crafted to capture the most          at the heart of all RL models, is an internally generated feed-
important aspects of the task being learned (e.g., Tesauro,               back signal known as Temporal Difference (TD) error. TD
1995). In psychological contexts, stimuli are chosen so that              error represents the discrepancy between the estimated value
the subject’s representation is transparent, and consequently             of an action prior to its execution and a new estimate based
                                                                      1264

on the immediate reward and the value of the ensuing state.            dient is an empirical function that describes how strongly sub-
   For the mathematically inclined, TD error is defined as             jects generalize between stimuli as a function of how much
                                                                       those stimuli differ. This function is monotonically decreas-
                 δ = rt + γ ·V(st+1 ) − Q(at , st ).                   ing, but it decreases more rapidly along attended dimensions
                                                                       than unattended dimensions (Jones, Maddox, & Love, 2005).
Here, st represents the current state (at time t), at is the action    Thus subjects generalize less between stimuli when they are
selected, and Q(at , st ) is the estimated value of that action.       attending to the dimensions those stimuli differ on. An al-
The immediate reward received is denoted rt , and V (st+1 ) is         ternative view is that the generalization gradient is fixed and
the estimated value of the ensuing state. The temporal dis-            isotropic, but the perceptual scaling of individual stimulus di-
count parameter, γ, represents the degree to which the learner         mensions is adjustable. Attention to a dimension serves to
values immediate versus delayed rewards.                               stretch the perceptual space so that stimuli differing on that
   A critical question for all RL models concerns the rela-            dimension are less similar and thus produce less generaliza-
tionship between value estimates (Q or V ) for different states.       tion (Nosofsky, 1986).
The simplest approach is to learn values for all states indepen-          Theories of selective attention in category learning propose
dently, but for most realistic tasks with large state spaces this      that people learn to reallocate their attention to improve per-
approach is unfeasible. Effective learning therefore requires          formance. ALCOVE, a model of categorization with learn-
generalization, or the use of knowledge about one stimulus or          able selective attention, has an attention weight for each di-
situation to make inferences or choose actions for other, simi-        mension that determines the degree of generalization along
lar stimuli. A number of methods have been proposed for im-            that dimension (Kruschke, 1992). The attention weights are
plementing generalization in RL, and in all cases, the pattern         learned by gradient descent on classification error, driven by
of generalization depends strongly on the way in which states          external feedback. This process leads attention to shift to
are represented. Representations relying on different features         more predictive dimensions, which leads to less generaliza-
produce different patterns of similarity and hence different           tion along these dimensions and greater generalization along
generalization. Learning will be most effective if general-            irrelevant dimensions. Selective attention can thus be thought
ization somehow respects the structure of the task, such that          of as a mechanism for representational learning, which facil-
the learner pools knowledge about states with similar conse-           itates future learning of the task by adapting generalization.
quences but discriminates between states that are meaning-
fully different.
                                                                                  Incorporating Attention into RL
                       Representation                                  The previous two sections suggest a natural integration be-
The various mechanisms for representation learning that have           tween RL and attention learning. RL’s major focus is in
been identified in cognitive psychology all have potential ap-         updating value estimates by computing sophisticated feed-
plication to RL as means for speeding learning through en-             back signals from temporal patterns of reward, but current RL
hancing generalization. Our work thus far has focused on               models do not address how value estimates are represented.
principles derived from research on category learning. Much            In contrast, theories of category learning focus on how repre-
of the literature on human category learning aims to un-               sentations are created that allow for effective generalization,
derstand how humans develop powerful internal represen-                but learning is driven by simple updating rules based on ex-
tations that facilitate learning and inference of conceptual           ternal feedback. We propose a natural unification, in which
knowledge. The mechanisms that have been studied include               the feedback signals and updating rules from RL drive the
selective attention (Kruschke, 1992; Nosofsky, 1986); fea-             representation-learning mechanisms identified in the catego-
ture discovery (Schyns et al., 1998), prototype formation              rization literature. This integration makes RL significantly
(Smith & Minda, 1998); hybrid rule-exemplar representa-                more flexible and autonomous, and therefore possibly more
tions (Erickson & Kruschke, 1998; Nosofsky, Palmeri, &                 aligned with biological learning.
McKinley, 1994); clustering representations that grow with                The critical empirical question we explore operational-
task complexity (Anderson, 1991); mutable representations              izes the idea that RL can adapt its own representation
that evolve among exemplars, prototypes, and rules (Love &             through learned selective attention. Specifically, we investi-
Jones, 2006; Love, Medin, & Gureckis, 2004); and concep-               gate whether attention learning can be driven by internally
tual networks based on causal knowledge (Murphy & Medin,               generated TD-error signals in the same way that has been
1985). In this paper, we examine the interaction of RL and             observed with explicit external feedback (Nosofsky, 1986).
selective attention.                                                   In a companion paper (Jones & Cañas, 2010), we present a
   Though attention has been studied under many guises in              formal model that embodies this hypothesis, by synthesiz-
psychology, its implications for learning and generalization           ing the learning mechanisms of ALCOVE (Kruschke, 1992)
have been primarily explored in categorization and animal              and Q-learning, a well-studied RL model (Watkins & Dayan,
conditioning. In these literatures, attention has been proposed        1992). The formalism of the integrated model shows a tight
to act by reshaping generalization gradients (Sutherland &             and mathematically elegant synthesis of the two mechanisms,
Mackintosh, 1971; Nosofsky, 1986). The generalization gra-             which we believe offers a strong candidate explanation of
                                                                   1265

how biological RL processes build their own representations.                                                Troll
Here we present an experiment that tests that explanation, by
assessing the human capacity for attentional learning via in-
ternal value and error signals as opposed to direct external                        Sun
feedback.                                                                          Shade
                          Experiment                                    Spore                              Goblin       Reward
The goal of the present experiment was to determine whether
internal TD-error signals can drive attention learning in the
absence of any immediate overt reward. The task consisted                          Figure 1: An overview of the task.
of a two-step decision process in which the action on the first
step probabilistically determined the stimulus on the second
                                                                    an LCD monitor over a black background. The subject se-
step. Only after the second action did the subject receive feed-
                                                                    lected an action by pressing either S (Sun) or C (Cave) on
back about reward.
                                                                    the keyboard. After this first response was given, the spore
   The second stage of the task was a simple decision task
                                                                    disappeared and a pair of cartoon mushrooms appeared in the
with two possible stimuli and two possible actions. A differ-
                                                                    center of the screen. The subject selected the second action
ent action was optimal (i.e., maximized reward) for each of
                                                                    by pressing T (Troll) or G (Goblin). The reward was then
these intermediate stimuli. Once this mapping was learned,
                                                                    presented as stacks of gold coins with a numeric value under-
one intermediate stimulus led to a higher reward than the
                                                                    neath. The mushrooms and the chosen creature remained on
other. RL predicts that once subjects learned the optimal ac-
                                                                    the screen while the reward was displayed.
tions on this second step, they would learn to assign differ-
                                                                       The transition after each step was animated, lasting
ential values to the two intermediate stimuli. These values
                                                                    1200 ms between the first response and intermediate stimu-
would in turn be used for computing a TD-error signal for
                                                                    lus, and 970 ms between the intermediate stimulus and the
actions in the first step, thereby allowing subjects to learn an
                                                                    reward. The reward remained on the screen for 800 ms. A
action policy that maximizes the probability of obtaining the
                                                                    blank screen separated the reward from the beginning of the
higher-valued intermediate stimulus.
                                                                    next trial for 200 ms.
   The stimulus for the first choice varied on two continuous
                                                                       The reward structure for the second step was defined as
dimensions, one of which was more predictive of the outcome
                                                                    shown in Table 1. Each mushroom color was associated with
of the first action (i.e., the intermediate stimulus) and hence
                                                                    a different optimal action. Under these actions, one mush-
of which choice was optimal. The key question was whether
                                                                    room (henceforth referred to as the “good” mushroom) af-
learning the first action through TD error would also lead to
                                                                    forded a higher reward.
learning of selective attention between stimulus dimensions,
such that subjects would shift attention to the more relevant
                                                                            Table 1: Reward Structure of the Second Stage
dimension. The stimulus set of the first step was designed
                                                                                                      Creature Sold to
so as to allow assessment of subjects’ attentional allocation
                                                                              Mushroom Color         Goblin        Troll
based on their patterns of errors, as described below.
                                                                                     Blue          [200, 220] [400 420]
Methods                                                                            Orange          [300, 320] [100 120]
150 undergraduate students from the University of Colorado,         Note: Reward on each trial was sampled uniformly from the
Boulder served as the experimental subjects in exchange for         range shown.
course credit.
   Subjects were instructed they would pretend to be mush-             The transition dynamics for the first step were defined as
room farmers. On each trial, they were presented with an im-        follows. For each action, the probability of one mushroom
age of a mushroom spore and asked to choose between two             color versus the other was a logistic function of the dimension
locations for growing the spore, Sun and Shade. This action         values of the spore, given by p = 1/(1+exp(A(30L+10N))),
determined the intermediate stimulus, a pair of blue or or-         where L and N represent the length and number of the spines,
ange mushrooms. They were then given the option to sell the         scaled to range from −1 to 1, and A represents the action
mushrooms to either a Troll or a Goblin, who paid them in           on the first step, coded here as ±1. The coefficients for L
gold coins. The structure of the task is outlined in Figure 1.      and N were counterbalanced between subjects, so that L was
   The stimulus in the first stage was a yellow spore shape,        the more relevant dimension for half the subjects and N was
consisting of a circular center measuring 2.3 cm in diame-          more relevant for the other half. The effect of this design was
ter and radial spines arranged evenly around the center. The        to create an optimal decision bound, at an angle of 18.4◦ to
spines ranged from 8 mm to 260 mm in length and varied in           one of the two axes, such that the action that maximized the
number between 20 and 100. Spores were uniformly sampled            probability of obtaining the good mushroom was determined
from a circular region inscribed within this two-dimensional        by which side of the boundary each spore lay on.
stimulus space. The spore was presented in the center of               Subjects were randomly assigned to Length-relevant and
                                                                1266

Figure 2: Predictions from selective attention in first step of
task. Attention to the more relevant (horizontal) dimension
leads to stretching of the stimulus space. Critical stimuli
(grey) near ends of optimal decision bound (solid line) are
predicted to lead to errors, producing rotation in best fit of
linear classifier to subject’s responses (dashed line).             Figure 3: Distribution of responses on first step for a typical
                                                                    subject. Solid line shows optimal bound. Dashed line shows
                                                                    fit of linear classifier.
Number-relevant conditions, which differed in which spore
dimension was more predictive. The roles of the creatures,
the colors of the mushrooms, and the labels for the first action    regarding psychological decision processes. In the compan-
were also counterbalanced between subjects. Each subject            ion modeling paper (Jones & Cañas, 2010), we fit a process
completed 240 trials (480 total decisions) in blocks of 40.         model based on exemplar generalization, and it makes the
                                                                    same predictions.
Predictions and Analysis                                               In the absence of selective attention, the representation of
Our theory predicts subjects to shift attention to the more         the stimulus space would remain circular, and therefore by
relevant spore dimension. Under the view of attention as a          symmetry there should be no systematic bias in the subject’s
transformation of perceptual space, subjects’ representations       estimated decision bound. Therefore, testing for the predicted
of the set of spores should become stretched along the more         bias is a diagnostic way to determine whether our postulated
relevant dimension and compressed along the less relevant           attention-learning mechanism is operating.
dimension, as shown in Figure 2. Consider the stimuli in the
highlighted areas of the figure. Under the attention-altered        Results
representation, most of their neighbors lie on the opposite         On average, subjects made the correct action on the second
side of the optimal decision bound. Therefore, similarity-          step of the task on 89.4% of trials. Figure 4 shows the distri-
based generalization will lead to higher rates of suboptimal        bution, across subjects, of the proportion of good mushrooms
actions for these critical stimuli, as compared to matched          obtained following the first step. The histogram shows a clear
stimuli on the other side of the optimal bound. The same            bimodality, wherein many subjects performed at chance for
prediction arises if one assumes subjects learn prototypes for      the first step, but a significant number were able to learn ef-
spores associated to the two actions, because each critical         fective actions.
stimulus is more similar to the opposite prototype (taken to be        As explained below, we only predict selective attention for
the centroid of the region on that side of the optimal bound).      subjects who learn the first stage of the task. Therefore we
Therefore our predictions do not depend on an assumption of         analyzed the responses of subjects who performed above 70%
exemplar-based generalization.                                      on the first stage. This cutoff was based on a visual inspection
   To test this prediction, we used bivariate logistic regres-      of Figure 4 to safely exclude subjects who were performing at
sion to fit a linear classifier to each subject’s responses. This   chance. A total of 30 subjects performed at or above 70% on
classifier estimated a linear boundary in stimulus space that       the first step of the task, 11 in the Length-relevant condition
best divided the spores the subject chose to grow in the sun        and 19 in the Number-relevant condition.
from those grown in the shade. To illustrate this analysis, Fig-
ure 3 shows the response distribution of a typical subject in                                 20
                                                                         Number of Subjects
the learning group (defined below). Open and closed circles
                                                                                              15
represent stimuli for which the subject selected each of the
two actions, the solid line represents the optimal bound, and                                 10
the dashed line represents the output of the linear classifier.                               5
The prediction from selective attention, based on the analy-
                                                                                              0
sis of expected errors described above, is that the boundary
separating each subject’s decisions will be rotated relative to                                    0.4   0.5        0.6       0.7      0.8
the optimal boundary, as shown by the dashed line in Fig-                                                Proportion of Good Mushroom
ure 2. Importantly, the estimation of a linear decision bound
is a purely descriptive analysis that makes no commitment              Figure 4: Distribution of performance on first step of task.
                                                                1267

   A linear classifier was fit to the first-step responses of each   who exhibit more selective attention should perform better
subject in the learning group. Figure 5 shows the orienta-           on the task. Therefore, performance acts as a cue to indicate
tions of the resulting decision bounds, indicated by dots on         which subjects are more likely to exhibit a measurable effect.
the circumference of the stimulus region. The mean orien-            The third reason is purely methodological, in that the linear
tation for each group is shown as a dashed line, and the op-         classifier requires a systematic set of responses in order to
timal bound as a solid line. The Number-relevant condition           estimate a meaningful decision bound.
is shown in black and the Length-relevant condition in grey.            An alternative to our proposal of attention learning is that
The mean orientation of the decision bound for subjects in           subjects simply disregarded one dimension of the stimulus
the Length-relevant condition was 7.96◦ from the Number              entirely. This more strategic explanation is still consistent
axis. This value was significantly different from the opti-          with our general theory of representation learning driven by
mal bound (18.4◦ ;t10 = −2.99, p = .014) as well as from             RL, but the mechanism would be incompatible with continu-
zero (t10 = 2.29, p = .045). The mean orientation for the            ous adjustment of attention weights. Regardless, the data rule
Length-relevant condition was 7.33◦ from the Number axis.            out this explanation. The fact that the mean bound orienta-
This too was significantly different from the optimal bound          tions were reliably different from zero (i.e., the less relevant
(t18 = −3.25, p = .004) and from zero (t18 = 2.14, p = .046).        axis) implies that subjects were sensitive to the less relevant
                                                                     dimension (they were just less sensitive to it than to the pri-
                                                                     mary dimension). Another possibility is that some subjects
                                                                     disregarded one dimension and others disregarded the other,
                                                                     with most subjects in each condition disregarding the less
      Number of Spines
                                                                     relevant dimension. However, this explanation predicts a bi-
                                                                     modal distribution of bound orientations at the subject level,
                                                                     which is clearly not present.
                                                                                        General Discussion
                                                                     We have shown that humans can learn to shift attention in a
                                                                     dynamic task where reward is not given immediately follow-
                                                                     ing the decision that attention acts on. This finding tightly
                                                                     aligns with the internal TD-error signals that RL relies on,
                                                                     and it shows that direct external feedback is not required in
                           Spine Length                              order to learn selective attention.
                                                                        At its core, RL uses predictions or knowledge about later
Figure 5: Orientations of empirical decision bounds for sub-         states to build predictions and knowledge about prior states.
jects in learning group. Small circles = subjects; dashed            Application of an RL model to our task predicts that after
lines = means; heavy solid lines = optimal bounds; black =           learning the second stage of the task, one mushroom becomes
Number-relevant; grey = Length-relevant.                             internally represented as more valuable than the other. This
                                                                     internal value in turn acts as a proxy reward that drives learn-
                                                                     ing in the first stage of the task. Our findings support the
Discussion                                                           proposal that this internal proxy reward signal is also capable
The results of the decision-bound analysis confirm that sub-         of driving attention learning.
jects made more errors on the critical stimuli. This predic-            An alternative to the interpretation that our subjects are us-
tion follows directly from the assumption of selective atten-        ing RL-like internal values for the intermediate stimuli is the
tion to the more relevant dimension. Because actions on the          possibility of an explicit system that learns about both stages
first step only led to colored mushrooms and not nominal re-         of the task simultaneously after the external reward at the end
ward, our results support the proposal that attention learning       of each trial. Fu and Anderson (2008) found evidence for
can be driven by internal value estimates and error signals.         such a mechanism in a task structurally similar to ours. Ex-
   We only predicted selective attention for higher-                 plicit learning based on declarative memory is not, however,
performing subjects for three reasons. First, our theory             incompatible with RL. RL as we have discussed thus far, in its
only predicts attention to be learned once some amount of            most simple form, only updates estimates about the most re-
learning has taken place in associating stimuli to appropriate       cent state. However, specific mechanisms, termed eligibility
actions. Attention learning essentially works as a bootstrap-        traces, have been explored within RL to maintain information
ping method operating by altering generalization and thus            across time steps to facilitate learning (Sutton & Barto, 1998).
requires some amount of reliable knowledge to begin with in          Eligibility traces permit simultaneous updating of multiple
order for adaptation of generalization to have a useful effect.      prior eligible states. Declarative memory may play an im-
Second, because our theory predicts a bidirectional relation-        portant role in encoding these eligibility traces, and therefore
ship between attention and value learning, those subjects            Fu and Anderson’s results do not preclude an underlying RL
                                                                 1268

mechanism for learning several steps of a task at once.               Ashby, F. G., & Maddox, W. T. (2005). Human category
   Furthermore, declarative memory is unlikely to have                  learning. Annu Rev Psychol, 56, 149-178.
played a role in the first step of the present experiment. First,     Bagnell, J. A., & Schneider, J. G. (2001). Autonomous he-
in Fu and Anderson’s design (2008), there was a direct cor-             licopter control using reinforcement learning policy search
relation between the action in the first step and the eventual          methods. IEEE Int Conf Robo, 1615-1620.
reward, which could support direct learning of the first action.      Erickson, M. A., & Kruschke, J. K. (1998). Rules and exem-
In our design, only the conjunction of the spore and the ac-            plars in category learning. J Exp Psychol Gen, 127, 107-
tion taken on it was directly related to the possible outcomes          140.
after the second step. Second, the spores were drawn from a           Fu, W., & Anderson, J. R. (2008). Dual learning processes in
rich set varying on two continuous dimensions, whereas the              interactive skill acquisition. J Exp Psychol-Appl, 14, 179-
second stage of the task was very simple. Therefore subjects            191.
likely learned values for the intermediate mushrooms, which           Jones, M., & Cañas, F. (2010). Integrating reinforcement
could then be used as feedback for the first action, well before        learning with models of representation learning. Proceed-
the relatively weak correlation between spore-action pairs              ings of the 32nd Annual Conference of the Cognitive Sci-
and final reward could be learned. Third, we have shown that            ence Society.
subjects’ decision bounds were consistently tilted away from          Jones, M., Maddox, W. T., & Love, B. C. (2005). Stimulus
unidimensional rules, indicating that subjects learned the first        generalization in category learning. Proceedings of the 27th
action using implicit information-integration processes not             Annual Conference of the Cognitive Science Society, 1066-
amenable to declarative memory (Ashby & Maddox, 2005).                  1071.
Though our current work does not completely preclude other            Kruschke, J. K. (1992). Alcove: An exemplar-based connec-
learning mechanisms, we sought to isolate mechanisms di-                tionist model of category learning. Psych Rev, 99, 22-44.
rectly related to RL and TD error, and our results show good          Love, B. C., & Jones, M. (2006). The emergence of multiple
support for such mechanisms.                                            learning systems. Proceedings of the 28th Annual Confer-
   Although not tested directly, the behavior of the subjects           ence of the Cognitive Science Society, 507-512.
who did not learn the first stage sufficiently in our task fits       Love, B. C., Medin, D. L., & Gureckis, T. M. (2004). Sustain:
well into the learning framework we propose. Before the dif-            A network model of category learning. Psychol Rev, 111,
ferential value of the mushrooms is learned, the feedback to            309–332.
all actions of the first step is constant, which drives attention     Murphy, G. L., & Medin, D. L. (1985). The role of theories
to generalize across the entire spore space. It is possible that        in conceptual coherence. Psychol Rev, 92, 289-316.
by the time some subjects learned the optimal actions for the         Nosofsky, R. M. (1986). Attention, similarity, and the
second step, they may have learned to entirely disattend any            identification-categorization relationship. J Exp Psychol
variability of the spores. This inattention is self-perpetuating        Gen, 115, 39-57.
and prevents future learning.                                         Nosofsky, R. M., Palmeri, T. J., & McKinley, S. C. (1994).
   The potential for learned inattention in dynamic tasks has           Rule-plus-exception model of classification learning. Psy-
interesting theoretical and practical implications, because it          chol Rev, 101, 53–79.
could make aspects of a task far removed from overt reward            Schultz, W., Dayan, P., & Montague, P. R. (1997). A neural
difficult to learn. From this perspective, it is clear that an          substrate of prediction and reward. Science, 275, 1593-
understanding of the mechanisms of attention learning could             1599.
be beneficial in designing human training programs, such as           Schyns, P. G., Goldstone, R. L., & Thibaut, J.-P. (1998). The
backward chaining to train intermediate value representations           development of features in object concepts. Behav Brain
before earlier stages are encountered.                                  Sci, 21, 1-54.
   The primary question we examined here was whether TD               Shepard, R. (1987). Toward a universal law of generalization
error, and therefore RL, can have an influence not just on              for psychological science. Science, 237, 1317–1323.
learning values of stimuli within a fixed representation, but         Smith, J., & Minda, J. (1998). Prototypes in the mist: The
whether the representation itself can be altered. Shifts in at-         early epochs of category learning. J Exp Psychol Learn,
tention alter the similarity structure of a stimulus space and          24(6), 1411–1436.
therefore typify the sort of changes in representation we pre-        Sutherland, N., & Mackintosh, N. (1971). Mechanisms of
dict RL to effect. That humans exhibited changes in repre-              Animal Discrimination Learning. NY: Academic Press.
sentation in the service of learning a new task involving fine        Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learn-
discrimination of stimuli suggests a rich interplay of repre-           ing: An Introduction. The MIT Press.
sentation learning and RL.                                            Tesauro, G. (1995). Temporal difference learning and td-
                                                                        gammon. Commun ACM, 38(3), 58-68.
                                                                      Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Ma-
                           References
                                                                        chine Learning, 8, 279–292.
Anderson, J. R. (1991). The adaptive nature of human cate-
   gorization. Psychol Rev, 98, 409–429.
                                                                  1269

