UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Considering the Source: Preschoolers (and Adults) Use Talker Acoustics Predictively and
Flexibly in On-Line Sentence Processing
Permalink
https://escholarship.org/uc/item/1f055182
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Author
Creel, Sarah
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

Considering the Source: Preschoolers (and Adults) Use Talker Acoustics Predictively
                                 and Flexibly in On-Line Sentence Processing
                                             Sarah C. Creel (creel@cogsci.ucsd.edu)
                                Department of Cognitive Science, University of California-San Diego
                                          9500 Gilman Drive, La Jolla, CA 92093-0515 USA
                             Abstract                                  dwellings with (Hirschfeld & Gelman, 1997) speakers who
   The identity of the person talking is likely to constrain the
                                                                       sound unfamiliar (they speak foreign languages). These
   things that they talk about. Adults can use talker acoustics to     studies suggest that children associate familiar-sounding
   make on-line predictions about upcoming spoken material             speech with familiar objects and positive affect.
   (Van Berkum et al., 2008). However, this cue to meaning                Beyond this, it is not clear whether children store more
   may take time to learn. Do preschoolers consider who is             nuanced semantic information in relation to speech
   talking when they are comprehending spoken sentences? I             acoustics. This information might be somewhat difficult to
   explored this question in two eye-tracked picture selection
                                                                       learn for two reasons. First, children may be working to
   experiments. Experiment 1 showed that children and adults
   use vocal cues to talker identity in predicting the color of        ignore talker-related acoustics to extract the attributes
   upcoming referents in spoken sentences. Experiment 2                related to meaning (dog spoken by Mom still means the
   showed that children and adults flexibly use acoustic cues to       same thing as dog spoken by Dad, so why pay attention to
   talker for first-person requests (“I want the square”) but          irrelevant acoustic variation?). Second, knowing who is
   reference to individuals for third-person requests (“Billy          talking may only be useful what the person is referring to
   wants the square”). This suggests that children aged 3-5 years      himself (“I really need a vacation”) and not when talking
   use who is talking to constrain the scope of reference in
   sentence processing, and know when this cue is likely to be
                                                                       about things irrelevant to himself (“It’s raining outside”).
   useful.                                                             That is, talker information may only be a reliable cue to
                                                                       meaning in a limited set of circumstances.
   Keywords: language development, talker identification,
   perspective-taking, spoken language processing
                                                                       Use of other non-phonemic acoustic attributes in
                          Introduction                                 comprehension
                                                                       Though talker information has not been explored as an
No two people sound alike. Some research indicates that
                                                                       influence on children’s on-line sentence processing, recent
this poses a challenge for language processing (Mullennix,
                                                                       studies on other non-phonemic acoustic cues—prosody and
Pisoni, & Martin, 1989; Nusbaum & Morin, 1992).
                                                                       vocal affect—provide some hints about the potential of
However, it may also provide additional, helpful
                                                                       talker as a semantic information source during development.
information to the comprehender. That is, knowing who is
                                                                       Children seem adept at processing prosodic information.
talking can provide useful information in processing spoken
                                                                       Snedeker and Yuan (2008) showed that children were
language. For instance, adult listeners make different
                                                                       sensitive to a speaker’s intonational phrase boundaries in
predictions about upcoming information in a sentence
                                                                       their interpretations of prepositional-phrase attachment. Ito,
depending on who is speaking it (Van Berkum, Van Den
                                                                       Jincho, Minai, Yamane, and Mazuka (2009) and Bibyk, Ito,
Brink, Tesink, Kos, & Hagoort, 2008), suggesting they have
                                                                       Wagner, and Speer (2009) found that children as young as 6
particular semantic associations with certain voice
                                                                       years use pitch accent to constrain upcoming referents to a
characteristics (e.g., a child’s voice vs. an adult’s voice).
                                                                       set of items contrasting on the pitch-accented dimension.
Thus, acoustic differences among talkers potentially have
                                                                       These studies suggest that children attend to non-phonemic
rich semantic associations (Geiselman & Crawley, 1983).
                                                                       sound patterns that cue differences in meaning.
But how long does it take the developing language learner
                                                                          Children seem to have more difficulty processing cues to
to form and use these associations in comprehending
                                                                       vocal affect. Morton and Trehub (2001) found that when
language?
                                                                       vocal affect conflicts with verbal content (e.g. hearing “I get
   Children are sensitive to familiar perceptual information
                                                                       to eat ice cream” in a sad voice, or “My dog got hit by a
about talkers from a very early age. For instance, they are
                                                                       car” in a happy voice), children cannot ignore the verbal
better at generalizing words between talkers with a familiar
                                                                       content when reporting the talker’s affect (reporting the first
accent than between talkers with an unfamiliar accent
                                                                       sentence as sounding happy, and the second as sounding
(Schmale & Seidl, 2009). This suggests that they are
                                                                       sad). Nonetheless, recent work by Berman, Graham, and
sensitive to the acoustic details in the speech signal. Less is
                                                                       Chambers (2009) using eye tracking, a more sensitive,
known about how much semantic information children
                                                                       implicit measure, suggests that children associate positive
glean from talker acoustics. We do know that children have
                                                                       and negative vocal affect cues with positively- and
less positive affective responses to (Kinzler, Dupoux, &
                                                                       negatively-valenced pictures (e.g. intact vs. broken dolls).
Spelke, 2007) and associate unfamiliar clothing, and
                                                                   1810

   Children may be using these cues by making associations           rates (50% and 63%). Adults (n = 29) were recruited from
between sound properties and semantic attributes. For                the University of California San Diego human participant
instance, pitch accent seems to semantically activate                pool, and received course credit for participation.
contrast sets. In the vocal emotion case, children might have
associations between sad vocal cues and non-intact objects           Visual stimuli. Pink and blue squares, triangles, circles, and
(Berman et al., 2009). This leaves open whether children are         five-pointed stars were constructed in Microsoft PowerPoint
able to use non-phonemic acoustic information in the speech          and saved as 200 x 200 pixel .jpg files. Scenes of Anna with
signal to make high-level inferences about the perspective           pink objects (a tutu, a bed, bunny slippers) and Billy with
of the talker.                                                       blue objects (a truck, a baseball cap, a watergun) were 1024
   In sum, children show some ability to glean semantic              x 768 pixel .jpg files.
information from two non-phonemic acoustic information
sources, prosody and vocal affect. Thus, one might expect            Auditory stimuli. Two native southern-Californian
that children would gain semantic information from non-              university students recorded requests for shapes, and
phonemic acoustic cues to talker as well. However, it is not         descriptions of Anna’s and Billy’s favorite colors, in child-
clear that children can go so far as to use it to invoke a           directed English. Recordings were made in a sound-
particular talker’s perspective.                                     attenuated chamber and saved to .wav files on a computer.
                                                                     Each utterance was edited for clarity, saved to its own sound
The current study                                                    file, and normalized to 70 dB. Target word (e.g. “square”)
   To explore children’s ability to exploit talker information       onset was at 1003 ms after the sentence began, on average.
in comprehending spoken language, I presented child and
adult participants with an eye-tracked picture selection task        Procedure. Each experiment had four brief phases. During
(Cooper, 1974; Tanenhaus, Spivey-Knowlton, Eberhard, &               each phase, sound was presented over high-quality
Sedivy, 1995) directed by two fictional child talkers, Anna          headphones as visual stimuli were presented on an LCD
and Billy. Each child professed a preferred color (pink vs.          monitor. First, each talker appeared, surrounded by three
blue), and then asked for pictures on screen (e.g. “the              pink (or blue) objects, and stated his/her preferred color.
square”), which were always their preferred colors. The              The talker named each colored object in turn. Children were
question of interest was whether children would visually             then tested in their ability to distinguish the colors: on eight
fixate the pictures in the talker’s preferred color over the         trials, they saw two of the same shape and heard Anna
non-preferred color pictures based on which talker they              (Billy) ask “Where’s the pink (blue) one?” Children did not
hear.                                                                proceed until they answered at least 7 of 8 trials in a row
   I deliberately chose gender-stereotyped color preferences,        correctly. This verified that they could distinguish the two
reasoning that capitalizing on children’s preexisting                colors, and further reinforced each talker’s preference. The
knowledge would minimize working memory demands that                 two favorite-color trials were then presented again. Finally,
might mask sensitivity. I also queried the children’s own            there was a 32-trial test phase where Anna and Billy each
color preferences, to determine whether they were able to            requested objects (stars, squares, triangles, or circles). On
predict color preferences (i.e., make looks to the talker’s          each trial, children heard (for instance) Anna saying
preferred-color pictures) when those preferences did not
match their own.                                                               (1) Can you help me find the square?
   In Experiment 1, I considered whether children (as well as
adults) were able to use talker information early in the                On every trial, two pictures were pink, and two were blue.
sentence as a cue to upcoming referent color. That is, are           Each talker requested squares, triangles, circles, and stars
they able to infer what shape the talker might request, given        equally often. In this phase, neither talker used a color term,
that the talker is Anna, who prefers pink? In Experiment 2, I        referring merely to the shapes themselves. Each shape+color
assessed children’s flexibility in using talker information by       combination occurred equally often in each screen position
making talker identity on its own a useless cue to referent          across trials. Each talker spoke on 50% of trials.
color. Specifically, each child talker asked for a shape for            Adults clicked the desired picture with a computer mouse.
herself half of the time, and for the other child the other half     Children pointed to their desired responses, which were then
of the time.                                                         mouse-clicked by an experimenter. The measure of interest
                                                                     was whether participants, before knowing what shape was to
                       Experiment 1                                  be requested, would visually fixate pink things upon hearing
                                                                     Anna’s voice and blue things upon hearing Billy’s voice.
Method
                                                                     Equipment. The experiment was run in Matlab using
Participants. Children (n = 24, ages 3-5 years) were
                                                                     PsychToolbox3 (Brainard, 1997; Pelli, 1997) and interfaced
recruited from local day-care and preschool facilities, and
                                                                     with the eye tracker using the Eyelink Toolbox
participated in the study at their day-care/preschool location.
                                                                     (Cornelissen, Peters, & Palmer, 2002). Participants’ eye
They were given a small toy as a thank-you gift. An                  movements were recorded by an Eyelink Remote eye
additional two children were excluded due to high error
                                                                 1811

tracker (SR Research, Mississauga, ON) at 4-millisecond                                                                                                                         Note that children cannot be egocentrically fixating their
(ms) resolution. Offline, this was down-sampled to 50-ms                                                                                                                      own preferred color. If they were, then they should show no
resolution to enable easier processing.                                                                                                                                       overall effect of the talker’s preferred color: pink looks on
                                                                                                                                                                              pink trials and pink looks on blue trials should cancel each
Results                                                                                                                                                                       other out. A more subtle version of this egocentricity
Figure 1 suggests that both children and adults were visually                                                                                                                 hypothesis is that children only fixate the talker’s preferred
fixating pictures of the talker’s preferred color well before                                                                                                                 color when it matches their own preferred color. This does
the onset of the target word. To quantify this, I analyzed the                                                                                                                not explain the results either: children whose preferred color
data as follows. First, trials with erroneous responses (7%                                                                                                                   matched neither talker (n = 12) still showed above-chance
overall) were discarded. Then, a measure of color                                                                                                                             looks to the talker’s preferred color at 600-1000 ms (t(11) =
preference was calculated, which I will call the “color-look                                                                                                                  3.75, p = 0.003) and 1000-1400 ms (t(11) = 5.65, p =
score.” This was the proportion of looks to the non-target                                                                                                                    0.0001). This implies that children can use their knowledge
picture of the talker’s preferred color, minus averaged                                                                                                                       of other individuals’ color preferences, even when different
looks to the two nonpreferred-color pictures. When this                                                                                                                       from their own, to constrain the domain of reference.
quantity was zero, listeners were not looking at pictures of
either color more than the other. When it exceeded zero,                                                                                                                      Discussion
listeners were looking more toward the talker’s preferred                                                                                                                     Both children and adults were able to use talker information
color. (Negative values would imply looks to the talker’s                                                                                                                     early in the sentence to “predict” the color of the upcoming
nonpreferred color, but this result did not occur in the                                                                                                                      referent: they looked more at blue things when Billy began
current experiment.) Bear in mind that eye movements                                                                                                                          talking, and at pink things when Anna began talking. This
based on spoken material were most likely planned about                                                                                                                       verifies that, in a relatively simple situation, children use
200 ms before they occurred, meaning that eye movements                                                                                                                       talker identity to constrain the referential domain of
planned based on a signal at 1000 ms will show up around                                                                                                                      upcoming sentential material. Children showed looking
1200 ms (Hallett, 1986).                                                                                                                                                      effects equivalent to adults, suggesting that they are as able
   An analysis of variance (ANOVA) was calculated on                                                                                                                          as adults to integrate talker information with verb
participants’ color-look scores in three 400-millisecond (ms)                                                                                                                 information (Anna + want = pink, Billy + want = blue). This
time windows, with Time Window (200-600, 600-1000,                                                                                                                            may depend on event knowledge that children have obtained
1000-1400) and Age (child, adult) as factors. The only                                                                                                                        through lifetime experience, or based on experimental
significant factor was Time Window (F(2,102) = 23.49, p <                                                                                                                     conditions, but in either case, children are able to exercise
.0001). Individual t-tests indicated that both children and                                                                                                                   this knowledge.
adults had color-look scores greater than zero—that is, they                                                                                                                     This experiment nicely demonstrates that children as well
were looking more to the talker’s preferred color—by 200-                                                                                                                     as adults are able to use talker characteristics to shape
600 ms (children: t(23) = 2.27, p = 0.03; adults: t(28) =                                                                                                                     predictions of upcoming referents. One account of these
3.21, p = 0.003), which was also significant at 600-1000 ms                                                                                                                   data is that children and adults are using talker information
(t(23) = 4.49, p = 0.0002; t(28) = 5.64, p < .0001) and 1000-                                                                                                                 to decide whose preferences to invoke to determine
1400 ms (t(23) = 7.35, p < .0001; t(28) = 5.99, p <                                                                                                                           upcoming reference—they are constraining the domain of
.0001).Thus, both groups seem to be adept at utilizing talker                                                                                                                 expected reference by talker. However, another explanation
information to decide whose preferences to invoke.                                                                                                                            is that participants made a simple low-level audio-visual
                                                                                                                                                                              association between talker-related acoustic properties and
                                  !#,"   <2/"=6:"759-">5"?./@"175";A:235BC"D//2"2;05@#"
                                                                                                                                                                              color. That is, they associated the sound of a talker’s voice
                                  !#+"
                                                                                                                                                                              with pinkness or blueness, rather than using talker acoustics
                                                                 """""""""""""""""""""""""""""""""""""""""G63@"6/;51"H"%!!">;"
                                                                                                                                                                              to access a representation of the talker as an individual with
  !"#$#"%&#'()##*+(%#($&,%-".+(
                                  !#*"                                                                                                                                        a color preference. On this latter account, they might look at
                                  !#)"
                                                                                                                                                         F2>-95"@.;-92="
                                                                                                                                                                              pink things even if Anna were to say “Let me out of this
                                  !#("                                                                                                                                        cage” because her voice is associated with pink things.
                                                                                                                                                             E7.9@"              Related to this issue is whether children are aware of
                                  !#'"                                                                                                                     -./0"123451"
                                                                                                                                                                              contexts where talker information is even useful in real-
                                                                                                                                                           -./0"61753"
                                  !#&"                                                                                                                                        world language processing. In particular, talker identity in
                                                                                                                                                           89:5"17./4;"
                                  !#%"                                                                                                                                        the real world may only be useful for prediction when the
                                                                                                                                                           -./0"123451"
                                                                                                                                                                              talker is talking about himself. When the talker is talking
                                  !#$"                                                                                                                     -./0"61753"
                                                                                                                                                                              about someone else—for instance, if Billy said that Anna
                                    !"                                                                                                                     89:5"17./4;"       wanted to see a particular shape—it would be
                                         !"       (!!"       $!!!"                                                               $(!!"   %!!!"   %(!!"       2@:91"
                                                                                                                                                                              disadvantageous to activate colors associated with Billy’s
                                                      /&0.(1"#0(+.'%.',.(#'+.%(                                                                                               voice. This means that a smart listener would be able to use
     Figure 1: Adults’ (solid) and children’s (filled) looks to                                                                                                               talker information in some (first person) situations, and
    pictures in Experiment 1. Upper right inset: an example                                                                                                                   ignore it in other (e.g. third person) situations. Presumably
             display where black=pink, gray=blue.
                                                                                                                                                                           1812

adults do this readily, but it is unclear whether children do                                                                        Results
so.                                                                                                                                  Both adults (Figure 2) and children (Figure 3) seem to use
                                                                                                                                     talker information flexibly: when Anna is the agent of the
                                                            Experiment 2                                                             sentence, they fixate pink things, regardless of whether
Experiment 2 explored whether children and adults were                                                                               Anna is the person talking. There were also somewhat later
able to use talker information to activate characteristics (i.e.,                                                                    target fixations in the 3rd-person condition than in the 1st-
color preferences) of each individual. The experiment was                                                                            person condition. While visually striking, this simply results
introduced as before, but now in the test phase each talker                                                                          from the 3rd-person sentences being slightly longer in
asked for a shape either for herself or for the other talker,                                                                        duration than the 1st-person sentences (averaging 970 ms to
followed by “Can you show me/him/her where it is?”:                                                                                  word onset vs. 798 ms to word onset, respectively).
                                                                                                                                        Error trials (5%) were discarded. Then, I conducted an
              (2) Anna: I want to see the square. Can you …                                                                          ANOVA on color-look scores with Age (child, adult), Time
              (3) Billy: Anna wants to see the square. Can you …                                                                     Window (200-600, 600-1000, 1000-1400) and Person (1st
              (4) Billy: I want to see the square. Can you …                                                                         person, 3rd person) as factors. This bore out the above
              (5) Anna: Billy wants to see the square. Can you…                                                                      observations. There was an interaction of Age x Time
                                                                                                                                     Window x Person (F(2,140) = 5.18, p = 0.007), so
   If children are learning low-level auditory-visual                                                                                individual ANOVAs were conducted for each Age. For
associations between talkers and colors, they should fixate                                                                          adults, only Time Window was significant (F(2,76) = 10.3,
pink things for (2) and (5) and blue things for (3) and (4).                                                                         p = 0.0001), with color-look scores increasing over time. T-
However, if they are learning information about individuals,                                                                         tests indicated that both 1st- and 3rd-person trials showed
then they may use talker information only in first-person                                                                            significant color looks at 600-1000 ms (t(38) = 2.13, p =
cases, and use reference to Anna or Billy in third-person                                                                            0.04; t(38) = 2.73, p < 0.01), and 1000-1400 ms (t(38) =
cases, to determine whose preferences to invoke. If so, they                                                                         2.25, p = 0.03; t(38) = 4.08, p = 0.0002). For children, there
should look to the agent’s preferred color-pictures, looking                                                                         was an effect of Time Window (F(2,64) = 23.48, p < .0001),
at pink things in (2) and (3) and blue things in (4) and (5).                                                                        with color-look scores increasing over time, and a Time
                                                                                                                                     Window x Person interaction (F(2,64) = 3.36, p = 0.04). T-
                                 !#,"    <"=2/1>;?"16";55"175";@:235#"                                                               tests comparing 1st-person and 3rd-person looks suggested
                                 !#+"                                                                                                nonsignificant differences in each time window (only 600-
 !"#$#"%&#'()##*+(%#($&,%-".+(
                                                              """""""""""=63A"6/;51"B"%!!"C;"
                                                                                                                                     1000 ms approached significance, t(32) = 1.82, p = 0.08).
                                 !#*"                                                                               4+%($."+#'(
                                                                                                                    -./0"123451"     Regardless, both 1st- and 3rd-person color-look scores were
                                 !#)"                                                                                                significant at 600-1000 ms (t(32) = 2.22, p = 0.03; t(32) =
                                                                                                                    -./0"61753"
                                 !#("                                                                                                4.84, p < .0001) and 1000-1400 ms (t(32) = 8.11, p < .0001;
                                                                                                                    89:5"17./4;"     t(32) = 4.78, p < .0001). This suggests that children, as well
                                 !#'"
                                                                                                                    -./0"123451"     as adults, used the talker’s voice on 1st-person trials, but
                                 !#&"                                                                                                reference (the child’s name) on 3rd-person trials, to
                                                                                                                    -./0"61753"
                                 !#%"                                                                                                determine whose color preferences to use in constraining the
                                                                                                                     89:5"17./4;"    referential domain. As before, results held for children (n=
                                 !#$"
                                                                                                                    5"6($."+#'(      18) whose favorite colors were neither pink nor blue.
                                   !"
                                                              "
                                        !"       (!!"        $!!!"                              $(!!"   %!!!"   %(!!"                                                    !#,"    <"=2/1>;?"16";55"175";@:235#"
                                               /&0.(1"#0(+.'%.',.(#'+.%(20+3(                                                                                            !#+"
                                                                                                                                         !"#$#"%&#'()##*+(%#($&,%-".+(
                                                                                                                                                                                                      """""""""""=63A"6/;51"B"%!!"C;"
                                                                                                                                                                         !#*"                                                                               4+%($."+#'(
                         Figure 2: Adult fixations to targets and other pictures on                                                                                                                                                                         -./0"123451"
                           1st-person (circles) and 3rd-person (squares) trials.                                                                                         !#)"
                                                                                                                                                                                                                                                            -./0"61753"
                                                                                                                                                                         !#("
                                                                                                                                                                                                                                                            89:5"17./4;"
                                                                                                                                                                         !#'"
Methods                                                                                                                                                                                                                                                     -./0"123451"
                                                                                                                                                                         !#&"
Participants. Children (n = 33) and adults (n = 39) were                                                                                                                                                                                                    -./0"61753"
recruited as in Experiment 1. Two more children with                                                                                                                     !#%"
                                                                                                                                                                                                                                                             89:5"17./4;"
extremely high error rates (34% and 44%) were excluded.                                                                                                                  !#$"
                                                                                                                                                                                                                                                            5"6($."+#'(
                                                                                                                                                                           !"
                                                                                                                                                                                                      "
Auditory stimuli. A new set of spoken instructions were
                                                                                                                                                                                !"       (!!"        $!!!"                              $(!!"   %!!!"   %(!!"
recorded by the same individuals as in Experiment 1.
                                                                                                                                                                                       /&0.(1"#0(+.'%.',.(#'+.%(20+3(
Procedure and Equipment. These matched Experiment 1
in all respects.                                                                                                                                                  Figure 3: Child fixations to targets and other pictures on
                                                                                                                                                                    1st-person (circles) and 3rd-person (squares) trials.
                                                                                                                                  1813

Discussion                                                          mismatch potential (N400) when the talker’s identity and
Children and adults in Experiment 2 succeeded at predicting         the action described were incongruous (e.g. a young child
the agent’s color preference. That is, they made more visual        saying “I like to drink a glass of wine”) than when they
fixations to shapes of the agent’s preferred color on both          were congruous (an adult saying the same sentence). The
first-person (“I want”) and third-person (“Anna/Billy               current work suggests that preschool-aged children are
wants”) trials. This implies that they use talker acoustics not     similarly able to use talker acoustics to calculate likely (and
just as a low-level auditory-visual association, but as a           unlikely) referents.
source of information about a participant in an action. Thus,          The current work, as well as Van Berkum’s, fits nicely
children as well as adults can use non-phonemic acoustic            with a perspective on language processing (Kamide,
information to activate information about an individual, and        Altmann, & Haywood, 2003; Bicknell, Elman, Hare,
then infer the likely referential domain for that individual.       McRae, & Kutas, 2008) in which comprehenders use any
                                                                    available linguistic and nonlinguistic cues to construct event
                    General Discussion                              representations on-line. Acoustic information linked to
                                                                    talker identity is apparently useful in constructing event
Two experiments suggest that children are able to use their         representations. Moreover, it is a robust enough cue that
knowledge about particular talkers to constrain the domain          preschool-aged children can use it rapidly on-line (see Bates
of upcoming referents. In Experiment 1, listeners were              & MacWhinney, 1987; Snedeker & Trueswell, 2004 for
instructed that Anna liked pink things, and Billy liked blue        further discussion of cue robustness and development).
things. They then heard Anna and Billy request shapes of               Perhaps the most unique contribution of this study is the
their preferred color. Both children and adults made more           implication that children are using talker acoustics to infer
visual fixations to the shapes of the talker’s preferred color      properties of individuals, or at least of groups of
than of the talker’s nonpreferred color. This suggested that        individuals. That is, children are able to encode that Anna
children were able to identify the talkers and use their            and Billy have particular color preferences, even when Anna
individual preferences to constrain on-line interpretation of       and Billy have different preferences than the children
the request.                                                        themselves. As demonstrated in Experiment 2, this does not
   However, an equally good explanation was that children           seem to be a simple auditory-visual association between
had associated female voice characteristics with pinkness,          Anna’s voice (or female voices) and pink, and Billy’s voice
and male voice characteristics with blueness, a low-level           (or male voices) and blue, but an association with Anna and
auditory-visual cue correspondence rather than knowledge            Billy as entities who have different preferences for color.
of an individual’s preferences. Experiment 2 ruled out this
explanation: listeners again heard Anna and Billy requesting        Remaining questions
shapes, but half the time, each talker requested a shape for
                                                                    One obvious question is how much of children’s ability to
the other talker. This meant that only on first-person trials
                                                                    use talker information in this task is subserved by children’s
(“I want”) was talker a useful predictor, while on third-
                                                                    long-term knowledge of gendered color preferences. A
person (“Anna wants”) trials, it was a misleading predictor.
                                                                    quick visual search of major toy retailers’ products confirms
Impressively, children and adults were both able to use
                                                                    strong tendencies for female toys to be pink (or purple), and
talker information on first-person trials, and proper nouns
                                                                    for male toys to be blue (or a number of other colors, but not
on third-person trials, to infer the identity of the sentential
                                                                    pink). Thus, children’s use of talker information here could
agent. That is, they always showed a visual fixation
                                                                    be due to a lengthy learning process through exposure to
preference toward the agent’s preferred-color shape, even
                                                                    gender-stereotyped objects in their environments. On the
when the agent was not the talker. This implies that, in a
                                                                    other hand, children might readily associate idiosyncratic
relatively simple task, children are able to use talker
                                                                    preferences with particular individuals. If so, then children
information selectively (only on first-person trials) to infer
                                                                    should also be able to use learned, non-gender-stereotyped
the identity—and thus the color preferences—of the agent.
                                                                    color preferences to constrain on-line language processing.
Implications for development of language                               An experiment in progress addresses this question, using
                                                                    black and white as the preferred colors. Only one child
processing
                                                                    (1.5%) in Experiments 1 and 2 reported black as his favorite
This research adds to the existing literature on cue                color, and none reported white, suggesting that children
integration in spoken language processing. Specifically, this       have little experience or gender-preference information for
work demonstrates that, in addition to prosody and vocal-           black and white. Further, color preference is
emotional cues, non-phonemic acoustic cues related to               counterbalanced across talker gender. With 15 child
talker can also be used to constrain processing on-line fairly      participants so far, there are robust looks to talkers’
early in life. This suggests excellent facility on the part of      preferred colors. This suggests that neither conformance to a
children to use non-phonemic acoustic cues to talker                gender-stereotypical color mapping nor long-term learning
identity to understand the situation described by a sentence.       is necessary for children to be able to use talker information
This work is similar to adult research by Van Berkum et al.         predictively. However, talker gender itself may still be an
(2008), in which listeners showed a larger semantic
                                                                1814

important social anchor point for encoding talker                 Geiselman, R. E., & Crawley, J. M. (1983). Incidental
preference.                                                          processing of speaker characteristics: Voice as
   Another question is how subtle children are in their              connotative information. Journal of Verbal Learning &
appreciation of talker information. Are they as keen in their        Verbal Behavior, 22, 15–23.
perceptions as adults? If not, how do they differ from            Goldinger, S. D. (1996). Words and voices: Episodic traces
adults? Direct comparisons may be limited somewhat by                in spoken word identification and recognition memory.
children’s level of social knowledge relative to adults—             Journal of Experimental Psychology: Learning, Memory,
adults may only seem more adept at using talker cues                 and Cognition, 22, 1166 – 1183.
because they have more subtle knowledge of social                 Hirschfeld, L. A., & Gelman, S. A. (1997). What children
variation.                                                           think about the relationship between language variation
   Finally, it is unknown how semantic knowledge based on            and social difference. Cognitive Development, 12, 213 –
talker characteristics relates to talker-specific perceptual         238.
facilitation of word-forms (e.g. Goldinger, 1996; see also        Ito, K., Jincho, N., Yamane, N., Minai, U., & Mazuka, R.
Creel, Aslin, & Tanenhaus, 2008). Does talker-specific               (2009). Use of emphatic pitch prominence for contrast
perceptual information covary with semantic usefulness?              resolution: An eye-tracking study with 6-year old and
Despite these remaining questions, though, the current               adult Japanese listeners. Paper presented at Boston
research forms a solid basis for further explorations of             University Conference on Language Development 34,
children’s sensitivity to talker as a cue to meaning.                Boston, MA.
                                                                  Kamide, Y., Altmann, G. T. M., & Haywood, S. L. (2003).
                          References                                 The time-course of prediction in incremental sentence
                                                                     processing: Evidence from anticipatory eye movements.
Allopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K.                Journal of Memory and Language, 49, 133-156.
   (1998). Tracking the time course of spoken word                Kinzler, K. D., Dupoux, E., & Spelke, E. S. (2007). The
   recognition using eye movements: evidence for                     native language of social cognition. Proceedings of the
   continuous mapping models. Journal of Memory and                  National Academy of Science, 104, 12577 – 12580.
   Language, 38, 419–439.                                         Morton, J. B., & Trehub, S. (2001). Children’s
Bates, E. A., & MacWhinney, B. (1987). Competition,                  understanding of emotion in speech. Child Development,
   variation and language learning. In B. MacWhinney (Ed.),          72, 834-843.
   Mechanisms of language acquisition. Hillsdale, NJ:             Mullennix, J.W., Pisoni, D.B., & Martin, C.S. (1989).
   Erlbaum.                                                          Some effects of talker variability on spoken word
Berman, J., Graham, S. A., & Chambers, C. (2009).                    recognition. Journal of the Acoustical Society of America,
   Preschoolers’ appreciation of vocal affect as a cue to a          85, 365-378.
   speaker’s intentions. Paper presented at Boston University     Pelli, D. G. (1997). The VideoToolbox software for visual
   Conference on Language Development 34, Boston, MA.                psychophysics: Transforming numbers into movies.
Bibyk, S., Ito, K., Wagner, L., & Speer, S. (2009). Children         Spatial Vision, 10, 437 – 442.
   can use contrastive pitch accent in on-line processing.        Schmale, R., & Seidl, A. (2009). Accommodating
   Paper presented at Boston University Conference on                variability in voice and foreign accent: Flexibility of early
   Language Development 34, Boston, MA.                              word representations. Developmental Science, 12, 583-
Bicknell, K., Elman, J. L., Hare, M., McRae, K., & Kutas,            601.
   M. (2008). Online expectations for verbal arguments            Snedeker, J., & Trueswell, J. (2004). The developing
   conditional on event knowledge. In B.C. Love, K. McRae,           constraints on parsing decisions: The role of lexical biases
   & V.M. Sloutsky (Eds.), Proceedings of the 30th Annual            and referential scenes in child and adult sentence
   Conference of the Cognitive Science Society (pp. 2220-            processing. Cognitive Psychology, 49, 238-299.
   2225). Austin, TX: Cognitive Science Society.                  Snedeker, J., & Yuan, S. (2008). Effects of prosodic and
Brainard, D. H. (1997). The Psychophysics Toolbox. Spatial           lexical constraints on parsing in young children (and
   Vision, 10, 433-436.                                              adults). Journal of Memory and Language, 58, 574-608.
Cooper, R. M. (1974). The control of eye fixation by the          Tanenhaus, M., Spivey-Knowlton, M., Eberhard, K., &
   meaning of spoken language. A new methodology for the             Sedivy, J. (1995). Integration of visual and linguistic
   real-time investigation of speech perception, memory, and         information in spoken language comprehension. Science,
   language processing. Cognitive Psychology, 6, 84–107.             268, 1632-1634.
Cornelissen, F.W., Peters. E., & Palmer, J. (2002). The           Van Berkum, J. J. A., van den Brink, D., Tesink, C. M. J.
   Eyelink Toolbox: Eye tracking with MATLAB and the                 Y., Kos, M., & Hagoort, P. (2008). The neural integration
   Psychophysics Toolbox. Behavior Research Methods,                 of speaker and message. Journal of Cognitive
   Instruments & Computers, 34, 613-617.                             Neuroscience, 20, 580 – 591.
Creel, S. C., Aslin, R. N., & Tanenhaus, M. K. (2008).
   Heeding the voice of experience: The role of talker
   variation in lexical access. Cognition, 108, 633 – 664.
                                                              1815

