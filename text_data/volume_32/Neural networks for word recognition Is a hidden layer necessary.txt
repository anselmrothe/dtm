UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Neural networks for word recognition: Is a hidden layer necessary?
Permalink
https://escholarship.org/uc/item/3051q9xv
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Dandurand, Frederic
Hannagan, Thomas
Grainger, Jonathan
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

               Neural networks for word recognition: Is a hidden layer necessary?
                              Frédéric Dandurand (Frederic.Dandurand@univ-provence.fr)
                               Laboratoire de Psychologie Cognitive, CNRS, Aix-Marseille University
                                               3, place Victor Hugo, 13331 Marseille, France
                                       Thomas Hannagan (thom.hannagan@gmail.com)
         Laboratoire de Sciences Cognitives et Psycholinguistique, EHESS/CNRS/DEC-ENS, École Normale Supérieure
                                                           29 rue d’Ulm, 75005 Paris
                                 Jonathan Grainger (jonathan.grainger@univ-provence.fr)
                               Laboratoire de Psychologie Cognitive, CNRS, Aix-Marseille University
                                               3, place Victor Hugo, 13331 Marseille, France
                              Abstract                                   primes formed of a subset of the target’s letters (e.g., grdn-
   We study neural network models that learn location invariant
                                                                         garden) compared with a prime formed of the same subset
   orthographic representations for printed words. We compare            of letters in the wrong order (e.g., gdrn-garden).
   two model architectures: with and without a hidden layer. We             A number of models have been proposed for an
   find that both architectures succeed in learning the training         intermediate level of coding that can account for these
   data and in capturing benchmark phenomena of skilled                  priming effects (see Grainger, 2008 for a review). Notably,
   reading – transposed-letter and relative-position priming.            the Grainger and Van Heuven (2003) model of orthographic
   Networks without a hidden layer use a strategy for identifying        processing was the inspiration for a computational model
   target words based on the presence of letters in the target
   word, but where letter contributions are modulated using the          that learned to map location-specific letter identities (letters
   interaction between within-word position and within-slot              coded as a function of their position in a horizontal array)
   location. This modulation allows networks to factor in some           onto location-invariant lexical representations (Dandurand,
   information about letter position, which is sufficient to             Grainger, & Dufau, 2010). Because parsimony dictates to
   segregate most anagrams. The hidden layer appears critical            assume a single intermediate level of representation, we
   for success in a lexical decision task, i.e., sorting words from      considered a neural network architecture with a single
   non-words. Networks with a hidden layer better succeed at
                                                                         hidden layer.
   correctly rejecting non-words than networks without a hidden
   layer. The latter tend to over-generalize and confuse non-               This network architecture with a hidden layer successfully
   words for words that share letters.                                   captured transposed-letter and relative-position priming
                                                                         effects (Dandurand et al., 2010). Intermediate
                                                                         representations were explicitly probed and analyzed as
   Keywords: Computational modeling, word recognition,
   neural networks, reading, priming effects.                            patterns of activation at the hidden layer (Hannagan,
                                                                         Dandurand, & Grainger, submitted; see also Plaut,
                          Introduction                                   McClelland, Seidenberg, & Patterson 1996 for a discussion
                                                                         of internal representations in neural networks). These
An important cognitive activity involved in skilled reading              patterns were found to have two important characteristics.
is the mapping of retinal images of letters onto abstract                First, letters seemed to be represented in a semi-location-
word representations. Skilled readers can identify words                 invariant fashion at the hidden layer. Second,
relatively easily (although not perfectly, see e.g., Rayner,             representations at the hidden layer were well-characterized
White, Johnson, Liversedge, 2006) even when letter order is              as a holographic overlap coding in which small changes of
jumbled, except for the first and last letters. This suggests            the inputs resulted in small differences in hidden layer
that at least one intermediate level of coding exists that               representations. More specifically, differences in patterns of
abstracts away from absolute letter position and instead                 hidden layer activations were monotonically related to
codes some information about relative letter order. Such an              differences in identity and position of input letters. For
intermediate level of representation has been studied using a            example, patterns of activity at the hidden layer were more
number of techniques including masked priming (see                       different for a two-letter substitution at the input (POLL vs.
Grainger, 2008 for a review). Robust priming effects found               BULL) than a single letter substitution (PULL vs. BULL)
include the transposed-letter effect and the relative-position           when position in the horizontal array was kept constant.
effect. The transposed-letter effect describes the superior              Furthermore, differences in patterns of activity were also
priming observed from primes formed by transposing two of                larger when the input word was moved by two positions in
the target’s letters (e.g., gadren-garden) compared with                 the alphabetic array (#THAT##### vs. ###THAT###) than
primes formed by substituting two of the target’s letters                moved by a single position (#THAT##### vs.
(e.g., galsen-garden). The relative-position priming effect              ##THAT####). Holographic overlap coding explains the
describes a processing advantage for targets preceded by                 observed transposed-letter and relative-position priming and
                                                                     688

makes a number of predictions which are tested in this              (one out of 26 possible letters, for each slot) and output
article; see (Hannagan et al., submitted) for details.              units (one out of 1179 words, also with logistic activation
   As they map letters onto words, skilled readers can also         functions). Networks learn to associate letter strings
perform lexical decision, that is, deciding if a string of          presented at the input with the corresponding output unit
letters is a word or a non-word (Meyer & Schvaneveldt,              coding for some word. For further details, see (Dandurand et
1971). Lexical decision has been extensively studied, and a         al., 2010).
number of models exist to account for human performance                We trained and tested samples of 10 networks for each
(e.g., Ratcliff, McKoon, & Gomez, 2004). In the current             condition (with and without a hidden layer). Networks
work, we test our models on a simple lexical decision task,         varied in the random initial values of their connection
assuming a minimal lexical read-out mechanism, namely               weights.
that words would activate output units more than non-                  In tests that involve lexical decision, we present some
words. We are not, however, claiming that this ability              pattern at the input and compute activations of all output
should be interpreted as a full-blown or realistic model of         units. Output units activated above a threshold value of 0.9
lexical decision. Note that performing lexical decision is not      are considered as active, and thus the word associated with
trivial for networks because non-words are never seen in            the unit as having been detected. For tests that involve
training as negative evidence, and thus networks may be             priming, a measure dubbed “target supremum measure”
expected to over-generalize what they consider as words.            (Dandurand et al., 2010) quantifies the ability of some
   In the current study, we revisit the assumption previously       prime to activate the output unit associated with the target
made for the need of a hidden layer. We ask if such a hidden        word more than any other active output unit 1.
layer is required for networks to learn location invariant
orthographic representations for printed words. To this                                         Results
effect, we contrast two model architectures: (1) the previous
model with a hidden layer and (2) a simpler model without a         Learning the training set
hidden layer. In this alternative model, letters are mapped to
words directly using a layer of connection weights. We              The training set comprises 1179 words, 24.0% (N = 283) of
compare the two architectures on a number of criteria: (1)          which are anagrams. Anagrams come in pairs (111 pairs x 2
their ability to learn the training set, including the anagrams     = 222 words), triplets (15 triplets x 3 = 45 words) and
present in the training data, (2) their size and complexity,        quadruplets (4 quadruplets x 4 = 16 words). These
(3) their capacity to simulate key priming effects, and (4)         quadruplets (1. live – evil – veil – vile; 2. team – meat –
their capacity to perform a simple lexical decision task.           mate – tame; 3. tied – diet – tide – edit; 4. pear – rape – reap
Finally, we investigate how processing and representations          – pare) should be especially difficult to discriminate because
differ, how networks without a hidden layer manage to               the same four letters activate four different target word
segregate anagrams, and how well these networks conform             units.
with the predictions made by holographic overlap coding.               Networks with a hidden layer achieve perfect
   Our goal is to gain insights into the role that the hidden       performance (100%) on the target supremacy measure for
layer plays in performing a word recognition task. Without a        the training set. In contrast, networks without a hidden layer
hidden layer, networks are computationally limited to taking        reach 98.6%, and more than 95% of anagrams were
decisions based on weighted combinations of input letters. It       successfully segregated. In the 1.4% of errors, activations of
is unclear how, and even if, such model could handle                output units (including the target) fail to reach the threshold
anagrams where the identity of input letters is insufficient to     of 0.9. These failure-to-recognize errors involved pairs of
discriminate words, and where position of letters has to be         anagrams (bear – bare, and read – dear) or sets of words
taken into account.                                                 from an orthographic neighborhood sharing three letters
                                                                    (bare – mare – pare, seep – seed – deep, and pull – burl –
                                                                    bull).
                           Methods
We compare two architectures of standard multilayer                 Model size and complexity
perceptron neural networks. The first one includes a single
                                                                    From a size and complexity perspective, the hidden layer
hidden layer of 91 hidden units with logistic activation
                                                                    adds 91 extra units, and an additional layer of processing.
functions, identical to (Dandurand et al., 2010). The second
                                                                    However, in terms of size, networks with a hidden layer
one has no hidden layer (inputs are directly connected to
                                                                    actually have fewer connection weights (132 219, i.e., 1179
outputs). In the two architectures, adjacent layers are fully
connected, and are trained using standard backpropagation
(learning rate = 0.1, momentum = 0.9) until an SSE of 30.              1
                                                                         Models allow for multiple outputs to be activated, but some
Training material consists of 1179 real words of four letters       competitive, winner-takes-all mechanism could be used to select
(same as the one used by McClelland, & Rumelhart, 1988)             the most active one. Item-level target supremum value was set to 1
presented in all 7 possible positions of an alphabetic array        when the prime activated the output unit associated with the target
(e.g., #ABLE#####, ######ABLE where # are empty,                    lexical item more than any other unit; it was set to 0 otherwise. The
blank slots). Local (sparse) coding is used for input letters       target supremum measure of a set of primes was computed as the
                                                                    mean of item-level values for the primes in the set.
                                                                689

outputs x (91 hidden + 1 bias) + 91 hidden x (260 inputs + 1                As we can see, patterns of results are very similar for
bias)) than networks without a hidden layer (307 719                      networks with (see Figures 5 and 6 in Dandurand et al.,
connection weights (1179 outputs x (260 inputs + 1 bias)),                2010) and without a hidden layer. More specifically,
despite having two layers of weights. We can think of the                 relative-position primes formed of forward letter subsets
hidden layer as enforcing data compression from 260 inputs                yield a higher target supremum measure than backward
to 91 hidden units, which reduces the number of                           primes (see Figure 1); and transposed-letter primes
connections required.                                                     containing central letters from the target word yield a larger
                                                                          supremum measure than primes with central letters from a
Priming effects                                                           different word (see Figure 2).
Networks are tested using the relative-position priming and
transposed-letter priming manipulations described in                      Lexical Decision
(Dandurand et al., 2010). Examples of primes for word                     To test for lexical decision, we assess performance (target
ABLE are overlapped on the graphs below, see (Dandurand                   supremum measure) on three simple testing conditions: (1)
et al., 2010) for details of the content of testing sets. Primes          words: all words seen in training in all positions (for a total
(e.g., ###ABE####) are expected to activate the target word               of 1179x7 patterns); (2) non-words: a sample of 100
(e.g., ABLE) more so than any other word, especially when                 patterns made of four random letters presented at a random
prime letters are in the correct, forward order (ABE) and not             position in the alphabetic array (e.g. #JKTS#####,
the reserved, backward (EBA) order.                                       ######HIQL, ###BXGA###); (3) letters: a sample of 100
                                                                          patterns, each made of a randomly selected letter repeated to
                                                                          match word length presented at a random position in the
                                                                          alphabetic array (e.g., ##AAAA####, #####HHHH#).
                                                                          Word patterns are expected to activate, and only activate,
                                                                          their target word unit. We also expect no output word unit to
                                                                          be activated above threshold for patterns in the non-words
                                                                          and in the letters conditions.
                                                                            Results are shown in Figure 3. As we can see, network
                                                                          with a hidden layer perform much better than networks
                                                                          without one. Networks without a hidden layer are especially
                                                                          poor at correctly rejecting letter patterns, activating several
                                                                          of the words that contain the letter. For example, input
                                                                          pattern ###PPPP### activates 85 word units above
                                                                          threshold including part, open, help, kept, step, post and
                                                                          ship. Similarly, for non-words, errors involve incorrectly
                                                                          activating words that share some letters with the target. For
                                                                          example, input pattern ####KNKR## activates the
                                                                          following word units above threshold: kind, dark, park,
     Figure 1 – Target supremum results for the relative-position         mark, link, monk, fork, tank, pork, cork, knot, and trek.
  priming test. Example primes provided for target word ABLE.
                                                                                                                       With a hidden layer     Without a hidden layer
                                                                                                            1
                                                                                                           0.9
                                                                                                           0.8
                                                                           Proportion correct (Accuracy)
                                                                                                           0.7
                                                                                                           0.6
                                                                                                           0.5
                                                                                                           0.4
                                                                                                           0.3
                                                                                                           0.2
                                                                                                           0.1
                                                                                                            0
                                                                                                                     Words                   Non-words                  Letters
                                                                                                            Figure 3 - Accuracy of networks at accepting words, and
                                                                                                                  rejecting non-words and repeated letters.
    Figure 2 – Target supremum results for the transposed-letter
  priming test. Example primes provided for target word ABLE.
                                                                    690

                          Discussion                                  This simple scheme makes each letter vote for the target
                                                                      word, and a word must get 4 votes to be fully activated. This
   To sum up our results, both networks with and without a
                                                                      may explain why letters activate very strongly a number of
hidden layer correctly recognized words at rates reaching
                                                                      targets, as AAAA also counts as 4 letters of evidence for
98.6% to 100%. Performance was high even on anagrams
                                                                      ABLE. However this does not explain how the network can
(95% to 100%). Both types of networks showed relative-
                                                                      distinguish between anagrams.
position (see Figure 1) and transposed-letter priming effects
                                                                         Figure 4 illustrates how networks might manage to
(see Figure 2). Networks with a hidden layer are more
                                                                      segregate anagram patterns. Boxes in the plot show the
complex due to the additional hidden units, but contain
                                                                      average magnitude of connection weights between within-
fewer connection weights. The critical benefit of the hidden
                                                                      word position on the Y axis and within-alphabetic-array
layer appears to be in the ability of networks to correctly
                                                                      (within-slot) location on the X axis for letters relevant to the
reject non-words and strings of repeated letters (see Figure
                                                                      identification of the target word. For example, for pattern
3).
                                                                      ABLE###### connection weights would be found at boxes
Segregating anagrams                                                  (X,Y): A(1,1), B(2,2), L(3,3) and E(4,4); whereas for
                                                                      pattern ###ABLE### relevant boxes would be A(4,1),
One of the most difficult aspects of the task is arguably that        B(5,2), L(6,3) and E(7,4).
of segregating anagrams. While regular words can be                      As we can see, there is a negative correlation (r = -0.73, p
discriminated on the basis of differences of at least one             < 0.01) in the first within-word position (P) between the
letter, anagram identification must rely solely on the relative       average magnitude of connection weights (C) and location
position of letters within word. The task appears especially          (L), while the correlation is positive in the last position (r =
difficult for the network without a hidden layer which is             0.67, p < 0.01). Namely, for the first letter of the word, the
limited to computing linear combinations of independent               connection weight is largest for smaller locations in the slot,
inputs.                                                               and decrease as location in slot increases. This makes
                                                                      intuitive sense, as A######### is better evidence for word
Networks with a hidden layer                                          ABLE (or any word that begins with letter A) than
   In networks with a hidden layer, holographic overlap               ######A###, which could be evidence for ######ABLE,
coding (Hannagan et al., submitted) can explain both                  but also for ####THAT## or any word having an A in any
transposed-letter priming and the ability of networks to              position. The correlation is reversed for the last slot where
segregate anagrams. During learning, networks form semi-              say letter E provides more evidence for ABLE if it appears
location specific representations for individual letters -            later in the word. The direction reversal suggests an
assigning similar representations to the same letter input            interaction between location (L) and within-word position
seen at different positions - that is, networks combine letters       (P).
in a continuous manner to build a string code. Displacing
letters (whether in primes or in anagrams) results in small,
but measurable differences in patterns of activation at the
hidden layer. In the case of transposed-letter priming, most
words have no orthographic neighbor, and therefore the
target word is still the most activated (e.g., WTIH activates
word WITH), and so will be recognized according to the
target supremum measure. Networks can capitalize on this
small difference in hidden pattern activation to segregate
words. It is plausible that this small difference gets
enhanced or amplified by the processing of the second layer
of weights (hidden to output weights) to generate the correct
classification of anagram patterns (e.g., ABLE and BALE as
distinct).
Networks without a hidden layer
   To gain insights into how networks without a hidden layer
can segregate anagrams, we study the connection weights                   Figure 4 - Average magnitude of weights connecting input units
between inputs and outputs after training. The first thing we                relevant to identifying an output word, by location in the
notice is that connection weights strongly code for the mere          alphabetic array (X axis) and by position with target word (Y axis).
presence of letters. Typically, connection weights are small           Black boxes correspond to positions where letters were never seen
for letters not present in the target word, and large for letters       in training (e.g., letter A was never seen in slots 8 to 10 for word
                                                                           ABLE, and similarly letter E was never seen in slots 1 to 3).
that are present, irrespective of position. For instance,
connections weights from input units that code letters A, B,
L, and E (in all slots where they have been seen during
training) are large to output unit coding for word ABLE.
                                                                  691

   To test for this interaction, we performed a linear                      XXXX#### and ######XXXX. As we can see, distances
regression with the following model (including LxP to test                  increase with displacement, in accordance with the
for interaction effects):                                                   proximity effect.
   C = b0 + b1L+b2P+b3LxP                                (1)
   In the fitted model, we get b0 = 31.0 (p < 0.001), b1 = -4.0                Table 1: Normalized Euclidian distance for networks with and
(p < 0.001), b2 = -8.9 (p < 0.001) and b3 = 62.9 (p < 0.001).                without a hidden layer, as a function of displacement of letters in
This confirms the significant interaction. Redoing the                                                the input vector
analysis with central locations only (4 to 7), we also get                                                                          Euclidian distance
significant coefficients, b0 = 22.6 (p < 0.001), b1 = -1.8 (p <               Displacement                                   With hidden        Without hidden
0.001), b2 = -4.6 (p < 0.001) and b3 = 30.4 (p < 0.001).                            2                                            1.3                  1.5
   To sum up, the processing strategy or coding scheme that                         3                                            2.2                  1.7
networks without a hidden layer develop can be described as
follows: most important is the number of letters shared                        Holographic overlap coding also makes a prediction about
between inputs and targets independently of position – we                   the effect of letter substitutions: the more letters are
can think of this as input letters providing independent votes              replaced, the larger the difference in activation should get.
for the target words that contain them. The presence of                     We empirically test this hypothesis by generating samples
letters is then modulated by the interaction between location               of 100 test items for which the target word and the location
and position. This scheme is sufficient to explain how                      of letters in the input slot is randomly chosen. We compute
networks can discriminate between anagrams. For instance                    the Euclidian distance between patterns of activation
in strings ABLE and BALE, an equal number of four letter                    generated in one of three conditions: (1) transposition –
votes go to each word, and connection weights between                       transpose two letters, randomly chosen (e.g., V1 = ABLE 
small slot positions and target word ABLE are slightly                      V2 = ABEL), (2) one letter substitution with a random letter
larger for letter A than letter B. In contrast, for target word             (e.g., V1 = ABLE  V2 = ABWE), (3) one letter
BALE, the connection weight is slightly larger for letter B                 substitution with another letter of the target – that is, a letter
than letter A. This difference enables the correct target to be             repetition (e.g., V1 = ABLE  V2 = BBLE).
activated.
   This coding scheme also accounts for the priming effects:                                               3
larger priming as the number of letters shared between
                                                                                                          2,5                                    Without hidden layer
primes and targets increase, and larger priming as the
                                                                              Normalized distance index
agreement increases between the order of letters in the                                                                                          With hidden layer
                                                                                                           2
prime and in the target.
                                                                                                          1,5
Comparison with holographic overlap coding
                                                                                                           1
  How does this processing strategy in networks without a
hidden layer compare to holographic overlap coding used by                                                0,5
networks with a hidden layer? As mentioned in the
introduction, holographic overlap coding makes two                                                         0
important predictions about similarity of activation patterns:                                                  Repetition        Substitution   Transposition
a proximity effect and a disruption of activation when                        Figure 5: Normalized Euclidian distance index as a function of
replacing letters with other letters of the word (e.g., AAAA                               transformation and architecture type
for word ABLE). The normalized Euclidian distance
between two activation vectors Act(V1) and Act(V2) is                          Holographic overlap coding predicts similar distances for
computed as follows:                                                        letter repetitions and substitutions, and a lower distance for
  dist = √(Σ Σ (Act(V1ij) – Act(V2ij))2) / (Npattern x Nactivation)         transpositions. As we see in Figure 5, this is precisely the
  Activations are taken at the hidden layer, or at the output               pattern of distances measured for networks with a hidden
layer for networks without a hidden layer. The two Σ                        layer. However, these predictions are not verified for
indicate summing over all patterns and all activation values.               networks without a hidden layer, namely because distances
  The proximity effect predicts that the Euclidian distance                 are too large for the letter repetition set. This somewhat
between activation vectors V1 and V2 should increase                        counter-intuitive result can be explained by the fact that
monotonically with the magnitude of displacement of the                     repeating a letter means, on average, replacing a letter with
vectors (i.e., distances). As shown in Table 1, a proximity                 a rather frequent letter compared to substituting with a
effect is observed indeed, when vectors V1 are in the central               randomly chosen one (as in the substitution case). And thus,
position (###XXXX###) and vectors V2 vary in position.                      many output words activate in the repetition case, which
Distances presented in the table are normalized using a                     increases the distance due to the higher activation of the
displacement of 1 as a reference (that is, V2 ##XXXX####                    non-target words. In sum, we fail to find evidence that
and ####XXXX##). Vectors V2 for displacement 2 are                          networks without a hidden layer implement a holographic
#XXXX##### and #####XXXX#; and for displacement 3:                          overlap coding scheme.
                                                                      692

Lexical decision, over-generalization and their                      units. As a result, networks with a hidden layer have fewer
theoretical implications                                             than half the number of connection weights of networks
                                                                     without a hidden layer.
   In the lexical decision task, correct rejection of non-words
                                                                        Computational models of word identification are expected
and letters can be interpreted as a test of generalization,
                                                                     to perform well at lexical decision, as humans do. The
which probes the network’s ability to correctly set the
                                                                     model with the hidden layer suggests a parsimonious
boundary of word acceptance. Based on a poverty of
                                                                     account of lexical decision as an emergent property of the
stimulus argument, we may expect networks to over-
                                                                     word recognition task (although, again, the setup is highly
generalize, that is being overly liberal in accepting strings as
                                                                     simplified, and further work would be necessary to fully
words, because networks see positive evidence for words
                                                                     assess how good of a lexical decision model this is). An
but never see any negative evidence, i.e., they are never
                                                                     alternative explanation consists in using an additional
trained to reject non-words. These over-generalization errors
                                                                     module (performed before, or in parallel with, word
are much more common in the network without a hidden
                                                                     identification). For the latter, a network without a hidden
layer. This has interesting theoretical implications for the
                                                                     layer is sufficient to simply recognize words.
functional role of the hidden layer where independent letters
are combined. Given that each letter/position has a uniquely
defined code, the network just has to find a way to integrate                             Acknowledgments
them so as to ensure that each combination is unique. For            This project was supported by the Agence Nationale de la
instance, using a simple averaging approach, the resulting           Recherche (grant no. ANR-06-BLAN-0337) and the
code for AAAA will be very close to A, in effect providing           Europrean Research Council (ERC-230313).
only evidence for one letter. Without combinations,
networks have to base their decisions on some position-                                        References
weighted voting scheme relating to the presence of letters.          Dandurand, F., Grainger, J., & Dufau, S. (2010). Learning
This scheme fails to reject non-words cases that consist of 4                  location invariant orthographic representations for
repetitions of a letter from the target word.                                  printed words. Connection Science, 22(1), 25-42.
   Beyond simply removing letter duplicates, the hidden                        doi:10.1080/09540090903085768
layer may well be coding for some letter combination, or             Grainger, J. (2008). Cracking the orthographic code: An
sub-lexical units, as postulated in the Grainger and Van                       introduction. Language and Cognitive Processes,
Heuven’s (2003) model and other models. A simple                               23(1), 1-35.
approach to lexical decision could thus be seen as follows:          Grainger, J., & van Heuven, W. J. B. (2003). Modeling
letters provide evidence for activating sub-lexical units.                     letter position coding in printed word perception.
These sub-lexical units would in turn be combined to                           In The Mental lexicon (pp. 1-23). New York: Nova
activate target words. For non-words, activation of sub-                       Science Publishers.
lexical units would be small, and result in activation of            Hannagan, T., Dandurand, F., & Grainger, J. (submitted).
output units that fall below threshold.                                        Broken symmetries in a location invariant word
                                                                               recognition network, Neural Computation.
                          Conclusion                                 McClelland, J. L., & Rumelhart, D. E. (1988). Explorations
   To summarize, the hidden layer developed a holographic                      in parallel distributed processing. Boston, MA:
overlap coding scheme which explains priming effects and                       MIT Press.
segregation of anagrams. Because it is sensitive to letter           Meyer, D. E., & Schvaneveldt, R. W. (1971). Facilitation in
substitutions, this scheme also allows networks with a                         recognizing pairs of words: Evidence of a
hidden layer to correctly reject most non-words.                               dependence between retrieval operations. Journal
   In contrast, networks without a hidden layer have                           of Experimental Psychology, 90(2), 227-234.
developed a strategy for identifying target words largely            Plaut, D. C., McClelland, J. L., Seidenberg, M. S., &
based on presence of letters but where letter contributions                    Patterson, K. (1996). Understanding Normal and
are modulated using the interaction between within-word                        Impaired Word Reading: Computational Principles
position and within-slot location. This modulation allows                      in Quasi-Regular Domains. Psychological Review,
networks to factor in some information about letter position,                  103(1), 56-115.
which is sufficient to segregate most anagrams, and                  Ratcliff, R., McKoon, G., & Gomez, P. (2004). A Diffusion
replicate the previously observed priming effects. On the                      Model Account of the Lexical Decision Task.
other hand, these networks are poor at the lexical decision                    Psychological Review, 111(1), 159-182.
task, as they tend to over-generalize and confuse non-word           Rayner, K., White, S., Johnson, R., Liversedge, S. (2006).
strings as words. As long as the number of letters is the                      Raeding Wrods With Jubmled Lettres; There Is a
same and that all input letters exist in the target word,                      Cost. Psychological Science, 17(3), 192-193
networks do not require that all letters in the target word are
present to activate it.
   The hidden layer also implements some data compression,
by forcing 260 input units to be represented onto 91 hidden
                                                                 693

