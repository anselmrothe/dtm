UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Bottom-Up Parsing Model of Local Coherence Effects
Permalink
https://escholarship.org/uc/item/4v3265ds
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Morgan, Emily
Keller, Frank
Steedman, Mark
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                        A Bottom-Up Parsing Model of Local Coherence Effects
                                                Emily Morgan (emily@ling.ucsd.edu)
                                          Department of Linguistics, 9500 Gilman Drive #108
                                                          La Jolla, CA 92093, USA
                                                 Frank Keller (keller@inf.ed.ac.uk)
                                             Mark Steedman (steedman@inf.ed.ac.uk)
                                                 School of Informatics, 10 Crichton Street
                                                          Edinburgh EH8 9AB, UK
                              Abstract                                   (2)      U/R: The coach smiled at the player thrown a frisbee.
   Human sentence processing occurs incrementally. Most mod-
   els of human processing rely on parsers that always build con-        (3)      A/U: The coach smiled at the player who was tossed
   nected tree structures. But according to the theory of Good                    a frisbee.
   Enough parsing (Ferreira & Patson, 2007), humans parse sen-
   tences using small chunks of local information, not always            (4)      U/U: The coach smiled at the player who was thrown
   forming a globally coherent parse. This difference is appar-
   ent in the study of local coherence effects (Tabor, Galantucci,                a frisbee.
   & Richardson, 2004), wherein a locally plausible interpreta-
   tion interferes with the correct global interpretation of a sen-      These four sentences, all intended to be close paraphrases of
   tence. We present a model that accounts for these effects using       one another, illustrate a puzzle: while the majority of read-
   a wide-coverage parser that captures the idea of Good Enough          ers reject (1), they accept (3) and (4), with mixed results for
   parsing. Using Combinatory Categorial Grammar, our parser
   works bottom-up, enforcing the use of local information only.         (2). These sentence differ on two dimensions: the past par-
   We model the difficulty of processing a sentence in terms of the      ticiple can be Ambiguous (such as tossed, which can be a
   probability of a locally coherent reading relative to the prob-       past participle or a past tense form) or Unambiguous (such as
   ability of the globally coherent reading of the sentence. Our
   model successfully predicts psycholinguistic results.                 thrown), and the relative clause can be Reduced (without who
   Keywords: sentence processing; parsing complexity; local              was) or Unreduced (with who was). Neither of these alterna-
   coherence; Good Enough parsing; Combinatory Categorial                tions generally changes the grammaticality of a sentence, so
   Grammar                                                               we would naively predict that if (4) is acceptable, then (1) is
                                                                         as well. Our challenge is to explain why this naive predic-
                          Introduction                                   tion is wrong. Intuitively, it seems that the local coherence
A major topic of inquiry in cognitive science is the process             of the substring the player tossed a frisbee in (1) as a plausi-
by which people produce and comprehend sentences. Hu-                    ble complete sentence is distracting from its globally correct
man sentence processing is known to proceed incrementally:               interpretation as an object with a relative clause.
people construct syntactic and semantic interpretations grad-               Tabor, Galantucci, and Richardson demonstrate the exis-
ually as a sentence unfolds, rather than waiting until after the         tence of local coherence effects as a psycholinguistic phe-
whole sentence has been received. But although we know that              nomenon in two different studies: in the first, they find in-
syntactic information becomes available progressively while              creased reading times at the ambiguous past participle in (1).
comprehending a sentence, it is still an open question to what           They present subjects with sentences from 20 sets of sen-
extent decisions made early in the parsing process can con-              tences like those seen above and measure reading times for
strain later decisions.                                                  each word using self-paced reading. In this methodology,
   One phenomenon that can shed light on this question is                longer reading times are taken to indicated increased process-
local coherence effects. Local coherence effects arise when              ing difficulty. As expected based on previous studies (e.g.
a sentence includes a substring with a plausible local inter-            Ferreira & Clifton, 1986), they find substantially increased
pretation that is incompatible with the global interpretation.           reading times for the Reduced cases as compared to the Unre-
(In other words, the interpretation is merely locally coherent,          duced cases, both on the past participle (e.g. tossed) and on
but not globally coherent.) A typical example (from Tabor,               the following word. Moreover, they find an unexpected in-
Galantucci, & Richardson, 2004) is:                                      teraction between Ambiguity and Reducedness: while the
                                                                         A/U reading times are not significantly different from the
(1)      A/R: The coach smiled at the player tossed a frisbee.           U/U reading times, the A/R reading times are substantially
A typical reader, seeing this sentence for the first time, will          increased relative to the U/R reading times. This superaddi-
find it difficult to understand and will likely judge it to be           tive difficulty of the A/R condition is the signature of a local
ungrammatical. But this difficulty is unexpected in light of             coherence effect.
similar sentences:                                                          In the second experiment, Tabor, Galantucci, and Richard-
                                                                         son replicate the first using a grammaticality judgement task.
                                                                     1559

                                                                                       assumption may be convenient. But for a parser to be credible
                                    Grammaticality Judgement Data from Tabor,          as a model of human sentence processing, it must be able to
                                        Galantucci, and Richardson (2004)              predict these psycholinguistic effects, which requires relaxing
                                    0.9                                                this assumption.
   Proportion of sentences judged
                                    0.8                                                   An alternate theory of sentence processing is Ferreira and
                                    0.7                                                colleagues’ Good Enough (GE) parsing. Ferreira and Patson
                                    0.6                                                (2007) describe GE parsing:
                                    0.5                               Unambiguous          People compute local interpretations that are sometimes
                                                                      Ambiguous
           ungrammatical
                                    0.4
                                                                                           inconsistent with the overall sentence structure, indi-
                                    0.3
                                                                                           cating that the comprehension system tries to construct
                                    0.2
                                                                                           interpretations over small numbers of adjacent words
                                    0.1
                                                                                           whenever possible and can be lazy about computing a
                                     0
                                                                                           more global structure and meaning.
                                          Unreduced     Reduced
                                                                                       The GE theory of parsing asserts that people do not con-
                                                                                       struct full representations for sentences the majority of the
                                                                                       time. Rather, they construct just enough to complete the task
Figure 1: Grammaticality judgement data from Tabor, Galan-                             at hand, only constructing a further representation if neces-
tucci, and Richardson (2004). The signature of a local coher-                          sary. Moreover, because people base their first-pass construc-
ence effect is the superadditive proportion of ungrammatical                           tions on local information and generally construct only partial
judgements in the Ambiguous/Reduced condition.                                         parse trees, these partial parses may contradict one another.
                                                                                       A GE parsing account can thus easily account for local co-
                                                                                       herence effects. We will develop a computational model of
They find decreased acceptance of Reduced sentences as
                                                                                       why local coherence effects arise within the framework of
grammatical, with an interaction between Ambiguity and Re-
                                                                                       GE parsing.
ducedness such that A/R sentences are judged unacceptable
superadditively often (see Figure 1). Once again, decreased                            Previous Models of Local Coherence Effects
acceptability judgements are taken to indicate processing dif-                         Two models have previously attempted to account for local
ficulty.                                                                               coherence effects: Levy (2008) uses a noisy-channel model
   Note that sentences in the A/R condition are not just stan-                         to argue that because there is uncertainty in linguistic input,
dard garden path sentences. In a standard garden path sen-                             the parse of a sentence should be modeled as a probability
tence, the disambiguating information comes after the reader                           distribution over a set of candidate sentences (including the
has already been led astray. In contrast, in sentences such                            intended sentence and its near-neighbors). Given such a prob-
as (1), the disambiguating information comes at the begin-                             ability distribution, the effect of reading each word can be
ning of the sentence. Thus the reader in theory already knows                          modeled and quantified in terms of a belief update. Levy pre-
that tossed cannot be a past tense form and must be a past                             dicts that a larger change in beliefs will correspond to greater
participle. Yet despite that, these sentences cause processing                         processing difficulty and longer reading times. This in turn
difficulty.                                                                            predicts local coherence effects because the rarer sentences
   A model of human sentence processing should be able to                              provoke larger changes in belief.
predict the difficulty of sentences with local coherence ef-                              Levy’s model only considers fully connected and gram-
fects. However, most existing models cannot. In particu-                               matical (partial) parses as candidates, thus it does not cap-
lar, most standard theories of parsing assume that that all                            ture the intuition of GE parsing. An additional limitation of
accrued knowledge from the parsing process is taken into                               the model is that due to the computational load of calculat-
account at all times. Models following this assumption can                             ing near-neighbors, it has only been implemented using a toy
straightforwardly account for standard garden paths because                            Probabilistic Context Free Grammar (PCFG), rather than a
there is nothing inconsistent about initially misinterpreting a                        richer, wide-coverage language model.
sentence before having access to the disambiguating informa-                              The other previously existing model of local coherence
tion. But these models cannot take the same position in ac-                            effects comes from Bicknell and Levy (2009). They again
counting for local coherence effects: when the disambiguat-                            model local coherence effects as arising from belief updates.
ing information has already been seen and smiled has already                           Specifically, they model them as the consequences of an up-
been recognized as the main verb of the sentence, they can-                            date from a bottom-up prior belief to a posterior belief that
not entertain the inconsistent possibility that tossed is also a                       takes top-down information into account. They thus pre-
main verb. Computational implementations of wide-coverage                              dict processing difficulty in the case of locally coherent sub-
parsers generally also make this assumption of global consis-                          strings because the bottom-up statistics make strong predic-
tency (e.g. Roark, 2001; Sturt, Costa, Lombardo, & Frasconi,                           tions about the category of the substrings, which are then con-
2003; Demberg & Keller, 2008). For many applications, this                             tradicted by top-down information.
                                                                                    1560

   This model begins to capture the idea of GE parsing by                  John      eats     apples        John        eats     apples
looking at substrings of different lengths. However, it has no              NP (S\NP)/NP NP                  NP      (S\NP)/NP NP
way to integrate the information it receives from these differ-                                   >             >T
                                                                                         S\NP            S/(S\NP)
ent substring lengths because evaluating these substrings is                                      <                          >B
                                                                                        S                         S/NP
post hoc, not an actual part of the parsing process. Addition-           (a) Right-branching deriva-                                  >
ally, like Levy’s (2008) model, it has only been implemented             tion                                           S
                                                                                                          (b) Left-branching derivation
using a toy PCFG.
   Thus there is currently no general, wide-coverage model of           Figure 2: Right- and left-branching CCG derivations for the
human parsing that implements a GE parsing strategy. Com-               sentence John eats apples. (S\NP)/NP is the CCG category
putational models of local coherence effects have instead had           for a transitive verb. Without type-raising, eats can only com-
to account for the phenomenon indirectly, either through a              bine with apples, yielding the typical right-branching deriva-
noisy channel model or by predicting the effects without ac-            tion in (a). With type-raising, John can combine immediately
tually simulating the parsing process, and have been confined           with eats, yielding the left-branching derivation in (b).
to parsing with small toy grammars. We will develop a model
to address these shortcomings.
                                                                        as person, number, and gender on NPs. These are notated as
     A New Model of Local Coherence Effects                             e.g. NP[3sf].
Our goal is to model the process by which local coherence ef-              A CCG derivation uses rules to combine categories. Pure
fects emerge as the result of Good Enough parsing, within the           CG relies on two rules, named > and <, to combine cate-
context of a wide-coverage parser. In the example sentence              gories:
The coach smiled at the player tossed a frisbee, our intuition
is that processing difficulty arises from the locally coherent          (5)      X/Y    Y →X      (>)
reading of the player tossed a frisbee, which distracts from            (6)      Y   X\Y → X      (<)
the globally coherent reading. Our model will capture this
intuition by using a strictly bottom-up parser to remove the            CCG introduces further combinatory rules that allow for
top-down influence of non-local constraints.                            more flexible notions of constituency than other grammar for-
   Strictly bottom-up parsing is frequently rejected as a plau-         malisms. In particular, it includes two lexical type-raising
sible model for human parsing because, it is claimed, it does           rules, named >T and <T:
not allow for incremental interpretation. The standard argu-            (7)      X → T /(T \X) (>T)
ment says that a clause can only be interpreted when it is
seen in full (i.e., at the end of a constituent). But in a strictly     (8)      X → T \(T /X) (<T)
right-branching language, this means that nothing can be in-            In these rules—which are here shown in the derivation, but
terpreted until the very end of the sentence because only then          in fact operate in the lexicon—T can be any lexical category
is any constituent completed.                                           taking X as argument. For instance, we could use >T to type-
   To overcome this objection, our parser uses the Combina-             raise NP to S/(S\NP). Applying this rule limits the other
tory Categorial Grammar (CCG) formalism to represent lin-               categories the NP can combine with. Intuitively, we can think
guistic structures. CCG was specifically designed to allow              of the output of this rule as similar to an NP with nominative
for incremental bottom-up parsing by using a more flexible              case-marking. It specifies not just that the word or phrase in
notion of constituents.                                                 question is a noun, but that it is a subject which must combine
Combinatory Categorial Grammar                                          with a predicate.
                                                                           These type raising rules allow us to parse a sentence in-
Combinatory Categorial Grammar is a grammar formalism
                                                                        crementally by forming nontraditional constituents, leading
based on Categorial Grammar (CG). We base our description
                                                                        to left-branching derivations (see Figure 2). CCG thus allows
of it here on Steedman (2000).
                                                                        each new word of the input to be incorporated into the ex-
   CCG revolves around functional categories and rules for
                                                                        isting constituent structure as it is encountered, which makes
combining them. Categories can be either functions or argu-
                                                                        incremental bottom-up parsing possible.
ments and are defined recursively: Base categories such as S
and NP represent arguments. Functions are of the form α/β               The Model
or α\β, where α and β are categories. To the right of the
                                                                        We take a bottom-up CCG parser as the basis of our model
slash is the argument of the function, and to the left is its
                                                                        of human sentence processing. In order to predict process-
result. The direction of the slash indicates the directionality
                                                                        ing difficulty caused by local coherence effects, we need a
of composition: / means the argument is to the right and \
                                                                        linking hypothesis to specify the relation between the parser
means the argument is to the left. An English verb phrase,
                                                                        output and psycholinguistic measures such as grammaticality
for example, will have the category S\NP, indicating that it
                                                                        judgements or reading times. Our linking hypothesis should
combines with an NP on its left and results in a sentence. We
                                                                        embody the theory of Good Enough parsing, focusing on in-
also allow a finite set of features on our base categories, such
                                                                    1561

terpretations of local substrings.                                     model over CCG categories: From the parent (starting with a
   We adapt a model proposed by Jurafsky (1996) to predict             ROOT node), a head is generated with a certain probability.
garden path effects. To make graded predictions, rather than           Then its sisters are generated with probability conditioned on
categorical distinctions, we will adopt a probabilistic frame-         the head category, the sister’s direction from the head, and
work, and consider the probabilities of various substrings             whether it is adjacent to the head. Although the number of
of a sentence. In particular, we could consider either the             CCG categories is theoretically infinite, our parser is con-
inside probability P(S → substring) (alternately written as            strained to only use categories that have appeared in the train-
P(substring | S)) or the inverse probability P(S | substring).         ing data set. With this constraint, the runtime of the parser is
We do not know of a computationally tractable way to calcu-            bounded by O(n3 ). The parser has been trained on sections
late P(S | substring) from our parser. Calculating the inside          1 through 22 of the CCGbank (Hockenmaier, 2003), a CCG
probability, on the other hand, is a fundamental part of the           version of the Penn treebank.
parsing process. It is most parsimonious to base our model                 Our experiments use two different lexicons. The first lexi-
on the inside probabilities that are already being calculated.         con is that taken from sections 1 through 22 of the CCGbank.
   Our intuition is that if an incorrect interpretation of a sub-      However, this lexicon is too small to parse the majority of
string is highly plausible relative to the correct interpretation      the sentences we wish to consider. To obtain a larger lexicon,
of the sentence, then it will cause processing difficulty. In          we parsed six months of the New York Times (comprising
a sentence such as The coach smiled at the player tossed a             approximately 50 million word tokens) taken from the Giga-
frisbee, the substring that we expect to cause difficulty is the       word corpus (Graff, 2003). Sentences from the corpus were
locally coherent substring the player tossed a frisbee. We             passed through the RASP tokenizer (Briscoe, Carroll, & Wat-
thus consider the ratio:                                               son, 2006) and then parsed using the C&C CCG parser (Cur-
                                                                       ran, Clark, & Bos, 2007). This state-of-the-art parser obtains
               P(S → the player tossed a frisbee)                      labelled precision of 84.8% and labelled recall of 84.5% on
    P(S → The coach smiled at the player tossed a frisbee)             section 23 of the CCGbank. It is extremely fast and provides
In this case, the ratio will be high because The player tossed a       the best parse accuracy from a CCG parser, making it conve-
frisbee is a relatively likely sentence. In the other three cases,     nient for obtaining large amounts of data to construct a larger
the ratio will be low because none of the following are very           lexicon. (However, it is not a cognitively plausible parser, as
plausible sentences:                                                   it relies on its supertagger and other cognitively implausible
                                                                       tricks to speed its parsing.) From this parsed sample, we ex-
(9)      the player thrown a frisbee                                   tracted the lexicon for use in the StatOpenCCG parser (with
(10)      the player who was tossed a frisbee                          the statistical parsing model over categories trained as before
(11)      the player who was thrown a frisbee                          on CCGbank data). Although this lexicon of course contains
                                                                       quite a few errors, we verify that it nonetheless parses our test
Although in theory this ratio could be as low as 0, in prac-           sentences correctly, placing the correct parses among the top
tice this does not occur because there is generally some (low          results.
probability) way to parse each phrase as a sentence. We take
this ratio as a measure of processing difficulty.                                              Experiments
                                                                       We present two sets of experiments in which we test
Implementation                                                         our model against the results from Tabor, Galantucci, and
We implement our model using a Combinatory Categorial                  Richardson (2004). The first uses a small but high-quality
Grammar parser based on the Cocke-Kasami-Younger (CKY)                 lexicon to parse two test cases. The second uses a larger,
algorithm. This algorithm was originally developed for Con-            error-ridden lexicon to parse a larger set of sentences. Recall
text Free Grammars and uses dynamic programing to parse                that Tabor, Galantucci, and Richardson’s (2004) study used
from the bottom up: given a sentence, it first calculates the          20 sets of sentences like those in (1)–(4).
probabilities of all ways to generate each word using a rule
X → word. For each potential pair of categories X1 and X2              Experiment 1: Test Cases using the CCGbank
that could have generated adjacent words w1 and w2 , it then           Lexicon
calculates the probabilities of all ways to generate that pair         Because CCGbank is derived from a human-annotated tree-
using a rule X3 → X1 X2 . This allows us to calculate the in-          bank, the quality of the lexicon it yields is high. Nevertheless,
side probability P(X3 → w1 w2 ). Continuing iteratively, we            it is small in comparison to human lexicons, and the passive
can calculate the inside probabilities of all substrings of the        relative constructions we are investigating are sparsely rep-
sentence.                                                              resented. In fact, the CCGbank lexicon contains only two
   We used a modified version of the StatOpenCCG parser,               words which are unambiguous ditransitive passive participles
developed by Christodoulopoulos (2008), which is it-                   (i.e., (S[pss]\NP)/NP but not (S[dcl]\NP)/NP—where [pss]
self an extension of the OpenCCG parser (White, 2008).                 indicates a past participle used in a passive construction, and
StatOpenCCG implements a statistical version of the CKY al-            [dcl] indicates a declarative sentence). These two words are
gorithm which operates using a generative head-dependency
                                                                   1562

                                             Predicted Processing Difficulty                                                            Predicted Processing Difficulty
                                              Experiment 1: "sent/written"                                                              Experiment 1: "offered/given"
                                    12                                                                                           9
                                                                                                                                 8
       Predicted difficulty ratio                                                                   Predicted difficulty ratio
                                    10
                                                                                                                                 7
                                     8                                                                                           6
                                     6
                                                                           Unambiguous                                           5                                    Unambiguous
                                                                           Ambiguous                                             4                                    Ambiguous
                                     4                                                                                           3
                                                                                                                                 2
                                     2
                                                                                                                                 1
                                     0                                                                                           0
                                          Unreduced        Reduced                                                                   Unreduced        Reduced
                                                           (a)                                                                                        (b)
Figure 3: Results from Experiment 1, two test cases using the high-quality CCGbank lexicon. In both sets of sentences, the
A/R case displays the correct pattern of superadditive difficulty.
written and given. Using these words, we construct two sen-
                                                                                                Table 1: Predicted difficulty ratios from all experiments,
tence sets, based on sentences used by Tabor, Galantucci, and
                                                                                                alongside grammaticality judgements from Tabor, Galan-
Richardson:
                                                                                                tucci, and Richardson (2004).
(12)                            He questioned a             congressman        (who   was)
                                sent/written a letter.                                                                Type            TG&R       Exp1: written    Exp1: given   Exp2
                                                                                                                      U/U              .28           1.27            5.45       5.74
(13)                            He addressed the woman (who was) offered/given a                                      A/U              .28           1.85            5.46       8.46
                                beer.                                                                                 U/R              .61           7.96            5.16       11.60
All words in these sentences are in the CCGbank lexicon. We                                                           A/R              .78           9.76            8.18       12.34
parse them using our high-quality lexicon.
                                                                                                less of this slight puzzle, the A/R case displays the correct
Results For these sentences, we obtain the predicted ratios:
                                                                                                pattern of superadditive difficulty.
                                         P(S → locally coherent substring)
                                             P(S → whole sentence)                              Experiment 2: Using the Gigaword Lexicon
                                                                                                Using the Gigaword lexicon, we are able to parse 13 out of the
Results are in Table 1 and Figure 3. We compare our results to
                                                                                                20 sentences in the Tabor study. (Sentences were excluded
the grammaticality judgements from Tabor, Galantucci, and
                                                                                                only if their past participles were not present in the lexicon.
Richardson (see Figure 1).
                                                                                                All other vocabulary items are present.) We standardize all
   As we see in Figure 3(a), the set of sentences (12) dis-
                                                                                                sentences to begin with a pronoun. Additionally, for the sake
plays the correct pattern of superadditive difficulty in the A/R
                                                                                                of parsing efficiency, we do not include the by phrases that
case. While there is little difference in difficulty between the
                                                                                                give the agent of the sentence. We further shorten two sen-
A/U and U/U conditions, there is a marked increase to the
                                                                                                tence sets in ways that do not affect the target part of the sen-
U/R condition, and a superadditive increase to the A/R con-
                                                                                                tence.
dition. This mirrors the pattern seen in Tabor, Galantucci, and
Richardson’s grammaticality judgements.                                                         Results Results from Experiment 2 are shown in Table 1
   We see the same superadditive pattern of difficulty in our                                   and Figure 4. We compare our results to the grammaticality
results for the set of sentences (13), shown in Figure 3(b).                                    judgements from Tabor, Galantucci, and Richardson (see Fig-
Somewhat surprisingly, the U/R condition is in fact predicted                                   ure 1). We find the correct trend of difficulties, with the A/R
to be marginally easier than the Unreduced sentences in this                                    condition most difficult, followed by U/R, followed by the
set. This may be because given is an extremely common                                           two Unreduced cases. We do not find the exact pattern of su-
word. Although it is unambiguous in that it cannot be a past                                    peradditive difficulty in the A/R case, due to the fact that the
tense, it is in fact a highly ambiguous word, with 18 entries in                                A/U case is in fact predicted to be much more difficult than
the CCGbank lexicon. For instance, it can serve as a preposi-                                   the U/U case, in contrast to the grammaticality ratings. Be-
tion, as in Given the weather, I will stay inside today. Regard-                                cause the Gigaword lexicon is very error-prone, it is difficult
                                                                                             1563

                                                                                      spans relative to the probability of the sentence as a whole.
                                       Predicted Processing Difficulty                With word by word predictions, we could model reading time
                                                Experiment 2                          data as well as grammaticality judgement data. Such a model
                                14                                                    would be applicable to a wide range of psycholinguistic data
                                                                                      beyond local coherence effects.
   Predicted difficulty ratio
                                12
                                10                                                                         Acknowledgments
                                 8                                                    This work was supported by EU IST Cognitive Systems IP FP6-
                                                                     Unambiguous      2004-IST-4-27657 ”Paco-Plus”.
                                 6                                   Ambiguous
                                 4
                                                                                                                References
                                                                                      Bicknell, K., & Levy, R. (2009). A model of local coherence ef-
                                 2                                                      fects in human sentence processing as consequences of updates
                                                                                        from bottom-up prior to posterior beliefs. In Proceedings of North
                                 0
                                                                                        American Chapter of the Association for Computational Linguis-
                                     Unreduced       Reduced                            tics: Human Language Technologies (NAACL HLT) 2009 Confer-
                                                                                        ence (pp. 665–673). Boulder, CO: Association for Computational
                                                                                        Linguistics.
                                                                                      Briscoe, E., Carroll, J., & Watson, R. (2006). The second release
                                                                                        of the RASP system. In Proceedings of the COLING/ACL 2006
Figure 4: Experiment 2 results. We find the expected pattern                            Interactive Presentation Sessions. Sydney, Australia: Association
of difficulty, but, due to the inflated predicted difficulty of the                     for Computational Linguistics.
U/R case, do not see superadditive difficulty in the A/R case.                        Christodoulopoulos, C. (2008). Creating a natural logic inference
                                                                                        system with Combinatory Categorial Grammar. Master’s thesis,
                                                                                        University of Edinburgh.
                                                                                      Curran, J. R., Clark, S., & Bos, J. (2007). Linguistically motivated
to draw any firm conclusions from this quirk in our results.                            large-scale NLP with C&C and Boxer. In Proceedings of the ACL
However, we note that the A/R case is correctly predicted to                            2007 Demonstrations Session (ACL-07 demo) (pp. 29–32). Mor-
                                                                                        ristown, NJ: Association for Computational Linguistics.
be substantially more difficult than either of the Unreduced                          Demberg, V., & Keller, F. (2008). A psycholinguistically motivated
cases.                                                                                  version of TAG. In Proceedings of the 9th International Workshop
                                                                                        on Tree Adjoining Grammars and Related Formalisms (pp. 25–
                                                 Conclusion                             32). Tübingen.
                                                                                      Ferreira, F. (2003). The misinterpretation of noncanonical sen-
We have presented a model of local coherence effects using a                            tences. Cognitive Psychology, 47, 164–203.
wide-coverage bottom-up Combinatory Categorial Grammar                                Ferreira, F., & Clifton, C., Jr. (1986). The independence of syntactic
                                                                                        processing. Journal of Memory and Language, 25, 348–368.
parser. Our model can accurately predict which sentences                              Ferreira, F., & Patson, N. D. (2007). The ‘Good Enough’ approach
humans will have difficulty in processing; specifically, it pre-                        to language comprehension. Language and Linguistics Compass,
dicts the local coherence effects found by Tabor, Galantucci,                           1(1–2), 71–83.
                                                                                      Graff, D. (2003). English Gigaword. Linguistic Data Consortium,
and Richardson (2004). Our results support the psycholin-                               Philadelphia. (DVD)
guistic plausibility of CCG and the Good Enough theory of                             Hockenmaier, J. (2003). Data and models for statistical parsing
parsing by demonstrating that a parser that uses bottom-up                              with Combinatory Categorial Grammar. Doctoral dissertation,
                                                                                        University of Edinburgh.
local information can both perform well as a wide-coverage                            Jurafsky, D. (1996). A probabilistic model of lexical and syntac-
parser and predict specific psycholinguistic results.                                   tic access and disambiguation. Cognitive Science: A Multidisci-
   Interestingly, the architecture of our version of the GE                             plinary Journal, 20(2), 137–194.
                                                                                      Levy, R. (2008). A noisy-channel model of rational human sentence
parser differs from Ferreira’s original proposal. Ferreira                              comprehension under uncertain input. In Proceedings of the Con-
(2003) proposes that GE parsing occurs via two separate                                 ference on Empirical Methods in Natural Language Processing
strategies: one “algorithmic” and one “heuristic”. In con-                              (EMNLP). Morristown, NJ: Association for Computational Lin-
                                                                                        guistics.
trast, our parser does not include this separation: all analyses,                     Roark, B. (2001). Probabilistic top-down parsing and language
both local and global, are produced by a uniform algorithm,                             modeling. Computational Linguistics, 29(2), 249–276.
and all are heuristically evaluated using the parsing model.                          Steedman, M. (2000). The syntactic process. Cambridge, MA: The
                                                                                        MIT Press.
This integration of strategies is a strength of our model, as it                      Sturt, P., Costa, F., Lombardo, V., & Frasconi, P. (2003). Learning
demonstrates how local coherence effects could emerge nat-                              first-pass structural attachment preferences with dynamic gram-
urally as an inherent part of the parsing process.                                      mars and recursive neural networks. Cognition, 88, 133–169.
                                                                                      Tabor, W., Galantucci, B., & Richardson, D. (2004). Effects of
   In future work, we would like to make not just sentence-                             merely local syntactic coherence on sentence processing. Journal
level predictions but word-by-word reading time predictions.                            of Memory and Language, 50, 355–370.
Given that we have an entire parse chart, such predictions                            White, M.        (2008).      Open CCG: The OpenNLP CCG li-
                                                                                        brary. (http://openccg.sourceforge.net/ [Online; accessed 27-
should be possible. We are currently choosing inside prob-                              July-2009])
abilities from two cells in the parse chart to compare, based
on outside knowledge of where processing difficulty is likely
to arise. We could do something similar for every cell in the
chart, considering the inside probability of the substring it
                                                                                   1564

