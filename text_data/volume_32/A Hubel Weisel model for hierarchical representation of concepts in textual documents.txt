UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Hubel Weisel model for hierarchical representation of concepts in textual documents
Permalink
https://escholarship.org/uc/item/57w312q3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Ramanathan, Kiruthika
Shi, Luping
Chong Chong, Tow
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

   A Hubel Weisel model for hierarchical representation of concepts in textual
                                                          documents
                                  Kiruthika Ramanathan (kiruthika_r@dsi.a-star.edu.sg),
                                     Shi Luping (shi_luping@a-star.edu.sg), Chong Tow
                                         Chong (chong_tow_chong@dsi.a-star.edu.sg)
                                        Data Storage Institute, (A*STAR) Agency for Science,
                                   Technology and Research, DSI Building, 5 Engineering Drive 1,
                                                           Singapore 117608
                           Abstract                               the existing memory, such that the new memory can be
  Hubel Weisel models of the cortex describe visual
                                                                  acquired without damage to the old ones.
  processing as a hierarchy of increasingly sophisticated
  representations. While several models exist for image
  processing, little work has been done with Hubel Weisel                             Related work
  models out of the domain of object recognition. In this
  paper, we describe how such models can be extended to              Mountcastle (1978) showed that parts of the cortical
  the representation of concepts, resulting in a model that
  shares several properties with the PDP model of                 system are organized in a hierarchy and that some
  semantic cognition. The model that we propose is also           regions are hierarchically above others. In general,
  capable of incremental learning, in which the knowledge         neurons in the higher levels of the visual cortex
  is stored in the strength of the neuron connections.            represent more complex features with neurons in the IT
  Degradation of old knowledge occurs as new knowledge            representing objects or object parts (Hubel and Weisel,
  is introduced to the system in a fashion that simulates         1965). Hubel Weisel models have therefore been
  decay theory in short term memory. The simulation
                                                                  developed for object recognition (Cadieu et al., 2007;
  model therefore captures several properties of cognitive
  conceptual memory, including generalization patterns,           Fukushima, 2003) proposing a hierarchy of feature
  the role of rehearsal and, hierarchical representation.         extracting simple (S) and complex (C) cells that allow
                                                                  for positional invariance. The combination of S-cells
                       Introduction                               and C-cells, whose signals propagate up the hierarchy
   There exist several bottom-up approaches to                    allows for scale and position invariant object
hierarchical models of object recognition that are based          recognition.
on the visual cortex. They make use of Mountcastle’s                 The idea of feature based concept acquisition has
(1978) theory of uniformity and hierarchy in the                  been well studied in psychological literature. Sloutsky
cortical column and the model of simple to complex                (2003) discusses how children group concepts based on,
cells of Hubel and Weisel (1965), modeling how simple             not just one, but multiple similarities, which tap the fact
cells from neighboring receptive fields feed into the             that those basic level categories have correlated
same complex cell, meaning that the complex cell has              structures (or features). This correlation of features is
phase invariant response.                                         also discussed in McClelland and Rogers (2003) who
   In this paper, we consider the following question. If          argue that information should be stored at the individual
the structure of the cortical column is uniform and               concept level rather than at the super ordinate category
hierarchical in nature and if the model of simple to              level allowing properties to be shared by many items.
complex cells can be used to model the visual cortex as              Our model is related to Hubel Weisel approaches in
discussed in prior works, then can such a model also be           that it implements a hierarchical modular architecture
used to represent other modalities of information such            for bottom-up propagation of conceptual information.
as the concepts derived from text? We are therefore               To our knowledge, however, this is the first
aiming to design a bottom up hierarchical memory for              implementation of a Hubel Weisel approach to non-
the representation of concepts, much the same way as it           natural medium such as text, and has attempted to
is designed for the representation of images. In this             model hierarchical representation of keywords to form
paper, we will define a concept as being a keyword in a           concepts.
document.                                                                         System architecture
   To deal with the dynamic nature of concept inputs,
we look at incremental learning of concepts from two              The system that we describe here is organized in a
aspects relevant to concept representation from text – (a)        bottom up hierarchy. This means that the component
with respect to new incoming features and (b) training            features are represented before the representation of
of hierarchies. To perform this, we apply a set of                concept objects. Our learning algorithm exploits the
geometric approximations to the incremental inputs and            property of this hierarchical structure. Each level in the
                                                              1106

hierarchy has several modules. These modules model                   process by which the parent modules learn from the
cortical regions of concept memory. The modules are                  outputs of the child modules. Here, consider the case
arranged in a tree structure, having several children and            shown in Figure 1b where the module 3 is the parent of
one parent. In our paper, we call the bottom most level              modules 1 and 2. Let x(1) be the output vector of
of the hierarchy level 1, and the level increases as one             module 1 and x(2) be the output vector of module 2. x(i)
moves up the hierarchy. The keywords from a                          represents the Euclidean distance from the input pattern
document form the inputs to the system. These are                    to the each output neuron i of the child modules. The
directly fed to level 1. Level 1 modules resemble simple             input to module 3,                     ||      , is the
cells of the cortex, in that they receive their inputs from          concatenation of the outputs of modules 1 and 2. A
a small patch of the input space. Several level 1                    particular concatenation represents a simultaneous
modules tile the input space, possibly with overlap. A               occurrence of a combination of concepts in the child
module at level 2 covers more of the input space when                module. Depending on the statistics of the input data,
compared to a level 1 module. It represents the union of             some combinations will occur more frequently, while
the input space of all its children level 1 modules.                 others will not. During the second stage of learning, the
However, a level 2 module obtains its inputs only                    parent module learns the most frequent combinations of
through its level 1 children. This pattern is repeated in            concepts in the levels below it. A GSOM is again used
the hierarchy. Thus, the module at the tree root (the top            in the clustering of such combinations. The learning
most level) covers the entire input space, but it does so            process thus defined can be repeated in a hierarchical
by pooling the inputs from its child modules. In the                 manner.
visual cortex, the level 1 can be considered analogous
to the area V1 of the cortex, level 2 to area V2 and so               Incremental learning
on.                                                                     In this and the subsequent sections of the paper, we
Learning the first batch of information                              will use the terms batch 0 to represent the first batch of
                                                                     documents. Batch 1 refers to the subsequent set of
  To understand how the model learns, let us consider                documents. Once the system learns the documents in
the inputs and outputs of a single module mk,i in level k            batch i, only the hierarchical structure and the neuron
of the system as shown in Figure 1a. Let x, representing             architecture are retained. All other information
connections {xj} be the input pattern to the module mk,i.            regarding the documents presented is discarded.
x is the output of the child modules of mk,i from the                   Incremental learning poses a challenge in Hubel
level k-1, and a represent the weights of the competitive            Weisel based computational models due to three
network. The vector a is used to represent the                       reasons. (1) Damage to the knowledge represented by
connections {aj} between x and the cells in the module               old neurons which is fundamental in competitive
mk,i. The output of a neuron in the module mk,i is given             learning. (2) Propagation of information in the
by       ∑        .                                                  hierarchical architecture. The number of output neurons
                                                                     of each child node increases with the introduction of the
                                                                     incremental batch. The input dimensions of the parent
                                                                     node are therefore changed and incompatible with the
                                                                     dimensions of the previous batch. (3) The irregularity
                                                                     in the input data dimensions. Where keywords are
                                                                     defined as concepts to be processed by the system, the
                                                                     keywords in an incremental batch will not be a subset
                                                                     of those in the previous batch. The architecture
              (a)
                                                                     therefore has to provide rules for the generation and
                                                (b)
   Figure 1a. Inputs and outputs to a single module mk,i. b. The     growth of new modules with respect to incoming
   concatenation of information from the child modules of the        incremental data.
       hierarchy to generate inputs for the parent module
                                                                     Preventing Damage to Old Memories: This problem
  During learning, each neuron in mk,i competes with                 is tackled using a sampling method using pseudo data
other neurons in the vicinity. Of the large number of                inspired from Liu et al (2008). The algorithm
inputs to a given module, a neuron is activated by a                 implemented summarizes data distribution in a cluster
subset of them. The neuron then becomes the spatial                  map. Given neuron a in a GSOM of N neurons,
center of these patterns. To ensure that there are no                consider the closest neuron b, a,b Є N , their midpoint
garbage neurons, we adopt in our creation of the
                                                                     is given by          2 . We generate a random set of
module, a model of Growing SOM (GSOM)
(Alahakhoon et al., 2000).                                           vectors around neuron a, bounded on both sides by
  When all the modules at level k finish training, the                       2 . These pseudo vectors generated during the
second stage of learning occurs. This comprises the
                                                                 1107

training of batch k implicitly reconstruct the data used                    For ease of analysis, assume that d=1
to train batches 0 to k-1.
                                                                                                       1     ′
Incremental learning in a hierarchy Let us consider
Figure 2, where the modules and are child modules
of .. At batch 0, the training sets xα and xβ, consisting
of p0 patterns each are used to generate the neurons yα
                                                                                          1
and yβ.                                                                                          ′                 ′
   iЄp0,     ,            ,       ||     ,                           (1)
                                                                                                 ′                ′
is passed to node . The vectors xα, xβ and                     are then     Where                                              ,
discarded.
                                                                                                                                          (3)
                                                                                             1
                                                                                                            1
                                                                                        ′                         ′
                                                                            Let                        and
                                                                            We obtain
                  (a)                               (b)
Figure 2. (a) Incremental learning stages. At batch 0, the training                   √2√1                      (4)
 patterns at level 1,       and cluster to form the neurons
                                                                            Where             ,,
    . For simplicity, we consider that only one neuron is generated
 after training batch 0. (b) Batch 1 and the approximation of the
                       pseudo vectors ,        and
                                                                                1                                 sin      2    2
   When batch 1, consisting of p1 vectors is now                            (11)
introduced,           and are approximated from and
respectively and used along with the new batch to train                             1                                         (5)
the GSOM modules α and β. After training, the neurons
                                                                            Substituting (5) into (4), we obtain
of the level 1 nodes             and         adapt to ′ and ′ . A
set of pseudo data are approximated from the neuron                                   2 1                                     (6)
   .
   From equation 2,             represents the Euclidean of                 Substituting (6) into (3), we obtain
and       from and respectively, i.e, for a set of p0’
                                                                                      1                                       (7)
pseudo data,
   iЄp0’,                          ||                                (2)    Solving (7), we have
               ,            ,              ,
   However, during the training of batch 1, the measure                            1                        (8)
for         should be the distance to ′ and ′ , the
updated neuron vectors. A set of adapted pseudo
                                                                            Based on Figure 3, if we approximate a             =    2 , we
vectors ′ should therefore be approximated.                                 obtain        1        . In implementation, to satisfy (8) we
   In Euclidean space, we can we can visualize the                          use the inequality (9) to assign the value of .
problem as shown in Figure 3,
                                                                                           2                (9)
                                                          y'
                                 D                                             We can therefore conclude that, a value specified
                                                                            by inequality (9) can be used to re-generate the dataset
                                                                               , 0,1, … , , …
                                                             R
                                                   θθ                                                , where ,           ,    ′ and ′
                 x                                      y                   is not the winning neuron.
                                       d
 Figure 3. Approximation of incremented pseudo vector for levels            Case (b):       is the winning neuron
                       2 and above in the hierarchy
                                                                               If y’ is the winning neuron, a random value of ,
   We consider two cases, (a) y’ is not the winning                         0             can be used to regenerate ,                   ,
neuron for the pattern x. (b) y’ is the winning neuron.
                                                                              ′ |, where y′α is the winning neuron.
Case (a).           is not the winning neuron, i.e, R<<d
                                                                        1108

Dealing with the problem of new input dimensions:                   concept cluster, the concept of penguin is separate from
A rule based approach of creating a new module to                   that of other clusters. This is shown in Figure 6.
process the new keywords is preliminarily proposed to
deal with the dynamically increasing input dimensions.
A module is trained and connected to parents only if the
number of concepts that it represents increases above a
predefined threshold. In order to avoid overcrowding,
heuristic rules have been put into place such that a
parent has atmost three children.
                    Experimental results
   To illustrate the cognitive properties of the training
model, we train the system using 21 concepts from 200
documents. The concepts included ideas such as “birds”,
“animals”, “flowers”, “trees” etc, same as the ones used
by McClelland and Rogers (2003). The following
preprocessing was performed to the documents. First,                  Figure 5. Euclidean distance between various concepts vs. the
the contents were analyzed and the stopwords removed.                                  number of training epochs
The concept terms were stemmed and grouped using
Wordnet (Fellbaum, 1998) before a tf.idf weighing                      In Figure 5, we observe the evolution of the
scheme was used to select the most relevant concepts to             Euclidean distance between concepts. The training
the batch. For visualization purposes,                              shows empirical properties of convergence. The
                                                                    distances between the various concepts are stable after
Hierarchical identification of concepts from wiki                   500 epochs of training. We can also observe promising
documents                                                           results from the concept representation point of view.
                                                                    For instance, the Euclidean distance between “pine”
                                                                    and “oak”, for instance, is larger than the Euclidean
                                                                    distance between “birch” and “oak”, which belong to
                                                                    the same family.
                                                                       Figure 6.a shows the top two levels of a five level
                                                                    hierarchy of hierarchy of concepts obtained (10
                                                                    concepts per GSOM module and 160 concepts used in
                                                                    training). We observe, as is the result in McClelland
                                                                    and Rogers that similar concepts tend to be near each
                                                                    other in space. For instance, “canary” and “sparrow”
                                                                    tend to be closer to each other, but far from “penguin”.
                                                                    In some cases, super ordinate terms, such as “bird”,
     Figure 4. The number of concept neurons at the top of the
   hierarchy vs. the number of features per bottom level module
                                                                    “tree” etc are mined as part of the hierarchy. There are
                                                                    some interesting observations that can be made here.
In this section we observe how our hierarchical model               We can see that the highest level (level 5) shows
captures the properties of semantic cognition outlined              general concepts while level 4 shows the concepts one
by McClelland and Rogers (2003, 2008). The training                 level lower. i.e., while the neuron 1 refers to “animals”,
data used by McClelland and Rogers is intuitively                   the neuron box “2” refers to more detailed
                                                                    differentiation of neuron 1. Further to this, the system
designed based on common sense knowledge. Our
                                                                    also shows some intermediate level categorization
system, on the other hand is trained using information              characteristics that taps item frequency effects. In
from 200 text descriptors of the concepts from                      McClelland and Rogers’ paper, they describe it as the
wikipedia. The snippets varied in length from 50 word               process by which certain descriptive terms such as
descriptions to 500 word descriptions. Figure 4                     “tree”, “bird” and “dog” tend to be acquired earlier than
illustrates the number of concept neurons at the top                the super ordinate terms such as “plant” or “animal” or
level as a function of the ratio of the number of features          more specific terms such as canary, pine or poodle. The
                                                                    general consensus for this is that parents use certain
to each level 1 module and the total number of features.
                                                                    intermediate level words more frequently when
When there are only two layers in the hierarchy, a                  speaking to children. As such, intermediate concepts,
larger number of concept neurons (16) are generated.                based on their frequency of usage, are also clustered
The number of concept neurons converges to between 6                more tightly into intermediate groups within super
and 8 for all other architectures. Typically, for a six             ordinate concepts.
                                                                1109

             e
   In the experimental                 d
                                       data,         some conncepts were ussed                                  daisy anda 7 instancess of sunflowerr. It is seem thhat the
more frequuently than oth                hers of the sam               me sub category.                         more frrequent conceppts are tied toggether with thee super
These incllude birch (12 instances) vs.. 10 instances of                                                        ordinatte concept neuuron (dog is tieed with animal, rose
pine and maple,
             m             16 instaances of rose vs.                  v 3 instances of                          with floower, sparrow with bird) andd so on.
                             goat                                                                                 cod
                                                                   dog
                           pig                                                                flounder
                                                                                                                            salmon
                 tulips                                                                                 sunfish
                                                                      Saalmon, fish, cod
   sunflower
                    roses
 daisy
                                                                                                                                                           (b)
                                  Roses, flowers,         Dog, animals
                                                                                      penguin
                                                         oak, tree
                                           Birch, tree                             Birch/oak
                         birch                                 Sparrow, birds
                                    pine
                                                                                                          pine
                      oak
                                                       maple
                                                               (a)                                                                                         (c))
     Figure 6 (a) Hierarchical representation off the 21 concepts in the memory syystem. (b) Groupiing due to second                                         dary characteristiics –
 “leaves” grooups all trees, “reed” for rose, salm                   mon etc. (c) Associiative retrieval off concepts based on            o activation of hiigh correlation neeurons
                                                                                                  in the hierarchy.
   At the lowerl            level, we    w can also observe            o                  alternaate            generallizes, creating concept repreesentation of flower        f
similarity grouping wh                 hich are basedd on individuual                                           and aniimal. Lower laayers now portrray the differenntiated
features. ForF instance, at               a level 1 moodule, one migght                                         concepts. In batch 2, the conceept of salmonn was
observe neeurons denotin                  ng concepts such            s             as “leavees”                introduuced. At this juuncture, the old informationn from
grouping birch,
              b            maple and      a oak. The concept “grow                                    w”        sunfishh is sub grouuped under fiish and sparrrow is
groups objects of all cateegories. “Red” indicates objects                                                      consideered a separatee entity. These experiments suuggest
that are redd – rose, cod, salmon. One such lower layyer                                                        merit in  i the approximations that we have desscribed
representattion is shown in              i Figure 6.c.                                                          earlier in the paper.
   From thhese results, one             o can expecct this model to
perform a sort of associiative retrieval of the kind thhat
humans peerform. For insstance, if one sees the conceept
“gold”, one would th                   hink of perhaaps “ring” and                                  a
subsequenttly “marriage”” and then “fam                                mily”, “childreen”                             Figgure 7. The top-levvel evolution of in
                                                                                                                                                              ncremental conceept
and so fortth. In the samee way, firing off the neuron “rred”                                                       repreesentation. (a) Battch 0 tulip and su
                                                                                                                                                             unflower. (b) Batcch 1:
                                                                                                                                sparrow and
                                                                                                                                          d sunfish. (c)Batch h 2: salmon
at level 1 willw lead to firring of correspponding nodess at
various levvels. This willl lead to otherr concepts beiing                                                      Hierarrchical represeentation in in         ncremental leaarning
fired, bothh sequentially              y and parallelyy. For instance,                                          In this experiment, concepts
                                                                                                                                           c             weree introduced oneo by
probing thhe model with                h “red” leads to                t the sequenttial                        one, beginning
                                                                                                                        b              withh “birch” andd “cod”. At some
firing as deescribed in Fig            gure 6.b.                                                                juncturre concepts arre reintroducedd to investigaate the
   Each cooncept can, in               n turn, be usedd as a probe to                                           effect of o new data annd rehearsal onn old data. Soome of
activate reelevant neuron                 ns. In this seense, the moddel                                        the resuults obtained arre discussed inn this section.
describes the t human chaain of thought process.                        p                  Workk is                 Figurre 8a shows thhe evolution of        o how the conncepts
in progresss to study how               w this process can be modelled
                                                                                                                “birch”” and “betula”” are representted (“Betula” is the
and how the    t firing can            n be controlledd by the wiriing
strength beetween modulees.                                                                                     scientiffic name for “birch”).
                                                                                                                                          “             Birchh is the first cooncept
                                                                                                                that was introduced to the system             m. At batch 0, the
Incremen     ntal learning performancce                                                                         distincttion between the   t concepts “birch”“         and “bbetula”
                                                                                                                appearss in level 3 of     o the hierarcchy. To the syystem,
Overview of incrementtal learning with                                 w             5 concep        pts
                                                                                                                “betulaa” and “birch” are two distinnct concepts, though      t
and 3 battches: In this section, insteaad of introduciing
                                                                                                                with a low Euclideann distance of 39.81.     3        When batch 2
all the cooncepts are on                 ne go, conceppts are learnt in
                                                                                                                is introoduced, the disstinction betweeen the two conncepts
batches. The T figure belo              ow shows the evolution of the                                t
                                                                                                                moves one level loweer to level 2 annd eventually too level
incrementaal concept nettwork at the top                               t level for the                t
                                                                                                                1. How   wever, as morre concepts arre introduced to the
first three batches (a) tu                ulips and sunfflowers only (b)                              (
                                                                                                                system, the presence of the new infformation makkes the
sparrow annd sunfish (c) salmon.        s                 In batcch 1, the top levvel
                                                                                                                system lose the distinnction betweenn the concepts “birch”
                                                                                                           1110

and “betula”, and the Euclidean distance between the                    before gradually decreasing to 0 once again. A similar
concepts reduces to 0 at batch 7. However, at batch 10,                 result is also observed in the relationship between terms
when the concept of birch is reintroduced, the                          “canary” and “islands”.
Euclidean distance between the two terms increases
    Figure 8. Evolution of the hierarchical and Euclidean relationship between the concepts (a)“birch” and “betula” vs “canary” and
                 “islands” (b) “Birch” and “cod” vs. “birch” and “dog” (c) “dog” and “flounder” vs. “goat” and “dog”
Figure 8b shows the representation of the concept                       now, this information is ignored and only the statistical
“birch” with respect to the concepts “dog” and “cod”.                   properties of keywords are taken into consideration in
“Birch” is introduced to the system at batch 0, “cod” at                the generation of the concept hierarchy. Work is under
batch 1 and “dog” at batch 4. The differentiation                       process to integrate semantic information into the
between the concepts “birch” and “cod” is at level 4                    model. Work is also under progress to include common
and converges to level 3. By batch 8, the concepts of                   sense knowledge in the model. We expect that these
“dog” and “cod” are of the same distance from “birch”.                  additions will make the model more cognitively
At this juncture, the system at level 3 no longer                       accurate. In addition to this, we are also incorporating
distinguishes between “birch”, “cod” and “dog”, but                     other aspects of cognition such as attention; interest etc
makes a distinction between “plant” and “animal”.                       to study the generation and behavior of the cognitive
Figure 8c shows a similar relationship of concepts “dog”                map.
with the concepts “flounder” and “goat”. The flounder-                                              References
dog distinction converges to level 2 (from Figure 8b,
we can see that the plant-animal distinction occurred at                Cadieu, C., Kouh, M., Pasupathy, A., Conner, C., Riesenhuber, M., &
level 3) while the dog-goat distinction converges to                       Poggio, T.A. (2007). A Model of V4 Shape Selectivity and
                                                                           Invariance. J ournal of Neurophysiology 98, 1733-1750
level 1. The Euclidean distance between the concept                     George D, Hawkins J (2005), A hierarchical Bayesian Model of
terms “dog” and “goat” converges to approximately                          Invariant Pattern Recognition in the Visual Cortex, International
700 which is close to the value that is obtained through                   Joint Conference on Neural Networks, 3, 1812-1817
batch learning (from Figure 5).                                         Vernon Mountcastle (1978), An Organizing Principle for Cerebral
                                                                           Function: The Unit Model and the Distributed System, The
                                                                           Mindful Brain (Gerald M. Edelman and Vernon B. Mountcastle,
            Conclusions and further work                                   eds.) Cambridge, MA: MIT Press.
                                                                        Hubel D and Weisel T (1965), Receptive fields and functional
   In summary, our model attempts to propose a                             architecture in two non striate visual areas (18 and 19) of a cat,
hierarchical Hubel Weisel model for the acquisition of                     Journal of NeuroPhysiology 28, pp229-289
concepts from text such that the concepts are                           Fukushima K(2003), Neocognitron for handwritten digit recognition
                                                                           1, Neurocomputing , 51C, 161-180
represented in a hierarchical connectionist network. The
                                                                        Alahakhoon D, Halgamuge S K, Srinivasan B (2000), Dynamic Self
result is a new framework that we have applied in two                      Organizing maps with controlled growth for Knowledge discovery,
scenarios. The first is concept acquisition where we                       IEEE Transactions on neural networks, 11(3), pp601-614
have shown that the system is able to represent                         Liu M, Liu Y C, Wang X L (2008), IGSOM: Incremental clustering
                                                                           based on self organizing map, International Conference on
everyday concepts in a hierarchical fashion, in a
                                                                           Intelligent Information hiding and multimedia Signal Processing,
manner similar to the PDP model. The system was                            IIHMSP’08, pp 885-890
interestingly also able to perform chain retrieval, in that             Fellbaum C (1998), Wordnet: An electronic lexical database, MIT
when “red” was given as a probe to the system, it was                      Press
                                                                        Mc Clelland J L and Rogers T T (2003), The parallel distributed
able to retrieve “robin” and by association “sparrow”.
                                                                           processsing approach to semantic cognition, Nature Reviews
Secondly, we have modeled information approximation                        Neuroscience, 4(4), pp310-322
and incremental learning, which models some                             Sloutsky VM (2003), The role of similarity in the development of
properties of short term memory.                                           categorization, Trends in Cognitie Sciences, 7, 246-251
                                                                        Rogers T T, McClelland J L (2008), Precis of Semantic Cognition, a
   There are several directions for further work in this
                                                                           Parallel distributed Processing approach, Brain and Behavioral
area. In addition to the pertinent issues of improving                     Sciences, 31, pp 689-749
computation time and processing algorithms to make
the system able to handle large sets of data, one
important direction is the incorporation of semantic
information into the hierarchical architecture. As of
                                                                  1111

