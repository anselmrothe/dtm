UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Probabilistic language acquisition: Theoretical, computational, and experimental analysis
Permalink
https://escholarship.org/uc/item/9bb0s32g
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Hsu, Anne
Chater, Nick
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                  University of California

                                              Probabilistic language acquisition:
                           Theoretical, computational, and experimental analysis
                                                 Anne S. Hsu (ahsu@gatsby.ucl.ac.uk)
                                    Division of Psychology and Language Sciences, 26 Bedford Way
                                                              London, WC1H 0AP
                                                    Nick Chater (n.chater@ucl.ac.uk)
                                    Division of Psychology and Language Sciences, 26 Bedford Way
                                    and Centre for Economic Learning and Social Evolution (ELSE)
                                                              London, WC1H 0AP
                               Abstract                                  Tenenbaum, 2009; Grünwald, 1994; Perfors, Regier, &
                                                                         Tenenbaum, 2006; Regier & Gahl, 2004).
   There is much debate over the degree to which language
   learning is governed by innate language-specific biases, or              Here we examine language acquisition from a
   acquired through cognition-general principles. Here we                probabilistic perspective on a theoretical, computational and
   examine the probabilistic language acquisition hypothesis on          experimental level. We first revisit Gold’s theorem and
   three levels: We outline a theoretical result showing that            show that language identification is possible from a
   probabilistic learning in the limit is possible for a very general    probabilistic perspective. Next we mention a recently
   class of languages. We then describe a practical                      proposed, general framework which can quantify
   computational framework, which can be used to quantify
   natural language learnability of a wide variety of linguistic
                                                                         learnability of constructions in natural language. This
   constructions. Finally, we present an experiment which tests          flexible framework allows for predictions to be made
   the learnability predictions for a variety of linguistic              concerning the natural language learnability of a wide
   constructions, for which learnability has been much debated.          variety of linguistic rules. Finally, we experimentally test
   We find that our results support the possibility that these           the learnability predictions obtained from this framework by
   linguistic constructions are acquired probabilistically from          comparing these predictions with adult grammaticality
   cognition-general principles.                                         judgments for a wide range of linguistic constructions.
   Keywords: child language acquisition: Gold’s theorem;
   poverty of the stimulus; probabilistic learning; simplicity                  Gold revisited: probabilistic language
   principle; adult grammar judgments; natural language
                                                                                   acquisition with a simplicity prior
                          Introduction                                   Inherent in a simplicity-based approach to language
                                                                         acquisition is the trade-off between simpler vs. more
A central debate in cognitive science revolves around
                                                                         complex grammars: Simpler, over-general grammars are
how children acquire their first language. A significant
                                                                         easier to learn. However, because they are less accurate
portion of this debate centers on how children learn complex
                                                                         descriptions of actual language statistics, they result in
linguistic structures, such as restrictions to general rules.
                                                                         inefficient encoding of language input, i.e. the language is
An example restriction-rule can be seen in the contraction of
                                                                         represented using longer code lengths. More complex
‘going to’: ‘I’m gonna leave’ is grammatical whereas ‘I’m
                                                                         grammars (which enumerate linguistic restrictions) are more
gonna the store’ is ungrammatical. Language
                                                                         difficult to learn, but they better describe the language and
communication requires the speaker to generalize from
                                                                         result in a more efficient encoding of the language, i.e.,
previously heard input. However, research shows children
                                                                         language can be represented using shorter code lengths.
rarely receive feedback when they produce an over-general,
                                                                         Under simplicity models, language learning can be viewed
ungrammatical sentence. Children also aren’t explicitly told
                                                                         in analogy to investments in energy-efficient, money-saving
which generalizations are allowed and which are not
                                                                         appliances. By investing in a more complicated grammar,
(Bowerman, 1988). These observations evoke the question:
                                                                         e.g. one which contains a restriction on a construction, the
how do children learn that certain overgeneralizations are
                                                                         language speaker obtains encoding savings every time the
ungrammatical without explicitly being told?
                                                                         construction occurs. This is analogous to investing in an
   Traditionally, linguists have claimed that such learning is
                                                                         expensive but efficient appliance that saves money with
impossible without the aid of innate language-specific
                                                                         each use. A linguistic restriction is learned when the
knowledge (Chomsky, 1975; Crain, 1991; Pinker, 1989;
                                                                         relevant linguistic context occurs often enough that the
Theakston, 2004). However, recently, researchers have
                                                                         accumulated savings makes the more complicated grammar
shown that statistical models are capable of learning
                                                                         worthwhile. Because complex grammars become worth
restrictions to general rules from positive evidence only
                                                                         while as linguistic constructions appear more often,
(Dowman, 2007; Foraker, Regier, Khetarpal, Perfors, &
                                                                       1720

simplicity models are able to learn restrictions based on        conversely, if the probability of a sentence being generated
positive evidence alone (See Figure 1).                          is greater than zero, then it is grammatical according to L.
                                                                    There is one additional mild constraint that we need to
                                                                 impose: that CP(s) has a finite entropy, i.e.,
                                                                              ∞              1 
                                                                  H (C p ) ∝ ∑ C p (s j )log        <∞
                                                                                             C (s ) 
                                                                             j =1             p  j  
                                                                 This is a modest constraint, because it follows from the
                                                                 assumption that the mean sentence length under distribution
                                                                 CP(s) is finite, which is clearly true for natural language.
                                                                    The learning problem proceeds as follows: A learner is
                                                                 given an initial sample of the corpus Sn. The question then
                                                                 is: how should the learner assign probabilities to the various
                                                                 possible computable distributions CQ that might have
                                                                 generated the corpus? This is equivalent to learning:
                                                                 Pr(CQ|Sn) ∝ Pr(Sn|CQ)Pr(CQ)
                                                                 Also, we ask how these probabilities change as the corpus
                                                                 grows arbitrarily long, i.e., as n tends to infinity? In
                                                                 particular, can the learner identify the true probability
                                                                 distribution, CP, in the limit?
                                                                    Intriguingly, it turns out that this is possible – and indeed
Figure 1: MDL simple grammar vs. efficient language
                                                                 that an ideal learner (Chater & Vitányi 2007)) will
encoding trade off. A) A simpler grammar is often over-
                                                                 ‘converge’ on the true probability distribution, CP, with
general, i.e., allows for ungrammatical sentences as well as
                                                                 probability of measure 1, given a sufficiently large corpus.
grammatical ones. Such an over-general grammar may be
                                                                 Suppose, for concreteness, that the learner “announces” its
easy to describe (i.e., short grammar encoding length), but
                                                                 current most probable generating distribution each time a
results in less efficient (longer) encoding of the language
                                                                 new sentence i arrives, based on the i sentences that he has
data. B) A more complex grammar may capture the
                                                                 received so far Si = {s1, s2,..., si}. More formally, the
language more accurately, i.e., allows only for grammatical
                                                                 following theorem holds: Consider any computable
sentences and doesn’t allow for ungrammatical sentences.
                                                                 probability distribution CP, from which samples, si, are
This more complex grammar may be more difficult to
                                                                 drawn independently to generate a semi-infinite corpus S.
describe (i.e., longer grammar encoding length), but will
                                                                 Let m’ be the number of initial items of S so that Sm’ is a
provide a shorter encoding of language data. C) Initially,
                                                                 “prefix” of S (i.e., a corpus consisting of the first m’ items of
with limited language data, the shorter grammar yields a
                                                                 s). With probability greater than 1-ε, for any ε > 0, there is
shorter coding length over-all, and is preferred under MDL.
                                                                 an m such that, under the simplicity principle, for all m’≥m,
However, with more language input data, the savings
                                                                 the most probable CQ, given Sm’ is the generating
accumulated from having a more efficient encoding of
                                                                 distribution CP, i.e., argmax(Pr(CQ|Sm’))=Cp.
language data correctly favour the more complex grammar.
                                                                    Why is this true? A full proof is beyond the scope of this
                                                                 paper (see Chater & Hsu, in preparation); but the essence of
  A central theoretical question is: given sufficient exposure
                                                                 the argument is the following. We know that almost all
to the language, can the learner recover a perfectly accurate
                                                                 random samples from P will be incompressible (i.e., n
description of that language? Gold (1967) famously showed
                                                                 sentences generated by the true generative model P will
that, under certain assumptions, this is not possible.
                                                                 have no shorter description than the entropy nH(P)). This
However, a range of more positive results have since been
                                                                 implies that, for typical data generated by P (which have
derived, e.g., (J. A. Feldman et al 1969; Chater & Vitányi
                                                                 summed probability arbitrarily close to 1), K(P)+nH(P) ≥
2007). Here we show that under a simplicity-based
                                                                 K(Sn)≥nH(P). Now for each Sn, consider the set of
probabilistic formulation, a new and strong positive result
                                                                 probability distributions Q which satisfy this criterion:
can be derived.
                                                                 K(Q)+nH(Q)≥K(Sn)≥nH(Q). For each n, there will be
  Suppose that the learner encounters sentences, s, which
                                                                 finitely many such Q; and, by our argument above, these
are independently sampled generated from a computable
                                                                 will include the true distribution P. Now, for each n, the
probability distribution, CP(s), which has Kolmogorov
                                                                 learner “announces” the simplest Q’, i.e., the Q’ such that
complexity K(CP). Here we will define learning a language
                                                                 for all Q, K(Q)≥K(Q’). We know that P will always be in
as the process of identifying this distribution. CP(s)
                                                                 this set, by the argument above. However, there are only
generates a corpus Sn = s1, s2,...sn,... which continues
                                                                 finitely many Q that are simpler than P. Once these simpler
indefinitely. We assume that CP(s) allows all and only
                                                                 Q have been eliminated, then P will be the shortest element
grammatical sentences in language L. That is, the
                                                                 in the set, and will be announced indefinitely thereafter. We
probability of generating all sentences s, that are
                                                                 know that each of this finite set will be eliminated for
grammatical in L, is greater than zero, CP(s) > 0; and
                                                               1721

sufficiently large n, because the expected excess cost of         applied to language, grammars can be represented as a set of
encoding data generated by P with distribution Q is               rules, such as that of a probabilistic context free grammar
nD(Q||P), where D(Q||P) > 0 unless Q=P; this excess cost          (PCFG) (Grünwald, 1994). An information-theoretic cost
tends to infinity as n tends to infinity. Hence, for some n’>     can then be assigned to encoding the grammar rules as well
n, after all probability distributions Q with shorter codes       as to encoding the language under those rules.
than P have been eliminated, P will be announced                     Hsu & Chater (2010) used an instantiation known as 2-
indefinitely.                                                     part MDL, which we will refer to as just MDL for brevity.
                                                                  In the context of language acquisition, the first part of MDL
        Practical framework for quantifying                       uses probabilistic grammatical rules to define a probability
                         learnability                             distribution over linguistic constructions, which combine to
                                                                  form sentences. Note that these probabilities are not
The positive learnability results indicate that the
                                                                  necessarily the real probabilities of sentences in language,
probabilistic approach can be practically applied to the
                                                                  but the probabilities as specified under the current
problem of language acquisition. Recently, researchers have
                                                                  hypothesized grammar. The second part of MDL consists of
used probabilistic models to show that many complex
                                                                  the encoded representation of all the sentences that a child
linguistic rules can be acquired by directly learning the
                                                                  has heard so far. MDL selects the grammar that minimizes
probability distribution of grammatical sentence structures
                                                                  the total encoding length (measured in bits) of both the
in language. These models learn this probability distribution
                                                                  grammatical description and the encoded language length1.
under a cognition general prior for simplicity (Dowman,
                                                                     According to information theory, the most efficient
2007; Foraker et al., 2009; Grünwald, 1994; Perfors et al.,
                                                                  encoding occurs when each data element is assigned a code
2006; Regier & Gahl, 2004). Many of these studies used
                                                                  of length equal to the smallest integer greater than or equal
restricted language sets. In the context of natural language, a
                                                                  to -log2(pn) bits, where pn is the probability of the nth
few studies have addressed specific linguistic cases such as
                                                                  element in the data. For our purposes, these elements are
anaphoric one (Foraker et al., 2009) and hierarchical phrase
                                                                  different grammar rules. The probabilities of these grammar
structure (Perfors et al., 2006).
                                                                  rules are defined by the grammatical description in the first
   Recently, a general quantitative framework has been
                                                                  part of MDL. Because efficient encoding results from
proposed which can be used to assess the learnability of any
                                                                  knowing the correct probabilities of occurrence, the more
given specific linguistic restriction in the context of real
                                                                  accurately the probabilities defined in the grammar match
language, using positive evidence and language statistics
                                                                  the actual probabilities in language, the more efficient this
alone (Hsu & Chater, 2010). This framework built upon
                                                                  grammar will be.
previous probabilistic modeling approaches to develop a
                                                                     Under MDL, the grammatical description is updated to be
method that is generally applicable to any given
                                                                  the most efficient one each time more data input is obtained.
construction in natural language. This new tool can be used
                                                                  Savings occur because certain grammatical descriptions
to explicitly explore the learnability in a corpus relative to
                                                                  result in a more efficient (shorter) encoding of the language
well-known information theoretic principles given a
                                                                  data. In general, more complex (i.e., more expensive)
grammatical description. When using this framework to
                                                                  grammatical descriptions allow for more efficient encoding
analyze learnability of a linguistic construction, there are
                                                                  of the language data. Because savings accumulate as
two main assumptions: 1) The description of the
                                                                  constructions appear more often, more complex grammars
grammatical rule for the construction to be learned. 2) The
                                                                  are learned (i.e., become worth investing in) when
choice of corpus which approximates the learner’s input.
                                                                  constructions occur often enough to accumulate a sufficient
Given these two assumptions, the framework provides a
                                                                  amount of savings. If there is little language data (i.e., a
method for evaluating whether a construction is present with
                                                                  person has not been exposed to much language) a more
adequate frequency to make it learnable from language
                                                                  efficient encoding of the language does not produce a big
statistics. The framework allows for comparison of different
                                                                  increase in savings. Thus, when there is less language data,
learnability results which arise from varying these two main
                                                                  it is better to make a cheaper investment in a simpler
assumptions. By making these assumptions explicit, a
                                                                  grammar as there is not as much savings to be made. When
common forum is provided for quantifying and discussing
                                                                  there is more language data, investment in a more costly,
language learnability.
                                                                  complicated grammar becomes worthwhile. This
                                                                  characteristic of MDL learning can explain the early
Minimum Description Length hypothesis
                                                                  overgeneralizations followed by retreat to the correct
Because this framework is detailed elsewhere (Hsu &
Chater 2010), we will only provide a brief overview here.
Learnability evaluations under a simplicity prior can be             1
instantiated through the principle of minimum description              The MDL framework can also be expressed as a corresponding
                                                                  Bayesian model with a particular prior (Chater, 1996; MacKay,
length (MDL). MDL is a computational tool that can be
                                                                  2003; Vitányi & Li, 2000). Here, code length of the model (i.e.,
used to quantify the information available in the input to an     grammar) and code length of data under the model (i.e., the
idealized statistical learner of language as well as of general   encoded language) in MDL correspond to prior probabilities and
cognitive domains (Jacob Feldman, 2000). When MDL is              likelihood terms respectively in the Bayesian framework.
                                                                1722

                                Table 1: Grammatical and ungrammatical sentences used in experiment.
          Construction            Grammatical usage                             Ungrammatical usage
          is                    She's as tall as he is.                         She is as tall as he's.
          arrive                The train arrived.                              He arrived the train.
          come                  The train came.                                 I came the train.
          donate                He donated some money to the charity.           He donated the charity some money.
          fall                  The ornament fell.                              He fell the ornament.
          disappear             The rabbit disappeared.                         He disappeared the rabbit.
          what is               What's it for?                                  What's it?
          shout                 I shouted the news to her.                      I shouted her the news.
          pour                  I poured the pebbles into the tank.             I poured the tank with pebbles.
          vanish                The rabbit vanished.                            He vanished the rabbit.
          whisper               I whispered the secret to her.                  I whispered her the secret.
          create                I created a sculpture for her.                  I created her a sculpture.
          who is                Who's it for?                                   Who's it?
          going to              I'm gonna faint.                                I'm gonna the store.
          suggest               I suggested the idea to her.                    I suggested her the idea.
          that                  Who do you think that she called?               Who do you think that called her?
          want to               Which team do you wanna beat?                   Which team do you wanna win?
grammar that has been observed in children’s speech                   results showed a large spread in learnability. Some
(Bowerman, 1988). The output of the framework described               constructions appeared readily learnable within just a few
in Hsu & Chater (2010) results in an estimated number of              years whereas other constructions required years that far
occurrences needed for a specific linguistic rule to be               outnumbered human life spans. Hsu & Chater (2010)
learned and corpus analysis is then used to assess how many           compared predicted MDL learnablity with child grammar
years on average are needed for the sufficient number of              judgments of constructions for which there was data
occurrences. The general applicability of this framework              collected from previous experimental work (Ambridge,
and its ability to produce clear learnability predictions allow       Pine, Rowland, & Young, 2008; Theakston, 2004). It was
us to take the crucial next step in addressing the language           found that child grammar judgments for the constructions
acquisition problem: experimentally assessing whether                 were more correlated with learnability than frequency
language might actually be probabilistically acquired.                counts (the entrenchment hypothesis (Theakston, 2004)).
             Testing learnability predictions
Hsu & Chater (2010) used the above framework to assess
language learnability of constructions, whose learnability
have been commonly debated. These all involve restrictions
on a general linguistic rule, which was described using
PCFG’s. Predictions for learnability in terms of years
needed was made for constructions whose learnability have
been commonly debated in the language acquisition field.
These included restrictions on the following 17
constructions2: contractions of want to, going to, is, what is
and who is; the optionality of that reduction; dative
alternation for the verbs donate, whisper, shout, suggest,
create, pour; transitivity for the verbs, disappear, vanish,
arrive, come, fall. See Hsu & Chater (2010) for the explicit          Figure 2: Estimated years required to learn construction.
grammar descriptions of linguistic rules to be learned. The           The constructions are sorted according to learnability: 1) is
                                                                      2) arrive 3) come 4) donate 5) fall 6) disappear 7) what is
   2
     Hsu & Chater (2010) also included analysis of two more           8) shout 9) pour 10) vanish 11) whisper 12) create 13) who
linguistic rules concerning the necessary transitivity of the verbs   is 14) going to 15) suggest 16) that 17) *want to. *Predicted
hit and strike. Though these verbs are traditionally known to be      years for learning want to is 3,800years.
transitive, in colloquial speech they have evolved to have a
ambitransitive usage: e.g. The storm hit. Lightening struck. In          Here we propose that construction learnability should also
COCA there are 3678 and 1961 intransitive occurrences of hit and
                                                                      correlate with adult grammaticality judgments: The more
strike respectively. Thus we did not assess rules regarding the
intransitivity of these verbs in our experiment.                      difficult a construction is to learn, the greater the difference
                                                                    1723

should be between judgments of the ungrammatical vs.             ungrammatical) 5) Sounds extremely odd (Definitely
grammatical uses of the construction.                            ungrammatical).
Model Predictions                                                Results
We conducted our learnability analysis using the full Corpus     Results show a strong correlation between averaged relative
of Contemporary American English (COCA), which                   grammaticality vs. log learnability as predicted by MDL,
contains 385 million words (90% written, 10% spoken). We         r=.35; p=.0045 (see Figure 3). Relative grammaticality for a
believe this is a reasonable representation of the               given linguistic construction is the grammatical rating for
distributional language information that native English          the ungrammatical sentence subtracted by the rating for the
language speakers receive. Learnability results using the        grammatical sentence. Note that 4 is the maximum possible
British National Corpus were similar to that from COCA           relative grammaticality because the lowest ungrammatical
(Hsu & Chater, 2010). Figure 2 shows the estimated               rating is 5 and the highest grammatical rating is 1. In
number years required to learn the 17 constructions. We          contrast, there is no correlation between relative
quantified learnability as log(1/Nyears), where Nyears was the   grammaticality and construction occurrence frequency, as
number of estimated years needed to learn a construction         would be predicted by entrenchment (see Figure 4).
(Hsu & Chater, 2010).
Learnability vs. entrenchment To verify that our
experimental results are not also trivially explained by a
simpler hypothesis, we will also compare experimental
results with the predictions of entrenchment theory.
Entrenchment is the hypothesis that the likelihood of a child
over-generalizing a construction is related to the
construction’s input occurrence frequency. There is some
relation between learnability and entrenchment predictions
because high construction occurrence frequencies do aid
learnability. However, learnability differs from mere
frequency counts because MDL also takes into account the
complexity of the grammatical rule that governs the
construction to be learned. Additionally, learnability is
influenced by whether the restricted form would be
commonly or uncommonly expected, if it were
grammatically allowed. Here, we propose that under
entrenchment hypothesis, the relative grammar judgment           Figure 3: Human grammar judgments vs. learnability
difference should be related to the construction’s input         analysis. Learnability is log of the inverse of the number of
occurrence frequency. (Frequencies estimated from COCA).         estimated years needed to learn the construction. Correlation
                                                                 values: r=.35; p=.0045
Experimental method
Participants 105 participants were recruited for an online
grammar judgment study (age range: 16-75 years, mean=34
years). Results were included in the analysis only for
participants who answered that they were native English
speakers (97 out of 105 participants). The majority (74%) of
our participants learned English in the United States. Other
countries included the UK (14%), Canada (5%), Australia
(4%). The rest learned English in either Ireland or New
Zealand.
Procedure Participants were asked to rate the
grammaticality of grammatical and ungrammatical
sentences using the 17 constructions whose learnability
were quantified above. These sentences (34 total) are
shown in Table 1. Grammar judgments ranged from 1-5: 1)
Sounds completely fine (Definitely grammatical) 2)               Figure 4: Human grammar judgments vs. log of occurrence
Probably grammatical (Sounds mostly fine) 3) Sounds              frequency. Frequencies were estimated using Corpus of
barely passable (Neutral) 4) Sounds kind of odd (probably        Contemporary American English.
                                                               1724

 Summary and Conclusions                                           grammar? In J.Hawkins (Ed.), Explaining Language
                                                                   Universals (pp. 73-101). Oxford: Blackwell.
 This presented work helps evaluate how much of first
                                                                 Chater, N. (1996). Reconciling simplicity and likelihood
 language is probabilistically acquired from exposure. We
                                                                   principles in perceptual organization. Psychological Review,
 show that, despite Gold’s theorem, language is identifiable
                                                                   103, 566-581.
 with a cognition general prior of simplicity under fairly
                                                                 Chater, N. & Hsu, A. (in preparation). Language learning in
 general assumptions. We then describe a recently
                                                                   the limit: theory and practice.
 formulated framework which allows probabilistic
                                                                 Chater, N. & Vitányi, P.M.B. (2007). `Ideal learning' of
 learnability to be quantified in the context of natural
                                                                   natural language: Positive results about learning from
 language. This framework makes concrete predictions in
                                                                   positive evidence, Journal of Mathematical Psychology,
 terms of years needed to learn particular linguistic rules,
                                                                   51,135-163.
 given an assumed formulation of the rules to be learned and
                                                                 Chomsky, N. (1975/1955). The Logical Structure of Linguistic
 the corpus which represents a learner’s language input.
                                                                   Theory. London: Plenum Press.
    There has now been a substantial body of work showing
                                                                 Crain, S. (1991). Language Acquisition in the Absence of
 that probabilistic language learning is theoretically and
                                                                   Experience. Behavioral and Brain Sciences, 14, 597-612.
 computationally possible. The important next step in
                                                                 Dowman, M. (2007). Minimum Description Length as a
 research on language acquisition is to assess whether
                                                                   Solution to the Problem of Generalization in Syntactic
 probabilistic learning actually occurs in practice. Here we
                                                                   Theory. Machine Learning and Language, (in review).
 make the supposition that if language is probabilistically
                                                                 Feldman, Jacob (2000). Minimization of boolean complexity
 acquired, then there should be evidence of this in adult
                                                                   in human concept learning. Nature, 403, 630-633.
 grammar judgments. There is a subtle leap of logic in this
                                                                 Feldman, J.A., Gips, J., Horning, J. J., & Reder, S. (1969)
 supposition. MDL learnability assumes that a grammar is
                                                                   Grammatical complexity and inference. Technical Report
 learned in an absolute sense: once a grammar is chosen
                                                                   CS 125, Stanford University.
 under MDL, that is the one used and there is no gradation of
                                                                 Foraker, S., Regier, T., Khetarpal, N., Perfors, A., &
 knowledge. However, here we are conjecturing that
                                                                   Tenenbaum, J. B. (2009). Indirect Evidence and the Poverty
 learnability should not only correlate with how long it takes
                                                                   of the Stimulus: The Case of Anaphoric One. Cognitive
 for linguistic rule to be acquired, but also with how certain
                                                                   Science, 33, 300.
 is one’s knowledge of that rule. The more certain one is of
                                                                 Gold, E. M. (1967). Language identification in the limit.
 a grammatical rule, the greater the difference should be
                                                                   Information and Control, 10, 447-474.
 one’s acceptability rating of the ungrammatical form
                                                                 Grünwald, P. (1994). A minimum description length approach
 relative to the grammatical form. Experimental results show
                                                                   to grammar inference. In S.Scheler, Wernter, & E. Rilof
 that predicted learnability correlates well with relative
                                                                   (Eds.), Connectionist, Statistical and Symbolic Approaches
 grammar judgments for the 17 constructions analyzed,
                                                                   to Learning for Natural Language. (pp. 203-216). Berlin:
 chosen as controversial cases from the literature. Our
                                                                   Springer Verlag.
 experimental results support the possibility that many
                                                                 Hsu, A. & Chater, N. (2010). The logical problem of language
 linguistic constructions that have been argued to be innately
                                                                   acquisition: A probabilistic perspective. Cognitive Science,
 acquired may instead be acquired by probabilistic learning.
                                                                   2nd revision submitted.
    Our learnability predictions were calculated using a large
                                                                 MacKay, D. (2003). Information Theory, Inference, and
 corpus (COCA) to represent the distributional language
                                                                   Learning Algorithms. Cambridge: Cambridge University
 input that native English speakers receive. This assumes
                                                                   Press.
 that the distributional information estimated from this
                                                                 Perfors, A., Regier, T., & Tenenbaum, J. B. (2006). Poverty of
 corpus is representative of that which influenced the
                                                                   the Stimulus? A rational approach. Proceedings of the
 language acquisition process in our adult participants. It also
                                                                   Twenty-Eighth Annual Conference of the Cognitive Science
 allows for the possibility that a speaker’s certainty about
                                                                   Society, 663-668.
 different linguistic rules is updated through adulthood using
                                                                 Pinker, S. (1989). Learnability and Cognition: The acquisition
 probabilistic learning. If so, older adults might more certain
                                                                   of argument structure. Cambridge, MA: MIT Press.
 in their grammar judgments, is a direction for future work.
                                                                 Regier, T. & Gahl, S. (2004). Learning the unlearnable: The
                                                                   role of missing evidence. Cognition, 93, 147-155.
 References                                                      Theakston, A. (2004). The role of entrenchment in children's
Ambridge, B., Pine, J., Rowland, C., & Young, C. (2008). The       and adults' performance on grammaticality judgment tasks.
 effect of verb semantic class and verb frequency                  Cognitive Development, 19, 15-34.
 (entrenchment) on children's and adults' graded judgements      Vitányi, P. & Li, M. (2000). Minimum Description Length
 of argument-structure overgeneralization errors. Cognition,       Induction, Bayesianism, and Kolmogorov Complexity.
 106, 87-129.                                                      IEEE Transactions on Information Theory, IT, 46, 446-464.
Bowerman, M. (1988). The 'No Negative Evidence' Problem:
 How do Children avoid constructing an overly general
                                                                 1725

