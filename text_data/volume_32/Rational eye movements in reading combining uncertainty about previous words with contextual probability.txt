UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Rational eye movements in reading combining uncertainty about previous words with
contextual probability
Permalink
https://escholarship.org/uc/item/1mm294p6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Bicknell, Klinton
Levy, Roger
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                Rational eye movements in reading combining
                   uncertainty about previous words with contextual probability
                                                   Klinton Bicknell & Roger Levy
                                                  {kbicknell, rlevy}@ling.ucsd.edu
                                                UC San Diego Department of Linguistics
                                        9500 Gilman Drive #108, La Jolla, CA 92093-0108 USA
                              Abstract                                 movement record. The past 30 years have seen a prolifera-
                                                                       tion of experimental studies investigating this topic, which
   While there exist a range of sophisticated models of eye move-      have answered a number of low-level questions such as the
   ments in reading, it remains an open question to what ex-
   tent human eye movement behavior during reading is adaptive         nature of the perceptual span and constraints on saccade la-
   given the demands of the task. In this paper, we help to an-        tency as well as questions concerning the relationship be-
   swer this question by presenting a model of reading that cor-       tween eye movements and higher-level cognitive processes
   rects two problems with a rational model of the task, Mr. Chips
   (Legge, Klitz, & Tjan, 1997). We show that the resulting model      such as the effect of word frequency and predictability (see
   is closer to human performance across two measures, support-        Rayner, 1998 for an overview). Sophisticated computational
   ing the idea that many components of eye movement behavior          models have been developed based on these findings, the
   in reading can be well understood as a rational response to the
   demands of the task.                                                most well-known of which are E-Z Reader (Reichle, Pollat-
   Keywords: computational modeling; rational analysis; eye
                                                                       sek, Fisher, & Rayner, 1998; Reichle et al., 2006) and SWIFT
   movements; reading                                                  (Engbert et al., 2005). Both E-Z Reader and SWIFT assume
                                                                       that lexical processing (or word recognition) is the primary
                          Introduction                                 driver for eye-movements in reading, and both have enjoyed
                                                                       considerable success, in large part because they achieve very
Choosing when and where to move one’s eyes during reading              good fits to eye movement data from reading in a number of
is one of the most complicated skilled tasks humans perform.           contexts, using a relatively small number of parameters. De-
While there are a number of computational models achiev-               spite their empirical strength, they fail to illuminate the rea-
ing good numerical fits on eye movement data from read-                son why human reading behavior looks the way it does in one
ing (e.g., Reichle, Pollatsek, & Rayner, 2006; Engbert, Nuth-          crucial respect – the extent to which it resembles a rational
mann, Richter, & Kliegl, 2005), it is still unclear to what ex-        response to the problem posed by reading.
tent the complex behaviors observed are rational responses
to the demands of the problem itself and to what extent they              One leading approach for answering such questions is that
arise from the idiosyncrasies and restrictions of human cog-           of rational analysis (Anderson, 1990), a paradigm in which
nition. Legge, Klitz, and Tjan (1997) started to answer this           one formalizes the goals and cognitive and physical con-
question with Mr. Chips, a model which predicts eye move-              straints relevant to a problem and develops a model of optimal
ments that approximate an optimal solution to one formaliza-           behavior under those condition. To the extent that the behav-
tion of the task of reading. Legge et al. pointed out that their       ior of the model is similar to that of humans, this provides
model’s behavior exhibits a number of patterns also found in           a new way of understanding the reason why human behavior
human reading, providing evidence for understanding those              looks the way it does – it is the best way to solve the problem.
behaviors as rational responses to the task. Despite its suc-          The relationship between rational models and models such as
cess, however, the Mr. Chips model oversimplifies two im-              E-Z Reader and SWIFT is well understood in terms of Marr’s
portant aspects of the problem of reading, and also has empir-         (1982) levels of analysis. Marr distinguishes three levels of
ical problems accounting for human reading behavior in two             mutually-constraining analyses that can be performed on cog-
domains. In this paper, we propose a model extending Mr.               nitive processes: the computational level, which specifies the
Chips that removes these two oversimplifications to make the           nature of the computation being performed, the information
model’s task more similar to that faced by humans. We show             relevant to solving it, and the way to combine that information
that the resulting model also remedies the two empirical defi-         to solve it; the algorithmic level, which specifies the represen-
ciencies in Mr. Chips, further supporting the notion that many         tation for the input and output and the algorithm by which the
aspects of human reading behavior can be explained as ratio-           agent goes about solving it; and the implementational level,
nal responses to the demands of reading.                               which specifies how the representations and algorithm are re-
   The essentials of the problem of making eye movements               alized neurally. In these terms, rational models generally pro-
in reading are determining how long to leave the eyes in               vide answers at the computational level of analysis. Models
a given spot and – when a reader decides to move them –                such as E-Z Reader and SWIFT help us to understand the al-
where to go. These decisions are made sequentially to pro-             gorithmic level, but cannot answer questions about the extent
duce the alternating sequence of fixations (relatively stable          to which human reading is rational.
periods) and saccades (movements) that characterizes the eye              Legge et al. (1997) presented a computational level analy-
                                                                   1142

sis of reading, formalizing the central task – as in E-Z Reader       whether each character is part of a word or not (e.g., a space).
and SWIFT – as one of serial word identification. They pre-           The number of characters in each of these ranges was chosen
sented the Mr. Chips model, which approximates optimal be-            to be representative of the perceptual span for readers of En-
havior under their formalization, and shows a number of sim-          glish, known to be around 17–19 characters (Rayner, 1998).
ilarities with human reading behavior. Here, we point out two
                                                                      Language knowledge The model’s knowledge of language
problems with their model of reading. First, their model takes
                                                                      consists of simply word frequency information, i.e., a uni-
the task to be to identify a string of independent words rather
                                                                      gram model. Note that this means the model cannot make use
than a coherent sequence, i.e., their model does not make use
                                                                      of the linguistic context to aid in word identification.
of linguistic context, which experimental work suggests that
humans use (McDonald & Shillcock, 2003). Second, it as-               Motor error The final component of the model’s knowl-
sumes that the task of the reader is to identify each word with       edge of the task is that of motor error, the distribution of a
complete certainty, yet recent evidence suggests that readers         saccade’s landing position given the intended target position
maintain uncertainty as to the identities of previous words           the model chooses. In Mr. Chips, the ith landing position `i is
(Levy, Bicknell, Slattery, & Rayner, 2009). In addition to            normally distributed around the ith intended target position ti
these problems in their model’s design, the model also makes          with a standard deviation of 30% of the intended distance1
incorrect predictions for two relatively basic measures of eye
                                                                                          `i ∼ N ti , (0.3 · |ti − `i−1 |)2 .
                                                                                                                             
movements in reading: saccade sizes and word skipping rates.                                                                            (1)
The model we present fixes these two design problems by
including linguistic context and using a flexible word iden-          Model
tification criterion, and results in improved performance in          We now give the algorithm that the Mr. Chips model uses to
accounting for human saccade sizes and word skipping rates.           select the intended target for the next saccade. First, note that
    The plan of the remainder of the paper is as follows. First,      given the visual input obtained by the model from the first to
we describe the details of the Mr. Chips model, along with its        the ith fixation I1i and the word frequency information, the
empirical successes and failures. Next, we describe our exten-        model can calculate the posterior probability of any possible
sion of the Mr. Chips model, and finally present two experi-          identity of a word w that is consistent with the visual input by
ments showing that fixing each of the two design problems             normalizing its probability from the language model by the
results in performance more like humans.                              total probability of all visually consistent identities,
                          Mr. Chips                                                                          χ(I1i , w)p(w)
                                                                                           p(w|I1i ) =                                  (2)
The task of reading in the Mr. Chips model (Legge et al.,                                                ∑ χ(I1i , w0 )p(w0 )
1997) is one of planning saccades for serial word identifica-                                             w0
tion. That is, the model works by gathering visual input from         where χ(I, w) is an indicator function with a value of 1 if w is
the current fixation location and using that visual input to plan     consistent with the visual input I and 0 otherwise, and p(w)
a saccade. That saccade is then executed (with some motor er-         is the probability of w under the language model.
ror), visual input is obtained from the new location, and the            To identify a given word, the model selects the saccade tar-
cycle repeats. When one word is identified with 100% confi-           get tˆi that, on average, will minimize the entropy in this dis-
dence, identification of the next word begins. Thus, the only         tribution, i.e., that is expected to give the most information
decision the model makes is where to move the eyes next.              about the word’s identity
There are just three sources of information relevant to mak-
                                                                                     tˆi = argmin E H(w|I1i )|ti , I1i−1
                                                                                                                           
ing that decision. Visual input and knowledge of the language                                                                           (3)
are combined to identify words, and knowledge of the mo-                                      ti
tor error in the system assists in the planning problem. Since                           = argmin ∑ H(w|I1i )p(Ii |ti , I1i−1 ).        (4)
it forms the basis for our model, we describe the Mr. Chips                                   ti     Ii
model here in detail, discussing in turn each of the sources of
                                                                      That is, the minimum can be found by calculating the condi-
information and then the algorithm by which the model com-
                                                                      tional entropy produced by each possible new input sequence
bines them to choose saccades. To match the description of
                                                                      and weighting those entropies by the probability of getting
our model later, we use a notation a bit different than Legge
                                                                      that input sequence given a choice of target location. In infor-
et al. to describe Mr. Chips.
                                                                      mation theory (Cover & Thomas, 2006), conditional entropy
Information sources                                                   is standardly defined as
Visual input The visual input in Mr. Chips consists of the
                                                                                     H(w|I1i ) = − ∑ p(w|I1i ) log p(w|I1i ).           (5)
veridical identities of the nine characters centered on the fix-                                        w
ated character (representing the visual fovea), as well as par-
                                                                          1 In the terminology of the literature, this model has only ‘ran-
tial information about the four characters on either side of
                                                                      dom’ motor error (variance) and not ‘systematic’ motor error (bias),
this range (representing the visual periphery). This partial in-      under the assumption that an optimal model would just compensate
formation is simply word boundary information, indicating             for any systematic problems with its motor control system.
                                                                  1143

The second term in the formula for tˆi , the probability of a par-         As noted above, however, it is also the case that the model
ticular visual input given a target location and previous input,        exhibits some behavior very different from that of human
is given by marginalizing over possible landing positions               readers. For example, the model’s average saccade length is
                                                                        just 6.3 characters, noticeably lower than that for humans,
             p(Ii |ti , I1i−1 ) = ∑ p(`i |ti )p(Ii |`i , I1i−1 ) (6)    who are around 8 (Rayner, 1998). Second, although, as men-
                                  `i
                                                                        tioned, the slope of the relationship between word skipping
and then possible words                                                 rates and word length has a similar slope for the model as for
                                                                        humans, the model skips far fewer words than humans do.2 In
            p(Ii |`i , I1i−1 ) = ∑ p(Ii |`i , w)p(w|I1i−1 ).     (7)    short, judging by these two measures, a rational model that is
                                  w
                                                                        using all the information available and expensively calculat-
Putting these together, we have that tˆi is selected as                 ing the best saccades to reduce entropy in word identification
                                                                        appears to be reading slower than humans do.
   argmin ∑ H(w|I1i ) ∑ p(`i |ti ) ∑ p(Ii |`i , w)p(w|I1i−1 ). (8)         In rational analysis, the fact that an ‘optimal’ model is per-
      ti    Ii              `i        w
                                                                        forming worse than humans (here in terms of speed) suggests
That is, we can calculate the expected conditional entropy for          two likely problems: (a) the model is not making use of all
each possible value of ti by summing over all possible inputs,          the information that humans use or (b) the model’s computa-
whose probabilities are given by summing over all possible              tional goal is not the same as the one that humans are solving.
identities of the word and landing positions. To see that this          As suggested above, we argue that in this case both reasons
sum ranges over a finite number of values, note first that there        are partially to blame. Since it has only word frequency infor-
are only a finite number of possible word identities w to sum           mation as its model of language, the Mr. Chips model cannot
over. Given the possible word identities, there are only a finite       make use of linguistic context to aid in word identification,
number of landing positions `i for which the visual informa-            while there is evidence that humans make heavy use of it.
tion could possibly help in identifying the word – any landing          The model also assumes that the goal is to identify each word
positions outside this range will not produce any reduction in          with 100% confidence, but experiments suggest that humans
entropy. Since there is a single visual input Ii for each combi-        do not. In the next section, we modify the Mr. Chips model
nation of landing position and word identity, this summation            to include some information about linguistic context and a
is over a finite range. To ensure finiteness of the search to find      flexible identification confidence criterion.
the value of ti that produces the minimum entropy, Mr. Chips
only searches those within the range of the `i that could give                              Extending Mr. Chips
some information about the current word. In case of ties, the           The model described here generalizes the Mr. Chips model in
model selects the furthest position to the right.                       three ways. First, it can use an arbitrary language model as its
                                                                        source of language knowledge, and thus make use of informa-
Comparing Mr. Chips to humans                                           tion about the linguistic context in word identification, solv-
Legge, Hooven, Klitz, Mansfield, and Tjan (2002) present a              ing the first problem with Mr. Chips we pointed out above.
number of ways in which the behavior of the Mr. Chips model             Second, it can move on to the next word after it achieves a
is similar to human reading behavior. The model produces                flexible level of certainty about the current word’s identity,
behavior that replicates a number of human findings in word             solving the second problem. Finally, our model also allows
skipping rates, initial fixation locations on words, and refix-         for the standard deviation of the motor error to be an arbi-
ation rates. The result for word skipping rates – where word            trary linear function of the intended distance, allowing us to
skipping for the model is defined as never having any of the            incorporate a more realistic motor error function. We describe
word’s characters as the centrally fixated character – is that          the model in the same format as we described the Mr. Chips
longer words are skipped less often, and the slope of the re-           model, first describing its sources of information, and then its
lationship between word length and skipping rate has a very             algorithm for selecting saccade targets.
similar slope for the model as for humans. For initial word
fixation locations, or landing positions, the model replicates          Information sources
the human behavior of most commonly landing at or just to               Visual input The visual input component is unchanged
the left of the word’s center, and also the fact that the land-         from the original Mr. Chips model.
ing position shifts toward the left as the launch site of the
saccade shifts further to the left. For refixations, the model          Language knowledge The model’s knowledge of language
mimics human behavior in showing the proportion of refix-               is represented by an arbitrary language model that can gen-
ations to increase with word length, and in addition, within            erate string prefix probabilities, e.g., an n-gram model or a
a given word length class, the model refixates low frequency                2 The graph given in Legge et al. (2002) appears to show remark-
words a higher proportion of the time than high frequency               ably similar word skipping rates between the model and humans, but
ones. Finally, as a function of landing position, refixations are       that graph is from the sole simulation in the paper for which Legge
                                                                        et al. assumed no motor error. When motor error is included, the
the least likely for the model, as for humans, when the initial         skipping rates are significantly lower for the model than for humans,
landing position is near the center of the word.                        as shown in Figure 1.
                                                                    1144

probabilistic context-free grammar (PCFG). Such models can                   The second term in Equation 10 is expanded as in Mr. Chips
capture the between-word dependencies needed for the model                   by marginalizing over possible landing positions
to make use of linguistic context in word identification.
                                                                                          p(Ii |ti , I1i−1 ) = ∑ p(`i |ti )p(Ii |`i , I1i−1 ), (13)
Motor error In our model as in Mr. Chips, the ith landing                                                       `i
position is normally distributed around the ith target location,
except that the standard deviation is an arbitrary linear func-              but now to incorporate information about the linguistic con-
tion of the intended distance                                                text, we must next marginalize over possible full sentence
                                                                             strings instead of possible words
                  `i ∼ N ti , (β0 + β1 |ti − `i−1 |)2
                                                      
                                                                     (9)
allowing for the use of a more realistic motor error function.                           p(Ii |`i , I1i−1 ) = ∑ p(Ii |`i ,W )p(W |I1i−1 ).     (14)
Experiments in this paper use the one from SWIFT (Engbert                                                      W
et al., 2005; β0 = 0.87, β1 = 0.084).                                        If we make the simplifying assumption that the model does
Algorithm                                                                    not consider possible future input about words that are after
                                                                             W(n) , this sum can again be finitely computed for a given ti by
As in the original Mr. Chips model, at any given point in
                                                                             a relatively straightforward dynamic programming scheme.
time, the model is working to identify one word. However,
                                                                             The range of possible values of ti to search through also grows
this revised model considers the goal of identifying this word
                                                                             relative to Mr. Chips, because the model must consider not
achieved when the marginal probability of some identity for
                                                                             only any position that can give visual input about W(n) itself,
the word given the visual input exceeds a predefined thresh-
                                                                             but also positions that can give information about any position
old probability α. This flexibility requires the algorithm to be
                                                                             of uncertainty, since that may indirectly help to identify W(n)
substantially modified to allow for uncertainty about previous
                                                                             through linguistic context. In the case where the language
word identities and the use of linguistic context. We denote
                                                                             model is an n-gram model, it can be shown that the mini-
the sequence of words as W , where the first word is W1 .
                                                                             mum value of ti that can contribute toward helping to identify
   Because every word in Mr. Chips was identified with com-
                                                                             W(n) only extends back to the first uncertain character after
plete certainty, the model always knew precisely at which po-
                                                                             the most recent string of n − 1 words for which the model has
sition the next word to be identified began, and its goal was
                                                                             no residual uncertainty. Having established the method of se-
always to identify this next word. Now that the model has
                                                                             lecting a saccade to identify W(n) , we next give a description
uncertainty about the identities of previous words, however,
                                                                             of the full algorithm of the model, including how to select n.
the goal must be changed. In the revised model, the reader is
always focused on some character position n, and its goal is                    The model always begins reading by focusing on identi-
to identify whether some word W(n) begins at that position,                  fying W(0) . Once the probability of some identity for W(0) is
and if so, which one, with confidence exceeding α. Once the                  greater than α, all the possible identities of W(0) that have
model has achieved this goal, it then chooses a new charac-                  not been ruled out by visual input are combined into a set of
ter position n via a procedure whose description we leave for                possible ‘prefixes’. Each of these prefixes has a conditional
later. To be explicit about this goal, we slightly update our                probability given the visual input, and each one predicts that
original equation for choosing tˆi , swapping w out for W(n) ,               the next word in the sentence begins at a particular position
                                                                             (i.e., two characters past the end of that string). Thus, the set
             tˆi = argmin ∑ H(W(n) |I1i )p(Ii |ti , I1i−1 )         (10)     of prefixes specify a probability distribution over the possible
                       ti   Ii                                               positions at which the next word begins. The model simply
where the conditional entropy is calculated assuming that                    selects the most likely such position as the next character po-
some word does in fact begin at position n. The fact that                    sition n to focus on identifying W(n) .
our language model can now make use of linguistic context                       Now in the general case, the system has a set of prefixes
means that the equation for finding the probability of the cur-              together with their conditional probabilities given the visual
rent word given some visual input (Equation 2) must also be                  input, and a position n, which it is trying to identity the word
changed to marginalize over identities of the preceding words                beginning at. It plans and executes saccades according to the
                                         (n)−1       (n)−1
                                                                             formula for tˆi , and after getting each new piece of visual infor-
   p(W(n) |I1i ) =   ∑     p(W(n) |I1i ,W1     )p(W1        |I1i ). (11)     mation, the model rules out not only possible candidates for
                     (n)−1
                    W1                                                       the current word, but also possible prefix strings, and renor-
These probabilities of strings consistent with the visual input              malizes both distributions. The model’s attempt to identify
are again given probabilities according to their probability in              W(n) can now end in one of two ways: (a) the model’s confi-
the language model normalized by the probability of all other                dence in some identity of W(n) exceeds the confidence thresh-
consistent strings (cf. Equation 2)                                          old α or (b) the model eliminates all possible candidates for
                                                                             W(n) and thus knows that no word begins at that position. In
                                  χ(I1i ,W )p(W )                            the former case, the model creates all possible concatenations
                   p(W |I1i ) =                      .              (12)
                                ∑ χ(I1i ,W )p(W )                            of prefixes ending 2 characters prior to W(n) (i.e., prefixes
                                W                                            whose next word begins at n) with all the possible identities of
                                                                         1145

W(n) , and adds these new strings to the set of prefixes. Then,
                                                                                    Table 1: Mean saccade size (and std. error) for each model
in both cases, it removes those original prefixes whose next
word begins at n from the set. Note that this update of the                           Model                            Mean saccade size
list of prefixes leaves unaffected prefixes that are incompati-                       Mr. Chips                        6.7 (.012)
ble with a word beginning at position n, but still compatible                         Without context, 90% criterion 7.1 (.013)
with visual input. Finally, since the set of prefixes again gives                     With context, 90% criterion      7.5 (.014)
a distribution over the starting position of the next word, the                       Humans                           ≈ 8 (Rayner, 1998)
model selects the most likely new n and the cycle continues.
                                                                                                          ●
                       Experiment 1                                                                                           model
                                                                                                    0.8       ●                ●                     University students
With our new model in place, we can now describe the two
                                                                      Proportion of words skipped
                                                                                                                                       Model with context, 90% Criterion
experiments we performed to test our hypotheses about the                                                         ●                 Model without context, 90% Criterion
reasons for the Mr. Chips model’s performance being below                                           0.6                                                       Mr. Chips
that of humans in terms of average saccade length and word                                                               ●
skipping rates. In Experiment 1, we test the hypothesis that
                                                                                                                                ●
one of the reasons that its performance was below humans is                                         0.4
due to its assumption that the goal of the reader is to identify
                                                                                                                                        ●
each word with 100% confidence. Specifically, we compare                                                                                       ●
                                                                                                    0.2                                                ●
the performance of our model using a 100% criterion vs. a                                                                                                     ●
90% criterion. The former is equivalent to Mr. Chips except                                                                                                           ●
for the more realistic motor error function, so for ease of ex-
position, we will refer to it simply as Mr. Chips.                                                            2           4             6              8              10
                                                                                                                      Word length (characters)
Methods
                                                                      Figure 1: Proportion of words skipped by word length for
Confidence criterion We set α = 1.0 to replicate Mr.
                                                                      each model. In all cases, the standard error of the mean for the
Chips, and α = 0.9 for the model using a slightly lower con-
                                                                      Normal approximation to the Binomial distribution is smaller
fidence criterion to trigger moving on to the next word.
                                                                      than the symbols. The human data is from Rayner and Mc-
Language model Both models used a unigram language                    Conkie (1976) and has no standard errors.
model, smoothed with Kneser-Ney under default parameters
(Chen & Goodman, 1998; equivalent to add-δ smoothing for
a unigram model). As in Legge et al. (2002), the models were          ilar to that of human readers, i.e., relaxing the assumption
trained on a 280,000 word corpus of Grimms’ Fairy Tales,              that words need to be identified with 100% certainty, alters
containing 7503 unique words. This corpus was normalized              the performance of the model across two measures to look
by Legge et al. to convert all letters to lowercase, remove all       more like human performance. Such a result adds some sup-
punctuation other than apostrophes, convert all numbers to            port to the idea that the relevant human behavior is well un-
their alphabetic equivalents, and remove all gibberish words.         derstood as a rational response to the demands of the task.
                                                                      It is also worth pointing out that the resulting model main-
Text Following Legge et al. (2002), we tested the models by           tains and uses uncertainty about previous input, something
simulating the reading of 40,000 words of text generated from         for which most models of sentence processing do not allow.
the language model, ensuring that the reading models had a
normative probability model for the text they were reading.                                                            Experiment 2
                                                                      In Experiment 2, we test the effect of allowing the model to
Results
                                                                      use the linguistic context as another source of information
The results for mean saccade size for both models are given           for word identification. Specifically, we compare the previous
in the top two rows of Table 1. As shown in the table, using a        two models to one that includes a 90% confidence criterion as
criterion of 90% increases the average size of saccades, bring-       well as a simple bigram model of linguistic context.
ing it a bit closer to the human estimate of about 8 characters.
The results for word skipping rates for the two models are            Methods
plotted as the lower two lines in Figure 1. The results show a        The methods are the same as Experiment 1, except that the
modest increase in word skipping rates across almost all word         new model uses a bigram language model, again smoothed
lengths for the new model with a lower criterion, bringing it         with Kneser-Ney under default parameters.
closer to human performance.
                                                                      Results
Discussion                                                            The results for average saccade length for the new model is
Although the gain is modest, the results of Experiment 1              given in the third row of Table 1. As shown in the table, giving
show that changing the goal of the model to one more sim-             the model information about linguistic structure increases the
                                                                   1146

average size of saccades a bit more, bringing it still closer to     between-word regressions be made, and how should reading
the human estimate of 8 characters.                                  behavior change as accuracy is valued more or less relative to
    The results for word skipping rates for the new model is         speed.
plotted as the second line in Figure 1. This new model gives
an even larger increase in word skipping rates across all word                             Acknowledgments
lengths, on top of the increase seen previously, bringing it         We are very grateful to Gordon Legge for sharing the cor-
more in line with human results. Skipping rates are 30%              pus used in the original Mr. Chips experiments. This work
closer to humans than the previous 90% criterion model on            also benefited greatly from useful discussion with Jeff El-
average, and for some word lengths are up to 75% closer.             man, Tom Griffiths, Andy Kehler, Keith Rayner, and Angela
                                                                     Yu, and feedback from the audience at the 84th Annual Meet-
Discussion                                                           ing of the Linguistic Society of America. The research was
The results of this experiment show a case in which making           partially supported by NIH Training Grant T32-DC000041
more of the information that is available to a human reader          from the Center for Research in Language at UC San Diego
also available to a rational model causes its behavior to more       to K.B., by a research grant from the UC San Diego Academic
closely approximate human performance. Together with the             Senate to R.L., and by NSF grant 0953870 to R.L.
previous result, this supports the notion that some aspects
of reading are well understood as a rational response to the                                    References
structure of the problem.                                            Anderson, J. R. (1990). The adaptive character of thought. Hills-
                                                                        dale, New Jersey: Lawrence Erlbaum Associates.
                                                                     Bicknell, K., & Levy, R. (2010). A rational model of eye movement
                     General Discussion                                 control in reading. In Proceedings of the 48th Annual Meeting
In this paper, we presented a new rational model of reading             of the Association for Computational Linguistics ACL. Uppsala,
                                                                        Sweden: Association for Computational Linguistics.
based on Mr. Chips, but which fixes two problems with it             Brown, S. D., & Steyvers, M. (2009). Detecting and predicting
– it uses information about linguistic context in word iden-            changes. Cognitive Psychology, 58, 49–67.
tification and a flexible identification criterion. Experiment       Chen, S. F., & Goodman, J. (1998). An empirical study of smoothing
                                                                        techniques for language modeling (Tech. Rep. No. TR-10-98).
1 showed that the model’s performance looks more like hu-               Cambridge, MA: Computer Science Group, Harvard University.
mans’ when the model’s goal is shifted to one more like              Cover, T. M., & Thomas, J. A. (2006). Elements of information
                                                                        theory (2nd ed.). Hoboken, NJ: John Wiley & Sons.
that of humans, 90% confidence in each word. Experiment              Engbert, R., Nuthmann, A., Richter, E. M., & Kliegl, R. (2005).
2 showed the model’s behavior looks even more like humans’              SWIFT: A dynamical model of saccade generation during read-
when the model can use information that is used by humans:              ing. Psychological Review, 112, 777–813.
                                                                     Legge, G. E., Hooven, T. A., Klitz, T. S., Mansfield, J. S., & Tjan,
linguistic context. Taken together, these results suggest that          B. S. (2002). Mr. Chips 2002: new insights from an ideal-
many facets of human reading behavior can be well explained             observer model of reading. Vision Research, 42, 2219–2234.
as resulting from a rational solution to the problem of reading.     Legge, G. E., Klitz, T. S., & Tjan, B. S. (1997). Mr. Chips: an
                                                                        Ideal-Observer model of reading. Psychological Review, 104,
This model adds to the growing literature on rational process           524–553.
models, exploring the extent to which human performance              Levy, R., Bicknell, K., Slattery, T., & Rayner, K. (2009). Eye move-
can be viewed as rational agents across a wide variety of com-          ment evidence that readers maintain and act on uncertainty about
                                                                        past linguistic input. Proceedings of the National Academy of
plex behaviors, such as multiple object tracking (Vul, Frank,           Sciences, 106, 21086–21090.
Alvarez, & Tenenbaum, 2009) and online change detection              Marr, D. (1982). Vision: A computational investigation into the
(Brown & Steyvers, 2009).                                               human representation and processing of visual information. San
                                                                        Francisco: W.H. Freeman.
    It is the case, however, that many aspects of human read-        McDonald, S. A., & Shillcock, R. C. (2003). Eye movements reveal
ing behavior cannot in principle be explained by a model                the on-line computation of lexical probabilities during reading.
                                                                        Psychological Science, 14, 648–652.
such as those described in this paper. This is because much          Rayner, K. (1998). Eye movements in reading and information
of what we know about human reading behavior is about fix-              processing: 20 years of research. Psychological Bulletin, 124,
ation durations, and these models have no notion of duration.           372–422.
                                                                     Rayner, K., & McConkie, G. W. (1976). What guides a reader’s eye
They cannot have a notion of duration because visual input is           movements? Vision Research, 16, 829–837.
veridical categorical information about a range of characters,       Reichle, E. D., & Laurent, P. A. (2006). Using reinforcement learn-
so that there is no reason to stay at a given location for more         ing to understand the emergence of “intelligent” eye-movement
                                                                        behavior during reading. Psychological Review, 113, 390–408.
than one timestep. Reichle and Laurent (2006) overcome this          Reichle, E. D., Pollatsek, A., Fisher, D. L., & Rayner, K. (1998). To-
problem by making the simplifying assumption that required              ward a model of eye movement control in reading. Psychological
processing times on words are a function only of their length.          Review, 105, 125–157.
                                                                     Reichle, E. D., Pollatsek, A., & Rayner, K. (2006). E-Z Reader:
    We believe, however, that the way forward for rational              A cognitive-control, serial-attention model of eye-movement be-
models of reading is to incorporate a model of noisy visual             havior during reading. Cognitive Systems Research, 7, 4–22.
input, so that the model can make decisions about fixation           Vul, E., Frank, M., Alvarez, G., & Tenenbaum, J. (2009). Explain-
                                                                        ing human multiple object tracking as resource-constrained ap-
durations to get more or less visual information. In other              proximate inference in a dynamic probabilistic model. In Y. Ben-
work (Bicknell & Levy, 2010), we explore the use of such                gio, D. Schuurmans, J. Lafferty, C. K. I. Williams, & A. Culotta
models to answer questions that are impossible to ask with              (Eds.), Advances in Neural Information Processing Systems 22
                                                                        (pp. 1955–1963).
non-rational models of reading such as when and why should
                                                                 1147

