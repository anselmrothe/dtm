UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Computational Modeling of Emotional Content in Music
Permalink
https://escholarship.org/uc/item/5bk7z3z4
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Monteith, Kristine
Martinez, Tony
Ventura, Dan
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                       Computational Modeling of Emotional Content in Music
                                       Kristine Monteith (kristinemonteith@gmail.com)
                                             Tony Martinez (martinez@cs.byu.edu)
                                                Dan Ventura (ventura@cs.byu.edu)
                                                     Department of Computer Science
                                                         Brigham Young University
                                                               Provo, UT 84602
                             Abstract                                    associate emotions with various facial expressions (Tolbert,
   We present a system designed to model characteristics which           2001). Scherer presents a framework for formally describ-
   contribute to the emotional content of music. It creates n-gram       ing the emotional effects of music and then outlines factors
   models, Hidden Markov Models, and entropy-based models                that contribute to these emotions, including structural, per-
   from corpora of musical selections representing various
   emotions. These models can be used both to identify emo-              formance, listener, and contextual features (Scherer, 2001).
   tional content and generate pieces representative of a target            In this paper, we focus on some of the structural aspects of
   emotion. According to survey results, generated selections            music and the manner in which they contribute to emotions in
   were able to communicate a desired emotion as effectively as
   human-generated compositions.                                         music. We present a cognitive model of characteristics of mu-
                                                                         sic responsible for human perception of emotional content.
   Keywords: Music cognition;          computational modeling;           Our model is both discriminative and generative; it is capable
   learning; music composition.
                                                                         of detecting a variety of emotions in musical selections, and
                         Introduction                                    also of producing music targeted to a specific emotion.
Music and emotion are intrinsically linked; music is able
                                                                                                Related Work
to express emotions that cannot adequately be expressed by
words alone. Often, there is strong consensus among listen-              A number of researchers have addressed the task of modeling
ers as to what type of emotion is being expressed in a par-              musical structure for the purposes of building a generative
ticular piece (Gabrielsson & Lindstrom, 2001; Juslin, 2001).             musical system. Conklin summarizes a number of statisti-
There is even some evidence to suggest that some perceptions             cal models which can be used for music generation, includ-
of emotion in music may be innate. For example, selections               ing random walk, Hidden Markov Models, stochastic sam-
sharing some acoustical properties of fear vocalizations, such           pling, and pattern-based sampling (Conklin, 2003). These
as sudden onset, high pitch, and strong energy in the high fre-          approaches can be seen in a number of different studies. For
quency range, often provoke physiological defense responses              example, Hidden Markov Models have been used to harmo-
(Ohman, 1988). Researchers have demonstrated similar low-                nize melodies, considering melodic notes as observed events
level detection mechanisms for both pleasantness and novelty.            and a chord progression as a series of hidden states (Allan &
(Scherer, 1984, 1988). There also appears to be some inborn              Williams, 2005). Similarly, Markov chains have been used
preference for consonance over dissonance. In studies with               to harmonize given melody lines, focusing on harmonization
infants, researchers found that their subjects looked signifi-           in a given style in addition to finding highly probable chords
cantly longer at the source of sound and were less likely to             (Chuan & Chew, 2007).
squirm and fret when presented with consonant as opposed to                 Wiggins, Pearce, and Mullensiefen present a system de-
dissonant versions of a melody (Zentner & Kagan, 1996).                  signed to model factors such as pitch expectancy and melodic
   There are a variety of theories as to what aspects of mu-             segmentation. They also demonstrate that their system can
sic are most responsible for eliciting emotional responses.              successfully generate music in a given style (Wiggins, Pearce,
Meyer theorizes that meaning in music comes from following               & Mullensiefen, 2009). Systems have also been developed
or deviating from an expected structure (Meyer, 1956). Slo-              to produce compositions with targeted emotional content.
boda emphasizes the importance of associations in the per-               Delgado, Fajardo, and Molina-Solana use a rule-based sys-
ception of emotion in music and gives particular emphasis                tem to generate compositions according to a specified mood
to association with lyrics as a source for emotional meaning             (Delgado, Fajardo, & Molina-Solana, 2009). Rutherford and
(Sloboda, 1985). Kivy argues for the importance of cultural              Wiggins analyze the features that contribute to the emotion
factors in understanding emotion and music, proposing that               of fear in a musical selection and present a system that allows
the “emotive life” of a culture plays a major role in the emo-           for an input parameter that determines the level of “scariness”
tions that members of that culture will detect in their music            in the piece (Rutherford & Wiggins, 2003). Oliveira and Car-
(Kivy, 1980). Tolbert proposes that children learn to associate          doso describe a wide array of features that contribute to emo-
emotion with music in much the same way that they learn to               tional content in music and present a system that uses this
                                                                     2356

information to select and transform chunks of music in ac-
cordance with a target emotion (Oliveira & Cardoso, 2007).                             Love:                         Joy:
The authors have also developed a system that addresses the                   Advance to the Rear                    1941
task of composing music with a specified emotional content               Bridges of Madison County              633 Squadron
(Monteith, Martinez, & Ventura, 2010). In this paper, we                            Casablanca              Baby Elephant Walk
illustrate how our system can be interpreted as a cognitive                         Dr. Zhivago                Chariots of Fire
model of human perception of emotional content in music.                       Legends of the Fall                Flashdance
                                                                                   Out of Africa                   Footloose
                         Methodology                                                                            Jurassic Park
The proposed system constructs statistical and entropic mod-                         Surprise:                 Mrs. Robinson
els for various emotions based on corpora of human-labeled                       Addams Family               That Thing You Do
musical data. Analysis of these models provides insights as to                    Austin Powers           You’re the One that I Want
why certain music evokes certain emotions. The models sup-                            Batman
ply localized information about intervals and chords that are                    Dueling Banjos                     Anger:
more common to music conveying a specific emotion. They                       George of the Jungle             Gonna Fly Now
also supply information about what overall melodic charac-              Nightmare Before Christmas               James Bond
teristics contribute to emotional content. To validate our find-                   Pink Panther              Mission Impossible
ings, we generate a number of musical selections and ask re-                     The Entertainer            Phantom of the Opera
search subjects to label the emotional content of the gener-                         Toy Story                       Shaft
ated music. Similar experiments are conducted with human-                         Willie Wonka
generated music commissioned for the project. We then ob-                                                          Sadness:
serve the correlations between subject responses and our pre-                          Fear:                    Forrest Gump
dictions of emotional content.                                                    Axel’s Theme                 Good Bad Ugly
   Initial experiments focus on the six basic emotions outlined                     Beetlejuice                    Rainman
by Parrott (Parrott, 2001)—love, joy, surprise, anger, sadness,              Edward Scissorhands              Romeo and Juliet
and fear—creating a data set representative of each. A sepa-                            Jaws                   Schindler’s List
rate set of musical selections is compiled for each of the emo-                Mission Impossible
tions studied. Selections for the training corpora are taken                 Phantom of the Opera
from movie soundtracks due to the wide emotional range                                Psycho
present in this genre of music. MIDI files used in the exper-           Star Wars: Duel of fhe Fates
iments can be found at the Free MIDI File Database.1 These                    X-Files: The Movie
MIDI files were rated by a group of research subjects. Each
selection was rated by at least six subjects, and selections
rated by over 80% of subjects as representative of a given
emotion were then selected for use in the training corpora.           Figure 1: Selections used in training corpora for the six dif-
Selections used for these experiments are shown in Figure 1.          ferent emotions considered.
   Next, the system analyzes the selections to create statisti-
cal models of the data in the six corpora. Selections are first
transposed into the same key. Melodies are then analyzed              freely available jMusic software.2 This component returns
and n-gram models are generated representing what notes are           a vector of twenty-one statistics describing a given melody,
most likely to follow a given series of notes in a given corpus.      including factors such as number of consecutive identical
Statistics describing the probability of a melody note given          pitches, number of distinct rhythmic values, tonal deviation,
a chord, and the probability of a chord given the previous            and key-centeredness. These statistics are calculated for both
chord, are collected for each of the six corpora. Information         the major and minor scales.
is also gathered about the rhythms, the accompaniment pat-               A separate set of classifiers is developed to evaluate both
terns, and the instrumentation present in the songs.                  generated rhythms and generated pitches. The first classi-
   The system also makes use of decision trees constructed            fier in each set is trained using analyzed selections in the
to model the characteristics that contribute to emotional con-        target corpus as positive training instances and analyzed se-
tent. These trees are constructed using the C4.5 algorithm            lections from the other corpora as negative instances. This
(Quinlan, 1993), an extension of the ID3 algorithm (Quinlan,          is intended to help the system distinguish selections contain-
1986) that allows for real-valued attributes. The decision tree       ing the desired emotion. The second classifier in each set is
classifiers classifiers allow for a more global analysis of gen-      trained with melodies from all corpora versus melodies previ-
erated melodies. Inputs to these classifiers are the default fea-     ously generated by the algorithm, allowing the system to learn
tures extracted by the “Phrase Analysis” component of the             melodic characteristics of selections which have already been
    1 http://themes.mididb.com/movies/                                    2 http://jmusic.ci.qut.edu.au/
                                                                  2357

accepted by human audiences.                                             For example, C4 is most likely to be accompanied by a C
   For the generative portion of the model, the system em-            major chord, and F4 is most likely to be accompanied by a
ploys four different components: a Rhythm Generator, a Pitch          G7 chord in selections from the “love” corpus (probabilies of
Generator, a Chord Generator, and an Accompaniment and                0.099 and 0.061, respectively). In the “sadness” corpus, C4
Instrumentation Planner. The functions of these components            is most likely to be accompanied by a C minor chord (prob-
are explained in more detail in the following sections.               ability of 0.060). As examples from the second set of dis-
                                                                      tributions, the G7 chord is most likely to be followed by the
Rhythm Generator                                                      G7 or the C major chord in selections from the “love” cor-
The rhythm for the selection with a desired emotional content         pus (both have a probability of 0.105). In selections from the
is generated by selecting a phrase from a randomly chosen se-         “sadness” corpus, the G7 chord is most likely to be followed
lection in the corresponding data set. The rhythmic phrase is         by the G7 or the C minor chord (probabilities of 0.274 and
then altered by selecting and modifying a random number of            0.094 respectively).
measures. The musical forms of all the selections in the cor-            The system then calculates which set of chords is most
pus are analyzed, and a form for the new selection is drawn           likely given the melody notes and the two conditional prob-
from a distribution representing these forms. For example,            ability distributions. Since many of the songs in the training
a very simple AAAA form, where each of four successive                corpora had only one chord present per measure, initial at-
phrases contains notes with the same rhythm values, tends to          tempts at harmonization also make this assumption, consid-
be very common. Each new rhythmic phrase is analyzed by               ering only downbeats as observed events in the model.
jMusic and then provided as input to the rhythm evaluators.
Generated phrases are only accepted if they are classified pos-       Accompaniment and Instrumentation Planner
itively by both classifiers.                                          The accompaniment patterns for each of the selections in
                                                                      the various corpora are categorized, and the accompaniment
Pitch Generator                                                       pattern for a generated selection is probabilistically selected
Once the rhythm is determined, pitches are selected for the           from the patterns of the target corpus. Common accompani-
melodic line. These pitches are drawn according to the n-             ment patterns included arpeggios, block chords sounding on
gram model constructed from melody lines of the corpus with           repeated rhythmic patterns, and a low base note followed by
the desired emotion. A melody is initialized with a series of         chords on non-downbeats.
random notes, selected from a distribution that models notes             For example, arpeggios are a common accompaniment pat-
most likely to begin musical selections in the given corpus.          tern in the corpus of selections expressing the emotion of
Additional notes in the melodic sequence are randomly se-             “love.” Two of the selections in the corpus feature simple,
lected based on a probability distribution of note mosts likely       arpeggiated chords as the predominant theme in their accom-
to follow the given series of n notes.                                paniments, and two more selections have an accompaniment
   For example, with the “joy” corpus, the note sequence (C4,         pattern that feature arpeggiated chords played by one instru-
D4, E4) has a 0.667 probability of being followed by an F4,           ment and block chords played by a different instrument. The
a 0.167 probability of being followed by a D4, and a 0.167            remaining two selections in the corpus feature an accompani-
probability of being followed by a C4. If these three notes           ment pattern of a low base note followed by chords on non-
were to appear in succession in a generated selection, the sys-       downbeats. When a new selection is generated by the system,
tem would have a 0.167 probability of selecting a C4 as the           one of these three patterns is selected with equal likelihood to
next note.                                                            be the accompaniment for the new selection.
   The system generates several hundred possible series of               Instruments for the melody and harmonic accompaniment
pitches for each rhythmic phrase. As with the rhythmic com-           are also probabilistically selected based on the frequency
ponent, features are then extracted from these melodies using         of various melody and harmony instruments in the corpus.
jMusic and provided as inputs to the pitch evaluators. Gener-         For example, melody instruments for selections in the “sur-
ated melodies are only selected if they are classified positively     prise” corpus include acoustic grand piano, electric piano,
by both classifiers.                                                  and piccolo. Harmony instruments include trumpet, trom-
                                                                      bone, acoustic grand piano, and acoustic bass.
Chord Generator
The underlying harmony is determined using a Hidden                   Evaluation
Markov Model, with pitches considered as observed events              In order to verify that our system was accurately model-
and the chord progression as the underlying state sequence            ing characteristics contributing to emotional content, we pre-
(Rabiner, 1989). The Hidden Markov Model requires two                 sented our generated selections to research subjects and asked
conditional probability distributions: the probability of a           them to identify the emotions present. Forty-eight subjects,
melody note given a chord and the probability of a chord              ages 18 to 55, participated in this study. Six selections were
given the previous chord. The statistics for these probability        generated in each category, and each selection was played
distributions are gathered from the corpus of music represent-        for four subjects. Subjects were given the list of emotions
ing the desired emotion.                                              and asked to circle all emotions that were represented in each
                                                                  2358

song. Each selection was also played for four subjects who
had not seen the list of emotions. These subjects were asked         Love:
to write down any emotions they thought were present in the          RepeatedPitchDensity <= 0.146
music without any suggestions of emotional categories on the         - RepeatedPitchPatternsOfThree <= 0.433: Yes
part of the researchers. Reported results represent percent-         - RepeatedPitchPatternsOfThree > 0.433: No
ages of the twenty-four responses in each category. To pro-          RepeatedPitchDensity > 0.146: No
vide a baseline, two members of the campus songwriting club
were also asked to perform the same task: compose a musical          Joy:
selection representative of one of six given emotions. Each          PitchMovementByTonalStep <= 0.287: No
composer provided selections for three of the emotional cat-         PitchMovementByTonalStep > 0.287
egories. These selections were evaluated in the same manner          - ClimaxPosition <= 0.968
as the computer-generated selections, with four subjects lis-        - - ClimaxTonality <= 0: No
tening to each selection for each type of requested response.        - - ClimaxTonality > 0
Reported results represent percentages of the four responses         - - - PitchMovementByTonalStep(Minor) <= 0.535: No
in each category.                                                    - - - PitchMovementByTonalStep(Minor) > 0.535: Yes
                                                                     - ClimaxPosition > 0.968: Yes
                            Results
                                                                     Surprise:
Figure 2 outlines the characteristics identified by the decision     RepeatedPitchPatternsOfFour <= 0.376: No
trees as being responsible for emotional content. For exam-          RepeatedPitchPatternsOfFour > 0.376
ple, if a piece had a Dissonance measure over 0.107 and a            - PitchMovementByTonalStep (Minor) <= 0.550
Repeated Pitch Density measure over 0.188, it was classified         - - ClimaxPosition <= 0.836: Yes
in the “anger” category. Informally, angry selections tend to        - - ClimaxPosition > 0.836
be dissonant and have many repeated notes. Similar infor-            - - - LeapCompensation <= 0.704: No
mation was collected for each of the different emotions. Se-         - - - LeapCompensation > 0.704
lections expressing “love” tend to have lower repeated pitch         - - - - KeyCenteredness <= 0.366: No
density and fewer repeated patterns of three, indicating these       - - - - KeyCenteredness > 0.366: Yes
selections tend to be more “flowing.” Joyful selections have         - PitchMovementByTonalStep(Minor) > 0.550: No
some stepwise movement in a major scale and tend to have a
                                                                     Anger:
strong climax at the end. The category of “surprise” appears
                                                                     Dissonance <= 0.107: No
to be the least cohesive; it requires the most complex set of
                                                                     Dissonance > 0.107
rules for determining membership in the category. However,
                                                                     - RepeatedPitchDensity <= 0.188: No
repeated pitch patterns of four are present in all the surpris-
                                                                     - RepeatedPitchDensity > 0.188: Yes
ing selections, as is a lack of stepwise movement in the major
scale. Not surprisingly, selections expressing “sadness” ad-         Sadness:
here to a minor scale and tend to have a downward trend in           TonalDeviation(Minor) <= 0.100
pitch. Fearful selections deviate from the major scale, do not       - OverallPitchDirection <= 0.500: Yes
always compensate for leaps, and have an upward pitch direc-         - OverallPitchDirection > 0.500: No
tion. Downward melodic trends do not deviate as much from            TonalDeviation (Minor) > 0.100: No
the major scale. Our model appears to be learning to detect
the melodic minor scale; melodies moving downward in this            Fear:
scale will have a raised sixth and seventh tone, so they differ      TonalDeviation <= 0.232: No
in only one tone from a major scale.                                 TonalDeviation > 0.232
    Tables 1 and 2 report results for the constrained response       - LeapCompensation <= 0.835
surveys. Row labels indicate the corpus used to generate a           - - OverallPitchDirection <= 0.506
given selection, and column labels indicate the emotion iden-        - - - TonalDeviation <= 0.290: Yes
tified by survey respondents. Based on the results in Table 1,       - - - TonalDeviation > 0.290: No
our system is successful at modeling and generating music            - - OverallPitchDirection > Yes
with targeted emotional content. For all of the emotional cate-      - LeapCompensation > 0.835: No
gories but “surprise,” a majority of people identified the emo-
tion when presented with a list of six emotions. In all cases,
the target emotion ranked highest or second highest in terms
of the percentage of survey respondents identifying that emo-        Figure 2: Decision tree models of characteristics contributing
tion as present in the computer-generated songs. As a gen-           to emotional content in music.
eral rule, people were more likely to select the categories of
“joy” or “sadness” than some of the other emotions, perhaps
                                                                 2359

because music in western culture is traditionally divided up       ity” were also used to describe the selections.
into categories of major and minor. A higher percentage of            Angry songs were often described using Parrott’s terms of
people identified “joy” in songs designed to express “love”        “annoyance” and “agitation.” Other words used to describe
or “surprise” than identified the target emotion. “Fear” was       angry songs included “uneasy,” “insistent,” and “grim.” De-
also a commonly selected category. More people identified          scriptions for songs in the “sad” category ranged from “pen-
angry songs as fearful, perhaps due to the sheer amount of         sive” and “antsy” to “deep abiding sorrow.” A few listeners
scary-movie soundtracks in existence. Themes from “Jaws,”          described a possible situation instead of an emotion: “being
“Twilight Zone,” or “Beethoven’s Fifth Symphony” readily           somewhere I should not be” or “watching a dog get hit by a
come to mind as appropriate music to accompany frightening         car.” Fearful songs were described with words such as “ten-
situations; thinking of an iconic song in the “anger” category     sion,” “angst,” and “foreboding.” “Hopelessness” and even
is more of a challenging task. Averaging over all categories,      “homesickness” were also mentioned.
57.67% of respondents correctly identified the target emotion
in computer-generated songs, while only 33.33% of respon-          Table 1: Emotional Content of Computer-Generated Music.
dents did so for the human-generated songs.                        Percentage of survey respondents who identified a given emo-
   For the open-ended questions, responses were evaluated by       tion for selections generated in each of the six categories.
similarity to Parrott’s expanded hierarchy of emotions. Each       Row labels indicate the corpus used to generate a given se-
of the six emotions can be broken down into a number of sec-       lection, and column labels indicate the emotion identified by
ondary emotions, which can in turn be subdivided into tertiary     survey respondents.
emotions. If a word in the subject’s response matched any
form of one of these primary, secondary, or tertiary emotions,                                    is e           sad
it was categorized as the primary emotion of the set. Results                    lov           su        ange       ne     fea
                                                                                    e   joy      rpr         r        ss      r
are reported in Tables 3 and 4. Again, row labels indicate the
corpus used to generate a given selection, and column labels            love     58%    75%     12%       4%      21%       0%
indicate the emotion identified by survey respondents.                   joy     58%    88%     25%       0%       4%       0%
                                                                      surprise    4%    54%     38%       0%      12%       8%
   The target emotion also ranked highest or second highest            anger      4%    04%     46%      50%      17%      88%
in terms of the percentage of survey respondents identifying          sadness     0%     8%     25%      42%      62%      58%
that emotion as present in the computer-generated songs for             fear     17%    21%     29%      12%      67%      50%
the open-ended response surveys. Without being prompted
or limited to specific categories, and with a rather conser-
vative method of classifying subject response, listeners were
still often able to detect the original intended emotion. Once
                                                                      Table 2: Emotional Content of Human-Generated Music.
again, the computer-generated songs appear to be slightly
more emotionally communicative. 21.67% of respondents                                             is e           sad
correctly identified the target emotion in computer-generated                    lov           su        ange       ne     fea
                                                                                    e   joy      rpr         r        ss      r
songs in these open-ended surveys, while only 16.67% of re-
spondents did so for human-generated songs.                             love      50%    0%     25%       25%     100%      0%
                                                                         joy     100%   25%      0%       0%       75%      0%
   Listeners cited “fondness,” “amorousness,” and in one
                                                                      surprise     0%    0%     50%       75%      50%     50%
rather specific case, “unrequited love,” as emotions present
                                                                       anger      25%   25%      0%       25%      50%     50%
in selections from the “love” category. One listener said it
                                                                      sadness     75%   25%     25%       25%      0%      25%
sounded like “I just beat the game.” Another mentioned “talk-
                                                                        fear      50%    0%      0%       0%      100%     50%
ing to Grandpa” as a situation the selection called to mind.
Reported descriptions of selections in the “joy” category most
closely matched Parrott’s terms. These included words such
as “happiness,” “triumph,” “excitement”, and “joviality.” Se-                             Conclusion
lections were also described as “adventurous” and “playful.”       Pearce, Meredith, and Wiggins (Pearce, Meredith, & Wig-
   None of the songs in the category of “surprise” were de-        gins, 2002) suggest that music generation systems concerned
scribed using Parrott’s terms. However, this is not entirely       with the computational modeling of music cognition be eval-
unexpected considering the fact that Parrott lists a single sec-   uated both by their behavior during the composition process
ondary emotion and three tertiary emotions for this category.      and by the music they produce. Our system is able to success-
By comparison, the category of joy has six secondary emo-          fully develop cognitive models and use these models to effec-
tions and 34 tertiary emotions. The general sentiment of           tively generate music. Just as humans listen to and study the
“surprise” still appears to be present in the responses. One       works of previous composers before creating their own com-
listener reported that the selection sounded like an ice cream     positions, our system learns from its exposure to emotion-
truck. Another said it sounded like being literally drunken        labeled musical data. Without being given a set of prepro-
with happiness. “Playfulness,” “childishness,” and “curios-        grammed rules, the system is able to develop internal mod-
                                                               2360

                                                                   Gabrielsson, A., & Lindstrom, E. (2001). The influence
Table 3: Emotional Content of Computer-Generated Music:
                                                                     of musical structure on emotional expression. Music and
Unconstrained Responses.
                                                                     Emotion: Theory and Research, 223-248.
                                is e                               Juslin, P. N. (2001). Communicating emotion in music per-
                                               sad
             lov            su         an         ne                 formance: A review and a theoretical framework. Music
                     joy      rpr        ger        ss   fea
                e                                           r        and Emotion: Theory and Research, 223248.
    love     21%     25%      0%         0%      0%       0%       Kivy, P. (1980). The corded shell: Reflections on musical
     joy      0%     58%      0%         4%      0%       0%         expression. Princeton, NJ: Princeton University Press.
  surprise    0%     12%      0%         8%      0%       0%       Meyer, L. (1956). Emotion and meaning in music. Chicago:
   anger      0%      8%      0%        17%      0%      25%         Chicago University Press.
  sadness     4%      0%      0%         4%     17%      17%       Monteith, K., Martinez, T., & Ventura, D. (2010). Auto-
    fear      0%      8%      0%        12%     17%      17%         matic generation of music for inducing emotive response.
                                                                     Proceedings of the International Conference on Computa-
                                                                     tional Creativity, 140-149.
Table 4: Emotional Content of Human-Generated Music:               Ohman, A. (1988). Preattentive processes in the generation
Unconstrained Responses.                                             of emotions. Cognitive perspectives on emotion and moti-
                                                                     vation, 127-144.
                               is e            sad                 Oliveira, A., & Cardoso, A. (2007). Towards affective-
             lov            su         ange       ne     fea         psychophysiological foundations for music production. Af-
                e    joy      rpr          r        ss      r        fective Computing and Intelligent Interaction, 511522.
    love      0%     25%      0%        0%       0%       0%       Parrott, W. G. (2001). Emotions in social psychology.
     joy      0%     25%      0%        0%       0%       0%         Philadelphia: Psychology Press.
  surprise    0%      0%      0%        0%      25%       0%       Pearce, M. T., Meredith, D., & Wiggins, G. A. (2002). Moti-
   anger      0%      0%      0%        0%      25%       0%         vations and methodologies for automation of the composi-
  sadness     0%      0%      0%        0%      25%       0%         tional process. Musicae Scientiae, 6(2).
    fear      0%      0%      0%       25%      25%      50%       Quinlan, J. R. (1986). Induction of decision trees. Machine
                                                                     Learning, 1(1), 81-106.
                                                                   Quinlan, J. R. (1993). C4.5: Programs for machine learning.
els of musical structure and characteristics that contribute to      San Mateo, CA: Morgan Kaufman.
emotional content. These models are used both to generate          Rabiner, L. R. (1989). A tutorial on hidden markov mod-
musical selections and to evaluate them before they are out-         els and selected applications in speech recognition. Proc.
put to the listener. The quality of these models is evidenced        IEEE, 77(2), 257-285.
by the system’s ability to produce songs with recognizable         Rutherford, J., & Wiggins, G. (2003). An experiment in the
emotional content. Results from both constrained and uncon-          automatic creation of music which has specific emotional
strained surveys demonstrate that the system can accomplish          content. Proceedings of MOSART, Workshop on current
this task as effectively as human composers.                         research directions in Computer Music, 35-40.
                                                                   Scherer, K. R. (1984). On the nature and function of emotion:
                    Acknowledgments                                  A component process approach. Approaches to emotion,
                                                                     293-317.
This work is partially supported by the National Science
                                                                   Scherer, K. R. (1988). On the symbolic functions of vocal
Foundation under Grant No. IIS-0856089.
                                                                     affect expression. Journal of Language and Social Psy-
                                                                     chology, 7, 79-100.
                        References
                                                                   Scherer, K. R. (2001). Emotional effects of music: Pro-
Allan, M., & Williams, C. K. I. (2005). Harmonising chorales         duction rules. Music and Emotion: Theory and Research,
  by probabilistic inference. Advances in Neural Information         223248.
  Processing Systems, 17, 25-32.                                   Sloboda, J. (1985). The musical mind: The cognitive psy-
Chuan, C., & Chew, E. (2007). A hybrid system for automatic          chology of music. Oxford: Oxford University Press.
  generation of style-specific accompaniment. Proceedings          Tolbert, E. (2001). Music and meaning: An evolutionary
  International Joint Workshop on Computational Creativity,          story. Psychology of Music, 24, 103-130.
  57-64.                                                           Wiggins, G. A., Pearce, M. T., & Mullensiefen, D. (2009).
Conklin, D. (2003). Music generation from statistical models.        Computational modelling of music cognition and musical
  Proceedings AISB Symposium on Artificial Intelligence and          creativity. Oxford Handbook of Computer Music and Dig-
  Creativity in the Arts and Sciences, 30-35.                        ital Sound Culture.
Delgado, M., Fajardo, W., & Molina-Solana, M. (2009). In-          Zentner, M., & Kagan, J. (1996). Perception of music by
  mamusys: Intelligent multi-agent music system. Expert              infants. Nature, 383(29).
  Systems with Applications, 36(3-1), 4574-4580.
                                                                2361

