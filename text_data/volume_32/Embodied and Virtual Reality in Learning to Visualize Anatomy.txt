UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Embodied and Virtual Reality in Learning to Visualize Anatomy

Permalink
https://escholarship.org/uc/item/2j52309r

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)

Authors
Jang, Susan
Black, John
Jyung, Robert

Publication Date
2010-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Embodied Cognition and Virtual Reality in Learning to Visualize Anatomy
Susan Jang (sj306@columbia.edu)
John B. Black (black@exchange.tc.columbia.edu)

Department of Human Development, Teachers College, Columbia University, 525 West 120th St., New York, NY 10027

Robert W. Jyung, MD (jyungrw@umdnj.edu)
University of Medicine and Dentistry of New Jersey, DOC, 90 Bergen St., 8100, Newark, NJ 07103
Abstract

programs for medical education now enable users to interact
directly with, as well as view, anatomical parts in threedimensions, with the potential to change the way medical
students learn anatomy, perform dissections and even
practice surgical procedures.
The advent of these programs raises questions for
cognitive psychologists, some of which this study aims to
address. At the broadest level: how are complex, internal
anatomical structures learned through 3-D viewing and
interactivity? What factors, from both a cognitive and
human-computer interaction perspective, contribute to the
learning of anatomy through these VR programs?
This study considers the above-mentioned factors under
the framework of embodied cognition: that cognition is
inextricably linked to our physical interactions with our
environment (Wilson, 2002). Using the embodied cognition
framework, this study explores the following research
questions: 1) Does the physical manipulation of, versus
solely viewing, a complex internal anatomical structure in a
virtual reality program facilitate a better visualization of the
structure? 2) Does spatial ability affect participants’
visualizations in this particular study?

This study examines the facilitative effects of embodiment of
a complex internal anatomical structure through threedimensional (“3-D”) interactivity in a virtual reality (“VR”)
program. Since Shepard and Metzler’s influential 1971 study,
it has been known that 3-D objects (e.g., multiple-armed cube
or external body parts) are visually and motorically embodied
in our minds. Such findings confirm the theory that our
mental images, and rotations of these images, are in fact
confined by the laws of physics and biomechanics, because
we perceive, think and reason in an embodied fashion. With
the advancement of new technologies, virtual reality
programs for medical education now enable users to interact
directly in a 3-D environment with internal anatomical
structures. Given that such structures are not readily viewable
to users and thus not previously susceptible to embodiment,
coupled with the VR environment affording all possible
degrees of rotation, how people learn from these programs
raises new questions. If we embody external anatomical parts
we can see, such as our hands and feet, can we embody
internal anatomical parts we cannot see? Does manipulating
the anatomical part in virtual space facilitate the user’s
embodiment of that structure and therefore the ability to
visualize the structure mentally?
Medical students grouped in yoked-pairs were tasked with
mastering the spatial configuration of an internal anatomical
structure; only one group was allowed to manipulate the
images of this anatomical structure in a 3-D VR environment,
whereas the other group could only view the manipulation.
The manipulation group outperformed the visual group,
suggesting that the interactivity that took place among the
manipulation group promoted visual and motoric
embodiment, which in turn enhanced learning. Moreover,
when accounting for spatial ability, it was found that
manipulation benefits students with low spatial ability more
than students with high spatial ability.
Keywords:
Embodied
Visualization.

cognition;

Virtual

Theoretical Background

reality;

Introduction
Virtual reality programs have the potential to be the most
dramatic change in the way anatomy is taught since
Vesalius’s richly illustrated volumes of the human body
based on careful and intricate cadaver dissections. Although
computer technology has undoubtedly transformed the
manner in which doctors evaluate and treat their patients
(e.g., CT scans, robotic surgery), the methods used to teach
medical students have been in place for centuries (e.g.,
lectures, anatomy textbooks, cadaver dissection). Some
believe this is all about to change. Virtual reality (“VR”)

The theoretical framework underlying and informing the
questions in this study bridges two distinct areas of
cognitive psychology through the lens of embodied
cognition: mental rotation and imagery and multimedia
learning.
Studies in mental rotation and imagery provide some of
the most compelling evidence of how cognition is rooted in
our bodily interactions with the environment. Shepard &
Metzler’s seminal research showed that people mentally
manipulate objects similarly to the way they would with
actual objects in physical space, and that the time it takes to
rotate the image increases linearly with the degree of
rotation (Shepard & Metzler, 1971; Shepard & Cooper,
1982). Subsequent research using the Shepard & Metzler
paradigm has confirmed the proposition that motor
processes are involved in mental rotation (Wexler et al.,
1998) and that motor cortices (primary/M1 or premotor
cortex) are activated when performing the task (Kosslyn et
al., 1998).
Additional research in mental rotation and imagery has
helped to clarify and refine the nature and extent to which
motor processes are connected to mental rotation and

2326

imagery. For example, it is known that there are differences
in the way we conduct mental rotations of an object as
compared with a body part. This difference arises because
the trajectory imagined, for example, for the observer’s
hand or foot is strongly influenced by the biomechanical
constraints specific to the actual movement of the hand and
foot. For example, people are faster and more accurate at
performing mental rotations of drawings of hands or
identifying which hand is pictured when they are asked to
imagine rotating the hand that does not require difficult
bodily movements (Parsons, 1987a, b; Schwoebel et al.,
2001). Given the details of the way the body actually
works, the motor imagery system actively facilitates or
constrains how quickly mental imagery is executed.
Neuropsychological studies have proven that motor
processes are recruited when we imagine and manipulate
complex 3-D structures in our mind but also that the body’s
biomechanical constraints actually affect our ability to
conduct mental rotations (Amorim et al., 2006).
Research in the area of multimedia learning endeavors to
complement the research discussed above in embodied
cognition and mental rotation and imagery by analyzing
how multimedia programs may be designed to maximize
learning and understanding. Recent theories and studies
have focused on how the motor or haptic channel, through
direct tactile manipulation and feedback can aid in deeper
learning and understanding and the degree to which
interactivity of any kind is productive (Meyer & Kieras,
1997; Chan & Black, 2006; Black, in press). For example,
Chan & Black (2006) investigated how seventh graders are
better at visualizing complex concepts such as Newtonian
mechanics if they are able to interact with a technology-rich
environment allowing for direct-manipulation animation
(“DMA”). DMA allowed learners to interact directly with
navigation controls, determine their viewing direction and to
control the pace of the navigation of the content. Chan &
Black found that DMA, which incorporated the haptic
channel in the learning process, provided learners with a
superior learning experience as compared with those who
were in the non-haptic groups (narrative-only, narrative-and
static visuals, narrative and animation) about causal
interactions and functional relations in systems.
Despite the ubiquity of computer programs that exist for
3-D visualizations of anatomy, there are very few empirical
investigations on what makes such programs effective.
These studies have started to investigate, from a humancomputer interaction and cognitive perspective, what factors
contribute to developing successful visualizations of
complex anatomy (or anatomy-like) structures from various
3-D visualization programs. From the corpus of these
studies, the following variables have emerged as being
significant: 1) manipulation (or interactivity) of the 3-D
object versus just viewing, 2) the importance of having
access to certain views and/or orientations of the structure,
and 3) spatial ability of the learner. The most significant
studies were conducted by Garg and his colleagues (Garg et
al., 1999, 2001, 2002) and Keehner and her colleagues

(2008a, b), who concluded that developing accurate
visualizations of an anatomical (or anatomical-like)
structure has more to do with participants’ access the critical
views and orientations of the structure than being able to
interact with it, and furthermore, that such programs should
be used carefully with those with lower spatial ability were
found to have had a harder time learning from such
programs.
Yet, it is curious that the exact opposite findings have
been found in some studies where active exploration
appeared to benefit those participants with low spatial
ability scores over those with higher spatial ability scores.
In a study conducted by Luursema et al. (2006), participants
were divided into groups viewing the same computergenerated 3-D images of anatomical parts of the abdomen,
but with half of the group viewing the images stereoptically
(using shutter-glasses), providing actual depth perception,
and the other half of the group viewing the images
binocularly (without shutter glasses). Luursema et al. found
that a “combination of computer-implemented stereopsis
(visual depth through seeing with both eyes) and dynamic
exploration (being able to continuously change one’s
viewpoint with respect to objects studied in real-time) is
beneficial to anatomical learning” (p. 455), and that
participants with low visuo-spatial ability benefited more
from this combination than participants with high visuospatial ability. In a more recent study, Meijer & van den
Broek (2010) also found that active exploration actually
improved low spatial participants’ 3-D mental
representations of complex 3-D objects (and had no effect
on middle or high spatial participants’ representations).

Research Design and Questions
This study builds from, as well as aims to overcome some
of the potential confounds of the previous studies, in
investigating the effects of interactivity and embodiment in
a VR system when learning a complex, internal anatomical
structure. First, the computer 3-D visualization program
used in this study is more intuitive from a visual and motor
processing standpoint. This VR system provides the user
with stereoscopic vision with 3-D goggles, allowing for full
depth perception of the object of study. In addition, this
system has a joystick that allows the user to interact
physically/motorically with the virtual object in a similar
manner as one would outside of a virtual environment.
Both these elements would, in theory, foster a stronger
sense of embodiment because of the more realistic and
natural aspects of visual and motor information in the VR
system. Therefore, it is possible that if the interface of the
VR program allows for a more intuitive mechanism for
viewing and rotating the virtual object, and is compatible
with the human body’s natural movements, a participant
might be able to develop a better internal 3-D visualization
of a complex anatomical structure.
Second, the stimulus used in this study is an internal
anatomical structure (as opposed to a fictitious structure or
external body part) – the inner ear. In Garg et al.’s studies,

2327

it is possible that the findings were confounded by the
stimulus material – the carpal bones – because it is a part of
the body that people are very familiar with both visually and
motorically. That is, the wrist falls on two natural planes,
and people are used to seeing as well as feeling their wrists
in those two common positions. Therefore, it is not
surprising that there are canonical views of the wrist that
would naturally transfer to canonical views of the carpal
bones within the wrist. Furthermore, using an internal
anatomical structure free of any joint articulation or specific
visual cues to orient the structure allows us to begin to
investigate how (if at all) and which canonical views users
develop of this structure during their study time.
In
essence, it is addressing the issue of whether the user
literally embodies (or maps onto him/herself) the internal
anatomical structure.
With these changes, this study addresses the following
research questions:
1) Does the physical manipulation of, versus solely viewing,
a complex internal anatomical structure in a virtual reality
program facilitate a better visualization of the structure? If
so, is there a difference in visualizing: a) different substructures within the larger structure that have different
shapes, i.e., line (e.g., the path of a nerve) versus circles
(e.g., semi-circular canals protruding off a surface); and b)
the structure from different vantage points (i.e., anatomical
planes)?
2) Does spatial ability affect participants’ visualizations in
this particular study? If so, does it have a different effect
for: a) participants who manipulate versus view the
structure; and/or b) participants with differing spatial
abilities (i.e., low versus high)?

Method
Participants
Seventy-six medical students between the ages of 20-38
years at the University of Medicine and Dentistry of New
Jersey, Newark, participated in this study. None of the
participants had formal instruction of the inner ear or prior
exposure to the VR machine.

Materials
The VR system and target anatomical structure
The VR machine is housed at the University of Medicine
and Dentistry, Newark. It generates a stereoscopic 3-D
environment that is viewed through stereoscopic 3-D
goggles. It has a free-moving, non-mounted joystick,
enabling the user to hold, control and manipulate the
movement (by rotating on an x-, y-, and z- axis) of the 3-D
representation of the anatomical structures in a similar
manner as one would be able to with a tangible object
outside of a virtual environment.
The target anatomical structure is the inner ear. The inner
ear is a structurally complex system concentrated in a very
small area in the human skull. The virtual ear model was
developed by an otolaryngologist at UMDNJ in conjunction

with the engineers of the VR program to ensure accuracy of
the model.
Pre-test measures
Participants took the following pre-tests prior to working on
the VR machine: 1) a background questionnaire which
includes questions on comfort level of using a joystick and
playing video games, as well as any prior use of working
with 3-D modeling programs; 2) an ear anatomy
questionnaire; 3) Vandenberg & Kuse (1978) Mental
Rotation Test (“MRT”). This is a standardized test of
spatial ability that assesses one’s ability to rotate and
visualize a 3-D structure; and 4) Ekstrom et al.’s Building
Memory Test. This standardized test was used assess
participants’ ability to remember the location of an object
within a map.
Post-test measure
A series of snapshots of the virtual ear model were taken in
the following anatomical planes: lateral, superior, inferior,
anterior and posterior. The purpose of using all these
anatomical planes is to create a 3-D “voxel” of the area of
study. For each plane, two snapshots were produced, one
without the facial nerve and another without the semicircular canals. Therefore, the post-test consisted of a total
of 10 snapshots.

Procedure
Each participant was tested individually. First, each
participant completed all four pre-test measures. Next, the
participant was randomly assigned to one of the two
conditions (manipulation, visual).
The manipulation
participant was given a brief training period with the
joystick in the VR machine. Once the participant indicated
that he/she felt comfortable using the joystick, the target
anatomical structure (inner ear model) was presented. After
providing a brief explanation of the inner ear and how it was
positioned in a surgical position, the participant was asked
to study the spatial configuration of two sub-structures
within the inner ear: the facial nerve and the semi-circular
canals. The manipulation participant was informed that
he/she could use the joystick to rotate the ear model, and
was given 5 minutes to study.
Each manipulation
participant’s study of the inner ear, based on his/her own
joystick movements, was recorded in the VR machine,
which was then shown to the yoked, visual participant.
After the study period, each participant was given the 10
post-test snapshots (randomized order by sub-structure) and
asked to draw in, to the best of his/her ability, the missing
sub-structure.

Coding
The drawings were assessed for accuracy of visual
representation on the following criteria: parts, angle and
placement and size. The various individual criterions were
summed to derive the following TOTAL scores: 1) overall
TOTAL; 2) TOTAL for each anatomical plane; 3) TOTAL
for facial nerve; 4) TOTAL for semi-circular canal. The
researcher and an independent coder coded the post-test.

2328

Both coders were blind to the identity of the participants and
condition assignment each coded all 760 drawings.

Analysis and Results
Participants in the manipulation condition scored higher
than those in the visual condition on all the TOTAL scores
(Table 1).
Table 1. Mean score analysis on all dependent measures
Manipulation
Visual
Mean (s.d.)
Mean (s.d.)
t(37), p
TOTAL
72.47 (7.303)
60.76 (11.391)
6.437, <.001
TOTAL by sub-structure
facial
nerve
32.39 (5.900)
26.71 (6.375)
4.870, <.001
semicircular
canals
39.74 (3.020)
33.82 (6.673)
5.029, <.001
TOTAL by anatomical plane
lateral
15.11 (2.051)
13.58 (2.937)
3.153, =.003
superior
13.92 (1.440)
11.63 (2.562)
5.663, <.001
inferior
13.79 (2.183)
10.74 (2.565)
6.481, <.001
anterior
14.18 (2.078)
12.13 (2.622)
4.830, <.001
posterior
15.03 (2.175)
12.45 (2.738)
4.700, <.001

A correlational analysis of all the pre-test measures with
TOTAL score showed that only MRT was correlated (r =
.0325). A one-way analysis of covariance (ANCOVA) was
conducted using MRT as the covariate. A significant
interaction effect was found between MRT and condition on
TOTAL score, F (1, 35) = 5.168, p < .029. Simple group
main effects tests were conducted to assess differences
between those who scored lower on the MRT (1 SD below
the mean = 11.274) and those who scored higher on the
MRT (1 SD above the mean = 27.166) (Figure 1).

what the participants in the manipulation group were doing.
For example, were there certain strategies used by the
manipulation participants that enabled better embodiments
of the inner ear? That is, were there common characteristics
of the manner in which manipulation participants rotated the
structure that led to highly successful performance on the
post-test?
Or, conversely, what were the common
characteristics among manipulation participants who
performed relatively poorly on the post-test?
A qualitative video profile of the top and bottom
performing manipulation participants showed that common
characteristics might exist on either end. First, among the
top performing manipulation participants, they quickly
oriented the structure into the posterior plane, which when
put in context of the human body means positioned in an
upright manner, standing up and looking forward. In
addition to standing the model upright, the top manipulators
often went back to this posterior view after exploring other
views (as though it grounded them in some way), suggesting
this view was the one they were most comfortable with. In
contrast, the lowest scoring manipulators did not position
the structure in an upright position as quickly as those in the
top scoring group. There was no particular familiar or
comfortable perspective that developed among the low
scorers. Second, all the high scoring participants spent more
time studying still positions as opposed to moving the object
continuously. In contrast, the low scoring manipulators
generally spent their time moving and rotating the object in
various, haphazard directions and not holding it still.
Third, and perhaps most interestingly, when the high
scoring manipulators moved between these still positions,
they moved in a “wiggling” manner between these two
planes. There were two kinds of wiggling among the top
scoring manipulation participants. One type was a wiggling
that constituted alternating between two still positions, in
what appeared to be a comparison and analysis of the two
positions. Another strategy demonstrated a different type of
wiggling: choosing one “still” position and varying the view
of that position by only a few degrees in either direction.

Discussion and Conclusions

Figure 1. Differences on TOTAL score performance between the
two conditions on two levels (high versus low) of the covariate
(spatial ability).

Given that the statistical analysis revealed that those in
the manipulation condition had more accurate 3-D
visualizations of the inner ear over those in the visual
condition, some ancillary questions arose with respect to

The main finding of this study is that manipulating, rather
than viewing, an internal anatomical structure in virtual
space strengthens the embodiment of that structure and
therefore the ability to visualize the structure.
As
demonstrated in the analysis (Table 1), the manipulation
group outperformed the visual group regardless of whether
the participants were visualizing different anatomical substructures or from different orientations (i.e., anatomical
planes). Participants who are afforded the opportunity to
manipulate in virtual space 3-D images of anatomical
structures with which they are not familiar outperform
participants who are only given the opportunity to watch the
3-D images being rotated. Such results support the general
framework of embodied cognition, that there is an intimate
connection between our motor and visual processes, and the
more explicit the connection, the better the learning.

2329

Beyond this main finding that the motor and visual
processes are connected and provide stronger learning, it is
posited that the participants may literally have tried to
embody (to varying degrees) the inner ear model by
mapping it onto their own bodies. Results of TOTAL
scores by anatomical plane, combined with the video
analysis, support the theory that the virtual inner ear was
embodied by the participants in this study in a more literal
sense of the term embodiment – that is, that they mapped
the structure onto (or within) their own bodies. The results
from the mean score analysis (Table 1) show that regardless
of condition, participants performed better on the planes we
are more familiar with seeing ourselves and others in
(lateral, posterior and anterior) over less familiar planes
(superior, inferior). As a general matter, we are much more
comfortable and familiar with looking at others face-to-face
rather than looking down a person’s head (superior) or up a
person’s chin (inferior). This conclusion is similar to ones
reached in studies by Parsons and others who have shown
that the real world biomechanical constraints on our
physical bodies do in fact constrain our mental abilities –
specifically the ability to rotate and visualize a body part in
our mind. Therefore, it is possible that participants in both
the manipulation and visual conditions found that
visualizing the ear from the superior and inferior planes was
a somewhat physically awkward perspective to embody, as
it is rare to look into the top of one’s head or look up into
one’s chin. Even though the virtual ear was displayed in the
absence of surrounding physical landmarks that would
immediately cause the viewer to orient the image in an
upright position, there was a way to orient the image (via
embodiment) that made it the anatomical plane more
familiar and more comfortable to the participants.
Further support for the embodiment theory is that the
video analysis revealed that the top manipulators developed
a canonical viewpoint (Palmer, Rosch & Chase, 1981) for
this model. Palmer et al. coined the term canonical
viewpoint to describe perspectives in which identification
performance of 3-D objects is best.
The canonical
viewpoint for the ear model appears to be the posterior
plane. The qualitative video analysis revealed that the top
scoring manipulators started their study with the structure
oriented in the posterior plane and often returned to this
position as though it was the most stable position. Given
that this is their canonical view, it strongly suggests that the
manipulators literally embodied – that is, they mapped onto
themselves the inner ear from the perspective of their own
body schema, or rather that they projected their bodies onto
the object in an embodied fashion, maintaining the body
axes (head-feet, front-back, and left-right) when doing so
(“bodily projection”, Lakoff & Johnson, 1999).
The results from this study also illustrate the facilitative
effects of interactivity on embodiment and that the
development of an internal visual representation of a 3-D
structure depends on the spatial ability of the participant.
Specifically, the benefits of embodiment in virtual reality
appear to be greater for those participants with lower spatial

ability. As shown in Figure 1, those who score lower on
tests of rotational spatial ability have more to gain from
interacting with a 3-D virtual reality environment than those
with high rotational spatial ability. There is a greater
difference between the two regression lines at a low MRT
score versus at a high MRT score, indicating that
manipulation, which strengthens embodiment, may help
those with lower spatial ability to perform as well as those
who have higher spatial abilities (and may not need the
manipulation experience).
It is important to note that the main effect of interactivity
runs counter to some of the more relevant studies discussed
in the literature review (Garg et al., Keehner et al.) who
argue that interactivity, which allows for complete freedom
of movement and exposure to views from varying
perspectives, may overload the learner and prevent effective
visualizations. Why is it then that in this study the
participants in the manipulation condition outperformed the
visual participants? Perhaps the answer is that this virtual
reality program provided a stronger sense of embodiment or
“presence” (Usoh et al., 1999) for the manipulation
participants with the intuitive interface and stereoscopic
depth perception of the target structure. Luursema et al.
(2006) used a similar program and found that the
combination of stereopsis and dynamic exploration to be
beneficial for anatomy learning.
There are some limitations in this study. Regarding the
dependent measure, the drawing test, it could be argued that
assessing the accuracy of visualization by evaluating a
participant’s drawings may favor participants who have
good drawing skills, while disadvantaging participants who
may have successfully understood the visual features of the
anatomical structures but were less skilled at transmitting
their understanding onto several sheets of paper. One way
that a future study might be able to address this limitation is
to complement the drawing test with an interview of the
participant in which the participant would describe his or
her understanding of the anatomical structures and/or
explain what he or she was trying to draw.
Another potential confound that exists in this study relates
to the yoked-pairs design. Although this design is effective
in terms of providing the participants in both conditions
with the same visual information, it may be argued that
certain strategies employed by the manipulation participant
may make no difference, or may in fact actually hinder the
visual participant when studying the model. For example,
the wiggling that was used by many top performing
manipulators may have introduced noise or confusion to the
yoked, visual participant, which in turn made learning less
effective. It is possible that a future study where the visual
participant watches a recording of a high scoring
manipulation participant without the wiggling could lead to
results where learning is equalized.
The findings from this study present significant
implications for the potential role of virtual reality in
educational settings generally, as well as in field of medical
education. Perhaps most significantly, this study suggests

2330

that it is possible to embody internal anatomical structures
that are not generally visible or familiar to people. While it
has been known for some time that we embody wrists and
hands, it has not previously been shown that we may be able
to embody an internal structure we are not even aware of,
such as components of the inner ear. It follows logically
that if it is possible to embody the inner ear with its
substructures (e.g., semi-circular canals and the facial
nerve), perhaps it is also possible to embody the spleen or
the liver or the heart. While further research in this area is
warranted, if is in fact the case that it is possible to embody
other parts of our anatomy, then there may be benefits to
approaching the teaching of anatomy with an understanding
of embodied cognition in mind.

References
Amorim, M., Isableu, B., & Jarraya, M. (2006). Embodied
spatial transformations: “Body Analogy” for the mental
rotation of objects. Journal of Experimental Psychology:
General, 135: 327-347.
Barsalou, L.W. (2008). Grounded cognition. Annual Review
of Psychology, 59, 617-645.
Chan, M.S. and Black, J.B. (2006) Direct-manipulation
animation: Incorporating the haptic channel in the
learning process to support middle school students in
science learning and mental model acquisition.
Proceedings of the International Conference of the
Learning Sciences. Mahwah, NJ: LEA.
Garg, A., Norman, G. R., Spero, L., & Maheshwari, P.
(1999). Do virtual computer models hinder anatomy
learning? Academic Medicine, 74, S87-S89.
Garg, A., Norman, G. R., Spero, L. (2001). How medical
students learn spatial anatomy. The Lancet, 357, 363-364.
Garg, A., Norman, G. R., Eva, K., Spero, L., & Sharan, S.
(2002). Is there any virtue in virtual reality? The minor
role of multiple orientations in learning anatomy from
computers. Academic Medicine, 77, S97-S99.
Keehner, M., Hegarty, M., Cohen, C., Khooshabeh, P., &
Montello, D. R. (2008a). Spatial reasoning with external
visualizations: what matters is what you see, not whether
you interact. Cognitive Science, 32, 1099-1132.
Keehner, M., Khooshabeh, P., & Hegarty, M. (2008b).
Individual differences among users: implications for the
design of 3D medical visualizations. In F. Dong, G.
Ghinea & S. Chen (Eds.), User Centered Design for
Medical Visualization (pp. 1-24). Hershey, PA: IGI
Global.
Kosslyn, S. M., Digirolamo, G. J., Thompson, W. L., &
Alpert, N. M. (1998). Mental rotation of objects versus
hands: neural mechanisms revealed by positron emission
tomography. Psychophysiology, 35, 151-161.
Lakoff, G., & Johnson, M. (1999). Philosophy in the flesh:
The embodied mind and its challenge to Western thought.
Chicago: University of Chicago Press.
Luursema, J., Verwey, W. B., Kommers, P., Geelkerken, R.
H., & Vos, H. J. (2006). Optimising conditions for

computer-assisted anatomical learning. Interacting with
Computers, 18, 1123-1138.
Meijer, F., & van den Broek, E. L. (2010). Representing 3D
virtual objects: interaction between visuo-spatial ability
and type of exploration. Vision Research, 50, 630-635.
Meyer, D. E. & Kieras, D. E. (1997). A computational
theory of executive control processes and human
multiple-task performance: Part 1. Basic Mechanisms.
Psychological Review, 104, 3-65.
Palmer, S., Rosch, E., & Chase, P. (1981). Canonical
perspective and the perception of objects. In J. Long & A.
Baddeley (Eds.), Attention and Performance IX (pp. 135151). Hillsdale, NJ: Erlbaum.
Parsons, L. M. (1987a). Imagined spatial transformations of
one's body. Journal of Experimental Psychology:
General, 116, 172-191.
Parsons, L. M. (1987b). Imagined spatial transformations of
one's hands and feet. Cognitive Psychology, 19, 178-241.
Shepard, R. N., & Metzler, J. (1971). Mental rotation of
three-dimensional objects. Science, 171:701-703.
Shepard, R. N. & Cooper, L. A. (1982). Mental images and
their transformations. Cambridge, Mass.: MIT Press.
Schwoebel, J., Friedman, R., Duda, N. & Coslett, H. B.
(2001). Pain and the body schema evidence for peripheral
effects on mental representations of movement. Brain,
124: 2098-2104.
Usoh, M., Arthur, K., Whitton, M. C., Bastos, R., Steed, A.,
Slater, M., et al. (1999). Walking, walking-in-place,
flying, in virtual environments. Paper presented at the
Proceedings of the 26th annual conference on computer
graphics and interactive techniques.
Wexler, M., Kosslyn, S. M., & Berthoz A. (1998). Motor
processes in mental rotation. Cognition, 68, 77-94.
Wilson, M. (2002). Six views of embodied cognition.
Psychonomic Bulletin & Review, 9(4), 625-636.

2331

