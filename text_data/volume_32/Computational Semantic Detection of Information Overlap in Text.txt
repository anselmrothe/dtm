UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Computational Semantic Detection of Information Overlap in Text
Permalink
https://escholarship.org/uc/item/4q4751qz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Author
Taylor, Julia
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                Computational Semantic Detection of Information Overlap in Text
                                           Julia M. Taylor (jtaylor@riverglassinc.com)
                                                    RiverGlass Inc, 2001 South First St
                                                        Champaign, IL 61820 USA
                               Abstract                                 means identical, even for short sentences when compared to
   This paper is an attempt to investigate whether a computer is
                                                                        very simple pictures. Several things should be noticed: 1.
   capable of finding similar information in structurally different     Length of the sentences in paraphrases has probably some
   texts, as people do it, without relying on lexical matching and      correlation to the length of the original sentences. 2. The
   without guessing the meaning of sentences based on word co-          choice of words for the description task is not limited by the
   occurrence. Considered texts describe the same event, but            original sentence, whereas it is possible that, in the
   each text may focus on different parts of the event. The             paraphrase, the subjects would try to come up with
   considered texts are not paraphrases, but rather human-              unnatural synonyms in their desire to paraphrase. 3. The
   produced descriptions of a simple picture. The goal is not to
   find similar words in texts, which can be easily done, but to        order of sentences is free in the picture description, while it
   meaningfully connect the overlapping concepts and                    is possible that the sentences would be ordered according to
   relationships used in the text descriptions. The meaning-            the original text in the paraphrase.
   based approach does not use any statistical/machine-learning            While paraphrase detections have received some attention
   techniques. The performance of a machine in finding                  from the machine-learning community (Fernando &
   similarity is compared to human performance not just in              Stevenson 2008, Clough et al 2002, Qiu et al 2006, Zhang &
   numbers but in the found information. The results show that
                                                                        Patrick 2005), to the best of our knowledge same picture
   the machine matches four out of the five human findings.
                                                                        descriptions have not been addressed. This is surprising
   Keywords: text duplication and similarity, information               because most real life event descriptions are more similar to
   overlap detection, meaning processing, ontological semantics.        picture descriptions than to paraphrasing tasks.
                                                                           The task of paraphrase limits information that is available
                             Overview                                   to the subjects to that in the task, while describing the
   This paper examines the use of the Ontological Semantic              picture provides more freedom of focus. For example, the
Technology (OST)—a modified version (Raskin et al 2010)                 sentence a black ball is on top of a green cube, can only be
of Ontological Semantics (Nirenburg & Raskin 2004)—for                  paraphrased in term of the provided information. Possible
processing similar texts and compares it to human                       paraphrases are: a green cube is under a black ball; a black
processing. Instead of selecting existing texts and assessing           sphere-shaped object is above a green cube; a ball is
their similarity, users were given the same picture to                  positioned on top of a cube, the ball is black and the cube is
describe. Clearly, the users will emphasize different objects           green. Notice that there may be a considerable variation
or events on the picture, but at the same time, because they            among paraphrases in terms on the words used, the order in
are all looking at the same picture, some of the provided               which they are described, and the number of clauses used in
information will overlap. The experiment is done to                     the description. What they all have in common, however, is
demonstrate the ability of the technology to understand the             the properties and attributes that connect the described
meaning of text, regardless of individual words that are used           objects: all describe shape either explicitly as in sphere-
and of the length of the sentences.                                     shaped object, or by accessing the knowledge of a shape of
   The OST claim to fame is that it “understands” the                   a lexical item as in ball or cube; and all describe color.
meaning of text. The meaning of text includes paraphrases               However, if picture is shown (Figure 1), other things may
of sentences or paragraphs. A large number of paraphrases               come into focus for different people, such as relative size of
can be produced from a single sentence, an even larger                  the objects.
number can be produced from a paragraph. Because of this
large number of potential paraphrases, and because it is
unclear which ones are good enough, instead of asking
people to paraphrase a text, we ask them to describe a
picture.                                                                           Figure 1: A black ball on top of green cube
   The untested assumption is that looking at the picture
should activate the same schema(ta) as reading a paragraph.                It would be interesting to see if such unmentioned-in-the
Thus, the main information received should be                           text characteristics would ever be brought up by the subjects
approximately the same whether looking at the picture or                in the paraphrase generation as unknown. It is, however, not
reading text. Instead of reconstructing original sentences              the purpose of this experiment. The only significant
after reading or listening to a text, the subjects were asked to        assumption for this paper is that the greater variation of text
describe what they see on the picture in their own words.               should be encountered in the picture experiment, which in
The tasks of paraphrase and describing a picture are by no              turn tests the machine’s capability of catching the overlap to
                                                                    2170

a much greater extent. On the other hand, it would be                             (property (facet(property-filler+))+)+
interesting to see if a coherent description of a situation                 property-filler
could be constructed from a union of all descriptions, as it is                      concept-name | literal value
likely that these descriptions, to some extent, complement                  property
each other.                                                                       attribute | relation
   It is this overlap information in descriptions reported by               facet
                                                                                                                               2
subjects, as well as the difference or the union, that is                         SEM | VALUE | DEFAULT | RELAXABLE-TO
captured and analyzed by the machine, as compared to the
                                                                          The current implementation of OST uses the following
overlap and difference in information in responses as
                                                                     three axioms:
perceived by human is the subject of the paper. The
theoretical knowledge obtained in this kind of research is                 subClassOf for concepts: IS-A (example: PHYSICAL-
applicable to an increasingly urgent task of easing the                     OBJECT IS-A OBJECT)
information overload by removing duplicate and                             subPropertyOf for properties: IS-A (example: COLOR IS-
overlapping information1.                                                   A PHYSICAL-OBJECT-ATTRIBUTE)
                                                                           inverse for properties: INVERSE (example: THEME
          Ontological Semantics Technology                                  INVERSE THEME-OF)
OST is an upgraded, much improved and implemented (and,                 Concept interpretation (without facets, for the ease of
on occasion, perverted) version of (Nirenburg & Raskin               reading) can be looked at using the following: given a set of
2004) that detects the meaning of text. Ontological                  objects D, where D is the disjoint union of Dc (concepts)
Semantics is a theory, methodology and technology for
                                                                     and Dd (literals), and given its interpretation function I, for
representing natural language meaning, for automatic
transposition of text into the formatted text-meaning                every atomic concept B, I[B]⊆Dc; for every literal V,
representation (TMR), and for further manipulation of                I[V]⊆Dd ; for every relation R, I[R]⊆Dc x Dc; for every
TMRs for inferencing and more advanced reasoning, both               attribute A; I[A]⊆Dc x Dd. Moreover, the following is true
theoretically and in a growing variety of applications. The          for concepts C and D:
main knowledge resources in OST are the language-
independent ontology and language-specific lexicons.                    I [ALL] = D
   The OST is not a toy system that works on a handful of               I [ε] = Ø
examples; instead, it works with unrestricted texts in real-            I [C D] = I [C]           I [D]
life applications, as well as avoiding the scalability                  I [and C D] = I [C]            I [D]
problems (see Raskin et al 2010).
                                                                        I [(Rel(D)))]= {x∈Dc| y              I [D], <x, y>∈I [Rel]}
Ontology                                                                I [(Rel(and C D))] = I [Rel(C)]             I [Rel(D)]
   The ontology contains information about the world; it is a           I [Rel(C D)] = I [Rel(C)]             I [Rel(D)]
constructed, engineered model of reality, a theory of the               I [C(Rel(D))] = I [C]             I [Rel(D)]
world (Gruber 1993, 1995; Nirenburg & Raskin 2004:138-                  I [(Att(V)))]= {x∈Dc| y              I [V], <x, y>∈I [Att]}
139). It is a structured system of concepts covering the
processes, objects, and properties in all of their pertinent            Clearly, concept C is a descendant of D if I[C]⊆I[D]; and
complex relations, to the grain size determined by an                I[(C(R(D))]⊆I[C]. Whenever relation Rel is defined with a
application or by considerations of computational                    domain D and range R, if I[C]⊆I[D] and I[E]⊆I[R], then
complexity. The ontology contains PROPERTIES, EVENTS,                C(Rel(E)) is equivalent to I [C] I [D(Rel(and E R))].
and OBJECTS. The concepts are named purely for the                      For the examples in this paper, it is sufficient to mention
convenience of a human: the label itself does not contribute         that when facets are involved, the highest priority facet
to the information content. Every OBJECT and EVENT is                takes precedence over the lower priority one.
defined with a number of properties, thus allowing the
concept to differ not only in label, but also in machine-            Lexicon
understandable information. The child concepts inherit
properties from the parent concepts.                                    The lexicon is the starting point for machine interpretation
   Formally, the OST ontology is a lattice of conceptual             of language in OST. Since Ontological Semantics is
nodes (for a construction of ontology and verification see           centered on meaning, we will largely concentrate on the
Hempelmann et al. 2010 and Taylor et al 2010 respectably),           semantic structure (sem-struc) part of the lexicon entries.
each of which is represented as:                                        In general, the lexicon can be looked at as a collection of
                                                                     words (and phrasals), organized such that each word is
      concept-name
   1                                                                    2
      The author believes that whether an overlap indicates an            The list shown has been enriched in the current implementation
importance of information in text is a separate (to her, dubious)    of OST, but since facets do not contribute much to this paper, the
hypothesis, which will not be addressed in this paper.               list is left as it was first introduced.
                                                                 2171

listed with all of its senses. Each sense of the word in a                       (beneficiary(value(^$var2)))
lexicon follows the following structure:                                         (intentionality(value(<0.3))(relaxable-to(<0.5)))
                                                                             )
   (WS-PosNo
                                                                         )
   (cat(Pos))
                                                                    )
   (synonyms “WS-PosNo”))
          (anno(def “Str”)(ex “Str”)(comments “Str”))               The entry shows that this sense of run means ‘unexpected
          (syn-struc((M)(root($var0))(cat(Con))(M))               meeting event’ (from sem-struc), and it needs a preposition
          (sem-struc(Sem))                                        into (from syn-struc) to be activated. It also shows that in its
   )                                                              normalized form the subject is usually the agent of the
                                                                  event, and the direct object is the beneficiary. Optional
where the following grammar defines what is allowed:
                                                                  properties such as time, place, etc are usually not shown in
   M  (Srole((root(Var))(cat(Cpos)))                             the lexical items.
        (Srole((opt(+))(root(Var))(cat(Cpos)))
        (M(M))                                                           OST On Black balls and Green Cubes
   Pos  N |                 (noun)                               OST uses the Semantic Text Analyzer (STAn) to interpret
         V|                 (verb)                               the meaning of sentences. The (machine generated) output
          Adj |             (adjective)                          of STAn is a text meaning representation (TMR) that shows
          Adv |             (adverb)                             the conceptual representation of the text, regardless of the
         …                                                       language of the input. Let us go back to the sentence a black
   Con  NP |                (as defined by rules omitted         ball is on top of a green cube. The resulting TMR is:
          VP |              here to save space)
          Con Con |                                                Event: pred1
          Pos                                                         (theme(value (physical-object1
   SRole  subject |                  (syntactic roles,                  (shape(value(sphere)))
            directobject |           only some are shown                (color(value(black)))
            pp-adjunct               to save space)                     (above(value(physical-object2
           …                                                                (shape(value(cube)))
   No       [1-9]                    (any digit)                             (color(value (green)))
   Str      [A-Z|a-z| |,|.]          (any string)                       )))
   Var  $varNo                                                     )))
            Str                                                  Possible paraphrases from the previous section is: a green
   Sem  C |                          (any ontology concept)      cube is under a black ball:
            ^Var(R(F(C))) |          (R, F, C from ontology)
            C(R(F(^Var)))            (C, R, F from ontology)       pred1
                                                                       (theme(value (physical-object1
   When the machine processes text with the help of the                  (shape(value(cube)))
resources, the ontological concepts are accessed through the             (color(value(green)))
(English) lexicon. For example, a lexical entry for the verb             (below(value(physical-object2
run will contain all the possible senses, of which #6 is                     (shape(value(sphere)))
shown below:                                                                  (color(value (black)))
   (run-v6                                                               )))
       (cat(v))                                                     )))
       (anno                                                      Another interesting paraphrase is: a ball is positioned on top
          (comments "...")                                        of a cube, the ball is black and the cube is green, which will
          (def "meet unexpectedly")                               result in the following:
          (ex "I ran into my teacher at the movies last
night."))                                                           put1
       (syn-struc                                                      (theme(value (physical-object1
          ((subject((root($var1))(cat(np))))                             (shape(value(sphere)))
          (root($var0))(cat(v))                                          (above(value(physical-object2
          (prep((root(into))(cat(prep))))                                    (shape(value(cube)))
          (directobject((root($var2))(cat(np)))))                       )))
       )                                                            )))
       (sem-struc                                                   pred1
          (meet-with                                                    (theme(value (physical-object1
              (agent(value(^$var1(should-be-                                 (shape(value(sphere)))
a(sem(human))))))                                                            (color(value(black)))
                                                              2172

       )))                                                                   therefore will partially differ from person to person.
   pred2                                                                     However, there should be an overlap in descriptions,
       (theme(value (physical-object2                                        focused on that central object, just as the paraphrases
            (shape(value(cube)))                                             showed.
            (color(value(green)))                                           The description of the background will differ from
       )))                                                                   person to person to a much greater degree. A very
                                                                             small overlap is expected from pairs of participants
   Notice that besides the PUT event, corresponding to is
                                                                             since the background is not in focus (metaphorically).
positioned, and the inverse of the BELOW-ABOVE properties,
                                                                            The activated schemata are not expected to be known
the rest of the information is identical for any purposes,
                                                                             to a computer, thus the computer will process only
including reasoning. The third example is especially
                                                                             information explicitly stated by the subjects.
interesting, as the colors are assigned to the indexed objects,
referenced by the previous sentence.                                 This is not at all an attempt to deal with the well-researched
   The intersection of the paraphrases, as indicated by the          figure-ground phenomenon (see Talmy 2000, vol, 1: 311-
TMRs once the inverse properties are used, are:                      344). Instead, we are only interested in the foreground
                                                                     display, but the background may provide individual
    pred1
                                                                     distinctions.
     (theme(value (physical-object1
        (shape(value(sphere)))                                       Methodology
        (color(value(black)))
        (above(value(physical-object2                                Once a picture was chosen, 3 subjects, unfamiliar with an
            (shape(value(cube)))                                     experiment’s goals and from unrelated occupations, were
             (color(value (green)))                                  asked to describe the picture. The picture was visible to the
        )))                                                          subject all the time, thus the description is not effected by
   )))                                                               the accuracy of their recollection of the picture. The
                                                                     instructions requested to describe only what is seen on the
   The union of the TMRs adds information only present in            picture, without alluding to any inferences or encyclopedic
the third example, namely that of PUT, thus, producing               knowledge that the picture may activate. The subjects were
   put1                                                              not given any specific time frame to complete the task.
     (theme(value (physical-object1                                     The described text was then entered into a machine for
        (shape(value(sphere)))                                       processing, and the union and intersection of information in
        (color(value(black)))                                        individual texts were computed. Whenever the descriptions
        (above(value(physical-object2                                contradicted each other, the contradictions were also added
            (shape(value(cube)))                                     to the union as alternative interpretation.
             (color(value (green)))                                     To check the validity of the found union and intersection,
        )))                                                          a person not participating in the description task and not
   )))                                                               involved in the OST part of the experimentation was asked
                                                                     to highlight the similarities in text. These similarities were
   If Figure 1 is described instead of paraphrases, and              then compared to the intersection of interpretations provided
sentences like a ball is smaller than a cube happen to be            by a computer.
added to the description, it is easy to see that the intersection       The foreground of a picture showed a moving elephant.
of TMRs will remain the same, while the union will add the           The background of the picture contained trees, shrubs and
additional size information.                                         other greenery, as well as a place where several cars were
                                                                     parked, as seen in Figure 2.
                  More Complex Pictures
   As demonstrated in the previous sections, OST is capable
of understanding the meaning of close paraphrases and
represent it in such a way that the differences and
similarities are shown. The next experiment aimed at
stretching the similarities as far as possible, but asking the
user to describe a picture instead of paraphrasing a text.
   The picture shown to the user was selected to depict an
unambiguous object in the foreground, while the
background contains objects that can be described either                          Figure 2: An elephant crossing the road
very briefly, if at all, or be paid as much attention as
possible. The hypotheses are:                                        Results of Human Description
         The description of the central element of the picture         The descriptions of the submitted texts varied length ( the
          is affected by individual/personal schemata, and           first text used 54 words, the second text used 124 words,
                                                                 2173

and the third text used 151 words) and structure of                ELEPHANT, ROAD, CAR, TREE.        Additionally, the following
sentences.                                                         descriptions of the concepts were found:
  The following similarities were noticed by a human in all
                                                                     undetermined_event
of the descriptions:
                                                                         (agent(value(elephant1)))
      Elephant’s existence.                                             (location(value(road1)))
      Road on which the elephant is located.                        car1
      Trees in front of the cars, in some spatial relation to           (behind(value(tree1(number(greater-than(1))))))
       the elephant                                                  put2
      Cars parked in the background                                     (instrument(value(car1)))
                                                                         (location(sem(parking-lot)))
  The following information was included in at least one of
the texts (author’s summary below):                                  In plain English, it says that there is an elephant that is
                                                                   doing something on the road, there is a car behind trees, and
      A large African male elephant is shown on the
                                                                   somebody left a car in the parking-lot. Clearly, what is
       picture and is moving either on the road or bare
                                                                   missing here from the overlap found by a human is that
       ground or crossing the road. The elephant has large
                                                                   there are trees in some special relation to the elephant.
       tusks, 4 legs, one visible ear, one visible eye, a tail
                                                                     The union of information was not as successful due to
       and a trunk. The front right leg of the elephant is
                                                                   coreference resolution mistakes (with STAn’s coreference
       bent at the knee.
                                                                   module not yet fully activated), however, the trivial unions
      There is dust on road and some dirt or hard soil on
                                                                   of information were found. The number of unconnected
       the edges of the road. The road is wide and paved.
                                                                   clusters of information was small enough, that based on the
      A row of trees are between the elephant and the cars,
                                                                   concepts connected through the overlap above, it is possible
       past the cars and on the berm. The trees are large
                                                                   to conclude that the three stories described similar
       with extensive but not overwhelming foliage. The
                                                                   information.
       grass is mostly yellowish and dusty.
                                                                     Perhaps it is worthwhile to demonstrate the computational
      Cars, red and light blue or white, are parked on the
                                                                   process in the discovery of the overlap. Consider the
       parking lot. The red car is a hatchback. The cars,
                                                                   following sentences:
       either 4 or 2, are all compact models. All cars are
       parked behind the trees on what may be a parking lot.             (1) A large grey elephant is moving on a road or bare
      A building that has yellow corner is behind the cars.              ground.
      It is a bright sunny day; the sky is blue with light              (2) This is a photograph of an elephant crossing a
       clouds.                                                            road. It is a large male African elephant.
                                                                         (3) Elephant is on asphalted road.
  From this description, it can be noticed that the hypothesis
of the central element of the picture being similarly                The sentences result in the following TMRs:
described between all participants could not be accepted.
                                                                     (1) land-animal-motion1
Interestingly, the descriptions varied in movement
                                                                          (phase(value(continue)))
information—it could be argued that it is not salient to the
                                                                          (agent(value (elephant1)))
central object itself—but not in the elephant’s location on
                                                                             (color(value(grey)))
the road. The description of the elephant and its body parts
                                                                          )))
did not vary as much between any 2 subjects as between all
                                                                          (location(value(road1 ground1)))
of them. It should also be noticed that there was no
                                                                     (2) pred1
contradictory description of the elephant itself. Thus,
                                                                          (theme(value(photograph
perhaps a better metric would be to find overlap used by the
                                                                               (representation-of(value(change-location1
majority of the participants, as opposed to all, for real-world
                                                                                   (agent(value(elephant1)))
applications.
                                                                                   (path(value(road1)))
  The second hypothesis, namely the difference in the
                                                                               )))
background descriptions due to focus on different elements
                                                                          )))
could not be rejected based on this small set. Between the
                                                                     pred2
objects that were noticed by all participants, the description
                                                                          (theme(value(elephant1
varied more than that of the central object, and often the
                                                                               (size(value(large)))
information was contradictory. For example, there was no
                                                                               (gender(value(male)))
agreement on the number of cars in the picture or their
                                                                               (location(pnd(Africa)))
colors and very different description of greenery.
                                                                          )))
Computational Description                                            (3) exist1
                                                                          (agent(value(elephant1)))
  Computational overlap, as expected, was clustered around                (location(value(road1
objects. Thus, the following concepts were identified:                         (made-of(value(asphalt)))
                                                               2174

         )))                                                        extensive experiments should be conducted. However, it is
                                                                    promising that the first result is not negative.
   From the above descriptions, we know the following
about the elephant:
                                                                                      Acknowledgements
   From (1): <land-animal-motion1, elephant1> I [agent]             The author is grateful to Victor Raskin and the anonymous
   From (2): <change-location1, elephant1> I [agent]                reviewers for their comments and to RiverGlass Inc for
   From (3): <exist1, elephant1> I [agent]                          permission to use examples from their proprietary resources.
   Taking the intersection of the events for which the                                       References
elephant is an agent results in x I [event]. Thus,
                                                                    Carroll, D. (2004), Psychology of Language, Thompson
producing undetermined_event(agent(value(elephant1))).
                                                                       Wadsworth, Belmont, California, 2004
   Continuing with each TMR, we find the following:                 Clough, P., Gaizauskas, R., Piao, S. & Wilks, Y. (2002)
   From (1): <land-animal-motion1, road1> I [location]                 METER: MEasuring TExt Reuse. In Proceedings of the
                                                                       40th Anniversary Meeting for the Association for
   From       (1):    <land-animal-motion1,      ground1> I
                                                                       Computational Linguistics (ACL-02), pages 152–159,
[location]                                                             Pennsylvania, PA.
   From (2): <change-location1, road1> I [path]                     Fernando, S. & Stevenson, M. (2008) A semantic approach
   From (3): <exist1, road1> I [location]                              to paraphrase identification. In Proceedings of the 11th
                                                                       Annual Research Colloquium of the UK Special-interest
   It can be easily noticed that ground1 occurs only in (1),           group for Computational Lingusitics, Oxford, England.
thus the intersection with (2) and (3) results in an empty set.     Gruber, T. R. (1993) A translation approach to portable
For road1, the calculation is similar to that of an elephant           ontology specification. Knowledge Acquisition, 5, 199-
with the only addition of parent-child relationship of                 200
location and path.                                                  Gruber, T. R. (1995) Toward principles for the design of
   It should also be noted that if we were to find an overlap          ontologies used for knowledge sharing. In N. Guarino
of (1) and (2) and discarded (3), the event in question would          and R. Poli, (Eds.), Special Issue: The Role of Formal
have a considerably finer grain. According to the ontology,            Ontology in the Information Technology. International
the most specific ancestor of both LAND-ANIMAL-MOTION                  Journal of Human and Computer Studies 43(5-6), 907-
and CHANGE-LOCATION is CHANGE-LOCATION. This means                     928
that while the sentences used different verbs to describe the       Hempelmann, C.F, Taylor, J. M., & Raskin, V. (2010)
movement of the elephant (crossing and moving), the OST                Application-guided       Ontological      Engineering,  In
understands what both mean and finds the general concept               Proceedings of International Conference on Artificial
for both, as opposed to ignoring the similarity in meaning.            Intelligence, Las Vegas, Nevada
   Similar processing is done for all sentences, resulting in       Nirenburg S., & Raskin, V. (2004) Ontological Semantics.
the above relationship for car1 and put2 in addition to                Cambridge, MA: MIT Press
elephant.                                                           Raskin, V., Hempelmann, C. F., & Taylor, J. M. (2010)
   The calculation of overlap is done in a similar manner,              Guessing vs. Knowing: The Two Approaches to
with the exception of the selection rules: each pair of                 Semantics in Natural Language Processing, In
concepts does not have to overlap in the found properties,              Proceeding of Annual International Conference
instead uniquely found relationships are added to the                   Dialogue 2010, Moscow, Russia
existing set.                                                       Qiu, L., Kan, M.Y, & Chua, T. S. (2006) Paraphrase
                                                                       recognition via dissimilarity significance classification.
                                                                       In Proceedings of the 2006 Conference on Empirical
                         Conclusion                                    Methods in Natural Language Processing, pages 18–26,
This paper was an attempt to investigate whether a computer            Sydney, Australia, July. Association for Computational
is capable of finding similar information in structurally              Linguistics.
different texts that describe the same event, each focusing         Talmy, L. 2000. Toward a cognitive semantics, vols. 1-2.
on potentially different parts of the event. The goal was not          Cambridge, MA: MIT Press
to find similar words in texts, which can be easily done, but       Taylor, J. M., Hempelmann, C. F., & Raskin, V. (2010) On
to meaningfully connect the overlapping concepts and                   an Automatic Acquisition Toolbox for Ontologies and
relationships used in the text descriptions. The approach is           Lexicons in Ontological Semantics, In Proceedings of
radically different from the machine-learning one. The                 International Conference on Artificial Intelligence, Las
performance of a machine in finding similarity was                     Vegas, Nevada
compared to human performance. The machine matched                  Zhang, Y. & Patrick, J. (2005) Paraphrase identification by
                                                                       text canonicalization. In Proceedings of the Australasian
four out of five human findings.
                                                                       Language Technology Workshop 2005, pages 160–166,
   It is too early to reach a conclusion that it is possible for
                                                                       Sydney, Australia, December.
computers to find overlap and difference between texts
similarly to those that humans find, and, of course, more
                                                                2175

