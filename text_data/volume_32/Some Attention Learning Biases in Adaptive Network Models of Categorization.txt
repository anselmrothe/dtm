UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Some Attention Learning “Biases” in Adaptive Network Models of Categorization
Permalink
https://escholarship.org/uc/item/9145w2v0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Matsuka, Toshihiko
Corter, James E.
Markman, Arther B.
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

  Some Attention Learning “Biases” in Adaptive Network Models of Categorization
                    Toshihiko Matsuka                   James E. Corter                   Arthur B. Markman
                     Chiba University         Teachers College, Columbia University     The University of Texas
                             Abstract                               attention weights. In particular, ALCOVE and RASHNL
   In two simulation studies, we compare the attention              seem to pay more attention to relatively independent
   learning predictions of three well-known adaptive                predictors, while SUSTAIN shows the reverse pattern. The
   network models of category learning: ALCOVE,                     present Simulation 1 seeks to confirm this finding with a
   RASHNL, and SUSTAIN. The simulation studies use                  novel stimulus structure designed for this purpose, while
   novel stimulus structures designed to explore the effects        Simulation 2 investigates an interesting phenomenon
   of predictor diagnosticity and independence, and                 whereby the models sometimes learn to pay attention to a
   differentiate the models regarding their tendencies to           completely nondiagnostic feature. First, we briefly describe
   learn     simple      rules      versus     exemplar-based
   representations for categories.             An interesting       the models.
   phenomenon is described in which the models
   (especially SUSTAIN and RASHNL) learn to attend to               ALCOVE (Kruschke, 1992) is a multi-layer adaptive
   a completely nondiagnostic constant dimension.                   network model of categorization based on the Generalized
   Keywords: category learning; selective attention;                Context Model (Nosofsky, 1986). The first layer of
   simulation.                                                      ALCOVE is a stimulus input layer. Each node in this layer
                                                                    represents the value of the presented stimulus on a single
                        Introduction                                dimension. Importantly, each dimension has an attention
  A key assumption of many computational models of                  strength (αi) associated with it.        Typically, attention
categorization is that category learners do not merely              strengths are initially equal across dimensions. However, the
form associations between instances and categories, but             model learns to reallocate attention as learning proceeds, by
also learn how to allocate attention to each individual             adjusting these weights. The second layer in the network is
stimulus “dimension” (e.g., color). The present paper               the exemplar layer. Each node in this layer corresponds to
focuses on three such adaptive network models of                    an exemplar, described by its position in the
classification learning: the ALCOVE model of Kruschke               multidimensional stimulus space. The activity of the
(1992); RASHNL (Johansen & Kruschke, 1999); and                     exemplar nodes is fed forward to the third layer, the
SUSTAIN (Love & Medin, 1998). These models are                      category layer, whose nodes correspond to the categories
multilayer adaptive network models that accept as input a           being learned. Separate learning rates are assumed for the
stimulus description (in the form of a set of input feature         association weights and attention strengths.
values), and produce as output category membership
predictions that are based on the activation levels of a set        RASHNL (Kruschke and Johansen, 1999) is a modified and
of output nodes that correspond to the possible category            extended version of ALCOVE. The modifications
responses. Over the course of training, these models learn          introduced in RASHNL include: limited attention capacity;
both what dimensions to attend to, and how to correctly             a capability for large and rapid shifts of attention; a
classify all the stimuli in the training set.                       gradually decreasing learning rate; and a parameter for
  These three adaptive network models differ in several key         salience of cues or features. RASHNL’s architecture is
aspects. ALCOVE and RASHNL are exemplar models, in                  similar to that of ALCOVE. However, each dimension has
the sense that each stimulus in the training set is allocated a     a dimensional salience parameter, the values of which are
node in the “hidden” or middle layer of the network. In             prespecified by the experimenter (i.e., not adjusted by
contrast, SUSTAIN can form either exemplar-level or                 learning). The dimensional attention strengths, αi, are
prototype-based representations. Prototypes are handled by          derived functions of separate underlying parameters, termed
using a reduced number of nodes in the hidden layer,                the “gains”, which are adjusted by learning. An additional
corresponding to potential generalizations.           SUSTAIN       parameter P is incorporated, that can be set to vary between
dynamically allocates new prototypes, allowing it to                fixed attention capacity (P = 1) or unlimited attention
possibly use multiple prototype nodes for each category             capacity (P = ∞).
defined by the training feedback.
  Exploring how these models adapt their attention weights          SUSTAIN (Love & Medin, 1998; Love, Medin & Gureckis,
is crucial to understanding their usefulness and validity by        2004), is comprised of two separate adaptive network
relating their learning accuracy predictions more directly to       components, a “supervised” network and an “unsupervised”
learning strategies. In previous studies (e.g., Matsuka &           one. The unsupervised network is a competitive network
Corter, & Markman, 2002; Corter, Matsuka, & Markman,                that clusters stimuli into prototypes. The term ‘prototype’ is
2007), we found that all three models can account for               used broadly, however, because an experimenter-defined
human classification accuracy learning curves, but show             category might be represented by one or many prototypes,
distinct patterns in their “learning curves” for dimensional        and a prototype might represent only a single stimulus. This
                                                                1762

flexibility also gives SUSTAIN the capability to form                Method: Table 1 shows the category structure used in
prototype-plus-exception representations or even exemplar-           Simulation 1. In a typical classification learning task the
level representations. This clustering network is dynamic            classes (A and B) might be diseases, the exemplars patients,
and incremental in its behavior, in the sense that new               and the five “dimensions” might represent five types of test
prototypes and/or exceptions are created when current                results or symptoms (each with two possible values).
prototypes are not predictive.                                       Correlations with the criterion are equal to .6 for
  The “supervised” network is a feedforward network that             Dimensions D1 and D2, to .2 for D3 and D4, and zero for
classifies a stimulus based on similarity between the input          D5. D3 and D4 differ in their configural validities, however:
pattern and the prototypes created by the unsupervised               The variable subset (D1, D2, D3) gives a perfect R-square
network. The activation of node j in the internal layer              (RSQ) of 1.0 when these three dimensions are used in a
depends on several parameters: λi, which represents the              linear model predicting the criterion, while the variable
“tuning” of the receptive field for a given dimension i, the         subset (D1, D2, D4) yields an RSQ of only .77. Addition of
distance between the centroid of prototype unit j and the            the orthogonal variable D5 alone does not increase the RSQ
input node on dimension i, and r, an overall attentional             of the predictor set (D1, D2), which is equal to .60.
parameter that can be adjusted to create tighter or looser             The dimensions also differ in their degree of independence
focus on highly tuned dimensions. The “tuning” (λi)                  from the other predictors. Dimension D3 is correlated .6
parameters in SUSTAIN are the primary determinants of                with D1 and with D2, while D4 is correlated -.2 with each
differences in attention among dimensions. When λi is large,         of these two predictors. D5 has a zero correlation with all
difference between the input and the prototype node on               the other predictors and the criterion. However, the
dimension i are “stretched” or emphasized. At the output             predictors D3-D5 are all comparable in one regard: each one
layer, SUSTAIN allows only the internal-layer unit with the          can be used in conjunction with D1 and D2 to distinguish all
highest post-transformed activation to determine output              category A exemplars from all category B exemplars. Thus,
node activations, leading to “winner-take-all” learning.             the simulation results for this structure should shed light on
                                                                     our hypothesis that this “exemplar separation” measure is
Comparing the Models’ Accounts of Attention Learning                 key to predicting ALCOVE’s and RASHNL’s attention
We are interested in the attention learning behavior of these        allocation behavior, by holding this factor constant across
models. One clear difference between models is that                  the “extra” dimensions D3-D5.
RASHNL was designed with multiple attention learning
iterations on each trial, in order to account for rapid shifts in             Table 1. Stimulus structure used in Simulation 1.
attention that ALCOVE cannot predict. However, other                                Class D1 D2 D3 D4 D5
                                                                                      A      1      1     1     1    1
differences among the models’ assumed attention
                                                                                      A      1      1     1     0    0
mechanisms have unknown implications. For example, it is                              A      1      1     1     0    1
not clear what follows from SUSTAIN’s use of feedback                                 A      1      0     0     1    0
from only the most-activated prototype to update the                                  A      0      1     0     1    1
dimensional tuning parameters. Because of the complexity                              B      1      0     1     0    1
of these multilayer network models and their dynamic                                  B      0      1     1     0    0
nonlinear performance, simulation studies are useful to                               B      0      0     0     1    1
establish the models' actual attention-learning behavior in                           B      0      0     0     1    0
complex learning tasks.                                                               B      0      0     0     0    1
                SIMULATION STUDIES                                     Using the three models, we simulated subjects (N=10,000)
Simulation 1                                                         who were trained for 20 blocks on the stimulus structure
   Our previous findings (e.g., Corter et al., 2008; Matsuka         shown in Table 1. For each individual subject parameter
et al. 2002) suggest that ALCOVE and RASHNL tend to                  values were randomly selected from a uniform distribution
incorporate dimensions that are relatively independent, even         within reasonable limits for each parameter. The main
orthogonal, to the other predictors, compared to SUSTAIN.            results recorded were the final-block attention weights for
As an alternative (but related) hypothesis, it may be that           the five dimensions.
relatively independent predictors are preferred by ALCOVE
and RASHNL because such dimensions often are more                    Results: Although we cannot identify any of the simulated
useful for distinguishing exemplars, especially between              subjects as being descriptively more plausible than others
categories. Simulation 1 explores this hypothesis by                 due to the lack of empirical data for this structure, we can
decoupling predictor diagnosticity (correlation with the             assess the normative success of each simulated subject, by
criterion), predictor independence (inversely related to             calculating their predicted final-block classification
correlation with the other predictors), and “exemplar                accuracy. Table 2 shows the mean final-block attention
separation” (i.e., whether a predictor can be used in                parameters for each model, by dimension. The table shows
conjunction with other strong predictors in order to                 the final weights only for “successful” simulated learners,
distinguish exemplars from different categories).                    those achieving at least 80% correct classification accuracy
                                                                 1763

by the final block. The results do not differ if all simulated       values. Finally, ALCOVE weights D5 higher than D4 and
learners are included, however. All three models give                D4 higher than D3, an ordering that is consistent with the
highest attention weight to the two high-diagnostic                  degree of independence of the three dimensions, while
dimensions D1 and D2. However, they differ widely in how             RASHNL weights D5 over D4 (but weights D3 highest, in
they distribute attention to the three remaining dimensions.         line with its configural validity). This result supports the
In particular, the results for ALCOVE show a surprising              hypothesis that ALCOVE tends to give higher weight to
pattern, with nearly as much attention paid to D4 and D5 as          more independent dimensions, even at the cost of finding a
to the two most diagnostic dimensions and with D3                    non-optimal solution. RASHNL and SUSTAIN both find
weighted least, even though D3 has the highest configural            the “optimal” configuration of dimensions (D1, D2, D3),
validity (RSQ = 1.0) in conjunction with D1 and D2. Thus,            and in fact exhibit the same ordering of weights
this pattern of weights can be said to be non-optimal; it is a       (D1≈D2>D3>D5>D4).               However, given that only
surprising result in that D5 is completely uncorrelated with         SUSTAIN weights D3 nearly as high as the two diagnostic
the criterion. This ordering is consistent with the hypothesis       dimensions, the results are consistent with the hypothesis
that ALCOVE prefers relatively independent predictors, and           that this model “prefers” dimensions that are correlated with
cannot be ascribed to differences in “exemplar separability”,        other important predictors, compared to the other models.
because this latter factor is held constant for D3, D4 and D5.
                                                                     Simulation 2
Table 2. Simulation 1: Final block relative attention weights for
                                                                       Simulation 2 explores two issues. The first is the idea that
dimensions for each model, for “successful” simulated learners,
with number (N) of successful learners out of 10,000 total.          SUSTAIN favors dimensions that are correlated with other
                     N      D1     D2     D3      D4      D5         predictors, at least relative to the other models. The second
                                                                     issue is the tendencies of the models to utilize exemplar
      ALCOVE 8480 .248 .247 .098 .199 .209
       RASHNL 7463 .274 .286 .183 .123 .135                          versus simple rule based strategies when both strategies are
      SUSTAIN 6855 .240 .248 .230 .136 .148                          sufficient for perfect performance.
                                                                       Our previous simulations suggest that ALCOVE and
  RASHNL and SUSTAIN both predict normatively                        RASHNL favor relatively independent predictors of the
satisfactory patterns of attention weights in the sense that         criterion. A form of independence that can arise with a very
they give highest attention to D1 and D2, with D3 third              poor predictor of a criterion is the case of a constant
highest. This set of predictors is the minimal sufficient set        predictor. A constant has a correlation of zero with the
for perfect prediction, therefore these weights may be               other predictors, and also with the criterion (very bad
considered to be the monotonically “optimal” weights.                diagnosticity indeed). We explore whether ALCOVE and
However, both RASHNL and SUSTAIN weight D5 higher                    RASHNL have any attraction to this type or predictor.
than D4. Again this is surprising, since D5 has zero                   There is reason to suspect that SUSTAIN may try to
correlations with the criterion (but also with the other             incorporate such a predictor.           Although a constant
predictors).                                                         dimension has zero correlation with other predictors, it will
                                                                     have maximal within-category consistency for any cluster.
Discussion: In this simulation RASHNL and SUSTAIN                    Thus, the inclusion of a constant dimension allows us to
yielded weights that are normatively justifiable by the              unconfound diagnosticity and between-predictor correlation
customary criterion of “configural validity”, by giving              from within-cluster consistency, possible aspects of the type
highest weighting to the three dimensions yielding a perfect         of dimensions found to be attractive to SUSTAIN in
multiple-R in predicting the criterion. However, they still          previous simulations.
gave nontrivial weights to the two remaining dimensions,               Inclusion of a constant dimension simulates important
D4 and D5. In this sense their attention allocation patterns         aspects of experimental stimuli that are usually ignored.
cannot be described as optimal.              Furthermore, most       The stimuli used in studies of category learning typically
simulated learners gave attention to more than one of these          have many perceptually or conceptually salient aspects that
“supplementary” dimensions, showing that the network                 are not coded or discussed by the experimenters, being
models do not always learn minimal sufficient rules.                 treated as irrelevant because they are constant for all stimuli.
  ALCOVE also gave highest weights to D1 and D2, but                 For example, stimuli that are line drawings of bug-like
gave third highest weight to D5, a dimension that has a              creatures may differ in head shape, number of legs, and type
correlation of zero with the criterion and with all the other        of tail, aspects that are coded and manipulated by the
predictors. This pattern seems “irrational’ by the usual             experimenter to define the diagnostic input features to
criterion of configural validity. However, we note that it is        categorization models. But the line drawings all share
reasonable from the standpoint of “exemplar separability”:           certain basic characteristics that are constant across stimuli.
by this measure, the set (D1, D2, D5) is adequate for the            Many models of similarity (e.g., Tversky, 1977; Markman
classification task. ALCOVE also gives non-trivial weights           & Gentner, 1993) assume that common features increase the
to the remaining two dimensions, D3 and D4, again                    similarity (and confusability) of stimuli. Thus, it seems
demonstrating that the network models do not tend to learn           interesting to use a simulation study to investigate what
minimal representations across a broad range of parameter
                                                                 1764

predictions the three network models make for use of such          based learning. 1 ALCOVE was relatively successful at
constant or common-feature information.                            ignoring the constant dimension D2, giving it about 1/4 the
                                                                   weight of the “rule” dimension D1. RASHNL showed a
Method: The category structure used for Simulation 2 is            different pattern of final weights, giving the highest weight
shown in Table 3. There are four exemplars of each                 to the dimension (D1) defining the unidimensional category
category, A and B.         Dimension D1 is a binary-valued         rule, an intermediate level to the constant dimension D2,
variable, with values that are logically necessary-and-            and the least attention to the exemplar-identifying
sufficient to identify each category. Dimension D2 is a            dimensions D3-D5. RASHNL’s capability to emphasize D1,
constant dimension that has values of 1 for all exemplars in       the rule dimension, is consistent with its capability to model
the population, regardless of category membership.                 simple rule-based strategies in other simulations we have
Dimensions D3, D4, and D5 are binary-valued dimensions             conducted. It is somewhat surprising that this model cannot
that together uniquely identify all eight exemplars. Note          learn to ignore the constant dimension D2. SUSTAIN gave
that this structure ensures that each network model not only       the least weight of any model to the “exemplar” dimensions
has a relatively easy categorization strategy available (a         D3-D5, and roughly as much weight as RASHNL to the
unidimensional rule on D1), but can adopt a minimal                perfectly diagnostic D1, but was the worst at ignoring D2,
attentional strategy that enables unique identification of all     the constant dimension, giving it equal weight with D1.
exemplars (attending to D3-D5).                                      Examination of the pattern of attention results across
   Using the three models, we simulated subjects                   different regions of the parameter space for each model
(N=100,000) who were trained for 20 blocks on the stimulus         revealed that one key parameter affecting the results is the
structure shown in Table 3. As in Simulation 1, for each           learning rate for association weights in the network. In order
individual subject parameter values were randomly selected         to display these results, we have created plots of the final
from a uniform distribution within reasonable limits for           pattern of attention weights for each model, separately for
each parameter. The main results recorded were the final-          different ranges of the learning rate parameter.
block attention weights for the five dimensions.                     Figure 1 presents the results for ALCOVE. The left panel
                                                                   plots the final attention weight for D2 (the constant
Table 3. Simulation 2: A simple two-category structure with one    dimension) versus that for D1 (the rule dimension). It can
necessary-and-sufficient “category” dimension (D1), a constant     be seen that ALCOVE does not completely ignore D2 at any
dimension (D2), and three dimensions (D3-D5) that uniquely         value of the learning rate, although D2 is consistently given
identify exemplars.                                                lower weight than D1. The right panel plots the summed
                Class D1 D2 D3 D4 D5
                                                                   final attention weights for D3-D5, the “exemplar”
                 A       1    1     1    1     0
                                                                   dimensions, versus the weight for D1. These plots show a
                 A       1    1     0    1     1
                 A       1    1     1    0     1                   strong and consistent effect of the learning rate for
                 A       1    1     0    0     0                   associations. For higher values of this parameter (the upper
                 B       0    1     1    1     1                   row of the panel), the total attention weight given to the
                 B       0    1     0    1     0                   exemplar dimensions tends to exceed that for D1, meaning
                 B       0    1     1    0     0                   that exemplar learning predominates. For lower values of
                 B       0    1     0    0     1                   the learning rate (the bottom row) the dimension defining
                                                                   the unidimensional rule (D1) is weighted highly, sometimes
Results: Table 4 reports the mean pattern of relative              even exclusively, meaning that a rule-based strategy is being
attention in the final block for the successful classification     used.
learners, defined as those who had at least 80%                      Figure 2 presents the corresponding plots for RASHNL.
classification accuracy in the final block.                        The left panel shows that RASHNL has trouble ignoring D2,
                                                                   the constant dimension, at any value of λw. However, D1
Table 4. Mean final relative dimensional attention weights, by
                                                                   (the rule dimension) tends to receive more attention than D2
model, for the best-fitting simulated subjects of Simulation 2.
Maximal mean attention weight for each model shown in bold type.
                                                                   in the majority of solutions. The right panel shows that most
            Model       D1     D2     D3    D4    D5               simulated subjects pay more attention to D1, the rule
           ALCOVE .338 .097 .188 .188 .188                         dimension, than to the exemplar dimensions. This is
           RASHNL .375 .231 .132 .132 .132                         especially true when the learning rate is very low (bottom
          SUSTAIN .389 .389 .074 .075 .075                         row).     However, the bottom row of the left panel
  As can be seen in the table, learners simulated by
ALCOVE gave the highest weight to D1, the dimension                   1
defining the simple rule. However, the total attention                  Support for this interpretation is given by supplementary
weight allocated by ALCOVE to the three dimensions                 simulations we have conducted, in which various numbers
uniquely identifying the exemplars (D3-D5) was greater             of dimensions (1, 2 or 3) are used to uniquely code the
than that given to the rule dimension D1, a pattern that           exemplars. Across all of these simulations, the total final
could be interpreted as showing predominantly exemplar-            weight given to these “exemplar” dimensions is roughly
                                                                   constant, regardless of the number of dimensions involved.
                                                               1765

underscores that for the low learning rates, considerable            Discussion: The results of Simulation 2 are striking. First,
attention is also paid to D2, the constant dimension.                both RASHNL and SUSTAIN pay considerable attention to
  Figure 3 shows that SUSTAIN yields a very different                a constant dimension (that has zero diagnosticity) under a
pattern of results for this structure. For all values of λw          wide range of parameter settings. In fact, RASHNL shows
SUSTAIN predicts that equal attention will be paid to D1             many solutions with relative weight exceeding 50% for D2
(the rule dimension) and D2 (the constant dimension). Also,          (with 5 dimensions). SUSTAIN invariably gives equal
the total amount of attention directed at D3-D5, the                 attention weight to D2 and D1, the unidimensional rule
“exemplar” dimensions, is fairly stable across values of the         dimension. In this sense it is the least successful of the
learning rate, but there is more variability at the higher           three models at ignoring D2. An explanation for this
learning rates. Interestingly, the apparent constraint that the      behavior of SUSTAIN is given below.
weights given to D1 and D2 be equal is so strong that any              Second, the network models also differ in their tendencies
increase or decrease in the total weight given to D3-D5              to adopt the rule-based solution (using dimension D1)
trades off against the summed relative weight given to D1            versus the exemplar-level representation (using D3-D5).
and D2, creating a line of possible solutions with a slope of        For ALCOVE, successful learners tend to give high total
-2 in each plot of the right-hand panel.                             attention weight to the “exemplar” dimensions D3-D5.
                                                                     These exemplar-based attention patterns occur often when
                                                                     the association learning rate is high, but rule-based attention
                                                                     patterns predominate when it is very low (Figure 1). For
                                                                     RASHNL, successful learners tend to weight the simple rule
                                                                     dimension (D1) more than the exemplar dimensions (D3-
                                                                     D5), and this tendency increases for low learning rates. Of
                                                                     the three models, SUSTAIN’s successful learners give the
                                                                     least attention to the exemplar-identifying dimensions D3-
                                                                     D5. SUSTAIN pays somewhat more attention to these
                                                                     exemplar-identifying dimensions when the association
                                                                     learning rate is very low, the opposite pattern to that shown
Figure 1. Simulation 2: Final relative attention weights for         by ALCOVE and RASHNL.
ALCOVE, separately for different values of λw, the learning rate        Surprisingly, SUSTAIN gave the same amount of
for network association weights. Left panel: D2 (y-axis) versus      attention to D2 as to D1. Clearly, this tendency of
D1 (x-axis) attention weights. Right panel: summed attention         SUSTAIN must arise from the structure and processing
weights for D3, D4 & D5 (y-axis) versus D1 (x-axis) attention
                                                                     assumptions of the model.            In fact, the reason that
weights. In each panel, the nine plots summarize results for
various ranges of the λw parameter. Top row: (>.8; .8-.4; .4-.2).    SUSTAIN finds D1 and D2 equally compelling is easy to
Middle row: (.2-.15; .15-.10; .10-.05).       Bottom row: (.05-      identify, and stems from how SUSTAIN utilizes its
.025; .025-.125; <.125).                                             reference points (i.e., clusters or prototypes) in
                                                                     learning. SUSTAIN utilizes only the single most activated
                                                                     cluster to determine an exemplar’s classification and to
                                                                     guide learning. In this model, the update in attention
                                                                     strength for each dimension is inversely proportional to the
                                                                     distance from the most activated cluster’s mean value and
                                                                     the value of the current input stimulus on that dimension
                                                                     (i.e., the smaller the dimensional distance, the more
                                                                     attention is increased for that dimension). For a constant
                                                                     dimension, any cluster and any input stimulus will have zero
                                                                     distance on that dimension, thus attention will be increased
Figure 2.    Simulation 2:   Final relative attention weights for    to the maximal degree possible on the constant dimension.
RASHNL                                                               In the present simulation, D1 is a perfect predictor with
                                                                     constant values within categories, thus any cluster that does
                                                                     not combine exemplars from across categories will also
                                                                     have zero distance on that dimension between the cluster
                                                                     centroid and the input stimulus, leading to an equivalent
                                                                     increase in attention strength to D2.
                                                                       The critical aspect of the processing assumptions here is
                                                                     the winner-take-all nature of the utilization of the clusters,
                                                                     which means that the diagnosticity of a dimension relative
                                                                     to contrasting clusters has less effect. The net result in
                                                                     statistical terms is that the potential increase in attention to a
Figure 3. Simulation 2: Final relative attention weights for         dimension is a function of the similarity of the input
SUSTAIN
                                                                 1766

stimulus and the cluster centroid on that dimension. This         information-board methods (Matsuka & Corter, 2008).
places greater emphasis on within-category similarity and         These hypotheses may then be used to design empirical
less on between-category differentiation, relative to the         studies by suggesting stimulus structures and tasks that best
processing assumptions of ALCOVE and RASHNL. This                 differentiate predictions of the models.
line of analysis suggests that SUSTAIN will tend to select
dimensions whose values have high category validity, P(f|c),                              References
over those with high cue validity, P(c|f), or with the best
information gain (cf. Corter & Gluck, 1992).                      Collins, A. M., & Quillian, M. R. (1969). Retrieval time
  Failing to ignore a dimension with zero diagnosticity             from semantic memory. Journal of Verbal Learning and
seems like a major flaw of the three models, at least from a        Verbal Behavior, 8(2), 240-247.
normative standpoint, because incorporating a constant
dimension in a category’s representation has cost without         Corter, J. E., & Gluck, M. A. (1992). Explaining basic
any obvious adaptive value. However, human data is                  categories: Feature predictability and information.
needed to see if constant dimensions are indeed attended to         Psychological Bulletin, 111(2), 291-303.
and incorporated into a category’s representation. It seems       Corter, J.E., Matsuka, T., & Markman, A. B. (2007).
unlikely that in a category learning experiment human               Attention allocation in learning an XOR classification task.
learners would waste time and effort memorizing or                  Poster presented at the Second European Cognitive
checking properties of a stimulus if those properties were          Science Conference (EuroCogSci 2007), Delphi, Greece,
seen to be useless for the task at hand.                            May 23-27.
  On the other hand, it might be that such constant properties    Kruschke, J. K. (1992). ALCOVE: An exemplar-based
are learned implicitly, whether or not they are useful in a         connectionist model of category learning. Psychological
specific experimental task. An example might indicate why           Review, 99, 22-44.
this is a reasonable possibility. A child learning the            Kruschke, J. K., & Johansen, M. K. (1999). A model of
category animal might notice that all animals have mass. Is         probabilistic category learning. Journal of Experimental
this fact incorporated into the child’s representation? This        Psychology: Learning, Memory, and Cognition, 25, 1083-
certainly seems reasonable, though some normatively                 1119.
motivated theories of mental organization (e.g., Collins and      Love, B. C., & Medin, D. L. (1998). SUSTAIN: A model
Quillian, 1969) hold that the property of having mass should        of human category learning. Proceeding of the Fifteenth
be stored at a superordinate level (say, under the category         National Conference on AI (AAAI-98), 671-676.
object) and merely inferred as needed in order to reason          Love, B. C., Medin, D. L., & Gureckis, T. M. (2004).
about animals and their properties.                                 SUSTAIN: A network model of human category learning.
                                                                    Psychological Review, 111(2), 309-332.
                                                                  Markman, A. B., & Gentner, D. (1993). Splitting the
                        Conclusions                                 differences: A structural alignment view of similarity.
  The present analyses and simulation results show that the         Journal of Memory and Language, 32, 517-535.
models examined here, ALCOVE, RASHNL, and                         Matsuka, T., and Corter, J.E. (2008). Process tracing of
SUSTAIN, incorporate differing attention learning                   attention allocation during category learning. Quarterly
mechanisms and processing assumptions that lead to distinct         Journal of Experimental Psychology, 61(7), 1067-1097.
predictions regarding attention learning in the simulation        Matsuka, T., Corter, J. E., & Markman, A. B. (2002).
studies reported here. The results from Simulation 1                Allocation of attention in neural network models of
supported the hypothesis that SUSTAIN tends to attend to            categorization. Proceedings of the 24th Annual Meeting of
dimensions that are correlated with other predictors, while         the Cognitive Science Society. Hillsdale, NJ: Lawrence
the other models give relatively greater attention to more          Erlbaum.
independent predictors, perhaps because they better support       Nosofsky, R. M. (1986). Attention, similarity and the
exemplar-level processing. Simulation 2 showed that the             identification–categorization relationship.    Journal of
three models differ in their tendencies to use rule-based           Experimental Psychology: General, 115, 39-57.
versus exemplar-based learning strategies.            Another     Rehder, B., & Hoffman, A. B. (2005). Thirty-something
surprising result from Simulation 2 was that all three models       categorization results explained: Selective attention,
incorporated a constant (i.e., completely nondiagnostic)            eyetracking, and models of category learning. Journal of
dimension into their representations to some degree.                Experimental Psychology: Learning, Memory, and
  We believe that simulation studies on attention allocation        Cognition, 31(5), 811-829.
in category learning are valuable for two reasons. First,         Tversky, A. (1977). Features of similarity. Psychological
they help us to better understand the behavior of complex           Review, 84, 327-352.
computational models of category learning. Second, they
can help to guide empirical work on attention by suggesting
new hypotheses about human attention learning, hypotheses
that can be verified using methods for assessing attention
such as eye-tracking (e.g. Rehder & Hoffman, 2005) or
                                                              1767

