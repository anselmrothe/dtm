UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Head and Hand Movements in the Orchestration of Dialogue
Permalink
https://escholarship.org/uc/item/4v80m010
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Battersby, Stuart
Healey, Patrick
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                    Head and Hand Movements in the Orchestration of Dialogue
                                         Stuart A. Battersby (stuart@dcs.qmul.ac.uk)
                                                   Queen Mary, University Of London
                                              Interaction, Media & Communication Group
                                        School Of Electronic Engineering & Computer Science
                                                             London, E1 4NS
                                           Patrick G. T. Healey (ph@dcs.qmul.ac.uk)
                                                   Queen Mary, University Of London
                                              Interaction, Media & Communication Group
                                        School Of Electronic Engineering & Computer Science
                                                             London, E1 4NS
                             Abstract                                 Coates, and Johnson (2002) and Jokinen and Vanhasalo
   Gaze and head orientation are considered to be the most im-        (2009)). However, eye gaze and, by association, head-
   portant non-verbal cues people use to help manage the flow         orientation are normally singled out as the most important
   of conversation. However, if there are more than two partici-      cues to the current orientation of participants in interaction
   pants, gaze and head orientation become problematic. People
   can only look at a single participant at a time. When speak-       (see, for example, Argyle (1975)). Kendon (1990) uses the
   ers concurrently engage with more than one participant, they       term ‘Face Address System’ to make the claim that speakers
   often make use of both head and hand orientation. We show          use their gaze to identify the intended recipient of their ut-
   two contrasts with existing findings. Firstly, people do not
   automatically look where the speaker is looking. Secondly,         terance and Streeck (1993) observed that it is the speaker’s
   we demonstrate that hand movements are more important for          gaze that addressees follow, potentially to the speaker’s ges-
   the interaction than head movements. Specifically, changes in      ture. Langton, Watt, and Bruce (2000) reflect upon the claims
   speaker hand orientation prompt quicker and more frequent re-
   sponses from recipients than changes in head orientation.          about gaze and although they agree on its importance, suggest
   Keywords: Dialogue; Non-verbal interaction; Multi-party;           that gaze cues should be considered along with cues from the
   Gesture; Gaze; Simultaneous engagement;                            head orientation and hands.
                          Introduction
                                                                         Gullberg (2003) provides a quantitative estimate of the rel-
   Consider the following situation: Ann, Bob and Claire are          ative importance of a speaker’s face and hands by measuring
discussing a film that Bob and Claire went to see the previous        the eye-gaze patterns of addressees. Her live condition con-
night. Ann asks “Was it good”? Claire responds by saying              sisted of two people one of which had watched a cartoon.
“I really enjoyed it” while Bob simultaneously pantomimes a           This person then retold the cartoon in narrative form to an
yawn. More than one person’s responses are potentially rele-          addressee who had been configured with eye tracking equip-
vant to the interpretation of the answer. Moreover, the orien-        ment. The gaze patterns of this addressee were recorded.
tation of each participant to those responses is also relevant.       Only 7% of the speaker’s gestures were fixated by the ad-
For example, it matters whether Bob is looking at Ann or              dressee. 96% of the time the addressee looked at the speaker’s
Claire as he pantomimes a yawn and it also matters whether            face; only 0.5% of the time was spent on their gestures with
Claire is aware that he is looking at her when he yawns.              the remaining time spent looking at other objects in the room.
                                                                      Whilst this data points to a marked difference in the relative
   Putting puzzles about mutual-knowledge to one side, this           importance of the head and the hands, the interactional situa-
example highlights the intuition that in multi-party interac-         tion is different to open multi-party conversation.
tions participants often face the challenge of simultaneously
monitoring the responses of several people to each contribu-          Coordinating Multi-Party Interactions
tion (see Goodwin (1979)). People can also design their con-             Although eye gaze is an effective cue to focus of attention
tributions in ways that directly convey how different partic-         in dyadic (two-person) interactions it has more limited value
ipants stand in different relationships to what is said. In a         in multi-party interactions. We can only look at one person at
variation of the example above, Claire might look at Ann and          a time and we can only monitor the gaze of one person at a
say “I really enjoyed it but Bob was bored” while simultane-          time. As Loomis, Kelly, Pusch, Bailenson, and Beall (2008)
ously pointing toward Bob as she says his name (see Healey            have shown, direction of eye gaze is difficult to estimate in
and Battersby (2009), for documented examples of this kind).          the physical arrangements typical of conversation. In small
                                                                      group conversations people are only able to judge another’s
   In the literature on non-verbal communication, a signif-           eye gaze direction with a maximum 4 ◦ retinal eccentricity
icant body of evidence has accumulated that shows ges-                whereas other people’s head orientation can be judged effec-
tures have managerial functions within dialogue (see Bavelas,         tively up to a 90 ◦ retinal eccentricity. This leads to the pre-
                                                                  1998

diction that, in multi-party conversations, auxiliary cues such       Procedure
as head and hand orientation should therefore be much more
                                                                         Each group completed three rounds, based on either three
important to the conduct of the interaction.
                                                                      Java or three Government tasks. On the first round one mem-
                                                                      ber of the triad is randomly assigned to a ‘learner’ role and the
   Healey and Battersby (2009) describe how in three-way              other two participants are assigned ‘instructor’ roles. These
task-oriented dialogues speakers frequently use combinations          roles are then rotated on subsequent rounds so that each par-
of head and hand orientation to enable simultaneous engage-           ticipant is as a learner once and an instructor twice. The in-
ment with two other participants. These moments of simul-             structors are asked to collaborate to teach the leaner the struc-
taneous engagement occurred on average once every 25 sec-             ture described in the task description. The learner is removed
onds. However, it is unclear what the consequences of the             from the group to another room whilst the instructors are
events are for the other participants in the interaction. Specif-     given the descriptions of the task for next round. Once the
ically, do these head and hand movements have any demon-              instructors signal that they understand the task, the descrip-
strable impact on the responses of the other participants?            tions are returned to the experimenter and the learner rejoins
                                                                      the group. All three participants are seated on pre-positioned
                                                                      stools in the AHI lab (see Fig. 1).
   This paper addresses the question of whether changes in
a speaker’s orientation reliably prompt changes in the be-
haviour of the other participants. It also compares the relative
impact of head and hand movements on other participants.
                            Method
Materials
   All data was gathered in the Augmented Human Interac-
tion (AHI) lab at Queen Mary. This lab houses a Vicon optical
motion capture system consisting of an array of 12 infra-red
cameras which track reflective markers attached to the cloth-
ing of participants. Each participant wears an upper body
motion capture suit and a baseball cap with reflective markers        Figure 1: Three participants wearing upper body motion cap-
attached. The motion capture system records the precise 3D            ture suits.
coordinates of each marker at a rate of 60 frames per second
(see Battersby, Lavelle, Healey, and McCabe (2008) for more
details). Video cameras are placed above and to either side
of the participants and are time synchronised with the motion            There was no time limit on the tuition stage and no restric-
capture system. Audio is recorded on the video cameras. Mo-           tions on the interaction other than they were not allowed to
tion capture data from each interaction is time synchronised          use pens or paper. The participants notified the experimenter
with the video data. A custom piece of software reads the mo-         when they finished each round of tuition. To motivate the
tion capture data and integrates it with hand-annotation data         participants to adequately teach and learn the material a post
from ELAN.                                                            completion test (comprising of a drag and drop arrangement
                                                                      of the classes for the computer program, or some multiple
Participants                                                          choice questions for the government material) was given after
                                                                      each round. Tasks were systematically rotated across groups
   Participants were recruited from undergraduate and mas-            and the order of the printed sheets of paper was randomised
ters courses at Queen Mary and either received pay or mod-            before each round.
ules credits and pay for their time. 33 participants (19 female
and 14 male) aged between 18 and 30 took part. Each group             Hand Coding of Target Events
consisted of three people meaning that the data presented are
                                                                         All interactions were recorded on video, with cameras
from 11 triads.
                                                                      above and either side of the group, using synchronised video
                                                                      recording. ELAN was used to hand code these videos. The
Task Description
                                                                      recordings were coded for all instances of simultaneous en-
   Six tuition tasks were developed that consisted of a descrip-      gagement in which a speaker who is making a gesture visibly
tion of either a short Java program or a description of a system      changes the orientation of their hands or head with respect
of Government. They were designed to involve an abstract hi-          to the other participants. For example, by turning their hand
erarchy with no direct visual analogue. All material was text         from one participant to another or changing their head orien-
based with no graphical descriptions.                                 tation. These changes were coded as:
                                                                  1999

• Head Moves: Here the head orientation changes but the
   gesture remains stationary
• Hand Moves: Here the gesture orientation changes, but the
   head orientation remains stationary
• Both Move: Here both the gesture and the head orientation
   shift
Motion Data Analysis
   Taking the hand coded target events for the speaker as                   Figure 3: Indexing head orientation responses
the starting point, the motion capture data was used to pro-
vide quantitative measures of recipients’ responses to target
events.                                                             changes in head orientation we set a criterion level of move-
                                                                    ment of a single frontal head marker in the vertical axis (see
Assigning Recipient Role The motion data was used to                Figure 4 for some sample movement). Only movement with
provide an operational definition of recipient role. Recipi-        a frequency of between 2Hz and 8Hz is used. This removes
ents are either primary or secondary recipients. This role is       some of the effects of gross body sways (below 2Hz) and very
judged by the head orientation of the speaker. We project           minor body shakes or fluctuations in data from the cameras
a vector from the middle of the forehead for each speaker.          (above 8Hz). Movements with an amplitude greater than 5cm
The orientation of this vector is compared to a centre line be-     are removed as these could likely be a result of shifts in posi-
tween the two recipients. The primary recipient is defined as       tion. The resulting signal, which is smoothed using a window
the recipient who is on the same side of the centre line as the     size equivalent to 0.5 seconds, is used to represent periods of
speaker’s current head orientation (see Figure 2).                  head movement that approximate nodding.
                                                                    Figure 4: Raw head movement motion capture data. An area
                                                                    of potential nodding is circled.
     Figure 2: Defining primary and secondary recipients
Indexing Head Orientation Responses It is impossible to                In order to analyse frequency of responses to each simulta-
be sure exactly which head movements correspond to changes          neous engagement event by the speaker we create a 5 second
of orientation by the recipients. Instead, we set a criterion       window after the event and score, for each recipient, whether
for counting movements as changes of orientation based on a         a head re-orientation and whether a head nod occurs in that
vector projected from each recipient’s head as it was for the       window. In order to provide a measure of response latency we
speaker. A change in orientation is thus defined as a shift         record the first change of head orientation or nod that occurs
of head orientation that crosses the centre line between the        after the target event and before another target event occurs.
speaker and the other recipient (see Figure 3).
                                                                    Baseline Response Rates In order to interpret the measures
Indexing Nod Responses A second index of responses,                 of responses to the target events, it is important to know what
‘nods’, was also generated from the motion capture data. As         the baseline likelihood of a recipient nodding or changing
for changes in head orientation it is impossible to be sure         orientation is. To provide this a control comparison sam-
when a head movement really constitutes a nod or is simply a        ple was created by randomly selecting points where some-
shift in position or unintentional motion of some kind. As for      one was speaking but not producing a target event. Recipient
                                                                2000

responses after these control points were then analysed in ex-          Combining the responses of the two recipients together we
actly the same way as for the target events.                         can compare the overall frequency of response to a target co-
                                                                     ordination event with the baseline response rate. For changes
                             Results                                 in head orientation the recipients’ baseline response rate is
   The total time for all the dialogues was 2 hours and 54 min-      41.3% and their response rate to target events is 48.6%; a
utes, each task took on average 5 minutes and 16 seconds. A          small but reliable difference ( χ2 = 5.75, p < 0.05).
total of 287 target, simultaneous engagement events involving           Table 3 illustrates the differences in response rate for each
a change in orientation of the speaker were identified.              type of event.
Inter-rater Agreement
   In order to check the reliability of the hand coding of event     Table 3: Response rates by type of event, measured by recip-
types by the 1st author, a random sample of 25 events taken          ient reorientations
from experimental and control data was independently coded
for event type by a second coder. The inter-rater reliability          Event Class      Response         Baseline             Sig
was good with Kappa = 0.78,(p < 0.001). The number of                                      Rate       Response Rate
each type of target event is show in Table 1.                          Head Moves         43.1%           41.3%               Not
                                                                                                                          Significant
     Table 1: Number of Changes in Speaker Orientation                 Both Move          56.2%           41.3%           χ2 = 10.26,
                                                                                                                           p < 0.01
                      Event Class     Count                            Hand Moves         63.0%           41.3%            χ2 = 8.14,
                      Head Moves        170                                                                                p < 0.01
                      Both Move          86
                      Hand Moves         31
                                                                        For target events in which only the head changes orienta-
                                                                     tion there is no significant increase in response rate (measured
                                                                     by a shift in recipient head orientation) relative to the baseline
Recipient Responses                                                  rate. However, for targets events that involve changes to both
   In analysing responses to changes of speaker orientation          gesture and head orientation we see a significant difference of
we distinguish the task role of the recipients (learner or in-       14.9% between the baseline and the target event. Where only
structor) and their recipient status at the time of the event        the gesture changes orientation there is a 21.7% increase in
(primary or secondary). In addition we code whether each             response rate.
recipient is oriented toward the speaker or the other recipi-
ent at the time the simultaneous engagement event, i.e. the
                                                                        A slightly different pattern is evident in the head nodding
change in speaker orientation, begins. These judgements are
                                                                     response measure. Combining target events, recipients re-
made using the motion capture data.
                                                                     spond 72.4% of the time compared to a background response
Recipient Orientation                                                rate of 66.0% ( χ2 = 5.08, p < 0.05). The breakdown by type
                                                                     is shown in Table 4.
   As Table 2 shows, at the point when the speaker initiates a
change of orientation, the primary recipient is more likely to
be looking at the speaker than the secondary recipient. The          Table 4: Response rates by type of event, measured by recip-
secondary recipient, by contrast, is equally likely to be look-      ient nodding
ing at the other participants ( χ2 = 16.9, p < 0.001)).
                                                                       Event Class       Response        Baseline             Sig
                                                                                           Rate       Response Rate
               Table 2: Initial Head Orientations                      Head Moves         70.0%           66.0%               Not
      Recipient Role            Oriented To    Oriented To                                                                Significant
                                  Speaker         Other                Both Move          73.6%           66.0%               Not
      Primary Recipient            68.0%          32.0%                                                                   Significant
      Secondary Recipient          50.2%          49.8%                Hand Moves         87.0%           66.0%           χ2 = 8.51,
                                                                                                                           p < 0.01
Response Frequency                                                      In order to provide a direct comparison of the recipient’s
In contrast to recipient orientation (and our preliminary find-      relative sensitivity to changes in the speaker’s head and hand
ings (Healey & Battersby, 2009)), there was no difference be-        orientation responses to ‘Head Moves’ events and ‘Hand
tween the response rates of the primary and secondary recip-         Moves’ events can be compared. This shows a significant
ients. Both were equally likely to respond.                          difference between the groups using the values for both head
                                                                 2001

re-orientations and head nods as a measure of response shown           orientation through the dialogue. The primary recipient is
above ( χ2 = 6.43, p < 0.02 and χ2 = 5.75, p < 0.02 respec-            more likely to be looking at the speaker than they are to be
tively).                                                               looking at the secondary recipient before a simultaneous en-
                                                                       gagement event occurs. Secondary recipients do not share
Response Latency                                                       this pattern though, and are equally likely to be looking at ei-
   The time elapsed between a target event until the first re-         ther party. It is interesting that this distinction between roles
sponse (nod or change of head orientation) for each recipi-            is not found when measuring responses, perhaps suggesting
ent was analysed in a Mixed Model linear analysis with Re-             that the target events unify the recipients’ behaviour.
cipients and Task as random factors and ‘Condition’ (Target
Event vs. Baseline) and Task Role (Learner vs Instructor) as
within subjects factors. This showed a reliable main effect               The clear difference between our data and that of previous
of Condition (F(1,1089) = 14.88, p = 0.00) but no main effect          findings is the introduction of the third person. It would be in-
of Task Role (F(1,1088) = 1.29, p = 0.25) and no Task Role ×           tuitive, and logical, to understand the conflicting results with
Condition interaction (F(1,1078) = 0.39, p = 0.53).                    the statement that multi-party dialogue is simply different to
   As Table 5 shows, recipients’ responses to target events            dyadic dialogue. Whilst this is true, there is also the pos-
were approximately 1 second faster than the baseline re-               sibility that multi-party dialogue only allows us to see fully
sponses.                                                               the underlying process that is present in all dialogue; dyadic
                                                                       interaction simply masks them.
   Table 5: Marginal Means for Recipient Response Times                                          Conclusion
     Condition           Marginal Mean       Standard Error               We examined a corpus of multi-party dialogues compris-
                                                                       ing of video and motion capture data. Moments where the
     Target Event          2.4 seconds             0.37
                                                                       speaker simultaneously engaged both recipients were coded
     Baseline Event        3.4 seconds             0.35
                                                                       for. These events were broken down by changes in the
                                                                       speaker’s orientation of their head, their gesture or both and
                                                                       the significance of these changes for the recipients was ex-
                          Discussion                                   amined. These changes in speaker orientation were shown to
   The results show two important contrasts with existing              hold interactional significance. In contrast to existing findings
findings on non-verbal cues and the co-ordination of inter-            in the literature, movements of the hands elicited a higher and
action. First, in the dialogues reported here people do not            faster response rate than movements of the head.
automatically look where the speaker is looking. In fact, in
the cases where the speaker only changes their head orien-                                        References
tation there is no reliable shift in recipient’s head-orientation.     Argyle, M. (1975). Bodily Communication. Bristol: Methuen
The second key finding is that changes of hand orientation are            & Co. Ltd.
significantly more likely to invoke a response from the recip-         Battersby, S. A., Lavelle, M., Healey, P. G. T., & McCabe, R.
ients than changes in head orientation; the opposite of what              (2008, May). Analysing Interaction: A comparison of 2D
would be predicted on the basis of previous work.                         and 3D techniques. In Conference on multimodal corpora.
                                                                          Marrakech.
   The results also show that recipients are demonstrably re-          Bavelas, J. B., Coates, L., & Johnson, T. (2002). Listener
sponsive to the target events, but with a pattern of responses            Responses as a Collaborative Process: The Role of Gaze.
that is different to that typically described in the literature.          Journal of Communication, 52, 566–580.
This provides support for the claim that they are distinctive          Goodwin, C. (1979). The interactive construction of a sen-
and significant interactional events. Although it is difficult to         tence in natural conversation. In G. Psathas (Ed.), Everyday
generalise beyond the particular task we have used, it seems              language: Studies in ethnomethodology (pp. 97–121). Irv-
likely that these moments of simultaneous engagement are a                ington Publishers.
response to the demands of co-ordinating a conversation with           Gullberg, M. (2003). Eye movements and gesture in hu-
multiple participants. As Healey and Battersby (2009) note,               man face-to-face interaction. In J. Hyönä, R. Radach, &
they are also distinguished by relying on physical co-presence            H. Deubel (Eds.), The mind’s eye: Cognitive and applied
in mutually shared space as a specific resource for interaction.          aspects of eye movements (pp. 685–703). Oxford: Else-
For example, they cannot be deployed in point-to-point video              vier.
communication.                                                         Healey, P. G. T., & Battersby, S. A. (2009). The Interactional
                                                                          Geometry of a Three-way Conversation. In Proceedings of
   Our analysis suggests that recipient role (primary or sec-             the 31st annual conference of the cognitive science society
ondary) can manifest itself non-verbally. Whilst hand move-               (pp. 785–790). Amsterdam.
ments are more marked than head movements in initiating re-            Jokinen, K., & Vanhasalo, M. (2009). Stand-up Gestures
cipient responses, we see differing patterns of recipient head            Annotation for Communication Management. In Nodalida
                                                                   2002

  2009 workshop multimodal communication: from human
  behaviour to computational models (pp. 15–20).
Kendon, A. (1990). Conducting Interaction: patterns of be-
  havior in focused encounters. University of Cambridge.
Langton, S. R. H., Watt, R. J., & Bruce, V. (2000). Do
  the eyes have it? Cues to the direction of social attention.
  Trends in Cognitive Sciences, 4(2), 50–59.
Loomis, J. M., Kelly, J. W., Pusch, M., Bailenson, J. N., &
  Beall, A. C. (2008). Psychophysics of perceiving eye and
  head direction with peripheral vision: Implications for the
  dynamics of eye gaze behaviour. Perception, 37, 1443–
  1457.
Streeck, J. (1993). Gesture as Communication I: Its Coor-
  dination with Gaze and Speech. Communication Mono-
  graphs, 60(275-299).
                                                              2003

