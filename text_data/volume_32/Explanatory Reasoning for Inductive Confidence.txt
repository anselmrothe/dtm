UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Explanatory Reasoning for Inductive Confidence
Permalink
https://escholarship.org/uc/item/1wg163fg
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 32(32)
Authors
Landy, David
Hummel, John
Publication Date
2010-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                             Explanatory Reasoning for Inductive Confidence
                                             David Landy (dlandy@richmond.edu)
                                          Department of Psychology, M03B Richmond Hall
                                              University of Richmond, VA, 23173 USA
                                          John E. Hummel (jehummel@illinois.edu)
                                            Department of Psychology, 603 E. Daniel St.
                                                      Champaign, IL 61820 USA
                            Abstract                                 mechanic is better than you are at knowing whether that
   We present Explanatory Reasoning for Inductive Confidence
                                                                     strange noise you car is making is likely to be dangerous.
   (ERIC), a computational model of explanation generation and          In order to apply explanations of past experiences to
   evaluation. ERIC combines analogical hypothesis generation        novel situations, a cognitive architecture must solve several
   and justification with normative probabilistic theory over        problems. First, it must be able to generate and retain
   statement confidences. It successfully captures a broad range     explanations in the first place. Second, it must have a way to
   of empirical phenomena, and represents a promising approach       generate novel hypotheses about a current situation from its
   toward the application of explanatory knowledge in new            beliefs about past circumstances. Finally, it must be able to
   situations.
                                                                     distinguish when a novel explanation is plausible in the
   Keywords: induction; analogy; probabilistic reasoning             current situation, and when it is not.
                                                                        Bayesian models, and particularly hierarchical Bayesian
                        Introduction                                 models, are adept at the last of these goals. For example, the
   We are constantly making guesses. When we come across             model of Kemp and Tennenbaum (2009) carves known
something new, we know about it in part from its relations           situations into disjoint domains, and applies to novel
to other things and we attribute to the novel the properties of      situations the domain assumptions that appear most
the familiar. For instance, when Apple announced the iPad,           appropriate. However, human reasoners also adapt
technology reporters alternately compared it to tablet PCs,          explanation patterns across multiple, dissimilar domains
which are similar in size and function, and the iPhone,              (Medin, Coley, Storms, & Hayes, 2003). Although such
which is similar in appearance and operating system. In              cross-domain reasoning is the sine qua non of analogical
each case, the game was to predict the features of the new           approaches to reasoning (Falkenhainer, Forbus, & Gentner,
object on the basis of the old ones.                                 1989; Hummel & Holyoak, 1997), models of analogy
   Property inductions of this kind—extending known                  generally provide no basis for generating probabilistic
properties of one category to other categories—have been             estimates of confidence in their inferences.
heavily studied in experimental psychology (see Heit, 2000,             In this paper, we present the model ERIC, Explanatory
for a review). Such inductions seem to take advantage of             Reasoning for Inductive Confidence (see also Landy &
taxonomic knowledge about category structures as well as             Hummel, 2009). ERIC uses a combination of analogical and
specific knowledge about particular categories (Shafto,              probabilistic reasoning to (a) generate explanations for
Kemp, Bonawitz, Coley, & Tenenbaum, 2008).                           newly learned facts, (b) evaluate the plausibility of those
   One intuition, pursued here, is that people make                  explanations in light of its existing knowledge, (c) use those
inductions by adapting explanations for known properties to          explanations to update its confidence in its existing
novel categories. People are habitual generators of                  knowledge and (d) make judgments about the plausibility of
explanations: Scientists explain natural phenomena;                  new inferences. The resulting model accounts for a large
engineers explain why structures will or will not support            body of empirical findings from the literature on inductive
various loads; mathematicians explain why a formal                   confidence (e.g., Heit, 2000; Shafto et al., 2008).
property does or does not hold of a particular situation or             A central tenet of the model is that the mind uses analogy
object; and everyone routinely explains much more                    to adapt old explanations to new situations and then uses
mundane things such as why the doorbell rang, why we                 those new explanations both to determine its confidence in
smell gas in the kitchen and why a child has a fever.                the new observation and to update its confidence in its
Explanations serve many cognitive functions, but perhaps             existing knowledge—both existing basic facts and existing
none is more important than their ability to support                 explanations. The knowledge updated includes both the
inductive inferences: A person who can explain a novel               source analogs (i.e., the old explanations used to generate
observation can have much greater confidence in their                the new ones) and the analogies themselves (i.e., the
inferences about the circumstances under which that                  mappings from the old [source] explanations to the new
observation is likely to be repeated than a person who               [target] explanations). As a result, if an analogy results in a
cannot explain it—which is why, for example, your auto               good explanation, then the model becomes more convinced
                                                                     both that the source was true and that the analogy was good.
                                                                 2894

   A second central tenet is that the mind generates these             4.   These explanations are added (provisionally) to
explanations permissively and habitually: Presented with                    knowledge, and the confidence of existing
any new “fact” or observation, the mind will generate as                    statements is updated using Bayesian inference.
many potential explanations of that fact as possible and               5.   Faced with a conclusion, the subject repeats
assign a likelihood or confidence value to each; in turn,                   process of explanation and confidence updating.
these values are used to update its confidence in the very             6.   Confidence in the conclusion is high to the degree
facts that participated in the explanations themselves.                     that the explanations are strong.
   Some models of induction (e.g., Kemp & Tennenbaum,
2009) explicitly carve knowledge into separate domains,              As input, ERIC takes an explanandum—either a premise
and assume that categorically different processes apply to        or a conclusion. As output, it generates potential
situations attributed to those domains (e.g., reasoning in one    explanations, each with an assigned confidence, and an
way about ontological knowledge and in a different way            estimate of the confidence in the explanandum itself.
about geographical knowledge). A third tenet of the model         Applied to property induction, the mechanism operates in
is that knowledge, including knowledge about generating           two stages: First, ERIC explains the premise(s) and any
processes, is applied to relevant situations regardless of        knowledge gleaned from those explanations is added to the
domain. That is, the processes underlying explanation and         knowledge base. Next, it explains the conclusion using that
confidence estimation are the same across and within all          augmented knowledge. The result of these processes is an
areas of knowledge: Any differences between, say,                 estimate of the likelihood that the conclusion is true.
ontological knowledge and other knowledge domains (e.g.,
geographical location, diet or behavioral traits) emerge as a     Knowledge Representation
natural consequence of the relationship between individual        All of ERIC’s knowledge is represented in standard
sets of facts, and not through an explicit and absolute           propositional notation, augmented to capture the logical and
categorization into domain.                                       causal relations that link propositions into explanations.
   Finally, in line with other integrative general knowledge      Atoms are of the form f(a), g(a, b, c), and so on.
models, the goal of ERIC is not to be entirely formally           Connectives ∧, ∨, and ~ are used in their usual sense to
consistent (Wang, 2009). For instance, it will not                mean and, or, and not.
necessarily be the case that a^~a is guaranteed to be false.         Two less universal connectives provide a language for
                                                                  representing explanations and analogical mappings. The
Property Induction                                                connective ⇒ denotes an explanatory or causal relationship.
We report a collection of simulations using ERIC to               For example, q⇒r should be read as “q (if true) would tend
perform a property induction task (Osherson, Smith, Wilkie,       to explain (cause) r.” In contrast to some prior models (e.g.,
López, & Shafir, 1990; Rips, 1975). In this task, a subject       Falkenhainer et al., 1989; Hummel & Holyoak, 1997),
(or ERIC) is given a premise, which is assumed to be true         causal connections are treated as special types, and not as
(e.g., “robins get disease d”), based upon which they are         generic two-place predicates (see also Hummel & Landy,
asked to estimate the likelihood of a conclusion (e.g., “birds    2009). Syntactically, they are equivalent in ERIC to a
get d”). The dependent measure of interest is the estimated       material conditional.
likelihood of the conclusion as a function of the relation           The second novel connective is the mapping relation,
between the major term in the premise (here, “robins”) and        q⇆r, which asserts that q and r map to each other in some
that in the conclusion (“birds”), and of the relation between     analogy, and provides ERIC’s initial estimate that q and r
these categories and the property induced (“disease d”).          might map to each other in some future analogy. Mapping
                                                                  connections have learned confidences.
                             ERIC
                                                                  Confidence Each statement, q, is assigned a confidence
Overview                                                          value between 0 and 1, which is intended to work much like
ERIC is based on the following assumptions about the              an intuitive probability that the statement is true. Indeed, we
nature of the property induction task:                            will refer to the confidence as “the probability of q,” or p(q).
     1. A person enters the laboratory with knowledge                Statements in the initial knowledge set have a preset
          (facts, explanations, theories) believed in with        initial confidence. Regular property statements and cause
          varying degrees of confidence.                          relations (e.g., q⇒r) that do not appear in the initial
     2. Faced with the premise, the subject tries to explain      knowledge have a confidence set to arbitrary low values
          it by building a fairly large set of potential          (0.1 and 0.001).
          explanations by analogy to known cases.
     3. Each explanation is assigned an inductive                 Explanations An explanation is a recursive binary modal
          confidence that combines confidence in the              structure, with the pattern E(explanation; explanandum),
          knowledge involved in the explanation and               where the explanandum is a statement, and the explanation
          confidence in the generating analogies.                 is a set of statements. They have the form of a modus
                                                                  ponens: Some set of (possibly recursively justified) causes
                                                              2895

    and an explanatory connective statement justify the effects.          shares any literals with q is postulated as a possible
    For instance, the explanation:                                        explanation for q. For example if q = g(a) and if f(a) is
                                                                          known, then one explanation postulated will be f(a)⇒g(a).
        E1 ( p,q, E 2 (r,r ⇒ q;q), p ∧ q ⇒ s;s) .                         Confidence in this shallow explanation will initially be set
                                                                          to a very low value. Second, existing explanations
    asserts that “p, q (where q is explained by r), and [p and q          (including those inside explanations) are expanded and
    cause s] jointly cause s.”                                            justified by analogy to other explanations in knowledge.
       An explanation differs from a causal connective in several            Any potentially useful analogical mapping, e.g.,
€                                                                         (a⇒b)⇆(c⇒d), is computed by mapping the elements of a,b
    ways. First, a causal connective is purely dispositional,
    while an explanation asserts that in fact, the explanation            onto those of c,d using Holyoak and Thagard’s (1989)
    explains the explanandum. An explanation thus encodes a               ACME mapping algorithm. ACME’s mapping strengths
    derivation pattern, rather than a potential relationship.             range between 0 and 1, and so translate conveniently into
    Further, an explanation carries its own internal semantics; it        confidences. ACME combines structural isomorphism and
    denotes a possible state of affairs.                                  semantic relationships. In ERIC, these semantic
                                                                          relationships are computed directly from the knowledge
    Knowledge base ERIC’s knowledge consists of three major               base (see Projectable Literals, below).
    classes of statements: simple property statements, such as               The best match produced by ACME is used as the basis
    eats(Robin, Worm); simple explanations, such as generic               for an analogy. This approach has two effects. First, the
    taxonomic explanations of the form isa(A, B) ^ x(B) ⇒                 explanatory relation is justified by the analogical statement,
    x(A); and taxonomic assertions, of the form isa(Robin,                using (3). Second, statements appearing in the analog but
    Bird). It is worth noting here that taxonomic assertions are          not in the current explanation are imported.
    simply property statements, and not a special part of the                These two processes are applied to each explanation in
    model mechanism.                                                      the current set a fixed number of times (three in the current
                                                                          simulations). Each explanation in the final set justifies the
    Justification                                                         conclusion; the result is the confidence in the conclusion.
    ERIC revises its beliefs (e.g., explanations) using two kinds
    of justification: analogical and explanatory. For either, the         Projectable Literals         Analogical similarity integrates
                                                                          structural overlap and semantic relationships (Taylor &
    effect of a justification, j, on an explanandum, i, is to update
    the probability of i according to a probabilistic-OR rule:            Hummel, 2009). That is, structural relations being equal,
                                                                          ERIC prefers analogies about identical or similar terms to
                                                                          comparisons among distantly related items.
                p(s)← p( j) + (1 − p( j)) p(s)                  (1)
                                                                             The semantic similarity—more accurately, projectability
                                                                          (Simmons & Estes, 2008; Sloutsky, Kaminski, & Heckler,
       Intuitively, (1) can be read as meaning that if the                2005)—of a onto b, pab, can come from either of two
    justification, j, is correct, then the assertion, s, it justifies     sources. If two terms have been related by past explanatory
  € must   be correct, but if it is not, then s might still be correct
    with (base rate) probability p(s).
                                                                          analogies, then the projectability is stored in the form of a
                                                                          mapping statement. The projectability of two previously
       The initial confidence of an explanation is simply the
                                                                          unrelated terms is calculated from ERIC’s knowledge:
    probability that all the statements in the explanation are true:                                                             (4)
                                                                                                  p = e −d ab
                                                                                                   ab
                                                                (2)           where
                           p( j) = ∏ e                                               dab = αsa + βsb − γsab − δmab
                                    e∈E
                                                                          α, β, γ €and δ are free parameters (15/40, 2/40, 1/40, and
    Analogical Justification Intuitively, an analogy, r⇆q,
                                                                          17/40, respectively). Here sa is the summed confidence in
    justifies q to the extent that the source analog (r) is true, and
    the mapping
             € is reliable. Thus,                                      € sentences in which a appears; sb and sab are defined
                                                                          analogously. Intuitively, a is projectable onto b to the extent
                                                                          that they appear in similar relational roles in LTM ( γsab ) or
                        p(j) = p(r)p(r⇆q)                       (3)
       The target of an analogical justification is always a causal       to the extent that b is a kind of a ( δmab ) and to the extent
    statement. These are updated by applying the justification            that a does not appear in roles in which b does not and vice-
    to the cause statement via equation (1), just as with                 versa ( αsa + βsb ). If a mapping connection exists
                                                                                                                        €     between a
    explanatory justification.                                            and b then ERIC uses the €mapping strength as pab: ERIC
                                                                          learns that facts about a generally apply to b.
    Explanation Generation                                              €
                                                                             The differential applicability of known explanations to
       When a new explanandum, q, is presented to ERIC, two               novel situations constructs a kind of soft domain separation.
    steps are recursively applied to generate new explanations            Although any knowledge can be applied to a new situation
    of q. First, each fact in the current knowledge base that             in principle, close knowledge will be applied with far more
                                                                      2896

confidence. As a result, cross domain analogies have most           In property induction, ERIC uses the knowledge base that
effect in the absence of other good explanations. This            results from explaining the premise to explain the
differential applicability of old explanation replaces the        conclusion. Since each explanation justifies the conclusion,
construction of explicit domains of explanations (Kemp &          confidence in the conclusion results from the application of
Tenenbaum, 2009) used in other approaches, and in general         (1) once for each explanation.
may implement generic symbolic rules (Gentner & Medina,             In principle, the resulting confidence values could be
1998; Sun, 2006).                                                 matched directly to human probability estimates. In practice,
                                                                  current limitations of the model (especially its extremely
Knowledge Revision                                                impoverished “knowledge”) make such point-by-point
In property induction, a certain number of premises               comparison uninformative, so our evaluation of the model
(collectively π) are followed by a conclusion statement, c.       will focus on the relative rankings of sets of explanations.
In calculating confidence in a conclusion, ERIC first
generates explanations of the premises. It uses these to
update its knowledge base. If π consists of multiple                               Simulations and Results
premises, then each individual premise is explained; the full     ERIC predicts that inductions, and even patterns of
set of explanations is the set of all possible combinations of    inductions, will be strongly dependent on knowledge, and
explanations for individual premises.                             particularly on contextually relevant knowledge. For this
  Learning a new premise means adding it to the knowledge         reason, conclusions about the predictions of ERIC must be
base with confidence=1. Learning a new fact should inform         made relative to some particular set of knowledge.
the learner to the degree that the fact was surprising; it
should increase confidence in things that would explain that      Taxonomic Simulations
fact. Both intuitions can be captured by Bayes law, if we are       Taxonomic relationships have received much attention in
careful about where our terms come from.                          the literature on category inductions; we decided to explore
                                                                  two knowledge bases built largely around taxonomic
                                                         (5)      knowledge. In the first, a taxonomic structure of “animals”
                                                                  was constructed with isa statements, including two
                                                                  mammals, six birds, and two reptiles. Animals were, in turn,
The prior probability, p(π), is the confidence in π resulting     defined by membership to the superordinate “living things.”
from the explanation process. Intuitively, p(π | e) is the        One general taxonomic explanation was included, over
                                                                  elements that did not appear in any other statements. The
confidence we would have in π if some particular fact e
were known with certainty. This value can be found by             pattern of this explanation was: isa(x,y) ^ f(y) ⇒ f(x).
repeating the process of justifying π, setting the confidence       The second knowledge base included all of these
of e to 1 for each fact that appears € in explanations for π,     taxonomic facts, but also included a fairly arbitrary set of
including analogy sources, and assertions of analogical           about 200 facts, including property statements and casual
validity. It should be clear that the use of this law is not      explanations, both taxonomic and not taxonomic. This
normative here, since the values are not strictly                 knowledge base tests the generality of the conclusions
probabilities. However, the law forms one good way to             across a noisier knowledge base.1
incorporate evidence into belief systems. ERIC postulates           Since inductions from a category to its subset are
that people use something like this kind of inference.            explanations, like all explanations, they are not certain.
                                                                  Furthermore, close ancestors generally provide more
                                                                  support than more distant ancestors. Figure 1 compares
                                                                  ERIC’s inductions from immediate superordinates of a
                                                                  category (“parents”), and from the superordinates’
                                                                  superordinates (“grandparents” see Figure 1). Thus, a
                                                                  premise “birds have x” provides more support to the
                                                                  conclusion “robins have x” than does “animals have x”.
                                                                  This pattern matches the empirically discovered category
                                                                  inclusion fallacy (Heit, 2000; Sloman, 1998). Figure 1
                                                                  shows that this same pattern appears with the richer
                                                                  knowledge base, as well.
                                                                    Within taxonomic categories at the same level (e.g., the
                                                                  species level), taxonomic proximity again can vary. Figure 2
                                                                  shows the results of simulations varying the taxonomic
                                                                  proximity, and also the number of premises in the induction
     Figure 1: The strengths of induction of a property from
 one category to a related category. In general, ERIC makes
                                                                    1
  stronger inductions from more closely related categories.           The full contents of all knowledge bases described here can be
                                                                  found online at http://www.richmond.edu/~dlandy/cogsci10/.
                                                              2897

                                                                  features. A second knowledge base had the same exemplars
                                                                  and features, plus explanations for each feature.
                                                                     ERIC computed confidence in the induction of a blank
                                                                  property from each premise bird to the conclusion bird.
                                                                  Figure 3 displays the results. Generally, as with people
                                                                  (Heit, 2000), increased typicality led to higher inductive
                                                                  confidence. One interesting exception to this pattern was
                                                                  that in the features only case, inductions were slightly
                                                                  stronger from the premise category with relatively few
                                                                  features than from the premise category with many typical
                                                                  categories. This is because this “unknown” category was
                                                                  exceptionally projectable, due to having very few features.
                                                                  When more explanations were available, the relatively high
                                                                  number of good potential explanations for the typical
                                                                  category dominated, leading to strong inductions.
                                                                  Causal Knowledge
                                                                     Because ERIC extends its knowledge based on the overall
     Figure 2: The strengths of induction of a property from      analogical quality, the predicate attributed to a premise and
    zero, one or two categories to others at the same level.      conclusion category can also strongly impact induction, if
                                                                  facts involving that premise or a related one are part of prior
(that is, the number of species of which the property was         knowledge. A predicate similar to those that appear as part
asserted). In the absence of knowledge, ERIC generally            of good, projectable explanations about similar categories
predicts that inductions tend to be stronger between              sets will generally form strong inductions; projectable
categories that are closely related (see Figure 2). More          predicates known to apply to very different creatures, or
premises tend to make inductions stronger; moreover, ERIC         those about which little is known, tend to project less well.
shows a general diversity effect: when multiple premises             We illustrated this property by creating knowledge
come from unrelated categories, that tends to increase            corresponding to the taxonomic and predatory structures
inductions more than when they have a common                      explored by Shafto et al (2008). For a set of seven animals,
superordinate. This is true in general because two close          predation and taxonomic facts were encoded in memory.
premises will tend to be best explained by explanations in        Two generic explanations involved a “disease” spread by
terms of their common superordinate, while diverse                predation, and an “organ” shared by animals sharing a
premises are likely to be explained in terms of distant           taxonomic category. Inductions were generated for each
superordinates. This pattern is complicated, however, by an       creature regarding a different “disease” and “bone.”
interaction between the diversity of the premises and their          As illustrated in Figure 4, inductions on the bone graded
similarity to the conclusion. If one premise category is close    taxonomically. Premises involving species with the same
to the conclusion category, a single premise category             parent (distance 0) generalized more strongly than more
already generalizes fairly strongly, because most                 distantly related species. Diseases also showed a taxonomic
explanations for the premise are highly mappable into the         structure, but less strongly than bones did. Furthermore, the
conclusion; adding a second close premise improves the            disease was strongly affected by ecological relationships,
induction very slightly or not at all. However, if the second     generating an asymmetry such that predators were judged
premise is from a very different category (making the             more likely to get diseases carried by their prey than were
premises more diverse), then ERIC’s explanations are likely       prey whose predator was known to catch the disease.
to be less finely tuned to the conclusion category, and
confidence decreases slightly. This pattern again matches
empirical literature (Osherson et al., 1990; Sloman, 1993).
Typicality
To explore how ERIC uses typicality information, we
augmented the taxonomic knowledge base with two kinds of
information. Both involved four members of a common
animal family (“birds”), with four features. The typical
member had the same four features. The typical plus
member had the same four features plus an additional two
not shared by other members. The typical minus had only
two of the features, and no additional features. The final               Figure 3: ERIC’s predictions of induction strength,
atypical member had two shared features, and two unique                   varying the typicality of the premise category.
                                                              2898

                                                                                     Acknowledgments
                                                                 This research was funded by AFOSR grant # FA9550-07-1-
                                                                 0147. Thanks to Eric Taylor, Brian Ross, and Derek
                                                                 Devnich for thoughts and comments during the
                                                                 development of ERIC.
                                                                                          References
                                                                Falkenhainer, B., Forbus, K. D., & Gentner, D. (1989). The
                                                                    structure-mapping engine: algorithm and examples,
                                                                    Artificial Intelligence, 41, 1-63.
                                                                Gentner, D., & Medina, J. (1998). Similarity and the
                                                                    development of rules. Cognition, 65(2-3), 263–297.
                                                                Heit, E. (2000). Properties of inductive reasoning.
                                                                    Psychonomic Bulletin and Review, 7(4), 569–592.
                                                                Holyoak, K. J., & Thagard, P. (1989). Analogical Mapping
                                                                    by Constraint Satisfaction. Cognitive Science, 13, 295-
                                                                    355.
                                                                Hummel, J. E., & Holyoak, K. J. (1997). Distributed
                                                                    representations of structure: A theory of analogical
                                                                    access and mapping. Psychological Review, 104(3),
                                                                    427–466.
                                                                Hummel, J. E., & Landy, D. (2009). From analogy to
                                                                    explanation: Relaxing the 1:1 mapping constraint...Very
     Figure 4: Dependency of inductive strength on both             carefully. In New Frontiers in Analogy Research:
             property and category relationships.                   Proceedings of the Second International Conference on
                                                                    Analogy. Sofia, Bulgaria.
   The latter still formed a strong induction in the disease    Kemp, C., & Tenenbaum, J. B. (2009). Structured Statistical
case, because a prey carrying a disease made a good                 Models of Inductive Reasoning. Psychological Review,
explanation for why a predator would have it; this                  116(1), 20-58.
explanation was thus well-supported during the premise          Landy, D., & Hummel, J. E. (2009). Explanatory reasoning
explanation phase of ERIC’s reasoning process. These                for inductive confidence. In New Frontiers in Analogy
patterns are quite similar to human judgments (Shafto et al.,       Research: Proceedings of the Second International
2008), and demonstrate ERIC’s ability to adjust the                 Conference on Analogy. Sofia, Bulgaria.
application of “rules” to different areas of knowledge.         Medin, D. L., Coley, J. D., Storms, G., & Hayes, B. K.
   Both properties showed taxonomic degradation. This is            (2003). A relevance theory of induction. Psychonomic
because both kinds of knowledge are in the system, and so           Bulletin & Review, 10(3), 517.
both affect, to some degree, the same judgments. The model      Osherson, D. N., Smith, E. E., Wilkie, O., López, A., &
predicts that people will also blend different theories and         Shafir,     E.     (1990).    Category-based     induction.
domains of knowledge when making inductions.                        Psychological Review, 97, 185-200.
                                                                Rips, L. J. (1975). Inductive judgments about natural
                        Conclusions                                 categories. Journal of verbal learning and verbal
                                                                    behavior, 14, 665-681.
   ERIC combines deductive probabilistic inference with
                                                                Shafto, P., Kemp, C., Bonawitz, E. B., Coley, J. D., &
inductive analogical inference to generate and evaluate the
                                                                    Tenenbaum, J. B. (2008). Inductive reasoning about
likelihood of explanations, the propositions they comprise
                                                                    causally transmitted properties. Cognition, 109(2), 175–
and the observations they explain. The resulting model, still
                                                                    192.
in an early stage of development, successfully predicts and
                                                                Sloman, S. A. (1993). Feature-based induction. Cognitive
explains a wide range of phenomena in the property
                                                                    Psychology, 25, 231-280.
induction literature. Much work remains to be done (e.g.,
                                                                Sloman, S. A. (1998). Categorical inference is not a tree: The
representing probabilities more realistically, allowing
                                                                    myth of inheritance hierarchies. Cognitive Psychology,
explanations to decrease as well as increase confidence, and
                                                                    35(1), 1–33.
making the generation of analogical explanations
                                                                Sun, R. (2006). Accounting for a variety of reasoning data
psychologically plausible rather than computationally
                                                                    within a cognitive architecture. Journal of Experimental
exhaustive, among many others), but at this point ERIC
                                                                    and Theoretical Artificial Intelligence, 18(2), 169-191.
seems a promising way to overcome the limitations of
                                                                Wang, P. (2009). Formalization of Evidence: A Comparative
purely analogical, and purely Bayesian approaches to
                                                                    Study. Journal of Artificial General Intelligence, 1, 25–
explanation generation and evaluation.
                                                                    53.
                                                             2899

