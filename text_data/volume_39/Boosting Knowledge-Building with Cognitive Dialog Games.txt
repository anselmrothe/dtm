                       Boosting Knowledge-Building with Cognitive Dialog Games
                                     Jonathan S. Herberg (jonathan.herberg@gmail.com)
                                            Ilker Yengin (yengini@ihpc.a-star.edu.sg)
                                   Praveena Satkunarajah (praveenas@ihpc.a-star.edu.sg)
                                             Margaret Tan (mtan@ihpc.a-star.edu.sg)
                                          Institute of High Performance Computing, A*STAR
                                       1 Fusionopolis Way, #16-16 Connexis, Singapore 138632
                              Abstract                                    argumentation, and more critical thinking (e.g., Carlson,
                                                                          2012; Weigand, 2016).
   Dialog game tools are text chat applications which aim to                 Along these lines, one possibility is that dialog games
   structure and promote students' collaborative learning by              applications may encourage more metacognition.
   having them select a label and sentence-opener for each                Metacognition in this context refers to thinking about
   message they type to their learning partner. In this
   experiment, we compared students’ learning from discussions
                                                                          knowledge states, including insufficient knowledge,
   via a dialog game tool to their learning via a standard freechat       whether one’s own or one’s learning partner. It is a core
   application. Students discussed topic questions with a                 factor for self-regulated learning patterns, which involve
   learning partner. They then individually completed a multiple          targeting one’s misconceptions and effectively integrating
   choice test, for assessing knowledge-gain, and a short-answer          newly learned information with prior knowledge (Azevedo
   test, to assess readiness for knowledge-building. Results              et al., 2009). In a collaborative learning context, in addition
   suggest that dialog games applications lead to increased               to metacognition encouraging self-correction of one’s
   readiness for knowledge-building, in the form of integrating
   distinct pieces of learned knowledge, than freechat                    misconceptions,      it   may      elicit   explanation    and
   applications. Follow-up analyses suggest that the degree of            re-representation of one’s knowledge to one’s learning
   concept overlap between students' dialog messages and topic            partners that in turn may support the construction of more
   keywords, as measured by a "semantic fingerprint" system, is           robust knowledge-representations.
   a potentially useful metric for predicting students'                      In other words, several patterns of behavior encouraged
   knowledge-building. Implications and potential applications            by dialog games applications may align with those that
   of our findings are discussed.
                                                                          promote generative learning. Generative learning is learning
   Keywords: collaborative learning; generative learning;                 which goes beyond mere memorization, involving deeper
   knowledge-building; metacognition; dialog games                        cognitive processing, manipulation, and restructuring of
                                                                          information (e.g., Fiorella & Mayer, 2015). The outcome is
                          Introduction                                    new knowledge that can be applied in novel situations. Self-
                                                                          explaining and re-representing information in order to teach
One technique that aims to enhance collaborative learning                 others are examples of learning strategies which can lead to
activities among students, and to promote their                           generative learning. Experimental evidence supports the
communicative interaction skills, is to employ the dialog                 notion that self-explaining can increase one’s integration of
games        approach.       Dialog-game        applications      are     learned knowledge and inferring of new knowledge (e.g.,
computerized education-tools that structure students’                     Ainsworth & Burcham, 2007).
interactive text chats by having them select the function of                 Tied to the notion of generative learning are the levels of
each dialog act they make. For each dialog act they also                  learning in Bloom’s taxonomy that go beyond remembering
choose a sentence opener “scaffold” from a set of options                 and understanding learning-domain information (Bloom,
available for the dialog act type. Such applications have                 1956). In particular, the “apply” and “analyze” levels
been demonstrated to facilitate construction of structured                involve transferring learned knowledge in order to solve
communication behavioral patterns such as helping,                        problems and infer new knowledge. Related to this notion,
information-seeking, probing, and instructing, between                    in tutor-learner dialogs, tutor behaviors that encourage
online learners (e.g., Ravenscroft, Wegerif, & Hartley,                   knowledge-building, or inference of new knowledge from
2007; Wells, 2014).                                                       existing knowledge, rather than shallow knowledge-telling
  Analyses of learners’ dialog patterns in their use of dialog            behaviors (e.g., when the tutor immediately jumps to correct
games applications suggest several avenues by which they                  a learner’s misconception, rather than eliciting the learner to
potentially may lead to more effective collaborative                      figure out his or her own misconception) entail more
learning. In particular, the structure of communication                   generative learning (Roscoe & Chi, 2007). The analysis of
promoted by dialog games implementations may improve                      tutor-learner dialogs by Chi et al. (2001) indicates that
common understanding of the knowledge perspective of                      certain dialog patterns, namely those which are interactive
one’s dialog partners, more effective and coherent                        in nature, (which means that they contain joint-actions),
                                                                          encourage more generative learning, whereas dialogs that
                                                                      2199

are dominated by the tutor lead to more shallow learning.             language processing system to obtain dialog metrics that
Whereas self-explaining is a constructive learning activity,          effectively predict better knowledge-building from students,
i.e. one that encourages knowledge inference, it is not an            in order to investigate the feasibility of integrating such
interactive constructive activity. According to Chi et al.,           features into dialog-games tools.
(2001) behaviors that are at-once both interactive and
constructive are the core drivers of effective tutor-learner
interactions. That is, the most effective tutor-learner dialogs
                                                                                                   Method
are ones in which new knowledge is jointly constructed for
the learner. In particular, their extensive analysis of tutor-        Participants
learner dialogs suggests distinct interactive patterns that           Participants included in the analyses were 56 9th grade
define effective knowledge-construction. An example of                students across three secondary schools in Singapore.
such a pattern is a tutor providing scaffold prompts (e.g.,           Signed parental consent was obtained for these students to
hints and highlights of relevant information) for the learner         participate at a pre-scheduled school-day time in school
to figure out the solution to a learning-domain question or           classrooms or computer labs that had been made available
problem. Chi et al. (2001) further crafted categories of              for the experiment, with laptops set up at desks in the
questions intended to assess whether a learner has acquired           rooms.1
information laid out in a learning-text (text-explicit
questions), has effectively integrated information from               Materials
different places in the learning text (text-implicit questions),      The learning domain was the human circulatory system that
or has successfully constructed and applied a mental model            was adapted from the Chi et al. (2001) peer-tutor dialog
for the learning domain, not explicitly described in the              study. Each of the 13 subsections was designed on the
learning text (model-implicit questions). Ainsworth et al.            computer screen to describe each topic (e.g., “The Blood
(2007) in their self-explanation learning studies have                Flow in the Heart”). Diagrams were added to facilitate
adopted some of these questions, referring to the latter two          comprehension. Bullet points under the diagrams described
categories as “implicit” and “knowledge-inference”                    the main concepts.
questions. However, we would argue that the integration of               The topic questions that students discussed via text chat
disparate pieces of domain knowledge toward figuring out              are shown in Table 1. Topic 1 corresponds to questions in
the answer to a question, as opposed to arriving at the               the Chi et al. taxonomy which require integration of distinct
answer by mere recall, is itself also a form of                       pieces of explicitly learned knowledge. That is, the two sub-
knowledge-inference, even if it does not involve an implicit          questions for Topic 1 are text-implicit knowledge-building
mental model. Thus, we regard successful answering of both            questions. Topic 2 question, in addition, requires the correct
text-implicit and model-implicit questions as entailing some          mental model of circulation (as a double-loop) to answer the
form of knowledge-building.                                           question effectively. Thus, it is a model-implicit knowledge-
   Our hypothesis for the study was that the patterns of              building question. Topic 3 provided students a general
communicative interaction promoted by a dialog-games                  discussion about the learning-domain concepts.
application would elicit more generative learning among                  There were pretest and posttest multiple choice questions
peer-learners than a free chat application. We developed a            to gauge students’ prior knowledge, and their recall and
dialog-games application and a control free chat application          understanding of the learning material. Also for the posttest,
and designed an experiment to evaluate students’                      students received six knowledge-building questions. They
collaborative learning outcomes. This included evaluating             are shown in Table 2. The first 3 are text-implicit questions,
students’ basic knowledge-gain through their performance              and the latter 3 are model-implicit questions, as developed
on multiple-choice items assessing (text-explicit) recall and         and utilized by Chi et al. (2001) and Ainsworth (2007).
understanding of the learning material. Critically, to test our          There were two conditions of the dialog games text chat
generative learning hypothesis we assessed participants’              tool employed for this experiment. For the Scaffold
readiness     for    knowledge-building,      through       their     condition, the application included dialog act labels for
performance on short-answer items that required them to
either integrate pieces of existing knowledge (recalled from
the learning-material) or to infer the answer by applying an             1
                                                                           Three of the 62 students who initially participated were
accurate implicit mental model based on recalled                      excluded from the analyses due to a technical error (one leading to
learning-material     information.      We    utilized     three      a posttest log not being created, and the other to one of the topics
text-implicit questions for the first category of knowledge-          between a pair not being discussed). There was a procedure error
building questions, and three model-implicit questions for            for two additional participants (i.e. they had kept their learning-
                                                                      material window open and used it for the posttest). Lastly, one
the second kind. Our study thus extends prior research by
                                                                      student withdrew participation assent during the posttest.         In
investigating whether specifically the scaffold functions of          addition, of the remaining 56, due to an ID entry error one
dialog game applications enhance collaborative learning and           participant lacked a pretest log, and therefore was excluded from
increase the potential for knowledge-building. Additionally,          the multiple-choice question analysis, and for two participants we
we also explored the possibility of applying a natural-               could not link their short-answer log IDs to their dialog screen IDs;
                                                                      they were excluded from the dialog metric analyses.
                                                                  2200

students to select, and corresponding sentence openers.                               Table 1: Topic discussion questions
These message types were based on speech-act theory and
were adopted from those used in other dialog game                      Topic No.                         Discussion Question(s)
implementations (Weigand, 2016). Students were also                        1           a) Why do we have valves in veins, but not in arteries
provided with a sheet that defined the different dialog acts                               and capillaries?
to guide them (see Table 3). Table 3 also shows examples of                            b) Why don’t we have valves in pulmonary veins?
sentence-openers that students could choose for each dialog                2           Why do we sometimes refer to the heart as a “double
                                                                                       pump”?
act. Figure 1 illustrates the design of the dialog game
                                                                           3           What do you think are the most interesting aspects of
window, with labels numbered indicating the steps for                                  the structure and function of the human circulatory
entering and sending a dialog message, as follows: (1) The                             system? Please discuss.
topic question that defines the parameters of a given dialog
is at the top of the screen. (2) Users may click on a bubble
next to a dialog message to make a reply to the specific                 Table 2: Posttest questions to assess Knowledge-Building
message (which can also be used to reply to earlier
messages in the chat history). Reply messages are indented            Item                            Short-Answer Question
                                                                       No.
relative to the original message. If no reply bubble is                 1      Why is there an artery that carries deoxygenated blood?
clicked, the entered text message will appear below all the
text messages in the chat window, with no indentation. (3)              2      Why do vessels get increasingly smaller as they get close to
                                                                               the body cells, and increasingly larger as they get nearer to the
Users select one of the six communicative act labels, and                      heart?
then select a linked sentence opener from a dropdown menu.              3      In which kind of blood vessels (arteries, veins, or capillaries)
The selected sentence opener appears at (4). (5) Users type                    is the blood pressure the lowest? Why?
in the rest of their message into the text box. Note that only          4      Why is your right ventricle less muscular than your left
one user may type into his or her text entry box message at a                  ventricle?
                                                                        5      The artery that carries blood from the right side of the heart to
time. If it is the other user’s turn, the shadow text in the box               the lungs (the pulmonary artery) carries about the same
says “Please wait your turn.” If it is the given user’s turn, it               amount of blood as the artery that carries blood from the left
says “Enter your text.” In addition, if a user has failed to                   side of the heart to the rest of the body (aorta). Why do they
first select a dialog act label and sentence opener, on                        carry the same amount of blood?
clicking the text entry box a reminder message will appear,             6      What would happen if the valves between the atria and the
                                                                               ventricles got stuck open and wouldn’t close?
and the user is unable to type into the box until making
these selections. (6) When a user has completed a message,
he or she clicks the “Send” button.                                       Table 3: Descriptions of communicative act labels, with
   The Freechat application was of similar design and                    example sentence-opener choices (Scaffold condition)
appearance as the Scaffold application, and included the
turn-by-turn use features, but did not feature the dialog act           Dialog Act               Description       Example Sentence Opener
label buttons and sentence-opener display. Thus, users took                                                                     Choices
turns simply entering messages, without the scaffold steps.             Information            To provide or               Let me explain…
                                                                                             describe relevant             Some facts are…
                                                                                           facts or knowledge.      My understanding is that…
                                                                        Propose             To bring up a new               I suggest that…
                                                                                             idea to consider.             Let us focus on…
                                                                                                                     I think it makes sense to…
                                                                        Challenge         To argue against, or           I disagree because…
                                                                                             provide evidence         A counter-argument is…
                                                                                              against a dialog        An alternative view is…
                                                                                                 statement.
                                                                        Question           To ask your dialog                  Why is it…
                                                                                               partner about              Can you explain…
                                                                                          something you don’t       What do you think about…
                                                                                                   know.
                                                                        Agreement             To agree with a                  I agree, …
                                                                                            statement made by               Good point, …
                                                                                          your dialog partner.
                                                                        Support               To argue for, or     I think this view is supported
                                                                                          provide evidence for                    by, …
                                                                                           a dialog statement.         To give an example, …
                                                                     Procedure
                                                                     Students were randomly assigned to the Scaffold condition,
                                                                     involving the text chat application that required them to
                  Figure 1: Dialog Game Screen.                      select dialog act labels and sentence openers, or to the
                                                                 2201

Freechat condition. There were 28 students for each                 For example, for Question 4, one point would be awarded
condition. Within each condition, the students were again           for an accurate description of the function of the left
randomly assigned to dialog-discussion pairs. Students in           ventricle, one for the right ventricle, and one point for the
each condition were taken to separate rooms for the study           inference that the right ventricle doesn’t need to pump blood
(Scaffold or Freechat). To minimize verbal and indirect             with as great force as the left, as the blood travels less
interaction, no student sat next to any other student. Each of      distance. Two raters, familiar with the scoring guide and the
two experimenters was also randomly assigned to conduct             learning material and related concepts, scored participants’
the session for each condition.                                     answers to these questions. They were kept naïve to the
                                                                    experimental condition for all the short-answer logs. The
In each study session room, pre-arranged laptops were               scores were averaged across the two raters. The intraclass
placed on the desks. The experimenter overviewed the                correlation for absolute agreement on the items was
session, which consisted of the following tasks:                    computed as ICC (1, 128) = 0.87 for the text-implicit items
                                                                    and ICC (1, 128) = 0.96 for the model-implicit items.
1) Students were given up to 7 minutes to individually
     complete the multiple-choice pretest (could click              Topic adherence. We conducted exploratory follow up
     “submit” if they finished early). (The timer for all tasks     analyses that utilized the “semantic fingerprint” system
     was viewable at the top of the application window).            developed by the Cortical.io Company (with the API
2) Following the pretest, students were taken to the                available on their website). The goal was to assess the
     learning material screen where they had 15 minutes to          feasibility of utilizing natural-language processing methods
     read and study the learning material.                          to predict students’ capacity for knowledge-building
3) Then the experimenter went over how to use the                   (short-answer performance) from their dialog messages.
     system. For the Scaffold condition, the experimenter           Such functions, if predictive, could be useful to incorporate
     went over the different communicative act labels, and          into dialog game applications, for teachers and students to
     the steps for entering in a message including a sentence       track (in an automated fashion) learning outcomes implicitly
     opener. Students also received a dialog act description        from dialogs. The Cortical.io system represents the meaning
     sheet (Table 3).                                               of words in terms of their distributional overlap in a large
4) The students were given a five-minute demo dialog                linguistic corpus (i.e., Wikipedia). Its theoretical basis is the
     session to help them get accustomed to the application.        notion of distributional semantics, or “word spaces” (e.g.,
5) Next, the students (with their randomly assigned                 Sahlgren, 2006). The more frequently that words co-occur
     learning partner) discussed the dialog questions for the       in near proximity in the corpus, the higher is their computed
     3 topics. For both conditions, students took turns             “semantic fingerprint overlap.” The metric can also be
     entering in a dialog message. They could also open a           extended by the system to compute the degree of semantic
     pop-up window that contained the learning-material,            fingerprint overlap among text segments and documents,
     which they could refer to for the discussions. For each        rather than of single words. For implementation details,
     topic, students had a 10 minute dialog discussion.             refer to De Sousa Webber (2015).
6) Following the end of their dialogs discussion, the                  Dialog file inputs were first corrected for spelling errors
     students completed the post-test individually. These           and abbreviations. What we refer to as “topic adherence” is,
     consisted of the same multiple-choice questions as in          for each topic dialog and participant, the semantic
     the pre-test (6 minutes). In addition, they had to answer      fingerprint overlap between the participant’s dialog
     the short-answer questions (as in Table 2) to assess           messages (entered into the system as a single “document”)
     knowledge-building, for which they were given 25               and pre-selected keywords intended to represent important
     minutes. For each portion of the posttest, students could      concepts related to the topic question. Refer to Table 1 for
     click a “submit” button if they finished early.                the Topic questions. For Topic 1, the keywords were:
                                                                    “valves,” “veins,” “arteries,” “capillaries,” “pulmonary,”
                                                                    and “pressure.” For Topic 2, they were: “heart,” “lungs,”
Measures                                                            “oxygen,” “blood,” and “pump.” For Topic 3, they were
Knowledge-gain. To assess students’ knowledge gain from             “valves,” “veins,” “arteries,” “heart,” “lungs,” “oxygen,”
reading the learning material and engaging in the dialog            “blood,” and “circulatory.” The additional dialog metrics of
discussions, their scores on the posttest multiple choice (out      mean number of words-per-turn, and total number of turns,
of 10 points) were compared to their pretest scores.                were used.
Knowledge-building. A scoring guide was developed that                              Findings and Discussion
allowed for 2 points maximum on each of the three
text-implicit questions, and 3 points maximum on each of            Knowledge-gain scores
the three model-implicit questions. Basically, a point was          Figure 2, on the two pairs of bars on the left, shows the
awarded for each piece of information relevant for inferring        mean scores across the Scaffold and Freechat conditions on
the answer to the question, and for each correct inference.         the pre-test and post-test multiple choice for assessing
                                                                2202

students’ level of recall and understanding of the domain           show the regressions separately on the Scaffold and the
material. It also displays the proportion-scores, so that tests     Freechat cases, respectively. For the Scaffold condition, the
with different scales can be displayed on the same chart.           overall regression trends toward statistical significance, and
Participants did not differ significantly on their pretest          the coefficient for topic-adherence reaches statistical
scores, t (53) < 1. Across both conditions, participants            significance. Total-turns trends in the direction of predicting
showed improvement on their post-test multiple-choice               increased knowledge-building scores. For the Freechat
scores relative to their pre-test scores, t (55) = 5.22, p <        condition, the overall regression also trends toward
.001, with an effect size of d = 0.76. The knowledge-gain           statistical significance, but with a non-significant coefficient
(post- minus pre- test score difference) in the Scaffold            for topic adherence, and with the total-turns coefficient
condition (M = 1.07) did not significantly differ from the          trending in the direction of predicting reduced knowledge-
Freechat knowledge-gain (M = 1.14), t (53) < 1. The two             building. Overall, across both regressions words-per-turn
conditions also did not differ significantly on the mean post-      appears to be a relatively weak predictor.
test multiple-choice scores, t (53) < 1.
                                                                        Table 4: Multiple regression for predicting knowledge-
Knowledge-building scores                                                               building (Scaffold condition)
To assess our hypothesis of increased knowledge-building
for the Scaffold condition, we first conducted a MANOVA                 Predictor            B          SE B   β         t      p
on the text-implicit and model-implicit scores. There was an         Topic adherence       19.53        7.64  0.55    2.56    0.02*
overall effect of condition, F (2, 53) = 3.19, p < .05. Figure        Words-per-turn        0.06        0.07  0.24    0.97    0.34
2, on the two pairs of bars on the right, shows the mean
                                                                           Total turns      0.42        0.21  0.52    1.96    0.06
proportion-scores across the two sets of knowledge-building
                                                                         2
questions (text-implicit and model-implicit). The follow-up            R = 0.27, F (3, 22) = 2.72, p = 0.07
tests indicated no effect of condition on the model-implicit
questions, t (54) < 1. However, for the text-implicit
questions, the mean score was higher in the Scaffold than               Table 5: Multiple regression for predicting knowledge-
the Freechat condition, t (54) = 2.39, p = .02, with an effect                         building (Freechat condition)
size of d = 0.64.
                                                                        Predictor            B         SE B    β         t      p
                                                                     Topic adherence        4.62        6.09 0.16      0.76    0.46
                                                                      Words-per-turn       -0.01        0.04 -0.08    -0.34    0.74
                                                                        Total turns        -0.13        0.08 -0.41    -1.62    0.12
                                                                         2
                                                                       R = 0.23, F (3, 24) = 2.32, p = .10
                                                                    Discussion
                                                                    Our hypothesis was partially supported. Namely, students in
                                                                    dialog-games interactions to discuss topic questions in the
                                                                    learning domain exhibited a higher readiness for
                                                                    knowledge-building, in the form of making text-implicit
                                                                    inferences, than students in the freechat discussions. There
                                                                    was no significant improvement on model-implicit
                                                                    questions. The increased knowledge-building readiness was
                                                                    also over and above any knowledge-gain, which did not
 Figure 2: Mean scores (+/- SE) in the Scaffold and Freechat
                                                                    significantly differ between conditions.
     for the multiple-choice pretest and posttest, and the
                                                                       In addition, the multiple-regression results suggest that
knowledge-building tests (text-implicit and model-implicit).
                                                                    natural-language processing methods may hold some
                                                                    promise in producing dialog metrics with predictive utility
                                                                    for knowledge-building. The predictive value (in terms of
Dialog metrics for knowledge-building
                                                                    the standardized Beta coefficient) of our topic-adherence
We conducted follow-up multiple-regression analyses to              metric was particularly more prominent for the Scaffold
explore whether the topic adherence scores obtained by the          condition than for Freechat condition. Also of interest,
semantic fingerprint system, along with the metrics of              though more caution is warranted for interpretation of non-
words-per-turn and number of turns, could be of use for             statistically significant trends, is that the total-number of
predicting students’ readiness for knowledge-building (i.e.,        dialog turns went in the direction of predicting more
their short-answer scores). Scores were averaged for each           knowledge-building for Scaffold condition, and less
participant across the three dialog topics. Tables 4 and 5          knowledge-building for Freechat condition. These trends
                                                                2203

may be indicative of qualitative differences in the nature of       the     predictive     value    of     dialog   metrics  for
dialogs with versus those without scaffolds, with a tendency        knowledge-building.
for scaffolds to raise the potential learning value of each
dialog turn, and to increase the potential knowledge-                                        References
building when dialog partners jointly discuss core concepts
                                                                    Ainsworth, S., & Burcham, S. (2007). The impact of text
in the learning domain. One possibility is that the dialog
                                                                       coherence on learning by self-explanation. Learning and
game scaffold functions in effect promote more self-
                                                                       instruction, 17(3), 286-303.
explanation in the process of developing explanations and
                                                                    Ainsworth, S., & Loizou, A.T. (2003). The effects of self-
arguments to one’s dialog partner. An extensive study of
                                                                       explaining when learning with text or diagrams. Cognitive
collaborative learning dialogs by Asterhan & Schwarz
                                                                       Science, 27(4), 669-681.
(2009), on the other hand, suggests that the process of
                                                                    Asterhan, C. S., & Schwarz, B. B. (2009). Argumentation
argumentation itself may be essential for driving conceptual
                                                                       and explanation in conceptual change: Indications from
change from the joint construction of explanations. In the
current context, if the dialog game scaffold functions                 protocol analyses of peer-­‐‑to-­‐‑peer dialog. Cognitive
encouraged more structured argumentation, this would open              Science, 33(3), 374-400.
the door for dialogs that are more focused on the main topic        Azevedo, R., Moos, D. C., Johnson, A. M., & Chauncey, A.
concepts to generate improved conceptual understanding of              D. (2009). Measuring cognitive and metacognitive
the learning domain.                                                   regulatory processes during hypermedia learning: Issues
  The conceptual foundation for applying the framework of              and challenges. Educational Psychologist, 45(4), 210-
dialog-games to learning is grounded in the notion of                  223.
learning as a dialectical, social, and interactive process (cf.     Bloom, B. S., Engelhart,, M. D. Furst, E. J., Hill, W.
Mercer & Littleton, 2007). Structuring a learning-discussion           H.; Krathwohl, D. R. (1956). Taxonomy of educational
as dialog-game is therefore seen as a means to encourage               objectives: The classification of educational goals.
effective argumentation and critical thinking (e.g.,                   Handbook I: Cognitive domain. New York, NY: David
McAlister, Ravenscroft, & Scanlon, 2004). In terms of                  McKay Company.
Bloom’s taxonomy, the potential, more immediate benefits            Carlson, L. (2012). Dialogue games: An approach to
of dialog-games can be viewed as focused on the application            discourse analysis (Vol. 17). Springer Science &
and analysis levels of learning. However, effective learning           Business Media.
at these levels requires first a solid groundwork of basic          Chi, M. T., Siler, S. A., Jeong, H., Yamauchi, T., &
understanding of concepts in a learning domain, and in turn           Hausmann, R. G. (2001). Learning from human
takes time. Reaching even higher levels of learning, and              tutoring. Cognitive Science, 25(4), 471-533.
unlocking creativity, is an ever increasing long-term process       De Sousa Webber, F. (2015). Semantic folding theory and
(cf. Bloom, 1956). Thus, dialog games may be beneficial for           its application in semantic fingerprinting. White paper
developing students’ creativity, but this would need to be            retrieved from http://www.cortical.io.
evaluated by an extended use of such applications for               Fiorella, L. & Mayer, R. E. (2016). Eight ways to promote
learning, e.g. over weeks or months.                                   generative learning. Educational Psychology Review,
  Along these lines, one limitation of the current study is            28(4), 717-741.
that it was a “single-shot” learning and evaluation session.        Mercer, N., & Littleton, K. (2007). Dialogue and the
For generative learning more time for absorbing,                       development of children's thinking: A sociocultural
processing, and transforming information may be an                     approach. Routledge.
essential element (Fiorella & Mayer, 2015). Thus, even on           Ravenscroft, A. (2007). Promoting thinking and conceptual
the text-implicit questions, for which there was a                     change with digital dialogue games. Journal of Computer
medium-sized effect for the difference between conditions,             Assisted Learning 23(6), 453-465.
the mean proportion of total points obtained was for both           Ravenscroft, A., Wegerif, R. & Hartley, R. (2007).
conditions only about half of the total possible. In addition          Reclaiming thinking: dialectic, dialogic and learning in
to being constrained by time for the current study, another            the digital age. BJEP Monograph Series II, Number 5-
note is that dialog-games are often applied for conversations          Learning through Digital Technologies, 1(1), 39-57.
among small-groups (Ravenscroft, 2007). It is possible that         Roscoe, R. D. & Chi, M. T. (2007). Understanding tutor
learning-dialogs for groups of 3 or 4 may allow for more               learning: Knowledge-building and knowledge-telling in
argumentation and perspective-taking opportunities than                peer tutors’ explanations and questions. Review of
two-way dialogs. Future research directions are indicated              Educational Research, 77(4), 534-574.
for “scaling” up dialog-games applications for                      Weigand, E. (2016). The dialogic principle revisited:
knowledge-building, both in terms of time (over a long-term            Speech acts and mental states (pp. 209-232). Springer
learning period) and in terms of group-size (e.g., from                International Publishing.
learning-pairs to learning-groups). Such extensions may             Wells, S. (2014). Supporting argumentation schemes in
lead to larger-scale knowledge-building effects, and increase          argumentative dialogue games. Studies in Logic,
                                                                       Grammar and Rhetoric, 36(1), 171-191.
                                                                2204

