                           Distributional learning and lexical category acquisition:
                                          What makes words easy to categorize?
                            Giovanni Cassani, Robert Grimm, Steven Gillis, and Walter Daelemans
                                Computational Linguistics and Psycholinguistics (CLiPS) Research Center
                                      Department of Linguistics, University of Antwerp, 13 Prinsstraat
                                                            B-2000 Antwerpen, Belgium
                                                       {name.surname}@uantwerpen.be
                                Abstract                                      This paper aims to explore distributional bootstrapping fur-
                                                                           ther and uses computational simulations to answer the follow-
   In this study, results of computational simulations on English
   child-directed speech are presented to uncover what distribu-           ing research question: what distributional properties of words
   tional properties of words make it easier to group them into            make it easier to categorize them on the basis of the contexts
   lexical categories. This analysis provides evidence that words          they co-occur with? The relation between distributional prop-
   are easier to categorize when (i) they are hard to predict given
   the contexts they occur in; (ii) they occur in few different con-       erties of words and the extent to which these can be easily
   texts; and (iii) their contextual distributions have a low entropy,     categorized in terms of lexical categories has been largely ne-
   meaning that they tend to occur more often in one of the con-           glected in previous research, but characterizing it is important
   texts they occur in. This profile fits that of content words, espe-
   cially nouns and verbs, which is consistent with developmental          for two main reasons. Firstly, it generates predictions about
   evidence showing that children learning English start by form-          the effect that several distributional properties of words have
   ing a noun and a verb category. These results further charac-           on lexical category acquisition in English speaking children:
   terize the role of distributional information in lexical category
   acquisition and confirm that it is a robust, reliable, and devel-       testing them can shed further light on the plausibility of distri-
   opmentally plausible source to learn lexical categories.                butional learning as an underlying mechanism for lexical cat-
   Keywords: Distributional bootstrapping; Lexical category ac-            egory acquisition. Importantly, it is not enough that a model
   quisition; Statistical learning; Computational psycholinguis-           behaves like humans: a statistical analysis of what drives the
   tics; Language acquisition
                                                                           model’s behavior is necessary to assess whether it is driven
                                                                           by the same factors that affect human behavior. Secondly,
                            Introduction                                   it can help to constrain the development of psychologically
Distributional bootstrapping (Maratsos & Chalkley, 1980) is                motivated models of lexical category acquisition, by showing
an influential account of how children start breaking into lan-            what information children are sensitive to when solving the
guage, and specifically of how they start grouping words into              task of grouping words into lexical categories.
lexical categories such as nouns and verbs. More specifi-                     In this work, computational simulations are used to carry
cally, it claims that children use patterns of co-occurrences              out a categorization experiment whose outcome is used as the
across linguistic units, such as words and morphemes, to                   dependent variable in a regression analysis aimed to uncover
group words that share similar contexts. Several computa-                  the effect of several distributional properties of words on cat-
tional simulations have shown that distributional information              egorization accuracy. Results shed light and generate predic-
is a rich, useful, and usable source of knowledge about lex-               tions on the mechanisms underlying distributional learning
ical categories (Mintz, 2003; Redington, Chater, & Finch,                  of lexical categories, and ultimately provide information to
1998; St. Clair, Monaghan, & Christiansen, 2010). More-                    guide and constrain the development of psychologically mo-
over, a number of behavioral experiments have confirmed that               tivated models of bootstrapping in language acquisition.
children use this information to group words together (Mintz,
Wang, & Li, 2014; Reeder, Newport, & Aslin, 2013).                                                    Methods
   Research on distributional bootstrapping has mostly fo-
cused on investigating which contexts constitute the best cues             Corpora and pre-processing
for the acquisition of lexical categories. Several proposals               In order to perform the computational simulations, tran-
that have been put forward share the approach of grouping to-              scribed interactions involving children and caretakers avail-
gether those words that share similar contexts of occurrence,              able in the CHILDES database (MacWhinney, 2000) were
but differ in the starting assumptions and the types of con-               used. More specifically, the Manchester corpus (Theakston,
texts they evaluate. For example, Mintz (2003) suggested                   Lieven, & Pine, 2001) from the British English part, and the
that frequent frames, i.e. trigrams consisting of two words                Suppes corpus (Suppes, 1974) from the American English
flanking an empty slot (a X b), are a psychologically plausi-              part were selected, since they have both been widely used
ble and highly effective type of context for acquiring lexical             in previous research on distributional bootstrapping of lexical
categories. St. Clair et al. (2010), on the contrary, provided             categories. The Suppes corpus consists of transcripts of one
evidence that better categorization can be achieved by using               child, Nina, recorded from 1;11 to 3;3, while the Manchester
bigrams (a X + X b) that can be readily combined to obtain                 corpus contains data of 12 children, recorded for varying pe-
trigram level information.                                                 riods within the age range 1;8 to 3;0. Both come with an au-
                                                                       216

tomatic categorization in terms of Part-of-Speech (PoS) tags,             in the same contexts are considered to be more similar and
which can be accessed on the MOR tier of the CHILDES an-                  clustered together: target words are categorized correctly if
notation scheme. The child-directed speech from the corpora               they are assigned the correct lexical category by the com-
was pre-processed to deal with some aspects of the transcrip-             putational simulation. The experiment was performed us-
tions. Two dummy symbols, #start and #end, were inserted at               ing Memory-Based Learning (MBL, (Daelemans & van den
the beginning and end of each utterance. This manipulation                Bosch, 2005)), a class of machine learning algorithms which
is motivated by evidence that sentence boundaries provide                 implements an exemplar-based strategy and categorizes new
useful distributional information (Freudenthal, Pine, & Go-               items using retrieval of or similarity to items stored in mem-
bet, 2008). It also allows us to exploit every utterance from             ory, with no explicit abstraction.
the corpus, including single word utterances: words occur-                   The categorization experiment consists of two main
ring in isolation are considered to be occurring in the bigrams           phases, which are referred to as training and testing in the
#start X and X #end, and in the trigram #start X #end.                    paper. During training, co-occurrence counts between target
   Corpora from individual children were processed sepa-                  words and contexts are collected on a portion of the input data
rately using a sliding window approach: starting from the first           and stored in memory. Each word is represented as a vector
lexical element of the utterance, each word was considered as             of counts, with each count indicating the co-occurrence fre-
target, and all bigrams and trigrams occurring next to it were            quency of the corresponding word and context. During test-
collected. These types of contexts were chosen given that                 ing, a new portion of the input is considered and the same pro-
they have been widely explored in previous research (Mintz,               cedure is applied. At the end of this second stage, the learner
2003; Monaghan & Christiansen, 2008; St. Clair et al., 2010).             has created two matrices of co-occurrence counts. Each word
As an example, consider the following utterance from the                  from the test matrix is categorized by comparing its vector
Manchester corpus: #start are~v you~n going~v to~funct                    of co-occurrences with all the vectors from the training ma-
put~v that~adv one~n inside~adv? #end. The first target                   trix, looking for the most similar one; the two are then clus-
word, are, occurs in two bigrams, #start X and X you~n,                   tered together. During learning, the model has no access to
and two trigrams, #start X you~n and X you~n going~v. For                 the correct lexical categories of the words and only groups
words in the middle of the utterance, three trigrams are avail-           them together based on their co-occurrence patterns, in an
able. The tags after the tilde indicate the lexical category to           unsupervised way. At the end of the process, the category of
which each word belongs according to the automatic catego-                two words that were clustered together is inspected: if they
rization. The original categories were collapsed to a coarser             share the same lexical category, the word from the test set has
set, consisting of five categories: nouns (n), including pro-             been categorized correctly. In this framework, the only factor
nouns; verbs (v), including auxiliaries, copulas, and non-                driving clustering is similarity, which is a well-documented
finite forms1 ; adjectives (adj), adverbs (adv), and function             cognitive mechanism in categorization (Sloutsky, 2003).
words (funct). The idea is to zoom in on the open classes,
                                                                             In order to divide each individual corpus into a training and
conflating the closed class words in a single category given
                                                                          a test set, utterances of child-directed speech were ordered
that function words are categorized later in development. No
                                                                          chronologically and split in two parts: (i) the first 70% of the
lemmatization is performed, and all information about lexical
                                                                          utterances were allocated for training; and (ii) the last 30% of
categories is preserved2 , although it is only used to evaluate
                                                                          the utterances were used as test set. To evaluate how different
whether categorization has been successful.
                                                                          distributional properties interact with time, operationalized as
   In order to minimize both the number of assumptions and
                                                                          a larger exposure to the input language, an incremental train-
that of possible decisions in the design of the experiment, all
                                                                          ing approach was implemented. In detail, training started on
bigrams and trigrams are considered: some will turn out to be
                                                                          the first 40% of all the utterances, then proceeded on the first
more informative to the categorization task than others, but
                                                                          45%, always increasing by 5 percentage points, up to the full
the analysis of this aspect of the problem falls outside of the
                                                                          training set (70% of the total utterances). The test set was
scope of this study. Larger n-grams are not considered due to
                                                                          kept constant to make sure that any change in performance
the limited size of the corpora: they would be too infrequent
                                                                          came from the knowledge inferred from the training set and
to affect categorization.
                                                                          not by differences in the test set.
Experimental setting                                                         The TiMBL package (Daelemans, Zavrel, van der Sloot, &
A categorization experiment was carried out, in which words               van den Bosch, 2009) was used to carry out the simulation,
were clustered together based on the similarity of the contexts           using the default IB1 algorithm (Aha, Kibler, & Albert, 1991)
in which they occurred in corpora of English child-directed               and cosine as a distance metric, because of its robustness to
speech (Redington et al., 1998). Words that tend to occur                 different frequencies in the co-occurrence vectors, and setting
                                                                          the number of nearest neighbors to 1. Moreover, no feature
    1 Results from Mintz (2003) show that merging pronouns with
                                                                          weighting based on co-occurrence statistics from the training
nouns, and auxiliaries, copulas, and non-finite forms with verbs does
not bias categorization results.                                          corpus was applied during the categorization experiment: this
    2 X dog~n and X dogs~n are different contexts, just as light~n X,     allows us to perform the categorization experiment without
light~v X, and light~adj X                                                weighting contexts according to their informativity, avoiding
                                                                      217

the effect of supervision on classification, which would be              vector and the harder it is to correctly group it with similar
psychologically questionable and bias the results.                       words. Importantly, normalized entropy provides a related
   Importantly, no claim is put forward that children actually           but different piece of information than contextual diversity:
keep track of all available bigrams and trigrams, or that they           the normalization ensures that the number of different con-
implement an analogue of the IB1 algorithm with the cho-                 texts a word occurs in does not affect entropy.
sen parameter setting. The interest of the current analysis is
purely in the information that supports learning and in the              A further independent variable was considered for both
analysis of the effects that distributional properties of words       words and contexts, i.e. time, operationalized as the amount
have on categorization, as operationalized using MBL.                 of training input on which the computational simulations
                                                                      were trained: time goes from 0 (i.e. 40% of all utterances
Statistical analysis                                                  in the corpus used as training set) to 6 (70% of all utterances
Four pieces of distributional information were computed for           in the corpus used as training set). Time should have a posi-
each word on the test set (last 30% of utterances of each cor-        tive effect, since exposing the model to more input language
pus) and used as predictors in a regression model:                    should provide more reliable and robust information about
                                                                      co-occurrence patterns.
 Token frequency: the log-transformed frequency count of                 The analysis was restricted on words that appeared in all
   each token. The transformation is motivated by evi-                13 individual corpora (12 from the Manchester corpus and
   dence from Keuleers, Diependaele, and Brysbaert (2010)             1 from the Suppes corpus), to reduce the effect of idiosyn-
   that lexical frequency effects are better captured by log-         crasies and focus on general patterns. All words with a token
   transformed frequency counts. A positive effect of fre-            frequency of 1 were also excluded from the analysis, because
   quency is expected (Ambridge, Kidd, Rowland, & Theak-              when this is the case, contextual diversity and entropy are
   ston, 2015), since more frequent items are typically learned       fully determined. If a word occurred only once, then it also
   better than less frequent ones.                                    occurred in only one context (diversity of 1), and its entropy
                                                                      is 0, because the full probability mass is on the only context
 Contextual diversity: the log-transformed count of how               the word occurred in.
   many different contexts a word occurs in. A negative effect           In order to analyze how easy it is to categorize a word,
   for contextual diversity is predicted: if a word occurs in         logistic mixed-effects models (Baayen, Davidson, & Bates,
   many different contexts, its co-occurrence vector is noisy         2008) were fitted using the “lme4” package in R (D. Bates,
   and it is harder to reliably group it with other words. This       Maechler, Bolker, & Walker, 2015). Random intercepts for
   is the case, e.g., of function words, like conjunctions and        corpus (13 levels) and word (456 levels, i.e. the single words
   determiners: they occur in all sort of contexts, making it         that survived the filtering steps just detailed) were included.
   hard to group them with similar words.                             The categorization outcome of each word was used as a
                                                                      binary dependent variable, with each correctly categorized
 Average conditional probability: the average conditional
                                                                      word coded as 1. Covariates were included in a step-wise
   probability of a word given all the contexts it occurs with.
                                                                      fashion, according to the improvement in fit measured by the
   Consider a toy example where the context the X occurs 100
                                                                      Akaike Information Criterion (AIC, (Akaike, 1973)).
   times, 15 of which with the word cat: p(cat|the X), is thus
   0.15. Assume also that the word cat occurs 40 times in the
                                                                                                  Results
   context a X, which in turn occurs 200 times: p(cat|a X)
   is 0.2. In order to obtain the average conditional proba-          The best converging logistic mixed-effects model included
   bility for the word cat, p(cat|the X) and p(cat|a X) are           main effects for average conditional probability, entropy,
   averaged, yielding 0.175. This independent variable is pre-        time, and contextual diversity. Adding a main effect for token
   dicted to have a negative effect on categorization: high con-      frequency resulted in the model not converging. Two-way in-
   ditional probability means that the contexts in which a tar-       teractions between time and conditional probability, entropy,
   get word occurs do not occur with other words, making              and lexical diversity were tested; however, when these were
   it hard to find shared contexts of occurrence between the          entered, the model did not converge. Table 1 provides the
   target and other words.                                            βs estimated for this model, expressed on the log-odds scale,
                                                                      while Figure 1 represents the effects graphically, with accu-
 Entropy: the entropy of the co-occurrence vector of a word           racy expressed as proportion. The final model resulted in a
   (Shannon, 1948), normalized by the number of contexts it           marginal R2 of 0.055 and in a conditional R2 of 0.913, sug-
   occurs with, so that entropy lies between 0 and 1. The en-         gesting that while the effect of predictors is significant, they
   tropy of a word is low when it occurs in the same context          do not explain much variance in the data. This is further ad-
   the majority of the times, while the more even the distri-         dressed in the discussion.
   bution of co-occurrences for a word, the higher its entropy.          As predicted, the average conditional probability of a word
   Entropy relates to diversity and its effect should go in the       given the contexts in which it occurs has a strong negative ef-
   same direction: the more a word occurs equally frequently          fect on the estimated accuracy (β = −12.17,t = −11.56, p <
   in the contexts it co-occurs in, the noisier its co-occurrence     0.001), and the same is true for the entropy of the distribu-
                                                                  218

Table 1: Mixed-effects model fitted to analyze what distribu-
tional properties make words easier to categorize. Estimates
(Est.) and standard errors (Std. Err.) are provided on the log-
odds scale. (Cond. Prob.: average conditional probability of
words given contexts; Cont. Div.: contextual diversity.
    Ind. Vars.          Est.   Std. Err.           z      p val.
    (Intercept)      14.185       1.298      10.928      < .001
    Cond. Prob.     -12.170       1.053     -11.560      < .001
    Entropy         -11.027       1.215      -9.077      < .001
    Time              0.078       0.011       6.838      < .001
    Cont. Div.       -0.893       0.255      -3.509      < .001
tion of co-occurrence counts of a word over all the contexts it
occurs in (β = −11.027,t = −9.077, p < 0.001). Time has a
significantly positive effect (β = 0.078,t = 6.838, p < 0.001),
showing that the clustering algorithm is actually exploiting
the larger amount of input language to better group similar
words together. Finally, contextual diversity has a signif-
icant negative effect (β = −0.893,t = −3.509, p < 0.001),
suggesting that words are easier to categorize when they oc-
cur in fewer contexts, matching the initial hypothesis. As it
was reported, adding frequency resulted in convergence is-
sues: this is most likely due to the filtering step. It is possible
that surviving words had similar frequency counts, making
it impossible for the model to find sufficient variation to esti-
mate the effect of token frequency on categorization accuracy,
once contextual diversity already entered the model (since it
improved the fit more than token frequency).
                          Discussion
The results that have been presented point to a relation be-
tween distributional properties of words and the degree to
which it is easy to categorize them into lexical category. The
easiest words appear to (i) be on average hard to predict given
the contexts in which they occur; (ii) have a very skewed dis-
tribution of co-occurrence counts with the contexts they occur
in, meaning that they tend to occur most often in one or few
contexts; and (iii) tend to generally occur in few contexts.
    First, being able to predict a word given the contexts it
occurs in is detrimental to categorization. This entails that
effective categorization depends on some uncertainty in the
co-occurrence patterns of words and contexts. Since catego-
rization works on similarity (Sloutsky, 2003), two words can
only be grouped together if they occur in the same context, i.e.        Figure 1: Main effects, with confidence bands, of average
they have something in common. The negative effect of con-              conditional probability, entropy, time, and contextual diver-
ditional probability of words given contexts also points to a           sity on how easy it is to categorize a word in terms of lexical
feature that contexts should have in order to be useful and us-         categories. The order, from top to bottom, reflects the im-
able, namely that they need to occur with more than one word.           provement in fit brought by each predictor. The y-axis rep-
As a matter of fact, the conditional probability of words given         resents probabilities estimated from the log-odds reported in
contexts is computed by dividing the co-occurrence count of             Table 1. Each axis is automatically scaled to provide a clear
the word and the context by the frequency count of the context          depiction of the effect. The plots were obtained using the ef-
itself. For the average conditional probability of word given           fects package in R (Fox, 2003).
context to be low, each context must occur with other words
                                                                    219

a substantial amount of times. This hypothesis fits evidence              that regardless of the fact that the set of target words con-
provided by Matthews and Bannard (2010) that children find                tained an equal number of nouns and verbs, noun categoriza-
it easier to group words together when these occur in contexts            tion is more effective.
that, in turn, occur with several different words.                           The reported evidence also parallels and complements re-
    The negative coefficients of entropy and contextual diver-            sults about word learning, which suggest children find it eas-
sity complement the negative effect of average conditional                ier to learn words (particularly nouns) when they occur in
probability: the latter indicates that words are easier to cat-           a variety of different contexts (Hills, Maouene, Riordan, &
egorize when they tend to occur in contexts only a fraction               Smith, 2010). While a comprehensive experiment is still
of the times the contexts themselves occur. βs for normal-                lacking that explicitly contrasts the effect of contextual di-
ized entropy and contextual diversity, on the contrary, tell that         versity on word learning and categorization, it emerges that
words are easier to categorize when they tend to occur most               this factor impacts both phenomena, although in opposite di-
often in one or few contexts. The ideal situation is thus that            rections. While a higher contextual diversity is beneficial for
of a word that always and only occurs in a single context,                word learning, it is detrimental to word categorization, as ap-
which however occurs with many other words, which also                    pears from the statistical analysis reported here. Further re-
only occur in that context (to reduce noise). The effects of              search about the interplay between different frequency effects
entropy and contextual diversity indicate that uncertainty in             (Ambridge et al., 2015) is needed to clarify to what extent dis-
word-context co-occurrence patterns is necessary at the con-              tributional learning drives and explains language acquisition
text level but detrimental at the word level: words need to oc-           in its many different aspects and sub-tasks.
cur in few contexts for effective categorization. This is likely             Lastly, this study investigated a fully distributional ex-
due to the fact that when contextual diversity and entropy are            planation of the developmental pattern of lexical category
high, the co-occurrence pattern of a word can be very noisy.              acquisition. However, the low R2 shows that the distribu-
    The distributional properties that make a word easier to              tional properties we investigated leave a substantial portion
categorize are rather distinctive of content words, especially            of variance unexplained, calling for further research on which
nouns: knowing a context, e.g. a determiner, it is hard to pre-           properties affected the machine learner and whether these
dict exactly which noun will appear next to it, because many              also influence children during lexical category acquisition.
different nouns (and some adjectives) are possible, which                 Moreover, current research has highlighted the importance
translates into a low conditional probability of words given              of other sources of information during lexical category ac-
contexts. Moreover, it is likely that a noun occurs with one              quisition and word learning (Roy, Frank, DeCamp, Miller, &
of the few determiners or possessive pronouns of the English              Roy, 2015), including morphology, phonetics, semantics and
language, thus scoring low on contextual diversity, and that              prosody (Monaghan & Christiansen, 2008). The influence of
most of the times it occurs with just a couple of specific de-            these sources of information should be further analyzed to
terminers or possessive pronouns, scoring low on entropy. In              complement research on distributional bootstrapping.
order to get a grasp of which lexical categories easier words                Summarizing, this study provided evidence about the ef-
belonged to, those words that were categorized correctly for              fect of different distributional properties of words on the ac-
at least 80% of the 13 individual corpora at the last stage of            quisition of lexical categories from distributional informa-
training were selected. This analysis highlighted 127 such                tion. Conditional probability, entropy, and contextual diver-
words: 2 function words, 101 nouns, and 24 verbs. This                    sity have a negative effect on categorization accuracy. Words
shows that the distributional properties of words that make               with these features tend to be content words, mostly nouns,
them easier to categorize strongly correlate with lexical cat-            which also appear to be the words children start grouping
egories, and that the same features are a possible candidate              earlier and most effectively. Future studies should assess
to explain why certain lexical categories are formed earlier              the cross-linguistic validity of these findings, to understand
than others3 . Furthermore, the majority of the 51 words                  whether the same distributional properties have similar ef-
that are never categorized correctly predominantly consists of            fects in typologically different languages. Moreover, a sim-
function words (26) and adverbs (18), the categories that are             ilar approach — performing statistical analysis on the out-
learned later in development (E. Bates, Dale, & Thal, 1995).              come of computational simulations — could be used to inves-
The observation that nouns are categorized best also relates              tigate what distributional properties make contexts more use-
to the observation that children form a productive noun cate-             ful. Finally, other computational models should be tested, to
gory earlier than any other category (Tomasello, 2000). The               compare their outcome to developmental data and shed light
reported evidence lends support to the hypothesis that the                on which architectures are closer to what children actually do.
so-called noun bias can be traced back to the distributional
properties of words belonging to different lexical categories                                      Conclusion
(Cassani, Grimm, Daelemans, & Gillis, submitted), showing
                                                                          The evidence presented in this study shows that specific dis-
    3 The bias towards nouns and verbs in categorization does not         tributional properties of words determine how easy it is to
result from an imbalance in the set of target words, consisting of 40     cluster them together based on the similarity of their co-
adjectives, 47 adverbs, 76 function words, 145 nouns, and 148 verbs.      occurrence patterns. In detail, words are easier to categorize
                                                                      220

(i) when they are hard to predict given the contexts they oc-       Keuleers, E., Diependaele, K., & Brysbaert, M. (2010). Prac-
cur in, (ii) when they generally occur in few contexts, and           tice effects in large-scale visual word recognition studies:
(iii) when they tend to occur more often in one context, hav-         A lexical decision study on 14,000 Dutch mono- and disyl-
ing low entropy. This study extends previous research on              labic words and nonwords. Front Psychol, 1, 174.
distributional bootstrapping by providing evidence that dis-        MacWhinney, B. J. (2000). The childes project: Tools for
tributional properties also affect which words are categorized        analyzing talk. the database. (3rd ed., Vol. 2). Mahwah,
more easily and which lexical categories are formed earlier.          NJ: Lawrence Erlbaum Associates.
                                                                    Maratsos, M. P., & Chalkley, M. A. (1980). The internal
                     Acknowledgments                                  language of children syntax: The nature and ontogenesis
This research was supported by a BOF/TOP grant (ID 29072)             of syntactic categories. In K. E. Nelson (Ed.), Children’s
of the Research Council of the University of Antwerp.                 language (Vol. 2, chap. 2). New York, NY: Gardner Press.
                                                                    Matthews, D., & Bannard, C. (2010). Children’s production
                         References                                   of unfamiliar word sequences is predicted by positional
                                                                      variability and latent classes in a large sample of child-
Aha, D. W., Kibler, D., & Albert, M. K. (1991). Instance-             directed speech. Cognitive science, 34(3), 465-488.
   based learning algorithms. Mach Learn, 6(1), 37-66.              Mintz, T. H. (2003). Frequent frames as a cue for grammat-
Akaike, H. (1973). Information theory and an extension of             ical categories in child directed speech. Cognition, 90(1),
   the maximum likelihood principle. In B. Petrov & F. Csáki         91-117.
   (Eds.), 2nd International Symposium on Information The-          Mintz, T. H., Wang, F. H., & Li, J. (2014). Word categoriza-
   ory, Tsahkadsor, Armenia, USSR. Budapest, Hungary:                 tion from distributional information: Frames confer more
   Akadémiai Kiadó.                                                 than the sum of their (bigram) parts. Cognitive psychology,
Ambridge, B., Kidd, E., Rowland, C. F., & Theakston, A. L.            75, 1-27.
   (2015). The ubiquity of frequency effects in first language      Monaghan, P., & Christiansen, M. H. (2008). Integra-
   acquisition. J Child Lang, 42(2), 239-273.                         tion of multiple probabilistic cues in syntax acquisition.
Baayen, H. R., Davidson, D. J., & Bates, D. M. (2008).                In H. Behrens (Ed.), Corpora in language acquisition re-
   Mixed-effects modeling with crossed random effects for             search: History, methods, perspectives (Vol. 6, p. 139-164).
   subjects and items. J Mem Lang, 59(4), 390-412.                    Amsterdam: John Benjamins Publishing.
Bates, D., Maechler, M., Bolker, B., & Walker, S. (2015). Fit-      Redington, M., Chater, N., & Finch, S. (1998). Distribu-
   ting linear mixed-effects models using lme4. J Stat Softw,         tional information: A powerful cue for acquiring syntactic
   67(1), 1-48.                                                       categories. Cognitive science, 22(4), 425-469.
Bates, E., Dale, P. S., & Thal, D. (1995). Individual differ-       Reeder, P. A., Newport, E. L., & Aslin, R. N. (2013). From
   ences and their implications for theories of language devel-       shared contexts to syntactic categories: The role of dis-
   opment. In P. Fletcher & B. MacWhinney (Eds.), The hand-           tributional information in learning linguistic form-classes.
   book of child language (p. 96-151). Oxford, UK: Black-             Cognitive psychology, 66(1), 30-54.
   well.                                                            Roy, B. C., Frank, M. C., DeCamp, P., Miller, M., & Roy,
Cassani, G., Grimm, R., Daelemans, W., & Gillis, S. (sub-             D. K. (2015). Predicting the birth of a spoken word. Proc
   mitted). Distributional bootstrapping and the noun bias:           Natl Acad Sci U S A, 112(41), 12663-8.
   Zooming in on developmental plausibility.                        Shannon, C. E. (1948). A mathematical theory of communi-
Daelemans, W., & van den Bosch, A. (2005). Memory-based               cation. The Bell system technical journal, 27, 379-423.
   language processing. Cambridge, UK: Cambridge Univer-            Sloutsky, V. M. (2003). The role of similarity in the develop-
   sity Press.                                                        ment of categorization. Trends in cognitive sciences, 7(6),
Daelemans, W., Zavrel, J., van der Sloot, K., & van den               246-251.
   Bosch, A. (2009, 31 May 2010). Timbl: Tilburg Memory             St. Clair, M. C., Monaghan, P., & Christiansen, M. H.
   Based Learner, version 6.3. reference guide. (Tech. Rep.).         (2010). Learning grammatical categories from distribu-
   Tilburg University.                                                tional cues: Flexible frames for language acquisition. Cog-
Fox, J. (2003). Effect displays in r for generalised linear           nition, 116(3), 341-360.
   models. J Stat Softw, 8(15).                                     Suppes, P. (1974). The semantics of children’s language. Am
Freudenthal, D., Pine, J. M., & Gobet, F. (2008). On the              Psychol, 29(2), 103-114.
   utility of conjoint and compositional frames and utterance       Theakston, A. L., Lieven, E. V. M., & Pine, J. M. (2001).
   boundaries as predictors of word categories. In V. Slout-          The role of performance limitations in the acquisition of
   sky, B. Love, & K. McRae (Eds.), Proceedings of the 30th           ”mixed” verb-argument structure at stage i. J Child Lang,
   annual meeting of the Cognitive Science Society (p. 1947-          28(1), 127-152.
   1952). Austin, TX: Cognitive Science Society.                    Tomasello, M. (2000). Do young children have adult syntac-
Hills, T. T., Maouene, J., Riordan, B., & Smith, L. B. (2010).        tic competence? Cognition, 74(3), 209-253.
   The associative structure of language: Contextual diversity
   in early word learning. J Mem Lang, 63(3), 259-273.
                                                                221

