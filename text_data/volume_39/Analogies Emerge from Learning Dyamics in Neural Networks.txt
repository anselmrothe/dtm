                 Analogies Emerge from Learning Dyamics in Neural Networks
                                          Andrew Lampinen (lampinen@stanford.edu)
                             Department of Psychology, Jordan Hall, 450 Serra Mall, Stanford CA 94305
                                              Shaw Hsu (cshawhsu@stanford.edu)
                      Department of Biophysics, James H. Clark Center, 318 Campus Dr., Stanford CA 94305
                                       James L. McClelland (mcclelland@stanford.edu)
                             Department of Psychology, Jordan Hall, 450 Serra Mall, Stanford CA 94305
                              Abstract                               that are completely non-overlapping in their inputs and out-
                                                                     puts (Hinton, 1986; Rogers & McClelland, 2008). In other
   When a neural network is trained on multiple analogous tasks,     words, if you train a neural network to solve two identical
   previous research has shown that it will often generate rep-
   resentations that reflect the analogy. This may explain the       tasks, using separate sets of inputs and outputs but sharing
   value of multi-task training, and also may underlie the power     the hidden units, in some cases it will generate representa-
   of human analogical reasoning – awareness of analogies may        tions that reflect the analogy (i.e. analogous items will gener-
   emerge naturally from gradient-based learning in neural net-
   works. We explore this issue by generalizing linear analysis      ate more similar patterns of activity in the hidden units than
   techniques to explore two sets of analogous tasks, show that      non-analogous items) (Rogers & McClelland, 2008). This
   analogical structure is commonly extracted, and address some      can lead to the ability to correctly make analogical inferences
   potential implications.
                                                                     about items not explicitly taught (Hinton, 1986). This ex-
   Keywords: neural networks; structure learning; representa-        traction of shared structure sets neural networks apart from
   tion; analogy; transfer;
                                                                     simple forms of statistical pattern recognition (Rogers & Mc-
                                                                     Clelland, 2008) such as linear data analysis techniques like
                          Introduction                               PCA.
Analogical transfer is often considered an essential compo-             Furthermore, recent work has shown that neural networks
nent of “what makes us smart” (Gentner, 2003). However,              can show benefits of training on multiple tasks (Dong et
there is a tension in the literature – Detterman (1993) has de-      al., 2015; Rusu et al., 2015, e.g.). Even a small amount of
clared that “significnt transfer is probably rare and accounts       learning on distinct but related tasks has been shown to im-
for very little human behavior.” Yet other authors have found        prove performance. For example, training a natural language
that in some cases analogical transfer between superficially         translation system on image captioning and autoencoding im-
dissimilar systems can be so natural that it may not even re-        proves translation performance (Luong et al., 2016). Learn-
quire explicit awareness of the analogy (Day & Goldstone,            ing on numerous language translation pairs can even give
2011). How can we reconcile these viewpoints?                        generalization without further training to unseen language
   One feature that often separates the researchers with these       pairs (Johnson et al., 2016). We suggest that these bene-
opposing viewpoints is the type of tasks and transfer they con-      fits may be due to neural networks ability to extract shared
sider. When Detterman (1993) says that the manipulations             structure. Because human experience is filled with distinct
necessary to show transfer have “the subtlety of [a] baseball        tasks that share common elements (language, various percep-
bat”, he cites work like that of Gick & Holyoak (1980) which         tual modalities, etc.) understanding the way that structure is
shows the difficulty of rapidly making an explicit mapping           learned across tasks may be essential to understanding human
between two superficially disparate domains to explicitly            intelligence and building better artificial intelligence systems.
solve a problem. By contrast, the Day & Goldstone (2011)                However, we have little understanding of how, why, or
experiments show transfer when participants learn about a            when neural networks are able to extract structural analogies
system by interacting with it over a longer period of time,          from their training data. Here, we describe a preliminary
and then transfer is measured implicitly on an analogous sys-        investigation into this question, and in the process describe
tem. We believe that this distinction between fast-explicit          a new approach to analyzing neural network representations
analogical transfer and slower-potentially-implicit analogical       that may yield more general insights. We begin with a very
transfer may explain much of the disagreement in the litera-         simple instantiation of a task with analogous structure.
ture. (See also Bransford & Schwartz (1999).)
   Previous work has shown that neural networks can provide                                  A Simple Task
a good model for “slow” analogical transfer in domains as            In the original work of Hinton (1986), a neural network was
broad as artificial grammar learning (Dienes et al., 1999) and       taught to answer queries about the structure of two perfectly
verbal analogies Kollias & McClelland (2013). In particu-            analogous family trees (one English and one Italian, see fig.
lar, one line of work shows that neural networks are capable         5), and was shown to generate representations that extract
of extracting analogous structure from knowledge domains             the analogy, in the sense that analogous people from differ-
                                                                 2512

ent families are represented similarly. Here, we pare this             no input or output weights to the other domain).1 Thus, since
task down to its barest essentials: two perfectly analogous            the final representational components that a linear network
domains with separate inputs and ouputs. For our task, the             learns are precisely the components of the SVD (Saxe et al.,
inputs can be thought of as the set of letters {R, L, ρ, λ}, and       2013), there will be no sharing of structure across domains.
the outputs as {P, D, S, π, δ, σ}. The task can be seen as map-            Furthermore, the optimal rank k approximation to a matrix
ping an input letter onto the letters that it can follow (e.g. “R”     is to take the top k components from the SVD (Mirsky, 1960).
can follow “D” as in “draw,” but cannot follow “S”), where             If a linear network’s hidden layers are restricted to rank lower
there is an analogy between the Latin and Greek letters. See           than that of the I/O correlation matrix, detail within the do-
below for the input-output (I/O) mapping:                              mains will be lost. Thus a linear neural network cannot solve
                                                                       the task perfectly if any of its hidden layers has a number
                        P    D   S   π   δ     σ                       of units smaller than the rank of the I/O correlation matrix.
                     R  1    1   0   0   0     0                       By contrast, a non-linear network can exploit the analogy be-
                     L  1    0   1   0   0     0                       tween the domains to find more parsimonious solutions. Is
                     ρ  0    0   0   1   1     0                       there a way to leverage linear insights in the non-linear case?
                     λ  0    0   0   1   0     1
                                                                       Methods: A Linearized Approach
When and how does a neural network extract the analogous
                                                                       As we shall see, while a linear network cannot extract the
structure across the domains in this simple task?
                                                                       analogous structure from the task, inserting a single non-
Methods: Linear Networks?                                              linearity after the output layer can allow it to do so. In the
                                                                       case that the non-linearity is a sigmoid, this essentially re-
There have been recent developments in the theory of linear
                                                                       duces the problem to logistic regression; here we will use rec-
neural networks which show that the process of learning is
                                                                       tified linear units in our analysis because their structure makes
entirely driven by the Singular Value Decomposition (SVD)
                                                                       the output patterns more intuitively interpretable. Once this
of the input-output correlation matrix (Saxe et al., 2013). The
                                                                       almost-linear network has solved the problem, consider its
SVD can be seen as breaking the structure of the task into
                                                                       outputs immediately prior to the non-linearity. These are pro-
individual “modes” – linear structures in the dataset, some-
                                                                       duced by the linear part of the network, and together with the
what like components in PCA. Specifically, a mode consists
                                                                       non-linearity suffice to produce the desired outputs. We can
of an input pattern (which can be interpreted in this case as the
                                                                       use these to turn the problem into a linearly analyzable one
input letters the mode responds to), a singular value (which
                                                                       – simply treat these pre-nonlinearity outputs as outputs of a
roughly corresponds to the amount of variance explained by
                                                                       linear network. Then the problem becomes susceptible to the
this mode), and an output mode (the output letters produced
                                                                       types of linear analyses discussed above.
by the given pattern on the inputs). For example, see Fig. 1
                                                                           Thus we trained a neural network with a single hidden layer
for the SVD of the I/O mapping for the letter task above.
                                                                       (4 units) and a single non-linearity (a rectifier at the output
   This decomposition tells us more about the task structure
                                                                       layer) to solve this task. See fig. 3 for a diagram of the
the network is using. There are three modes in the SVD. The
                                                                       network. No biases were used, weights were initialized uni-
first (left output mode/top input mode) represents the differ-
                                                                       formly between 0 and 0.1, all training was done by Stochastic
ence between the Latin and Greek letters, so it is positive for
                                                                       Gradient Descent (i.e. in each epoch the data are presented
the Greek inputs and negative for the Latin outputs, and is
                                                                       one at a time in a random order, and the weights are updated
positive for the Greek outputs and negative for the Latin out-
                                                                       after each data point) with η = 0.01 for 500 epochs.
puts. The next two components represent the distinctions be-
tween the letters R and L, and the letters ρ and λ, respectively.      Results
Saxe et al. (2013) showed these results have implications for
the learning of non-linear networks as well, so linear neural          The solution that the nonlinear network discovers the major-
networks can be a more tractable place to analyzee learning            ity of the time (about 75%) is to output the same pattern on
dynamics. In addition, using the I/O SVD allows the dis-               both sets of output units, but offset the “incorrect” domain
covery of representational components which are distributed            sufficiently negative so that it is hidden by the rectified, thus
across units, so it is more general than simply examining what         the task that the linear portion of the network is effectively
aspects of the task individual hidden units represent, or ex-          performing at convergence is:
amining the weight matrices directly. Thus one might hope                                     P    D    S   π    δ     σ
to answer our questions in a linear framework.                                           R    1    1    0    0   0    −1
   However, linear networks cannot represent analogous                                   L    1    0    1    0  −1     0
structure from non-overlapping inputs and outputs at conver-                             ρ    0    0   −1    1  1      0
                                                                                         λ    0   −1    0    1  0      1
gence. With non-overlapping inputs and outputs, the I/O cor-
relation matrix is block diagonal, and the SVD modes will                  1 Where there are duplicated singular values, the SVD is not
thus occur within blocks (this is why in Fig. 1 the modes              unique, so more precisely we mean there exists a basis which makes
showing separation between the letters in each domain have             the SVD is block diagonal.
                                                                   2513

 (Note that the network can actually map the first element
of one domain onto either element of the other. We discuss                                                                          
                                                                                                         1  0.5   0.5   0     0    0
the solution shown here one for clarity, the other just shuffles                                       1   0.5   0.5   0     0    0
some rows and columns.)                                                        base rates by domain = 0
                                                                                                                                     
                                                                                                             0     0    1    0.5  0.5
   The SVD of this linearized mapping shows a rank 2 solu-                                               0   0     0    1    0.5  0.5
tion (see fig. 2). The first component is similar to the first
component of the regular SVD, in that it reflects the separa-          Finally it learns the internal structure of the domains (they
tion of the domains, but the second component collapses the           are not learned at exactly the same time, which is learned first
other two components of the linear SVD. In other words, the           depends on the initilization). Around epoch 400 it has solved
analogy has been learned – the network is using the parallels         the task completely, with some sort of offset structure in the
between the two tasks to reach a more parsimonious solution.          non-linear case, or without in the linear case:
                                                                                                                                    
It is able to incorporate the analogy into its computations by                                          1    1     0   0      0   −1
                                                                                                       1    0     1   0     −1    0
allowing both the sets of outputs to vary, and simply suppress-                solution with offsets = 
                                                                                                       0    0   −1    1      1    0
                                                                                                                                     
ing the outputs from the “wrong” domain for its current task.                                           0  −1      0   1      0    1
   Because this solution is rank 2, a non-linear network with
                                                                       For most of the learning process, the networks are extract-
two hidden units should be able to solve the task, whereas a
                                                                      ing similar structure, so one might expect that even the linear
linear network will require three. We have verified these re-
                                                                      network would show some representation of the analogy at in-
sults empirically for this task. Thus the ability of a non-linear
                                                                      termediate stages of learning. Indeed, once the base rates by
neural network to extract common structure from multiple
                                                                      domain are learned, both the linear and non-linear networks
tasks can allow it to find more parsimonious (i.e. lower-rank)
                                                                      begin to extract the analogy between the domains. See fig. 4
solutions. We would like to highlight this point: the represen-
                                                                      for a plot of how much each domain’s input mode projects to
tation of the analogy in the SVD is not purely epiphenomenal
                                                                      the other domain’s output mode, i.e. “cross-talk” between
– it makes a more parsimonious solution possible.
                                                                      the domains. This is a simple measure of the extent to which
                                                                      the network is extracting shared structure. However, while
Evolution of the I/O Mappings                                         both networks develop some representation of the analogy
When a non-linear network has only two hidden units, it must          initially, this activity extinguishes rapidly in the linear net-
extract the analogy to be able to solve the task, but with more       work, while it persists in the non-linear network.
hidden units there are a variety of solutions that could poten-          Why do both networks show some representation of the
tially emerge (such as just learning the mapping of each input        analogy initially? We will analyze this in the linear case. At
to its output pattern independently). However, our network            the stage when the base rates by domain have been learned,
extracted shared structure on about 75% of the runs we con-           adding a little bit of shared structure actually reduces mean-
ducted (as measured by more than 20% score on the cross-              squared error (MSE). If the network moves from the base
projection metric described below). What drives this fairly           rates by domain pattern to the pattern shown below, the small
consistent extraction of analogy? In this section we consider         increase in MSE from the ±0.1 values is more than offset by
the evolution of the outputs over the course of learning.             the decrease from splitting the 0.5 values into 0.4 and 0.6.
                                                                                                                             
   The output structure of the network goes through a fairly                             1    0.6     0.4   0   0.1     −0.1
consistent progression, which we will describe qualitatively                           1
                                                                                             0.4     0.6   0   −0.1      0.1 
                                                                                                                              
                                                                                       0     0.1    −0.1   1    0.6      0.4 
at various key stages (the exact values depend on the ini-
                                                                                         0   −0.1     0.1   1   0.4       0.6
tialization, so the matrices here are approximations to within
about ±0.1). The outputs begin as small positive numbers,              Indeed, suppose there is a hidden unit which responds dif-
approximately 0 (because the weights are initialized uni-             ferentially within the domains (as they all will to some extent
formly between 0 and 0.1). Next, the network captures the             because of the random initialization). The updates of the out-
base rate activations of each output unit, around epoch 75.           put weights for this unit will point in the direction of analogy
(Note that this is already accounted for in the SVD, because          extraction once the base rates by domain have been learned.
the output variables are centered before computing the SVD).          See below for the output error, hidden unit activity, and cor-
                                                                      responding weight updates in the case that the hidden unit
                      
                       0.5 0.25  0.25  0.5   0.25   0.25
                                                                     responds positively to the first element of each domain, and
                       .    ..    ..   ..     ..     ..             negatively to the other.2 (Note that the output weight updates
         base rates =  ..    .     .    .      .      .             for a hidden unit are proportional to the product of the output
                       0.5 0.25  0.25  0.5   0.25   0.25
                                                                      error and the hidden unit’s activation.)
 Then the network captures the existence of the two domains               2 In the general case representations will be distributed across the
but not the structure within them (around epoch 140). This            hidden units, and so there will not be a unit which responds to the
                                                                      analogy and nothing else, but this is simply a rotation of the repre-
corresponds to the first component of either SVD. Up to this          sentation space, and because of the linearity of derivatives the same
point, a linear network follows a similar learning trajectory.        general pattern will emerge.
                                                                  2514

                             =                                 ×                                 ×
(a) Input-output mapping               (b) Output modes Unl            (c) Singular values Snl           (d) Input modes Vnl
(transposed from text)
      Figure 1: SVD of I/O correlation matrix (colors are scaled to show qualitative features, red = +, white = 0, blue = -)
                             =                                 ×                                 ×
   (a) Input-output mapping            (b) Output modes Ulz            (c) Singular values Slz           (d) Input modes Vlz
Figure 2: SVD of linearized I/O correlation matrix (colors are scaled to show qualitative features, red = +, white = 0, blue = -).
Note how Fig. 2a becomes Fig. 1a if the negative values are hidden by a nonlinearity.
Figure 3: Simple task network, showing a sample propagation
of an input through the network with the single non-linearity     Figure 5: Family trees from Hinton (1986), (reproduced with
at the output. (Circles represent inputs or fully connected       permission).
units, squares represent non-linearities.)
                                                                  Figure 6: Family tree task network (Circles represent inputs
Figure 4: I/O SVD component cross-projection (dot product         or fully connected units, squares represent non-linearities. El-
between output mode of an SVD component and the response          lipses denote units omitted from the diagram – the hidden
of the network to the other domain’s input mode)                  layer and all input and output groups had 12 units apiece.)
                                                              2515

                                                                      We denoted a mode as showing significant extraction of the
                                                                      analogy if it showed a stronger similarity between the weights
            output error         unit  unit output weight updates
                                                                      for the two families’ inputs than 95% of its permutations did.
     0   +    − 0 0 0             +    0 + − 0 0               0
     0   −    + 0 0 0             −    0 + − 0 0               0      We repeated this analysis across 100 network initilizations.
     0    0   0 0 + −             +    0 0 0 0 + −
     0    0   0 0 − +             −    0 0 0 0 + −                    Results
             net output weight update: 0 + − 0 + −                    We found a great deal of analogous structure was extracted.
                                                                      The runs had a median of 4 modes showing significant analo-
 Summing these updates captures the analogy between the               gous structure extraction, and all the runs had at least one sig-
domains. The network will exploit this analogy to reduce              nificant mode (for comparison, if 5% of the modes showed
error, even if it must eventually discard it in the linear case.      significant results by chance, we would still expect 54% of
                                                                      the runs to yield no significant results). To account for the
   Reanalyzing Hinton’s Family Tree Example                           symmetry of the tree under flipping, we repeated the same
Next, we briefly turn our attention to the example of Hinton          analysis after permuting the second family’s input columns
(1986). Hinton’s task involves learning two isomorphic fam-           appropriately. Since the network has no way to distinguish
ily trees, one English and one Italian (see fig. 5). This struc-      the “regular” mapping from this “flipped” mapping during
ture is taught implicitly by presenting a person (e.g. “Jen-          learning, we would expect to see a similar frequency of sig-
nifer”) and a relationship (e.g. “Father”), and training the          nificant modes for each, and indeed the distributions are sim-
network to produce the correct target person (“Andrew” in             ilar. Furthermore, the runs had a median of 6 modes showing
this case). There are 52 such relationships per family.               significant extraction of either the regular or flipped mapping,
                                                                      and in all of the runs it had extracted 3 or more components
Methods                                                               that significantly represent one analogy or the other (if we as-
Hinton used the same inputs for type of relationship for both         sume 5% false positives, we would expect results this extreme
families. To highlight the extraction of analogous structure          in only 0.01% and 3% of the runs, respectively). See fig. 7.
we separated these into distinct input banks (these could be          The frequency of analogy extraction suggests this may be a
thought of as the English and Italian words for different rela-       central feature of how neural networks solve tasks.
tions, e.g. “uncle” vs. “zio” ). We also reduced his network             Although we have focused on broad analogies between the
down from 3 hidden layers to a single hidden layer with 12            families here, we would like to note that analyzing the SVDs
units. Unlike the simple problem above, this problem is not           can give more detail. In some cases modes reflect an analogy
linearly separable, so we included a non-linearity at the hid-        only in the “person” inputs, or only in the “relationship” in-
den layer as well as the output (see fig. 6). We trained this         puts. Within a family, analyzing the SVD modes can outline
network by SGD with η = 0.005 for 1000 epochs.                        the structure the network is extracting, e.g. modes often ap-
   In a task which requires multiple non-linearities, we cannot       pear which represent the gender of the target of a relationship
perform as simple an analysis as in the earlier task. However,        like “mother”. We have omitted these analyses due to length
by definition each layer of the network has only a single non-        constraints.
linearity, and so we can perform an analysis like the above on
each layer. In this way we can understand something about                                        Disussion
the computations that layer is performing. However, the in-           We have outlined a new technique for analyzing neural net-
terpretation will not be as simple as above.                          work representations and their learning dynamics: analyz-
   This difficulty is compounded by the complexity of the             ing the SVD of the “linearized” mapping at each layer (i.e.
structure being learned in each family. In the simple problem         the mapping from the inputs to the pre-nonlinearity activity).
above it was possible to “eyeball” the structure extraction, but      This allows us to bring the power of linear analyses to bear on
here the structure is too rich. There are a variety of possible       the rich phenomena that occur only in non-linear networks.
ways the families can be mapped onto one another (e.g. flip-             Using this technique, we have explored how a simple neu-
ping the tree left to right and swapping all genders), and it’s       ral network can extract the analogy between simple tasks with
possible that the networks are extracting overlapping struc-          non-overlapping inputs and outputs. We showed that, while
ture from several of these analogies. In this setting, how can        a linear network cannot represent analogies, a single non-
we examine whether the network is learning the analogy?               linearity at the output layer can allow the network to represent
   As a first test of this, we looked for representation of the       the analogy, and that this structure emerges naturally (even in
analogy in the input modes of the first layer SVD. To do              a linear network) from gradient descent once the base rates
this, we computed the dot product of each mode’s weights              by domain have been learned. A linear network must discard
for one family with that mode’s weights for the other family,         this analogy to reach its optimal solution, but a non-linear
and then tested how significant this similarity was by com-           network is able to retain it by simply offsetting the outputs
paring it to the null distribution generated nonparametrically        to a sufficiently negative value, and does so the majority of
by randomly permuting the columns of the input mode matrix            the time in our results. Here we used rectifiers, but the same
1000 times and computing the same dot product for each one.           general solution is achievable with other nonlinearities.
                                                                  2516

                                                                                         Acknowledgments
                                                                     This material is based upon work supported by the NSF GRF
                                                                     under Grant No. DGE-114747.
                                                                                             References
                                                                     Bransford, J. D., & Schwartz, D. L. (1999). Rethinking
                                                                       Transfer : A Simple Proposal With Multiple Implications.
                                                                       Review of Research in Education, 24(1), 61–100.
                                                                     Day, S. B., & Goldstone, R. L. (2011). Analogical Transfer
                                                                       From a Simulated Physical System. Journal of Experimen-
                                                                       tal Psychology: Learning, Memory, and Cognition, 37(3),
                                                                       551–567. doi: 10.1037/a0022333
                                                                     Detterman, D. K. (1993). The Case for the Prosecution:
 Figure 7: How many of the input modes from the SVD                    Transfer as an Epiphenomenon. In Transfer on trial: In-
 showed significant projection onto the regular or flipped             telligence, cognition, and instruction (pp. 1–24).
 analogies (with null distribution for comparison)                   Dienes, Z., Altmann, G. T. M., & Gao, S.-J. (1999). Map-
                                                                       ping across Domains Without Feedback: A Neural Net-
                                                                       work Model of Transfer of Implicit Knowledge. Cognitive
    We then broadened our approach to explore the family tree          Science, 23(1), 53–82. doi: 10.1207/s15516709cog2301
 task originally proposed in Hinton (1986). Because this task        Dong, D., Wu, H., He, W., Yu, D., & Wang, H. (2015). Multi-
 is not linearly separable, we created a general network with          Task Learning for Multiple Language Translation. Acl,
 two nonlinear layers, and applied our analysis to each layer.         1723–1732.
 We found evidence of a great deal of extraction of two pos-
                                                                     Gentner, D. (2003). Why We’re So Smart. In Language in
 sible analogies between the families in the network (either
                                                                       mind: Advances in the study of language and thought. (pp.
 the intended isomorphism between the family trees, or one
                                                                       195–235).
 in which one family tree was flipped left-to-right and gender-
 reversed), and that networks seemed generally to be discov-         Gick, M. L., & Holyoak, K. J. (1980). Analogical Problem
 ering elements of both analogies. Indeed, representation of           Solving. Cognitive P, 12, 306–355.
 the analogies seemed even more common than on the simpler           Hinton, G. (1986). Learning distributed representations of
 task. On the simple task 25% of the networks showed no evi-           concepts. doi: 10.1109/69.917563
 dence of common structure extraction, but on the family tree        Johnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y.,
 task every network extracted at least three input modes that          Chen, Z., . . . Dean, J. (2016). Google’s Multilingual
 projected significantly onto one of the analogies.                    Neural Machine Translation System: Enabling Zero-Shot
    These results suggest that sensitivity to analogy may be a         Translation. arXiv, 1–16.
 natural feature of gradient based learning in nonlinear neural      Kollias, P., & McClelland, J. L. (2013). Context, cortex, and
 networks. This may underlie many of the “slow” analogical             associations: A connectionist developmental approach to
 transfer effects we highlighted in the introduction. Further-         verbal analogies. Frontiers in Psychology, 4(NOV), 1–14.
 more, this may be a part of why learning multiple tasks facil-      Luong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., & Kaiser,
 itates more rapid learning and better performance in machine          L. (2016). Multi-task Sequence to Sequence Learning.
 learning systems, and it may have important implications for          Iclr, 1–9.
 cognition. The power and generality of human cognition may          Mirsky, L. (1960). Symmetric gauge functions and unitarily
 result from extracting common structure from the diverse but          invariant norms. The Quarterly Journal of Mathematics,
 deeply related tasks we engage in throughout our lives.               11(1), 50–59. doi: 10.1093/qmath/11.1.50
 Future Directions                                                   Rogers, T. T., & McClelland, J. L. (2008). A simple model
                                                                       from a powerful framework that spans levels of analysis.
1. In our analysis we analyzed the input modes of the first            Behavioral and Brain Sciences, 31, 729–750.
    layer and the output modes of the second layer. In the fu-       Rusu, A. A., Gomez Colmenarejo, S., Gulcehre, C., Des-
    ture it will be important to explore modes that map into and       jardins, G., Kirkpatrick, J., Pascanu, R., . . . Hadsell, R.
    out of the hidden layer, and what they imply about the rep-        (2015). Policy Distillation. arXiv, 1–12. doi: 10.1038/na-
    resentations at the hidden layer. This would also allow us         ture14236
    to apply this analysis to deep networks.                         Saxe, A. M., McClelland, J. L., & Ganguli, S. (2013). Ex-
2. Learning representations that reflect analogies may provide         act solutions to the nonlinear dynamics of learning in deep
    amortized inference about potential analogical structure in        linear neural networks. Advances in Neural Information
    the world. Can this support explicit analogical reasoning?         Processing Systems, 1–9.
                                                                 2517

