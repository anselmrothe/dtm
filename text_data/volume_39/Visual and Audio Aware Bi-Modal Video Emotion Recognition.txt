                   Visual and Audio Aware Bi-Modal Video Emotion Recognition
                                                  Siqi Xiang (sqxiang@buaa.edu.cn)
                                                 Wenge Rong (w.rong@buaa.edu.cn)
                                                 Zhang Xiong (xiongz@buaa.edu.cn)
                      School of Computer Science and Engineering, Beihang University, Beijing 100191, China
                                                    Min Gao (gaomin@cqu.edu.cn)
                                                Qingyu Xiong (xiong03@cqu.edu.cn)
                          School of Software Engineering, Chonqing University, Chongqing 401331, China
                              Abstract                                 ture that can carry information from two different modalities
   With rapid increase in the size of videos online, analysis and
                                                                       (domains) at the same time. Such methods can be roughly di-
   prediction of affective impact that video content will have         vided into two categories in terms of the way the features are
   on viewers has attracted much attention in the community.           combined, i.e., later fusion of classifiers (Yi, Wang, Zhang,
   To solve this challenge several different kinds of information      & Yu, 2015), and early fusion scheme, in which features
   about video clips are exploited. Traditional methods normally
   focused on single modality, either audio or visual. Later on        are concatenated into a final classifier (Dai et al., 2015; P.,
   some researchers tried to establish multi-modal schemes and         Hayrapetyan, Tapaswi, & Stiefelhagen, 2015).
   spend a lot of time choosing and extracting features by differ-
   ent fusion strategy. In this research, we proposed an end-to-          In this research, we employed the idea of modal fusion and
   end model which can automatically extract features and target       then proposed an end-to-end framework to integrate the vi-
   an emotional classification task by integrating audio and vi-       sual and audio features for video emotion analysis. Recently
   sual features together and also adding the temporal character-
   istics of the video. The experimental study on commonly used        with the development of deep learning techniques, a lot of
   MediaEval 2015 Affective Impact of Movies has shown this            advanced methods have been proposed for feature extraction.
   method’s potential and it is expected that this work could pro-     In this research, we used convolutional neural network (CNN)
   vide some insight for future video emotion recognition from
   feature fusion perspective.                                         to extract video emotion related features as CNN has proven
                                                                       its success in learning intermediate representations from low-
   Keywords: videos; multi-modal scheme; modal fusion; end-
   to-end; temporal characteristics                                    level features (Acar, Hopfgartner, & Albayrak, 2014). Af-
                                                                       terwards, taking into account the temporal characteristics of
                          Introduction                                 video, we further use Long Short Term Memory (LSTM)
To better understand and analyse people’s emotion response             model (Hochreiter & Schmidhuber, 1997) to integrate the ex-
during watching videos, it is essential to study the cognitive         tracted temporal features since it performs well on tasks that
determinants beneath the video presentation. Currently, con-           require integration of state information over time. Finally a
tent based approaches are the main trend for video emotion             multi-layer perceptron (MLP) is employed to classify the fi-
analysis, and a lot of models have been proposed to help iden-         nal video emotions.
tify the emotions evoked by videos (Hanjalic, 2006), among                To confirm the validity of the proposed method, we imple-
which affective analysis based on video visual contents have           ment it in the Affective Impact of Movies Task 3 in the Medi-
been studied for several years. Several approaches which em-           aEval challenge 2015 (Sjöberg et al., 2015). The task has now
ployed different machine learning models such as Bayesian              become a state-of-the-art benchmark which attracted a large
network (Soleymani, Kierkels, Chanel, & Pun, 2009), Hid-               number of research teams to test their models on this data set.
den Markov Models (Kang, 2003) have been proposed and                  The experimental study result against different bench experi-
proven applicable to tackle with this challenge.                       ments on this dataset shows the proposed method’s potential
   Though visual content based video emotion analysis has              in detecting video’s emotion.
proven applicable in real applications, there still exists chal-
lenges since even the same scene could cause different emo-                                   Related Work
tions (Choe, Chun, Noh, Lee, & Zhang, 2013). Recently au-              In the content-based video research, many researchers have
dio related features have also proven its effectiveness in emo-        used a lot of models to identify the emotions triggered by
tion analysis (Cui, Jin, Zhang, Luo, & Tian, 2010). For ex-            the video. Hanjalic argued the possibility to classify films
ample, Xu et al. tried to use audio emotional events (AEE)             according to their emotions and proposed the concept of “ex-
such as laughing, horror sounds and other features to detect           pectation of emotion”, which is defined as one or a group
horror and comedy movies (Xu, Chia, & Jin, 2005).                      of emotions that a filmmaker wishes to use to communicate
   While previous studies focused on video or audio features           with a certain culture or a particular audience through the
alone in detecting video emotion have proven their ease in             film (Hanjalic, 2006). Through this concept, he proposed the
implementation, to further improve the classification perfor-          video content information and its underlying characteristics
mance, some researchers indicate the possibility by combin-            to predict emotion. Later on, Soleymani et al. proposed a
ing visual features with audio features to form a hybrid fea-          Bayesian framework to detect scene affect and the arousal
                                                                   3554

                   Video segment and low-level feature extraction                               Visual and audio feature fusion                          Temporal feature integration and
                                                                                                                                                               emotion analysis
                                                                                                           Visual feature
                                                                                                           hidden layer
                                                                                                                 V0
                                                           Alexnet finetune
                                                                                                                 V1                      Ct-1       ht-1
                                                                                                                                 f0
                                                                                                   … ...
                                                                                                                                                             LSTM unit                    Mulit-layer Perception
                                                                                                                 ...
             Key frame      Visual image
                                                                                                                                                                                              h0
                                             3     96    256      384      384    256                           +1               f1              Input                      Output                     h0
                                           input conv-1 conv-2   conv-3   conv-4 conv-5 fc6       fc7                                            Gate                        Gate             h1
                                                                                                                                                             it                      ot                        y0
                                                                                                                                                    gt                                                 h1
     Video                                                                                                                                                         +                          h2
                                                                                                                 a0                      Input Modulation                                                      y1
                                                                                                                                                                                                       ...
                                                                                                                                 ...           Gate
                                                                                                                                                    ft                                        ...
                                                                                                                                                                                                               y2
                                                                                                   … ...         a1                                                                                    +1
                                                                                                                                           Forget                                             hn
                                                                                                                                            Gate
             Audio signal       Audio
                             spectrogram                                                                         ...
                                             3     96    256      384      384    256                                            fn
                                                                                                                                                                       Ct
                                           input conv-1 conv-2   conv-3   conv-4 conv-5           fc7
                                                                                          fc6
                                                                                                                +1
                                                                                                                            Fusion
                                                                                                                            feature
                                                                                                           Audio feature
                                                                                                           hidden layer
                                    Figure 1: Visual and Audio Aware Video Emotion Analysis Framework
and valence values with content features were used to clas-                                                used convolution neural network to capture visual features to
sify video emotions into 3 classes, i.e., calm, excited positive                                           classify video into seven emotions (Levi & Hassner, 2015).
and excited negative (Soleymani et al., 2009). Similarly Ar-
ifin and Cheung established a framework based on the hier-                                                                             Proposed Approach
archical coupling of dynamic Bayesian networks to establish
the dependencies of the pleasure-activity-dominance emotion                                                The overall pipeline of the proposed visual and audio aware
model (Arifin & Cheung, 2008).                                                                             emotion analysis framework is depicted as Fig. 1, where the
   There are also various studies on video affective charac-                                               whole process is divided into three steps: 1) video segment
terization using audio features, e.g., rhythm, tempo, mel-                                                 and low level feature extraction; 2) bi-modal visual and audio
frequency cepstral coefficients (MFCC), pitch, zero crossing                                               feature fusion; and 3) temporal feature integration and emo-
rate. For example, three feature sets, i.e., intensity, timbre and                                         tion classification.
rhythm were extracted from audio to classify video emotion
using Gaussian mixture models (Lu, Liu, & Zhang, 2006).                                                    Video segment and low level feature extraction
Similarly, Xu et al. tried to use audio emotional events (AEE),
                                                                                                           To analyze video emotion, it is necessary to firstly divide a
including laughing, horror sounds and other features to detect
                                                                                                           video into short videos with a length of t seconds. In this
horror and comedy movies (Xu et al., 2005).
                                                                                                           study we set t = 1 so that a video of length T will have T
   In fact, there is a complex interaction between the audio                                               slices. This segmentation has two benefits. First, since the
and visual contents to determine the perceived mood. As                                                    length of each video is different, this segmentation gives us
such the video emotion analysis has begun to use feature                                                   better access to the visual and audio features. Second, Be-
fusion method to classify emotion into different classes (Yi                                               cause of the temporal characteristics of the video, cutting the
et al., 2015). Similarly, Trigeorgis et al. selected the low                                               video into the same segments can be used for subsequent re-
level descriptors with the traditional adaboost as a classifier                                            current neural networks.
(Trigeorgis et al., 2015). Wang and Cheong derived the char-
                                                                                                              For each segment, we need to extract its visual and audio
acteristics of multimodality by probabilistic inference based
                                                                                                           features separately. As to the visual features, we extract the
on two SVM models (Wang & Cheong, 2006), where one
                                                                                                           k key frames for each segment. Due to the strong correla-
SVM model is designed to process audio data and extracts
                                                                                                           tion among frames within a second, we select k = 1. The
the corresponding advanced audio information, while another
                                                                                                           key frame is defined as the frame with the closest RGB his-
SVM model is used to classify the captured video segments.
                                                                                                           togram to the mean RGB histogram of the whole video clip
   However, since these framework extracts basic features,                                                 using the Manhattan distance (Zhu, Jiang, Peng, & Zhong,
they lack the ability to use raw inputs to automatically learn                                             2016). Assume that a video clip V contains n frames, the
mid-level representations. With the development of deep                                                    RGB histogram of i-th frame is defined as h(i). The Man-
learning techniques, some deep learning based approaches                                                   hattan distance D between two frames i and j is calculated as
are also proposed in the literature. For example, Kahou et                                                 follows:
al. used a deep convolution neural network to analyse facial
expressions within a frame and used a deep belief net to cap-
ture audio feature (Kahou et al., 2016). Levi and Hassner also                                                                          D(i, j) = |h(i) − h( j)|                                               (1)
                                                                                                 3555

and the key frame will be:                                           Temporal feature integration and emotion analysis
                                        n                            Though previous steps we have obtained fused features from
                                    1                                visual and audio perspective, there is still a challenge about
                      arg min D(i,     ∑ h( j))              (2)
                           i        n  j=1                           how to predict corresponding emotion status. Furthermore,
                                                                     in previous step the features are about a single frame, taking
   After getting the key frame, it will be resized to 256 ∗ 256      into account the temporal characteristics of video, it is nec-
pixel, as suggested in (Krizhevsky, Sutskever, & Hinton,             essary to study how these features can be used over time. In
2012) as input for fine-tuning. The concept of fine-tuning           this research we will use the LSTM model to fuse sequence
is to use a model pre-trained on a large dataset, replacing its      features together.
last layers, and fine-tune the weights on new task using back-          Recurrent Neural Networks (RNNs) are powerful networks
propagation. In this study, AlexNet (Krizhevsky et al., 2012)        and it can model input sequences of different lengths, be-
is employed. AlexNet consists of five convolution layers and         cause the parameters of the network can be shared over dif-
three fully connected layers. Here we select the fc7 layer of        ferent parts (Mikolov, Karafiát, Burget, Cernocký, & Khu-
AlexNet which has 4096 neurons as our visual features.               danpur, 2010). RNNs are often trained by Back-Propagation
   As to the audio features, the traditional methods for audio       Through Time (BPTT) algorithm, but the main problem with
emotion analysis need to select proper audio features, e.g.,         the BPTT is that the gradients tend to vanish or explode which
MFCC, energies, flatness, and etc. But they often have to con-       was resulted by propagating the gradients down through lay-
duct a lot of repeat tests to choose the best features. In order     ers. Therefore it is difficult to learn efficient long-term de-
to take full advantage of the depth convolution neural network       pendencies. To overcome this limitation, the Long-Short-
model in extracting data features, the original features of the      Term-Memory (LSTM) (Hochreiter & Schmidhuber, 1997)
data should be kept as much as possible in order to avoid            units have been created to capture long-term dependencies.
losing information. In this research, we process the audio           LSTMs have the ability to remove or add information to the
to spectrogram (Barker & Virtanen, 2016), which is a visual          cell state through a well-designed structure called a “gate”. It
representation of the spectrum of frequencies in a sound. We         is believed that the LSTMs can model the temporal aspect
set the window function to 40ms and the hop size to 20ms to          of induced emotions in our task. Various units have been
generate a spectrogram every second using short-time Fourier         proposed in the community to constitute a LSTM. In this re-
transform with a Hamming window (Allen, 1977). The re-               search, we employed the LSTM units described in (Zaremba
sulting image is resized to 256 ∗ 256 pixels, here we also use       & Sutskever, 2014). The LSTM unit of time step t consists
the method of AlexNet finetune to extract the fc7 layer as a         of three sigmoidal gates, i.e., input gate it , output gate ot , for-
feature of the spectrogram.                                          getting gate ft . The most important part of the LSTM unit is
                                                                     a linear self-loop state cell ct . The memory cell unit ct is a
Bi-modal visual and audio feature fusion                             sum of two terms: the previous memory cell unit ct−1 which
From the last step we obtained visual and audio features for         is modulated by ft , and gt , a function of the current input and
video emotion analysis. However, the length of features of           previous hidden state, modulated by the input gate it. ht de-
both visual and audio is long and there maybe many redun-            notes the hidden layer’s output at step t. We can update our
dancy in the features. It will be helpful if we can combine the      hidden layer for time step t as follows:
two types of features and then reduce the overall dimension.
   Let xa ∈ RD denotes audio features and xv ∈ RD denotes                               it = σ(Wxi xt +Whi ht−1 + bi )                  (6)
visual features, where D ∈ R is the dimension of audio and                             ft = σ(Wx f xt +Wh f ht−1 + b f )                (7)
visual features, the joint representation of features by fusion
modal can be written as:                                                               ot = σ(Wxo xt +Who ht−1 + bo )                   (8)
                                                                                     gt = tanh(Wxc xt +Whc ht−1 + bc )                  (9)
                 x f = αa g(xa ; wa ) + αv g(xv ; wv )       (3)
                                                                                           ct = ft    ct−1 + it   gt                   (10)
where g(.) denotes the hidden layer of both audio and visual                                  ht = ot   tanh(ct )                      (11)
channel. αa defines the weights of audio features and αv de-
fines the weights of visual features at the same time. The           where xt is the current fusion feature, ht−1 is the previous
hidden layer of audio features is:                                   hidden layer vector. x y denotes the element-wise product
                                                                     of vectors x and y. In addition, Wxi , Wx f , Wxo , Wxc , Whi , Wh f ,
                   g(xa ; wa ) = θ((wa , xa ) + ba )         (4)     Who , Whc are weights for the gates, and bi , b f , bo , bc are biases
                                                                     for the gates. σ is the nonlinear methods (e.g., sigmoid or
where θ denotes the activation function (rectified linear units      tanh).
(Zeiler et al., 2013), sigmoid etc.) of the audio hidden layer.         The output of the last time step of LSTM unit will be the
Similarly the hidden layer of visual feature is:                     input of the fully connected neural network, also known as
                                                                     multi-layer perception (MLP).The hidden layers and parame-
                    g(xv ; wv ) = θ((wv , xv ) + bv )        (5)     ters of MLP will discuss in experiment. The prediction layer
                                                                 3556

will have 3 units yl (l = 0, 1, 2) and the class probability is       where α indicates the relevant importance between audio and
calculated by taking the softmax as below:                            visual features. In this research we set α = 0.56, as indicated
                                                                      in (Goyal, Kumar, Guha, & Narayanan, 2016).
                                     exp(yl , c)                         Afterwards we also compare our results against state-of-
                yl : p(yl = c) =                  0         (12)
                                 ∑c0 ∈C exp(yl , c )                  art systems in the MediaEval 2015 challenge. These systems
                                                                      include: later fusion models with manually selected features
where C denotes the three emotion states. Finally the label           (Yi et al., 2015; Chakraborty et al., 2015), early fusion mod-
with the max probability will be the expected label.                  els with manually selected features (P. et al., 2015; Trigeorgis
                                                                      et al., 2015), later fusion models with automatically selected
                     Experimental Study                               features (Tiwari et al., 2016), early fusion models with au-
Dataset                                                               tomatically selected features (Dai et al., 2015; Seddati et al.,
In order to fairly verify the performance of our proposed             2015).
method, we implement it on the dataset provided by Me-
                                                                      Experiment Settings
diaEval 2015 Afective Impact of Movies task (Sjöberg et
al., 2015), which consists of 10,900 short video clips ex-            We tested the different feature dimensions and found that the
tracted from 199 Creative Commons-licensed movies of var-             final result did not change much in the range of 250 to 1000.
ious genres. It is an extension of the LIRIS-ACCEDE dataset           We decided to use feature size of 512 for both visual pathway
(Baveye, Dellandréa, Chamaret, & Chen, 2015), which origi-           and audio pathway. Therefore the fusion feature as the final
nally contains 9,800 excerpts extracted from 160 movies.The           LSTM model input has 512 dimensions. LSTM model can
MediaEval 2015 task added 1,100 video clips additionally              handle different video length, the longest video is 18 seconds
from 39 movies. The dataset is divided into training set and          that is 18 time steps. The system is trained end-to-end to pre-
test set. The training set consists of 6,144 videos extracted         dict the videos emotion class at each time step. It is found
from 100 movies while the test set includes 4,756 videos              that the most significant parameter is the number of LSTM
extracted from the remaining 99 movies. These videos last             hidden layers. We compared LSTM networks with 64, 128,
from 8 to 12 seconds and start and end with a cut or fade.            256, and 512 hidden units, separately. Finally, we found that
The ground truth for each of 10,900 video clips consists of           256 hidden units can be selected to achieve the best results,
discrete labels for arousal (calm-neutral-active) and valence         as shown in Fig. 2. Afterwards we selected MLP as our clas-
(negative-neutral-positive).                                          sifier in which rectified linear units were used as nonlinear
                                                                      functions and stochastic gradient descent with minibatches
Evaluation Metrics & Baseline                                         was used for parameter updates (Zeiler et al., 2013). Also we
In order to evaluate the affective detection task, the offi-          used categorical cross-entropy loss function to get the best
cial and complete method is global precision (Sjöberg et al.,        results. The hidden layer uses dropout to prevent overfitting,
2015), which is the proportion of the number of correctly as-         and the factor is set 0.5. The number of hidden layer of MLP
signed videos in the total video samples and is defined as:           and the units’ number can also affect the model results, and
                                                                      ultimately we chose one hidden layer with 64 hidden units.
                        Precision = Nc /Nt                  (13)
                                                                                 55.95
where Nc is the number of videos which are assigned to the                        55.9
correct class, and Nt is the total number of test videos. In this                55.85
research, we only compare the results obtained for the arousal                    55.8
classification. This is because compared to arousal, valence                     55.75
is not sensitive in the dataset. As such comparing the results                    55.7
of the arousal classification is a commonly adopted choice                       55.65
(Sjöberg et al., 2015).                                                          55.6
                                                                                         64       128     256        512
   To evaluate applicability of the model fusion approach, in
this research we compared it against the proposed approach
in predicting arousal values using only the image features or         Figure 2: Arousal Accuracy with Different Number of Hid-
audio features. Furthermore, we also compared the proposed            den Layer Units
approach against early fusion and later fusion methods, re-
spectively. In the early fusion model we simply concatenate
the audio and video features together, while in later fusion          Result and Analysis
schema, we firstly trained two MLP classifiers to represent           Table 1 presents the proposed method’s performance using
the two modalities separately. Their predictions are denoted          different feature space and fusion strategy. It is observed
as pa and pt and the overall output emotion class can be as-          that the performance of all feature fusion strategies are bet-
signed by                                                             ter than using only single feature. It is because that video
                       p = αpa + (1 − α)pt                  (14)      images are the main cause of people’s emotions, but audio
                                                                  3557

can complement the lack of information in video images. It            include information from other domains/modalities, e.g., ab-
is further found that our proposed model fusion method is             stract words (Siakaluk, Knol, & Pexman, 2014), which de-
better than both the simple early fusion and late fusion, af-         serve future study in the future work.
firming the effectiveness of multi-modal emotion classifica-
tion. This maybe because early fusion leads to the sparsity of                              Acknowledgment
input vectors and late fusion has little consideration for visual     This work was partially supported by the National Natural
and audio’s correlation (Williams et al., 2009).                      Science Foundation of China (No. 61332018), the National
                                                                      Department Public Benefit Research Foundation of China
                                                                      (No. 201510209), and the Basic and Advanced Research
 Table 1: Comparison of accuracy by different fusion models           Projects in Chongqing (No. cstc2015jcyjA40049).
              Approaches           Arousal Accuracy(%)
          Visual features only             55.51                                                References
          Audio featues only               55.14                      Acar, E., Hopfgartner, F., & Albayrak, S. (2014). Under-
             Early fusion                  55.71                         standing affective content of music videos through learned
             Later fusion                  55.70                         representations. In Proceedings of the 20th Anniversary
             Modal fusion                  55.89                         International Conference on MultiMedia Modeling, Part I
                                                                         (pp. 303–314).
                                                                      Allen, J. (1977). Short term spectral analysis, synthesis, and
    Table 2 is the experimental result of the propose method             modification by discrete fourier transform. IEEE Trans-
against most recently revealed results. The result demon-                actions on Acoustics Speech and Signal Processing, 25(3),
strates the feasibility and superiority of end-to-end training           235-238.
for video emotion classification. It is found that one system’s       Arifin, S., & Cheung, P. Y. K. (2008). Affective level video
result (Yi et al., 2015) is slightly higher (less than 0.1%) than        segmentation by utilizing the pleasure-arousal-dominance
the proposed one. However, its features are selected manu-               information. IEEE Transactions on Multimedia, 10(7),
ally, which is time-consuming, not universal and not portable.           1325–1341.
What’s worse, their feature dimension is also long. End-to-           Barker, T., & Virtanen, T. (2016). Blind separation of audio
end training has better transfer learning properties and the             mixtures through nonnegative tensor factorization of mod-
training process is convenient. Using a well-trained model               ulation spectrograms. IEEE/ACM Transactions on Audio,
for another similar problem only needs a simple refinement.              Speech and Language Processing, 24(12), 2377–2389.
It is also observed from the table that the method proposed in        Baveye, Y., Dellandréa, E., Chamaret, C., & Chen, L. (2015).
(Tiwari et al., 2016) has the similar feature size to ours, while        LIRIS-ACCEDE: A video database for affective content
the proposed model outperforms their final arousal accuracy.             analysis. IEEE Transactions on Affective Computing, 6(1),
This may because their feature fusion approach is rough and              43–55.
does not consider the temporal characteristics. This demon-           Chakraborty, R., Maurya, A. K., Pandharipande, M., Hassan,
strates that temporal features could play a role in video emo-           E., Ghosh, H., & Kopparapu, S. K. (2015). TCS-ILAB
tion analysis to a certain extent. As for the other methods, our         - mediaeval 2015: Affective impact of movies and violent
result can outweigh them which shows that modal fusion has               scene detection. In Working Notes Proceedings of the Me-
a great advantage compared with simple early fusion and later            diaEval 2015 Workshop.
fusion. Fusing visual and audio feature in a mid-level is a po-       Choe, W., Chun, H., Noh, J., Lee, S., & Zhang, B. (2013).
tential strategy since visual and audio information in video             Estimating multiple evoked emotions from videos. In Pro-
have a certain interaction. It can also inferred that CNN has            ceedings of 35th Annual Meeting of the Cognitive Science
good performance in visual and audio feature extraction.                 Society (pp. 2046–2051).
                                                                      Cui, Y., Jin, J. S., Zhang, S., Luo, S., & Tian, Q. (2010). Mu-
             Conclusion and Future Work                                  sic video affective understanding using feature importance
Video emotion recognition is an important challenge as de-               analysis. In Proceedings of the 9th ACM International Con-
tecting affective attitudes is an important research field in            ference on Image and Video Retrieval (pp. 213–219).
cognitive science. It is argued that visual and audio informa-        Dai, Q., Zhao, R., Wu, Z., Wang, X., Gu, Z., Wu, W., & Jiang,
tion are both important in detecting video emotion. There-               Y. (2015). Fudan-huawei at mediaeval 2015: Detecting
fore in this paper we used a deep learning architecture to               violent scenes and affective impact in movies with deep
fuse visual and audio modalities for video affective classi-             learning. In Working Notes Proceedings of the MediaEval
fication. This end-to-end framework has the advantages of                2015 Workshop.
simple training and convenient transplantation and demon-             Goyal, A., Kumar, N., Guha, T., & Narayanan, S. S. (2016).
strates that modal fusion with small size of features can com-           A multimodal mixture-of-experts model for dynamic emo-
pare against most state-of-art results obtained by participants          tion prediction in movies. In Proceedings of 2016 IEEE
of the MediaEval 2015 Affective Impact of Movies task. Fur-              International Conference on Acoustics, Speech and Signal
thermore, it would be interesting to study if it is feasible to          Processing (pp. 2822–2826).
                                                                  3558

            Table 2: Comparison with state-of-the-art approaches of MediaEval 2015 Affective Impact of Movies task
                  Approaches           Fusion method Feature selection type Feature length Arousal Accuracy(%)
                (Yi et al., 2015)       Later fusion           Manual                >4,000                55.93
          (Trigeorgis et al., 2015)     Early fusion           Manual                   1000               55.72
                (P. et al., 2015)       Early fusion           Manual              >100,000                51.90
         (Chakraborty et al., 2015)     Later fusion           Manual                   1,000              48.95
             (Tiwari et al., 2016)      Early fusion         Automatic                   500               55.85
            (Seddati et al., 2015)      Early fusion         Automatic                20,000               52.44
               (Dai et al., 2015)       Early fusion         Automatic                10,000               48.70
              Proposed approach         Modal fusion         Automatic                  512                55.89
Hanjalic, A. (2006). Extracting moods from pictures and           Sjöberg, M., Baveye, Y., Wang, H., Quang, V. L., Ionescu,
   sounds: towards truly personalized TV. IEEE Signal Pro-          B., Dellandréa, E., . . . Chen, L. (2015). The mediaeval
   cessing Magazine, 23(2), 90-100.                                 2015 affective impact of movies task. In Working Notes
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term           Proceedings of the MediaEval 2015 Workshop.
   memory. Neural Computation, 9(8), 1735–1780.                   Soleymani, M., Kierkels, J. J. M., Chanel, G., & Pun, T.
Kahou, S. E., Bouthillier, X., Lamblin, P., Gülçehre, Ç.,        (2009). A bayesian framework for video affective repre-
   Michalski, V., Konda, K., . . . Bengio, Y. (2016). Emonets:      sentation. In Proceedings of 3rd International Conference
   Multimodal deep learning approaches for emotion recog-           on Affective Computing and Intelligent Interaction.
   nition in video. Journal of Multimodal User Interfaces,        Tiwari, S. N., Duong, N. Q., Lefebvre, F., Demarty, C.-
   10(2), 99–111.                                                   H., Huet, B., & Chevallier, L. (2016). Deep features
Kang, H. (2003). Affective content detection using HMMs.            for multimodal emotion classification. Retrieved from
   In Proceedings of the 11th ACM International Conference          https://hal.inria.fr/hal-01289191
   on Multimedia (pp. 259–262).                                   Trigeorgis, G., Coutinho, E., Ringeval, F., Marchi, E.,
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Im-          Zafeiriou, S., & Schuller, B. W. (2015). The ICL-TUM-
   agenet classification with deep convolutional neural net-        PASSAU approach for the mediaeval 2015 “affective im-
   works. In Proceedings of 26th Annual Conference on Neu-          pact of movies” task. In Working Notes Proceedings of the
   ral Information Processing Systems (pp. 1106–1114).              MediaEval 2015 Workshop.
Levi, G., & Hassner, T. (2015). Emotion recognition in the        Wang, H. L., & Cheong, L. F. (2006). Affective understand-
   wild via convolutional neural networks and mapped binary         ing in film. IEEE Transactions on Circuits and Systems for
   patterns. In Proceedings of 2015 ACM on International            Video Technology, 16(6), 689–704.
   Conference on Multimodal Interaction (pp. 503–510).            Williams, S., Oliker, L., Vuduc, R. W., Shalf, J., Yelick, K. A.,
                                                                    & Demmel, J. (2009). Optimization of sparse matrix-vector
Lu, L., Liu, D., & Zhang, H. (2006). Automatic mood de-
                                                                    multiplication on emerging multicore platforms. Parallel
   tection and tracking of music audio signals. IEEE Trans-
                                                                    Computing, 35(3), 178–194.
   actions on Audio, Speech and Language Processing, 14(1),
                                                                  Xu, M., Chia, L., & Jin, J. S. (2005). Affective content anal-
   5–18.
                                                                    ysis in comedy and horror videos by audio emotional event
Mikolov, T., Karafiát, M., Burget, L., Cernocký, J., & Khu-
                                                                    detection. In Proceedings of 2005 IEEE International Con-
   danpur, S. (2010). Recurrent neural network based lan-
                                                                    ference on Multimedia and Expo (pp. 622–625).
   guage model. In Proceedings of 11th Annual Conference of
                                                                  Yi, Y., Wang, H., Zhang, B., & Yu, J. (2015). MIC-TJU in
   the International Speech Communication Association (pp.
                                                                    mediaeval 2015 affective impact of movies task. In Work-
   1045–1048).
                                                                    ing Notes Proceedings of the MediaEval 2015 Workshop.
P., M. V., Hayrapetyan, S., Tapaswi, M., & Stiefelhagen, R.       Zaremba, W., & Sutskever, I. (2014). Learning to execute.
   (2015). KIT at mediaeval 2015 - evaluating visual cues for       CoRR, abs/1410.4615.
   affective impact of movies task. In Working Notes Proceed-     Zeiler, M. D., Ranzato, M., Monga, R., Mao, M. Z., Yang,
   ings of the MediaEval 2015 Workshop.                             K., Le, Q. V., . . . Hinton, G. E. (2013). On rectified linear
Seddati, O., Kulah, E., Pironkov, G., Dupont, S., Mahmoudi,         units for speech processing. In Proceedings of 2013 IEEE
   S., & Dutoit, T. (2015). Umons at mediaeval 2015 affec-          International Conference on Acoustics, Speech and Signal
   tive impact of movies task including violent scenes detec-       Processing (pp. 3517–3521).
   tion. In Working Notes Proceedings of the MediaEval 2015       Zhu, Y., Jiang, Z., Peng, J., & Zhong, S. (2016). Video affec-
   Workshop.                                                        tive content analysis based on protagonist via convolutional
Siakaluk, P. D., Knol, N., & Pexman, P. M. (2014). Effects of       neural network. In Proceedings of 17th Pacific-Rim Con-
   emotional experience for abstract words in the stroop task.      ference on Multimedia, Part I (pp. 170–180).
   Cognitive Science, 38(8), 1698–1717.
                                                              3559

