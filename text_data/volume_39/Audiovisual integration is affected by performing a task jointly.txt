                    Audiovisual integration is affected by performing a task jointly
                                                     Basil Wahn (bwahn@uos.de)
                 Institute of Cognitive Science – Neurobiopsychology, University of Osnabrück, Wachsbleiche 27,
                                                        49090 Osnabrück, Germany
                                                Ashima Keshava (akeshava@uos.de)
                 Institute of Cognitive Science – Neurobiopsychology, University of Osnabrück, Wachsbleiche 27,
                                                        49090 Osnabrück, Germany
                                                Scott Sinnett (ssinnett@hawaii.edu)
                             Department of Psychology, University of Hawai'i at Mānoa, 2530 Dole Street,
                                                       Honolulu, HI 96822-2294, USA
                                             Alan Kingstone (alan.kingstone@ubc.ca)
                              Department of Psychology, University of British Columbia, 2136 West Mall,
                                              Vancouver, British Columbia, Canada V6T1Z4
                                                    Peter König (pkoenig@uos.de)
                 Institute of Cognitive Science – Neurobiopsychology, University of Osnabrück, Wachsbleiche 27,
                                                        49090 Osnabrück, Germany
                  Institut für Neurophysiologie und Pathophysiologie, Universitätsklinikum Hamburg-Eppendorf,
                                                            Hamburg, Germany
                               Abstract                                  well as costs. In particular, if the multisensory inputs
                                                                         contain redundant information (e.g., visual and auditory
Humans constantly receive sensory input from several sensory
modalities. Via the process of multisensory integration, this input      stimuli originate from the same spatial location), human
is often integrated into a unitary percept. Researchers have             localization performance is faster and more accurate (e.g.,
investigated several factors that could affect the process of            Körding et al. 2007; Rohe & Noppeney, 2015; Wahn &
multisensory integration. However, in this field of research, social     König 2015a,b; 2016). Yet, if the sensory inputs provide
factors (i.e., whether a task is performed alone or jointly) have        conflicting information (e.g., visual and auditory stimuli
been widely neglected. Using an audiovisual crossmodal                   originate from different spatial locations but still coincide in
congruency task we investigated whether social factors affect            time), human localization performance is slowed down and
audiovisual integration. Pairs of participants received congruent or     less accurate (Heed, Boukje, Sebanz, & Knoblich, 2010;
incongruent audiovisual stimuli and were required to indicate the
                                                                         Plöchl et al., 2016; Rohe & Noppeney, 2015; Spence,
elevation of these stimuli. We found that the reaction time cost of
responding to incongruent stimuli (relative to congruent stimuli)        Pavani, & Driver, 2004). In the past, researchers have
was reduced significantly when participants performed the task           explored how attentional processes influence the
jointly compared to when they performed the task alone. These            multisensory integration process, and more generally, how
results extend earlier findings on visuotactile integration by           attentional processing is distributed across the sensory
showing that audiovisual integration is also affected by social          modalities (e.g., Alais, Morrone, & Burr, 2006; Alsius,
factors.                                                                 Navarra, Campbell, & Soto-Faraco, 2005; Helbig & Ernst,
   Keywords: multisensory integration; joint action; task                2008; Wahn & König, 2015a,b, 2016; Wahn, Murali,
   distribution; social cognition.                                       Sinnett, & König, 2017; for recent reviews, see Talsma,
                                                                         2015; Wahn & König, 2017). However, to date, researchers
                                                                         have largely neglected how social factors could affect the
                         Introduction                                    integration process. Thus we know relatively little about
                                                                         how the social presence of another person, and/or how
   In everyday life, humans constantly process sensory input             performing a task with another person, influences the
from several sensory modalities. If sensory input from                   process of multisensory integration.
multiple sensory modalities coincides in space and/or time,                 To date, to the best of our knowledge, only two studies
it is frequently integrated into a unitary percept (Alais &              (Heed et al., 2010; Teneggi, Canzoneri, di Pellegrino, &
Burr, 2004; Ernst & Banks, 2002; Körding et al. 2007; Rohe               Serino, 2013) have addressed the extent to which social
& Noppeney, 2015; for a review, see: Spence, 2007) – a                   factors can modulate multisensory integration. In Heed et
process referred to as “multisensory integration”.                       al.’s experiment participants performed a visuotactile
Multisensory integration can result in perceptual benefits as            congruency task that was either performed alone, or jointly
                                                                     1296

with another person. When the task was performed alone,                                         Methods
participants were required to hold two foam cubes, one in
each hand, and indicate with foot pedal presses the spatial           Participants
elevation of a tactile stimulus that could either appear at the          Twelve pairs of individuals (15 female, M = 21.92 years,
top of the cube (i.e., felt at the index finger) or at the bottom     SD = 3.35 years) participated in the study at the University
of the cube (i.e., felt at the thumb). The participants also          of Osnabrück. Prior to the experiment, participants signed
simultaneously received irrelevant visual stimuli that either
                                                                      informed written consent. The study was approved by the
appeared at the same spatial location as the tactile stimulus
                                                                      ethics committee of the University of Osnabrück. After the
or not (i.e., stimuli were presented either in congruent or
                                                                      experiment had been completed, participants were debriefed
incongruent positions). Thus, the visual stimuli provided
                                                                      and received monetary compensation or participation hours.
either conflicting or redundant spatial information, resulting
in costs or benefits of multisensory integration, respectively.       Experimental setup
Heed et al. (2010) replicated earlier results (Spence, Pavani,
& Driver, 1998; 2004) by finding that reaction times were                Participants sat in a dark room in front of a computer
faster when indicating the location of the tactile target if the      screen (Apple 30” LCD screen, resolution 2560 x 1600
visual stimulus appeared in a congruent position compared             pixels, 77.53 x 48.46 visual degrees) at a distance of 50 cm.
to an incongruent position. This effect is referred to as the         Four USB speakers (Mini HiFi USB 2.0 mini speaker),
“crossmodal congruency effect” (CCE). That is, when the               which were connected via a USB hub (Orico HF9US-2P
tactile and visual stimuli provide redundant information              USB 9-Port HUB) were arranged in a 2 x 2 grid above and
(i.e., are presented in the same (congruent) position),               below the monitor (vertically and horizontally 1600 pixels,
localization performance is faster compared to when                   equivalent to 48.45 visual degrees, apart) in front of the
conflicting information is provided (i.e., stimuli are                participants (Figure 1). The positions of the visual flashes
presented in different, i.e. incongruent positions). When             (80 x 80 pixels, 2.42 visual degrees wide, 100 ms) were
participants performed the task in pairs, one of them                 arranged in the same 2x2 grid, such that the visual flashes
indicated the elevation of the tactile stimuli (as before)            were observed from approximately the same spatial
while the second participant indicated the elevation of the           locations as the auditory stimuli (sine wave tone, 4800 Hz,
visual stimuli. But note, the person detecting tactile stimuli        100 ms) – they were vertically displaced by 2.4 cm.
was still exposed to the congruent or incongruent visual                  Participants sat in two chairs placed in front of the
stimuli. Heed et al. found that the magnitude of the CCE              computer screen (left and right of the fixation cross,
was reduced when performing the crossmodal congruency                 respectively) with keyboards on their laps.
task jointly compared to performing it alone. In particular,
when participants performed the task jointly, incongruent
presentations had less of an effect on reaction times when
compared to performing the task alone. This observation
suggests that the cost of incongruent presentations on
multisensory integration is reduced when the task is
performed jointly.
   To date, the modulation of the CCE by social factors as
found by Heed et al. (2010) has not been investigated with
other sensory modalities. In particular, it is an open question
whether audiovisual integration is similarly affected by
social factors. Given that the tactile sensory modality
processes events in close proximity while the auditory
sensory modality is also able to sense more distal events, it                           Figure 1: Experimental Setup.
is not clear whether audiovisual integration would be
similarly affected by social factors as visuotactile                  Experimental conditions and procedure
integration. Thus rather than the visuotactile congruency                In the experiment, participants performed an audiovisual
task as used in Heed et al. the present study required                congruency task either alone or jointly. In this task,
participants to perform an audiovisual congruency task,               participants received visual flashes and auditory tones,
either alone or jointly. If social factors modulate the CCE           originating either from the same (i.e., congruent) or a
for audiovisual stimuli, we predict that the CCE will be              different (i.e., incongruent) spatial elevation. In addition,
reduced when performing the audiovisual congruency task               stimuli could originate either from the same or opposite
jointly as compared to performing the task alone.                     side. For example, either both stimuli could originate from
Conversely, if social factors do not affect the CCE, then the         the left side or one could originate from the left and one
CCE should not be modulated regardless of whether the task            from the right side. The task was to indicate the elevation of
is performed jointly or alone.                                        one of these stimuli using the keyboard with the mapping of
                                                                      keys F/up & C/down for the visual stimuli; keys K/up &
                                                                  1297

M/down for the auditory stimuli. We set the time limit for             easier to localize than auditory stimuli, CCE effects are only
responses to 2 seconds (see Figure 2, for a trial overview).           observed for the participants responding to the auditory
   When participants performed the task jointly, they sat              stimuli. Prior to performing inferential statistical tests, we
next to each other in front of the computer screen in close            tested whether the normality assumption was given with a
proximity (~10 cm) to ensure that they shared peripersonal             Shapiro-Wilk test. In the case of a violation, we transformed
space (Heed et al., 2010). In this condition, one participant          the data using a log transformation.
would indicate the elevation of the auditory stimuli while
the other participant would indicate the elevation of the                                         Results
visual stimuli. When participants performed the task alone,
one participant was asked to wait outside the experiment                  On a descriptive level (see Figure 3A & B), when
room while the other participant performed the task,                   examining the reaction times of correctly localized auditory
indicating the stimulus elevation for their assigned modality.         cues, participants were slower to localize the cues in the
Note, regardless of whether participants performed the task            incongruent condition compared to the congruent condition.
alone or jointly, the seating positions of participants                This observation establishes the well-known CCE effect.
remained constant in all conditions within a pair and were             Furthermore, in line with earlier studies (Heed et al., 2010),
counterbalanced across pairs (i.e., in half of the pairs, the          the CCE was more pronounced for stimuli that were shown
participant responding to the auditory stimuli was sitting on          on the same side compared to the opposite side. Importantly,
the right side).                                                       for same side stimuli, the CCE was reduced profoundly in
    A                                         B                        the joint condition relative to the individual condition.
                                                                           A                                                          A
               100 ms                                  2 seconds
    Figure 2: Trial overview. (A) Participants simultaneously
   received a visual and auditory stimulus. (B) Participants
 were required to indicate the elevation of one of the stimuli
     using the keyboard. In this example trial, the auditory
stimulus would be in the upper location on the right side, the
  visual stimulus in the bottom location on the left side (i.e.,
             Prediction:
     an incongruent    opposite     1 & 2trial).
                         Conditionside           congruency
                                           show After       eﬀect.
                                                       two seconds
             In condition 3, P1 distractor processing is reduced.
          passed, the next trial started automatically.
   In sum, the experiment consisted of a 2x2x2 factorial
design with Congruency (Congruent, Incongruent), Side
(Same, Opposite), and Condition (Individual, Joint) as
factors.
  The experiment consisted of six blocks, each composed of
144 trials. In these trials, each combination of the factor                 B                                                         B
levels for the factors Congruency and Side occurred equally
often in a randomized order. The factor Condition was
varied across blocks. That is, there were three types of
blocks: 1) The participant responding to the visual stimuli
performing the task alone, 2) the participant responding to
the auditory stimuli performing the task alone, 3) both
participants performing the task jointly. Participants
performed a pseudorandomized sequence of these three
types of blocks twice. We avoided repetitions of the same
block type in consecutive blocks.
  The experiment took approximately 40 minutes. It was
programmed in Python 2.7.3.
Data preparation and analysis
   In line with Heed et al. (2010), we restricted our analysis
to the participant in a pair responding to the auditory
stimuli. That is, given that visual stimuli are considerably
                                                                   1298

    Figure 3: Mean reaction time (in seconds) as a function of     involving the factor Condition (Condition: F(1,11) = 0.31, p
   the factors Condition (Individual, Joint) and Congruency        = .588; Condition x Congruency: F(1,11) = 0.02, p = .881;
  (congruent, incongruent), separately for same side (A) and       Condition x Congruency x Side: F(1,11) = 0.003, p = .954).
     opposite side stimuli (B). Error bars in both panels are      These results indicate that a speed-accuracy tradeoff does
                   standard error of the mean.                     not explain the reduced CCE for the joint condition relative
                                                                   to the individual condition reported above because the
    We tested whether these observations were statistically        accuracy did not vary as a function of whether the task was
reliable by performing a 2x2x2 repeated measures ANOVA             performed in pairs or alone. Thus the latency benefit of the
with the factors Congruency (Congruent, Incongruent), Side         joint condition relative to the alone condition was not
(Same, Opposite), and Condition (Individual, Joint). As the        acquired at the expense of committing more errors.
assumption of normality was violated, we applied a log                In sum, the results for same side stimuli indicate that the
transformation to the reaction times prior to entering them to     CCE is reduced significantly when participants perform an
the ANOVA.                                                         audiovisual crossmodal congruency task jointly compared to
    We found a significant main effect for the factor              when they perform it alone.
Congruency (F(1,11) = 19.73, p < .001). We found
significant two-way interactions between the factors Side
         A
and Congruency     (F(1,11) = 38.42, p < .001) and the factors           A
Condition and Congruency (F(1,11) = 6.00, p = .032). The
former interaction effect suggests that the magnitude of the
CCE is reduced for opposite side stimuli compared to same
side stimuli. Importantly, the latter interaction effect
suggests that the CCE is reduced for the joint condition
compared to the individual condition. In addition, we also
observed a three-way interaction (F(1,11) = 6.61, p = .026),
suggesting that the reduced CCE for the joint condition
compared to the individual condition depends on whether
stimuli appear on the same side or opposite sides. To further
investigate the three-way interaction effect, we performed
two 2x2 repeated measures ANOVAs (Condition x
Congruency), restricting the data either to only same side or
opposite side stimuli. For same side stimuli, we found a
significant main effect of Congruency (F(1,11) = 27.29, p <
.001) and a significant interaction between the factors
Condition and Congruency (F(1,11) = 9.62, p = .01). This
          B that for same side stimuli, performing a task
demonstrates                                                             B
jointly indeed reduced the CCE. However, for opposite side
stimuli, we only found a significant main effect of
Congruency (F(1,11) = 5.58, p = .038) but no interaction
effect between the factors Condition and Congruency
(F(1,11) = 0.07, p = .801). Both of these results are in line
with the findings by Heed et al. (2010). That is, when
investigating visuotactile integration, Heed et al. (2010)
similarly found that the CCE effect was reduced in the joint
condition relative to the individual condition for same side
stimuli but not for opposite side stimuli.
    We also tested an alternative explanation of these results
by a speed-accuracy tradeoff. That is, in the joint condition,
participants potentially could have localized the incongruent
cues faster at the expense of being less accurate in their
responses. To investigate this, we repeated the 2x2x2
repeated measures ANOVA with the dependent variable
fraction correct (for a descriptive overview, see Figure 4A
& B). We found significant main effects for the factors Side              Figure 4: Mean fraction correct as a function of the
(F(1,11) = 73.32, p < .001) and Congruency (F(1,11) =                   factors Condition (individual, joint) and Congruency
29.87, p < .001) and a significant interaction effect between       (Congruent, Incongruent), separately for same side (A) and
these two factors (F(1,11) = 66.14, p < .001). Importantly,            opposite side stimuli (B). Error bars in both panels are
we did not find a significant main effect or interaction                             standard error of the mean.
                                                               1299

                        Discussion                                  condition. That is, an already lower CCE may not allow for
                                                                    any additional modulations by social factors.
                                                                       Future studies could also test whether the social effects
   The present study investigated whether the modulation of
                                                                    found in this study can alternatively be explained by other
the CCE by social factors found in earlier studies
                                                                    factors (Stenzel & Liepelt, 2016). For instance, it could be
investigating visuotactile integration (Heed et al. 2010) can
                                                                    investigated whether a non-human co-actor (e.g., a robot)
also be observed for audiovisual presentations. In line with
                                                                    responding to the distractors is sufficient to find the effects
Heed et al., we found that the CCE is indeed reduced for
                                                                    in the present study (Stenzel et al., 2012).
same side stimuli when participants perform an audiovisual
crossmodal congruency task jointly compared to performing              More generally, the present findings are relevant to, and
it alone. Furthermore, we found that the data are not               may benefit, real-world situations in which humans perform
explained by a speed-accuracy tradeoff. Collectively, the           tasks jointly while processing multisensory information.
                                                                    That is, our data and the earlier findings of Heed et al.,
present results extend Heed et al.’s earlier findings of a
                                                                    (2010) suggest that the benefits of multisensory integration
modulation of the CCE for a visuotactile crossmodal
                                                                    are preserved when performing a task jointly (i.e.,
congruency task, and indicate that this social effect
                                                                    participants respond faster to congruent multisensory
generalizes to audiovisual integration.
                                                                    stimuli) while the costs of multisensory integration are
   A possible “mechanism” for our present social effect
                                                                    reduced (i.e., participants are slowed down less by
could be a co-representation process (Sebanz, Knoblich, &
                                                                    incongruent stimuli). Future studies could investigate further
Prinz, 2003; for reviews see: Sebanz, Bekkering, &
                                                                    how the benefits of multisensory processing (e.g., due to
Knoblich, 2006; Vesper et al., 2017). That is, when
                                                                    multisensory integration (Alais & Burr, 2004; Ernst &
participants perform the task jointly, participants co-
                                                                    Banks, 2002; Körding et al. 2007; Rohe & Noppeney,
represent the task of their partner (e.g., that the partner
                                                                    2015), sensory augmentation (König et al., 2016; Goeke,
responds to the visual stimuli) which could lead to a reduced
                                                                    Planera, Finger, & König, 2016), or circumventing limited
processing of the stimuli relevant for the partner but
                                                                    attentional resources (Alais & Burr, 2004; Arrighi, Lunardi,
irrelevant for the own task. As a consequence, the irrelevant
                                                                    & Burr, 2011; Wahn, et al. 2016; for a review, see: Wahn &
stimuli could be perceived as less distracting for
                                                                    König, 2017)) may facilitate human performance in other
incongruent stimulus presentation but still sufficiently
                                                                    joint settings.
processed for congruent presentations, yielding faster
reaction times. Alternatively, the effects in the present study
could be explained by a dynamic modulation of the co-               Acknowledgments
actor’s peripersonal space as found in an earlier study             This research was supported by H2020—H2020-
(Teneggi et al., 2013) or by a general withdrawal of                FETPROACT-2014641321—socSMCs (for BW & PK).
attention to the stimuli to which the co-actor responds
(Szpak et al., 2015).                                                                        References
   Future studies could discern further how social factors
contribute to the modulation of the CCE. In the present             Alais, D., & Burr, D. (2004). The ventriloquist effect results
study, pairs of participants performed the crossmodal               from near-optimal bimodal integration. Current Biology, 14,
congruency task in the same peripersonal space and both             257-262.
participants performed the task. Earlier findings (Heed et al.,
2010) showed that the CCE for visuotactile stimuli is only          Alais, D., Morrone, C., & Burr, D. (2006). Separate
affected by social factors if both participants perform the         attentional resources for vision and audition. Proceedings of
task and are located in their respective peripersonal spaces.       the Royal Society of London B: Biological Sciences, 273,
It is an open question whether a reduction of the CCE for           1339-1345.
audiovisual stimuli would be observed when only one of
these factors is manipulated. For instance, when participants       Alsius, A., Navarra, J., Campbell, R., & Soto-Faraco, S.
are in the same peripersonal space but only one of them             (2005). Audiovisual integration of speech falters under high
performs the task, or when both of them perform the task            attention demands. Current Biology, 15(9), 839-843.
but from separate peripersonal spaces. In contrast to the
tactile modality, both the visual and the auditory modality         Arrighi, R., Lunardi, R., and Burr, D. (2011). Vision and
investigated here sample distant events. Thus, it is quite          audition do not share attentional resources in sustained
conceivable that visuotactile integration is dependent on           tasks. Frontiers in Psychology. 2:56.
jointly executing the task in peripersonal space while this         doi:10.3389/fpsyg.2011.00056
might not be the case for audiovisual integration.
   As another point of note, our finding that performing the        Ernst, M. O., & Banks, M. S. (2002). Humans integrate
crossmodal congruency task jointly affects the CCE for              visual and haptic information in a statistically optimal
same side stimuli but not for opposite side stimuli could be        fashion. Nature, 415, 429–433.
explained by the observation that for opposite side stimuli
the CCE was already greatly reduced in the individual
                                                                1300

Goeke, C. M., Planera, S., Finger, H., & König, P. (2016).        become human-like interaction partners: Corepresentation
Bayesian alternation during tactile augmentation. Frontiers       of robotic actions. Journal of Experimental Psychology:
in Behavioral Neuroscience, 10.                                   Human Perception and Performance, 38(5), 1073.
Heed, T., Habets, B., Sebanz, N., Knoblich, G. (2010).            Szpak, A., Loetscher, T., Churches, O., Thomas, N. A.,
Others’ actions reduce crossmodal integration in                  Spence, C. J., & Nicholls, M. E. (2015). Keeping your
peripersonal space. Current Biology, 20, 1345–1349.               distance: Attentional withdrawal in individuals who show
                                                                  physiological signs of social discomfort. Neuropsychologia,
Helbig, H. B., & Ernst, M. O. (2008). Visual-haptic cue           70, 462-467.
weighting is independent of modality-specific attention.
Journal of Vision, 8, 1-16.                                       Talsma, D. (2015). Predictive coding and multisensory
                                                                  integration: An attentional account of the multisensory
König, S. U., Schumann, F., Keyser, J., Goeke, C., Krause,        mind. Frontiers in Integrative Neuroscience, 9:19.
C., Wache, S., ... & König, P. (2016). Learning new               doi:10.3389/fnint.2015.00019
sensorimotor contingencies: Effects of long-term use of
sensory augmentation on the brain and conscious                   Teneggi, C., Canzoneri, E., di Pellegrino, G., & Serino, A.
perception. PLoS ONE, 11(12), e0166647. doi:                      (2013). Social modulation of peripersonal space
10.1371/journal.pone.0166647                                      boundaries. Current Biology, 23(5), 406-411.
Körding, K. P., Beierholm, U., Ma, W. J., Quartz, S.,             Vesper, C., Abramova, E., Bütepage, J., Ciardo, F., Crossey,
Tenenbaum, J. B., & Shams, L. (2007). Causal inference in         B., Effenberg, A., ... , & Wahn, B. (2017). Joint action:
multisensory perception. PLoS ONE, 2(9), e943.                    Mental representations, shared information and general
                                                                  mechanisms for coordinating with others. Frontiers in
Plöchl, M., Gaston, J., Mermagen, T., König, P., &                Psychology, 7, 2039.
Hairston, W. D. (2016). Oscillatory activity in auditory
cortex reflects the perceptual level of audio-tactile             Wahn B., & König P. (2015a) Audition and vision share
integration. Scientific Reports, 6:33693.                         spatial attentional resources, yet attentional load does not
                                                                  disrupt audiovisual integration. Frontiers in Psychology,
Rohe, T., & Noppeney, U. (2015). Cortical hierarchies             6:1084. doi:10.3389/fpsyg.2015.01084
perform Bayesian causal inference in multisensory
perception. PLoS Biology, 13(2), e1002073.                        Wahn B., & König P. (2015b) Vision and haptics share
                                                                  spatial attentional resources and visuotactile integration is
Sebanz, N., Knoblich, G., & Prinz, W. (2003). Representing        not affected by high attentional load. Multisensory Research
others' actions: just like one's own? Cognition, 88, 11-21.       28, 371-392. doi:10.1163/22134808-00002482
Sebanz, N., Bekkering, H., & Knoblich, G. (2006). Joint           Wahn, B., Schwandt, J., Krüger, M., Crafa, D., Nunnendorf,
action: bodies and minds moving together. Trends in               V., & König, P. (2016). Multisensory teamwork: using a
Cognitive Sciences, 10, 70-76.                                    tactile or an auditory display to exchange gaze information
                                                                  improves performance in joint visual search. Ergonomics,
Spence, C., Pavani, F., & Driver, J. (1998). What crossing        59, 781-795. doi: 10.1080/00140139.2015.1099742
the hands can reveal about crossmodal links in spatial
attention. Abstracts of the Psychonomic Society, 3, 13.           Wahn B., & König P. (2016) Attentional resource allocation
                                                                  in visuotactile processing depends on the task, but optimal
Spence, C., Pavani, F., & Driver, J. (2004). Spatial              visuotactile integration does not depend on attentional
constraints on visual-tactile cross-modal distractor              resources. Frontiers in Integrative Neuroscience, 10:13.
congruency effects. Cognitive, Affective, & Behavioral
Neuroscience, 4, 148-169.                                         Wahn B., Murali, S., Sinnett, S., & König P. (2017)
                                                                  Auditory stimulus detection partially depends on
Spence, C. (2007). Audiovisual multisensory integration.          visuospatial attentional resources. i-Perception, 1-17. doi:
Acoustical Science and Technology, 28(2), 61-70.                  10.1177/2041669516688026
Stenzel, A., & Liepelt, R. (2016). Joint Simon effects for        Wahn B., & König P. (2017) Is attentional resource
non-human co-actors. Attention, Perception, &                     allocation across sensory modalities task-dependent?
Psychophysics, 78(1), 143-158.                                    Advances in Cognitive Psychology, 13(1), 83-96. doi:
                                                                  10.5709/acp-0209-2
Stenzel, A., Chinellato, E., Bou, M. A. T., del Pobil, Á. P.,
Lappe, M., & Liepelt, R. (2012). When humanoid robots
                                                              1301

