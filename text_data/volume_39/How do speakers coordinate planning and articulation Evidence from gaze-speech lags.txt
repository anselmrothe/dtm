      How do Speakers Coordinate Planning and Articulation? Evidence from Gaze-
                                                               Speech Lags
                                             Chiara Gambi (cgambi@exseed.ed.ac.uk)
                                               Department of Psychology, 7 George Square
                                                           Edinburgh, EH8 9JZ UK
                                            Matthew Crocker (crocker@coli.uni-sb.de)
                          Department of Computational Linguistics and Phonetics, C7.1 University Campus
                                                         Saarbrücken, 66123 Germany
                              Abstract                                    Meyer, & Konopka, 2014), or task context (Meyer et al.,
                                                                          2003).
   How do speakers coordinate planning and articulation of                   We thus know a great deal about factors that can
   more than one word at the same time? Here, we test whether             influence the coordination of planning and articulatory
   they dynamically estimate how long it takes to (i) plan and            processes. On the contrary, we know very little about the
   (ii) articulate the words they intend to produce as a means of
                                                                          mechanisms that underlie the timely coordination of these
   achieving such coordination. German speakers named two
   pictures without pausing, while their eye-movements were               processes. For example, we know that planning style is
   recorded. In line with previous reports, after their gaze left the     influenced both by the accessibility of linguistic units, and
   first picture, speakers took longer to start speaking (i.e., the       by the ease-of-apprehension of the referent (Konopka &
   gaze-speech lag was longer) when the name of the first                 Meyer, 2014). This suggests that the mechanism involved
   picture was shorter. But while gaze-speech lags were also              must be sensitive to the difficulty of the planning process at
   longer when the second picture was harder to name, the two             different stages, but there are at least two ways in which
   effects did not interact. We argue that speakers’ flexible
   planning abilities might be accounted for by reactive, rather
                                                                          such a mechanism could operate.
   than proactive planning mechanisms.                                       One possibility is that a flexible planning system operates
                                                                          reactively. Once speakers encounter some difficulty, a
   Keywords: planning; estimation; duration; coordination;                compensatory mechanism is triggered. For instance, if
   gaze-speech lag.
                                                                          accessing a particular word is difficult (i.e., takes time),
                                                                          attention might be (temporarily) shifted to another process
                          Introduction                                    (e.g., retrieving a different word). But in addition, the
Speakers plan ahead of articulation (Griffin & Bock, 2000;                planning system might, at least in part, allocate resources to
Levelt, Roelofs, & Meyer, 1999) and usually complete                      different processes in a proactive manner. Such a proactive
lexico-semantic planning for at least a whole phrase                      planning mechanism could be learnt from previous
(Martin, Crowther, Knight, Tamborello, & Yang, 2010;                      experience with producing language (in general, or within a
Smith & Wheeldon, 1999), phonological planning for a                      particular task), and indeed there is evidence that planning
whole word (Meyer, 1996; Smith & Wheeldon, 2004), and                     style can be primed by previous experience with the same
articulatory planning for a whole syllable (Meyer, Roelofs,               sentence structure (Van de Velde et al., 2014). A proactive
& Levelt, 2003) before beginning to speak. While this                     planning mechanism would of course be beneficial in
allows for rapid and fluent speech production, the                        maximizing fluency, as it may allow speakers to avoid
incremental nature of planning also raises the question of                future difficulties, by anticipating their likely occurrence
how speakers manage the timely coordination of planning                   and taking appropriate steps before they even arise. This
and articulatory processes.                                               idea is reminiscent of some models of motor control (e.g.,
                                                                          Wolpert and Flanagan, 2011).
Mechanisms for Flexible Advance Planning
Several studies have uncovered regularities in speakers’                  Proactive Planning: Candidate Evidence?
amount of advance planning; for example, suggesting that                  To our knowledge, no language production study has
speech onsets are comparable across short and long words                  investigated this issue directly. However, in one seminal
because speakers usually complete articulatory processing                 study, Griffin (2003) suggested that speakers might estimate
for only the first syllable of a word prior to speech onset               how long both planning and articulating a word will take,
(Damian, Bowers, Stadthagen-Gonzalez, & Spalek, 2010;                     and then combine such estimates to determine how to time
see also Meyer, Roelofs, & Levelt, 2003). Importantly, in                 one process with respect to the other in order to minimize
recent years it has also become clear that the amount of                  future disfluencies (i.e., to plan proactively).
advance planning speakers perform is not fixed, but rather                   To illustrate, imagine a speaker of German preparing to
varies with properties of both the utterance and the                      produce Abschlussballkleider (dresses for the high-school
speaker’s recent experience (Konopka, 2012; Van de Velde,                 prom). Let us assume, for the purpose of illustration, that
                                                                      2061

the speaker retrieves Abschlussball (prom) and Kleider                     articulatory retrieval for one syllable. By contrast, for a
(dresses) separately (Sandra, 1990)1. If so, the speaker will              polysyllabic word1 the gaze shift occurs only later (once
need to get the first syllable of Kleider ready to be                      articulatory retrieval of the first syllable is already
articulated by the time articulation of Abschlussball is                   underway), thus leading to a shorter lag.
ending. To do this, the speaker could estimate both how
long it will take her to get Kleider ready (i.e., retrieval                This study
difficulty) and how long it will take her to say Abschlussball             If Meyer et al.’s (2007) explanation is correct, then the
(i.e., articulation duration). She could then determine that               reversed-length effect on gaze-to-speech lags is not
she will probably have enough time to prepare Kleider                      evidence that speakers estimate duration, contrary to
while saying Abschlussball (a long word), so she can start                 Griffin’s (2003) suggestion. Moreover, neither study
speaking right away. But if the first word is short, such as               demonstrates that speakers can combine estimates of
Sport in Sporttitelseite (sport title page), she might instead             retrieval difficulty with estimates of duration, because they
have to delay speech onset in order to prepare more of the                 did not manipulate the difficulty of retrieving word23. Here,
second word before starting to speak. Similarly, she may                   we provide a test of this hypothesis: If speakers take into
need to delay speech onset if the second word is particularly              account not only word1 length (monosyllabic vs.
difficult to retrieve.                                                     polysyllabic words), but also word2 retrieval difficulty,
    However, the evidence in support of Griffin’s proposal is              gaze-speech lags should be affected by both variables.
currently somewhat mixed. Griffin (2003) asked speakers to                 Moreover, the effects of the two variables should interact,
name two pictures one after the other, without pausing,                    reflecting the workings of a proactive planning mechanism
while their eye-movements were recorded. Critically, the                   underlying the tight coordination of articulation (of word1)
name of the picture that was mentioned first (word1) could                 and planning (of word2).
be either short (monosyllabic) or long (plurisyllabic). In this               For word2, we chose a manipulation that is both known to
task, speakers usually shift their gaze from the first to the              reliably affect the earliest stages of picture naming, and very
second picture as soon as they have retrieved the                          easy to identify for participants: Pictures were either
phonological representation for word1 (Griffin, 2001; Meyer                visually intact or degraded (see Figure 1). We reasoned this
& Van der Meulen, 2000). The gaze shift generally occurs                   would provide the most favorable test of Griffin’s proposal,
before overt articulation of word1. Importantly, the interval              as speakers were placed in ideal conditions for estimating
between this gaze shift and speech onset (i.e., the gaze-                  the difficulty of retrieving word2; although degradation does
speech lag) is longer when word1 is shorter (a reversed                    not affect the difficulty of retrieving word2 directly, it makes
word-length effect). According to Griffin (2003), this shows               accessing the corresponding concept more difficult, which
that speakers estimate word1 duration: they begin speaking                 then has a knock-on effect on the time it takes to fully
earlier (with respect to the gaze shift, thus leading to a                 prepare word2. To give speakers ample opportunity to adjust
longer gaze-speech lag), when word1 is shorter in order to                 to the relevant level of difficulty, and to avoid carryover
have more time to retrieve the second picture’s name                       effects, degradation varied between participants.
(word2) before, rather than during, articulation of word1.                    As in previous studies, the gaze-speech lag should be
   However, while Meyer, Belke, Häcker, and Mortensen                      longer when word1 is shorter. In addition, if speakers can
(2007) replicated Griffin’s finding2, they also provided a                 estimate retrieval difficulty, it should also be longer when
different explanation. We know that speakers may begin                     word2 takes longer to retrieve. Crucially, there should be a
retrieving the articulatory code of the first syllable of a word           significant interaction, with word2 difficulty having a larger
as soon as they complete phonological processing for this                  effect when word1 is shorter. As there is less scope for
syllable (i.e., without waiting for phonological processing of             completing word2 retrieval during the articulation of word1
the whole word to be completed); in turn, as soon as they                  when word1 is short, speakers should aim to complete most
have retrieved such code, they can begin speaking. But if                  of word2 retrieval before speech onset; instead, when word1
word1 is monosyllabic, the moment of the gaze shift (which                 is long, the speaker can benefit from extra time after the
coincides with completion of phonological processing for                   onset of articulation, and increases in word2 retrieval
the whole word; see above) also happens to coincide with                   difficulty may not affect the gaze-to-speech lag as strongly.
the start of articulatory retrieval. As a result, the gaze-                   If Meyer et al.’s proposal is correct, however, the gaze-
speech lag will last at least the time it takes to perform                 speech lag should only depend on word1 length, and the
                                                                           reversed-length effect on gaze-speech lags would not be
   1 In reality, compound words (especially very frequent ones)
                                                                           evidence for a proactive planning mechanism. Given the
                                                                           potential theoretical relevance of Griffin’s (2003) original
might be planned as a single phonological sequence (Jacobs &
Dell, 2014). This does not affect the interpretation of our results, as    interpretation of her findings, testing her claim in full, as we
we did not ask our participants to produce compounds, but rather           do in this study, would advance our understanding of the
sequences of two unrelated words.
   2 While Griffin (2003) found a reversed word-length effect on
speech latencies (i.e., longer latencies when word1 was shorter) as
                                                                              3 Although Griffin (2003) varied word2 frequency and length,
well as on gaze-speech lags, Meyer et al. (2007) only found this
effect on gaze-speech lags.                                                between-items differences were very small.
                                                                       2062

mechanisms underlying flexible planning in language
production.                                                              Participants were first familiarized with picture names.
                                                                      After identifying their dominant eye, they were seated about
                            Method                                    60 cm from a 24-inch LCD monitor. A head-mounted
                                                                      Eyelink 2000 recorded data from the dominant eye (pupil-
Participants                                                          only, sampling frequency: 250 Hz). Participants were asked
                                                                      to avoid head movements and blinking, and named the
Thirty-two native speakers of German (24 female, Mage =               pictures in left to right order. It was stressed they should
23.8 yrs, SD = 2.6), with self-reported normal vision and no
                                                                      avoid pausing between the two words. A high-quality
language impairments, were paid 8 euros/hour to participate
                                                                      microphone (Philips SBC ME 570) recorded participants’
in this and another eye-tracking experiment (not reported             productions for the entire duration of the trial (5.5 seconds);
here). One participant was replaced because of excessive              speech onset latencies, and the duration of the pause
head movements. Sample size was determined on the basis
                                                                      between names (if present) were then measured offline (in
of previous research (Griffin, 2003; Meyer et al., 2007)
                                                                      Praat; Boersma & Weenink, 2010).
Materials
   We selected 128 black and white line drawings from the
picture naming norms of Bates, et al. (2003). Of these, 64
pictures with high name agreement were used as left
pictures. Left pictures were named first, so we refer to the
left picture names as word1. For half the items (Long),
word1 ranged from 2 to 4 syllables (15 2-syllable words, 11
3-syllable words, and 6 4-syllable words)4, with a mean
length of 2.31 syllables (SD = 0.64). The other 32 pictures
had monosyllabic names (Short). Short and long names
were yoked in pairs matched for name agreement (Short:
.93(.10), Long: .91(.10); t(31)= 1.17, p > .2), log-frequency                Figure 1: A sample trial illustrating manipulations of
(Short: 2.62(.46), Long: 2.54(.45); t(31)= 1.51, p > .1) in             word1 length (short vs. long) and word2 retrieval difficulty
SUBTLEX-DE (Brysbaert et al., 2011), and initial                                       (intact vs. degraded pictures).
phoneme.
   Sixty-four additional pictures were used as right pictures,                  Presentation was controlled using Experiment
and were always named second in the task (word2). Two                 Builder (Version 1.10.165). Before each trial, a fixation dot
right pictures were associated with each pair of left pictures.       was presented where the left picture would subsequently
Right pictures had high name agreement (M = .94, SD =                 appear. As soon as the participant fixated it, the
.11); word2 had a mean length of 2.14 syllables (SD = .71),           experimenter triggered presentation of the stimuli (this was
a mean frequency of 2.53 (SD = .64), and was semantically             also used for drift correction). The left and right pictures
and phonologically unrelated to each word1 it was paired              were then displayed simultaneously on opposite sides of the
with. We created degraded versions of all right pictures by           screen, 324 pixels (or about 9° of visual angle) apart. All
superimposing a mask of ten parallel white lines (about 35pt          pictures were scaled to a dimension of 290x290 pixels, with
apart, and about 15pt-thickness; see Figure 1); on average            surrounding interest areas measuring 405x307 pixels (i.e.,
the mask deleted 35% of all non-white pixels (SD = 2.3 %,             11° of visual angle horizontally, 9° vertically).
min = 30%, max = 41%).                                                          The eye-tracker was calibrated twice using a nine-
                                                                      point calibration grid, first after two practice trials, and then
Design and Procedure                                                  halfway through the session. The first trial after the practice
   Length varied within participants and items, whereas               session was a warmup trial, and was not analyzed. A session
Degradation varied within items but between participants.             lasted 15-20 minutes.
To control for differences due to uninteresting properties of
the right pictures, we constructed two lists of items,                                             Results
reversing pairings of left and right pictures (e.g., if in list 1        Only trials in which both pictures were named fluently
Bank was paired with Hund, and Brücke with Krone, list 2              (i.e., with no repetitions or filled pauses, and with a silent
featured Bank – Krone and Brücke – Hund); 8 random                    pause no longer than 200ms between the words) and using
orders were generated for each list.                                  the expected names were analyzed (intact group: 87.99%;
                                                                      degraded: 83.01%). Following Meyer et al. (2007), we also
                                                                      discarded trials on which the pictures were not fixated in the
   4 Variation in the Long condition was not sufficient to allow      order of mention (only one trial, degraded group), and trials
treating this variable as a continuous predictor in the analyses.     on which the right picture was not fixated before speech
Instead, Length was treated as a categorical predictor (Short vs.     onset (intact: 148 trials, or 16.43%; degraded: 34 trials, or
Long) throughout.
                                                                  2063

4.00%)5, as on such trials the gaze-to-speech lag would have           First-Pass Gaze to the Left Picture
been negative.                                                            The time spent looking at the left picture before gaze was
   For the remaining trials we analyzed speech onset                   shifted to the right was not affected by Degradation,
latencies, first-pass gaze to the left picture (the sum of all         whether alone (B=17ms, SE=27, t=.63; χ2(1)=0.35, p=.555,
fixations to the left picture before the shift of gaze to the          CI=[-36,71]; see Table 1, bottom) or in interaction with
right picture), and the gaze-speech lag (time between the              Length (B=-10ms, SE=24, t=-.43; χ2(1)=0.18, p=.668,
end of the first-pass gaze to the left picture and speech              CI=[-58,37]). However, left pictures were fixated for longer
onset). In all analyses, we fit linear mixed-effects models            if they had long than short names (B=-66ms, SE=25, t=-
using the lme4 package (D. Bates, Maechler, & Dai, 2014)               2.70; χ2(1)=7.21, p=.007, CI=[-115,-18]), confirming that
in R (R, Version 3.1.3). Fixed effects were contrast coded             speakers shifted their gaze as soon as they completed
and centered. Random effects structure was maximal (Barr,              phonological retrieval for word1.
Levy, Scheepers, & Tily, 2013). All p values are from log-
likelihood ratio tests; 95% confidence intervals for model                                       Discussion
estimates are from the confint function (method=“Wald”).
We report the critical speech-gaze lag analyses first.                    We asked speakers to produce fluent two-word utterances
                                                                       and showed that the way they coordinate planning of the
Gaze-Speech Lag                                                        second word and articulation of the first word depends on
                                                                       both the length of the first word and the difficulty associated
   As expected, the gaze-speech lag was both shorter when
                                                                       with retrieving the second word. The gaze-speech lag was
word1 was long than when it was short (B=65ms, SE=12, t=
                                                                       shorter when participants were preparing to produce a long
5.56, χ2(1)=21.46, p<.001, CI=[42,88]) and longer for
                                                                       word1 and longer when word2 was harder to retrieve.
participants naming degraded than intact right pictures (B=-
                                                                          However, we found no evidence for an interaction
140ms, SE=62, t=-2.24, χ2(1)=4.63, p=.031, CI=[-262,-17];
                                                                       between word1 length and word2 retrieval difficulty. As
see Table 1, top). Crucially, however, there was no
                                                                       expected, speakers in both groups took longer to articulate
interaction between Length and Degradation (B=-14ms,
                                                                       word1 when it was polysyllabic (554ms for the intact group,
SE=21, t=-.69, χ2(1)=0.47, p=.491, CI=[-55, 26]).
                                                                       539ms for the degraded group) than when it was
                                                                       monosyllabic (401ms for the intact group, 393ms for the
Speech Onset Latencies
                                                                       degraded group). This difference (about 150ms) is actually
   After removing a further 7 (0.40%) outliers (longer than            larger than the difference in speech onset times between the
2500ms), we found speech onset latencies were not affected             two groups of speakers (about 125ms). So, speakers in the
by Length, whether alone (B=-14ms, SE=27, t=-.50;                      degraded group could have had sufficient extra time during
χ2(1)=0.21, p=.645, CI=[-67,40]), or in interaction with               the production of a long word1 to compensate for the
Degradation (B=5ms, SE=25, t=.19; χ2(1)=0.04, p=.846,                  additional difficulty associated with retrieving the name of a
CI=[-44,54]). However, speech onset latencies were longer              degraded picture. In other words, if these speakers had
for participants in the degraded than in the intact group (B=-         planned proactively, they could have started speech earlier
125ms, SE=60, t=-2.09; χ2(1)=4.80, p=.028, CI=[-243,-8];               (with respect to the gaze shift) when word1 was long than
see Table 1, middle).                                                  when it was short, as only in the latter case delaying speech
                                                                       onset would have benefitted fluency. Had they done so,
    Table 1: Mean gaze-speech lag, speech onset latency, and           gaze-speech lags would have been longer for participants in
  first-pass gaze to the left picture, in milliseconds (standard       the degraded group than participants in the intact group (as
  deviation of participants’ means in brackets), as a function         we observed) but more so when participants were preparing
                of word1 Length and Degradation.                       to produce a short word1, than when they were preparing a
                                                                       long word1.
     Gaze-speech lag                                                      This is not what we observed. Instead, participants in the
                            Degraded               Intact              degraded group appear to have used a different strategy,
     Long                   437(201)               311(150)            delaying speech onset regardless of word1 length. Therefore,
     Short                  504(210)               362(158)            a strong version of Griffin’s (2003) proposal is ruled out by
   Speech onset latency                                                our findings, as our speakers did not appear to be able to
                            Degraded                Intact             combine estimates of articulation duration with estimates of
   Long                     1108(169)               985(166)           retrieval difficulty in order to precisely time articulation (of
   Short                    1102(211)               979(201)           word1) with respect to planning (of word2).
   First-pass gaze to the left picture                                    Meyer and colleagues (2007)’s proposal, instead, is
                            Degraded                Intact             compatible with our results. First, it provides an alternative
   Long                     678(77)                 699(100)           explanation of the reversed word-length effect on the gaze-
   Short                    619(71)                 635(100)           speech lag, which does not require a proactive planning
                                                                       mechanism. In addition, it may also explain the later speech
   5 Perhaps parafoveal information was sufficient for speakers to     onsets for speakers in the degraded group, as Meyer et al.
identify intact right pictures more often than degraded ones.          (2007) recognized that speakers may not always start
                                                                   2064

      Figure 2: First-pass gaze to the left picture, gaze-speech lag and speech onset latency (means and 95% bootstrap CI), as
                                               a function of word1 Length and Degradation.
articulation as soon as the articulatory code of the first               distortion in the spectral properties of the sounds they
syllable of a word has been retrieved.                                   generate. We are not aware of any studies that investigated
   We suggest that speakers in the degraded group buffered               whether speakers anticipate and correct for the duration of a
the first syllable of word1 when word2 representations failed            sound in a similar way as they do for spectral properties
to reach some activation threshold sufficiently early, or                (e.g., pitch).
levels of competition within the production system (see                     Second, in order to show the expected behavior under a
Nozari, Dell, & Schwartz, 2011) remained too high.6                      proactive planning account, our speakers would have had to
Importantly, this type of planning mechanism can be                      anticipate not just duration, but also retrieval difficulty. The
considered reactive rather than proactive: It deals with                 latter is, unlike duration or pitch, a property of the process
difficulties (with word2 retrieval) as they arise. It need not           of planning itself, rather than an externally perceivable
involve a mechanism that dynamically anticipates the                     outcome of the planning process. As such, anticipating
likelihood of future difficulties, deploying different planning          retrieval difficulty might involve a kind of “second-order”
strategies depending on this likelihood being higher (i.e.,              forward model. Speakers might be able to learn such
when word1 is short) or lower.                                           forward models, but perhaps only with extensive training.
   Interestingly, based on our findings, it appears that speech             In conclusion, the reversed word-length effect cannot be
is not planned proactively at the level of whole words. This             interpreted as evidence that the flexibility of speakers’
appears to contrast with what we know about planning at the              planning reflects the workings of a proactive mechanism.
level of single sounds or syllables (e.g., Hickok, 2012),                However, speakers are able to reactively compensate for
where there is evidence that speakers build forward models               retrieval difficulty, delaying speech onset when the need
of upcoming speech movements that allow them to                          arises.
anticipate (and quickly correct, if necessary) what they are
going to sound like (e.g., Niziolek, Nagarajan, & Houde,                                     Acknowledgments
2013).                                                                   We thank Raphael Morschett for assistance with preparation
   What might account for such discrepancy? We see at least              of the word lists and data collection. This research was
two possibilities. First, research into forward models for               supported by research funds assigned to the Chair of
speech has largely focused on speakers’ ability to correct a             Psycholinguistics by the University of Saarland. Chiara
                                                                         Gambi is supported by a Leverhulme Trust Research Project
   6 Alternatively, retrieval difficulties with word could have
                                                       2                 Grant to Hugh Rabagliati and Martin Pickering (RPG-2014-
interfered with preparation of the articulatory code for the first       253).
syllable of word1, and slowed it down. However, note that
articulatory retrieval does not appear to impose huge demands on
central attention (Roelofs & Piai, 2011). Also, this interference                                 References
account would have also predicted a smaller effect of degradation        Barr, D. J., Levy, R., Scheepers, C., & Tily, H. (2013).
on the gaze-speech lag when word1 was long, because with long               Random effects structure for confirmatory hypothesis
words the temporal overlap between articulatory encoding for the
first syllable of word1 and word2 retrieval should have been shorter.
                                                                     2065

  testing: Keep it maximal. Journal of Memory and                  Meyer, A. S. (1996). Lexical access in phrase and sentence
  Language, 68(3), 255-278.                                          production: Results from picture-word interference
Bates, D., Maechler, M., & Dai, B. (2014). Lme4: Linear              experiments. Journal of Memory and Language, 35(4),
  mixed-effects models using Eigen and S4 (Version 1.1-7).           477-496.
  Retrieved from http://lme4.r-forge.r-project.org/                Meyer, A. S., Belke, E., Häcker, C., & Mortensen, L.
Bates, E., D’Amico, S., Jacobsen, T., Székely, A.,                   (2007). Use of word length information in utterance
  Andonova, E., Devescovi, A., . . . Pléh, C. (2003). Timed          planning. Journal of Memory and Language, 57(2), 210-
  picture naming in seven languages. Psychonomic Bulletin            231.
  & Review, 10(2), 344-380.                                        Meyer, A. S., Roelofs, A., & Levelt, W. J. M. (2003). Word
Boersma, P., & Weenink, D. (2010). Praat: Doing phonetics            length effects in object naming: The role of a response
  by computer (Version 4.6.22) [Computer Software].                  criterion. Journal of Memory and Language, 48(1), 131-
  Retrieved from http://www.praat.org/                               147.
Brysbaert, M., Buchmeier, M., Conrad, M., Jacobs, A. M.,           Meyer, A. S., & Van der Meulen, F. F. (2000). Phonological
  Bölte, J., & Böhl, A. (2011). The word frequency effect:           priming effects on speech onset latencies and viewing
  A review of recent developments and implications for the           times in object naming. Psychonomic Bulletin & Review,
  choice of frequency estimates in German. Experimental              7(2), 314-319.
  Psychology,      58(5),     412-424.     doi:10.1027/1618-       Niziolek, C. A., Nagarajan, S. S., & Houde, J. F. (2013).
  3169/a000123                                                       What does motor efference copy represent? Evidence
Damian, M. F., Bowers, J. S., Stadthagen-Gonzalez, H., &             from speech production. Journal of Neuroscience, 33(41),
  Spalek, K. (2010). Does word length affect speech onset            16110-16116.
  latencies when producing single words?. Journal of               Nozari, N., Dell, G. S., & Schwartz, M. F. (2011). Is
  Experimental Psychology: Learning, Memory, and                     comprehension necessary for error detection? A conflict-
  Cognition, 36(4), 892.                                             based account of monitoring in speech production.
Experiment-Builder. (Version 1.10.165) [Computer                     Cognitive Psychology, 63(1), 1-33.
  Software]. Ottawa, Ontario, Canada: SR Research.                 R. (Version 3.1.3) [Computer Software]. Vienna, Austria: R
  Retrieved from www.pstnet.com                                      Development Core Team. Retrieved from http://www.R-
Griffin, Z. M. (2001). Gaze durations during speech reflect          project.org
  word selection and phonological encoding. Cognition,             Roelofs, A., & Piai, V. (2011). Attention demands of
  82(1), B1-B14.                                                     spoken word planning: A review. Frontiers in
Griffin, Z. M. (2003). A reversed word length effect in              Psychology, 2, 10.3389/fpsyg.2011.00307.
  coordinating the preparation and articulation of words in        Sandra, D. (1990). On the representation and processing of
  speaking. Psychonomic Bulletin & Review, 10(3), 603-               compound words: Automatic access to constituent
  609.                                                               morphemes does not occur. Quarterly Journal of
Griffin, Z. M., & Bock, K. (2000). What the eyes say about           Experimental Psychology, 42(3), 529-567.
  speaking. Psychological Science, 11(4), 274-279.                 Smith, M., & Wheeldon, L. (1999). High level processing
Hickok, G. (2012). Computational neuroanatomy of speech              scope in spoken sentence production. Cognition, 73(3),
  production. Nature Reviews Neuroscience, 13(2), 135-               205-246.
  145.                                                             Smith, M., & Wheeldon, L. (2004). Horizontal information
Jacobs, C. L., & Dell, G. S. (2014). ‘hotdog’, not ‘hot’‘dog’:       flow in spoken language production. Journal of
  the phonological planning of compound words.                       Experimental Psychology: Learning, Memory, and
  Language, Cognition and Neuroscience, 29(4), 512-523.              Cognition, 30(3), 675-686.
Konopka, A. E. (2012). Planning ahead: How recent                  Van de Velde, M., Meyer, A. S., & Konopka, A. E. (2014).
  experience with structures and words changes the scope             Message formulation and structural assembly: Describing
  of linguistic planning. Journal of Memory and Language,            “easy” and “hard” events with preferred and dispreferred
  66(1), 143-162.                                                    syntactic structures. Journal of Memory and Language,
Konopka, A. E., & Meyer, A. S. (2014). Priming sentence              71(1), 124-144.
  planning. Cognitive Psychology, 73, 1-40.                        Wolpert, D. M., & Flanagan, J. R. (2001). Motor prediction.
Levelt, W. J. M. (1989). Speaking: From intention to                 Current biology, 11(18), R729-R732.
  articulation. Cambridge, MA: MIT Press.
Levelt, W. J. M., Roelofs, A., & Meyer, A. S. (1999). A
  theory of lexical access in speech production. Behavioral
  and Brain Sciences, 22(1), 1-75.
Martin, R. C., Crowther, J. E., Knight, M., Tamborello, F.
  P., & Yang, C.-L. (2010). Planning in sentence
  production: Evidence for the phrase as a default planning
  scope. Cognition, 116(2), 177-192.
                                                               2066

