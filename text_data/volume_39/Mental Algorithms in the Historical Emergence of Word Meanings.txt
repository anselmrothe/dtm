              Mental Algorithms in the Historical Emergence of Word Meanings
                                           Christian Ramiro (chrisram@berkeley.edu)
                                                        Cognitive Science Program
                                                     University of California, Berkeley
                                           Barbara C. Malt (barbara.malt@lehigh.edu)
                                                         Department of Psychology
                                                              Lehigh University
                                          Mahesh Srinivasan (srinivasan@berkeley.edu)
                                         Department of Psychology, Cognitive Science Program
                                                     University of California, Berkeley
                                                Yang Xu (yang xu ch@berkeley.edu)
                                         Department of Linguistics, Cognitive Science Program
                                                     University of California, Berkeley
                              Abstract                                     Our starting point is a set of influential ideas from cogni-
                                                                        tive science and linguistics suggesting that word meanings or
   Words frequently acquire new senses, but the mental process
   that underlies the historical emergence of these senses is often     categories might be structured in non-arbitrary ways. For ex-
   opaque. Many have suggested that word meanings develop in            ample, pioneering work by Rosch (Rosch, 1975) showed that
   non-arbitrary ways, but no attempt has been made to formalize        common semantic categories signified by words such as bird
   these proposals and test them against historical data at scale.
   We propose that word meaning extension should reflect a drive        and furniture tend to exhibit a prototype structure, such that
   towards cognitive economy. We test this proposal by exploring        certain members of a category are more representative than
   a family of computational models that predict the evolution of       others (e.g., robins and sparrows are more representative as
   word senses, evaluated against a large digitized lexicon that
   dates back 1000 years in English language history. Our find-         birds than penguins or bats are). Although this theory has
   ings suggest that word meanings not only extend in predictable       since been adapted to describe how word meanings might be
   ways, but also that they do so following an historical path that     structured (Lakoff, 1987) or extended over time (Geeraerts,
   tends to minimize cognitive cost - through a process of nearest-
   neighbor chaining. Our work contributes a formal approach to         1997), it has not been computationally specified or evalu-
   reverse-engineering mental algorithms of the human lexicon.          ated broadly in accounting for historical patterns in how word
   Keywords: Word meaning; semantic change; polysemy;                   senses emerge. A prominent alternative proposal is exem-
   chaining; nearest neighbor algorithm; lexicon                        plar theory (e.g., Medin & Schaffer, 1978; Nosofsky, 1986),
                                                                        which suggests that all encountered members of the category
   Over history, words have frequently acquired new senses,
                                                                        are stored and used in categorization judgments, although dif-
and become polysemous (Bréal, 1897). But the mental pro-
                                                                        ferent members may be weighted differently. This proposal
cess that underlies the historical emergence of word senses
                                                                        has also been used to describe how language might change
is often opaque. Wittgenstein’s notion of family resemblance
                                                                        over time, particularly concerning phonological and seman-
(Wittgenstein, 1953, p31-32) highlights the challenge for re-
                                                                        tic representation (Bybee, 2006). To our knowledge, how-
searchers, showing that the many senses of the word game
                                                                        ever, there has been no formal comparison of prototype and
form “a complicated network of similarities overlapping and
                                                                        exemplar theories with respect to their ability to explain the
criss-crossing” with nothing identifiably in common (as for
                                                                        historical emergence of word senses.
board games, card games, ball games, Olympic games, and
so on). The network is presumably a reflection of the com-                 A critical addition to this theoretical terrain is the idea of
plex path the word game took in the historical development              chaining - popularized by Lakoff and other scholars (Lakoff,
of its meaning. Decades of research have suggested possi-               1987; Malt, Sloman, Gennari, Shi, & Wang, 1999) - as a
ble ways that word meanings might be mentally structured                possible mechanism that constrains word meaning extension.
or extended over time, but none has been tested formally                Chaining operates by linking an emerging idea (an incipi-
against historical data at scale. We propose that word mean-            ent word sense) to a highly-related, already lexicalized word
ings should develop historically in ways that minimize cog-             sense. When this process repeats over time, a chained struc-
nitive effort, hence reflecting a drive towards cognitive econ-         ture in meaning space results. Recent work by Xu et al.
omy (Zipf, 1949; Rosch, 1975). We test this proposal by for-            (2016) has explored a preliminary version of this proposal
malizing previous theories in computational models that pre-            via a nearest-neighbor model in a single semantic domain
dict how word senses might emerge over time, contributing a             – household containers – but no systematic formalization or
principled approach to reverse-engineering mental algorithms            evaluation of chaining has been applied to explain the his-
of the human lexicon.                                                   torical emergence of word senses more broadly. Further, al-
                                                                    986

though chaining seems plausible as a mechanism, its theoret-
                                                                          Table 1: Proposed models of word meaning extension.
ical value has been limited in two respects: 1) No work has
formally specified why chaining might be a preferred mech-               Name                Description
anism for the development of word meanings; 2) No large-                 Random (null)       mt+1 ∼ random draw m∗
scale assessment of chaining vs. alternative mechanisms has              Exemplar            mt+1 ∼ Emi [sim(m∗ , mi )]
been performed against historical records of word sense ex-              Prototype           mt+1 ∼ sim(m∗ , prototype(m1 , ..., mt ))
tension, leaving open how chaining fares with respect to alter-          Progenitor          mt+1 ∼ sim(m∗ , m1 )
natives. These issues leave open the question of whether the             Local               mt+1 ∼ sim(m∗ , mt )
evolution of word meanings follows a cognitively predictable             Chaining            mt+1 ∼ maxti=1 sim(m∗ , mi )
path, and if so, what principles explain this process.
   In the current work, we hypothesize that the emergence
of word meanings should follow an historical path that min-
                                                                      historical emerging order of all senses of a word. All of our
imizes collective cognitive effort. In particular, we propose
                                                                      models are parameter-free and thus make minimal assump-
that chaining should be a preferred algorithm for extending
                                                                      tions in the computational formulation.
word meanings across history because it tends to minimize
                                                                         1. The random algorithm – or null model – predicts the
the cognitive cost of linking novel ideas with existing ones
                                                                      historical emergence of a word’s senses to be random. This
- a critical property not previously considered with regard to
                                                                      would only be plausible if word senses emerge purely based
historical sense extension. To test the validity of this argu-
                                                                      on immediate communicative needs with no further cognitive
ment, we motivate nearest-neighbor chaining with tree-based
                                                                      constraints.
computer algorithms that minimize edge lengths in a graph.
We then formalize the process of chaining as a cognitively               2. The exemplar algorithm adapts from work by Nosofsky
economical strategy for encoding novel ideas into an existing         (1986), whereby the emerging sense at t + 1 is predicted to
lexicon (cf. Xu, Malt, & Srinivasan, 2016).                           be the one that bears the highest semantic similarity on av-
                                                                      erage (or the highest sum of semantic similarities, which is
   We critically assess our proposal by developing a family
                                                                      equivalent in our case) with existing senses of a word at time
of computational algorithms of word meaning extension - in-
                                                                      t. We define semantic similarity identically in all algorithms,
spired by the previous literature that described above - and
                                                                      and we defer its formal definition to a later section.
evaluate them against a large historical database of word-
meaning records in English, spanning over 1,000 years. Our               3. The prototype algorithm is adapted from work by Rosch
research extends a growing body of work which suggests                (1975) and Geeraerts (1997) and predicts the emerging sense
that structures of language conform to efficient design princi-       at t + 1 to be the one that bears the highest semantic similar-
ples (Zipf, 1949; Rosch, 1975; Piantadosi, Tily, & Gibson,            ity with the prototypical sense at time t. The prototype at t
2011; Kemp & Regier, 2012; Kirby, Tamariz, Cornish, &                 is defined as the sense that bears the highest semantic simi-
Smith, 2015), by bringing the perspective of cognitive econ-          larity with existing senses of a word prototype(m1 , ..., mt ) ←
omy to bear on the evolution of polysemy.                             maxi ∑ j6=i sim(m j , mi ). Thus, this algorithm allows the most
                                                                      representative sense of a word to change as a function of time,
    Modeling the emergence of word meanings                           as more word senses develop.
                                                                         4. The progenitor algorithm is a variant of the prototype
Computational formulation                                             model that assumes a fixed prototype that is always the ini-
We present here a formulation of five cognitive algorithms            tial, progenitor word sense (i.e., the earliest sense recorded in
that might predict the historical emergence of word mean-             history). It predicts the emerging sense at t + 1 to be the one
ings, along with a null model. Given the initial, progeni-            that bears the highest semantic similarity (among all candi-
tor meaning of a word, each non-null algorithm postulates             date senses) with respect to the progenitor sense.
a distinct chaining mechanism by which novel word senses                 5. The local algorithm assumes that word meanings
might emerge over time by “attaching to” existing meanings.           emerge in a temporal linear chain, where the emerging sense
Each algorithm generates a prediction of the historical or-           at t + 1 is the one that bears the highest semantic similarity
der through which the meanings for any given word should              with the sense that appears at time t. Critically, senses that
emerge, which we then test against the historical record. In          appear prior to t have no influence on the emerging sense
effect, we reverse-engineer the mental mechanisms of sense            at t + 1 on this model. This algorithm posits that sense ex-
extension.                                                            tension will yield minimal cost locally between consecutive
   Table 1 summarizes the full set of proposed algorithms.            time points, as opposed to yielding globally minimal cost (de-
Here m stands for meaning or word sense, and t stands for             scribed below).
time. Each algorithm infers the word sense that emerges at               6. The chaining (or nearest-neighbor) algorithm is closely
time t + 1 (mt+1 ), based on existing senses of a word up to          related to Prim’s algorithm for constructing a minimal span-
time t (m1 , ..., mt ). The inferred sense is drawn from the can-     ning tree (Prim, 1957) - but with a fixed (as opposed to ran-
didate pool of senses (denoted by m∗ ) that appear after t for        dom) starting point, i.e., it always begins with the progeni-
a given word. A perfect model would fully recapitulate the            tor sense of a word. In essence, this algorithm predicts the
                                                                  987

 Exemplar (cost = 6.9)                 Prototype (cost = 5.7)                  Progenitor (cost = 9.9)
               t14                                      t14                                         t14
                                                                                                                          stem off directly from senses). The prototype algorithm pre-
                    t12
                     t10
                             t13                             t11
                                                              t9
                                                                     t12                                  t12
                                                                                                            t9 t7
                                                                                                                   t8     dicts a dynamic radial structure (Lakoff, 1987), where tem-
 t15                                    t15                                     t15
                                                                 t7
                         t11
                                                                                                                          poral chains are established by linking novel senses to pro-
     t7                    t2                  t8                  t3                      t10                  t3
      t8
             t5                                           t5                                           t5                 totype senses, while allowing the prototype to change over
                                                              t2                       t11       t4         t2
  t9
            t4       t3
                                         t13
                                               t10   t4
                                                                                 t13
                                                                                                                          time. The progenitor algorithm predicts a strict radial struc-
          t6                                       t6                                          t6                         ture where all senses stem from the earliest progenitor mean-
                 Local (cost = 3.8)                        Chaining [nearest neighbor] (cost = 2.9)                       ing. The local algorithm predicts a linear temporal chain of
                               t14                                               t11
                                t12           t13                                        t9        t10
                                                                                                                          senses by attaching each emerging sense to the existing sense
               t15                                                                        t8
                                  t11     t10                   t15
                                                                                              t7                          that appears one time point earlier. Finally, the chaining al-
                        t5
                                  t4
                                            t9                           t12
                                                                                    t4
                                                                                                t6                        gorithm renders a tree structure that branches off as needed
                     t6               t2                               t13                 t2                             to preserve nearest-neighbor relations between emerging and
                                 t3                              t14               t3
                t7
                            t8                                               t5                                           existing senses. Importantly, the chaining algorithm yields
                                                                                                                          the minimal aggregated edge lengths, hence rendering a min-
Figure 1: Simulation of the proposed algorithms of word                                                                   imal cost in semantic space. This result is robust to variations
sense extension. The solid red circle symbolizes the progeni-                                                             in simulation parameters and is a consequence of the close
tor sense of a word. The blue circles represent emerging word                                                             link between the nearest-neighbor chaining algorithm and the
senses, and the arrows indicate the predicted path that each                                                              concept of a minimal spanning tree.
algorithm makes about order of emergence. The time labels                                                                    Below, we test the extent to which these algorithms can
indicate the predicted sequence of emergence. The cost is the                                                             recapitulate the emergence of word senses, as recorded in a
aggregated Euclidean distances traversed by the arrows.                                                                   large historical lexicon of English.
                                                                                                                                                Treatment of data
emerging sense at t + 1 to be the one that bears the highest se-
mantic similarity to any of the existing senses up to t, hence                                                            Historical lexicon
rendering a chain that connects nearest-neighboring senses                                                                To evaluate our proposed algorithms, we used the Histor-
over time. In contrast with the other algorithms described                                                                ical Thesaurus of English (HTE) (Kay, Roberts, Samuels,
above, this chaining algorithm is also similar to single link-                                                            Wotherspoon, & Alexander, 2015) - a unique large-scale
age clustering (Gower & Ross, 1969) which tends to yield a                                                                historical lexicon constructed from the Oxford English
tree (i.e., each tree node is a sense in this case) with mini-                                                            Dictionary. This database includes approximately 800,000
mal edge lengths among nodes of a graph (i.e., the graph is                                                               word forms and their senses, dated and recorded over a span
a network of senses of a word, developed in history). Due                                                                 of over 1,000 years - from Old English to the present day.
to this property, the chaining algorithm assumes the least cu-                                                            Each word sense in the HTE is annotated with the date of
mulative historical cognitive effort for the extension of word                                                            its emergence (and, where applicable, obsolescence) and
senses (where effort is inverse to the degree of association be-                                                          part of speech, and is structured in a fine-grained semantic
tween emerging and existing senses of a word), providing the                                                              hierarchy that features about a quarter of a million concepts.
computational implementation of our hypothesis.                                                                           Consecutive tiers of the hierarchy typically follow a IsA or
                                                                                                                          PartOf relation. For example, one sense of the word game
Simulation of sense extension algorithms                                                                                  under the HTE code “01.07.04.04” is defined in terms of a
To illustrate how nearest-neighbor chaining would yield a                                                                 four-tier hierarchy: The world (01)→Food and drink
near-minimal-cost historical path, we provide a simulation for                                                            (01.07)→Hunting (01.07.04)→Thing hunted/game
the proposed algorithms of sense extension as follows.                                                                    (01.07.04.04).
     We generated 15 randomly placed points in a two-
dimensional plane that represents the meaning space for a hy-                                                             Measure and validation of semantic similarity
pothetical word (see Figure 1). We took Euclidean distance                                                                To quantify similarity between word senses, we defined a
between-points as a proxy for semantic distance (or inverse                                                               measure using the semantic hierarchy in the HTE and then
semantic similarity) between two senses. We also designated                                                               validated it against human judgments. Specifically, we ap-
the bottom-right point in the space as the progenitor sense,                                                              proximated psychological similarity between a pair of word
i.e., it is the earliest seeding sense for the word that is a given                                                       senses sim(mi , m j ) by a common measure of similarity used
to any algorithm. We then applied the family of sense exten-                                                              in psychology that is bounded in the range of (0,1) (Nosofsky,
sion algorithms to the remaining data points and visualized                                                               1986; Shepard, 1987):
the path of emerging senses predicted by each algorithm. Fig-
ure 1 shows that these algorithms yield distinct typologies and                                                                                sim(mi , m j ) = e−d(mi ,m j ) .        (1)
paths in the simulated meaning space. Specifically, the exem-
plar algorithm links novels senses to all existing senses based                                                              Here d(mi , m j ) represents thesaurus-based conceptual dis-
on average distances between them (illustrated by chains that                                                             tance between two meanings, which we defined by the inverse
develop from spaces between senses as opposed to those that                                                               of a conceptual similarity measure (s(·, ·)), commonly used in
                                                                                                                      988

natural language processing (Wu & Palmer, 1994; Jurafsky &                      GLOVE word vector model, which has been trained on 6 bil-
Martin, 2009):                                                                  lion words (Faruqui & Dyer, 2015; Pennington, Socher, &
                                                                                Manning, 2014).
                                                                                    Having validated our measure of semantic similarity, we
                                                  2 × |p|                       used it to assess the mental algorithms described above.
        d(mi , m j ) = 1 − s(mi , m j ) = 1 −                    .      (2)
                                              l(mi ) + l(m j )
                                                                                Choices of words
   Here |p| is the number of parent tiers shared by senses mi                   We focused our analyses on explaining word sense extension
and m j , and l(·) is the depth of a meaning in the semantic                    in a set of the most common English words. Specifically,
hierarchy. This measure gives 1 if two meanings are identical,                  we worked with the most frequent 6318 words in the British
and 0 if they have nothing in common. Table 2 illustrates the                   National Corpus (BNC). Some of the word forms are dupli-
calculation of this measure with a concrete example.                            cated in this set because one word can function in multiple
                                                                                part-of-speech categories. However, our results were robust
Table 2: Illustration of conceptual similarity based on two                     regardless of whether we collapsed these words by form or
senses of game recorded in the HTE. Since the two senses                        distinguished them by part-of-speech.
share two parent tiers (i.e., The social world→Leisure) in the
hierarchy, the conceptual similarity is s(•, ?) = 2×2     5+6 = 11 .
                                                                    4                           Model evaluation and results
                                                                                We used model likelihood to assess the performance of each
  Description of sense                HTE code                   Symbol         proposed algorithm.1 We defined likelihood as a probability
  Celebratory social event            03.13.02.02|04                  •         function that specifies the degree to which a model accounts
  Ancient match/competition 03.13.04.01|02.02                         ?         for the entire sequence of senses that historically emerged
                                                                                for a given word. To be concrete, for a sequencce of senses
            03:The Social World •?
                                                                                m1 , m2 , m3 , ..., mt , the likelihood L is the joint probability of
                                                                                observing such a sequence under a certain model M :
    01:Community · · · 13:Leisure •?                                              LM = p(m1 )p(m2 |m1 )p(m3 |m1 , m2 )...p(mt |m1 , ..., mt−1 ).
                                                                                                                                                         (3)
      02:Social Event•                         04:Sports? · · ·                     We assumed that the progenitor sense is always given, so
                                                                                p(m1 ) = 1. For all remaining emerging senses, the set of
  02:Large/Public Event•                 01:Match/Competition?                  algorithms     can be evaluated by calculating likelihood based
                                                                                on their specifications in Table 1. For example, the progenitor
                                                                                model would yield a likelihood for the emerging sense at t = 2
 04:Celebratory games• 02:Series of (as public spectacle)? (conditioned on that appeared at t = 1) as follows:
                                   02:Greek & Roman Antiquity?                                                        sim(m∗ , m1 )
                                                                                             p(m2 |m1 ) =                                       .        (4)
                                                                                                               ∑m∗ ∈{m2 ,...,mt } sim(m∗ , m1 )
                                                                                    The algorithm then steps through each point in time and
   We validated this measure of semantic similarity via stan-                   the likelihood correspondingly calculates the degree to which
dard techniques in natural language processing, by evaluating                   the algorithm predicts the true emerging sense at that point,
its performance in predicting human judgments of word sim-                      among a candidate pool of senses that appear after.
ilarities. Following Resnik (1995), we approximated word                            Because our null hypothesis is that there exists no pre-
similarity by using the pair of senses for the two words                        dictability in how word senses develop in history, we eval-
that results in maximum sense similarity, defined as follows:                   uated each cognitive algorithm against the random null algo-
wordsim(wi , w j ) = maxmi ∈senses(wi ),m j ∈senses(w j ) s(mi , m j ).         rithm, using the log likelihood ratio (LLR) - a standard metric
   Our measure of semantic similarity yielded a Spearman’s                      for model comparison in statistics: LLR = log(LM /Lnull ).
correlation of 0.43 (p < 0.001) on Lex-999 (Hill, Reichart,                     This quantity should be greater than 0 if a given model ac-
& Korhonen, 2015), which provides a well-known data set                         counts for word sense extension better than the null, and the
of human word similarity judgments. The performance of                          converse if the null does better. For any given word, the likeli-
our measure of semantic similarity is better than the corpus-                   hood function of the null can be determined theoretically, and
based skipgram (Word2Vec) model, which has been trained                         it is simply the inverse of factorial of N − 1 for a word with
                                                                                                                      1
on 1 billion words of Wikipedia text (Mikolov, Chen, Cor-                       N senses: Lnull = 1 × N−1      1
                                                                                                                  × N−2                     1
                                                                                                                           × ... × 11 = (N−1)!    . Thus the
rado, & Dean, 2013) and roughly on par with the same model                      log likelihood ratio indicates whether a model predicts the se-
trained on 300 billion words (Faruqui & Dyer, 2015). In ad-                     quence of emerging word senses better than chance.
dition, our measure of semantic similarity obtained a Spear-
                                                                                    1 Because   each of the models we examined is parameter-free,
man’s correlation of 0.52 (p < .001) on Sim-353 (Finkelstein
                                                                                metrics that take into account model complexity such as the
et al., 2001), another common data set of human word relat-                     Bayesian Information Criterion would give identical results to those
edness judgments, which is comparable to the state-of-the-art                   only taking into account likelihood.
                                                                            989

                                                                           senses over time.
                                       1.2
                                             Exemplar                                             Conclusions
                                             Prototype
                                       1.0   Progenitor                    We presented the first large-scale computational investigation
                                             Markov                        of the mental algorithms that determine how words evolve
   Log likelihood ratio against null
                                             Chaining                      new senses over time. We found that the historical emer-
                                       0.8
                                                                           gence of word senses in English is not arbitrary; Instead, it
                                       0.6                                 has exhibited a high degree of predictability over the past
                                                                           millennium. Our findings indicate that the order in which
                                       0.4                                 word senses emerge can be best accounted for by a process of
                                                                           nearest-neighbor chaining, which supports the view that the
                                       0.2
                                                                           historical development of the lexicon follows a trajectory that
                                                                           tends to minimize cognitive effort. Our current analysis fo-
                                                                           cuses on sense extension within individual word forms, but it
                                       0.0
                                                                           would be useful to extend our analysis to examine how dif-
                                                                           ferent words compete to express novel meanings. Our explo-
                                                                           ration of the mental algorithms that underlie historical sense
Figure 2: Summary of model performances against the null.                  extension opens new, interdisciplinary venues for reverse en-
“0.0” on the y-axis indicates performance of the null model.               gineering the evolution of the human lexicon.
Bar height indicates the mean log likelihood ratio averaged
over the entire pool of most common words from the BNC                                         Acknowledgments
corpus. Error bars indicate 95% confidence intervals.                      We thank Marc Alexander for helping with data licensing,
                                                                           and the University of Glasgow and the Oxford University
                                                                           Press for making the Historical Lexicon of English avail-
   Figure 2 summarizes the results. The bar plot shows that                able. We thank Charles Kemp and Terry Regier for discus-
each of the proposed algorithms accounts for the historical                sions on chaining and the Computational and Experimental
data that we examined significantly better chance (p < 0.001               Methods group in Linguistics at UC Berkeley for construc-
from 1-tailed t-tests), reflected in the positive log likelihood           tive comments. This project was partly funded by NSF award
ratios. This observation suggests that the null hypothesis can             SBE-1041707 to the Spatial Intelligence and Learning Center
be rejected: The emerging order of word senses in the English              (SILC) and NSF award SBE-16302040 to MS.
lexicon is not purely random.
   Critically, the nearest-neighbor chaining algorithm yielded                                     References
the highest overall likelihood among all models, and this re-              Bréal, M. (1897). Essai de sémantique: Science des signifi-
sult was statistically significant according to paired t-tests be-           cations. Paris: Hachette.
tween the chaining model and each of the remaining models                  The British National Corpus, version 3 (BNC XML Edi-
(p < 0.001 in all four comparisons). This observation pro-                   tion). (2007). (Distributed by Oxford University Com-
vides evidence that word senses emerge in cognitively effi-                  puting Services on behalf of the BNC Consortium. URL:
cient ways by approximating a minimal spanning tree over                     http://www.natcorp.ox.ac.uk/)
the course of history. As such, these data support our hypoth-             Bybee, J. L. (2006). From usage to grammar: The mind’s
esis about nearest-neighbor chaining as the dominant mental                  response to repetition. Language, 82(4), 711–733.
algorithm for the historical emergence of word senses.                     Faruqui, M., & Dyer, C. (2015). Non-distributional word
   To illustrate the nearest-neighbor chaining process, we vi-               vector representations. arXiv preprint arXiv:1506.05230.
sualized the predicted chaining path for the English word                  Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan,
game. Figure 3 shows a low-dimensional projection (via                       Z., Wolfman, G., & Ruppin, E. (2001). Placing search
multi-dimensional scaling with a random starting point) of                   in context: The concept revisited. In Proceedings of the
all senses of game as a noun, taken from the HTE database.                   10th international conference on world wide web (pp. 406–
As can be seen, the chaining algorithm forms a minimal span-                 414).
ning tree among the senses of game, by linking neighboring                 Geeraerts, D. (1997). Diachronic prototype semantics: A
nodes that are semantically close. Importantly, this process of              contribution to historical lexicology. Oxford: Oxford Uni-
meaning extension tends to support branching and the forma-                  versity Press.
tion of local clusters, identified roughly in this case by the             Gower, J. C., & Ross, G. J. S. (1969). Minimum spanning
three sense groups of “hunting game” (upper-left cluster),                   trees and single linkage cluster analysis. Applied Statistics,
“scheme” (middle cluster), and “sports and entertainment”                    18(1), 54–64.
(upper-right cluster) in Figure 3. This offers a computational             Hill, F., Reichart, R., & Korhonen, A. (2015). Simlex-999:
basis for family resemblance (Wittgenstein, 1953) and poly-                  Evaluating semantic models with (genuine) similarity esti-
semy, by allowing words to develop both related and distinct                 mation. Computational Linguistics.
                                                                     990

   Ultimate success (1380)
                 Game (food) (1848)
                                                                                           A plot (1250)
                                                                                           A plan (1698)                           Standard of performance (1851)
                     Sport derived from (hunting) (1297)                                                                              Game/definite spell of play (1250)
                                                                                           End/purpose/object (1573)                  (Sport) in ancient world (1400)
                   Thing hunted/game (1290)                                                                                             Position/advantage (1677)
                                                          Unscrupulous/rapacious (1883)
                                                                                                          Types of (sport) (1840)            A frolic (1838)
                                                                                                                                             Game (entertainment) (1300)
                                                                                                     Portion of play in a game (1250)         Mere amusement (1250)
                                                                                                                                              Entertainment (OE)
            Animals hunted (1500)                                                                                                             A pastime (entertainment) (OE)
              Caught/killed in hunting (1290)                                                                                               Method of play (1750)
                                                                                                                                        Celebratory games (1400)
           Course of a game (1827)
(Animals) kept for pleasure (1482)                                   Spirit (1747)
                                                         Amorous caressing (1230)        Jest/pleasantry (OE)
                                                                                      Lack of seriousness (1250)
Figure 3: Historical chaining in the English word game. The two-dimensional space is generated by multi-dimensional scaling
based on sense similarities. The solid red circle marks the earliest meaning. The arrows indicate the predicted path from the
chaining algorithm. The annotations include a gloss for the sense and its recorded period of emergence in the HTE.
Jurafsky, D., & Martin, J. H. (2009). Speech and language                            are optimized for efficient communication. Proceedings of
   processing: An introduction to natural language process-                          the National Academy of Sciences, 108(9), 3526–3529.
   ing, computational linguistics, and speech recognition (2nd                     Prim, R. C. (1957). Shortest connection networks and some
   ed.). New Jersey: Pearson Education.                                              generalizations. Bell Labs Technical Journal, 36(6), 1389–
Kay, C., Roberts, J., Samuels, M., Wotherspoon, I., &                                1401.
   Alexander, M. (2015). The historical thesaurus of english,                      Resnik, P. (1995). Using information content to evaluate
   version 4.2. Glasgow: University of Glasgow.                                      semantic similarity in a taxonomy. arXiv preprint cmp-
Kemp, C., & Regier, T. (2012). Kinship categories across lan-                        lg/9511007.
   guages reflect general communicative principles. Science,                       Rosch, E. H. (1975). Cognitive representations of semantic
   336(6084), 1049–1054.                                                             categories. Journal of Experimental Psychology: General,
Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015).                             104(3), 192.
   Compression and communication in the cultural evolution                         Shepard, R. (1987). Toward a universal law of generaliza-
   of linguistic structure. Cognition, 141, 87–102.                                  tion for psychological science. Science, 237(4820), 1317–
Lakoff, G. (1987). Women, fire, and dangerous things: What                           1323.
   categories reveal about the mind. Chicago: University of                        Wittgenstein, L. (1953). Philosophical investigations. Ox-
   Chicago Press.                                                                    ford: Basil Blackwell.
                                                                                   Wu, Z., & Palmer, M. (1994). Verbs semantics and lexical
Malt, B. C., Sloman, S. A., Gennari, S., Shi, M., & Wang,
                                                                                     selection. In Proceedings of the 32nd annual meeting on
   Y. (1999). Knowing versus naming: Similarity and the
                                                                                     association for computational linguistics (pp. 133–138).
   linguistic categorization of artifacts. Journal of Memory
                                                                                   Xu, Y., Malt, B. C., & Srinivasan, M. (2016). Evolution
   and Language, 40(2), 230–262.
                                                                                     of polysemous word senses from metaphorical mappings.
Medin, D. L., & Schaffer, M. M. (1978). Context theory of
                                                                                     In Proceedings of the 38th annual meeting of the cognitive
   classification learning. Psychological Review, 85(3), 207.
                                                                                     science society.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Ef-                         Xu, Y., Regier, T., & Malt, B. C. (2016). Historical semantic
   ficient estimation of word representations in vector space.                       chaining and efficient communication: The case of con-
   arXiv preprint arXiv:1301.3781.                                                   tainer names. Cognitive Science, 40, 2081–2094.
Nosofsky, R. M. (1986). Attention, similarity, and the                             Zipf, G. K. (1949). Human behavior and the principle of
   identification–categorization relationship. Journal of Ex-                        least effort. Boston: Addison-Wesley.
   perimental Psychology: General, 115(1), 39.
Pennington, J., Socher, R., & Manning, C. D. (2014). Glove:
   Global vectors for word representation. In Emnlp (Vol. 14,
   pp. 1532–1543).
Piantadosi, S. T., Tily, H., & Gibson, E. (2011). Word lengths
                                                                             991

