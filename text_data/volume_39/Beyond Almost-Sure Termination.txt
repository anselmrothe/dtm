                                            Beyond Almost-Sure Termination
                                                  Thomas F. Icard (icard@stanford.edu)
                                               Department of Philosophy, Stanford University
                               Abstract                                 conditioning continuous distributions have also been thor-
                                                                        oughly investigated (Ackerman et al., 2011).
   The aim of this paper is to argue that models in cognitive              These important advances notwithstanding, the aim of the
   science based on probabilistic computation should not be re-
   stricted to those procedures that almost surely (with probabil-      present paper is to argue that in cognitive science the focus
   ity 1) terminate. There are several reasons to consider non-         on a.s.-termination is overly restrictive. As a foundation for
   terminating procedures as candidate components of cognitive          theorizing about cognitive processes we should consider the
   models. One theoretical reason is that there is a perfect cor-
   respondence between the enumerable semi-measures and all             class of all probabilistic computations, not just those that a.s.
   probabilistic programs, as we demonstrate here (generalizing         halt. To use terminology introduced more formally below,
   a better-known fact about computable measures and almost-            cognition should be modeled on the more general class of
   surely halting programs). One practical reason is that the line
   between almost sure termination and non-termination is elu-          enumerable semi-measures, rather than the smaller class of
   sive, as well as arbitrary. We argue that this matters not only      computable probability measures. We offer two arguments
   for theorists, but also potentially for a learner faced with the     for this claim, one practical and one theoretical.
   task of inducing programs from experience.
                                                                           The practical argument is that the line between a.s. ter-
                                                                        mination and non-termination is elusive and arbitrary. This
                          Introduction                                  point is illustrated with a simple example, where the bound-
The metaphor of cognition as computation provides a fruitful            ary can be studied concretely. The theoretical argument is
and flexible foundation for cognitive science. While compu-             that the correspondence between the semi-measures definable
tation can be understood broadly to encompass many differ-              by probabilistic machines and enumerable semi-measures is
ent paradigms and formats (Rescorla, 2015), it is generally             more basic and canonical than that between measures defin-
presumed that an upper bound on what can be computed by                 able by a.s.-terminating machines and computable measures.
the human mind is that which can be computed by a Turing                We give a simple, self-contained proof of this first correspon-
machine, or a program in any other universal model of com-              dence (Theorem 1), which subsumes the second as a special
putation, such as lambda calculus, recurrent neural networks            case (Corollary 1). This proof is elementary, and is arguably
with rational weights, combinators, Java programs, and so on.           simpler than direct proofs of the corollary. We also discuss
   Some early proponents of the computational theory of                 some connections to program induction, and possible reper-
mind (e.g., Putnam 1967) focused attention on probabilistic             cussions for probabilistic inference.
computation, allowing randomization in state transitions; and
random mechanisms have been central in psychological mod-                   Background on Probabilistic Computation
els going back at least to stimulus-response theory, which              Consider any universal language for describing computations.
had formal connections to probabilistic automata (Suppes,               Allowing programs in one of these languages access to an
1969). In recent work, computation with random elements                 unlimited source of iid samples from a Bernoulli(0.5) distri-
has taken on new significance, where mental representations             bution brings us to the setting of universal probabilistic lan-
themselves are characterized in terms of probabilistic proce-           guages. For instance, a Turing machine might have an ad-
dures or programs (Goodman et al., 2014), and noise is seen             ditional read tape with an infinite sequence of random bits,
not just as a nuisance, but as deeply tied to an agent’s ca-            while a lambda term might make use of a choice operator ⊕,
pacity for prediction and induction. Although probabilistic             where M ⊕ N reduces to M or N, each with probability 0.5.
machines cannot compute any more functions than determin-               Just as the Church-Turing Thesis states that any two reason-
istic machines, this shift in emphasis raises new and distinct          able deterministic models of computation will be equivalent,
questions. For instance, how expressive is a given class of             one might hypothesize that any reasonable way of adding fair
probabilistic machines for representing useful distributions?           coin flips to these models will give rise to an equivalent model
   Much of the recent theory of probabilistic computation—              of probabilistic computation. For the rest of this paper we will
particularly that motivated by application to cognition—has             remain neutral about which of these versions we adopt.
focused on computable probability distributions, specifically              Non-termination, even for very simple, e.g., monitoring
restricting to procedures that terminate almost surely (a.s.),          processes, is of course a desirable feature of many mecha-
that is, with probability 1. This has given rise to a rich body         nisms involved in control, where inputs are continually pro-
of work. For instance, it can be shown that the computable              cessed (Botvinick and Cohen, 2014). However, our interest
distributions correspond to the a.s.-terminating probabilistic          here is non-termination even for stand-alone programs with-
Turing machines (see, e.g., Freer et al. 2012), as well as to           out input, so we restrict attention to this setting.
the a.s.-terminating stochastic lambda terms (Dal Lago and                 A probabilistic program π, in any machine language, gen-
Zorzi, 2012). The limits of computability in the context of             erates an output w—let us suppose outputs are (or at least
                                                                    2255

encode) binary sequences, so that w ∈ {0, 1}∗ —with some                                 and verifying a.s. termination of a probabilistic program is of
probability, which we will write µπ (w). That is, µπ (w) is the                          even higher complexity (Π02 , see Kaminski and Katoen 2015).
sum of the probabilities of all the execution sequences that                             It follows that the only way to ensure a.s. termination is to re-
halt with output w. The program π implicitly represents a                                strict to smaller, controlled fragments. This is confining both
distribution on binary strings; however, this distribution may                           for the cognitive scientist proposing psychological models,
not be a proper probability distribution on {0, 1}∗ , as it may                          and for the learning agent who may need to construct and in-
be that ∑w µπ (w) < 1. This will happen if the program fails to                          duce programs on the fly.
halt with some probability 1 − ∑w µπ (w) > 0. A function µ for
which ∑w µ(w) ≤ 1 is called a (discrete) semi-measure, and it                            Between Termination and Non-Termination
is called a probability measure if this holds with equality.                             The boundary between a.s.-terminating programs and non-
   A semi-measure µ is (computably) enumerable if it is ap-                              terminating programs often looks quite arbitrary. To illus-
proximable from below; that is, if for each w ∈ {0, 1}∗ there                            trate, we use a very simple example close to recent work on
is a computably enumerable weakly increasing sequence                                    intuitive physics (e.g., Sanborn et al. 2013; Battaglia et al.
q0 , q1 , q2 , . . . of rational numbers, such that lim qi = µ(w).                       2013). This work models people’s ability to understand and
                                                             i→∞
Most semi-measures are not enumerable, but for any prob-                                 predict physical events using probabilistic programs for con-
abilistic program π, the semi-measure µπ will be enumer-                                 structing internal “simulations” that operate in rough accor-
able. To approximate µπ (w) from below, consider the set Wi                              dance with physical laws. The example here is far less so-
of strings v with length l(v) ≤ i, such that π accesses (ex-                             phisticated, merely concerning speed along a single spatial
actly) the bits of v before terminating with output w. Letting                           dimension. Consider a proverbial tortoise-and-hare scenario,
    ∆                                                                                    where the tortoise is moving ahead at a constant rate follow-
qi = ∑v∈Wi 2−l(v) , it is then evident that lim qi = µ(w). The-
                                                     i→∞                                 ing a slight head start, and the erratic rabbit is nonchalantly
orem 1 below states the converse of this observation, that in                            racing to catch up. We might suppose that the hare leaps for-
fact every enumerable semi-measure µ is µπ for some π.                                   ward some random distance about every fourth step that the
   A semi-measure µ is called computable if it is enumerable                             tortoise takes. The question is when, if ever, the hare will
and for each w there is also a computably enumerable weakly                              catch up. Imagine this prediction arising from mental simu-
decreasing approximating sequence q0 , q1 , q2 , · · · → µ(w).                           lations of something like the following program π↓ :
There are computable semi-measures that are not probability
measures, but every enumerable probability measure is com-                                  t = 5; h = 0
putable: we can enumerate the sum ∑w0 6=w µ(w0 ) by dovetail-                               while (h < t):
ing to obtain q∗0 , q∗1 , q∗2 , . . . , and then 1 − q∗0 , 1 − q∗1 , 1 − q∗2 , . . .           t = t + 1
converges from above to 1 − ∑w0 6=w µ(w0 ) = µ(w). Corollary                                   if (flip(0.25)):        h = h + Uniform(1,7)
1 below states that the computable probabilities measures are                               return t
exactly those of the form µπ for some a.s.-terminating pro-
gram π (see, e.g., Freer et al. 2012; Dal Lago and Zorzi 2012).                          For instance, a person observing a rabbit chasing a tortoise
   In addition to encompassing the space of randomized algo-                             might extract a program like π↓ in order to make predictions
rithms, probabilistic programs are of special interest in cog-                           about what will happen some number of steps later.
nitive science because of their ability to provide compact rep-                             Where Hk is the distance traveled by the hare at stage k,
resentations of quite complex distributions, e.g., over com-                             consider the random variable Xk = (5 + k) − Hk , measuring
binatorially rich spaces (Goodman et al., 2014; Piantadosi                               the extent of the tortoise’s current lead. It is easy to show
et al., 2016). By encoding these distributions only implicitly                           that the sequence {Xk } forms a random walk martingale, and
through the program’s objective probability of returning dif-                            specifically that E[Xk+1 | X1 , . . . , Xk ] = Xk , and so E[Xk ] = 5
ferent outputs, they make an attractive candidate for plausible                          for all k. By the recurrence property for symmetric random
representations of subjective probability (see Icard 2016 for                            walks, we reach Xk ≤ 0 at some stage k almost surely. Thus
discussion). Moreover, it is often possible to define programs                           this program π↓ halts with probability 1. (Cf. Chakarov and
for automatically representing conditional distributions, and                            Sankaranarayanan 2013 for powerful a.s.-termination proof
thus to apply and adapt the tools of Bayesian statistics to this                         techniques that cover examples like this.)
setting (Tenenbaum et al., 2011; Freer et al., 2012).                                       While π↓ as written terminates, small changes in the pa-
                                                                                         rameters of the program lead to positive probability of non-
            Why Non-Terminating Programs?                                                termination. For instance, if t is instead incremented by 1 + ε
The enumerable semi-measures form a larger, and arguably                                 at each step, or if the increase in h is drawn uniformly from
more natural, class than the computable measures, but what                               an interval (1, 7 − ε), for ε > 0, then the resulting program
is the reason to include them in our study of cognitive agents?                          π↑ may not halt because the expected distance between the
   The claim of this section is that there is a tension between                          tortoise and hare constantly increases. In particular, there
allowing rich, interesting programs and ensuring those pro-                              will be a constant C > 0, such that for any fixed xk , we have
grams a.s. terminate. It is well known that testing whether a                            E(Xk+1 |xk ) − xk = C. Hence E(Xk+1 ) = E(E(Xk+1 | Xk )) =
deterministic program halts is an undecidable (Σ01 ) problem,                            E(Xk ) +C, from which it follows E(Xk ) = 5 + kC for each k.
                                                                                     2256

This means that the long run expected value of Xk is infinite,                  tails about how the program might be implemented. The idea
and the program fails to halt with some positive probability.                   that we can construe some psychological models in a sim-
   One might suspect that this theoretical distinction could                    ilar manner is very familiar in cognitive science (Marr and
have practical repercussions. Would we not want some guar-                      Poggio, 1976). Characterizations of mental phenomena us-
antee that our program would eventually halt? The problem                       ing grammars, recursive constructions, and other devices that
with this line of thought is that, from a practical perspective,                license unbounded computations are legitimized by potential
non-termination is not any worse than eventual termination                      gain in conceptual clarity and modularity. We understand
but only after an inordinate amount of time. Simulating the                     while-loops, models of Newtonian mechanics, and so on, in
program π↓ above—and terminating computation whenever                           a very general way: we have a good sense of what they can
the number of steps reaches an upper bound of, say, 107 —we                     do, what problems they can be used to solve, and how they
see that the program reaches this upper bound about .01% of                     can be combined with other tools to form even more power-
the time. Though a large majority (∼75%) of runs end within                     ful devices. From this perspective it is unsurprising that such
100 steps, the empirical average runtime is in the tens of thou-                devices would make their way into our cognitive models.
sands.1 Thus, in some small number of cases we would pre-                          Such issues about levels of analysis are beyond the scope
sumably have to terminate computation anyway. From this                         of this paper. The present suggestion is simply that the best
simulation perspective, the behavior of π↑ , taking ε > 0 to be                 arguments favoring liberal use of a.s.-terminating probabilis-
very small, is empirically nearly indistinguishable. The fact                   tic machines as cognitive models should extend to the class
that some of these runs might never terminate is immaterial,                    of all probabilistic machines. Just as there may be practical
practically speaking.                                                           reasons to avoid computable, but algorithmically intractable,
   This argument is about possible non-termination, and it                      procedures in practice, so it may make sense in many cases to
does not distinguish between computable and merely com-                         avoid use of procedures that might not terminate. That does
putably enumerable distributions. If we increment t by a                        not delegitimize their use in cognitive modeling.
computable real number 1 + ε, then, though π↑ might never
halt, µπ↑ is actually a computable semi-measure, with a com-                    Program Induction
putable probability of not halting. However, this situation                     The argument up to this point has been largely negative, that
again may be practically no different from a situation in                       there is no reason to exclude effective semi-measures as pos-
which 1 + ε can only be approximated from below. This pa-                       sible components of a cognitive model. But there also may
per is a plea mainly for non-terminating programs, and one                      be good positive reasons to include them when we consider
could in principle accept non-termination but still insist on                   the learning problem of inducing programs from observations
computability. There may be contexts where insistence on                        (see, e.g., Lake et al. 2017 for application of this idea to hu-
computability may be appropriate (see the section below on                      man cognition). Given the high complexity of verifying a.s.-
conditioning); the claim of this paper, however, is that we                     termination, the learner seems to be faced with a dilemma:
ought not make this restriction in general.                                     either restrict search to a small fragment of possible programs
                                                                                or risk hypothesizing programs that may not halt.
A Remark on Levels of Analysis
                                                                                   For example, it is difficult to imagine a sufficiently flexible
The argument that π↓ and π↑ are practically indistinguishable                   class of programs—say, a class built out of a few primitive
assumes that we may have to terminate computation beyond a                      constructions such as while-loops and simple arithmetical
certain point no matter which one we run, and that the result-                  operations like addition—from which one could easily ob-
ing behavior will look nearly indistinguishable. A possible                     tain the program π↓ above, but not one of the variants π↑ that
objection at this stage is that by enforcing an upper bound on                  might have some probability of not halting. It is not that one
computation time, we are effectively only considering pro-                      would prefer to construct π↑ over π↓ , but that they are equally
grams that a.s. (in fact, always) halt anyway. That is, the                     preferable and that separating them in a principled way might
larger program encompassing both the simulation model it-                       be difficult, no matter what method is used to perform the in-
self and whatever controls the simulations always terminates                    duction. That is, even if the goal is to construct an a.s.-halting
after a bounded amount of time.                                                 program, flexibility in program construction might require the
   This objection is fine as far as it goes, but it undercuts the               possible construction of non-halting programs.
motivation for considering rich, e.g., recursive, probabilistic                    In light of this possibility, a natural suggestion is to con-
programs to begin with. When we write the program π↓ above                      sider enriching program induction frameworks with more ex-
in Java, for example, we understand it as encoding an ab-                       pansive classes of programs. Consider Bayesian approaches
stract procedure that could in principle run for any amount of                  to program induction. Where C is some class of (semi-) mea-
time, even though we know no concrete implementation of π↓                      sures on a space X , e.g., on {0, 1}∗ —so that we can ask about
has this property. Indeed, π↓ abstracts away from many de-                      P(X) for any P ∈ C and X ∈ X —we could have a prior mea-
    1 It                                                                        sure ν over C that induces a mixture distribution Pν on X :
         is even possible for an a.s.-terminating program to have in-
finite expected run time. Consider a program defining a geometric
distribution that repeatedly flips a fair coin until first flipping a heads                                  ∆
                                                                                                   Pν (X) =      ∑ ν(P) P(X).
after n steps, then continutes for 2n more steps thereafter.                                                     P∈C
                                                                            2257

Thanks to Theorem 1, we can always think of each element               presented with b—to develop a notion of similarity between
of C as a semi-measure µπ defined by a probabilistic program           arbitrary representations. The basic idea is that similarity is
π from some class Π, so ν defines a prior on programs in Π.            roughly proportional to the length of the shortest (determin-
   Hierarchical Bayesian models fit this description, where C          istic) program that would be required to transform one rep-
is typically a parametrized family of distributions and ν is a         resentation into the other. Enumerability is exactly what is
hyperprior over those parameters (though hierarchical mod-             needed to derive (a generalized version of) the Universal Law.
els may include more levels), and they are often explicitly               But what about simplicity-based Solomonoff induction?
encoded as probabilistic programs. Provided one can define             There are several issues with Solomonoff induction (includ-
appropriate likelihood functions ν(Y |P) and P(Y |X), it makes         ing the variant here for semi-measures, due to Zvonkin and
sense to condition such a mixture distribution on data Y using         Levin 1970). One well known problem is that the resulting
Bayesian inference:                                                    prior is very sensitive to the choice of universal Turing ma-
                                                                       chine U. In fact, it has been shown that there is a perfect
               Pν (X|Y ) =     ∑ ν(P|Y ) P(X|Y ).              (1)     correspondence between weightings ν on the class of enu-
                              P∈C
                                                                       merable semi-measures and choices of universal Turing ma-
By updating ν alongside candidate ground-level distributions           chines U for the Solomonoff prior (Wood et al., 2011). In
P, such methods capture effects of learning at multiple levels         other words, the class of Solomonoff priors just is the class of
of abstraction, such as the ability to transfer general principles     mixture semi-measures Pν in which ν assigns positive weight
inferred in one domain to novel but related domains.                   to all the enumerable semi-measures. It is therefore question-
   Probabilistic programs generalize hierarchical Bayesian             able whether this framework really does provide a foundation
models to allow wider classes C of measures. For instance,             for understanding simplicity, since it is not clear what an “un-
work by Piantadosi et al. (2016) considers learning in a con-          biased” choice of U or ν would be (see Sterkenburg 2016).
text where C is defined by logical expressions of the sort typ-           A larger problem with Solomonoff induction, however,
ically used in natural language semantics. Evidently, there is         concerns the complexity of conditional inference. Whereas
no reason we could not consider classes that include enumer-           each Solomonoff prior is itself computably enumerable, con-
able semi-measures as well. An alluring possibility is to take         ditioning on data leads to a function that is not even enumer-
C to be the class of all enumerable semi-measures—i.e., all            able (specifically, we go from Σ01 to ∆02 , see Leike and Hut-
programs—with ν assigning a weight to each. Because C is               ter 2015, cf. also the next section). Since the whole point
then computably enumerable, there are many effective semi-             of Solomonoff induction is to learn, this is a disappointing
measures ν assigning positive weight to all probabilistic pro-         result. In particular, it means that no probabilistic program
grams, and even here Pν is guaranteed to be an enumerable              can represent a conditioned Solomonoff prior. Given the cen-
semi-measure, and thus definable by a program. Learning in             trality of induction, we would like to understand better what
this setting is somewhat fraught (see below), but at least such        we can do with conditioning, and whether Bayesian program
a semi-measure can be represented. By contrast, when C is              induction is even possible when some of the candidate pro-
the class of computable measures there is no computable ν              grams have positive probability of not halting. This leads us
with support exactly C , since that set is undecidable.                to the next section.
Simplicity Bias
                                                                           Conditioning Enumerable Semi-Measures
In this broader setting of program induction, as in hierarchi-
cal models, it is presumed that a good prior on C is one that          The fact previously mentioned—that effective semi-measures
favors simpler hypotheses. This might be achieved, for in-             are not closed under conditioning—appears problematic, at
stance, by defining ν with a probabilistic grammar so that             least for Bayesian applications of probabilistic programs. It
shorter programs are automatically given higher probability.           is especially noteworthy given that the computable measures
A very general proposal for biasing simpler functions, known           are closed under conditioning in the discrete setting. While
as Solomonoff induction, is based on ideas from Kolmogorov             a full discussion of conditionalization is beyond the scope of
complexity. In brief, the proposal is to assign probability to a       this paper, it is worth briefly clarifying the issue. (Of course,
string w in proportion to the shortest (deterministic) program         for non-Bayesian approaches to learning programs, e.g., Nee-
that, when run on a universal Turing machine U, produces w             lakantan et al. 2016, this may not even be problematic.)
as output. The intuition is, data that could be produced by               Conditioning an effective semi-measure may produce a
simpler mechanisms should be a priori more likely.                     function that is only “limit computable” (Leike and Hutter,
   As an aside, there are other applications of simplicity-            2015), meaning the conditional probability of each string can
based constructions inspired by Kolmogorov complexity that             be approximated, but the sequence of rationals need not ap-
make use of enumerable semi-measures. As an example, in                proach its limit (even weakly) monotonically. The intuition
their generalization of Shepard’s Universal Law of General-            behind this is clear. To determine µ(X|Y ) we must compute
ization, Chater and Vitányi (2003) assume enumerable “con-            µ(X,Y )/µ(Y ). If all we can do is approximate each of µ(X,Y )
fusability” probabilities, P(Ra |Sb )—specifying how likely it         and µ(Y ) from below, and we know nothing about how fast we
is that a subject will give a response appropriate to a when           are converging to the correct values, then we know absolutely
                                                                   2258

nothing about the ratio µ(X,Y )/µ(Y ) at any finite stage.           realistic inference methods such as MCMC (see, e.g., Good-
   In the computable setting, as Freer et al. (2012) explain, it     man et al. 2008). Algorithmic tractability is an obvious worry,
is straightforward to define a single Turing machine QUERY           but this is already a worry when everything is computable,
that takes a program π and a Boolean condition κ (also repre-        and it is not obvious that including effective semi-measures
sented as a probabilistic program), and produces a representa-       exacerbates the problem. Moreover, even in the computable
tion of the posterior distribution QUERY(π, κ). The idea is to       case, for the continuous setting conditionalization is not in
divide the infinite random bit stream into infinitely many ran-      general a computable operation (Ackerman et al., 2011).
dom bit streams, and find the first one that satisfies κ. Then       Consequently, as computational-level models of learning and
run π using this bit stream to generate an output w. As long as      inference, it appears that the effective semi-measures fare no
κ a.s. terminates and returns ‘true’ with positive probability,      worse than the computable measures.
µQUERY(π,κ) correctly defines the posterior distribution.
   By the aforementioned result, we know there can be no                                            Universality
machine that conditions an arbitrary semi-measure µπ on an
arbitrary condition κ. Nonetheless, provided κ stipulates a          In this final section we offer a proof of the correspon-
computable condition, even when µπ is merely computably              dence between probabilistic machines and enumerable semi-
enumerable, QUERY(π, κ) will correctly represent the condi-          measures. Specifically, we show that every enumerable semi-
tioned semi-measure, which shows that the conditional distri-        measure can be represented by a probabilistic machine. The
bution is also enumerable. Thus, enumerable semi-measures            proof is similar to proofs for the computable case (e.g., Freer
are closed under conditioning with computable queries.               et al. 2012), but we can only use enumerability and must al-
   In this sense the situation for enumerable semi-measures          low for probability of non-halting executions. The exposition
is no worse than that for computable measures: both can be           is intended to be accessible and intuitive.
conditioned with computable observations in a uniform way.
                                                                         Let µ be an enumerable semi-measure on {0, 1}∗ . That is,
For many settings this does not seem at all limitative. As a
                                                                     for each word w ∈ {0, 1}∗ , there is a computably enumerable
simple example, we could imagine conditioning the program
                                                                     weakly increasing sequence q0 , q1 , q2 , . . . of rational numbers
π↑ on the statement that the tortoise reached at least 15 steps.
                                                                     such that lim qi = µ(w). Assume without loss that q0 = 0.
This is an easily, indeed finitely, verifiable proposition.                            i→∞
   The main limitative result is rather the one we already           Note then that µ(w) = ∑∞          i=0 (qi+1 − qi ). Our aim is to show
mentioned: though we can represent complex semi-measures             that µ = µπ for some machine π.
such as Solomonoff priors, the probabilities of even basic ob-           Let h , i : N × N → N be a fixed (computable) bijective
servations like “the first object is a 0” are not computable.        pairing function with first projection ρ1 (n) = k when n =
Nonetheless, we can hope for something in this direction.            hk, ii. Let w0 , w1 , w2 , . . . be a fixed enumeration of {0, 1}∗ ,
Sufficient conditions for a conditional mixture semi-measure         each with a fixed enumeration of approximating rationals
Pν to be semi-computable are not terribly stringent. First, the      qk0 , qk1 , . . . converging from below to µ(wk ). We define a se-
prior ν over semi-measures µ ∈ C should be computable (e.g.,         quence of rational (thus computable) numbers as follows:
this holds if the semi-measures/programs are generated by a
probabilistic grammar). Second, as above, the specific data                                   r0    =
                                                                                                     ∆
                                                                                                           q00 = 0
Y must be computably verifiable for each µ ∈ C . If both of                                          ∆
                                                                                                           rn + qki+1 − qki
                                                                                                                            
                                                                                             rn+1   =
these are satisfied, then the adapted version of (1)
                              ν(µ)µ(Y ) µ(X,Y )                    where we assume 0 = h0, 0i and n + 1 = hk, ii.
         Pν (X|Y ) = ∑
                           µ   ∑µ0 ν(µ0 )µ0 (Y ) µ(Y )                   Our machine π works in stages, observing a random se-
                                                                     quence of bits a0 , . . . , a j−1 while producing an enumeration
                          ∑µ ν(µ) µ(X,Y )                            r0 , . . . , r j−1 . At each stage j, we observe a bit a j and add a
                     =
                            ∑µ ν(µ) µ(Y )                            rational r j , then check whether, for any n with 0 ≤ n < j, the
is enumerable, and thus representable by a single program,           following condition (2) is satisfied:
e.g., using an operation like QUERY. Though this falls short
                                                                                           j                              j
of full Solomonoff-style induction, it does generalize what is
usually done with Bayesian program induction. It also re-                        rn < ∑ ai 2−i − 2− j and rn+1 > ∑ ai 2−i + 2− j         (2)
                                                                                         i=0                            i=0
veals a distinctive positive reason to entertain specific effec-
tive semi-measures as candidate cognitive models. Granted                                         j
our previous suggestion that it might be beneficial for learn-       That is, where p̃ = ∑i=0 ai 2−i is the rational generated so far,
ing to consider wide classes of programs, putting a com-             we know our randomly generated real number will lie some-
putable prior on such a class will result in a enumerable mix-       where in the interval ( p̃ − ε, p̃ + ε), and (2) tells us that this
ture semi-measure Pν that can be conditioned.                        interval sits inside the interval (rn , rn+1 ). If this holds, output
   Questions about conditioning in this more general setting         wρ1 (n+1) . Otherwise, move on to stage j + 1.
clearly merit further attention, especially in relation to more          Each word w has its probability mass µ(w) distributed
                                                                 2259

across different intervals in [0, 1]. Specifically:                    Freer, C., Roy, D., and Tenenbaum, J. B. (2012). To-
                                                                          wards common-sense reasoning via conditional simula-
               µ(wk ) =              ∑      rn+1 − rn
                                                                          tion: Legacies of Turing in artificial intelligence. In
                               n:ρ1 (n+1)=k
                                ∞
                                                                          Downey, R., editor, Turing’s Legacy. ASL Lecture Notes.
                         =     ∑   (qki+1 − qki ).                     Goodman, N. D., Mansinghka, V. K., Roy, D., Bonawitz, K.,
                               i=0                                        and Tenenbaum, J. B. (2008). Church: A language for
                                                      j                   generative models. In Uncertainty in Artificial Intelligence.
The procedure generates approximations p̃ = ∑i=0 ai 2−i to a
                                                                       Goodman, N. D., Tenenbaum, J. B., and Gerstenberg, T.
random real number, and as soon as we are guaranteed that
                                                                          (2014). Concepts in a probabilistic language of thought.
this random number is in one of our intervals between rn and
                                                                          In Margolis, E. and Laurence, S., editors, The Conceptual
rn+1 = rn + (qki+1 − qki ), i.e., that no further bits will take us
                                                                          Mind: New Directions in the Study of Concepts. MIT Press.
out of that interval (condition (2) above), we halt and output
the string wk corresponding to the interval, with k = ρ1 (n+1).        Icard, T. F. (2016). Subjective probability as sampling
Clearly, the probability of outputting w is exactly µ(w), and             propensity.     Review of Philosophy and Psychology,
the probability of not halting at all is 1 − ∑w µ(w).                     7(4):863–903.
                                                                       Kaminski, B. L. and Katoen, J.-P. (2015). On the hardness of
Theorem 1. Probabilistic machines correspond exactly with
                                                                          almost-sure termination. In Mathematical Foundations of
the enumerable semi-measures.
                                                                          Computer Science, pages 307–318.
   As every enumerable probability measure is computable,              Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman,
we have the following well-known corollary.                               S. J. (2017). Building machines that learn and think like
Corollary 1. A.s.-terminating probabilistic machines corre-               people. Behavioral and Brain Sciences. forthcoming.
spond exactly with the computable measures.                            Leike, J. and Hutter, M. (2015). On the computability of
                                                                          Solomonoff induction and knowledge-seeking. In 26th In-
                          Conclusion                                      ternational Conference on Algorithmic Learning Theory.
Defining distributions by means of programs in a universal             Marr, D. and Poggio, T. (1976). From understanding compu-
probabilistic language yields exactly the computably enumer-              tation to understanding neural circuitry. MIT Memo 357.
able semi-measures. Our claim has been that this wider class,
                                                                       Neelakantan, A., Le, Q. V., and Sutskever, I. (2016). Neu-
going beyond a.s.-terminating programs, provides a sensible
                                                                          ral programmer: Inducing latent programs with gradient
foundation for theorizing about representation, inference, and
                                                                          descent. In International Conference on Learning Repre-
learning in cognitive science. Assuming we want to make a
                                                                          sentations (ICLR).
clean separation between computational and algorithmic lev-
                                                                       Piantadosi, S. T., Tenenbaum, J. B., and Goodman, N. D.
els of analysis—which is evidently necessary to justify use
                                                                          (2016). The logical primitives of thought. Psychological
of anything beyond (probabilistic) finite-state automata in the
                                                                          Review, 123(4):392–424.
first place—we see no reason to restrict attention to programs
that a.s. terminate, neither for the theorist nor for the learner.     Putnam, H. (1967). Psychophysical predicates. In Capitan,
                                                                          W. H. and Merrill, D. D., editors, Art, Mind, and Religion.
                          References                                      Pittsburgh University Press.
Ackerman, N. L., Freer, C. E., and Roy, D. M. (2011). Non-             Rescorla, M. (2015). The computational theory of mind. In
   computable conditional distributions. In Proceedings of                Zalta, E. N., editor, Stanford Encyclopedia of Philosophy.
   Logic in Computer Science (LICS).                                   Sanborn, A. N., Mansinghka, V. K., and Griffiths, T. L.
Battaglia, P. W., Hamrick, J. B., and Tenenbaum, J. B. (2013).            (2013). Reconciling intuitive physics and Newtonian me-
   Simulation as an engine of physical scene understand-                  chanics for colliding objects. Psych. Rev., 120(2):411–437.
   ing. Proceedings of the National Academy of Sciences,               Sterkenburg, T. F. (2016). Solomonoff prediction and Oc-
   110(45):18327–18332.                                                   cam’s razor. Philosophy of Science, 83:459–479.
Botvinick, M. M. and Cohen, J. D. (2014). The computational            Suppes, P. (1969). Stimulus-response theory of finite au-
   and neural basis of cognitive control: Charted territory and           tomata. Journal of Math. Psych., 6(3):327–355.
   new frontiers. Cognitive Science, 38:1249–1285.                     Tenenbaum, J. B., Kemp, C., Griffiths, T. L., and Goodman,
Chakarov, A. and Sankaranarayanan, S. (2013). Probabilistic               N. D. (2011). How to grow a mind: Statistics, structure,
   program analysis with martingales. In International Con-               and abstraction. Science, 331:1279–1285.
   ference on Computer Aided Verification (CAV).                       Wood, I., Sunehag, P., and Hutter, M. (2011). (Non-) Equiva-
Chater, N. and Vitányi, P. (2003). The generalized universal             lence of Universal Priors. In Dowe, D., editor, Solomonoff
   law of generalization. Journal of Mathematical Psychol-                85th Memorial Conference, pages 417–425. LNCS.
   ogy, 47:346–369.                                                    Zvonkin, A. K. and Levin, L. A. (1970). The complexity of
Dal Lago, H. and Zorzi, M. (2012). Probabilistic operational              finite objects and the development of the concepts of in-
   semantics for the lambda calculus. RAIRO - Theoretical                 formation and randomness by means of the theory of algo-
   Informatics and Applications, 46(3):413–450.                           rithms. Uspekhi Matematicheskikh Nauk, 25(6):85–127.
                                                                   2260

