 A Neural Network Model for Taxonomic Responding with Realistic Visual Inputs
                                          Giorgia Fenoglio (giorgia.fenoglio@edu.unito.it)
                                              Dipartimento di Informatica, Corso Svizzera 185,
                                                               10149, Torino, Italy
                                            Roberto Esposito (roberto.esposito@unito.it)
                                              Dipartimento di Informatica, Corso Svizzera 185,
                                                               10149, Torino, Italy
                                            Valentina Gliozzi (valentina.gliozzi@unito.it)
                                                    Center for Logic, Language, Cognition
                                              Dipartimento di Informatica, Corso Svizzera 185,
                                                               10149, Torino, Italy
                                Abstract                                       Our starting point is Mayor and Plunkett (2010)’s neuro-
                                                                            computational model of the taxonomic constraint. The model
   We propose a neural network model that accounts for the emer-
   gence of the taxonomic constraint in early word learning. Our            consists of two self-organizing maps (a visual and an acous-
   proposal is based on Mayor and Plunkett (2010)’s neurocom-               tic map) connected with Hebbian connections. The model
   putational model of the taxonomic constraint and overcomes               successfully explains how it is possible to generalize a single
   one of its limitations, namely the fact that it considers arti-
   ficially built, simplified stimuli. In fact, while in the original       word-object association to a whole class of objects. Essen-
   model the visual stimuli are random, sparse dot patterns, in our         tially, this is the result of Hebbian learning creating word-
   proposed solution they are photographic images from the Im-              object associations over a previous conceptual organization
   ageNet database. In our model the represented objects in the
   image can be of different size, color, location in the picture,          of the visual and acoustic space.
   point of view, etc.. We show that, notwithstanding the aug-                 Here we want to go beyond one limitation of Mayor and
   mented complexity in the input, the proposed model compares              Plunkett (2010)’s model, namely the fact that it considers
   favorably with respect to Mayor and Plunkett (2010)’s model.
                                                                            artificially built, simplified stimuli: in their model the vi-
                                                                            sual stimuli are random, sparse dot patterns, in the style of
                            Introduction                                    (Posner, Goldsmith, & Welton Jr, 1967), whereas the acous-
A central issue in the current understanding of early lexical               tic stimuli are manipulations of acoustic signatures extracted
acquisition concerns how infants learn the reference of words.              from sounds produced by a speaker, leading to a simplified
Quine (1960) famously raised the point that for every word                  acoustic input stimulus.
heard in a given circumstance, there are several possible ref-                 Would the model still work if we considered realistic visual
erences: in order to infer the appropriate one, infants have                inputs, instead? In order to address this question, we have
to rule out several possible alternatives. An influential so-               expanded the original model’s visual component making it
lution to the issue has been proposed by Markman (1989),                    able to process realistic visual stimuli, that in our case are
suggesting that infants rule out inappropriate references by                images taken from the ImageNet dataset. More precisely, we
means of three constraints. By the whole object constraint                  have added to the visual component of Mayor and Plunkett
children assume that novel words refer to objects as a whole,               (2010) an InceptionV3 deep network (Szegedy et al., 2015)
rather than to their parts, substance, color, or other properties.          which is at the state of the art in the image classification task.
By the mutual exclusivity constraint children assume that two               The deep network processes the visual scene in the image,
labels usually do not refer to the same object. Last, but cen-              builds a representation for it, and feeds the representation to
tral to this paper, by the taxonomic constraint children extend             the visual self-organizing map.
words to taxonomically-related objects (at the level of basic
                                                                               In order for the whole model to work, these representations
categories): when a child hears the word “dog” pronounced
                                                                            need to contain a description of the main object of the visual
by a caregiver while pointing at a specific dog, she general-
                                                                            scene, independent from the context. Understanding the na-
izes the reference of “dog” to all dogs, not just to the one in
                                                                            ture of the image representations built at the various levels
front of her.
                                                                            of the network is indeed one of the main points of debate in
   Here we propose a neural network model that accounts
                                                                            deep neural networks (Zeiler & Fergus, 2014; Zhou, Khosla,
for the emergence of the taxonomic constraint in early word
                                                                            Lapedriza, Oliva, & Torralba, 2014; Agrawal, Girshick, &
learning, and can process realistic visual stimuli1 . This is the
                                                                            Malik, 2014). In order to assess whether the InceptionV3
first step towards the development of a model able to cope
                                                                            deep network feeds into the visual self-organizing map mean-
with visual and auditory stimuli that are both realistic.
                                                                            ingful object representations, we performed several clustering
    1 For the time being we leave the question of realism of the acous-     experiments. These experiments investigated whether repre-
tic part to future work.                                                    sentations deriving from images of objects can be clustered
                                                                        1997

together by off-the-shelf clustering algorithms. They showed            (i.e., the distance between the input vector x and j0 s weight
that the representations provided by InceptionV3 are reason-            vector: q j = kx − w j (n)k)), and τ is a normalization constant.
ably well organized. A further test investigated whether the               Once the visual and acoustic maps have stabilized into
visual self-organizing map could organize the representations           a topological organization, proper word learning can start.
received from the deep neural networks in a topologically sat-          This is the Hebbian Learning phase, in which the two kinds
isfactory manner.                                                       of stimuli are simultaneously presented to the model. For
    We then tested the whole model to see if it still exhibits          each joint presentation of a visual and acoustic stimulus, the
a taxonomic responding, generalizing learned word-object                synapses between the two maps are strengthened. In partic-
associations to the whole category. Results show that our               ular, for each neuron v on the visual map and neuron p on
model, despite starting from more realistic visual stimuli,             the acoustic map, the Hebbian connection uv,p is strengthened
does replicate Mayor and Plunkett (2010)’s success on tax-              proportionally to the resulting neural activations av and a p , as
onomic responding when few joint word-object associations               follows:
are considered.                                                                        uv,p (n + 1) = uv,p (n) + 1 − e−λav a p           (2)
         Mayor and Plunkett (2010)’s model                              where λ is the Hebbian training learning rate.
Mayor and Plunkett (2010) neurocomputational model of tax-                 A single Hebbian learning event, combined with the pre-
onomic constraint (Figure 1) is based on two Self-Organizing            viously acquired categorization capabilities of the visual and
Maps (SOMs): a visual map and an acoustic map, represent-               acoustic SOMs, allows the model to generalize the associa-
ing the primary visual cortex and the primary auditory cortex           tion to other stimuli belonging to the same category.
respectively.                                                              Once training is complete, the model is tested for its abil-
    The stimuli presented to the                                        ity of comprehension and production. Comprehension is as-
two maps are artificially built:                                        sessed by considering what visual category is retrieved when
the visual stimuli are random                                           a word is presented to the auditory map and activation is prop-
dot patterns, whereas the audi-                                         agated via Hebbian connections. Production is assessed by
tory stimuli are extracted from                                         considering what word is produced by the auditory map when
the acoustic signatures of ut-                                          a visual stimulus is presented to the visual map, and activation
tered words; the acoustic signa-                                        is propagated through Hebbian connections.
tures are manipulated in order           Figure 1: Mayor and
                                                                           The ability of the model to extend the learned word-object
to create simpler inputs.                Plunkett (2010) model
                                                                        associations to other words and objects belonging to the same
    Learning is a two-phase pro-                                        category is measured by the Taxonomic Factor which is the
cess. First, the two maps are independently trained to learn            percentage of correct word-object associations generated by
to categorize the visual and the acoustic stimuli. This first           the model. Results show that when the SOMs are adequately
learning phase is preliminary to word learning, and unsuper-            trained the Taxonomic Factor reaches 80% after a single Heb-
vised. The two maps are trained using the standard learning             bian learning trial.
algorithm for self-organizing maps. In short, a stimulus x is              One of the limitations of Mayor and Plunkett (2010)’s orig-
presented to each neuron of the map, and the Best-Matching              inal model is that it uses artificially built input stimuli that are
Unit (BMU) is selected: this is the unit i whose weight vector          much simpler than what would derive from realistic contexts.
wi is closest to the stimulus x (i.e. i = arg min j kx − w j k).        Here we address this limitation, for what concerns the visual
    The weights of the best matching unit and of its surround-          module, by introducing deep convolutional neural networks
ing units are updated in order to maximize the chances that             as shown in the next sections.
in the future the same unit (or the surrounding units) will be
selected as the best matching unit for the same stimulus or                     Deep Convolutional Neural Networks
for similar stimuli. At iteration n + 1, the weights for neuron
 j are updated as follows:                                              In the last few years research on deep networks contributed
                                                                        to reach human (sometimes super-human) performances on
          w j (n + 1) = w j (n) + η(n)hi, j (n)(x − w j (n))     (1)    several difficult tasks (Hinton et al., 2012; Li & Wu, 2015;
                                                                        Socher, Bauer, Manning, & Ng, 2013; Yue-Hei Ng et
where η is the learning rate, and hi, j is the neighborhood             al., 2015). In particular, in 2011 a deep convolutional
function between i and j hi, j (n) is defined as hi, j (n) =            model achieved for the first time super-human performances
exp(−di,2 j /2σ(n)2 ), where di, j is the distance between i and j      in a visual pattern recognition task and, in the following
on the map’s grid, and σ(n) is the width of the gaussian.               year, the AlexNet Convolutional Neural Network (CNN)
    After a while, the two maps learn to adequately represent           model won the ImageNet competition by a significant mar-
the stimuli of their training set in a topologically significant        gin (Krizhevsky, Sutskever, & Hinton, 2012) over traditional
way: close units respond similarly to similar stimuli. The              competitors. These successes contributed to a growing inter-
neural activation a j of a neuron j in response to a stimulus x         est in deep networks and today deep-network-based models
                         qj
is defined as: a j = e− τ , where q j is the quantization error         are at the forefront of research in many different areas and are
                                                                    1998

setting performance records in tasks of interest for the cogni-                                      h
tive sciences community such as image (e.g., (Russakovsky et
al., 2015)) and speech recognition (e.g., (Xiong et al., 2016)).
   Despite unheard performances achieved in many different
tasks, deep models present important shortcomings that are
far from being completely addressed. The most important
problem from the point of view of the forthcoming discus-
sion is the difficulty they present for what concerns the under-
standing of their internal working. Consequently, recent re-
search investigated ways to make sense of the contents of the
network providing interesting insights. For instance, Zeiler
and Fergus (2014) use “deconvolution networks” to visual-
ize the patterns that causes the activation of nodes in each
layer; in Zhou et al. (2014) scenes are iteratively simplified
or occluded to investigate which image patches and which ob-
jects contribute to the activation of nodes in a given layer; in
(Agrawal et al., 2014) the authors investigate the presence of
grand-mother-cells and of distributed representations in deep
networks.
   While the understanding of the representations built by
these networks is still scattered and incomplete, some of the
insights seem to be well supported. An important one con-
cerns the hierarchical organization of the features: low-level
(coarser) features are nearest to the network input, while           Figure 2: The proposed model: the visual component con-
higher-level (more abstract) features are nearest to the output      tains a deep convolutional network (InceptionV3) in order to
(for an idea of the kind of features extracted at the different      process realistic images. The representation built by the deep
levels see for instance (Zeiler & Fergus, 2014)). Interestingly      neural network is then fed into the visual SOM. The acoustic
this organization mirrors a well known characteristic of the         component, on the contrary, only contains an acoustic SOM,
representations in the primate inferior temporal (IT) cortex,        as in Mayor & Plunkett (2010)’s model, and can only process
and hence it hints at a possible cognitive justification of this     simplified acoustic stimuli.
computational model. To this regard it is interesting to men-
tion that recent research investigated the connection between
                                                                     of handling realistic images. In fact, in this proposal the au-
the representations built by several computational models and
                                                                     ditory input is a mere placeholder that does not provide any
the representations in the IT cortex and found that deep neural
                                                                     real processing ability.
networks are among the best models (Serre, 2016; Kriegesko-
rte, 2015). For instance, in (Khaligh-Razavi & Kriegeskorte,            In practice, we shall assume that an oracle provides the
2014), the authors investigate a wide range of computational         auditory SOM with a perfect representation of the auditory
models and suggest that deep CNNs are, not only the best             stimulus, or label, in the form of a binary vector. The vec-
performing in term of accuracy, but also the best at explain-        tor contains a 1 in position i if the utterance provided to the
ing the IT representation (albeit still in an incomplete way).       auditory module corresponds to the i-th label; it contains a 0
                                                                     otherwise.
   Given the great accuracy they achieve and the possible cog-
                                                                        The visual module shall, on the contrary, be able to cope
nitive plausibility of the CNNs, we have chosen to use these
                                                                     with realistic images and, while we still assume that each im-
particular models as the visual component of the word-object
                                                                     age contains a main object corresponding to the concept to be
association model we propose in the next section.
                                                                     learned, we pose no additional constraints. Images for a given
                                                                     concept can, for instance, be of different size, color, location
                       Proposed model
                                                                     in the picture, point of view, etc.. For instance, the “dogs”
In order to solve the problem of the lack of realism in the          concept may be represented by images of dogs of different
visual stimuli in the Mayor and Plunkett (2010) model, we            size, color, breed and be portrayed in different contexts, un-
propose to replace the input of the visual SOM with a repre-         der different illuminations and poses.
sentation built by a CNN as shown in Figure 2.                          The visual module is the concatenation of the InceptionV3
   The long term objective of this research is to find a cog-        network and the SOM network we already introduced. Incep-
nitively plausible model able to reproduce the word-object           tionV3 is a stack of Inception Modules, which parallelize and
association abilities observed in infants using realistic image      combine several convolution and pooling operations provid-
and audio stimuli. In this paper we keep a simplified auditory       ing a richer output while still maintaining a small number of
input and focus instead on providing a visual module capable         parameters. At the end of the stack of inception modules the
                                                                 1999

model contains a pooling layer of length 2048 which is fully           lows one to cluster the input images into groups that correlate
connected with a shallow feed-forward neural network which             well with the classes assigned with the images themselves.
is then used to classify the input image.                              This is arguably an evidence that such a representation can
   In the Inception architecture a representation of the input         be usefully exploited as the input of the SOMs. In the “Word
is propagated through the layers up to the top of the network          Learning” Section we focus on the complete model, repli-
where it is used to train the classifier. A question worth inves-      cate part of the experiments in (Mayor & Plunkett, 2010),
tigation is if and where a good representation of the concept          and compare our results with those reported in that paper.
in the input image is created by the deep network. In this                All the experiments have been performed on two datasets.
paper we work under the assumption that such representation            A first dataset is composed by 10 classes associated with
exists and argue that it has to be found in the last pooling layer     100 stimuli each, for an overall 1.000 stimuli. A sec-
(just before the fully connected neural classifier). Based on          ond dataset contains 100 classes associated with 100 stim-
this assumption, we propose to use the vector containing the           uli each, for an overall of 10.000 stimuli. Since the re-
value of the 2048 neurons in that layer as the representation of       sults for the two datasets are very similar, for the sake of
the stimuli for the visual SOM. To verify the validity and the         readability we focus on the smaller dataset and refer to
consequences of this assumption, we performed two sets of              (Fenoglio, 2016) for the details of the experiments on the
experiments: in the “Representation Quality” sub-section of            larger dataset. The code for the complete model along with
the Experiments section we investigate the nature of the pro-          the datasets used can be found at https://github.com/
posed representation, while in the subsequent section (Word            ml-unito/NNsTaxonomicResponding.
Learning) we investigate the quality of the complete model.
In our simulations we used the pre-trained Inception network
                                                                       Representation Quality
provided by the TensorFlow library2 .                                  In order to assess the quality of the representation found in
   The complete system is trained as outlined in the “Mayor            the last pooling layer of the InceptionV3 model, we investi-
and Plunkett (2010)’s model” section. In summary, given the            gate how well these representations can be clustered together.
representation from the CNN and the simplified auditory in-            For each image we extract the representation found in the last
put, the two SOMs (composed by 20 × 30 neurons each) are               pooling layer of the deep network, we then cluster the result-
trained to cluster together similar representations using the          ing representations using a K-means and an agglomerative
standard SOM training algorithm. In our tests, the two SOMs            algorithm. For both algorithms the number of clusters is set
attain their best topological organization of the objects in the       to 10. The clustering experiments have been conducted using
training set after 60 epochs (the learning rate is set to 0.3 and      the scikit-learn python library3 .
decreases linearly at each epoch). Afterwards, the associa-               Figures 3 and 4 report results for K-means clustering.
tion between the visual and the auditory input is created us-          Analogous results hold for agglomerative clustering. In par-
ing Hebbian connections between the two maps: two stimuli              ticular, Figure 3 reports, for each class, a bar showing how
belonging to the same category are presented together to the           the class objects are partitioned among clusters; Figure 4 re-
model, the visual stimulus is processed to extract its repre-          ports, for each cluster, a bar showing the distribution of the
sentation and presented along with the auditory stimulus to            classes within it. We then investigate the topological organi-
the corresponding SOMs. Finally the SOMs activations are               zation provided by the visual SOM out of the representations
used to update the Hebbian connections using the update rule           created by the deep model. We report in Figure 5 a repre-
in Formula 2.                                                          sentation of the topology found by the visual SOM after 60
   To better cope with the variability in the input represen-          learning epochs.
tation, we introduce two variations to the Hebbian training            Discussion The experiments show that the two clustering
(with respect to the procedure outlined in (Mayor & Plunkett,          algorithms are able to find good, albeit not perfect, partitions
2010)): i) we allow the network to learn from an increasing            for the representations. In particular, Figure 3 shows that the
number of stimuli pairs (in the original paper a single pair of        objects in 7 out of 10 classes are mostly assigned homoge-
stimuli is presented to the network), this allows us to study          neously to a single cluster: in two of the remaining cases the
how performances increase as the number of presentations               objects are almost all distributed among two classes, while in
grows; ii) we suppress the activation of a neuron in a SOM if          a single case (and only for the k-means clustering algorithm)
its activation value is below 0.6.                                     the objects are distributed on three clusters. Figure 4 shows
                                                                       a similar picture, but from the point of view of the clusters:
                         Experiments                                   in almost all cases (8 out of 10) we have clusters which are
In the following two sub-sections we investigate two impor-            almost pure. The remaining two clusters conglomerate ob-
tant facets of the proposed model. In particular, in the “Rep-         jects from different classes acting almost as folders where all
resentation Quality” Section we show that the representation           uncertain objects are put.
found in the last pooling layer of the InceptionV3 network al-            Overall, it seems that the clustering algorithms do find a
                                                                       way to partition the representations of the objects into co-
    2 https://github.com/tensorflow/models/tree/master/
inception                                                                 3 http://scikit-learn.org/stable/modules/clustering
                                                                   2000

                                                                     Figure 5: SOM clustering of the visual stimuli representations
Figure 3: Per class distribution of objects into clusters (K-
means clustering). Colors represents different clusters. Blue        associated to that word4 . The percentage of correct words
is used to represent the cluster containing the majority of the      produced by the model when tested through all the classes is
objects of a given class; orange, yellow, and green are used         the Taxonomic Factor.
to represent the second, third, and fourth most represented             We have performed a number of experiments where we
clusters.                                                            varied the number of presentations per class used to update
                                                                     the Hebbian connections. Specifically we let the number of
                                                                     presentations vary from 1 to 15. For each experiment we
                                                                     repeated the test over 1.000 different training sets (we kept
                                                                     fixed the SOM and let vary the images presented to the Heb-
                                                                     bian learning module) and report the average taxonomic fac-
                                                                     tor over an independent test set composed by additional 1000
                                                                     images (100 images per class). Results are shown in Figure
                                                                     6.
Figure 4: Distribution of classes among clusters (K-means
clustering).
herent clusters. This is consistent with what happens for the
topological organization that the SOMs create for the repre-
sentations provided by the deep networks. Figure 5 shows
that, with the exception of few cases, the visual SOM is able
to group all related objects into nearby spaces.
                                                                     Figure 6: Taxonomic factor of the model, using an increasing
Word Learning                                                        number of pairs of stimuli per class during the training of the
                                                                     Hebbian Connection (on the x-axis).
In order to evaluate the performance of our model in the task
of word learning, we calculate the Taxonomic Factor of the
model as defined in (Mayor & Plunkett, 2010). We do so by            Discussion The experiments show that the Taxonomic Fac-
testing the model for its production skills: for each class, 100     tor steadily grows as more word-object associations are pre-
images are presented to the visual module, the activation is         sented and reaches an accuracy above 80% (which is compa-
propagated through the deep neural network, then fed into the        rable with results in Mayor and Plunkett (2010)) at the fourth
visual SOM. The activation of the visual SOM’s best match-           joint presentation.
ing unit is propagated through the Hebbian connections up
to the acoustic SOM. At the end of the process the resulting             4 An area in the map is associated to a word if the activation of the
most active unit on the acoustic map is identified. It will be       neurons within it are at their peak when they respond to a stimulus
considered correct if it belongs to the area of the acoustic map     of that particular word.
                                                                 2001

                           Conclusions                              Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Im-
                                                                      agenet classification with deep convolutional neural net-
In this paper we have proposed an extension of the Mayor and          works. In Advances in neural information processing sys-
Plunkett (2010) model for taxonomic responding. We have               tems (pp. 1097–1105).
addressed the issue of adding realism to the visual stimuli.        Li, X., & Wu, X. (2015). Constructing long short-term mem-
As a difference with respect to the original model in which           ory based deep recurrent neural networks for large vocab-
these inputs were random dot patterns, the model can now              ulary speech recognition. In Acoustics, speech and signal
deal with realistic images as those in the ImageNet dataset.          processing (icassp), 2015 ieee international conference on
This is possible thanks to the insertion of a deep convolu-           (pp. 4520–4524).
tional neural network in the visual component of the model.         Markman, E. M. (1989). Categorization and naming in chil-
Notwithstanding the higher complexity of the stimuli consid-          dren: Problems of induction. Mit Press.
ered, our model exhibits taxonomic responding with perfor-          Mayor, J., & Plunkett, K. (2010). A neurocomputational
mances comparable to the original one.                                account of taxonomic responding and fast mapping in early
   In our future work we will address the issue of making the         word learning. Psychological review, 117 1, 1–31.
acoustic module work with realistic stimuli. It can be inter-       Posner, M., Goldsmith, R., & Welton Jr, K. (1967). Perceived
esting to explore whether a deep neural network for acoustic          distance and the classification of distorted patterns. Journal
processing, as for instance the one proposed in (Xiong et al.,        of Experimental Psychology, 73(1), 28-38.
2016), could be nested into the acoustic part of the model in a     Quine, W. V. O. (1960). Word and object. Cambridge,Mass.:
way similar to what we already did for the visual component.          MIT Press.
   We will also explore whether the model proposed here can         Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
be used to provide a mechanistic account of the whole object          Ma, S., . . . Fei-Fei, L. (2015). Imagenet large scale visual
constraint proposed by Markman (1989) by which a word is              recognition challenge. International Journal of Computer
associated to the whole object instead of anyone of its prop-         Vision, 115(3), 211–252.
erties. We conjecture that a model as the one proposed, with        Serre, T. (2016). Models of visual categorization. Wiley
the deep component that extracts a representation of an object        Interdisciplinary Reviews: Cognitive Science, 7(3), 197–
out of a more complex visual scene, can be adequate to the            213.
purpose: the whole object constraint may naturally emerge           Socher, R., Bauer, J., Manning, C. D., & Ng, A. Y. (2013).
from the association of the word to the object’s representa-          Parsing with compositional vector grammars. In In pro-
tion formed by the deep network. Important to this regard             ceedings of the ACL conference.
is the current discussion about the nature of the object repre-     Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
sentation built by deep networks (Ullman, Assif, Fetaya, &            Anguelov, D., . . . Rabinovich, A. (2015). Going deeper
Harari, 2016; Tang & Kreiman, 2017).                                  with convolutions. In Proceedings of the ieee conference
                                                                      on computer vision and pattern recognition (pp. 1–9).
                            References                              Tang, H., & Kreiman, G. (2017). Recognition of occluded
                                                                      objects. In Computational and Cognitive Neuroscience of
Agrawal, P., Girshick, R., & Malik, J. (2014). Analyzing              Vision. (ed Zhao, Q). Singapore: Springer-Verlag, 14, 57–
   the performance of multilayer neural networks for object           77.
   recognition. In European conference on computer vision           Ullman, S., Assif, L., Fetaya, E., & Harari, D. (2016). Atoms
   (pp. 329–344).                                                     of recognition in human and computer vision. Proceedings
Fenoglio, G. (2016). A computational model for word learn-            of the National Academy of Sciences, 113(10), 2744–2749.
   ing on real world data through Deep Neural Networks.             Xiong, W., Droppo, J., Huang, X., Seide, F., Seltzer, M.,
   Unpublished master’s thesis, Turin University. (https://           Stolcke, A., . . . Zweig, G. (2016). The microsoft
   github.com/gfbfenoglio/NNsTaxonomicResponding/                     2016 conversational speech recognition system. CoRR,
   blob/master/Documents/MasterThesis.pdf, chapters                   abs/1609.03528.
   6,8)                                                             Yue-Hei Ng, J., Hausknecht, M., Vijayanarasimhan, S.,
Hinton, G., Deng, L., Yu, D., Dahl, G., rahman Mohamed,               Vinyals, O., Monga, R., & Toderici, G. (2015). Beyond
   A., Jaitly, N., . . . Kingsbury, B. (2012). Deep neural net-       short snippets: Deep networks for video classification. In
   works for acoustic modeling in speech recognition. Signal          Proceedings of the ieee conference on computer vision and
   Processing Magazine.                                               pattern recognition (pp. 4694–4702).
Khaligh-Razavi, S.-M., & Kriegeskorte, N. (2014, Novem-             Zeiler, M. D., & Fergus, R. (2014). Visualizing and under-
   ber 6). Deep Supervised, but Not Unsupervised, Models              standing convolutional networks. In European conference
   May Explain IT Cortical Representation. PLOS Computa-              on computer vision (pp. 818–833).
   tional Biology, 10(11).                                          Zhou, B., Khosla, A., Lapedriza, À., Oliva, A., & Torralba,
Kriegeskorte, N. (2015). Deep neural networks: a new frame-           A. (2014). Object detectors emerge in deep scene cnns.
   work for modeling biological vision and brain information          CoRR, abs/1412.6856.
   processing. Annual Review of Vision Science, 1, 417–446.
                                                                2002

