                            Calculating Probabilities Simplifies Word Learning
     Aida Nematzadeh                    Barend Beekhuizen                   Shanshan Huang                  Suzanne Stevenson
     Dept. of Psychology              Dept. of Computer Science         Dept. of Computer Science         Dept. of Computer Science
   Univ. of Calif., Berkeley             University of Toronto             University of Toronto            University of Toronto
 nematzadeh@berkeley.edu                barend@cs.toronto.edu        sunny.huang@mail.utoronto.ca          suzanne@cs.toronto.edu
                              Abstract                                    We formulate various in-the-moment learning mechanisms
                                                                       that introduce different kinds of competition – i.e., the way
   Children can use the statistical regularities of their environ-
   ment to learn word meanings, a mechanism known as cross-            in which the strength of a word–meaning alignment depends
   situational learning. We take a computational approach to in-       on and interacts with other possible alignments. Each mech-
   vestigate how the information present during each observation       anism corresponds to certain statistics of the word learning
   in a cross-situational framework can affect the overall acqui-
   sition of word meanings. We do so by formulating various            input, such as the weighted frequency of word–meaning pairs
   in-the-moment learning mechanisms that are sensitive to dif-        or their conditional probabilities. We show that the different
   ferent statistics of the environment, such as counts and con-       types of competition lead to various kinds of mutual exclu-
   ditional probabilities. Each mechanism introduces a unique
   source of competition or mutual exclusivity bias to the model;      sivity behaviours. Mutual exclusivity has been proposed as
   the mechanism that maximally uses the model’s knowledge of          an explicit bias, in which children assume each word has a
   word meanings performs the best. Moreover, the gap between          single meaning (e.g., Markman, 1987; Markman & Wachtel,
   this mechanism and others is amplified in more challenging
   learning scenarios, such as learning from few examples. Key-        1988). Here, mutual exclusivity of words and/or meanings
   words: cross-situational word learning; computational model-        arises from competition in a way that focuses learning.
   ing; word learning biases                                              We take a computational modeling approach to investigate
                                                                       the effectiveness of these mechanisms in overall acquisition
                           Introduction                                of word meanings in various long-term word learning scenar-
How do people acquire the meanings of words as they be-                ios. Using a computational model enables us to explore the
gin to learn a language? A well-supported proposal is cross-           impact of different learning mechanisms in a variety of con-
situational learning (e.g., Pinker, 1989), which suggests that         ditions, and to examine the role of one factor (e.g., frequency)
people are sensitive to the regularities that repeat in different      while controlling for another one (e.g., utterance length). We
situations, and use such evidence to identify the commonal-            find that the mechanism that maximizes the use of the accu-
ities, from which they can infer word meanings. As an ex-              mulated knowledge of learned meanings performs the best.
ample, when a child hears what a cute kitty, be nice to the            Interestingly, the performance gap between this mechanism
kitty, etc., she/he could infer that the word kitty refers to the      and others is most significant in more difficult learning con-
common referent in all these situations, i.e., a cat. Recent           ditions, such as learning of low frequency words given long
word learning experiments confirm that both adults and in-             utterances. This shows that using conditional probabilities
fants keep track of cross-situational statistics across learning       (as opposed to counts) and introducing competition (leading
trials, and infer the correct word–meaning mappings even in            to a mutual exclusivity bias) improves overall word learning
highly ambiguous conditions (e.g., Yu & Smith, 2007; Smith             and might be necessary to guide learning in the presence of
& Yu, 2008; Yurovsky, Fricker, Yu, & Smith, 2014).                     ambiguity or little data.
   Despite empirical evidence for statistical cross-situational
learning, the exact mechanisms in play are still not fully un-         A Cross-situational Word Learning Framework
derstood. In this paper, we focus on the first step of a cross-
situational framework – the learning that occurs on each ob-           There has been an increased interest in the last decade in de-
servation of a word, which we call in-the-moment learning.             veloping computational models as tools to study word learn-
Given the words in an utterance and their potential meanings           ing in people. Of particular interest are cross-situational
in the accompanying situation, there are many possible ways            learners that are incremental (e.g., Siskind, 1996; Fazly, Al-
to associate words and meanings, but only some of these as-            ishahi, & Stevenson, 2010; Kachergis, Yu, & Shiffrin, 2012),
sociations are correct. We refer to these in-the-moment as-            which is necessary in studying developmental learning pat-
sociations of words and meanings as alignments, and con-               terns. Notably, the model of Fazly et al. (2008; 2010) (hence-
sider different strategies for assessing the strength of these         forth FAS) is the first probabilistic model that robustly pre-
alignments, drawing on the evolving knowledge of word                  dicts a range of observed behavior in child word learning.
meanings. We note that previous research has considered                Moreover, this model has been adopted and extended by a se-
“hard” (or binary) in-the-moment learning strategies, where            ries of successive work (e.g., Nematzadeh, Fazly, & Steven-
an alignment is either considered by the learner or not (e.g.,         son, 2012a; Grant, Nematzadeh, & Stevenson, 2016), demon-
Trueswell, Medina, Hafri, & Gleitman, 2013); we instead ex-            strating its robustness in accounting for empirical data. We
amine “soft” strategies where alignments have strengths be-            adopt the FAS word learning framework to examine various
tween zero and one.                                                    in-the-moment learning mechanisms.
                                                                   853

The FAS Model                                                             and f in an overall association score assoc(w, f ), which is
Word learning input and output. The model’s input is a                    updated at each time t that w and f co-occur in an input pair:
sequence of utterance–scene pairs simulating what the child
                                                                                   assoct (w, f ) = assoct−1 (w, f ) + at ( f |w)           (3)
hears and perceives, respectively. Each utterance is a set of
words (ignoring their order), and the corresponding scene is              where assoct−1 (w, f ) = 0 if w and f have not co-occurred
a set of semantic features that represents possible meanings              prior to t.
of words in the utterance (see Ex. 1). Word meanings are                     After updating the association scores, the meaning proba-
represented by multiple features, which exposes the model to              bility p(·|w) of each word w in the current input is adjusted
naturalistic commonalities among the words.                               using a smoothed version of this formula:
         Utterance: { Joel, eats }
         Scene: { PERSON, JOEL, ACT, CONSUME, ... }               (1)                                       assoct (f , w)
                                                                                          pt+1 (f |w) =                                     (4)
The output of the model, at each step in learning, is the cur-                                            ∑ assoct (fj , w)
                                                                                                        fj ∈M
rent representation of the meaning of each word w as a proba-
bility distribution, p(·|w), over all possible semantic features          where M is the set of all features observed up to time t.
 f that the model has observed in the input scenes.                          In Eqn. (4), the probability of a feature given a word is a
The word learning problem. Given a corpus of utterance–                   normalization of their association score over all possible fea-
scene pairs, the goal of the model is to learn the meaning                tures, which introduces another source of competition, this
probability distribution, p(·|w), for all words w. Prior to re-           time, among features for a given word. This competition can
ceiving any input, all features f are equally likely for a word.          be thought of as a mutual exclusivity bias in the reversed di-
As the model processes each input pair, the probability is ad-            rection of the alignment score in Eqn. (2); here a word can
justed to reflect the cross-situational evidence in the corpus,           only be strongly associated to a limited number of features.
in two steps: (a) in-the-moment learning on this input pair               Using Sets of Features as Referents
and (b) update of the word meaning probabilities using the
                                                                          In FAS, an input scene is the set union of all meaning features
accumulated evidence over all inputs.
                                                                          for all words in the corresponding utterance. This represen-
In-the-moment learning. Given an utterance and a scene,                   tation lacks information that would be apparent to a child,
which features in the scene are part of a word’s meaning?                 namely that each set of meaning features belongs to a sin-
There are different possible ways to determine whether a se-              gle entity or event – e.g., PERSON and JOEL, or ACT and
mantic feature is associated with a word in the input pair, and           CONSUME in Ex. 1. However, replacing such sets of fea-
the corresponding strength of that association. FAS assumes               tures with a single symbol corresponding to the word mean-
that each feature f in scene St at time t, independently of the           ing would prevent the model from learning semantic similar-
other features, is aligned to all the words w in the utterance            ities among the words (e.g., Nematzadeh, Fazly, & Steven-
Ut with a particular strength (see Figure 1a):                            son, 2012b). Instead, following Alishahi, Fazly, Koehne, and
                                                                          Crocker (2012), we simply maintain each set of semantic fea-
                                    pt (f |w)
                    at (w|f ) =                                   (2)     tures corresponding to the meaning of each word in the utter-
                                 ∑ pt (f |w0 )                            ance, and we call these sets of features referents, as in Ex. 5:1
                                w0 ∈Ut
The alignment strength between a feature f and word w de-                       Utterance { Joel, eats, an, apple }
                                                                                Scene: { {PERSON , JOEL}, {ACT, CONSUME , ...},
pends on the current probability that f is part of the meaning                  {SINGULAR , INDEFINITE , DETERMINER , ... },                (5)
of w – i.e., pt ( f |w) – as well as the probabilities that f is part           {APPLE , FRUIT, FOOD , ...} }
of the meaning of other words in the utterance (the denomi-                  A scene is now a set of referents, each of which is a set
nator above).                                                             of semantic features corresponding to the meaning of a word.
    In this way, Eqn. (2) has words in the utterance “com-                In the FAS model, calculation of alignment strength between
pete” to be associated with a given feature: a higher align-              a word w and feature f at time t uses the meaning proba-
ment strength of one word with a feature necessarily results              bility pt ( f |w). Now, aligning words with referents (as in 5)
in a lower alignment strength for other words with that fea-              requires consideration of strength of alignment of a word with
ture. This can be interpreted as a directional mutual exclu-              a set of features. In calculating alignment strength for a word
sivity bias: the alignment formulation limits the number of               w and a referent r at time t, we change the FAS model to
words a feature can be strongly associated with, but does not             consider sim(vt (w), v(r)), the cosine similarity between the
directly limit the number of features a word can be associated            word’s current meaning representation and the representation
with.                                                                     of the referent, where v(r) and vt (w) are vectors in which
Updating the word meanings. How is the information                        the elements are meaning features. For vt (w), the value for
learned from an input pair incorporated into a learner’s long-                1 We use the term referent to denote anything referred to by a
term knowledge of word meanings? The learner incremen-                    word – an object or event, or set of semantic properties (e.g., { IN -
tally accumulates the alignment strengths between each w                  DEFINITE , SINGULAR } for an).
                                                                      854

each component feature f is pt ( f |w). (I.e, vt (w) is a vector     By normalizing the weighted count of sim(vt (w), v(r)), this
corresponding to pt (·|w).) For v(r), the element values are 1       alignment formulation can be interpreted as the conditional
for features present in the definition of r and 0 otherwise. In      probability of r given w, rather than a simple count.
this way, alignment strength for word w and referent r is in-        Word competition. Here, we consider a competition that is
fluenced by the strength of the meaning probabilities p( f |w)       instead analogous to the competition of words for a feature in
for all features f that are part of the representation of r. In      FAS; “word-comp” is the reverse of ref-comp, because here
the remainder of the paper, we explore variations in how the         words compete for a referent. This leads to a directed mu-
alignment process actually does this, in ways that implement         tual exclusivity bias, but in the opposite direction to ref-comp.
different types of mutual exclusivity biases.                        The word-comp mechanism asserts a preference for each ref-
In-the-Moment Learning Mechanisms                                    erent to be strongly associated with a single word, by having
                                                                     words compete for a referent, while the alignments of refer-
Competition in the model. We observed above that the                 ents are independent of each other (see Figure 1d). This bias
alignment strength calculation in Eqn. (2) instantiates a form       is formulated by normalizing the sim(vt (w), v(r)) over the
of mutual exclusivity bias, because words are competing to be        words in the utterance (as FAS did):
strongly associated with a feature during this in-the-moment
learning process. With the change of aligning words to ref-                                      sim(vt (w), v(r))
                                                                                  at (w|r) =                                       (8)
erents instead of to features, we have the opportunity to ex-                                 ∑ sim(vt (w0 ), v(r))
plore various ways to formulate competition in determining                                   w0 ∈Ut
the strength of alignments. The three alignment formulations         This formulation also yields a conditional probability, but
explored here implement (1) no competition among words               here of w given r.
or referents, (2) competition of referents for a word (as in         The association score. We note one final change to the FAS
Alishahi et al., 2012), and (3) competition of words for a ref-      model to deal with referents: We must modify Eqn. (3) to
erent (analogous to the competition of words for a feature in        keep track of associations between a word w and all the fea-
FAS). Each of these ways of viewing competition implements           tures of a referent r. Since a feature f can occur in more than
a different approach to mutual exclusivity in the model, and         one referent in scene S, which can have multiple alignment
we will explore the resulting impact on word learning in the         scores, we use the maximum alignment score of a referent
results.                                                             that contains the feature in updating the feature’s association
No competition. The no-competition mechanism (hence-                 score:
forth, no-comp) serves as a baseline for comparison to the
other two. It assumes no mutual exclusivity bias – all the               assoct (w, f ) = assoct−1 (w, f ) + max at (w, r0 )       (9)
                                                                                                             r0 ∈S: f ∈r0
alignments between words and referents are calculated inde-
pendently, and the value of one alignment does not effect any           The meaning probabilities in the model continue to be cal-
of the others (see Figure 1b). We formulate such an alignment        culated between individual features and a word. Recall that
between a word w and a referent r as simply the similarity be-       the meaning probability distribution p(·|w), as a conditional
tween vt (w) and v(r) as described above:                            probability over semantic features, enforces a competition
                                                                     among them for the probability mass.
                 at (w, r) = sim(vt (w), v(r))               (6)
                                                                                             Experiments
This formulation can be seen as a simple weighted count,             Set-up
where each feature relevant to r (valued 1 in v(r)) contributes
to the overall alignment strength proportionally to the model’s      The utterances in the input are child-directed speech taken
prior knowledge of its meaning probability with that word.           from the Manchester corpus (Theakston, Lieven, Pine, &
                                                                     Rowland, 2001) in CHILDES (MacWhinney, 2000). To cre-
Referent competition. Here we adopt the alignment formu-
                                                                     ate the associated scene representations, each word in the cor-
lation of Alishahi et al. (2012), which we call “ref-comp”
                                                                     pus is entered into a gold-standard lexicon with a set of se-
because referents compete for alignment with a word. This
                                                                     mantic features representing its gold-standard meaning, fol-
mechanism implements a directed mutual exclusivity bias in
                                                                     lowing the procedure of Fazly et al. (2008). The referents
which each word has a preference to be strongly associated
                                                                     shown in Ex. 5 correspond to the gold-standard meanings of
with one referent. In other words, referents in the scene com-
                                                                     each of those words. (The word–mapping in the lexicon is
pete for a given word, while the alignments of words are in-
                                                                     only used to generate scenes, and is not seen by the model.)
dependent of each other (see Figure 1c). This preference can
                                                                     The model is trained on 20K utterance–scene pairs, at which
be implemented by normalizing the sim(vt (w), v(r)) over all
                                                                     point behaviour is stable.
the referents in the scene:
                                                                        In the following experiments, we examine the quality of
                              sim(vt (w), v(r))                      the individual learned word representations in two ways: the
             at (r|w) =                                      (7)     average acquisition score of all words observed by the model,
                           ∑ sim(vt (w), v(r0 ))                     and the proportion of observed words that is learned. The
                          r0 ∈St
                                                                 855

              (a) FAS.                    (b) No competition.            (c) Referent competition.         (d) Word competition.
Figure 1: Types of alignment mechanisms. Lines of the same color/style compete simultaneously. Thickness indicates varying
strength of alignment during a competition.
                                                                       aligning features individually as in FAS, the correct features
                                                                       for a word may be aligned more or less strongly (depending
                                                                       on competition for each from other words), so that the over-
                                                                       all meaning probability vector may not converge as easily to
                                                                       the full set of correct features. By contrast, when a word has
                                                                       a strong alignment with the correct referent – which corre-
                                                                       sponds to the gold-standard meaning of the word – all fea-
                                                                       tures of the referent are boosted in the meaning probability
                                                                       of the word, yielding improved learning in word-comp over
                                                                       FAS.
                                                                          Second, we find an interesting asymmetry between the two
                                                                       mechanisms involving competition between the words for a
                                                                       referent (word-comp) and between the referents for a word
(a) Average acquisition score (b) Proportion of learned words
over time.                       over time.                            (ref-comp). Each imposes a conditional probability formu-
                 Figure 2: Developmental plots                         lation of competition, but word-comp performs much better,
                                                                       with ref-comp behaving no better than the no-comp model.
acquisition score of each word w is obtained by comparing              In fact, the advantage of using referents instead of individual
the word meaning representation v(w) with a gold standard              features is completely eliminated in both the no-comp and the
representation of the word gold(w) using cosine similarity:            ref-comp mechanisms, as both perform worse than FAS.
                  acq(w) = sim(v(w), gold(w))                 (10)        The source of this asymmetry, we believe, is the deploy-
                                                                       ment of learned knowledge by the model. In both the no-
where gold(w) is a vector over all semantic features, with             comp and the ref-comp model (Figure 1(b), (c)), a learned
value 1 for features part of the gold-standard meaning of w            word meaning is compared to the referents in isolation from
and 0 otherwise. An observed word counts as “learned” if               the learned meanings of the other words in the utterance. In
its acq score is higher than some threshold θ, here set to 0.7         this set-up, the knowledge of other word meanings cannot
based on the experiments of Fazly et al. (2010).                       help to guide the model to determine how good a word’s
                                                                       alignment to some referent is. By contrast, the word-comp
Results                                                                model (Figure 1(d)) tunes the alignments by comparing how
Overall Learning Patterns                                              similar various learned word meanings are to a referent.
   Over time, all models converge to high average acq scores              One might expect that mutual exclusivity in the reverse di-
(Figure 2a) and proportions of words learned (Figure 2b), but          rection (as in the ref-comp model) would achieve the same
with substantial differences between them. Notably, we find            effects: Tuning the similarity between a word meaning and a
that on the average acq score, the word-comp formulation               referent by the similarity between that word meaning and all
performs better than the original FAS (.96 vs. .86), while the         other referents should guide the model to correct associations
ref-comp and no-comp models do not learn the representa-               more quickly than not doing so. However, we do not find this
tions as well (both .83).                                              effect. We will return to the reason for this lack of effect in
   Two factors may underlie the varying performance of the             the section on the role of frequency.
models: the semantic grouping of features into referents (dis-            Competition is clearly important in focusing alignments
tinguishing our models from FAS), and the type of in-the-              and facilitating learning, but only in the context of appro-
moment competition (and resulting type of mutual exclusiv-             priately constraining information: the most effective learning
ity). For the first factor, the word-comp mechanism provides           occurs when the competition draws on the maximal amount
the most direct comparison to FAS: it uses the same direction          of learned knowledge in the model, in the form of the devel-
of bias – in which words compete to align with the elements            oping meaning probabilities. In what follows, we consider
of the scene – but using referents instead of features. The            the impact of increased ambiguity in forming alignments, or
grouping into referents appears to improve learning. When              decreased knowledge about words, to see how these factors
                                                                   856

                                                                                                     1.0   mechanisms                                                                        frequency = high              frequency = low
                                                                                                            FAS10                                                                     1.0
                            1.0                                                                             no−comp
average acquisition score                                                 average acquistion score
                                                                                                                                                           average acquistion score
                                                                                                     0.9    ref−comp
                                                                                                            word−comp                                                                 0.9
                            0.9
                            0.8                                                                      0.8
                                                                                                                                                                                      0.8
                            0.7                                                                      0.7
                                                                                                                                                                                      0.7
                            0.6
                                                                                                     0.6                                                                              0.6
                            0.5
                                        high               low
                                                                                                     0.5                                                                              0.5
                                               frequency
                                                                                                                        long                    short                                       long          short          long          short
                                                                                                                          mean length of utterance                                                     mean length of utterance
                                  (a) Split over word frequency.
                                                                                                      (b) Split over mean length of utterance.                                        (c) Split over MLU and word frequency.
                                                                   Figure 3: Average acquisition score after 20K input items.
impact these various mechanisms. Because the proportion of                                                                               in principle go equally well with any referent in the situation,
words learned shows similar relative behaviours to the acq                                                                               but a new referent not equally well with any word in the ut-
score, in the remaining analysis we focus on comparing acq                                                                               terance.
scores of each of the models after 20K inputs.                                                                                           The Role of Utterance Length
The Role of Frequency                                                                                                                       Above, we found that the different types of competition
   Children are able to learn word meanings in various condi-                                                                            gave more pronounced results for low-frequency words than
tions, sometimes after only a few observations. Previous re-                                                                             for high-frequency ones. Similarly, we can explore whether
search suggests that children use biases such as mutual exclu-                                                                           there is a differential impact of utterance length on the dif-
sivity to guide their learning. Learning low-frequency words                                                                             ferent models. To simulate this, we manipulated the input
is also a challenge for computational models, and understand-                                                                            generation procedure so that the model was trained only on
ing the mechanisms that improve learning from little evidence                                                                            utterances of length 5 or higher (long-corpus), or 3 and lower
can shed light on how children address this issue. The type of                                                                           (short-corpus). Looking at Figure 3b, we observe that the
competition in our various models plays an important role in                                                                             acquisition scores are globally lower when the models are
their performance on low-frequency words. Figure 3a shows                                                                                trained on long sentences only, likely due to the fact that there
that for the two models with competition over words – the                                                                                is more uncertainty about which words and which referents
FAS and word-comp models – there is no decrease in perfor-                                                                               belong together.
mance for words of low frequency (< 5) compared to high                                                                                     Here we see that the word-comp model is the only one
frequency (> 10), while for the other two models, no-comp                                                                                to not substantially decline in performance when compar-
and ref-comp, there is a dramatic drop off in learning.                                                                                  ing learning on the short-corpus and long-corpus. While the
   Specifically, the competition among words in the FAS and                                                                              competition over words seems to help the FAS and word-
word-comp models – which maximizes the use of learned                                                                                    comp models equally in dealing with low-frequency words,
knowledge in focusing alignments – appears to play a crucial                                                                             here the bundling of features into referents as in word-comp
role in enabling these models to learn low-frequency words.                                                                              is also necessary for performance to be robust to the added
Comparing the alignments in Figure 1c and Figure 1d in the                                                                               ambiguity of long utterances. The FAS model cannot “scale
face of a novel word and its novel referent (as an extreme                                                                               up” to deal with the very long unstructured lists of features in
case of low frequency) will clarify the utility of the learned                                                                           the long-corpus input. This also explains why the model of
meaning probabilities. In the word-comp model (Figure 1d),                                                                               Alishahi et al. (2012) (the ref-comp approach) worked well
the meaning probabilities of previously-seen words compet-                                                                               in their experiments but not here: the utterances they used
ing for a new referent will not have a very good match to the                                                                            all had two words, unlike the naturalistic data we train on
feature vector for the new referent (since their probabilities                                                                           above, indicating that ref-comp also cannot scale effectively.
will have been adjusted to better fit referents they have been                                                                           Interestingly, as shown in Figure 3c, the word-comp model is
seen with). The novel word will have uniform meaning prob-                                                                               particularly robust to the challenge of learning low-frequency
abilities that will enable it to better match the new referent,                                                                          words in the corpus of longer utterances, with a very small
and thus will have a stronger alignment than previously-seen                                                                             decrease in performance compared to the other models.
words. By contrast, in the ref-comp model (Figure 1c), the                                                                               The Role of Referential Uncertainty
uniform probabilities of the new word will equally match all                                                                                To explore the impact of referential uncertainty – the occur-
the referents competing for it, whether they have been seen                                                                              rence of many more potential referents in a scene than there
before or not. There is no prior knowledge in the model in                                                                               are words – we create a subcorpus that uses every ith utterance
this competition that indicates the previously-seen referents                                                                            from our full corpus, and uses the utterances in between those
have a better fit with other words. Thus a competition among                                                                             to generate “extra” referents in the scenes for utterances in the
words works well for novel or low-frequency words by draw-                                                                               subcorpus. Here we report results on 20K inputs with refer-
ing on the fact that previously-seen words will not compete                                                                              ents added to each scene Si from 0, 1, or 2 utterances in ad-
as strongly for a new(er) referent. In short: a new word can                                                                             dition to referents taken from utterance Ui . Figure 4 presents
                                                                                                                                   857

                                      1.0
                                                                                                           References
          average acquisition score
                                      0.9                                           Alishahi, A., Fazly, A., Koehne, J., & Crocker, M. W. (2012).
                                      0.8                                             Sentence-based attentional mechanisms in word learning:
                                                                                      evidence from a computational model. Frontiers in psy-
                                      0.7
                                                                                      chology, 3.
                                      0.6                                           Fazly, A., Alishahi, A., & Stevenson, S. (2008). A proba-
                                      0.5                                             bilistic incremental model of word learning in the presence
                                            0             1
                                                referential uncertainty
                                                                          2
                                                                                      of referential uncertainty. In CogSci Proceedings.
                                                                                    Fazly, A., Alishahi, A., & Stevenson, S. (2010). A probabilis-
Figure 4: Average acquisition score after 20K input items,
                                                                                      tic computational model of cross-situational word learning.
split over different amounts of referential uncertainty.
                                                                                      Cognitive Science, 34(6), 1017–1063.
the results for no referential uncertainty, along with the two                      Grant, E., Nematzadeh, A., & Stevenson, S. (2016). The
added levels of uncertainty. As we expect, the learning per-                          interaction of memory and attention in novel word gener-
formance of all models degrades with higher referential un-                           alization: A computational investigation. In CogSci Pro-
certainty. However, in contrast to our previous results, here                         ceedings.
there is little benefit from either word-based competition or                       Kachergis, G., Yu, C., & Shiffrin, R. (2012). An associa-
feature bundling. The high degree of ambiguity introduced                             tive model of adaptive inference for learning word–referent
by these levels of referential uncertainty may be better dealt                        mappings. Psychonomic Bulletin and Review, 1–8.
with by attentional mechanisms that focus joint attention on                        MacWhinney, B. (2000). The CHILDES project: Tools for
a likely subset of relevant referents prior to alignment.                             analyzing talk (3rd ed., Vol. 2: The Database). Erlbaum.
                                                                                    Markman, E. M. (1987). How children constrain the possible
                           Conclusions and Future Work                                meanings of words. In U. Neisser (Ed.), Concepts and con-
                                                                                      ceptual development: Ecological and intellectual factors in
Previous research shows that children are sensitive to the
                                                                                      categorization (Vol. 1, pp. 255–287). CUP.
cross-situational statistics of their environment: i.e., they can
                                                                                    Markman, E. M., & Wachtel, G. F. (1988). Children’s use
use the regularities across different situations to learn word
                                                                                      of mutual exclusivity to constrain the meanings of words.
meanings. However, the detailed mechanisms responsible for
                                                                                      Cognitive Psychology, 20(2), 121 - 157.
cross-situational word learning are still not fully understood,
                                                                                    Nematzadeh, A., Fazly, A., & Stevenson, S. (2012a). A com-
such as precisely what information is used from each ob-
                                                                                      putational model of memory, attention, and word learning.
servation in identifying the correct word meaning, and how
                                                                                      In CMCL Proceedings (pp. 80–89).
this information is incorporated in the accumulated knowl-
                                                                                    Nematzadeh, A., Fazly, A., & Stevenson, S. (2012b). Inter-
edge about a word. Moreover, children are good at learning
                                                                                      action of word learning and semantic category formation in
word meanings in a variety of situations: they can learn a
                                                                                      late talking. In CogSci Proceedings (pp. 2085–2090).
novel word from a few example and also acquire words from
                                                                                    Pinker, S. (1989). Learnability and cognition: The acquisi-
ambiguous/noisy conditions. Previous research has suggested
                                                                                      tion of argument structure. Cambridge, Mass.: MIT Press.
that children are equipped with biases that guide them in word
                                                                                    Siskind, J. M. (1996). A computational study of cross-
learning by reducing the difficulty/ambiguity of a learning sit-
                                                                                      situational techniques for learning word-to-meaning map-
uation. The necessity of these biases in children, and whether
                                                                                      pings. Cognition, 61, 39–91.
they are innate or learnable, are issues that have been debated
                                                                                    Smith, L. B., & Yu, C. (2008). Infants rapidly learn word-
among cognitive scientists.
                                                                                      referent mappings via cross-situational statistics. Cogni-
   Here, we show that one such bias – the mutual exclusiv-                            tion, 106(3), 1558–1568.
ity bias that limits the number of meanings a word takes –                          Theakston, A. L., Lieven, E. V., Pine, J. M., & Rowland, C. F.
can be modeled as a competition mechanism during in-the-                              (2001). The role of performance limitations in the acqui-
moment learning. The competition exists when the model                                sition of verb–argument structure: An alternative account.
assesses possible word and referent associations with condi-                          Journal of Child Language, 28, 127–152.
tional probabilities as opposed to counts. In other words, the                      Trueswell, J. C., Medina, T. N., Hafri, A., & Gleitman, L. R.
bias or competition is a learning mechanism that is able to                           (2013). Propose but verify: Fast mapping meets cross-
condition in-the-moment learning to the learned knowledge                             situational word learning. Cog. Psych., 66(1), 126–156.
of word meanings. We observe that the role of the bias is par-                      Yu, C., & Smith, L. B. (2007). Rapid word learning under
ticularly significant when the learning is more challenging:                          uncertainty via cross-situational statistics. Psychological
for example, for learning low-frequency words or from longer                          Science, 18(5), 414–420.
utterances. Previous research has investigated how cognitive                        Yurovsky, D., Fricker, D. C., Yu, C., & Smith, L. B. (2014).
processes such as memory and attention interact with cross-                           The role of partial knowledge in statistical word learning.
situational word learning (e.g., Nematzadeh et al., 2012a).                           Psychonomic Bulletin and Review, 21(1), 1–22.
Future work should study how these cognitive processes af-
fect the in-moment-learning.
                                                                              858

