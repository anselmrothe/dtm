                               Strategic exploration in human adaptive control
                         Eric Schulz1 , Edgar D. Klenske2,3 , Neil R. Bramley4 & Maarten Speekenbrink1
                  1 Department    of Experimental Psychology, University College London, London, WC1H0AP, UK
                    2 Max Planck Institute for Intelligent Systems, Spemannstraße 38, 72076 Tübingen, Germany
                       3 Max Planck ETH Center for Learning Systems, ETH Zurich, 8092 Zurich, Switzerland
                            4 Department of Psychology, New York University, New York, NY, 10003, USA
                               Abstract                                   this approach is that not all uncertainty should be treated
   How do people explore in order to gain rewards in uncer-               equally but rather that exploration should be driven by both
   tain dynamical systems? Within a reinforcement learning                the current knowledge of the system and the agent’s overall
   paradigm, control normally involves trading off between ex-            goal. Exploration, according to this definition, is a strategic
   ploration (i.e. trying out actions in order to gain more knowl-
   edge about the system) and exploitation (i.e. using current            action. We will refer to this kind of exploration as strategic
   knowledge of the system to maximize reward). We study a                exploration.
   novel control task in which participants must steer a boat on
   a grid, aiming to follow a path of high reward whilst learning            Many real-world control scenarios are non-episodic, such
   how their actions affect the boat’s position. We find that partic-     that there are no “second chances” and one may be unable
   ipants explore strategically yet conservatively, exploring more
   when mistakes are less costly and practicing actions that will         to return to known states. In such situations, one must treat
   be required later on.                                                  exploration strategically and with great caution to avoid acci-
   Keywords: Reinforcement Learning, Strategic Exploration,               dents (Klenske & Hennig, 2015). Imagine visiting a country
   Control, Exploration-Exploitation
                                                                          with left-hand traffic from a country with right-hand traffic.
                           Introduction                                   Strategically exploring in order to learn how to drive on the
                                                                          left side could allow you to make your mistakes on the quiet
Deciding how to act under uncertainty is a core problem for
                                                                          roads first before hitting the highway. Moreover, as turning
cognition. Cognitive agents must be able to navigate a world
                                                                          right will be more difficult than you are used to, practicing
whose dynamics are initially unknown and generally uncer-
                                                                          how to turn right is more important than practicing how to
tain, learning to generate rewards as they go along. In the
                                                                          turn left and therefore should be exercised more frequently.
context of reinforcement learning, we can think of control as
a trade-off between exploration (i.e. trying out actions to gain             In machine learning, problems of planning under uncer-
more knowledge about the underlying system) and exploita-                 tainty have been approached via Bayesian reinforcement
tion (i.e. using current knowledge of the system to maximize              learning (BRL; Poupart, 2010), which assigns probabilistic
reward). However, whether and how human explorative con-                  beliefs over the dynamics of a system and the costs of states
trol reflects future goals and current uncertainty is still unclear       and actions in order to reason about potential changes to be-
(Wilson, Geana, White, Ludvig, & Cohen, 2014). Are human                  liefs from future observations, and their influence on future
explorative actions strategic and goal-directed? Or are they              decisions (Duff, 2002). BRL provides a useful framework
rather passive, for instance involving a simple “exploration              for assessing strategic exploration behavior as we do here.
bonus” that treats uncertainty equally across all actions?                More specifically, we will make use of the duality between
   Traditionally, reinforcement learning models have ad-                  reinforcement learning and control, that is tasks in which an
dressed exploration rather implicitly, letting the agent learn            agent has to keep a system at a certain state in order to gener-
about the underlying system en passant via outcomes pro-                  ate rewards (Feldbaum, 1960; Klenske & Hennig, 2015).
duced while high rewards are chased (Rescorla, Wagner et al.,
1972). Exploration, according to this definition, is what hap-               In what follows, we will assess how participants exert con-
pens when an agent optimizes noisily. We will refer to this               trol within a novel control paradigm. Therein, strategic explo-
kind of exploration as passive exploration.                               ration allows them to produce greater long-term rewards —
   More recently, exploration has been incorporated into rein-            formally, within a non-episodic, finite-horizon system with
forcement learning models more explicitly via an exploration              initially unknown dynamics. We will build on recent work by
bonus (Schulz, Konstantinidis, & Speekenbrink, 2016; Wu,                  Klenske & Hennig (2015) and assess behavior in two tasks:
Schulz, Speekenbrink, Nelson, & Meder, 2017). An explo-                   one in which, due to time-varying state costs, exploration can
ration bonus assigns additional utility to less explored actions          be delayed until it is more opportune; and one in which the
and thereby assumes that the agent values uncertainty equally             learning agent can distinguish between more and less impor-
across all actions. Exploration, according to this definition, is         tant exploration of directional actions. We first discuss in
what happens when expectations are inflated by their attached             more detail the task and the three perspectives on exploration
uncertainties. We will refer to this kind of exploration as ag-           in control theory passive,agnostic and strategic exploration.
nostic exploration.                                                       We then assess qualitative predictions derived from these in
   Another line of research tries to redefine exploration as              two experiments. We find participants’ behavior to be more
goal-directed behavior (e.g., Thrun, 1992). The idea behind               in line with predictions derived by strategic exploration.
                                                                      1047

                          Control task                                      more than if they entered the same angle at another position
In a simple computer game, participants have to navigate a                  further up. Formally, at each time t, the (vertical) position of
boat as it crosses a sea towards regions in which they can                  the boat, yt , depends on a two-dimensional latent state vari-
earn a higher bonus. The boat moves incrementally from left                 able xt and independent random noise γt as
to right and by changing its current angle of direction (see
                                                                                            yt = Cxt + γt           γt ∼ N (0, σγ ).           (1)
Figure 1), participants could attempt to steer the boat up or
down, so as to remain in calm waters (blue) and avoid perilous              The latent state depends through a nonlinear function on the
rough seas (red). The overall goal of the game was to min-                  previous latent state, the controller input (i.e., the chosen an-
                                                                            gle) ut , and additional noise ξt , as:
                                                                                      xt+1 = Aφ(xt ) + But + ξt             ξt ∼ N (0, Σξ ),   (2)
                                                                            where
                                                                                                                                     1
                                                                                                                         "           #
                                                                                            1       0.4 x1,t          0     0        x +5
                                                                                                                                  1+e 1,t
                                                                                Aφ(xt ) =                         +                  1       , (3)
                                                                                            0        1      x2,t     θ1    θ2       −x +5
                                                                                                                                 1+e 1,t
                                                                            θ = [0.8, 0.4]> , and B = [0, 1]> . The underlying drift is deter-
                                                                            mined by the shifted sigmoid functions on the right-hand side
                                                                            of Equation 3. Given a finite-time horizon with terminal time
                                                                            T , the following quadratic cost function was used:
                                                                                                            T
                                                                                           L (x, u) = ∑ (xt − rt )> Wt (xt − rt )              (4)
                                                                                                           t=0
                                                                            where r = [r0 , . . . , rT ] is the target trajectory and Wt the time-
Figure 1: Example path in Experiment 1, Free Late condition. Star
= starting position at t = 0, circles = subsequent positions. On each       varying state cost. The goal of the controller is to find the
trial, the gray arrow shows contribution of underlying current and          action sequence u = [u0 , . . . , uT ] that minimizes the expected
black arrow contribution of control angle. At t = 0 the participant         cost (and thereby maximizes the expected reward) to the hori-
takes a control angle of 0 and drifts upward. On the 10 subsequent
trials they attempt to counteract this upward drift by setting a nega-      zon T .
tive angle. During the free exploration stage they use wider angles
to explore the variation in the strength of the current at different y                               Control strategies
positions. This allows them to discover that strength of the current is
strongest in the center, approaching zero toward the top, constantly        Controlling a system as defined in Eqs. (1) – (3) is difficult,
pulling the boat upwards.                                                   as the state dynamics are nonlinear with an unknown function
                                                                            φ and parameters A and B. The controller then not only needs
imize the “cost” of the voyage while simultaneously learn-                  to control the states in accordance to the reference path r, but
ing both how to control their boat and about an underlying                  also learn the parameters (and functions) in order to derive a
position-dependent “current” that drags the boat off course.                good control strategy u. Thus, the controller not only needs
In some periods, the area of low cost was very narrow, while                to control the states, but also control her knowledge about the
in other periods, the area was very wide. Analogous to real                 model, hence the term dual control.
sailing, participants had to learn to control the boat through                  We will now provide a description of the three different
experience, by trying different angles and observing the ef-                forms of exploration mentioned earlier, and their qualitative
fect on the boat’s position. This exploration is costly when                predictions in the present control task. The predictions are
the low-cost region is narrow, whilst exploration is almost                 shown graphically in Figure 2 for the variants of the task used
“free” when the low-cost region is very wide.                               in Experiment 1, which tests whether participants will post-
    Our control task is adapted from Klenske & Hennig (2015).               pone exploration until it is most opportune, and Experiment
Therein, the boat is influenced by two factors, its current                 2, which tests whether participants perform strategic (direc-
position x1 and an underlying current x2 . This means that                  tional) exploration.
where the boat will end up on the next trial is influenced
by the chosen angle, its current position, and the underly-                 Passive exploration by certainty equivalence
ing current which is determined by an unknown nonlinear                     A certainty equivalence controller completely ignores uncer-
function. Within our experiments, the underlying current de-                tainty about the dynamics and derives a control strategy as
creased from its full strength in the middle of the sea to zero             if the current (mean) estimates of the system are accurate and
at the upper and lower edges, and constantly pulled the boat                knowledge about the system is perfect. Effectively, any learn-
upwards. For example, if participants entered the angle of 0                ing about the system happens passively, as the control strat-
in the center of the sea, the boat would be pulled upwards                  egy does not focus on minimizing uncertainty. As no active
                                                                        1048

A                                                 B                                                C
Figure 2: Control environments and qualitative model predictions for Experiment 1 and 2. The agent moves one step right each time point
(trials are delimited by white vertical lines) and can control the angle upwards/downwards in which a boat is steering. The background color
represent the cost function; the more red, the lower the score; dark blue areas mark free exploration trials. Qualitative model predictions taken
from Klenske & Hennig (2015) are represented by horizontal line called ’predictive regions’. Black lines represent the predictive region for
passive exploration, white lines for agnostic exploration, and purple lines for strategic exploration. The more space between the horizontal
lines at a trial, the wider the region and the higher the expected variance of the controller’s actions.
A: Strategic exploration holds off exploration until it comes at lower cost (broader trust region during the dark blue patch) and consequently
performs better than passive exploration later on (narrower trust region).
B: If free exploration phase is moved to the end, strategic exploration explores less overall and expedites exploration to earlier, more costly
stages, thereby reducing performance early on in order to achieve the best performance later on.
C: Instead of agnostically exploring both directions in the same way, strategic exploration uses the free exploration phase to try to move in a
trajectory which is rewarding in the future, thereby performing better later on.
exploration is encoded into this model, it might miss out on                system as well as possible given current knowledge (exploita-
important information that could be beneficial to produce bet-              tion) and learning about the system through experimentation
ter rewards later on. This form of control predicts no explo-               in order to control it better later on (exploration), is known to
ration, even when exploration is ‘free’ and beneficial to future            be intractable.
rewards.                                                                       Approximate dual control involves three conceptual steps
                                                                            which together yield what, from a contemporary perspec-
Agnostic exploration by exploration bonus                                   tive, amounts to an approximate solution to Bayesian RL:
To promote exploration, a straightforward adaptation of the                 First, determine the optimal trajectory under the current mean
certainty equivalent controller is to introduce a Bayesian ex-              model of the system (as in certainty equivalent control). Sec-
ploration bonus. Effectively, this means adapting the cost                  ond, construct a local quadratic expansion around the nom-
function so that the costs of actions which reduce uncer-                   inal trajectory that approximates the effects of future obser-
tainty in the model of the control dynamics–as measured by                  vations. Third, within the current time step t, perform the
the standard deviation of the posterior distribution over the               prediction for an arbitrary control input ut and optimize ut nu-
parameters at each observation point–is temporarily reduced                 merically by repeated computation of steps 1 and 2 at varying
(cf. Srinivas, Krause, Kakade, & Seeger, 2009). This model                  uk to minimize the approximate cost (see Klenske & Hennig,
is still myopic as it only considers uncertainty at the current             2015, for implementation). Approximate dual control does
control step. Moreover, exploration is not strategic, as all                not treat all exploration equally but rather explores strategi-
uncertainty is treated equally and it does not take into ac-                cally by, for example, holding off exploration until it is less
count what knowledge might be most important later on. Un-                  costly or by exploring actions that will become important
der agnostic exploration, the expected behavior would be the                later on.
attempt to identify all uncertain components, irrespective of
their future usefulness.                                                           Experiment 1: Holding off exploration
                                                                            Our first experiment was designed to test passive exploration
Strategic exploration as dual control                                       against both agnostic and strategic exploration by including a
BRL involves reasoning about the effect of actions on future                low-cost period which was either introduced relatively early
rewards and beliefs. Where an exploration bonus renders re-                 (“Free Early” condition) or at the end of the task (“Free Late”
ducing uncertainty rewarding in itself, in BRL, reducing un-                condition). When a low-cost period is introduced early, con-
certainty is only attractive insofar as it is expected to result in         trollers can make good use of it to explore and better their
an increase in future rewards. Optimal BRL requires deter-                  performance in later periods, while exploring in a low-cost
mining the consequences of strengthening beliefs on future                  period at the end of the task is not beneficial as there are no
rewards, thereby finding the optimal balance between explo-                 later rewards to reap.
ration and exploitation. Unfortunately, the optimal solution                   Both conditions experienced an initial stage of medium
to the dual control problem of simultaneously controlling the               state costs (see Figure 2). However, whereas for the Free
                                                                       1049

Early condition that stage is followed by a stage of free ex-         Additionally, they had different target areas (rt ) on each trial
ploration (no costs of errors) which then leads to a stage of         marked by dark blue colors and how far they were off from
very high cost, the Free Late condition experiences the stage         the target area was penalized differently (based on Wt ). An
with high state costs first before then experiencing the stage        example trial from the task (for the Free Early condition) is
with no costs (the two stages are swapped).                           depicted in Figure 1.
   We expected participants to behave as strategic controllers           The cost function was shown to participants through the
and to initially hold off exploration in the Free Early condi-        color of each position in the sea. Participants could earn be-
tion until it comes at no cost in the low-cost period, allowing       tween 0 (positions with a red background) and 100 points (po-
them to be prepared for the most difficult final stages. In con-      sitions with a blue background) per trial.
trast, participants in the Free Late condition were expected
to explore more in the initial period, in order to be prepared        Participants
for the second, most difficult stage. In addition, we expected        Sixty-one participants were recruited via Amazon Mechani-
participants in the Free Late condition to explore less in the        cal Turk and received $1 and a bonus of up to $1. Thirty-nine
low-cost period compared to those in the Free Early condi-            participants were male and the mean age was 31.3±8.4.
tion, as late exploration no longer brings benefits if the task
                                                                      Results
is nearly over. Finally, we expected participants in the Free
Early condition to generally perform better than participants         The distribution of boat position, as well as average chosen
in the Free Late group, as early exploration would enhance            angles, are depicted in Figure 3. We can see that, overall,
their knowledge of the system for the remainder of the task.
                                                                                                                     Free Early
Design
                                                                                                             Free
                                                                                        Medium                                                      High
The manipulation involved changing the order of the refer-                                                 exploration                ● ● ● ● ● ●
ence trajectory (the state values that would produce the high-
est rewards) and state weightings.
   In the Free Early-condition the reference trajectory and
state weightings were:                                                               ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●                    ● ●                 ● ● ● ●
                                             
                      0             7            0
            r1:22 =       r      =      r    =
                      0 23:28       0 29:30      0
                                    
                      −7              0                                                                                                                   ● ● ● ● ● ●
           r31:36 =         r37:40 =
                       0              0
and                                                                       Position                                    Free Late
                                                                                                                                                          Free
            1     0            0     0           10       0                             Medium                                     High
                                                                                                                                                              exploration
   W1:10 =           W11:20 =          W21:40 =                                                                ● ● ● ● ● ●
            0     0            0     0            0       0
In the Free Late condition, these were:                                              ● ● ● ● ● ● ● ● ● ● ● ●                 ● ●                 ● ● ● ● ● ● ● ● ● ● ● ● ● ●
                                          
                     0             7          0
            r1:12 =      r      =      r   =
                     0 13:18       0 19:20    0
                                   
                     −7              0                                                                                             ● ● ● ● ● ●
           r21:26 =        r27:40 =
                      0              0
                                                                                      1
                                                                                      2
                                                                                      3
                                                                                      4
                                                                                      5
                                                                                      6
                                                                                      7
                                                                                      8
                                                                                      9
                                                                                     10
                                                                                     11
                                                                                     12
                                                                                     13
                                                                                     14
                                                                                     15
                                                                                     16
                                                                                     17
                                                                                     18
                                                                                     19
                                                                                     20
                                                                                     21
and
                                                                                     22
                                                                                     23
                                                                                     24
                                                                                     25
                                                                                     26
                                                                                     27
                                                                                     28
                                                                                     29
                                                                                     30
                                                                                     31
                                                                                     32
                                                                                     33
                                                                                     34
                                                                                     35
                                                                                     36
                                                                                     37
                                                                                     38
                                                                                     39
                                                                                     40
                                                                                                                             Trial
                                                                      Figure 3: Boat positions by condition in Experiment 1. The heat map
                                                         
            1     0            10     0             0     0
   W1:10 =           W11:30 =             W31:40 =                    reflects number of participants who were at that position on a given
            0     0             0     0             0     0           trial. Error bars represent the standard error of the average position
                                                                      per trial. Arrows indicate the average chosen angle. Black dots mark
                                                                      the target trajectory and periods with different state weights are de-
                                                                      limited by vertical lines.
Materials
Participants were told that they had to navigate a boat through       participants managed to steer the boat reasonably well. A
the sea in a sailing competition. On every trial, their boat was      linear regression of condition, cost function weights (coded
at a current position (yt ) and they had to determine an angle        as dummy variables), and trial number onto participants’
(ut , between -180◦ and 180◦ ) in which they wanted to sail.          scores (see Table 1) showed that, unsurprisingly, cost function
                                                                   1050

weights had the largest effect on participants’ scores. More-         upwards and then precise increments downwards, whereas
over, performance increased significantly over trials. Impor-         participants in the Down-Up condition should explore to do
tantly, condition affected overall performance, such that par-        the opposite, as knowledge about these actions will be useful
ticipants in the Free Early condition performed better than           later on. Note that mimicking the later target trajectory dur-
participants in the Free Late condition. This confirms the hy-        ing the free exploration phase is better then trying upwards
pothesis that participants would benefit from early free explo-       and downwards movements at one position as the current, and
ration.                                                               with that the effect of a chosen angle on the position, varies
                                                                      nonlinearly depending on the boat’s position.
     Table 1: Regression estimates for Experiment 1. r2 = 0.38.
                                                                      Design
                    Estimate     Std. Err.    t value    Pr(>|t|)     The underlying dynamics were exactly the same as in Ex-
        Intercept        99.9          2.76      36.2      0.000      periment 1. The manipulation solely concerned the reference
      Condition         -3.04          1.07     -2.84      0.004      trajectory, which for the Up-Down condition was:
   Med. cost: 1         -15.7          2.13     -7.39      0.000                                              
  High cost: 10         -47.3          1.30     -36.3      0.000                      0             3             5
                                                                            r1:23 =      r24:26 =       r27:29 =
            Trial        0.18          0.07      2.61      0.009                      0             0             0
                                                                                                                         
                                                                                      7             5             3            0
                                                                           r30:32 =     r33:35 =        r36:38 =     r39:40 =
                                                                                      0            0              0            0
   Another hypothesis was that participants in the Free Early
condition would explore more during the free exploration              And for the Down-Up condition, the reference trajectory was:
stage than participants in the Free Late condition. Confirming
this hypothesis, the participant-wise variance of chosen an-                                       r = −r
gles during the free exploration stage was significantly larger
for the Free Early condition than for the Free Late condi-            The state weighting was the same for both groups:
tion (t(59) = 2.62, p < 0.01). As such, participants indeed                        
                                                                                     1 0
                                                                                                     
                                                                                                       0 0
                                                                                                                        
                                                                                                                           10  0
                                                                                                                                 
seemed to strategically adapt their exploration behavior to the            W1:2 =           W3:21 =             W22:40 =
                                                                                     0 0               0 0                  0  0
underlying cost function.
   While we expected participants in the Free Late condition
to explore more in the initial stage of medium difficulty than
                                                                      Materials
those in the Free Early condition, a similar test to the one
above did not confirm this (t(59) = 0.63, p > 0.5). As such,          Participants were again told that they were taking part in a
there is no clear evidence that participants in the Free Late         sailing contest. Participants in the Up-Down condition were
condition used the medium difficulty period to explore in or-         then shown the control environment sketched out in Figure 2
der to perform better in the high-difficulty period.                  (right panel), whereas participants in the Down-Up condition
   Overall, participants in Experiment 1 showed hallmarks of          experienced the same control environment but flipped around
strategic exploration. However, they did not explore as vigor-        the center horizontal axis.
ously as approximate dual control predicted, often only doing         Participants
so during completely free exploration periods. As soon as ex-
                                                                      Forty-six participants were recruited via Amazon Mechanical
ploration is somewhat costly, participants seem to shift focus
                                                                      Turk and received $1 and a bonus of up to $1. 16 participants
back to normal (perhaps certainty-equivalence based) control,
                                                                      were female and the mean age was 34.32±11.17.
thereby more conservatively trading off between exploration
and exploitation.                                                     Results
                                                                      Figure 4 shows participants’ boat position by group. Again,
       Experiment 2: Directional exploration                          participants seemed to be able to learn how to steer the boat
The second experiment was designed to distinguish between             towards its targets in both groups. As before, we performed
agnostic and strategic exploration, involving the explicit ex-        a linear regression of the weights, trials and condition onto
ploration of directional actions. The design was again based          participants’ score (see Table 2).
on ideas put forward by Klenske & Hennig (2015). In both                 The weights had again the largest effect on participants’
conditions, a free exploration phase was followed by a high           scores and participants’ scores improved over time. There
difficulty period, in which controllers either had to move the        was no significant difference between the scores of the two
boat first up then down again (Up-Down condition) or first            conditions.
down and then up again (Down-Up condition).                              Strategic exploration is visible in the Up-Down condition
   If exploration is indeed strategic rather than agnostic and        as participants’ mean position goes up and then down again,
simply based on an exploration bonus, then participants in the        thereby showing clear signs of practicing the route to come.
Up-Down condition should focus exploration in the free ex-            This can also be found by testing the difference between con-
ploration phase on first learning to travel precise increments        dition’s average position during times of free exploration,
                                                                  1051

                                            Down−Up                                                                           Discussion and Conclusion
                            Free                                                                               Scenarios in which we have to explore to effectively exploit
              Med                                             High
                         exploration                                                                           dynamical systems are ubiquitous in daily life. We introduced
                                                                                                               a novel control task and assessed to what extent people’s ex-
                                                                                                               ploration can be seen as a strategic, opportunistic, and goal-
                                                                                                               directed behavior.
              ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●                                           ● ●         We found that participants displayed hallmarks of strate-
                                                                                                               gic exploration, exploring differently depending on the cost
                                                              ● ● ●                           ● ● ●
                                                                                                               function and, in some cases, practicing part trajectories which
                                                                      ● ● ●           ● ● ●                    would become important later on. However, strategic explo-
                                                                              ● ● ●                            ration seemed more conservative than that of an idealized ap-
                                                                                                               proximate dual control strategy. During periods of medium
   Position
                                                                                                               cost, participants seemed reluctant to explore in order to ben-
                                            Up−Down                                                            efit their performance during a following high-cost period in
              Med
                            Free
                                                              High
                                                                                                               Experiment 1. For controllers who learn and choose actions
                         exploration                                          ● ● ●
                                                                                                               more noisily than statistical algorithms, perhaps the future
                                                                      ● ● ●           ● ● ●                    benefits of this costly exploration did not outweigh the imme-
                                                              ● ● ●                           ● ● ●
                                                                                                               diate costs. Participants also did not always play out strate-
                                                                                                               gies of future importance during free exploration trials as in-
              ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●                                           ● ●      dicated by Experiment 2. As participants in the Up-Down
                                                                                                               condition could easily follow the underlying upward-current,
                                                                                                               participants in the Down-Up condition had to go against the
                                                                                                               current in order to explore strategically. Therefore, the dif-
                                                                                                               ference in exploration behavior could imply that, for humans,
                                                                                                               serendipity still plays a part in discovery of effective explo-
              1
              2
              3
              4
              5
              6
              7
              8
              9
              10
              11
              12
              13
              14
              15
              16
              17
              18
              19
              20
              21
              22
              23
              24
              25
              26
              27
              28
              29
              30
              31
              32
              33
              34
              35
              36
              37
              38
              39
              40
                                                                                                               ration strategies.
                                                 Trial                                                            As strategic exploration requires considerable planning,
Figure 4: Boat positions by condition for Experiment 2. See legend                                             even when dual control is approximate, it is likely to require
of Figure 3 for further details.                                                                               considerable mental effort. Future research could look into
                                                                                                               possible heuristics which approximate strategic exploration
    Table 2: Regression estimates for Experiment 2. r2 = 0.45.                                                 whilst further reducing computational costs.
                                Estimate          Std. Err.             t-value               Pr(>|t|)                                   References
                                                                                                               Duff, M. O. (2002). Optimal Learning: Computational procedures
     Intercept                       97.8             2.72                 35.9                 0.000                 for Bayes-adaptive Markov decision processes. Ph.D. thesis,
    Condition                        0.23             1.75                 0.13                  0.89                 University of Massachusetts Amherst.
 Med. cost: 1                       -11.3             1.69                -6.56                 0.000          Feldbaum, A. (1960). Dual control theory. Avtomatika i Tele-
                                                                                                                      mekhanika, 21(9), 1240–1249.
 High cost: 10                      -26.6             1.37                -19.4                 0.000          Klenske, E. D., & Hennig, P. (2015). Dual control for ap-
         Trial                       0.16             0.06                 2.41                  0.01                 proximate bayesian reinforcement learning. arXiv preprint
                                                                                                                      arXiv:1510.03591.
                                                                                                               Poupart, P. (2010). Bayesian reinforcement learning. In Encyclope-
which was significantly higher for the Up-Down condition                                                              dia of Machine Learning, (pp. 90–93). Springer.
(t(44) = 3.21, p < 0.01). Strategic exploration was not as                                                     Rescorla, R. A., Wagner, A. R., et al. (1972). A theory of pavlovian
                                                                                                                      conditioning: Variations in the effectiveness of reinforcement
pronounced in the Down-Up condition, as the mean position                                                             and nonreinforcement. Classical conditioning II: Current re-
seems closer to a straight line than the later target trajectory.                                                     search and theory, 2, 64–99.
Since the prevailing current would nudge any passive partic-                                                   Schulz, E., Konstantinidis, E., & Speekenbrink, M. (2016). Putting
                                                                                                                      bandits into context: How function learning supports decision
ipants who aimed straight ahead upward, a bias toward the                                                             making. bioRxiv, (p. 081091).
upper half is to be expected in both conditions. There is no                                                   Srinivas, N., Krause, A., Kakade, S. M., & Seeger, M. (2009). Gaus-
evidence that participants in the Down-Up condition explored                                                          sian process optimization in the bandit setting: No regret and
                                                                                                                      experimental design. arXiv preprint arXiv:0912.3995.
less, as there was no difference in the variance of chosen in-                                                 Thrun, S. B. (1992). Efficient exploration in reinforcement learning.
puts during the free exploration phase between the conditions                                                  Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A., & Cohen,
(t(44) = −0.32, p > 0.75). Participants in the Down-Up con-                                                           J. D. (2014). Humans use directed and random exploration to
                                                                                                                      solve the explore–exploit dilemma. Journal of Experimental
dition chose angles which were on average more downwards                                                              Psychology: General, 143(6), 2074.
during the first 10 trials (t(44) = −3.17, p < 0.01). Thus,                                                    Wu, C. M., Schulz, E., Speekenbrink, M., Nelson, J. D., & Meder,
perhaps participants in the Down-Up condition also explored                                                           B. (2017). Mapping the unknown: The spatially correlated
                                                                                                                      multi-armed bandit. In Proceedings of the 39th Annual Con-
strategically, but were less able to steer the boat in a clear and                                                    ference of the Cognitive Science Society.
consistent “practice run” of the desired future route.
                                                                                                            1052

