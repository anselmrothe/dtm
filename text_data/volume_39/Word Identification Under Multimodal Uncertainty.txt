                              Word Identification Under Multimodal Uncertainty
                               Abdellah Fourtassi                                           Michael C. Frank
                               afourtas@stanford.edu                                       mcfrank@stanford.edu
                             Department of Psychology                                    Department of Psychology
                                 Stanford University                                        Stanford University
                                Abstract                                    they weight each modality according to its relative reliability,
                                                                            or do they over-rely on a particular modality?
    Identifying the visual referent of a spoken word – that a partic-
    ular insect is referred to by the word “bee” – requires both the           In this paper, we propose a probabilistic framework where
    ability to process and integrate multimodal input and the abil-         such reasoning can be expressed precisely. We characterize
    ity to reason under uncertainty. How do these tasks interact            uncertainty in each modality with a probability distribution,
    with one another? We introduce a task that allows us to ex-
    amine how adults identify words under joint uncertainty in the          and we predict ideal responses by combining these probabil-
    auditory and visual modalities. We propose an ideal observer            ities in a optimal way. Our work can be seen as an extension
    model of the task which provides an optimal baseline. Model             to previous Bayesian models of phoneme identification (e.g.,
    predictions are tested in two experiments where word recogni-
    tion is made under two kinds of uncertainty: category ambigu-           Feldman, Griffiths, & Morgan, 2009), where, instead of a uni-
    ity and distorting noise. In both cases, the ideal observer model       modal input, we model a bimodal one. A few studies have
    explains much of the variance in human judgments. But when              explored some aspects of audio-visual processing in a prob-
    one modality had noise added to it, human perceivers system-
    atically preferred the unperturbed modality to a greater extent         abilistic framework (e.g., Bejjanki, Clayards, Knill, & Aslin,
    than the ideal observer model did.                                      2011). In these studies, the researchers focused on the spe-
    Keywords: Language; audio-visual processing; word learn-                cific case of phoneme recognition from speech and lip move-
    ing; speech perception; computational modeling.                         ment, however, where information is tightly correlated across
                                                                            modalities.
    Language uses symbols expressed in one modality (e.g.,                     In the present work, we study, rather, the case of arbitrary
the auditory modality, in the case of speech) to communicate                mapping between sounds and visual objects. We test partici-
about the world, which we perceive through many different                   pants on their ability to process audio-visual stimuli and use
sensory modalities. Consider hearing someone yell “bee!” at                 them to recognize the underlying word. More precisely we
a picnic, as a honeybee buzzes around the food. Determining                 study the case where both the word’s form and the word’s ref-
reference involves processing the auditory information and                  erent are ambiguous, and we examine the extent to which hu-
linking it with other perceptual signals (e.g., the visual image            mans conform to, or deviate from the predictions of the ideal
of the bee, the sound of its wings, the sensation of the bee                observer model. Moreover, some previous studies on audio-
flying by your arm).                                                        visual processing documented cases of modality preference,
    This multimodal integration task takes place in a noisy                 when people rely predominantly on the visual modality (e.g.,
world. On the auditory side, individual acoustic word tokens                Colavita, 1974) or the auditory modality (e.g., Sloutsky &
are almost always ambiguous with respect to the particular se-              Napolitano, 2003). Thus, we will explore if participants in
quence of phonemes they represent, which is due to the inher-               our task show evidence of a modality preference.
ent variability of how a phonetic category is realized acousti-                The paper is organized as follows. First, we introduce our
cally (e.g., Hillenbrand, Getty, Clark, & Wheeler, 1995). And               audio-visual recognition task. We next present the ideal ob-
some tokens may be distorted additionally by mispronuncia-                  server model. Then we present two behavioral experiments
tion or ambient noise. Perhaps the speaker was yelling “pea”                where we test word recognition under audio-visual uncer-
and not “bee.” Similarly, a sensory impression may not be                   tainty. In Experiment 1, audio-visual tokens are ambiguous
enough to make a definitive identification of a visual cate-                with respect to their category membership. In Experiment 2,
gory.1 Perhaps the insect was a beetle or a fly instead.                    we intervene by adding noise to one modality. In both exper-
    Thus, establishing reference requires reasoning under a                 iments participants show qualitative patterns of optimal be-
great deal of uncertainty in both modalities. The goal of this              havior. Moreover, while participants show no modality pref-
work is to characterize such reasoning. Imagine, for example,               erence in Experiment 1, in Experiment 2 they over-rely on
that someone is uncertain whether they heard “pea” or “bee”,                visual input when the auditory modality is noisy.
does it make them rely more on the visual modality (e.g., the
object being pointed at)? Vice versa, if they are not sure if                    The Audio-Visual Word Recognition Task
they saw a bee or a fly, does that make them rely more on the               We introduce a new task that tests audio-visual word recog-
auditory modality (i.e., the label)? More importantly, when                 nition. We use two visual categories (cat and dog) and two
input in both modalities is uncertain to varying degrees, do                auditory categories (/b/ and /d/ embedded in the minimal pair
     1 In the general case, language can of course be visual as well as     /aba/-/ada/). For each participant, an arbitrary pairing is set
auditory, and object identification can be done through many modal-         between the auditory and the visual categories, leading to two
ities. For simplicity, we focus on audio-visual matching here.              audio-visual word categories (e.g., dog-/aba/, cat-/ada/).
                                                                        373

                  Figure 1: Overview of the task
   In each trial, participants are presented with an audio-
visual target (the prototype of the target category), immedi-
ately followed by an audio-visual test stimulus (Figure 1).
The test stimulus may differ from the target in both the au-
ditory and the visual components. After these two presenta-
tions, participants press “same” or “different.”
   This task is similar to the task introduced by Sloutsky and
Napolitano (2003) and used in subsequent research to probe
audio-visual encoding. However, unlike this previous line
                                                                        Figure 2: Illustration of our model using simulated data. A
of research, here participants are not asked whether the two
                                                                        word category is defined as the joint bivariate distribution of
audio-visual presentations are identical. Instead, the task is
                                                                        an auditory category (horizontal, bottom panel) and a visual
category-based: They are asked to press “same” if they think
                                                                        semantic category (vertical, left panel). Upon the presenta-
the second item (the test) belonged to the same category as the
                                                                        tion of a word token w, participants guess whether it is sam-
first (target) (e.g., dog-/aba/), even if there is a slight differ-
                                                                        pled from the word category W1 or from W2 . Decision thresh-
ence in the word, in the object, or in both. They are instructed
                                                                        old is where the guessing probability is 0.5.
to press “different” only if they think that the second stimulus
was an instance of the other word category (cat-/ada/).
   The task also includes trials where pictures were hid-                  We have two word categories: dog-/aba/ (W1 ) and cat-/ada/
den (audio-only) or where sounds were muted (visual-only).              (W2 ). Participants can be understood as choosing one of these
These unimodal trials provide us with participants’ catego-             two word categories (Figure 2). For an ideal observer, the
rization functions for the auditory and visual categories and           probability of choosing category 2 when presented with an
are used as inputs to the ideal observer model, described be-           audio-visual instance w = (a, v) is the posterior probability
low.                                                                    of this category:
                   Ideal Observer Model                                                                   p(w|W2 )p(W2 )
                                                                                 p(W2 |w) =                                                 (1)
The basis of our ideal observer model is that individual cate-                                  p(w|W2 )p(W2 ) + p(w|W1 )p(W1 )
gorization functions from each modality should be combined                 We make the assumption that, given a particular word cat-
optimally. In each modality, we have two categories: /ada/              egory, the auditory and visual tokens are independent:
(A = 1) and /aba/ (A = 2) in the auditory dimension, and cat
(V = 1) and dog (V = 2) in the visual dimension. We assume,
                                                                                     p(w|W ) = p(a, v|W ) = p(a|W )p(v|W )                  (2)
for the sake of simplicity, that the probability of membership
in each category is normally distributed:                                  Under this assumption, the posterior probability reduces to:
                        p(a|A) ∼ N(µA , σ2A )                                                                     1
                                                                                  p(W2 |w) =                                                (3)
                                                                                                1 + (1 + ε) exp(β0 + βa a + βv v)
                       p(v|V ) ∼ N(µV , σV2 )
                                                                                    µA1 −µA2                        µ2 −µ2     µ2 2 −µV2 1
   In the bimodal condition, participants see word tokens with          with βa =       σ2A
                                                                                             , βv = µV 1σ−µ
                                                                                                         2
                                                                                                           V2
                                                                                                              , β0 = A22σ2 A1 + V 2σ 2     and
                                                                                                         V               A           V
audio-visual input, and have to make a categorization deci-                       p(W1 )
sion. We define word tokens as vectors in the audio-visual              1+ε =     p(W2 ) is the proportion of the prior probabilities. If
space, w = (a, v). A word category W is defined as the joint            the identity of word categories is randomized, and if W1 is the
distribution of auditory and visual categories. It can be char-         target, then ε measures a response bias to “same” if ε > 0, and
acterized with a bivariate normal distribution:                         a response bias to “different” if ε < 0.
                                                                           In sum, the posterior 3 provides the ideal observer’s pre-
                      p(w|W ) ∼ N(MW , ΣW )                             dictions for how probabilities that characterize uncertainty in
                                                                    374

each modality can be combined to make categorical decision                  A                 1.00                                                     1.00
about bimodal input.
                                                                            Prob. different                                          Prob. different
                                                                                              0.75                                                     0.75
                                                                                              0.50                                                     0.50
                       Experiment 1
                                                                                              0.25                                                     0.25
In Experiment 1, we test the predictions of the model in the
                                                                                              0.00                                                     0.00
case where uncertainty is due to similar auditory categories,                                        0     1        2     3    4                                  0    1         2           3   4
                                                                                                           Auditory distance
and similar visual categories. Crucially, the similarity is such                                                                                                           Visual distance
                                                                            B
that the distributions overlap. To simulate such uncertainty                                  1.00                                                     1.00
in a controlled fashion, we use a continuum along the second
                                                                            Prob. different                                          Prob. different
                                                                                              0.75                                                     0.75
formant (F2) linking the words /aba/ and /ada/, and we use a                                  0.50                                                     0.50
morph that links a dog prototype and a cat prototype.                                         0.25                                                     0.25
Methods                                                                                       0.00                                                     0.00
                                                                                                     0     1        2     3    4                                  0    1         2           3   4
Participants We recruited a planned sample of 100 partic-                                                  Auditory distance                                               Visual distance
ipants, recruited from Amazon Mechanical Turk. Only par-
ticipants with US IP addresses and a task approval rate above             Figure 3: Average human responses in the auditory-only con-
85% were allowed to participate. They were paid at an hourly              dition (left), and visual-only condition (right). A) represents
rate of $6/hour. Data were excluded if participants completed             data from Experiment 1, and B) data from Experiment 2. Er-
the task more than once (2 participants). Moreover, as spec-              ror bars are 95% confidence intervals. Solid lines represent
ified in the preregistration (https://osf.io/h7mzp/), par-                unimodal logistic fits.
ticipants were excluded if they had less than 50% accurate
responses on the unambiguous training trials (6), and if they
reported having experienced a technical problem of any sort               als, 4 each from the bimodal, audio-only, and visual-only con-
during the online experiment (14). The final sample consisted             ditions). After completing training, we instructed participants
of 76 participants.                                                       on the structure of the task and encouraged them to base their
                                                                          answers on both the sounds and the pictures (in the bimodal
Stimuli For auditory stimuli, we used the continuum intro-                condition). There were a total of 25 possible combinations
duced in Vroomen, van Linden, Keetels, de Gelder, and Ber-                in the bimodal condition, and 5 in each of the unimodal con-
telson (2004), a 9-point /aba/–/ada/ speech continuum created             ditions. Each participant saw each possible trial twice, for a
by varying the frequency of the second (F2) formant in equal              total of 70 trials/participant. Trials were blocked by condition
steps. We selected 5 equally spaced points from the original              and blocks were presented in random order.
continuum by keeping the end-points (prototypes) 1 and 9,
as well as points 3, 5, and 7 along the continuum. For visual             Results
stimuli, we used a morph continuum introduced in Freedman,                Unimodal conditions this is the case where the pictures
Riesenhuber, Poggio, , and Miller (2001). From the original               were hidden, or where the sounds were muted. Average cate-
14 points, we selected 5 points as follows: we kept the item              gorization judgments and fits are shown in Figure (3, A). The
that seemed most ambiguous (point 8), the 2 preceding points              categorization function of the auditory condition was steeper
(i.e., 7 and 6) and the 2 following points (i.e., 9 and 10). The          than that of the visual condition. The fit was done using the
6 and 10 points along the morph were quite distinguishable,               Nonlinear Least Squares (NLS) R package, as follows. For an
and we took them to be our prototypes.                                    ideal recognizer, the probability of choosing category 2 (that
Design and Procedure We told participants that an alien                   is, to answer “different”) when presented with an audio in-
was naming two objects: a dog, called /aba/ in the alien lan-             stance a, is the posterior probability of this category p(A2 |a).
guage, and a cat, called /ada/. In each trial, we presented the           If we assume that both categories have equal variances, the
first object (the target) on the left side of the screen simulta-         posterior probability reduces to:
neously with the corresponding sound. The target was always
                                                                                                                                        1
the same (e.g., dog-/aba/). The second sound-object pair (the                                            p(A2 |a) =                                                                                  (4)
test) followed on the other side of the screen after 500ms and                                                           1 + (1 + εA ) exp(βa0 + βa a)
varied in its category membership. For both the target and the                                           µA1 −µA2                  µ2A −µ2A
                                                                                                                                     2                        1
test, visual stimuli were present for the duration of the sound           with βa =                        σ2A
                                                                                                                    and βa0 =          2σ2A
                                                                                                                                                                  . εA is the response bias
clip (∼800ms). We instructed participants to press “S” for                in the auditory-only trials.
same if they thought the alien was naming another dog-/aba/,                 For this model (as well all other models), we fixed the val-
and “D” for different if they thought the alien was naming                ues of the means to be the end-points of the corresponding
a cat-/ada/. For each participant, we randomized the sound-               continuum: µA1 = 0 and µA2 = 4 (and similarly µV 1 = 0, and
object mapping as well as the identity of the target.                     µV 2 = 4). To determine the values of the bias and the variance,
   The first part of the experiment trained participants using            we fit a model for each modality, collapsed across partici-
only the prototype pictures and the prototype sounds (12 tri-             pants. For the auditory modality, we obtained εA = −0.20 and
                                                                    375

A
                                  Human data                        Ideal observer                                   Bimodal fit
                      1.00                               1.00                                        1.00
    Prob. different
                      0.75                               0.75                                        0.75
                                                                                                                                            Visual Dist
                                                                                                                                              0
                      0.50                               0.50                                        0.50                                     1
                                                                                                                                              2
                      0.25                               0.25                                        0.25                                     3
                                                                                                                                              4
                      0.00                               0.00                                        0.00
                             0    1      2     3     4          0    1      2         3      4              0    1        2         3   4
                                 Auditory distance                  Auditory distance                           Auditory distance
B
                                  Human data                        Ideal observer                                   Bimodal fit
                      1.00                               1.00                                        1.00
    Prob. different
                      0.75                               0.75                                        0.75
                                                                                                                                            Visual Dist
                                                                                                                                              0
                      0.50                               0.50                                        0.50                                     1
                                                                                                                                              2
                      0.25                               0.25                                        0.25                                     3
                                                                                                                                              4
                      0.00                               0.00                                        0.00
                             0    1      2     3     4          0    1      2         3      4              0    1        2         3   4
                                 Auditory distance                  Auditory distance                           Auditory distance
Figure 4: Proportion of “different” judgments as a function of auditory distance. Solid lines represent average human responses
(left), predictions of the ideal observer (middle), and the bimodal fit (right). Dashed lines represent average human responses in
the unimodal conditions. Colors represent values in the visual continuum. A) represents data from Experiment 1, and B) data
from Experiment 2.
σ2A = 2.04. For the visual modality, we obtained εV = −0.11                           visual space along which the posterior (Eq. 3) is equal to 0.5.
and σV2 = 3.34.                                                                       The decision threshold takes the following form:
Bimodal condition We fit a model to human responses in
                                                                                                                       σV2
the bimodal condition, collapsed across participants, finding                                                   v=−        a + v0                   (5)
ε = −0.32, σ2Ab = 5.00 and σV2 b = 7.27. The fit explained                                                             σ2A
94% of total variance.
                                                                                         If the slope derived from the bimodal fit is greater than the
Ideal observer model We derived the predictions of the                                slope of the ideal observer, this finding would suggest a gen-
ideal observer model by using the values of the variances                             eral preference for the auditory modality (similarly, a smaller
derived from the unimodal conditions, and the response bias                           slope would suggest a preference for the visual modality).
derived from the bimodal condition, and by substituting these                         The limit cases are when there is exclusive reliance on the
values into the expression of the posterior in Eq. 3. Figure                          auditory cue (a vertical line), and where there is exclusive re-
(4, A) shows participants’ responses in the bimodal condition                         liance on the visual (a horizontal line). Figure 5 (top left)
(left), as well as the prediction of the ideal observer (middle),                     shows the decision threshold in the audio-visual space with a
and the bimodal fit models (right).                                                   constant intercept; the fit to human data (solid black line) was
                                                                                      very close to the ideal observer threshold (red line). Non-
Response bias We found negative values in all response                                parametric resampling of the data showed no evidence of a
bias terms, which suggests a general bias toward answering                            deviation from the slope of the ideal observer (5, bottom left).
“different.” This bias is probably due to the categorical nature
of our same-different task: when two items are ambiguous but                          Discussion
perceptually different, this could cause a slight preference for
“different” over “same”.                                                              Qualitatively, participants’ judgments were similar to the pre-
                                                                                      dictions of the ideal observer model (remember that the lat-
Modality preferences We next analyzed whether there was                               ter was obtained by optimally combining fits to the uni-
a preference for one or the other modality when making de-                            modal data). Consider, for example, the contrast between the
cisions in the bimodal condition, beyond that explained by                            auditory-only case (dashed black line in Figure 4, top left) and
the variance in categories implied by the unimodal responses.                         the bimodal case (solid colored lines). Higher certainty in the
This preference would manifest as a deviation from the de-                            visual modality generally influenced responses, with greater
cision threshold predicted by the ideal observer model. The                           visual distance leading to more “different” ratings and less
decision threshold is defined as the set of values in the audio-                      visual distance leading to more “same” ratings. Similar ob-
                                                                                376

servations can be made about the contrast between the visual-             Methods
only case and the bimodal case.
                                                                          Participants A planned sample of 100 participants was re-
   Overall, we found that the ideal observer model explained              cruited online through Amazon Mechanical Turk. We used
much of the variance in judgments (r2 = 0.89). But although               the same exclusion criteria as in the previous experiment; the
we see a qualitative resemblance between human data and                   final sample consisted of 93 participants.
the model, there were quantitative differences. For example,
model predictions were more influenced by the visual modal-               Stimuli and Procedure We used the same visual stimuli as
ity at the auditory midpoint (the point of highest uncertainty)           in Experiment 1. We also used the same auditory stimuli, but
than human judgements, and were more compressed at the                    we convolved each item with Brown noise of amplitude 1 us-
endpoints (the points of lowest uncertainty).                             ing the audio editor Audacity (2.1.2). The procedure was ex-
   Formally, there was an increase in the value of the vari-              actly the same as in the previous experiment, except that test
ance associated with each modality. Whereas the ideal ob-                 stimuli were presented with the new noisy auditory stimuli.
server model predicted the weights to be proportional to 1/σ2A
                                                                          Results
and 1/σV2 , for the auditory and the visual modalities, respec-
tively (see expression 3), the fit to human data suggested that           Unimodal conditions We fit a model for each modality,
the real weights were proportional to 1/σ2Ab and 1/σV2 b . Our            collapsed across participants. For the auditory modality, our
analysis of modality preference showed that the relative val-             parameter estimates were εA = −0.18 and σ2A + σ2N = 4.70.
ues of these variances were almost the same (Figure 5, left).             For the visual modality, we found εV = −0.24 and σV2 = 3.93.
Thus, 1) the bimodal presentation introduced a certain level              Figure 3 (bottom) shows responses in the unimodal condi-
of randomness in the participants’ responses, and 2) this in-             tions as well as the unimodal fits. In contrast to Experiment 1,
creased randomness did not affect the relative weighting of               auditory responses were flatter (showing more uncertainty).
both modalities, i.e., participants were weighting modalities
                                                                          Bimodal condition We fit a model to human responses in
according to their relative reliability. The latter explains the
                                                                          the bimodal condition, collapsed across participants. We es-
qualitative resemblance between the predictions of the ideal
                                                                          timated ε = −0.38, σV2 b = 5.21, and σ2Ab + σ2Nb = 9.84. The
observer and human data, and the former explains the quanti-
                                                                          fit explained 97% of total variance.
tative discrepancy.
   In sum, we found that participants followed the ideal ob-              ideal observer model We generated the predictions of the
server model in that they weighted modalities according to                ideal observer model by using the values of the variances de-
their reliabilities. In real life, however, tokens can undergo            rived from the unimodal conditions, and the response bias de-
distortions due to noisy factors in the environment. In Exper-            rived from the bimodal condition, and by substituting these
iment 2, we explore this additional level of uncertainty.                 values into the expression of the posterior in Eq. 3. Results
                                                                          are shown in Figure 4 (bottom).
                         Experiment 2                                     Modality preferences Participants’ decision threshold sug-
Imagine that the speaker generates a target production t from             gested a preference for the visual modality (the non-noisy
an auditory category t|A ∼ N(µA , σ2A ). In Experiment 1, we              modality). Indeed non-parametric resampling of the data
assumed that the observer could directly retrieve this produc-            showed a decrease in the value of the slope (5, right).
tion token. But if the observer is in a noisy environment, they
                                                                          Discussion
do not hear exactly this produced target, but the target per-
turbed by noise, which we assume, following Feldman et al.                We found, similar to Experiment 1, that participants gener-
(2009), that it is normally distributed: a|t ∼ N(t, σ2N ). When           ally showed qualitative patterns similar to the ideal observer
we integrate over t, we get:                                              model (r2 = .91). But we also found a similar discrepancy
                                                                          at the quantitative level. The ideal observer model predicted
                                                                          the modality weights to be proportional to 1/(σ2A + σ2N ) and
                       a|A ∼ N(µA , σ2A + σ2N )                  (6)
                                                                          1/σV2 , for the auditory and the visual modalities, respectively.
   In this experiment, we explored the effect of this added               The fit to human data suggested that the empirical weights
noise2 on performance in our task. We tested a case where                 were proportional to 1/σ2Ab and 1/σV2 b . An interesting differ-
one modality was ambiguous and noisy (auditory), and where                ence with Experiment 1, however, was that participants had
the other modality was ambiguous but not noisy (visual).                  a clear preference for the non-noisy modality, as the values
We were interested to know if participants would treat this               of the relative variances were different (Figure 5, right). This
new source of uncertainty as predicted by the ideal observer              preference affected the relative weighting, where, contrary to
model, and whether noise in one modality would lead to some               Experiment 1, the visual modality had greater weight than
systematic preference for the non-noisy modality.                         what could be expected from its relative reliability alone.
                                                                              It is important to understand that this preference was not
    2 Note that we are considering environmental noise, which is dif-     the mere consequence of the added noise increasing the vari-
ferent from the noise inherent to perception.                             ance of the auditory modality, since this increase was already
                                                                      377

                                                                    formational reliability. Only when we intervened by adding
                                                                    noise to one modality in Experiment 2, did participants show
                                                                    a systematic preference for the non-noisy modality. One pos-
                                                                    sible explanation for this preference could be that people do
                                                                    not combine cross-modal uncertainties of a similar kind (e.g.,
                                                                    ambiguity in both modalities) in the same way they would
                                                                    combine uncertainties of different kinds (e.g., ambiguity in
                                                                    one modality and noise in the other). For instance, it could be
                                                                    that the latter, but not the former, cause the over-reliance on a
                                                                    particular modality.
                                                                       Overall, in both Experiments, the majority of the variance
                                                                    could be explained by an ideal observer that combined multi-
                                                                    modal information optimally. In the light of this main result,
                                                                    we can revisit some previous findings in the literature. For
                                                                    instance, Sloutsky and Napolitano (2003) reported a domi-
                                                                    nance for the auditory modality in children. This dominance
                                                                    disappears or reverses in adults. Could this difference be
                                                                    driven by changes across development in the level of percep-
Figure 5: Top: decision thresholds in the audio-visual space.       tual noise affecting the intrinsic relative reliability of modal-
Red dotted line is the prediction of the ideal observer. Blue       ities (by analogy to Experiment 2)? More work is needed
dotted lines are cases where modality preference is twice as        to carefully examine this (and other) speculations, and more
strong as the ideal observer. Solid line is the threshold de-       generally, to determine the extent to which the optimal com-
rived from human data. Bottom: comparison of the threshold          bination account helps us better understand the mechanisms
slope between the ideal observer and the fit to human data.         of word processing and learning.
Error bars are 95% confidence intervals computed via non-
parametric bootstrap.                                                                    Acknowledgements
                                                                    This work was supported by a post-doctoral grant from the
                                                                    Fyssen Foundation.
accounted for in the ideal observer model. The preference
was, rather, a form of over-reliance on the visual modality.                                   References
                   General Discussion                               Bejjanki, V. R., Clayards, M., Knill, D. C., & Aslin, R. N.
                                                                       (2011). Cue integration in categorical tasks: Insights from
Understanding language requires both the ability to integrate          audio-visual speech perception. PLoS ONE, 6.
multimodal input, and the ability to deal with uncertainty. In      Colavita, F. B. (1974). Human sensory dominance. Percep-
this work, we explored a case where both abilities were at             tion & Psychophysics, 16.
play. We studied the case of identifying a word when both its       Feldman, N., Griffiths, T., & Morgan, J. (2009). The influ-
form (auditory) and its referent (visual) were ambiguous with          ence of categories on perception: Explaining the perceptual
respect to their category membership (Experiment 1), and               magnet effect as optimal statistical inference. Psychologi-
when the form was perturbed with additional noise (Experi-             cal review, 116(4), 752–782.
ment 2). We introduced a model that instantiated an ideal ob-       Freedman, D., Riesenhuber, M., Poggio, T., , & Miller, E.
server, predicting how information from each modality could            (2001). Categorical representation of visual stimuli in the
be combined in an optimal way. In both experiments, partici-           primate prefrontal cortex. Science, 291.
pants showed the qualitative patterns of the ideal observer.        Hillenbrand, J., Getty, L. A., Clark, M. J., & Wheeler, K.
   There were, however, quantitative differences. Audio-               (1995). Acoustic characteristics of american english vow-
visual presentation increased the level of randomness in the           els. Journal of the Acoustical Society of America, 97.
participants’ responses. One possible explanation is that this      Robinson, C. W., & Sloutsky, V. M. (2010). Development of
phenomenon was caused by the arbitrary nature of the form-             cross-modal processing. Wiley Interdisciplinary Reviews:
meaning mapping. Previous studies suggest that while re-               Cognitive Science, 1.
dundant multimodal information improves performance (e.g.,          Sloutsky, V. M., & Napolitano, A. (2003). Is a picture worth a
determining the frequency of a bouncing ball from visual and           thousand words? preference for auditory modality in young
auditory cues), arbitrary mappings generally tends to hinder           children. Child Development, 74.
performance (for review, see Robinson & Sloutsky, 2010).            Vroomen, J., van Linden, S., Keetels, M., de Gelder, B., &
   Interestingly, however, in Experiment 1 this increase in            Bertelson, P. (2004). Selective adaptation and recalibra-
randomness occurred at a similar rate for both the auditory            tion of auditory speech by lipread information: dissipation.
and the visual modality, and thus, it did not affect their rel-        Speech Communication, 44.
ative weighting. The latter was primarily determined by in-
                                                                378

