                              Enhancing metacognitive reinforcement learning
                                         using reward structures and feedback
                                               Paul M. Krueger1 (pmk@berkeley.edu)
                                              Falk Lieder1 (falk.lieder@berkeley.edu)
                                        Thomas L. Griffiths (tom griffiths@berkeley.edu)
                       Department of Psychology, University of California Berkeley, Berkeley, CA 94720 USA
                                                     1
                                                       These authors contributed equally.
                               Abstract                                     We start by introducing the theory of reinforcement learn-
                                                                         ing that our approach is based upon. The following two sec-
   How do we learn to think better, and what can we do to pro-
   mote such metacognitive learning? Here, we propose that cog-          tions apply this theory to model the problem of deciding how
   nitive growth proceeds through metacognitive reinforcement            to decide and the process by which people learn to do so.
   learning. We apply this theory to model how people learn how          We then use this theory to motivate a novel computational
   far to plan ahead and test its predictions about the speed of
   metacognitive learning in two experiments. In the first experi-       method for designing feedback structures that promote cog-
   ment, we find that our model can discern a reward structure that      nitive plasticity and experimentally test the predictions of our
   promotes metacognitive reinforcement learning from one that           theory. We close with a discussion of the implications of our
   hinders it. In the second experiment, we show that our model
   can be used to design a feedback mechanism that enhances              results for cognitive training.
   metacognitive reinforcement learning in an environment that
   hinders learning. Our results suggest that modeling metacog-                  Planning and reinforcement learning
   nitive learning is a promising step towards promoting cognitive
   growth.                                                               A sequential decision problem can be modeled as a Markov
   Keywords: Decision-Making; Planning; Metacognitive Rein-              decision process (MDP)
   forcement Learning; Cognitive Training
                                                                                               M = (S , A , T, γ, r, P0 ) ,              (1)
                           Introduction
One of the most remarkable aspects of the human mind is its              where S is the set of states, A is the set of actions, T (s, a, s0 )
ability to improve itself based on experience. Such learning             is the probability that the agent will transition from state s to
occurs in a range of domains, from simple stimulus-response              state s0 if it takes action a, 0 ≤ γ ≤ 1 is the discount factor
mappings, motor skills, and perceptual abilities, to problem             on future rewards, r(s, a, s0 ) is the reward generated by this
solving, cognitive control, and learning itself (C. S. Green &           transition, and P0 is the probability distribution of the initial
Bavelier, 2008; Bavelier, Green, Pouget, & Schrater, 2012).              state S0 (Sutton & Barto, 1998). A policy π : S 7→ A speci-
Demonstrations of cognitive and brain plasticity have in-                fies which action to take in each of the states. The expected
spired cognitive training programs. The success of cognitive             sum of discounted rewards that a policy π will generate in the
training has been mixed and the underlying learning mecha-               MDP M starting from a state s is known as its value function
nisms are not well understood (Owen et al., 2010; Anguera                                         "
                                                                                                    ∞
                                                                                                                                 #
et al., 2013; Morrison & Chein, 2011). Feedback is an im-
portant component of many effective cognitive training pro-
                                                                                      VMπ (s) = E   ∑ γt · r (St , π(St ), St+1 )  .     (2)
                                                                                                   t=0
grams, but it remains unclear what makes some feedback
structures more effective than others, and there is no prin-             The optimal policy π?M maximizes the expected sum of dis-
cipled method for designing optimal feedback structures.                 counted rewards, that is
   To address these problems, we model cognitive plasticity                                          "                               #
                                                                                                       ∞
as metacognitive reinforcement learning. This perspective al-                      π?M = arg max E
lows us to translate methods for accelerating reinforcement                                    π
                                                                                                       ∑ γt · r (St , π(St ), St+1 )   . (3)
                                                                                                      t=0
learning in robots (Ng, Harada, & Russell, 1999) into feed-
back structures for cognitive training in humans.                           Solving large planning problems is often intractable be-
   Here, we evaluate this approach in the domain of planning.            cause the number of possible action sequences grows expo-
As a first step, we developed a metacognitive reinforcement              nentially with the number of steps one plans ahead. When
learning model of how people learn how many steps to plan                the state space S is discrete and relatively small, dynamic
ahead in sequential decision problems, and we test its predic-           programming can be used to find optimal plans in polyno-
tions empirically. The results of our first experiment suggest           mial time (Littman, Dean, & Kaelbling, 1995). But the high-
that our model can discern which reward structures are more              dimensional, continuous state spaces people have to plan with
conducive to metacognitive learning. In our second experi-               in real life are too large for these methods. Instead, peo-
ment, we find that feedback structures designed based on our             ple seem to rely on approximate planning strategies (Huys
model can accelerate learning to plan.                                   et al., 2015) and often decide primarily based on immediate
                                                                    2469

and proximal outcomes while neglecting the long-term con-              the number of remaining moves, and the meta-level action
sequences of their actions (Myerson & Green, 1995). Despite            h ∈ H = {1, 2, 3, 4} is the planning horizon used to make a
its fallibility, looking only a few steps ahead can drastically        decision. The meta-level reward function rmeta integrates the
simplify the planning problem, and this may often be a ne-             cost of planning with the return of the resulting action:
cessity for bounded agents with imperfect knowledge of the                                                         h
environment (Jiang, Kulesza, Singh, & Lewis, 2015). Since                                                                          (k,hk )
                                                                                rmeta (mk , hk ) = −cost(hk ) + ∑ r(st , plant             ), (5)
cutting corners in the decision process is both necessary and                                                     t=1
problematic, good decision-making requires knowing when                             (k,h)
that is admissible and when it is not. Knowing how much to             where plant        is the t th action of the plan formed by looking
plan is therefore an important metacognitive skill to learn.           h steps ahead in the meta-level state mk . The meta-decision-
   Previous work suggests that this metacognitive skill can            maker receives this reward after the plan has been executed in
be learned through trial and error (Lieder & Griffiths, 2015).         its entirety. If the meta-decision-maker selects short planning
Learning through trial and error can be understood in terms of         horizons there can be multiple plan-act-reward-learn cycles
reinforcement learning (Sutton & Barto, 1998). While certain           within a single trial. The cost of planning cost(hk ) is deter-
reinforcement learning algorithms can, in principle, learn to          mined by the branching factor b of the decision tree according
solve arbitrarily complex problems, reinforcement learning             to
can also be very slow—especially when rewards are sparse                                        cost(hk ) = λ · bhk · hk ,                    (6)
and the optimal policy is far from the learner’s initial strategy.     where bhk is the number of plans, hk is the number of steps
A common approach to remedy this problem is to give the                per plan, and λ is the cost per planning step.*
algorithm pseudo-rewards for actions that do not achieve the
goal but lead in the right direction (Ng et al., 1999).While                   Metacognitive reinforcement learning
previous work has developed this idea to accelerate learning           Solving the problem of deciding how to decide optimally is
a direct mapping from states to actions, we will leverage it to        computationally intractable but the optimal solution can be
accelerate learning to plan.                                           approximated through learning (Russell & Wefald, 1991).
                                                                       We propose that people use reinforcement learning (Sutton
                   Deciding how to decide                              & Barto, 1998) to approximate the optimal solution to the
People can use many different decision strategies. This poses          meta-decision problem formulated in Equation 4.
the problem of deciding how to decide (Boureau, Sokol-
Hessner, & Daw, 2015). Previous research on meta-decision-             Model
making has focused on the arbitration between habits ver-              Our model of metacognitive reinforcement learning builds on
sus planning (Keramati, Dezfouli, & Piray, 2011; Dolan                 the semi-gradient SARSA algorithm (Sutton & Barto, 1998)
& Dayan, 2013). While this is an important meta-control                that was develop to approximately solve MDPs with large or
problem, it is only one part of the puzzle because people              continuous state spaces. Specifically, we assume that people
are equipped with more than one goal-directed decision-                learn a linear approximation to the meta-level Q-function
mechanism. Hence, when the model-based system is in
                                                                                                             7
charge, it has to be determined how many steps it should plan                          Qmeta (mk , hk ) ≈
ahead. Ideally, the chosen planning horizon should achieve
                                                                                                            ∑ w j · f j (mk , hk ),           (7)
                                                                                                            j=1
the optimal tradeoff between expected decision quality ver-
sus decision time (Vul, Goodman, Griffiths, & Tenenbaum,               whose features f comprise one indicator variable for each pos-
2014) and mental effort (Shenhav et al., 2017).                        sible planning horizon h ( f1 = 1(h = 1), · · · , f4 = 1(h = 4)),
   Here, we make the simplifying assumption that people al-            one indicator variable for whether or not the agent planned
ways choose the action that maximizes their sum of expected            all l steps until the end of the task ( f5 = 1(h = l)), the num-
rewards over the next h steps, for some value of h that differs        ber of steps that were left unplanned ( f6 = max{0, l − h}),
across decisions. A planning horizon of h = 1 entails looking          and the number of steps the agent planned too far ( f7 =
only at the immediate outcome of each action (myopic one-              max{0, h − l}). The semi-gradient SARSA algorithm learns
step planning) whereas a planning horizon larger than one              the weights of these features by gradient descent. To bring
entails solving a sequential decision problem to form a multi-         it closer to human performance, our model replaces its gra-
step plan. Under this assumption, the meta-decision problem            dient descent updates by Bayesian learning. Concretely, the
is to select a planning horizon h from a set H = {1, 2, · · · , },     weights w are learned by Bayesian linear regression of the
execute the plan, select a new planning horizon, and so on.            bootstrap estimate Q̂(mk , hk ) of the meta-level value function
More formally, this problem can be formalized as a meta-               onto the features f. The bootstrap estimator
level MDP (Hay, Russell, Tolpin, & Shimony, 2012). In our                          Q̂(mk , hk ) = rmeta (mk , hk ) + hµt , f(m0 , h0 )i       (8)
task, the meta-level MDP is
                                                                           * This equation assumes a constant branching factor and an upper
                  Mmeta = (Smeta , H , Tmeta , rmeta ) ,       (4)     bound on the complexity of planning. People’s planning time likely
                                                                       increases less than exponentially fast with the planning horizon but
where the meta-level state m ∈ Smeta = {0, 1, 2, 3, 4} encodes         our approximation may be sufficient for small problems.
                                                                   2470

is the sum of the immediate meta-level reward and the pre-
dicted value of the next meta-level state m0 . The predicted
value of m0 is the scalar product of the the posterior mean µt
of the weights w given the observations from the first t ac-
tions (where t = ∑kn=1 hn ) and the features f(m0 , c0 ) of m0 and
the planning horizon h0 that will be selected in that state.
   We assume that the prior on the feature weights reflects that
it is beneficial to plan until the end (P( f5 ) = N (µ = 1, σ =
0.1)), although planning is costly (P( f1 ) = P( f2 ) = P( f3 ) =
P( f4 ) = N (µ = −1, σ = 0.1)), and that planning too much is
more costly than planning too little (P( f7 ) = N (µ = −1, σ =
0.1) and P( f6 ) = N (µ = 0, σ = 0.1)).
   Given the posterior on the feature weights w, the planning
horizon h is selected by Thompson sampling. Specifically,
to make the kth meta-decision, a weight vector w̃ is sampled
from the posterior distribution of the weights given the series
of meta-level states, selected planning horizons, and resulting
value estimates experienced so far. That is,
                                    w̃k ∼ P(w|Ek ),                     (9)
                                                                                   Figure 1: Screenshot of a problem from Experiment 1.
where the set Ek = {e1 , · · · , ek } contains the meta-decision-
maker’s experience from the first k meta-decisions; to be
precise, each meta-level                experience e j ∈ Ek is a tuple        $2.00. Participants played a series of flight planning games.
  m j , h j , Q̂(m j , c j ; µ j ) containing a meta-level state, the com-     The environment consisted of six different cities, each con-
putation selected in it, and the bootstrap estimates of its Q-                 nected to two other cities (Figure 1). Participants began each
value. The sampled weight vector w̃ is then used to predict                    trial at a given city, and were tasked with planning a specified
the Q-values of each possible planning horizon h ∈ H ac-                       number of flights. Each flight was associated with a known
cording to Equation 7. Finally, the planning horizon with the                  gain or loss of money, displayed onscreen. Thus, the partic-
highest predicted Q-value is used for decision-making.                         ipants’ task was to plan a route that would maximize their
   By proposing metacognitive reinforcement learning as a                      earnings or minimize their losses, based on the number of
mechanism of cognitive plasticity, our model suggests that re-                 planning steps required for that game.
ward and feedback are critical for cognitive growth. Concep-                      The experiment comprised thirteen trials total: a sequence
tualizing metacognitive reinforcement learning as a regres-                    of three practice problems which required planning 2, 3, and
sion problem suggests that learning how to best think about                    3 steps ahead, respectively, followed by ten 4-step prob-
a problem should require less practice the stronger the cor-                   lems, with a break after trial eight. The order of the two
relation between the features f(m, c) (i.e., the predictors) and               3-step problems was randomized, and the order of the ten
the resulting reward net the cost of thinking (i.e., the crite-                4-step problems was randomized across the last ten trials of
rion; Green, 1991). Here, we apply our model to predict                        the experiment. Participants were assigned randomly to one
how quickly people can learn that more planning leads to                       of two conditions: environments with reward structures de-
better results from the reward structure of the practice prob-                 signed to promote learning (“diagnostic rewards”), or envi-
lems. According to the model, learning should be fastest                       ronments with reward structures designed to hinder learning
when the reward increases deterministically with the plan-                     (“non-diagnostic rewards”).
ning horizon both within and across problems. By contrast,                        The problems of the diagnostic rewards condition were au-
learning should be slower when this relationship is degraded                   tomatically generated to exhibit four characteristics:
by additional variability in the rewards that is unrelated to
planning. The following experiments test this prediction and                  1. For each l-step problem, planning h < l steps ahead gener-
illustrate the model’s utility for designing feedback structures                  ates l − h suboptimal moves. In other words, each myopic
that promote metacognitive learning.                                              planner makes the maximum possible number of mistakes.
                                                                              2. When the number of moves is l, then planning l steps ahead
  Experiment 1: Reward structures can help or                                     yields a positive return, but planning h < l steps ahead
                        hinder learning to plan                                   yields a negative return.
Methods
                                                                              3. The return increases monotonically with the planning hori-
We recruited 304 adult participants from Amazon Mechan-                           zon from 1 to the total number of moves.
ical Turk. The task took about 25 minutes, and participants
were paid $2.50 plus a performance-dependent bonus of up to                   4. Each starting position occurs at least once.
                                                                           2471

   The reward structures used for the non-diagnostic rewards
condition were created by shifting the diagnostic reward
structures so as to degrade the correlation between planning
horizon and reward. Concretely, for half of the problems all
rewards were shifted down such that no amount of planning
could achieve a return better than −$10. Since the original
problems were such that the 1-step planner always performed
worst, the shift was −r1l+X where r1 is the return of the 1-step
planner, l is the number of steps in the planning problem,
and X is a random number between 10 and 20 that differed
across problems (X ∼ Uniform([10, 20])). For the other half
of the problems, all rewards were shifted up by − r1 +X l   such
that all planners achieve a return of at least +$10. These re-
ward structures make it extremely difficult for metacognitive
reinforcement learning to discover that planning is valuable,
because the random shifts greatly diminish the correlation be-        Figure 2: Model predictions and human performance in Ex-
tween planning horizon and reward.                                    periment 1. Error bars indicate the standard error of the mean.
                                                                      Model predictions were averaged over 500 simulations.
Results
Both model simulations and human behavior demonstrated
enhanced learning in environments with diagnostic rewards.            p < 0.01). When the rewards were diagnostic of good plan-
Figure 2 shows the mean performance of the metacognitive              ning, participants’ choices in the first step of the 4-step prob-
reinforcement learning model, and the mean performance of             lems accorded 10.3% more frequently with 4-step planning
human participants. Here, performance is measured as rela-            (t(302) = 3.57, p < 0.001). For 3 remaining steps there
tive reward                                                           was a significant increase in choices according with opti-
                                                                      mal 1-step (p < 0.01), 2-step (p < 0.01) and 4-step plan-
                Rrel = (R − Rmin )/(Rmax − Rmin ),          (10)      ning (p < 0.01). For 2 remaining steps, there was a signif-
where R is the total reward received during the trial, and Rmin       icant increase in choices according with optimal 1-step plan-
and Rmax are the highest and lowest possible total reward on          ning (p < 0.0001) without a decrease in agreement with other
that trial, respectively.                                             planning horizons. Finally, on the last move participants’
   To measure the effects of condition and trial number               choices in the environment with diagnostic rewards corre-
on performance in human participants, we ran a repeated-              sponded 5.8% more frequently with optimal 1-step planning
measures ANOVA. This revealed a significant effect of both            (t(302) = 3.71, p < 0.001), and significantly less frequently
trial number (F(9, 2989) = 3.44, p < 0.001) and condition             with 2-step and 3-step planning (p < 0.01 and p < 0.001). In
(F(9, 3029) = 15.26, p < 0.0001), such that participants im-          summary, diagnostic rewards led to better agreement between
proved over time, and participants with diagnostic feedback           the planning horizon and the number of remaining steps.
performed better than those without. To measure learning
                                                                          Experiment 2: Using feedback to promote
in each group, we ran a simple linear regression of the rel-
ative reward on the trial number. This revealed a significant                               learning to plan
regression equation for participants who received diagnostic          When one has control over the reward structure of an environ-
rewards (F(2, 302) = 11.28, p < 0.01), with an R2 of 0.59,            ment, creating rewards tailored to faster learning may be fea-
but not for participants who received non-diagnostic rewards          sible. However, often environmental rewards are fixed. In Ex-
(F(2, 302) = 3.51, p > 0.05), with an R2 of 0.31, suggesting          periment 2, we tested whether providing feedback may be an
that improvement in performance occurred with diagnostic              effective alternative approach to accelerating learning. When
rewards, but not without.                                             participants do not plan enough to find the optimal route, this
   To analyze the frequency with which participants chose             could be because the time cost of planning an optimal route
the optimal route, we performed a multinomial logistic re-            outweighs its benefits. To change that, we provided feedback
gression of whether or not each participant chose the optimal         in the form of timeout penalties for short-sighted decisions.
route on trial number and group. This revealed significant
effects of trial number (p < 10−6 ) and group (p < 0.0001).           Methods
   In addition, we found that participants interacting with a di-     We recruited 324 adult participants on Amazon Mechani-
agnostic reward structure learned to plan significantly further       cal Turk. The task took about 30 minutes, and participants
ahead than participants interacting with the non-diagnostic           were paid $3.00 plus a performance-dependent bonus of up
reward structure. When there were four steps left, the aver-          to $2.00. Participants played twenty trials of the flight plan-
age planning horizon was 2.96 with diagnostic rewards com-            ning game described above. These trials were divided into a
pared to 2.65 with non-diagnostic rewards (t(596) = 2.94,             training block and a testing block. The training block con-
                                                                  2472

sisted of six trials requiring 2-step planning, followed by ten
trials requiring 3-step planning. The testing block consisted
of four additional 3-step trials. The order of the 2-step tri-
als and the order of the 3-step trials were randomized across
subjects. Participants were randomly assigned to either the
feedback condition or the control condition.
   In the training block, participants in the feedback condition
were told their apparent planning horizon at the end of every
trial and penalized with a timeout that reflected the amount
of planning they had eschewed. Concretely, we set the dura-
tions of the timeouts such that the cost of short-sighted de-
cisions was proportional to the amount of necessary planning
the participant had eschewed. Specifically, the forgone cost of
planning was estimated by cost = 2l−ĥ , where l is the number
of moves for that trial, ĥ is the participant’s apparent plan-
ning horizon, and 2 is the branching factor since each step           Figure 3: Results of Experiment 2. The metacognitive RL
entailed a binary decision. The participant’s planning hori-          model predicts that feedback accelerate learning to plan. Hu-
zon was estimated by the number of consecutive moves con-             man behavior shows a similar pattern of results.
sistent with the optimal policy, beginning with the last move,
followed by the second-to-last, etc. At the end of each trial of      design repeated-measures ANOVA on participant perfor-
the first block, participants in the feedback group were penal-       mance during the 3-step trials. This revealed a significant
ized with a timeout delay for sub-optimal routes. The delay           effect of feedback (F(9, 4521) = 8.54, p < 0.01) and trial
was calculated as 7 · (cost − 1) seconds. During this period,         number (F(9, 4521) = 1.85, p < 0.05) on relative reward. To
participants were unable to proceed to the next trial. If partic-     measure learning in each group, we performed a simple lin-
ipants performed the optimal route, they were able to proceed         ear regression of relative reward on trial number for the 3-
immediately to the next trial.                                        step trials in the training block (i.e., when participants in the
   The control group received no feedback and had to wait             feedback group received feedback). This revealed a signifi-
a fixed amount of time at the end of every trial in block 1,          cant regression equation for the feedback group (F(2, 322) =
regardless of their performance. This fixed period was set to         5.28, p = 0.05), with an R2 of 0.40 but not for the control
8 seconds, to match the mean timeout period for participants          group (F(2, 322) = 1.57, p > 0.05), with an R2 of 0.16. This
in the feedback group (7.9 seconds). Neither group received           suggests that participants who received feedback improved
feedback or delays in the test block.                                 during the training block but the control group did not.
   The planning problems presented in this experiment were
                                                                         Feedback increased the model’s average performance in
created in two steps. In the first step, we created 2- and 3-
                                                                      both the training block and the transfer block. We next
step problems with maximally diagnostic reward structures
                                                                      tested whether the enhanced learning of the feedback group
(according to the criteria used in Experiment 1) subject to
                                                                      during training resulted in better performance in the trans-
the constraint that the first move with the highest immediate
                                                                      fer block (trials 17-20) where they no longer received any
reward was optimal for exactly half of those problems. In the
                                                                      feedback. A two-sample t-test revealed that the feedback
second step, we modified these problems so as to deteriorate
                                                                      group’s advantage in the testing block was nearly significant
the correlation between planning horizon and reward using
                                                                      (t(1294) = 1.53, p = 0.063). Figure 3 compares our partici-
the same method we employed to create the non-diagnostic
                                                                      pants’ performance to the model predictions.
reward structures used in Experiment 1.
                                                                         As predicted by our model, a multinomial logistic regres-
Model Predictions                                                     sion of whether or not each participant chose the optimal
We applied the metacognitive reinforcement learning model             route on trial number and feedback, revealed significant ef-
described above to the problem of learning how many steps             fects of trial number (p < 0.0001) and feedback (p < 0.01).
one should plan ahead. We simulated a run of the experi-                 Feedback appeared to increase people’s planning horizons:
ment described above with 1000 participants in each condi-            when there were two remaining moves, the choices of the
tion. The simulations predicted a gradual increase in the rel-        feedback group accorded 4% less often with myopic choice
ative return from the first 3-step problem to the last one (see       (t(1398) = −2.17, p < 0.05), 7% more often with optimal 2-
Figure 3). With feedback, the relative return increased faster        step planning (t(1398) = 3.44, p < 0.001), and 4% more of-
and reached a higher level than without feedback.                     ten with optimal 3-step planning (t(1398) = 2.43, p < 0.05).
Results                                                                                         Discussion
To quantify the effects of condition and trial number on per-         In this article, we have introduced a computational model
formance (measured as relative reward), we ran a mixed-               of how people learn to decide better. Its central idea is that
                                                                  2473

learning how to think can be understood as metacognitive re-           Bavelier, D., Green, C. S., Pouget, A., & Schrater, P. (2012). Brain
inforcement learning. Our model extends previous research               plasticity through the life span: learning to learn and action video
                                                                        games. Annual review of neuroscience, 35, 391–416.
on strategy selection learning (Lieder et al., 2014; Lieder &          Boureau, Y.-L., Sokol-Hessner, P., & Daw, N. D. (2015). Deciding
Griffiths, 2015) by capturing that choosing cognitive oper-             how to decide: self-control and meta-decision making. Trends in
ations is a sequential decision problem with potentially de-            cognitive sciences, 19(11), 700–710.
                                                                       Dolan, R. J., & Dayan, P. (2013). Goals and habits in the brain.
layed rewards rather than a one-shot decision. The new model            Neuron, 80(2), 312–325.
correctly predicted the effects of reward structure and feed-          Green, C. S., & Bavelier, D. (2008). Exercising your brain: a review
back on learning to plan: Experiment 1 suggested that our               of human brain plasticity and training-induced learning. Psychol-
                                                                        ogy and aging, 23(4), 692.
model captures the effect of reward structures on the speed of         Green, S. B. (1991). How many subjects does it take to do a regres-
metacognitive learning. We then applied our theory to design            sion analysis. Multivariate behavioral research, 26(3), 499–510.
feedback for people’s performance in environments whose                Hay, N., Russell, S., Tolpin, D., & Shimony, S. (2012). Select-
                                                                        ing computations: Theory and applications. In N. de Freitas &
reward structure is not diagnostic of good planning. Experi-            K. Murphy (Eds.), Uncertainty in artificial intelligence: Proceed-
ment 2 confirmed the model’s prediction that this intervention          ings of the twenty-eighth conference. P.O. Box 866 Corvallis, Ore-
would be effective.                                                     gon 97339 USA: AUAI Press.
                                                                       Huys, Q. J. M., Lally, N., Faulkner, P., Eshel, N., Seifritz, E., Ger-
   Our results suggest two pragmatic approaches to promot-              shman, S. J., . . . Roiser, J. P. (2015). Interplay of approximate
ing cognitive growth: first, designing reward structures that           planning strategies. Proceedings of the National Academy of Sci-
                                                                        ences, 112(10), 3098–3103.
are diagnostic of the quality of reasoning, planning, and              Jiang, N., Kulesza, A., Singh, S., & Lewis, R. (2015). The depen-
decision-making; second, providing feedback on the process              dence of effective planning horizon on model accuracy. In Proceed-
by which a decision was made. In Experiment 2 we followed               ings of the 2015 international conference on autonomous agents
                                                                        and multiagent systems (pp. 1181–1189).
the latter approach by designing feedback based on the cost            Keramati, M., Dezfouli, A., & Piray, P. (2011). Speed/accuracy
of planning; but other types of feedback may also be useful.            trade-off between the habitual and the goal-directed processes.
If cognitive plasticity is based on model-free reinforcement            PLoS Comput Biol, 7(5), e1002055.
                                                                       Lieder, F., & Griffiths, T. L. (2015). When to use which heuristic: A
learning as assumed by our theory, then its speed should crit-          rational solution to the strategy selection problem. In Proceedings
ically depend on how well the feedback people receive upon              of the 37th annual conference of the cognitive science society.
performing cognitive operations reflects their value. There-           Lieder, F., & Griffiths, T. L. (2016). Helping people make better
                                                                        decisions using optimal gamification. In Proc. 38th annu. conf.
fore, feedback structures that align immediate feedback with            cogn. sci. soc., philadelphia (pp. 2075–80).
long-term value should be maximally effective at promot-               Lieder, F., Plunkett, D., Hamrick, J. B., Russell, S. J., Hay, N., &
ing cognitive plasticity and learning to make better decisions.         Griffiths, T. (2014). Algorithm selection by rational metareason-
                                                                        ing as a model of human strategy selection. In Z. Ghahramani,
Future experiments should test this hypothesis by designing             M. Welling, C. Cortes, N. Lawrence, & K. Weinberger (Eds.), Ad-
feedback structures using the optimal gamification method in-           vances in neural information processing systems 27 (pp. 2870–
troduced by Lieder and Griffiths (2016). Feedback designed              2878). Curran Associates, Inc.
                                                                       Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995). On the
using optimal gamification could be especially beneficial be-           complexity of solving markov decision problems. In Proceedings
cause the underlying method of reward shaping is designed               of the eleventh conference on uncertainty in artificial intelligence
to accelerate model-free reinforcement learning (Ng et al.,             (pp. 394–402).
                                                                       Morrison, A. B., & Chein, J. M. (2011). Does working memory
1999). Critically, to promote learning how to decide, people            training work? the promise and challenges of enhancing cogni-
should decide without any assistance and only receive feed-             tion by training working memory. Psychonomic bulletin & review,
back after their choice.                                                18(1), 46–60.
                                                                       Myerson, J., & Green, L. (1995). Discounting of delayed rewards:
   We hope that our theory of metacognitive reinforcement               Models of individual choice. Journal of the experimental analysis
learning will be a step towards establishing a scientific foun-         of behavior, 64(3), 263–276.
                                                                       Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under
dation for designing feedback for cognitive training and other          reward transformations: Theory and application to reward shaping.
interventions for promoting cognitive growth. Future work               In I. Bratko & S. Dzeroski (Eds.), Proceedings of the 16th Annual
will evaluate alternative forms of feedback, address the prob-          International Conference on Machine Learning (pp. 278–287). San
                                                                        Francisco: Morgan Kaufmann.
lem of transfer and retention, and design more effective train-        Owen, A. M., Hampshire, A., Grahn, J. A., Stenton, R., Dajani, S.,
ing paradigms using tasks that are maximally diagnostic of              Burns, A. S., . . . Ballard, C. G. (2010). Putting brain training to
how people think and decide.                                            the test. Nature, 465(7299), 775–778.
                                                                       Russell, S., & Wefald, E. (1991). Principles of metareasoning. Ar-
Acknowledgments. This work was supported by grant number                tificial Intelligence, 49(1-3), 361–395.
                                                                       Shenhav, A., Musslick, S., Lieder, F., Kool, W., Griffiths, T., Cohen,
ONR MURI N00014-13-1-0341. The authors thank Thomas Hills,              J., & Botvinick, M. (2017). Toward a rational and mechanistic
Peter Dayan, Rika Antonova, Silvia Bunge, Stuart Russell, Amitai        account of mental effort. Annual Review of Neuroscience, 40.
Shenhav, and Sebastian Musslick for feedback and discussions.          Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An
                                                                        introduction. Cambridge, MA, USA: MIT press.
                                                                       Vul, E., Goodman, N., Griffiths, T. L., & Tenenbaum, J. B. (2014).
                           References                                   One and done? optimal decisions from very few samples. Cognitive
                                                                        science, 38(4), 599–637.
Anguera, J. A., Boccanfuso, J., Rintoul, J. L., Al-Hashimi, O.,
 Faraji, F., Janowich, J., . . . others (2013). Video game training
 enhances cognitive control in older adults. Nature, 501(7465), 97–
 101.
                                                                   2474

