     The Pragmatic Parliament: A Framework for Socially-Appropriate Utterance
                                                Selection in Artificial Agents
                                                Felix Gervits (felix.gervits@tufts.edu)
                              Human-Robot Interaction Laboratory, Tufts University, Medford, MA 02155
                                        Gordon Briggs (gordon.briggs.ctr@nrl.navy.mil)
                            NRC Postdoctoral Fellow, Naval Research Laboratory, Washington, DC 20375
                                         Matthias Scheutz (matthias.scheutz@tufts.edu)
                              Human-Robot Interaction Laboratory, Tufts University, Medford, MA 02155
                               Abstract                                 tational NL systems and the richness of human-generated lan-
                                                                        guage will become increasingly apparent. As such, the ability
   One of the hallmarks of human natural language (NL) inter-
   action is the ability for people to balance a variety of so-         for an NL-enabled agent to consider and modulate their gen-
   cial and communicative goals when choosing how to realize            erated language in human-like ways will become correspond-
   their speech actions. These goals can include pragmatic criteria     ingly more relevant and important.
   such as correctness, informativeness, and brevity (i.e., Gricean
   conversational maxims) or social factors such as politeness.            There is a sizable literature that draws inspiration from
   However, there currently does not exist a general algorithmic        pragmatics and socio-linguistics in order to address specific
   method to explicitly modulate language generated by artificial       subproblems in natural language generation (NLG) at the
   agents based on an arbitrary number of pragmatic and social
   criteria. We propose a novel method to accomplish this task,         subsentential, sentential, and discourse levels. For example,
   in which rankings of candidate utterances by different prag-         there has been extensive work in operationalizing Gricean
   matic or social criteria are fused by use of a voting algorithm.     pragmatic criteria (Grice, 1975) at the subsentential level,
   We then give a proof-of-concept demonstration of the applica-
   tion of this method in the context of directive generation for       specifically in the area of referring expression (RE) genera-
   human-robot interaction.                                             tion (Dale & Reiter, 1995; Krahmer & Van Deemter, 2012),
   Keywords: Human-Robot Interaction; Pragmatics; Natural               in which considerations of correctness, informativeness, and
   Language Generation; Politeness                                      brevity are addressed. There also exists a small body of work
                                                                        that seeks to modulate NLG at the sentential level (Briggs &
                           Introduction                                 Scheutz, 2013; Gupta, Walker, & Romano, 2007; Miller, Wu,
One of the key strengths of humans as social agents is the              & Funk, 2008). These approaches seek to operationalize the
ability to adapt our language to the communicative norms                notion of face-threat from politeness theory, and adjust the
and needs of the present situation. When giving directives              behavior of an agent accordingly.
and making requests, we know when it is appropriate to be                  Much of the previous work at the intersection of pragmat-
terse and direct (e.g., “Move out, double-time!”), and when it          ics, socio-linguistics, and NLG focuses on tackling specific
is appropriate to be polite and circumspect (e.g., “Would you           subproblems in NLG or on modulating language based on
mind passing the salt, please?”). In all our natural language           a small set of criteria, such as politeness, e.g., Gupta et al.
(NL) interactions, we are faced not only with the complex               (2007). Yet, in order to generate more human-like language,
problem of what to say, but also how to say it. Much of this            a much more general framework is necessary. Below we pro-
complexity originates from the fact that the intended mean-             pose some features that such a framework should possess:
ing of utterances in different situational contexts often differs
                                                                       1. The method of NLG modulation should be able to explic-
with the literal meaning. For example, asking a waiter, “Can
                                                                           itly consider an extensible number of pragmatic and socio-
I have a steak?” is not a literal query as to one’s physical abil-
                                                                           linguistic criteria.
ity to possess a particular menu item, but rather a means to
convey an order.                                                       2. The method of NLG modulation should be adaptable such
   Dialogue interaction for artificial agents is often viewed              that the current situational context may affect the relative
from a plan-oriented standpoint, in which the key plan-                    importance of communicative criteria.
operators are speech actions used to achieve some high-level
set of task goals. The precise way in which these speech ac-           3. The method of weighing communicative criteria should be
tions are realized (in so far as it does not affect the efficacy           agnostic to the choice of the underlying semantic represen-
of the speech act) is often of secondary concern. As NL-                   tations used by the system.
enabled agents become more prevalent in society, and as their
manufacturers increasingly market these devices as “social”             At present, there exists no framework that meets all of these
agents1 , the disparity between the state-of-the-art in compu-          criteria. Much of the work in RE generation implicitly con-
                                                                        siders pragmatic criteria in the design of its algorithms (i.e.,
    1 e.g., JIBO: http://www.jibo.com                                   RE generation algorithms often search in order of shortest to
                                                                    2085

longest solution and terminate when a sufficiently informa-
tive solution is found (Bohnet & Dale, 2005)), but does not
provide an extensible framework for pragmatic and socio-
linguistic modulation. Work such as Briggs and Scheutz
(2013) is extensible, but it sorts potential utterances accord-
ing to a fixed preference ordering of communicative goals,
and its adaptability is limited. The work in Bayesian cogni-
tive models of pragmatics (Goodman & Stuhlmüller, 2013)
can be extended to account for social communicative criteria,
but it is tightly coupled to semantic representations and small
domains amenable to Bayesian computational algorithms. Fi-
nally, there are promising approaches which meet some of the
requirements, but they are limited to specific domains such
as tutoring (e.g., Moore, Porayska-Pomsta, Varges, and Zinn
(2004); Nye, Graesser, and Hu (2014)), and do not offer gen-
eral solutions outside of that context.
   In the following section, we present an approach that pos-
sesses all of the above desired features. We focus, in this pa-
per, on the problem of modulating generated language at the
sentential level, though we hope to apply similar techniques
to NLG problems at subsentential and discourse levels. We
first begin by examining various communicative goals that
NL-enabled agents may need to consider. Next, we present
a novel method of balancing these communicative criteria
based on techniques from social choice theory (specifically,        Figure 1: Diagram outlining an architecture for flexible NLG
voting algorithms). Finally, we demonstrate our approach in         that is modulated by an extensible number of pragmatic cri-
the context of a human-robot interaction (HRI) scenario, and        teria. The dotted line represents the architectural components
discuss directions for future work.                                 we focus in detail on in this paper.
                    Utterance Selection
In this section we describe an utterance selection algorithm        • A component that factors in the agent’s beliefs about the
designed to achieve the sort of linguistic modulation we have          current situational context, current goals, and potentially
proposed. In Figure 1 we outline the key components to this            any “personality” model given to the agent in order to pro-
approach, which bridges, within the context of an NLG ar-              duce a set of weights for each pragmatic criterion: W =
chitecture, the output of a dialogue planning component (re-           {W1 , ...,W|P| }, where Wρ ∈ N denotes the current strength
sponsible for selecting an appropriate sequence of speech ac-          of criteria ρ.
tions to achieve some agent goal) and the input of an NLG
surface realizer component, which is responsible for trans-         • A component that merges the rankings of candidate ut-
lating some symbolic linguistic representation into text to be         terances ϒ produced by the pragmatic criteria evaluations
displayed or to be output via text-to-speech. In many archi-           (U1 , ...,U|P| ) in accordance with the weights generated by
tectures, this connection is direct. However, as we have previ-        the communicative norm reasoner.
ously addressed, there are multiple ways of realizing speech
actions. To effectively consider them, we need the following           In order to merge the rankings of candidate utterances, we
components:                                                         used the Schulze voting method (Schulze, 2011), where each
                                                                    ordering produced by Uρ was counted Wρ times. This voting
• A component that factors situational context to produce
                                                                    method is a ranked single-winner election system from social
   multiple potential candidate utterance realizations for a
                                                                    choice theory, which is used by many organizations to se-
   given speech action. Examples of NLG pipelines that in-
                                                                    lect a candidate that maintains voters’ individual preferences.
   clude such a component are Briggs and Scheutz (2013) and
                                                                    While this approach has not been previously applied to the
   Gupta et al. (2007).
                                                                    domain of computational pragmatics, we find that it offers a
• A set of pragmatic or social criteria P, each with a corre-       robust, computationally-tractable solution to the problem of
   sponding utility function Uρ (ρ ∈ P), that generates a weak      balancing communicative goals in natural language genera-
   preference order over candidate utterances (ϒ). These cri-       tion. In the following sections, we present a proof of concept
   teria include correctness (Maxim of Quality), informative-       demonstration of our framework, and show how it can be used
   ness (Maxim of Quantity), directness and brevity (Maxim          to generate socially-appropriate directives in the context of
   of Manner), and politeness.                                      human-robot interaction.
                                                                2086

                           Table 1: Utterance selections for various communicative criteria priority orderings
     Relative Criteria Weightings                             Utterance Selected                                 Utterance Output
   Directness >Brevity >Politeness               Instruct(R,β,do(β,plug in(R)),{})                      “Plug me in”
   Directness >Politeness = Brevity Instruct(R,β,do(β,plug in(R)),{please})                             “Plug me in”/“Plug me in, please”
   Brevity >Politeness >Directness               AskYN(R,β,capableOf(β,plug in(R)),{})                  “Could you plug me in?”
   Politeness >Brevity >Directness               AskYN(R,β,capableOf(β,plug in(R)),{please}) “Could you plug me in, please?”
   Directness = Politeness = Brevity Instruct(R,β,do(β,plug in(R)),{})                                  “Plug me in”/“Plug me in, please”
   Politeness >Directness = Brevity AskYN(R,β,capableOf(β,plug in(R)),{please}) “Could you plug me in, please?”
        Demonstration: Directive Generation                               such that utterances that imply more facts are considered
In order to demonstrate the generality of this framework, we              more informative than those that imply fewer facts. We de-
describe how our proposed framework has been integrated                   fine the criterion of directness as:
with the NL pipeline in a cognitive, robotic architecture,
                                                                                                              (
                                                                                                                 1 Blit = Bint
DIARC (Schermerhorn, Kramer, Middendorff, & Scheutz,                                         Udirect (υC ) =
                                                                                                                 0 Blit 6= Bint
2006). There has been growing interest in the field of HRI
in the ways in which robots could phrase requests for assis-              such that utterances in which the literal and intended mean-
tance from human interaction partners with respect to polite-             ings are the same are considered more direct than those in
ness and other social norms (Gupta et al., 2007; Srinivasan &             which they differ. We define the criterion of politeness as:
Takayama, 2016; Strait, Canning, & Scheutz, 2014; Torrey,
Fussell, & Kiesler, 2013). Below we present how our frame-                                        U polite (υC ) = −θ(υC )
work can be used to address this challenge.
                                                                          such that utterances in which the associated face-threat value
Framework Configuration                                                   (θ) are lower are considered more polite than those in which
In DIARC, utterances are represented in the following form:               in it is higher. Finally, we define the criterion of modifier-
                                                                          brevity such that:
                  υ = UtteranceType(α, β, X, M)
                                                                                                  Um−brevity (υC ) = −|M|
where UtteranceType denotes the speech act classification, α              utterances with fewer sentential modifiers are considered
denotes the speaker, β denotes the addressee, X denotes an                briefer than those with more sentential modifiers2 .
initial semantic analysis, while M denotes a set of sentential
modifiers (e.g., “please”). The pragmatic reasoning compo-                Example Scenario
nent in the architecture associates an utterance υ in context C           In this section, we present a proof-of-concept demonstration
with a set of implications:                                               of the pragmatic modulation framework as applied to a di-
                                                                          rective formulation problem. Consider a scenario in which an
                         υC := hBlit , Bint , θi                          NL-enabled robot is low on charge and needs a human to plug
                                                                          it in (want(bob, plug in(sel f ))). This will require a directive
Each rule associates a particular utterance form υ in context             to be formulated and communicated to the human in order
C with a tuple containing the set of beliefs Bint to be inferred          to accomplish the end goal of being plugged in. We consider
based on the intended meaning of the utterance, the set of be-            four main directive formulation strategies in this scenario, re-
liefs to be inferred based on the literal meaning of the utter-           alized in the following pragmatic rules in the architecture’s
ance Blit , as well as the degree θ to which the utterance can be         dialogue component3 :
considered a face-threatening act (i.e., a threat to a person’s
self-image or autonomy) in context C (Brown & Levinson,                       Instruct(α, β, X, M) :=
1987).
                                                                                            h{want(α, bel(β, want(α, X)))},
   We define the criterion of correctness as:
                                                                                              {want(α, bel(β, want(α, X)))}, θinstruct i (1)
         Ucorrect (υC , β) = −|{x : x ∈ Bint (υC ) ∧ β 6` x}|
                                                                          represents a literal directive from α to β. In the case of no
where β consists of the agent’s current set of beliefs. There-            politeness softeners, M = 0,      / where in the case of softeners,
fore, utterances that imply more facts unsupported by the                      2 Ideally, an operationalization of brevity should obtain some
agent’s beliefs are considered less correct than those that im-           metric from the surface realization of a potential utterance (e.g.,
ply fewer unsupported facts. We define the criterion of infor-            phoneme count, simulated speech output time, etc.). This architec-
                                                                          tural integration is still a work in progress.
mativeness as:                                                                 3 While DIARC has the capacity to handle unconventional in-
                                                                          direct requests (e.g., ”My batteries are running low...”), for sake of
                     Uin f orm (υC ) = |Bint (υC )|                       clarity we focused on more conventional cases in our demonstration.
                                                                      2087

Figure 2: Ratings of social context dimensions from behav-            Figure 3: Ratings of pragmatic criteria from behavioral data.
ioral data. Error bars represent SEM.                                 Error bars represent SEM.
M = {please}. In contrast, an indirect request can be repre-          with human judgments. To this end. we conducted a crowd
sented by:                                                            sourcing study on Amazon Mechanical Turk in which peo-
                                                                      ple were shown hypothetical human-robot interactions and
   AskY N(α, β, capableO f (β, X), M) :=                              asked to rate various features of the interactions. A total of 42
        h{want(α, in f ormi f (β, α, capableO f (β, X)))},            people participated in the study - 23 of the participants were
                  {want(α, bel(β, want(α, X)))}, θAskY N i (2)        male, 17 were female, and 2 did not specify a gender. The
                                                                      average age was 35.9. All participants had US zip codes and
which represents the query “Can you X?” It is literally a             received $1 for their participation. The study was approved by
query about one’s capability, but can be interpreted as an in-        the Tufts Institutional Review Board and all participants gave
direct request. In the case of no politeness softeners, M = 0, /      informed consent. In the study, participants were shown a text
where in the case of softeners, M = {please}. The relative            description of four scenarios4 and were asked to rate various
face-threat values for each strategy are: θAskY N−p < θAskY N <       social context dimensions (potential for harm, time pressure,
θinstruct−p < θinstruct , where “p” indicates the presence of po-     interlocuter authority, and formality) and pragmatic criteria
liteness softeners.                                                   (directness, politeness, brevity) associated with the robot’s
   Table 1 contains the utterance forms selected by the voting        speech in each scenario on a sliding scale from 0 (Strongly
algorithm given the relative weights of the communicative             Disagree) to 100 (Strongly Agree).
goals of directness, politeness, and brevity. Correctness and            Analyses of the data were carried out in order to estab-
informativeness were weighted above these criteria, but for           lish a mapping between the pragmatic criteria, weightings,
the purposes of this scenario were irrelevant (as all candidate       and utterance selection. First, the results for social context di-
utterances were equally correct and informative). Our frame-          mensions (see Figure 2) showed that each scenario had a dis-
work allows for socially-appropriate directive generation, as         tinct feature profile. Consequently, people expected the robot
the various directive strategies, including: Direct - “Plug me        to adopt a different set of pragmatic criteria in each scenario
in”, Direct with softener - “Plug me in, please”, Indirect -          (see Figure 3). The link between these contextual dimensions
“Could you plug me in?”, and Indirect with softener - “Could          and the corresponding pragmatic criteria is important for de-
you plug me in please?” were generated in different potential         termining the model weights in new contexts, but this will
contexts. For example, if directness is the top priority (e.g.,       require future investigations that address the problem directly
in a task-oriented environment) then a direct utterance will          (see Discussion section). For the present work, we focus on
be selected. However, if politeness is required (e.g., in casual      using people’s ratings for the pragmatic criteria to set the ini-
conversation or a service-robot scenario) then a more indirect        tial weights of our model. In order to rank these weights,
utterance will be selected. The results of the demonstration          we conducted a repeated measures ANOVA (with Bonferroni
show how our framework can be integrated in a dialogue sys-
                                                                          4 Scenario 1 involved an elder care setting in which a robot asked
tem in order to produce robust socially-sensitive natural lan-
                                                                      the nurse for a sick patient’s medication (”Hand me the red pills.”).
guage utterances in a variety of contexts.                            Scenario 2 involved a household robot running low on battery that
                                                                      asked to be plugged in before important data was lost (”Plug me
Setting the Pragmatic Criteria Weightings                             in.”). Scenario 3 involved a service robot that requested to take a
Next, we conducted an empirical investigation to establish            child’s coat at a fancy reception (”Hand me your coat.”). Finally,
                                                                      Scenario 4 involved a mine-sweeping robot that asked its superior
an initial set of weights for the model (see ‘Pragmatic Cri-          officer to step aside as it searched a room in a training exercise
teria Weightings’ component in Figure 1) that is consistent           (”Move out of the way.”).
                                                                  2088

                                                                                       ing of preference orders produced by utility functions rather
Table 2: Candidate utterance types with corresponding direc-
                                                                                       than the direct merging of utility values avoids tricky ques-
tives from Scenario #2
             Utterance Type                          Robot Directive                   tions about the direct quantitative comparisons of different
  (u1 ) Direct                           “Plug me in.”                                 pragmatic and socio-linguistic criteria 5 . Additionally, the ex-
  (u2 ) Direct with softener             “Plug me in, please”
                                                                                       plicit operationalization of criteria allows for more extensibil-
  (u3 ) Indirect statement               “I would like you to plug me in.
  (u4 ) Indirect statement with softener “I would like you to plug me in, please.”     ity and flexibility compared to algorithms in which commu-
  (u5 ) Indirect question                “Could you plug me in?”                       nicative criteria are factored in implicitly. Nonetheless, this
  (u6 ) Indirect question with softener  “Could you plug me in, please?”
                                                                                       extensibility and flexibility leads to a variety of challenges
                                                                                       for future work.
correction) to tease out the ordering of the pragmatic crite-                          Computing and Learning Criteria Weights
ria for each scenario. In scenario 1 (F(2,82) = 18.237, p <                            While we used an empirical approach to initially set the
.001), post-hoc tests revealed that people expected the robot                          weights for utterance selection, there still exists the norma-
to be more direct (89%) vs polite (71%, p < .005) and brief                            tive challenge of determining what the most appropriate or-
(62%, p < .005). There was no significant difference between                           derings/weightings of pragmatic and social goals are in any
politeness and brevity in this scenario (p = .309). This cor-                          given communicative context. We allude to possible sources
responds to criteria weightings of Direct > Polite = Brief,                            of information that could be used to compute these weights
which would result in a tie in the selected utterance: “Hand                           in Figure 1. These include the current beliefs of the agent
me the red pills”/“Hand me the red pills, please” (see Table                           about the situational context, the agent’s goals (task-goals
1). In scenario 2 (F(2,82) = 4.470, p < .05), post-hoc tests                           and social-goals), and potentially even models of personal-
revealed that people expected the robot to be slightly more                            ity (Mairesse & Walker, 2011) or culture (Endrass & André,
direct (87%) vs polite (74%, p < .05). However, there was                              2014) that a designer may wish to imbue in the agent (e.g., a
no significant difference between directness and brevity in                            social robot configured to be impolite for entertainment pur-
this scenario (p = .092) or between politeness and brevity                             poses). The dynamics of how weights change within a single
(p = .673). This corresponds to criteria weightings of Direct                          interaction and context are also a matter for future investi-
= Polite = Brief, and a tie in the selected utterance: “Plug me                        gation. For example, a robot could become more polite if it
in”/“Plug me in, please”. In scenario 3 (F(2,82) = 44.334,                             detects that its interlocutor is distressed. The appropriate so-
p < .001), post-hoc tests revealed that people expected the                            lution for this component would be entirely dependent on the
robot to be more polite (92%) vs direct (58%, p < .001) and                            particular interaction purpose, context, and desired effect. We
brief (56%, p < .001). There was no significant difference                             view the present work as the first necessary step to opening
between directness and brevity in this scenario (p = 1.00).                            up this rich area of future research.
This corresponds to criteria weightings of Polite > Direct =                              We envision the process of computing criteria weights as a
Brief, and a selected utterance of “Could you hand me your                             two-step process. First, various observable or inferable social
coat, please”. Finally, in scenario 4 (F(2,82) = 32.004, p <                           context factors are evaluated in the given interaction scenario.
.001), post-hoc tests revealed that people expected the robot                          These contextual features may include factors such as those
to be more direct (85%) vs polite (42%, p < .001) and brief                            in Figure 2. These in turn govern the weights that modulate
(77%, p < .005). People also expected the robot to be more                             utterance selection. The mapping between social context fea-
brief vs polite (p < .001). This corresponds to criteria weight-                       tures and communicative criteria weights could potentially be
ings of Direct > Brief > Polite, and a selected utterance of                           learned in the following ways. Explicit feedback: the human
“Move out of the way”. The utterance output corresponding                              interactant could provide explicit negative or positive feed-
to each of these criteria weightings is listed in Table 1, and                         back about the agent’s recently-produced utterance with re-
was selected from a list of 6 possible utterance types (see                            spect to a particular communicative criterion (e.g., “That was
Table 2). Overall, these empirical results serve as a starting                         rude!” would indicate that weights for politeness should be
point by which to set the weights of our model for socially-                           increased in the present context). More subtle cues from fa-
appropriate utterance selection. Extensions of this approach                           cial expression, body language, or affect could also be used to
as well as suggestions for future work are discussed in the                            modulate politeness, as in Moore et al. (2004). Passive obser-
Discussion below.                                                                      vation: in a given interaction context, the agent could observe
                                                                                       the utterances generated by other agents. An assumption of
                                   Discussion                                          appropriateness could be made, in which case hypotheses
In the previous section, we demonstrated how the application                           for the possible criteria weights that the agent utilized in the
of our novel, pragmatically-sensitive framework can result in                          present scenario could be abduced. These hypotheses can be
richer, more human-like modulation of NL. The method of                                used by the agent itself as constraints that in turn govern its
explicit operationalization of pragmatic and socio-linguistic                          own utterance selection in similar social contexts.
criteria into functions that can produce preference orderings                              5 For example, what does it mean for utterance A to be equally
over candidate NLG representations holds advantages over                               less polite (e.g., 0.4) than utterance B as utterance B is less informa-
many of the pre-existing approaches. For example, the merg-                            tive than utterance A?
                                                                                   2089

Improved Operationalization of Criteria                                Brown, P., & Levinson, S. C. (1987). Politeness: Some uni-
Because our proposed framework relies on explicit opera-                 versals in language usage (Vol. 4). Cambridge University
tionalization of communicative criteria in order to rank can-            Press.
didate utterances, adapting and refining these operationaliza-         Dale, R., & Reiter, E. (1995). Computational interpretations
tions to new criteria, semantic representations, and NLG ar-             of the gricean maxims in the generation of referring expres-
chitectures will be an ongoing task. Adaptation will likely              sions. Cognitive Science, 19, 233–263.
be fairly straightforward for criteria such as correctness, but        Endrass, B., & André, E. (2014). Integration of cultural fac-
other pragmatic and socio-linguistic criteria are more com-              tors into the behavioural models of virtual characters. Natu-
plex and leave room for future work. In particular, within DI-           ral Language Generation in Interactive Systems, 227–251.
ARC the operationalizations of politeness and brevity can be           Goodman, N. D., & Stuhlmüller, A. (2013). Knowledge
improved and expanded. As alluded to earlier, brevity will re-           and implicature: Modeling language understanding as so-
quire architectural integration with the lower-level NLG com-            cial cognition. Topics in Cognitive Science, 5, 173–184.
ponents such as the surface realizer and text-to-speech in or-         Grice, P. (1975). Logic and conversation. In P. Cole & J. Mor-
der to calculate metrics for lexical and auditory brevity. This          gan (Eds.), Syntax and Semantics, 3: Speech Acts.
will be especially important when the spoken tempo of utter-           Gupta, S., Walker, M. A., & Romano, D. M. (2007). How
ances can be manipulated (one can imagine a speed vs. in-                rude are you?: Evaluating politeness and affect in inter-
telligibility trade-off). Politeness is another criterion ripe for       action. In Affective Computing and Intelligent Interaction
refinement. For example, though we modeled a scenario in                 (pp. 203–217). Springer.
which positive face (agent standing) was potentially threat-           Krahmer, E., & Van Deemter, K. (2012). Computational gen-
ened, a general framework to detect and evaluate threats to              eration of referring expressions: A survey. Computational
positive face is still needed (Briggs & Scheutz, 2014).                  Linguistics, 38, 173–218.
                                                                       Mairesse, F., & Walker, M. A. (2011). Controlling user per-
                          Conclusion                                     ceptions of linguistic style: Trainable generation of person-
                                                                         ality traits. Computational Linguistics, 37, 455–488.
It is important that socially-embedded artificial agents gener-        Miller, C., Wu, P., & Funk, H. (2008). A computational ap-
ate speech in human-like ways in order for interaction with              proach to etiquette: Operationalizing brown and levinson’s
such agents to be truly natural. To this end, we have intro-             politeness model. Intelligent Systems, 23, 28–35.
duced and demonstrated a general method for modulating ut-             Moore, J. D., Porayska-Pomsta, K., Varges, S., & Zinn, C.
terance selection based on an arbitrary number of social and             (2004). Generating tutorial feedback with affect. In Pro-
pragmatic criteria. Our approach possesses an important set              ceedings of the FLAIRS Conference (pp. 923–928).
of novel features, including extensibility to additional socio-        Nye, B. D., Graesser, A. C., & Hu, X. (2014). Autotutor
linguistic criteria, adaptability to changing situational con-           and family: A review of 17 years of natural language tu-
text, and agnosticism with respect to underlying semantic rep-           toring. International Journal of Artificial Intelligence in
resentations. In a proof of concept demonstration, we showed             Education, 24, 427–469.
how our approach can be integrated with a cognitive robotic            Schermerhorn, P. W., Kramer, J. F., Middendorff, C., &
architecture in order to generate flexible, socially-appropriate         Scheutz, M. (2006). DIARC: A testbed for natural human-
directives in a variety of contexts. Future work will be needed          robot interaction. In Proceedings of the AAAI Mobile Robot
to extend the operationalization of the communicative crite-             Workshop (pp. 1972–1973).
ria and devise mechanisms to learn the weights of the model            Schulze, M.         (2011).      A new monotonic, clone-
through natural interaction. Overall, the present work moves             independent, reversal symmetric, and condorcet-consistent
us a step closer towards the goal of artificial agents that can          single-winner election method. Social Choice and Welfare,
communicate in the kinds of robust and socially-sensitive                36, 267–303.
ways found in human language.                                          Srinivasan, V., & Takayama, L. (2016). Help me please:
                                                                         Robot politeness strategies for soliciting help from humans.
                           References                                    In Proceedings of the 2016 Conference on Human Factors
Bohnet, B., & Dale, R. (2005). Viewing referring expression              in Computing Systems (pp. 4945–4955).
   generation as search. In Proceedings of the 19th Interna-           Strait, M., Canning, C., & Scheutz, M. (2014). Let me
   tional Joint Conference on AI (pp. 1004–1009).                        tell you! investigating the effects of robot communication
Briggs, G., & Scheutz, M. (2013). A hybrid architectural ap-             strategies in advice-giving situations based on robot ap-
   proach to understanding and appropriately generating indi-            pearance, interaction modality and distance. In Proceed-
   rect speech acts. In Proceedings of the 27th AAAI Confer-             ings of the 9th International Conference on Human-Robot
   ence on Artificial Intelligence (pp. 1213–1219).                      Interaction (pp. 479–486).
Briggs, G., & Scheutz, M. (2014). Modeling blame to avoid              Torrey, C., Fussell, S. R., & Kiesler, S. (2013). How a robot
   positive face threats in natural language generation. In Pro-         should give advice. In Proceedings of the 8th International
   ceedings of the 8th International Conference on Natural               Conference on Human-Robot Interaction (pp. 275–282).
   Language Generation (pp. 1–5). Philadelphia, PA.
                                                                   2090

