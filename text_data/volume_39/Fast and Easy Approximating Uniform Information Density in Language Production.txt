        Fast and Easy: Approximating Uniform Information Density in Language
                                                                 Production
                                            Jesús Calvillo (jesusc@coli.uni-saarland.de)
                                                         Saarland University, Germany
                              Abstract                                    density structures. Here I present a way to balance these pres-
                                                                          sures in order to obtain sentences with more uniform surprisal
   A model of sentence production is presented, which imple-
   ments a strategy that produces sentences with more uniform             profiles, which could be later linked to a bandwidth-limited
   surprisal profiles, as compared to other strategies, and in accor-     communication channel.
   dance to the Uniform Information Density Hypothesis (Jaeger,              The language production model proposed here extends the
   2006; Levy & Jaeger, 2007). The model operates at the al-
   gorithmic level combining information concerning word prob-            one presented by Calvillo, Brouwer, and Crocker (2016),
   abilities and sentence lengths, representing a first attempt to        which produces sentences describing a given semantics by
   model UID as resulting from underlying factors during lan-             maximizing word probabilities. The semantic representations
   guage production. The sentences produced by this model
   showed indeed the expected tendency, having more uniform               used are a variation of those defined by the Distributed Situa-
   surprisal profiles and lower average word surprisal, in compar-        tion Space model (DSS, Frank, Koppen, Noordman, & Vonk,
   ison to other production strategies.                                   2003; Frank, Haselager, & van Rooij, 2009). The rest of this
   Keywords: information density; sentence production; rational           section briefly presents the DSS model as well as the model
   analysis; connectionist; semantics
                                                                          described by Calvillo et al. (2016).
                           Introduction                                   Distributed Situation Space
For a given semantics, humans are able to produce a large                 The DSS model (Frank et al., 2003, 2009) defines a mi-
number of surface representations that express its meaning.               croworld in terms of a finite set of basic events (e.g.,
However, some constructions are preferred over others, some               play(charlie, chess)) —the smallest meaning-discerning
sentences are easier to understand, while some others are                 units of propositional meaning in that world.              Basic
more difficult, so people tend to avoid them.                             events can be conjoined to form complex events (e.g.,
   Uniform Information Density Hypothesis (UID, Jaeger,                   play(charlie, chess) ∧ win(charlie)).         However, the mi-
2010; Levy & Jaeger, 2007) presents one way to rank sen-                  croworld poses both hard and probabilistic constraints on
tences according to how uniform their surprisal profiles are;             event co-occurrence; as a result, some complex events are
where a sentence is preferred if the surprisal of each of its             very common, and some others impossible to happen.
words remains uniform. This is explained as a rational strat-                A situation-state space is a large set of m microworld obser-
egy of language production at the computational level of                  vations defined in terms of n basic events, yielding an m × n
analysis, as such strategy maximizes the probability of suc-              matrix (see Table 1). Each observation in this matrix is en-
cessful communication in a bandwidth-limited noisy channel                coded by setting basic events that are the case in the given ob-
while maximizing information transmission. Alternatively,                 servation to 1 (True) and those that are not to 0 (False). This
and without the assumption of a noisy channel, comprehen-                 matrix is constructed by sampling m observations such that
sion effort is also minimized utilizing a UID strategy (Levy              no observation violates any hard world knowledge constraint,
& Jaeger, 2007), provided that the effect of surprisal on com-            and such that the m observations approximate the probabilis-
prehension effort is superlinear (Hale, 2001; Levy, 2008).                tic nature of the microworld. The resulting matrix encodes
   Empirical evidence supports this hypothesis (e.g., Aylett              then all knowledge about the microworld, where each col-
& Turk, 2004; Bell et al., 2003), however, as far as one                  umn, also called situation vector, represents the meaning of
can tell, no modeling attempts explore this at the algorith-              each basic event in terms of the observations in which the
mic or implementational levels. Here, a mechanistic account               basic event is true.
of sentence production is presented, which balances on the                   Frank et al. (2009) successfully used these DSS representa-
one hand speed of information transmission and on the other               tions in a connectionist comprehension model. They defined
hand comprehension and production effort. The sentences                   a microworld consisting of 44 basic events centered around
produced by this strategy present more uniform surprisal pro-             three people. Then they constructed a situation-state space
files, compared to other strategies, and thus, represent a first          by sampling 25, 000 observations. As an example, in this
approximation to UID.                                                     space the situation vector for play(charlie, chess) would cor-
   In particular, the model assumes that speakers act under               respond to a column in the matrix, where each dimension
three different pressures: a first one, pushing speakers to be            corresponds to one observation, and its value would be 1 if
fast under time restrictions; a second one, related to produc-            Charlie is playing chess in that observation. Finally, they
tion effort, pushing speakers to produce available content first          reduced the dimensionality of the resulting 25k-dimensional
(see Ferreira & Dell, 2000); and a third one, related to com-             situation vectors to 150 dimensions using a competitive layer
prehension effort, pushing speakers to avoid high information             algorithm.
                                                                      1709

                                                                                                              The identity of the word that was produced at time-step
                 Table 1: Situation-state space.
                                                                                                           t − 1 is forwarded to the hidden layer through monitoring
                                                                                                           units connecting the output layer to the hidden layer, where
                                      basic event1   basic event2   basic event3         basic eventn
                                                                                                           only the output unit of the word produced at time-step t − 1 is
                                                                                                           activated (set to 1), while all other units are set to 0.
                                                                                                              Finally, the hidden and output layers also receive input
                                                                                   ...
                                                                                                           from a bias unit with a constant activation of 1.
   observation1                        1              0               0            ...     1
   observation2                        0              1               1            ...     1                                       UID Model
   observation3                        1              1               0            ...     0
                                                                                                           The here proposed model architecture, shown in Figure 1,
   ...                                 .              .               .            ...     .
                                                                                                           consists of two paths of processing: the first one (above, in-
   observationm                        0              1               0            ...     0
                                                                                                           side the dotted rectangle), computes word probabilities given
                                                                                                           the context, and is identical to the model of Calvillo et al.
                                                                                                           (2016); and the second one (below), receives the output of
DSS Language Production                                                                                    the former and computes derivation length estimations, i.e.,
DSS representations were also used by Calvillo et al. (2016)                                               how long a sentence can be if a particular word is produced.
in a connectionist model of language production, showing                                                   We call probabilities the layer containing the output of the
that they are suitable for modeling production.                                                            first path, and der lengths the layer containing the output of
                                                                                                           the second path.
   While Calvillo et al. (2016) used the same microworld as
Frank et al. (2009), the DSS representations were modified in                                                 The output of these two paths is then combined in a final
order to avoid the competitive layer dimensionality reduction.                                             layer (words) that receives unmodified copies of the activa-
Instead, the original 25-k dimensional situation vectors were                                              tion of probabilities and der lengths and whose activation is a
converted to belief vectors. Each dimension of the latter is                                               combination of these two types of information. At this point
equal to the conditional probability of each basic event given                                             the model produces the word with the highest activation in
the original 25k-dimensional DSS representation that is asso-                                              words, whose identity is then passed to the first hidden re-
ciated to each sentence.1 The result is a 44-dimensional vec-                                              current layer through monitoring units in order to process the
tor that avoids the loss of information associated to the com-                                             next word production. Finally, production stops when an end-
petitive layer algorithm, and consequently renders a higher                                                of-sentence marker is produced.
performance in a language production task.                                                                    The rest of this section presents in more detail each of these
                                                                                                           parts, along with their justification.
   The architecture of the model presented by Calvillo et
al. (2016), represented by the dotted rectangle in Figure 1,
implements an extension of a Simple Recurrent Network
(Elman, 1990) with a 45-unit input layer, a 120-unit recur-
rent hidden (htan) layer, and a 43 unit (softmax) output layer.
The input layer contains 44 units corresponding to the 44 ba-
sic events in the microworld, plus one binary unit indicating
whether the model must output an active sentence (1), or a
passive one (0). The output layer contains 43 units matching
the number of available words in the vocabulary.
   Time in the model is discrete. At each time step t, the re-
current hidden layer receives as input the DSS representation,
its own activation at time step t − 1 (zeros at t = 0) and the
identity of the word that was produced at time step t − 1 (ze-
ros at t = 0). Activation of the hidden layer is then propagated
                                                                                                                         Figure 1: UID Production Model.
to the softmax output layer.
   The activation of the output layer yields a probability dis-
tribution over the available words, where the word produced                                                Semantic and Linguistic Information
at time-step t is defined as the one with highest probabil-
                                                                                                           The information content or surprisal of a sentence s is defined
ity (highest activation). Production stops after an end-of-
                                                                                                           as its negative log probability −logP(s). Moreover, sentences
sentence marker has been produced.
                                                                                                           express events in the world, such that a sentence can be paired
    1 This vector is computed by calculating the dot product between                                       with one or more events, and vice versa. Therefore, we can
the situation-state matrix and the original 25k-dimensional situation                                      decompose the probability of a sentence s into:
vector, and then normalizing each dimension of the resulting vec-
tor by the sum over the dimensions of the original 25k-dimensional                                                             P(s) = ∑ P(s|ei )P(ei )
situation vector.                                                                                                                       i
                                                                                                        1710

where ei is an event in the world that is paired with s.            In this respect, producing the most probable word, and there-
   From this, we can distinguish two kinds of information:          fore most available, at each time step minimizes (to some ex-
P(ei ), related to each event that can be paired with the sen-      tent) production effort by locally minimizing linguistic sur-
tence; and P(s|ei ), related to the linguistic elements used in     prisal:
this particular sentence to express ei .
   We call the first one semantic surprisal, and the second one                wt+1 = arg min −logP(w|DSS, w0 , ..., wt )
linguistic surprisal (cf. Frank & Vigliocco, 2011). Semantic                              w
surprisal represents how unexpected the events conveyed by
                                                                    where w is a word in the vocabulary and DSS is the semantic
the sentence are. Linguistic surprisal can be seen as the in-
                                                                    representation related to eα . This is already implemented by
formation that the sentence conveys, given that the semantics
                                                                    the model described by Calvillo et al. (2016), where the word
is already known; thus, it is not information about the world,
                                                                    produced at each time step is the one with highest conditional
but about the sentence itself.
                                                                    probability given the semantics and the previously produced
   These two types of information cannot be easily disen-
                                                                    words. In our model these probabilities are obtained at the
tangled because they are embedded in each sentence/event.
                                                                    Probabilities layer in Figure 1.
Knowing the identity of an event gives information about
the possible related sentences, and vice versa. Nonetheless,
                                                                    Being Fast
based on our definition, we can express total semantic sur-
prisal of a sentence s as:                                          The information contained by a sentence results from the sum
                                                                    of the information contained by each of its words. Thus,
                  SemSurp(s) = −log∑ P(ei )                         knowing that the semantic surprisal related to eα should sum
                                         i
                                                                    up to −logP(eα ), and that this information is distributed
where ei is each event that can be expressed by s.                  among the words in the sentence, we can calculate average
   While one sentence can be paired with several events, nor-       word semantic information/surprisal with respect to eα :
mally when a speaker produces a sentence, he/she has one
specific event in mind eα . Thus, while total semantic surprisal                                           −logP(eα )
                                                                                  E[WordSemSurpeα ] =
is as described above, the semantic information/surprisal that                                                   n
the speaker is trying to communicate is only:
                                                                    where n is the number of words in the sentence. Hence, if
                            −logP(eα )                              one wants to maximize average semantic information trans-
                                                                    mission of the desired event eα , it suffices to minimize n.
   As a result, the relevant information associated with a spe-        We hypothesize that in general speakers tend to maximize
cific sentence s assuming that the speaker is trying to com-        information transmission of the desired semantics eα by min-
municate the event eα is given by:                                  imizing n, and therefore by favoring shorter sentences.
                                                                       The model presented minimizes sentence lengths by esti-
                 Surpeα (s) = −logP(s|eα )P(eα )                    mating at each time step a score that reflects the expected
                                                                    derivation length that would follow the production of a cer-
                   = −logP(s|eα ) − logP(eα )                       tain word. This is done by the second path shown in Figure
                                                                    1, below. This path is constituted by a hidden recurrent layer
where the semantic information −logP(eα ) remains constant
                                                                    followed by a softmax layer. The recurrent layer contains 30
across all different surface realizations that could convey it;
                                                                    sigmoid units and receives as input the DSS semantic repre-
in contrast to the linguistic information −logP(s|eα ), which
                                                                    sentation, the output of probabilities, and its own activation
can vary widely depending on the specific syntactic structures
                                                                    at time step t − 1 (zeros at t = 0). Activation of this layer
or words that the speaker chooses.
                                                                    is then propagated to a softmax layer (der lengths) with di-
Being Easy to Produce                                               mensionality equal to the size of the vocabulary(43), and that
                                                                    calculates for each word a probability value DL, where values
Surprisal Theory (Hale, 2001; Levy, 2008) states that the cog-
                                                                    closer to 0 represent longer derivations and values closer to 1
nitive effort associated to the processing of a word is pro-
                                                                    represent shorter derivations, and where probability mass is
portional to its surprisal. Evidence supporting this has been
                                                                    distributed among all words that can be produced at the given
shown for comprehension (e.g., Hale, 2001; Levy, 2008), and
                                                                    time step. Finally, these layers receive also input from a bias
production (e.g., Griffin & Bock, 1998). Therefore, one can
                                                                    unit with a constant activation of 1.
assume that a rational model of production would try to min-
                                                                       A model that produces at each time step the word that
imize effort for both interlocutors.
                                                                    maximizes this score would prefer words leading to shorter
   While comprehension effort is minimized following a UID
                                                                    derivations, regardless of their information content:
strategy, production effort can be minimized by following
an Availability Based Production strategy (ABP, Ferreira &
Dell, 2000), where items are produced as they are available.                 wt+1 = arg max DL(w|DSS, probabilitiest+1 )
                                                                                        w
                                                                1711

Being Easy to Comprehend                                                 information, we compute a probability distribution over the
A model combining the previous two strategies would pro-                 vocabulary that reflects the length of the sentences that one
duce sentences with more uniform surprisal profiles, com-                can expect after producing a particular word.
pared to a model that only applies one of them. However,                    Given a DSS representation and a derivation point, for each
these strategies do not take into account that world events              possible word production wi , we get its minimum derivation
with high surprisal represent higher comprehension effort.               length min dl(wi ), which is the length of the shortest sen-
   Speakers know beforehand how unexpected the event they                tence that can be produced if wi is produced. Afterwards we
are trying to communicate is. Therefore, one can propose                 calculate a score dl(wi ):
that they balance these two strategies according to this infor-
mation. That is, when the speaker is trying to communicate                         dl(wi ) = max{min dl(w)} − min dl(wi ) + 1
                                                                                               w
an event eα with low surprisal, the speaker would prefer to
be faster; but, when the event represents high surprisal, the            which is equal to the difference between the greatest min dl
speaker would prefer sentences with lower linguistic surprisal           value among all the words that can be currently produced and
and possibly longer. Thus, at each time step, the model would            the min dl associated to each specific word wi , plus 1. Fi-
produce the word that maximizes the score:                               nally, in order to have a proper distribution, we normalize by
                                                                         dividing by the sum over all the possible word continuations.
   wt+1 = arg max{(1 − P(eα ))P(w|...) + P(eα )DL(w|...)}                   These scores are the values expected at the output layer
               w
                                                                         of der lengths. According to these, all possible word pro-
   This final model is expected to produce sentences with                ductions at a specific derivation point have some probability
more uniform surprisal profiles, compared to strategies that             mass that is inversely proportional to the length of the shortest
only maximize one of these measures, or that do not take into            sentence that can be obtained by following that production.
account semantic surprisal.
   In our model this is computed at the words layer (see Fig-            Semantic Probability. For each DSS representation in the
ure 1), which receives P(w|...) values from the probabilities            examples set, a semantic probability value P(eα ) was com-
layer and DL(w|...) scores from the der lenghts layer. The               puted. Considering that the model is trained only on the pairs
value of P(eα ) is assumed to be known.                                  given in the examples set and that all sentences are presented
                                                                         an equal number of times during training, then the probability
                Training and Evaluation                                  of a DSS representation is given by the number of sentences
Examples Set                                                             related to that representation divided by the total number of
                                                                         sentences in the examples set.
We use the same examples set as Calvillo et al. (2016), which
                                                                            However, since P(eα ) is used to balance word probabilities
consist of a set of pairs {(DSS1 , ϕ1 ), . . . , (DSSn , ϕn ))} where
                                                                         and derivation lengths, less biased values are needed because
each DSSi ∈ [0, 1]45 is formed by a DSS representation plus              as it is, P(eα ) is in general very low, and 1 − P(eα ) is very
an extra bit that indicates whether the model must produce a             high. Therefore instead of normalizing by the total number
a passive sentence (0) or an active one (1); and ϕi is the set of        of sentences, normalization is done with respect to the high-
all the sentences that encode the information contained in the           est number of sentences that can be related to a DSS repre-
corresponding DSSi and in the expected voice.                            sentation, which is 130. Hence, for each DSS, its probability
   The sentences are those generated by the microlanguage                P(eα ), or henceforth P(DSS), is given by the number of sen-
defined by Frank et al. (2009) (see their Tables 5–8). This              tences paired with the representation, divided by 130.
microlanguage consists of 40 words that can be combined
into 13556 sentences according to its grammar. After adding              Training Procedure
determiners (a,the) and an end-of-sentence marker (.), there
                                                                         Since the output layer receives unmodified copies from prob-
were 43 words, which were encoded at the output layer
                                                                         abilities and der lengths, the connections from the latter to
probabilities in the form of localist vectors. After ruling out
                                                                         the former are fixed one-to-one and do not need training. In
sentences expressing situations that are not allowed by the
                                                                         other words, the ith unit of probabilities is only connected to
microworld, there were a total of 8201 sentences related to
                                                                         the ith unit of words with a connection weight fixed to 1, and
782 DSS representations.
                                                                         likewise for the connections between der lenghts and words.
   This set was used because it pairs each semantic repre-
                                                                            Prior to training, all weights on the projections between
sentation with several sentences, allowing to define different
                                                                         layers (with the exception of those mentioned in the last para-
ranking functions. In future work a new set could be defined
                                                                         graph) were initialized with random values drawn from a nor-
in order to assess more specific phenomena.
                                                                         mal distribution N (0, 0.1). Weights on the bias projections
Derivation Length Scores. For each DSS representation,                   were initially set to zero.
we know beforehand the sentences that can encode it accord-                 Training consists of setting the connection weights lead-
ing to the grammar. Furthermore, we know at each deriva-                 ing to the computation on the one hand of probabilities and
tion point what words can be produced and how long the sen-              on the other hand of der lengths, corresponding to the two
tences would be if a particular word is produced. Using this             paths of processing. Accordingly, training is performed in
                                                                     1712

two phases, in both cases using cross-entropy backpropaga-           time step the model produces the word with:
tion (Rumelhart, Hinton, & Williams, 1986) with weight up-
dates after each word in the sentence of each training item.         • Min Linguistic Surprisal
probabilities. The first phase corresponds to the training of        • Min Derivation Length
the path leading to probabilities, which is performed as de-
scribed by Calvillo et al. (2016), where the model is trained        • Max Word Probability +/* Derivation Length Score
to predict the next word given the semantic representation and       • Complete Model
the previously produced words.
   During this phase, the monitoring units were set at time             For each DSS representation in the examples set that was
t to what the model was supposed to produce at time t − 1            related to more than one sentence (968), the model generated
(zeros for t = 0). This reflects the notion that during train-       a sentence according to each production strategy.
ing the word contained in the training sentence at time-step            In order to measure surprisal, a language model was trained
t − 1 should be the one informing the next time step, regard-        implementing a Simple Recurrent Network (Elman, 1990).
less of the previously produced (and possibly different) word.       This model was trained on the whole set of sentences for 200
During production, the monitoring units are set to 1.0 for the       epochs with a learning rate of 0.24 which was halved each
word that was actually produced and 0.0 everywhere else.             time there was no improvement in performance. Using this
   This path was trained for a maximum of 200 epochs, each           language model, surprisal values were calculated for each one
one consisting of a full presentation of the training set, which     of the words of the produced sentences.
was randomized before each epoch. Note that each item of                Uniformity of information density was measured in terms
this set consisted of a DSSi paired with one of the possi-           of standard deviation of word surprisal, assuming that com-
ble sentence realizations describing the state of affairs rep-       plete uniformity would produce a standard deviation of 0.
resented in DSSi . Hence, during each epoch, the model saw
all the possible realizations of DSSi . An initial learning rate                      Results and Discussion
of 0.124 was used, which was halved each time there was no           The results can be seen in Table 2, where the columns denote
improvement of performance during 15 epochs. No momen-               respectively: production strategy, production accuracy (Acc)
tum was used. Training halted if the maximum number of               as defined by Calvillo et al. (2016) and denoting how precise
epochs was reached or if there was no performance improve-           the sentences convey the given semantics, average sentence
ment over a 40-epoch interval.                                       length (AvDL), average word surprisal (AvS), and standard
                                                                     deviation of surprisal (Std).
der lengths. The second path can be trained after the train-
ing of the first one is completed. During this phase, the con-
nection weights calculated during the first phase are fixed, so               Table 2: Results of each production strategy.
that only the second path weights are modified.
   At each time step, the DSS is fed into the first path, which                                Acc       AvDL    AvS    Std
outputs a probability distribution over the vocabulary. This                    Min LS         99.67      9.01   1.0    0.89
is fed into the second recurrence, as well as the DSS rep-                      Min DL         99.86      7.55   1.20   0.97
resentation. Monitoring units are handled exactly as in the                 Max P(+/*)DL       99.82      7.77   1.16   0.95
first training phase. The activation of the second recurrence                Max 3P-2DL        98.23     10.15   0.89   0.84
is then propagated to der lengths. Its output is compared to                   SemSurp         97.67     10.17   0.89   0.83
the derivation length values, as defined in the previous sec-
tion, and finally the connection weights are updated.                   As expected, minimizing linguistic surprisal (Min LS) led
   Training of this path was performed for a maximum of 80           to lower surprisal values compared to minimizing derivation
epochs, with the training items arranged in the same way as          lengths (Min DL). Combining these two strategies by a sum
in the previous phase. An initial learning rate of 0.24 was          or product led to results almost identical to each other, and
used, which was halved each time there was no improve-               very close to Min DL, suggesting that derivation length scores
ment of performance during 10 epochs. No momentum was                were mostly dominating production.
used. Training halted if the maximum number of epochs was               Given that linguistic surprisal and derivation lengths are
reached or if there was no performance improvement over a            different in nature, one can expect a more complex relation
20-epoch interval.                                                   between them in order for the resulting score to be helpful.
                                                                     Consequently, grid search was performed in order to find lin-
Evaluation                                                           ear factors that would minimize the standard deviation of sur-
                                                                     prisal. The resulting model corresponds to the fourth row in
The model presented defines a production strategy as an in-          Table 2, where the model produces at each time step the word
teraction between production goals. Thus, in order to assess         that maximizes:
the model, its productions were compared to those obtained
by using the following alternative strategies, where at each               3P(w|DSS, w0, .., wn ) − 2DL(w|DSS, probabilities)
                                                                 1713

where one can see that minimizing linguistic surprisal is fa-                                 References
vored, while minimizing derivation lengths is penalized. As          Aylett, M., & Turk, A. (2004). The smooth signal redundancy
a result the sentences produced are longer than only minimiz-          hypothesis: A functional explanation for relationships be-
ing linguistic surprisal. However, uniformity of information           tween redundancy, prosodic prominence, and duration in
density is higher than with the previous models and addition-          spontaneous speech. Language and speech, 47(1), 31–56.
ally average surprisal is lowest.                                    Bell, A., Jurafsky, D., Fosler-Lussier, E., Girand, C., Gre-
   The final row in Table 2 presents the results of the model          gory, M., & Gildea, D. (2003). Effects of disfluencies,
that incorporates semantic probabilities. For this case grid           predictability, and utterance position on word form varia-
search was also used, which led to a model that at each time           tion in english conversation. The Journal of the Acoustical
step produces the word that maximizes:                                 Society of America, 113(2), 1001–1024.
      (3.5 − P(DSS))P(w|...) + (P(DSS) − 2.5)DL(w|...)               Calvillo, J., Brouwer, H., & Crocker, M. W. (2016). Connec-
                                                                       tionist semantic systematicity in language production. In
which is very similar to the previous model, but with some in-         Proceedings of the 38th annual conference of the cognitive
fluence from semantic probabilities. While the performance             science society.
of this model is very similar to the previous one, its sentences     Elman, J. L. (1990). Finding structure in time. Cognitive
present slightly higher uniformity of information density; and         Science, 14(2), 179–211.
the influence of semantic surprisal is in the expected direc-        Ferreira, V. S., & Dell, G. S. (2000). Effect of ambiguity
tion, where semantics with high surprisal produce longer sen-          and lexical availability on syntactic and lexical production.
tences and vice versa.                                                 Cognitive psychology, 40(4), 296–340.
   The small difference between the last two strategies could        Frank, S., Haselager, W. F. G., & van Rooij, I. (2009). Con-
be caused by the nature of the language model, which re-               nectionist semantic systematicity. Cognition, 110(3), 358–
ceives no semantic information during training, which means            379.
that rather than being a joint model of semantics and sen-           Frank, S., Koppen, M., Noordman, L. G. M., & Vonk, W.
tences, it only considers word sequences. Furthermore, the             (2003). Modeling knowledge-based inferences in story
production model here proposed uses semantic surprisal at a            comprehension. Cognitive Science, 27(6), 875–910.
sentence level, while speakers can be sensitive to this infor-       Frank, S., & Vigliocco, G. (2011). Sentence comprehension
mation incrementally at a word level. These issues will be             as mental simulation: an information-theoretic perspective.
addressed in future work.                                              Information, 2(4), 672–696.
   In general the model outlined here shows: first, that as ex-      Griffin, Z. M., & Bock, K. (1998). Constraint, word fre-
pected, shorter sentences are more dense in terms of informa-          quency, and the relationship between lexical processing
tion content. Second, that longer sentences present informa-           levels in spoken word production. Journal of Memory and
tion in a more uniform way. Third, that sentences with more            Language, 38(3), 313 - 338.
uniform information densities present in average lower word          Hale, J. (2001). A probabilistic earley parser as a psy-
surprisal, therefore minimizing comprehension effort. And              cholinguistic model. In Proceedings of the second meet-
finally and most importantly, that sentences with higher uni-          ing of the north american chapter of the association for
formity of information density can be produced by balancing            computational linguistics on language technologies (pp. 1–
sentence lengths and word probabilities. In future work, this          8). Stroudsburg, PA, USA: Association for Computational
can help to address uniformity for a given channel capacity.           Linguistics.
                                                                     Jaeger, T. F. (2006). Redundancy and syntactic reduction
                         Conclusion                                    in spontaneous speech. Unpublished doctoral dissertation,
This article presents a model of language production that              Stanford University.
takes into account word probabilities and sentence lengths           Jaeger, T. F. (2010). Redundancy and reduction: Speakers
in order to produce sentences with uniform surprisal pro-              manage syntactic information density. Cognitive psychol-
files, and in order to model the Uniform Information Den-              ogy, 61(1), 23–62.
sity Hypothesis. The sentences produced by this model were           Levy, R. (2008). Expectation-based syntactic comprehen-
compared to those produced using other strategies, showing             sion. Cognition, 106(3), 1126 - 1177.
that the proposed model produces sentences with more uni-            Levy, R., & Jaeger, T. F. (2007). Speakers optimize infor-
form surprisal profiles and lower average word surprisal. This         mation density through syntactic reduction. Advances in
model represents a first attempt to model the Uniform Infor-           neural information processing systems, 19, 849.
mation Density Hypothesis at the algorithmic level, where            Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986).
uniformity arises by balancing word probabilities and sen-             Learning representations by back-propagating errors. Na-
tence lengths in a mechanistic way.                                    ture, 323(6088), 533–536.
                   Acknowledgements
I would like to thank Matthew W. Crocker and Harm Brouwer
for their helpful comments while writing this article.
                                                                 1714

