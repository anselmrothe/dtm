                               A Two-Step Signal Detection Model of Belief Bias
                                           Rachel G. Stephens (r.stephens@unsw.edu.au)
                                          School of Psychology, University of New South Wales
                                                          Sydney, NSW 2052 Australia
                                                John C. Dunn (john.dunn@uwa.edu.au)
                                    School of Psychological Science, University of Western Australia
                                                           Perth, WA 6009 Australia
                                                Brett K. Hayes (b.hayes@unsw.edu.au)
                                          School of Psychology, University of New South Wales
                                                          Sydney, NSW 2052 Australia
                               Abstract                                 2010; Evans, Barston, & Pollard, 1983; Newstead, Pollard,
   When asked to assess the deductive validity of an argument,
                                                                        Evans, & Allen, 1992; Roberts & Sykes, 2003).
   people are influenced by their prior knowledge of the content.
   Recently, two competing explanations for this belief bias                               Table 1: Sample syllogisms.
   effect have been proposed, each based on signal detection
   theory. Under a response bias explanation, people set more                      Believable                  Unbelievable
   lenient decision criteria for believable than for unbelievable         Valid    No beers are krabbers.      No drinks are krabbers.
   arguments. Alternatively, believable and unbelievable                           Some krabbers are           Some krabbers are
   arguments may differ in subjective argument strength for both                   drinks.                     beers.
   valid and invalid items. Two experiments tested these                           Some drinks are not         Some beers are not
   accounts by asking participants to assess the validity of                       beers.                      drinks.
   categorical syllogisms and rate their confidence. Conclusion-
   believability was manipulated either within- or between-               Invalid  No drinks are krabbers.     No beers are krabbers.
   groups. A two-step signal detection model was applied to                        Some krabbers are           Some krabbers are
   examine the effects on the relative location of the decision                    beers.                      drinks.
   threshold and the distributions of argument strength.                           Some drinks are not         Some beers are not
   Equivalent belief bias effects were found when believability                    beers.                      drinks.
   was manipulated within- and between-groups, supporting the
   view that the belief bias effect is due to response bias.               Such effects are often seen as evidence that believability
   Keywords: belief bias; deductive reasoning; signal detection
                                                                        affects the quality of deductive reasoning – people’s ability
   theory; response bias                                                to distinguish valid from invalid arguments (see Dube et al.,
                                                                        2010 for a review). Theoretical accounts such as the
                          Introduction                                  selective scrutiny model (Evans et al., 1983), misinterpreted
                                                                        necessity model (e.g., Markovits & Nantel, 1989; Newstead
An important phenomenon for theories of reasoning is that               et al., 1992) or the mental models approach (e.g., Oakhill,
people show a belief bias when asked to assess the logical              Johnson-Laird, & Garnham, 1989) propose explanations in
validity of arguments. The tendency to accept or reject a               which believability affects how validity is evaluated.
conclusion as valid is not based purely on logical structure               However, deciding whether an argument is valid also
but is also swayed by its compatibility with prior knowledge            involves response bias – the willingness to endorse the
(e.g., Evans, Newstead, & Byrne, 1993; Markovits &                      argument, regardless of one’s ability to discriminate valid
Nantel, 1989; Shynkaruk & Thompson, 2006). Table 1                      and invalid forms. Controversially, recent work has used
shows typical stimuli – categorical syllogisms – in which               confidence ratings and signal detection theory to show that
the validity of the argument is crossed with the believability          belief bias only reflects changes in response bias. That is,
of the conclusion. In the validity discrimination task,                 people are more willing to respond “valid” for believable
participants are asked to judge whether the conclusion                  arguments (Dube et al., 2010; Trippas et al., 2014). In this
below the line necessarily follows from the premises above              view, believability does not change one’s subjective
the line. Key findings based on arguments like these are that           evaluation of argument validity.
people are more likely to endorse valid than invalid                       In reaction to this response bias account, it has been
arguments, but they are also more likely to endorse                     suggested that data patterns consistent with changes in
arguments with believable than with unbelievable                        response bias can also be explained by believability
conclusions. In many cases these factors also interact; for             affecting the subjective strength of both valid and invalid
example, the difference between the acceptance rates of                 arguments (Klauer & Kellen, 2011; Singmann & Kellen,
valid and invalid arguments is often greater for unbelievable           2014). Under this alternative argument strength account, if
than for believable arguments (e.g., Dube, Rotello, & Heit,             an argument has a believable conclusion (whether valid or
                                                                    1138

invalid) then it will be viewed as more logically valid and         area under the invalid distribution to the right of the
thus garner more endorsements.                                      threshold. Two important ways that performance can change
   In adjudicating between these accounts, a key                    is by the threshold shifting (i.e., changes in response bias),
consideration is that believability is usually manipulated          and/or the valid distribution shifting relative to the invalid
within a single experimental session. The argument strength         distribution (i.e., changes in discriminability or sensitivity).
account is consistent with evidence that response bias is
unlikely to change from trial to trial (e.g., Stretch & Wixted,
1998). However, to our knowledge it is currently unknown
how believability affects performance if instead it is
manipulated between different groups of participants, where
response bias is free to differ. As we explain below, we
hypothesized that if the response bias account is correct then
the same belief bias effects on model parameters should
appear when believability is manipulated within groups and
between groups (i.e., equivalent ordinal effects on response
bias and no effects on discriminability).                                     Figure 1: Standard signal detection model.
   Given the important implications for theories of
reasoning, we aimed to extend the investigation of response            Adding confidence judgments to the validity
bias in deductive reasoning. We took three key steps. First,        discrimination task allows for a more fine-grained analysis
we sought to replicate the within-group findings of Dube et         of changes in signal detection parameters. It is assumed that
al. (2010) – including confidence ratings – that they used to       people set a response threshold for n-1 response options on
support the response bias account. Second, we applied an            the confidence scale – five are shown in Figure 1 for a six-
extended signal detection model that was specifically               point confidence scale. Performance can then be examined
tailored to the two-step task in which participants first make      using receiver operating characteristic (ROC) curves, which
a binary valid/invalid decision, then rate their confidence.        plot hit rates against false alarm rates at different confidence
Our goal was to confirm whether such a model would still            levels (see examples in Figure 2). Evidence for a difference
suggest that believability does not affect accuracy, but that       in the discriminability of valid and invalid arguments would
the response bias or argument strength accounts are                 be suggested by points from two conditions falling on
required. Third, to avoid the issue of whether response bias        different curves. Better discrimination is suggested by ROC
can change trial-by-trial, in a second experiment we                curves that fall further from the diagonal, towards the upper
manipulated believability between groups. Our goal was to           left – hit rates are higher relative to false alarm rates. In
examine whether the key effects generalized to this design,         contrast, conventional evidence for a difference in response
which would support the response bias account.                      bias is suggested by points from two conditions falling on
   To this end, in the following sections, we outline (a) how       different positions along the same curve. A more lenient
signal detection theory can be applied to deductive                 threshold is suggested by points sitting further towards the
reasoning, (b) the novel two-step signal detection model,           right, corresponding to both higher hit rates and higher false
and (c) two experiments that manipulate believability within        alarm rates. Signal detection models can be fit to ROC
or between groups, to which we apply the model.                     curves to test for changes in argument discrimination or
                                                                    response bias, which would be supported by reductions in fit
      Signal Detection Theory and Belief Bias                       due to constraining either the relative location of the valid
Signal detection theory (SDT) is a useful framework to              distribution or the criteria, respectively.
examine belief bias because it allows us to separate changes
in discriminability (i.e., differentiating valid and invalid
arguments) versus response bias (i.e., the “decision stage”;
cf. Dube et al., 2010; Rotello & Heit, 2009). In this
framework, arguments fall along a continuum of subjective
argument strength, with distinct Gaussian distributions for
valid and invalid arguments, as shown in Figure 1. The
distance between the means of these distributions reflects
how well people can distinguish valid and invalid
arguments. People also set a response threshold along the
continuum, endorsing any argument that exceeds it in
strength (i.e., the tallest “Invalid”/”Valid” threshold in the
figure). Thus the hit rate (endorsement rate for valid
arguments) is given by the area under the valid distribution
to the right of the threshold, and the false alarm rate
(endorsement rate for invalid arguments) is given by the                    Figure 2: ROC curves from Dube et al. (2010).
                                                                1139

   An important but controversial result was reported by                Resolving this debate has been difficult because in many
Dube et al. (2010), who compared and fit ROC curves for              of the key studies (e.g., Dube et al., 2010; Trippas et al.,
believable and unbelievable syllogisms like those in Table           2014), believability has been manipulated within a block of
1. Their ROC model fitting showed that argument                      arguments. The response bias account assumes that people
believability affected response bias but did not affect              will shift their criteria on a trial-by-trial basis, depending on
discriminability. Participants were simply more willing to           whether an argument is believable or unbelievable.
endorse believable arguments (see Figure 2). This response           However, this assumption is controversial. In the
bias account of belief bias is illustrated in the top panel of       recognition memory literature, although trial-by-trial shifts
Figure 3. Here there are two distributions – one for invalid         in criteria are possible, it appears that often this does not
and one for valid arguments – but two sets of decision               occur (Rotello & Macmillan, 2007; Starns & Olchowski,
thresholds – a more lenient set for believable arguments,            2015; Stretch & Wixted, 1998). One way to address this
and a more conservative set for unbelievable arguments               issue is to manipulate believability between participants.
(only three criteria per set are shown, to avoid clutter). A         Uncontroversially, different groups are then free to set
similar account has been proposed for belief bias in causal          different response criteria.
conditional arguments such as modus ponens (Trippas et al.,             In order to resolve whether belief bias is driven by
2014).                                                               changes in response bias or argument strength, we carried
   However, this is not the only way to interpret overlapping        out two experiments and tested a new signal detection
ROC curves. The response bias interpretation has been                model of reasoning. Experiment 1 confirmed that we could
contested because an alternative argument strength account           replicate the ROC shifts found by Dube et al. (2010), with
is possible, as illustrated in the bottom panel of Figure 3          believability manipulated within-participants. In Experiment
(Klauer & Kellen, 2011; Singmann & Kellen, 2014). This               2, we investigated whether the same effects appeared when
approach assumes a single fixed set of decision thresholds,          believability was manipulated between groups. If the
but four different distributions – distinct invalid and valid        response bias account is correct, then the same distributions
distributions for both unbelievable and believable                   of response strength for valid and invalid arguments should
arguments. Discriminability is assumed to be the same for            apply to those seeing only believable or unbelievable
believable and unbelievable arguments, but the believable-           arguments (because there are only two distributions), but the
valid AND believable-invalid distributions are shifted to the        groups will differ in response criteria. Therefore, we would
right (i.e., they are stronger on average).                          see different hit rates and false alarm rates for the believable
                                                                     and unbelievable argument groups, replicating the Dube et
                                                                     al. (2010) ROC shifts and differences in the response
                                                                     criterion parameter based on model fitting.
                                                                        Alternatively, if the argument strength account is correct,
                                                                     then the pair of invalid and valid distributions would be in
                                                                     different locations for believable and unbelievable groups.
                                                                     However, each group would be free to set criteria relative to
                                                                     the locations of their invalid and valid distributions – each
                                                                     group has no reason to adopt criteria that are in different
                                                                     locations relative to their distributions. Therefore, we would
                                                                     see the same hit and false alarm rates for both groups, with
                                                                     no ROC shifts nor differences in the criterion parameter.
                                                                        Accurately testing the competing accounts of belief bias
                                                                     requires model fitting with a model that properly captures
                                                                     the task. Therefore, we extended the signal detection model
                                                                     developed by Dube et al (2010), to treat the valid/invalid
                                                                     decision and confidence judgments as two separate steps.
                                                                     As outlined below, this kind of model is more appropriate
                                                                     for the two-step task than a traditional signal detection
                                                                     model (Moran, Teodorescu, & Usher, 2015). We first
                                                                     present the model. We then report experiments using
                                                                     within- and between-participant manipulations of
Figure 3: (a) The response bias account. There are fixed             conclusion believability and fit the model to these data.
invalid (I) and valid (V) distributions. Criteria are shifted to
the left for believable arguments (black lines) relative to                    Two-Step Signal Detection Model
unbelievable arguments (grey lines). (b) The argument
strength account. There are fixed criteria. Invalid-believable       In the two-step validity discrimination task that we use,
(IB) and valid-believable (VB) distributions are shifted to          participants make a “valid”/”invalid” decision, and then rate
the right, relative to the invalid-unbelievable (IU) and valid-      their confidence. Despite the sequential nature of these
unbelievable (VU) distributions.                                     judgments, in the standard procedure for generating
                                                                 1140

empirical ROC curves, data from the response categories are                 Method
recoded to form a single scale with judgments that range                    Participants. One-hundred-and-seventeen students (30
from high-confidence “valid” to low-confidence “valid”,                     males) at the University of New South Wales, Sydney,
then low-confidence “invalid” to high-confidence “invalid”                  participated for course credit. Mean age was 18.8 years (SD
(e.g., Dube et al., 2010; Trippas et al., 2014). Typically,                 = 2.3). Participants were randomly allocated to Experiment
these ROC curves are then fit using the standard single-step                1 (N = 38) or one of the groups in Experiment 2 (believable
SDT model that we outlined above, with a criterion                          N = 40, unbelievable N = 39).
parameter separating each adjacent pair of recoded
confidence levels. However, visual inspection of these                      Stimuli. In Experiment 1, participants evaluated 64
empirical ROC curves suggests that they differ from the                     arguments across two blocks of 32 trials, with 16 believable
smooth concave curve typically found – they instead exhibit                 and 16 unbelievable arguments per block – half of which
a “hinge” or “elbow” where valid and invalid response                       were valid in each case. In Experiment 2, participants
categories join, as apparent in Figure 2, particularly for the              evaluated either 32 believable or 32 unbelievable arguments
unbelievable-ROC. In order to successfully model this                       (half valid).
feature, the standard SDT model was extended to                                Example stimuli are shown in Table 1. The arguments
incorporate changes in evidence accumulation and                            were based on those of Experiment 2 by Dube et al. (2010),
variability in the period between the initial validity                      and were constructed using their 16 syllogistic problem
judgment and the subsequent confidence judgment.                            frames (e.g., All X are Y; Some Z are not Y; Therefore some
   The two-step SDT model is similar to a standard SDT                      Z are not X). Half were valid and half were invalid. Each
model with the exception that confidence judgments are                      problem frame had the conclusion structure, Some Z are not
based on a noisy version of the evidence value on which the
                                               x ~ N  , 
                                                                            X (or Some X are not Z), and was assigned content involving
validity judgment was made. Let                                 be the      a category-exemplar relationship (e.g., drinks-beers, dogs-
strength of given argument. Let c be a decision criterion                   poodles, plants-weeds).
such that if x  c , respond “valid”, else respond “invalid”.                  Conclusion believability was manipulated by simply
                                                                            reversing the order of the category and exemplar (e.g., Some
We propose that a confidence judgment is based on x*, a
                                                                            drinks are not beers vs. Some beers are not drinks). We
noisy memory trace of argument strength, x. That is,
 x*  x  x , for x ~ N  ,  . If   0 then additional
                                                                            verified the believability of the conclusion statements in a
                                                                            separate study by 34 people drawn from a similar population
argument strength is accumulated in the interval between                    to the main experiments. Based on ratings on a 5-point scale
the two decisions (cf., Moran et al., 2015). Suppose, there                 (1 = unbelievable, 3 = neutral, 5 = believable), the 32
are k confidence categories labeled, in sequence, from most                 statement pairs with the most extreme average ratings were
confident to least confident. Then, associated with these                   selected from a set of 38 pairs (Believable: M = 4.95, SD =
category labels is set of points on the strength continuum,                 0.09; Unbelievable: M = 1.59, SD = 0.35). To minimize the
U  u0 , u1 , , uk                   u  u1   uk u0                  effects of premise believability, the premises included a
                        , such that, 0                     ,          ,     nonsense term (e.g., krabbers, junids).
 uk                                V  v0 , v1 , , vk                      The semantic content was split into four subsets of eight
         , and a set of points,                            , such that,
                                                                            category-exemplar pairs, so the content could be assigned to
 v0  v1   vk v0   vk  
                    ,         ,              . Then, if the response        all       four      believability-by-validity       conditions,
                     ui  x*  ui 1                                        counterbalanced across participants. Experiment 1
is “invalid” and                      or, if the response is “valid”        participants (believable and unbelievable within-
     vi  x*  vi 1                                                        participants) saw the category-exemplar content once per
and                   then respond with the ith category label.
                                                                            block and the 16 problem frames twice per block (once as
   The hypotheses of interest were primarily tested by
                                                                            believable and once as unbelievable versions), forming the
comparing the fits of nested versions of this model using the
                                                                            64 arguments over two blocks. Content assignment was
likelihood ratio test. Although the response bias and
                                                                            controlled for this group so that in the second block, each
argument strength accounts are formally identical for a
                                                                            participant saw the same content in the same problem
traditional signal detection model, this is not strictly true for
                                                                            structures as in their first block, but with conclusion
the two-step model. Therefore, both accounts can be tested
                                                                            believability reversed. At the start of the second block, these
when believability is manipulated within-participants.
                                                                            participants were warned that there would be similar content
                                                                            but the specific arguments would be different. Experiment 2
                           Experiments                                      participants (believable-only and unbelievable-only groups)
In two experiments, participants evaluated the validity of                  saw each category-exemplar content once and the 16
categorical syllogisms, which included logically valid and                  problem frames twice, forming the 32 arguments.
invalid arguments with believable or unbelievable                              Before beginning the experiment, all participants received
conclusions in a 2x2 design. Experiments 1 and 2                            two valid and two invalid practice problems with abstract
manipulated believability within- and between-groups,                       content (e.g., “All M are P…”) and different structures that
respectively.                                                               were not included in the main task.
                                                                        1141

Procedure. Participants were shown the set of arguments in
random order, presented one-by-one on a computer, with a
line separating the conclusion from the premises. The
instructions asked participants to assume that the premises
were true and assess whether the conclusion logically
followed from them. Valid arguments were defined as those
for which the sentence below the line was necessarily true,
given that the information above the line was true (and
invalid = not necessarily true). Participants were told that
the arguments would contain a nonsense word. A trial
counter was presented at the top left corner of the screen.
Participants clicked on either the “Valid” or “Invalid”
button presented underneath a given argument, then rated
their confidence on a scale that appeared, ranging from 50
(Guessing) to 100 (Certain) in increments of ten.
Results
Both experiments replicated previously observed argument
endorsement patterns and belief bias effects (see Table 2;
e.g., Dube et al., 2010; Evans et al., 1983; Newstead et al.,
1992). Analysis of variance (ANOVA) revealed that
participants endorsed (i.e., responded “valid”) valid
arguments more often than invalid arguments: Experiment
1, F(1, 37) = 64.28, p < .001, η2 = .35; Experiment 2, F(1,
77) = 127.24, p < .001, η2 = .40. Participants endorsed
believable arguments more often than unbelievable
arguments: Experiment 1, F(1, 37) = 38.59, p < .001, η2 =
.12; Experiment 2, F(1, 77) = 19.92, p < .001, η2 = .13.
Notably, as shown in the Table, there was a larger
difference between the acceptance rates of valid and invalid      Figure 4: Observed ROC curves (Obs) and expected scores
arguments for unbelievable than for believable arguments:         from the unconstrained model (Exp), for Experiments 1 and
Experiment 1, F(1, 37) = 5.50, p = .02, η2 = .01; Experiment      2 (panels a and b, respectively).
2, F(1, 77) = 9.81, p = .002, η2 = .05.
                                                                     We compared this unconstrained model against two
   Table 2: Performance in Experiments 1 and 2. Hit rate is       nested models: a constant discriminability model and a
   p(“Valid”|Valid); False alarm rate is p(“Valid”|Invalid).      constant criterion model in which (respectively)
                                                                  discriminability or the “valid”/”invalid” decision criterion
   Experiment    Condition        Hit rate   False alarm rate     for the initial binary judgment was constrained across
        1        Believeable        0.83           0.56           believable and unbelievable conditions. For both
                 Unbelievable       0.71           0.37           experiments, the fit of the constant discriminability model
        2        Believeable        0.82           0.58           did not significantly differ from that of the unconstrained
                 Unbelievable       0.75           0.34           model: Experiment 1, G2(1) = 0.23, p = .63; Experiment 2,
                                                                  G2(1) = 0.001, p = .97. This shows that, in line with Dube et
   The ROC curves for each experiment are presented in            al. (2010), discriminability did not differ between
Figure 4 (unfilled points). Both show effects that are            believability conditions.
consistent with shifts in response criteria and comparable to        The constant criterion model led to a reduction in fit
Dube et al. (2010; cf. Figure 2), although we used more           compared to the unconstrained model: Experiment 1, G2(1)
confidence response options. In each experiment, the points       = 47.89, p < .001; Experiment 2, G2(1) = 77.75, p < .001.
for believable and unbelievable arguments fall on similar         This indicates that, in line with the response bias account,
curves, though the believable points are shifted further to       the “valid”/“invalid” decision threshold differed between
the top-right corner than the unbelievable points.                believability conditions. Importantly, this was true both
   We first fit an unconstrained two-step signal detection        when believability was manipulated within-groups
model to each experiment. As shown by the filled points in        (Experiment 1) and between-groups (Experiment 2).
Figure 4, the predicted ROC points correspond reasonably             When a (non-nested) variant of the two-step model was
well with the empirical results for both experiments, though      applied to Experiment 1 that allowed the believable
there are some small departures for Experiment 1:                 distributions to shift (i.e., the argument strength account),
Experiment 1, G2(12) = 22.54, p = .03; Experiment 2,              we found that it also provided a satisfactory fit to the data:
G2(12) = 15.83, p = .20.                                          G2(20) = 30.18, p = 0.07. In other words, an argument
                                                              1142

strength account of belief bias could also explain the                Dube, C., Rotello, C. M., & Heit, E. (2010). Assessing the
Experiment 1 data. Such a model cannot sensibly be applied              belief bias effect with ROCs: It's a response bias effect.
to Experiment 2. Nevertheless, as we argued above, the                  Psychological Review, 117(3), 831-863.
response bias account can more readily explain belief bias            Evans, J. St. B. T., Barston, J. L., & Pollard, P. (1983). On
effects that occur between-groups.                                      the conflict between logic and belief in syllogistic
                                                                        reasoning. Memory & Cognition, 11, 295–306.
                         Discussion                                   Evans, J. St. B. T., Newstead, S. E., & Byrne, R. M. J.
We investigated whether belief bias effects in deductive                (1993). Human reasoning: The psychology of deduction.
reasoning could be explained as a response bias effect.                 Hove, UK: Lawrence Erlbaum Associates Ltd.
Experiment 1 replicated the belief bias effects of Dube et al.        Klauer, K. C., & Kellen, D. (2011). Assessing the belief
(2010), with conclusion believability manipulated within-               bias effect with ROCs: Reply to Dube, Rotello, and Heit
block. We applied a new two-step signal detection model to              (2010). Psychological Review, 118(1), 164-173.
better suit the two-step task, and confirmed that belief bias         Markovits, H., & Nantel, G. (1989). The belief-bias effect in
effects are consistent with a shift in response bias, rather            the production and evaluation of logical conclusions.
than discriminability. Experiment 2 extended the same                   Memory & Cognition, 17, 11–17.
results to an equivalent task with believability manipulated          Moran, R., Teodorescu, A. R., & Usher, M. (2015). Post
between-groups.                                                         choice information integration as a causal determinant of
   Under the response bias account (Dube et al., 2010;                  confidence: Novel data and a computational account.
Trippas et al., 2014), this pattern is explained by a shift in          Cognitive Psychology, 78, 99–147.
decision threshold, such that there is a more lenient criterion       Newstead, S. E., Pollard, P., Evans, J. St. B. T., & Allen, J.
for believable conclusions. Under the argument strength                 L. (1992). The source of belief bias effects in syllogistic
account (Klauer & Kellen, 2011; Singmann & Kellen,                      reasoning. Cognition, 45, 257–284.
2014), the belief bias effect reflects higher mean strength for       Oakhill, J., Johnson-Laird, P. N., & Garnham, A. (1989).
believable-valid and believable-invalid arguments than for              Believability and syllogistic reasoning. Cognition, 31,
unbelievable-valid and unbelievable-invalid arguments.                  117–140.
   It could be argued that participants in Experiment 1 were          Roberts, M. J., & Sykes, E. D. A. (2003). Belief bias and
unlikely to change their criteria trial-to-trial for different          relational reasoning. The Quarterly Journal of
levels of believability, favoring the argument strength                 Experimental Psychology: Section A, 56, 131–154.
account. However, this account would have difficulty with             Rotello, C. M. & Heit, E. (2009). Modeling the effects of
Experiment 2, where participants saw only believable or                 argument length and validity on inductive and deductive
only unbelievable arguments. There, the two groups had no               reasoning. Journal of Experimental Psychology:
reason to position their criteria in different locations relative       Learning, Memory, and Cognition, 35, 1317-1330.
to their distributions. Thus if belief bias primarily reflects a      Rotello, C. M., & Macmillan, N. A. (2007). Response bias
change in argument strength, the belief bias effects should             in recognition memory. The Psychology of Learning and
have disappeared. The fact that they did not suggests that              Motivation, 48, 61-94.
the most plausible explanation of belief bias in the current          Singmann, H., & Kellen, D. (2014). Concerns with the SDT
data sets is a change in response bias.                                 approach to causal conditional reasoning: A comment on
   Therefore, addressing the debate between response bias               Trippas, Handley, Verde, Roser, McNair, and Evans
and argument strength accounts of belief bias, we agree that            (2014). Frontiers in Psychology, 5, 402.
believable conclusions are most likely to affect the decision         Shynkaruk, J. M., & Thompson, V. A. (2006). Confidence
stage, lowering the decision threshold rather than appearing            and accuracy in deductive reasoning. Memory &
more logically valid. Just as people may require stronger               Cognition, 34, 619–632.
evidence to endorse that an unusual event occurred (Starns            Starns, J. J., & Olchowski, J. E. (2015). Shifting the
& Olchowski, 2015), it seems that people also require                   criterion is not the difficult part of trial-by-trial criterion
stronger evidence to endorse a syllogism with an                        shifts in recognition memory. Memory & Cognition, 43,
unbelievable conclusion. As Dube et al. (2010) concluded,               49-59.
this is problematic for theories of reasoning that propose            Stretch, V., & Wixted, J. T. (1998). On the difference
that believability affects the process of evaluating validity           between strength-based and frequency-based mirror
(e.g., Evans et al., 1983; Markovits & Nantel, 1989;                    effects in recognition memory. Journal of Experimental
Newstead et al., 1992; Oakhill et al., 1989). Future work               Psychology: Learning, Memory, and Cognition, 24, 1379-
should address whether the same findings generalize to                  1396.
other reasoning problems such as causal conditionals.                 Trippas, D., Verde, M. F., Handley, S. J., Roser, M. E.,
                                                                        McNair, N. A., & Evans, J. St. B. T. (2014). Modeling
                                                                        causal conditional reasoning data using SDT: Caveats and
                         References
                                                                        new insights. Frontiers in Psychology, 5, 217.
Benjamin, A. S. (2001). On the dual effects of repetition on
   false recognition. Journal of Experimental Psychology:
   Learning, Memory, and Cognition, 27, 941–947.
                                                                  1143

