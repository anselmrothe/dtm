   Leveraging Response Consistency within Individuals to Improve Group Accuracy
                                              for Rank-Ordering Problems
                                          Brent Miller (brent.miller@vanderbilt.edu)
                                                           2301 Vanderbilt Place
                                                            Vanderbilt University
                                                            Nashville, TN 37240
                                             Mark Steyvers (mark.steyvers@uci.edu)
                                         2201 Social & Behavioral Sciences Gateway Building
                                                          University of California
                                                              Irvine, CA 92697
                             Abstract                                   Müller-Trede, 2011) presumably because differences in
                                                                        mental representations and associated random error is larger
   Averaging the estimates of a number of individuals has been          across individuals.
   shown to produce an estimate that is generally more accurate            In order to improve the accuracy of the group average,
   than those of the individuals themselves. Similarly, averaging       many approaches have been developed to identify and
   responses from a single individual can also lead to a more
   accurate answer. How can we best combine estimates within
                                                                        upweight more expert or accurate judgments in the group
   and between individuals to create an accurate group estimate?        average, including performance or contributor weighting
   We report empirical results from a general knowledge rank-           (Budescu and Chen; Cooke, 1991; Bedford & Cooke, 2001;
   ordering experiment and demonstrate that individuals that            Aspinal, 2010), consensus (Shanteau et al. 2002; Wang et al.
   provide more consistent answers across repeated elicitations         2011; Batchelder & Romney, 1988; Batchelder & Anders,
   are also more accurate. We develop a consistency weighting           2012; Lee, Steyvers, de Young & Miller, 2012; Lee,
   heuristic and show that repeated elicitations within an              Steyvers, & Miller, 2014) as well as subjective confidence
   individual can be used to improve group accuracy. We also
   develop a Thurstonian cognitive model which assumes a                and metacognitive judgments (Koriat, 2012; Prelec, 2004).
   direct link between the process that explains the accuracy of           We will focus on the role of response agreement within
   an individual and response consistency and show how the              subjects as an indicator for expert judgment. Previous
   model can infer accurate group answers.                              research has shown that expert judgments tend to be more
   Keywords: Bayesian Modeling; Rank Ordering; Knowledge;               consistent over time (Einhorn, 1972, 1974) and that intra-
   Recall; Wisdom of Crowds; Within; Expertise; Uncertainty;            subject reliability can be used as a proxy for expertise
   Coherence; Consistency.                                              (Shanteau, Weiss, Thomas, & Pounds, 2002; Weiss &
                                                                        Shanteau, 2003; Weiss, Brennan, Thomas, Kirlik, & Miller,
                         Introduction                                   2009). This work has focused on the idea of highly
   There has been a lot of interest recently regarding how the          specialized expertise and across-question consistency for
judgments of individuals can best be combined to make                   tasks such as perception and categorization (Weiss &
group estimates that are as accurate as possible. When there            Shanteau, 2003; Weiss, Brennan, Thomas, Kirlik, & Miller,
is a ground truth – one single, verifiable correct answer –             2009). As opposed to previous research, we focus on tasks
the group average is often more accurate than most or all of            where expertise may be question-specific; subjects may
its constituent individual judgments (Davis-Stober,                     have knowledge for some questions, but not for others,
Budescu, & Broomell, 2014; Wallsten, Budescu, Erev, &                   making their question level consistency more informative
Diederich, 1997; Yaniv & Foster, 1997) even if the correct              about their expertise than the overall domain consistency.
answer is unknowable at the time of questioning (Lee,                      One challenge for using intra-subject consistency as an
Steyvers, de Young, & Miller, 2012). When repeated                      indicator for expert judgment is that other factors can
judgments are averaged within one individual as opposed to              contribute to response agreement, including decision
across individuals, a similar phenomenon occurs. For                    strategies and episodic recall (Vul & Pashler, 2008;
example, when a single person produces two estimates for                Hourihan & Benjamin, 2010). For example, in Vul and
the same underlying quantity, the average of the two                    Pashler's experiment, subjects were prompted for a second
estimates is generally less erroneous than the individual               estimate either in the same experimental session or after a
estimates (Vul & Pashler, 2008; Herzog & Hertwig, 2009;                 delay of three weeks. The intra-subject averages were most
Ariely et al. 2000). A standard explanation for these                   accurate after a delay of three weeks, suggesting that
averaging benefits is that random error associated with                 subjects were less likely to simply recall the first answer
probabilistic mental representations and processes partially            after a long delay. The requirement of a long temporal delay
cancel out in the average. A larger averaging benefit is                between repeated questions to avoid episodic recall might
typically found when averaging judgments across as                      not be practical in scenarios where subject judgments need
opposed to within subjects (Rauhaut & Lorenz, 2011;                     to be aggregated over a short interval.
                                                                    793

   In this paper, we focus on rank-ordering questions where        distracter questions, subjects were asked to rank teams for
the task is to rank-order a set items such as Presidents by        the NFL and NBA based on what they thought their final
terms in office or US cities by population size (Miller,           season standing would be.
Hemmer, Steyvers, Lee, 2009; Lee et al. 2012; Lee,                    Subjects were given the eight knowledge questions in a
Steyvers, Miller, 2014). In contrast to simple yes/no or           random order, and items for each question were initially
percentage estimation question involving single quantities,        placed in random positions. Subjects were then given the
rank-ordering questions involve the retrieval and                  distracter questions. Subjects were then prompted to give
coordination of many pieces of information, making it less         responses for the eight questions again, in the same order
likely that a subject can explicitly remember a previous           they appeared in the first elicitation, but with a new random
response. In the absence of easily available episodic              initial placement of the items for each question.
strategies, subjects can be asked for a second response               All questions had a ground truth obtained from Pocket
almost immediately after their first, eliminating the need for     World in Figures and various online sources. An interactive
multiple conditions and removing any question anchoring            interface was presented via a web browser on computer
effects.                                                           screens. Subjects were instructed to order the presented
   Our contribution in this paper is threefold. First, we show     items (e.g., “Order these books by their first release date,
that the crowd within an individual effect observed by Vul         earliest to most recent”), and responded by dragging the
and Pashler exists for rank-ordering tasks, indicating that        individual items on the screen using the computer mouse
there is a degree of statistical independence between              and “snapping” them into the desired locations in the
repeated elicitations for rank-ordering judgments. Second,         ordering, as in previous experiments. Transitions between
we demonstrate that the agreement between the first and            question blocks were marked by a holding page reminding
second response is related to each subjects’ response              subjects of the instructions for the tasks. At no point were
accuracy. We present a simple consistency weighting                subjects informed that they would be answering the same
heuristic where rank-ordering judgments from individuals           questions twice.
that are consistent across repeated questions are given larger
weight in the group average. We demonstrate that this              Results
consistency weighting heuristic significantly improves             Assessing Accuracy Performance was measured relative to
group accuracy. Finally, we introduce a new repeated-              the ground truth using Kendall’s tau distance τ. This metric
elicitation variant of a Thurstonian model for rank-ordering       is used to count the number of pair-wise disagreements
that has been explored elsewhere (see Steyvers et al., 2009        between the reconstructed and correct ordering (lower is
& Lee et al., 2012). We compare the performance of the             better). The larger the distance, the more dissimilar the two
repeated-elicitation model and the original variant, and           orderings are. Values of τ range from: 0 ≤ τ ≤ N(N-1)/2,
demonstrate that accounting for the variance in an                 where N is the number of items in the order (ten for all of
individual’s responses improves overall group aggregation          our questions). A value of zero means the ordering is
performance                                                        exactly right, a value of one means that the ordering is
                                                                   correct except for two neighboring items being transposed,
                        Experiment                                 and so on up to the maximum possible value of forty-five
                                                                   (indicating that the list is completely reversed). An average
Method                                                             score of 22.5 is expected for random performance.
The experiment was composed of 8 rank ordering questions,          Averaged Responses We first evaluated whether or not
and an additional 3 distracter questions; the distracter           averaging the responses within each individual reduced the
questions were included to increase the delay between              error relative to the individual responses, indicating
subject responses. Increased delay between responses has           statistically independent error of the sort observed in the
been shown previously to increase response independence            simple recall tasks of Vul and Pashler (2008). Table 1
and effect size (see Vul & Pashler, 2008). Subjects were 120       shows the median Kendall’s tau distance for individual
undergraduate students between the ages of 18 and 22 at the        rank-ordering problems for the first and second response as
University of California, Irvine who were compensated with         well as the combined first and second response using the
course credit.                                                     Borda aggregation method (see modeling section for Borda
   Selection for the non-distracter questions was based on         details). Subjects’ error on the first and second responses
difficulty, as determined by the accuracy of subjects in           were not significantly different, on average, and varied
previous experiments (Steyvers et al., 2009; Miller et al.,        according to question difficulty. The averaged first and
2011). Approximately one third of questions were selected          second responses of each subject (combined column in
for being easier (U.S. Holidays, U.S. Presidents, Book             Table 1) was less erroneous than the first and second
Release Dates), three for being moderately difficult               responses – t(120)=2.16, p<.05 and t(120)=2.87, p<.01
(Country Landmasses, U.S. Cities, European Cities), and            respectively – replicating the findings of Vul and Pashler
one two for being particularly difficult (10 Amendments,           (2008) for rank ordering tasks.
World Cities). All were general knowledge questions that
subjects were likely to have had exposure to. For the
                                                               794

  Table 1: Subject response error (Kendall’s tau)
  across individual rank-ordering problems.
    Problems                 1st     2nd   Combined
    Landmass                  9      10         8
    Holidays                  7       8         7
    Presidents                7       7         6
    Books                    11      11         10
    Euro Cities              15      16         14
    US Cities                16      14         14
    World Cities             21      21         20
    10 Amendments            16      15         15
          AVERAGE           12.5    12.7       11.9
Response Consistency and Accuracy If subject response
consistency is correlated to the precision of an individual’s
knowledge, then multiple independent responses should be
further apart from each other the less knowledgeable a
subject is. We quantified (inverse) response consistency as
the Kendall’s tau distance between subjects’ responses.                 Figure 1: Correlation between response disagreement
Subjects with a larger distance between their first and                 and accuracy for the first answer (top panel) and
second judgment should show a higher tau distance to the                second answer (lower panel).
ground truth. Figure 1 illustrates this relationship separately
for the first and second response. The correlation between          points based upon their location in a given response: 1 point
each subject’s response disagreement, and the error of their        for being in position 1, 2 points for being in position 2, up to
first and second responses, is ρ=.51 and ρ=.55 respectively.        10 points for a list of 10 items. In a standard Borda
This correlation is observed not only across all questions,         aggregation method, the points are added across all rank-
but also for each individual question. The correlation              orderings provided by subjects and the items are ordered
between response disagreement and accuracy appears to               according to the sum totals for each item. In our modified
scale linearly with overall subject accuracy for the problem.       Borda aggregation method, we add a weighting factor for
                                                                    each individual subject in order to upweight subjects that are
                         Modeling                                   more consistent. Specifically, we calculate the point total
While averaging across a given individual’s responses                  for each item ∈ 1, … , } by:
yields answers that are more accurate, the improvement is
                                                                                                         ,
far smaller than averaging two responses across subjects
(Miller et al., 2011). Given a large number of subjects, it is      where , is the rank of item k for subject j∈ 1, … , }, and
unclear whether repeated elicitations would improve group               is the weight given to subject j. As in a standard Borda
responses aggregation if they are merely treated as extra           method, the sums of these points for each item are then
subjects. Can within-subject response consistency be                ranked from smallest to largest to determine the final Borda
integrated into a between-subject aggregation model to              aggregate rank ordering
improve overall accuracy? To test this, we evaluate two                For an unweighted aggregate rank-ordering, the subject
models – a heuristic approach based on Borda aggregation            weights were set to the same value for all participants. We
method and a Thurstonian cognitive model of subject                 used this as the baseline for comparison. For the aggregate
behavior.                                                           rank-ordering weighted by response consistency, we use the
                                                                    inverse of the tau disagreement between the first and second
Borda Aggregation                                                   rank-ordering:
In order to assess if incorporating within-subject response                                      1⁄        1
consistency can improve between-subject estimates for rank          where we add one to the distance in order to avoid zero
ordering tasks, we used a modified version of Borda count           division. Therefore, the rank-orderings of participants with
aggregation that incorporates subject weighting. Borda              larger response consistency have a stronger influence on the
aggregation is a representative aggregation heuristic that has      aggregate rank ordering.
been used widely elsewhere (see Miller et al., 2009). In               Figure 2 shows the aggregation results. As we found
traditional Borda count aggregation, all items are assigned         previously (Miller et al., 2009), unweighted Borda
                                                                795

aggregation outperforms the average subject for all eight
questions. Additionally, the weighted Borda model performs
as well as or better than the unweighted model for all but
two of the questions. The weighted Borda model performed
worse for the Presidents question because most subjects
performed so well that weighting over-penalized the many
subjects with near-correct responses. Similarly, the model
performed poorly on the European Cities question because
there were so few subjects that performed well. Aggregation
for the unweighted Borda model was performed across both
trials so as not to give the weighted model the advantage of
extra subject responses. This superior performance in
reconstructing the ground truth ordering demonstrates that
response consistency can be used to improve group
accuracy for rank ordering tasks. Next we explore whether a
cognitive model of the rank-ordering task can better
describe subject behavior and more accurately reconstruct
the ground truth.                                                     Figure 2: Aggregation performance of unweighted and
                                                                      weighted Borda aggregation across first and second
Thurstonian Model                                                     responses, compared to the average subject
                                                                      performance.
Given that subject response consistency is clearly related to
accuracy in rank-ordering tasks, what kind of mechanism            lowers accuracy. In addition, the larger uncertainty also
might be responsible for this observed behavior? We                leads to increased differences in orderings between different
developed a probabilistic model based upon a Thurstonian           responses. Therefore, the model assumes an inherent
approach. In a Thurstonian representation, the latent ground       connection between response consistency and accuracy –
truth ordering for a specific problem is represented by            they are both driven by a latent parameter i that represents
coordinates on an interval scale. As Figure 3a illustrates,        the (inverse) expertise level of a subject for a particular
each item k is represented as a latent coordinate k on an
interval dimension. Note that this represents not the actual
ground truth but the latent truth as perceived by a group of
individuals. The one-dimensional representation of items is
appropriate as all problems in our study involve one-
dimensional relative judgments (e.g. the size of items and
the timing of events).
   Each individual i is assumed to have access to all of the
ground truth latent coordinates , but without precise
knowledge about their exact locations. This uncertainty is
represented with normal distributions that are centered on
the shared latent ground truth locations and with a subject-
level i that represents the uncertainty of the individual
about the item locations. Note that for a given subject, all
items have the same standard deviation which is a strong
assumption but simplifies the model considerably.
   As Figure 3b shows, the subject draws mental samples
from these item distributions. Repeated elicitations are
modeled simply by repeating the sampling process which
leads to a new set of samples. The rank-ordering produced
by a subject is then based on the order of the mental
                                                                     Figure 3: Illustration of the Thurstonian Model for
samples.
                                                                     repeated elicitations. (a) The latent ground truth is
   As illustrated in Figure 3c, different subjects can have
                                                                     represented as a set of coordinates on an interval scale
different uncertainty i, and this influences not only the           (b) Uncertainty about the latent ground truth is
response accuracy but also the response consistency. For             represented by Gaussian noise and responses are created
example, the larger uncertainty associated with the subject          by sampling latent values from each item distribution (c)
illustrated in Figure 3c leads to more transposition errors in       Example of a subject with larger uncertainty about the
the mental samples associated with a given response – it             ground truth and larger variability in the item samples
becomes more likely that samples of nearby distributions
                                                                     across the first and second response
are out of order (relative to the latent ground truth) which
                                                               796

question.
    This multiple-elicitation model is different from previous
Thurstonian models that we have presented, where subjects
only give a single response per question (Steyvers et al.,
2009; Miller & Steyvers, 2011). This extended model
allows us to examine whether accuracy and response
consistency can be described with the same underlying
mechanism.
   We apply Bayesian estimation techniques to infer the
group representation from individual orderings. Figure 4
shows the Thurstonian model for a single question across
subjects using graphical model notation (see Koller,
Friedman, Getoor, & Taskar, 2007; Shiffrin, Lee, Kim, &
Wagenmakers, 2008, for statistical and psychological
introductions). Each node represents a model variable, and            Figure 5: Aggregation performance of weighted Borda,
the graph structure is used to indicate the conditional               traditional Thurstonian, and repeated Thurstonian
dependencies between these variables. Stochastic and                  models.
deterministic variables are indicated by single-and double-
                                                                   ordering of all of their mental samples yij = Rank(xij).
bordered nodes (, , x and y respectively), and observed
                                                                      While the generative model is relatively straightforward,
data are represented by a shaded node (y). The plate
                                                                   the inference is challenging because the observed data yij is
represents independent replications of the graph structure,
                                                                   a deterministic ranking. We utilized MCMC procedures
which corresponds to multiple elicitations from each
                                                                   originally developed by Yao and Böckenholt (1999), which
individual i and across individuals for each question j.
                                                                   allowed us to estimate the posterior distribution over the
   To explain how these data are generated, the model
begins with the underlying ground truth location of the            latent variables xijk, j, and  given the observed orderings
                                                                   yij. We use Gibbs sampling to update the mental samples xijk,
items, given by the vector . The latent ground truth  is
given a flat prior such that all item locations are equally        and Metropolis-Hastings updates for j and .
likely a priori. Each individual has an associated uncertainty        Figure 5 shows the accuracy of three aggregation models,
                                                                   and demonstrates that the repeated elicitation Thurstonian
parameter j ~ Gamma(0, 1/) where  is a hyper-
                                                                   model performed best overall. It outperformed the weighted
parameter that determines the variability of the expertise
                                                                   Borda model and also outperformed a Thurstonian model
levels across individuals. We set  = 3 in the current model.
                                                                   that is given both the first and second response of
   To determine the order of items for the ith repetition, the
                                                                   participants but treats the second responses as coming from
jth individual samples a location xijk for each item k where
                                                                   a new set of participants. Additionally, the repeated
xijk ~ Normal(k,j). The sample xijk represents the realized      elicitation Thurstonian model matched or exceeded other
mental representation for the individual at that particular        models’ performance for each individual question.
time. The ordering for each individual is determined by the           The advantage of the repeated elicitation Thurstonian
                                                                   model over the Thurstonian model where the first and
                                                                   second responses are not linked to the same subject is not
                                                                   due to the fact that it has access to additional response
                                                                   information (it uses the same set of subject responses), but
                                                                   because the model simultaneously infers a subject’s
                                                                   uncertainty based upon their disagreement with other
                                                                   subjects and their disagreement with themselves. In this
                                                                   way, we have some confidence in the Thurstonian
                                                                   representation of individual-level uncertainty for subject
                                                                   item recall, both as a generative model and as a means of
                                                                   yielding more accurate group estimates for rank ordering
                                                                   tasks.
                                                                                            Conclusions
                                                                   In this paper, we have shown that repeated elicitations for
                                                                   general knowledge rank-ordering tasks exhibit statistically
                                                                   independent error, and the variance of that error is
                                                                   correlated to the accuracy of subject responses for easy and
   Figure 4: Graphical model of the Thurstonian model for          difficult questions. Additionally, we have shown that this
   repeated elicitations.                                          response consistency can be used to improve group
                                                               797

aggregate accuracy in reconstructing the ground truth             Koller, F., & Friedman, N. (2007). Graphic models in a
answer for rank ordering knowledge tasks. These findings            nutshell. In L. Getoor & B. Taskar (Eds.), Introduction to
might also be applicable to tasks that do not have a known          statistical relational learning (pp. 13-56). MIT Press.
ground truth, as we have discussed elsewhere (Lee et al.,         Koriat, A. (2012). When Are Two Heads Better than One
2012). Finally, we introduced a cognitive model of rank-            and Why? Science, 336(6079), 360–362.
order judgement wherein a subject-level uncertainty               Lee, M. D., Steyvers, M., de Young, M., & Miller, B.
parameter accounted for both subject response accuracy and          (2012). Inferring expertise in knowledge and prediction
response consistency, and found that it was best able to            ranking tasks. Topics in Cognitive Science, 4(1), 151–163.
capture subject behavior and reconstruct the original ground      Lee, M. D., Steyvers, M., & Miller, B. (2014). A Cognitive
truth ordering for each of our questions. This lends credence       Model for Aggregating People’s Rankings. PLOS ONE,
to the idea of a combined probabilistic mechanism for               9(5), e96431.
consistency and accuracy underlying the subject behavior          Miller, B. J., Hemmer, P., Steyvers, M., & Lee, M. D.
observed in these complex knowledge recall tasks.                   (2009, July). The wisdom of crowds in rank ordering
                                                                    tasks. In Proceedings of the 9th international conference
                        References                                  of cognitive modeling.
Aspinall, W. (2010). A route to more tractable expert             Miller, B. J., & Steyvers, M. (2011, July). The wisdom of
   advice. Nature, 463(7279), 294–295.                              crowds with communication. In Proceedings of the 33rd
Ariely, D., Tung Au, W., Bender, R. H., Budescu, D. V.,             annual conference of the cognitive science society.
   Dietz, C. B., Gu, H., Wallsten, T. S., Zauberman, G.           Müller-Trede, J. (2011). Repeated judgment sampling:
   (2000). The effects of averaging subjective probability          Boundaries. Judgment and Decision Making, 6(4), 283.
   estimates between and within judges. Journal of                Prelec, D. (2004). A Bayesian Truth Serum for Subjective
   Experimental Psychology: Applied, 6(2), 130–147.                 Data. Science, 306(5695), 462–466.
Batchelder, W. H., & Anders, R. (2012). Cultural                  Rauhut, H., & Lorenz, J. (2011). The wisdom of crowds in
   Consensus Theory: Comparing different concepts of                one mind: How individuals can simulate the knowledge of
   cultural truth. Journal of Mathematical Psychology,              diverse societies to reach better decisions. Journal of
   56(5), 316–332.                                                  Mathematical Psychology, 55(2), 191–197.
Batchelder, W. H., & Romney, A. K. (1988). Test theory            Shanteau, J., Weiss, D. J., Thomas, R. P., & Pounds, J. C.
   without an answer key. Psychometrika, 53(1), 71–92.              (2002). Performance-based assessment of expertise: How
Bedford, T., & Cooke, R. (2001). Probabilistic Risk                 to decide if someone is an expert or not. European
   Analysis: Foundations and Methods. Cambridge                     Journal of Operational Research, 136(2), 253–263.
   University Press.                                              Shiffrin, R. M., Lee, M. D., Kim, W., & Wagenmakers, E.-
Budescu, D. V., & Chen, E. (2014). Identifying Expertise to         J. (2008). A Survey of Model Evaluation Approaches
   Extract the Wisdom of Crowds. Management Science,                With a Tutorial on Hierarchical Bayesian Methods.
   61(2), 267–280.                                                  Cognitive Science, 32(8), 1248–1284.
Cooke, R. M. (1991). Experts in Uncertainty: Opinion and          Vul, E., & Pashler, H. (2008, July). Measuring the crowd
   Subjective Probability in Science. Oxford University             within: Probabilistic representations within individuals.
   Press.                                                           Psychological Science, 19 (7), 645–647.
Davis-Stober, C. P., Budescu, D. V., Dana, J., & Broomell,        Wang G., Kulkarni, S. R., Poor, H. V., & Osherson, D. N.
   S. B. (2014). When is a crowd wise? Decision, 1(2), 79–          (2011). Aggregating Large Sets of Probabilistic Forecasts
   101.                                                             by Weighted Coherent Adjustment. Decision Analysis,
Einhorn, H. J. (1972). Expert measurement and mechanical            8(2), 128–144.
   combination. Organizational Behavior and Human                 Wallsten, T. S., Budescu, D. V., Erev, I., & Diederich, A.
   Performance, 7(1), 86–106.                                       (1997). Evaluating and combining subjective probability
Einhorn, H. J. (1974). Expert judgment: Some necessary              estimates. Journal of Behavioral Decision Making, 10 (3),
   conditions and an example. Journal of Applied                    243–268.
   Psychology, 59(5), 562–571.                                    Weiss, D. J., Brennan, K., Thomas, R., Kirlik, A., & Miller,
Herzog, S. M., & Hertwig, R. (2009). The wisdom of many             S. M. (2009). Criteria for performance evaluation.
   in one mind: Improving individual judgments with                 Judgment and Decision Making, 4(2), 164–174.
   dialectical bootstrapping. Psychological Science, 20(2),       Weiss, D. J., & Shanteau, J. (2003). Empirical assessment of
   231–237.                                                         expertise. Human Factors, 45(1), 104–116.
Hourihan, K. L., & Benjamin, A. S. (2010). Smaller is better      Yaniv, I., & Foster, D. P. (1997). Precision and accuracy of
   (when sampling from the crowd within): Low memory                judgmental estimation. Journal of Behavioral Decision
   span individuals benefit more from multiple opportunities        Making, 10 (1), 2132.
   for estimation. Journal of Experimental Psychology,            Yao, G., & Böckenholt, U. (1999). Bayesian estimation of
   Learning, Memory, and Cognition, 36(4), 1068–1074.               Thurstonian ranking models based on the Gibbs sampler.
                                                                    British Journal of Mathematical and Statistical
                                                                    Psychology, 52(1), 79–92.
                                                              798

