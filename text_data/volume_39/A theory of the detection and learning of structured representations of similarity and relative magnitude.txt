        A theory of the detection and learning of structured representations of
                                     similarity and relative magnitude
       Leonidas A.A. Doumas1 (alex.doumas@ed.ac.uk); Aaron Hamer1 (aaron.hamer@ed.ac.uk);
  Guillermo Puebla1 (pueblaramirezg@gmail.com); Andrea E. Martin1, 2 (andrea.martin@mpi.nl)
          1
            Department of Psychology, University of Edinburgh, 7 George Square, Edinburgh, EH8 9JZ, United Kingdom
                              2
                                Max Planck Institute for Psycholinguistics; Nijmegen, The Netherlands
                            Abstract                               of objects and relations and learning new concepts by
Responding to similarity, difference, and relative magnitude is    building new combinations of these innate elements.
ubiquitous in the animal kingdom. However, humans seem                Some models attempt to account for the origins of
unique in the ability to represent relative magnitude and          abstract     concepts      without     assuming       innate
similarity as abstract relations that take arguments (e.g.,        representations of relational concepts. For example,
greater-than (x,y)). While many models use structured
                                                                   BART (Lu, Chen, & Holyoak, 2012) uses feature lists
relational representations of magnitude and similarity, little
progress has been made on how these representations arise.         generated by human subjects or corpora analysis to find
Models that use these representations assume access to             properties associated with items in the world which
computations of similarity and magnitude a priori. We detail a     instantiate particular relations. BART has difficulty with
mechanism for producing invariant responses to “same”,             some edge cases of relational cognition (e.g., reasoning
“different”, “more”, and “less” which can be exploited to          about something like an atom being bigger than
compute similarity and magnitude as an evaluation operator.        something else when it has not experienced instances
Using DORA (Doumas, Hummel, & Sandhofer, 2008), these              where an atom was bigger than anything), but the model
invariant responses can serve to learn structured relational       makes a serious effort to account for development of
representations of relative magnitude and similarity from pixel
                                                                   analogy-making with minimal assumptions about the
images of simple shapes.
                                                                   starting representations of the learning system.
                                                                      In a similar vein, DORA (Doumas, Hummel, &
                       Introduction                                Sandhofer,       2008)      explains    how      structured
                                                                   representations (i.e., predicates) can be acquired from
   Reacting to similarity, and magnitude (“same”/                  unstructured representations (i.e., feature vectors). While
”different”, “more”/”less”; SDML) are hallmarks of                 DORA learns relational representations that can take any
complex organisms. For example, gerbils use the retinal            arguments (including edge cases and completely novel
size of a stimulus to estimate its distance (Goodale,              arguments ; Doumas et al., 2008), DORA assumes a
Ellard, & Booth, 1990), rats choose the larger of two food         system to detect the invariant features that underlie the
rewards (Kim et al., 2015), and pigeons learn to group             abstract concepts that it learns.
pictures of 16 identical items in one set, and pictures of            A complete account of how people acquire structured
16 different items in a different set (Young, Wasserman,           representations of abstract SDML relations must solve
& Garner, 1997).                                                   three problems. First, there must be some invariant
   Humans, however, go beyond simple detection of                  features which remain constant across instances of the
relative magnitude and similarity. We make an analogies            relation which the perceptual/cognitive system can learn
between a nucleus and the sun because they are both                to detect. Second, the system must isolate these
larger than their orbiting bodies (electrons and planets).         invariants from other properties of the objects engaged in
We infer this relationship because we represent relative           the relation to be learned. Third, the system must learn a
magnitude and similarity as abstract relations that take           predicate representation of the relational properties (i.e.,
arguments (i.e., as predicates; see Holyoak, 2012).                an explicit entity that can be bound to arbitrary, novel
   Our ability to reason about abstract SDML manifests             arguments).
in a variety of domains such as analogy (e.g., Holyoak &              We solve the first problem with an extension to DORA
Thagard, 1995), categorisation (e.g., Medin, Goldstone,            which produces invariant responses to similarity and
& Gentner, 1993), and concept learning (e.g., Doumas &             relative magnitude. We have previously shown how
Hummel, 2013). While models that use structured                    DORA can solve the second and third of these problems
representations have had success in accounting for how             (Doumas et al., 2008). We begin with a brief overview
humans use abstract SDML, these models say little about            of DORA, describe the process which produces invariant
where the representations they use come from in the first          features for SDML, and provide simulations
place. For example, SME (Falkenhainer, Forbus, &                   demonstrating how DORA solves all three problems to
Gentner, 1989), STAR (Halford et al., 1998), and LISA              learn structured relational representations of SDML.
(Hummel & Holyoak, 1997, 2003) account for many
phenomena from the analogy literature, but require the
relations they use to make these analogies be hand-coded                                     Model
by the modeler. Similarly, Bayesian models of concept              DORA
development and learning (e.g., Kemp, 2012; Kemp &                    DORA (Doumas, et al., 2008) is a symbolic-
Tenenbaum, 2007, 2009; Lake et al., 2016) assume                   connectionist model, based on the LISA (Hummel &
relational structures a priori, starting with a vocabulary
                                                                 1955

Holyoak, 1997, 2003) model of analogy. DORA learns            dynamically without violating their independence (i.e., it
structured relational representations from unstructured       is not sufficient to represent bindings using only
representations of objects (e.g. feature vectors).            conjunctive units; see, e.g., Doumas & Hummel, 2005;
                                                              von der Malsburg, 1999). DORA uses systematic
LISAese Representations We begin by describing                asynchrony of firing to dynamically bind roles to their
the end state of DORA’s representations (i.e., its            fillers (see Doumas et al., 2008). As a relational
representations after it has gone through learning).          representation in the driver becomes active, bound
Relational propositions are represented by a hierarchy of     objects and roles fire in direct sequence. Information
distributed and localist codes (see Figure 1). At the         about role-filler bindings is carried by proximity of firing
bottom, semantic units code the features of objects and       (e.g., with roles firing directly before their fillers). This
roles in a distributed fashion. In the next layer, localist   sequence-based binding keeps roles and their fillers
predicate-object (PO) units representing individual           distinct and thus independent. Using the example in
predicates (or roles) and objects, are connected to these     Figure 1, in order to bind bigger to block and smaller to
distributed semantic representations. In the next layer,      ball (and so represent larger (block, ball)), the units
localist role-binding (RB) units link predicates and          corresponding to bigger fire directly followed by the
objects into specific role-filler pairs. At the top of the    units corresponding to block, followed by the units for
hierarchy, localist proposition (P) units link RB units into  coding smaller followed by the units for ball.
complete relational propositions. Importantly, while we
use different names for the units in different layers, and    Mapping DORA uses LISA’s mapping algorithm (see
different shapes to distinguish these units in diagrams,      Hummel & Holyoak, 1997; Doumas et al., 2008). DORA
we do so only for the purposes of expositional brevity.       learns mapping connections between units of the same
These are just nodes in different layers of a network. RB     type in the driver and recipient (e.g., between PO units in
units are just like PO units, except for the fact that they   the driver and PO units in the recipient). These
are in a different layer, and, therefore, take input from     connections grow whenever corresponding units in the
and pass input to different layers of units.                  driver and recipient are active simultaneously. The
                                                              connections act as mappings between corresponding
                                                              structures in separate analogs. They also permit
                                                              correspondences learned in mapping to influence
                                                              correspondences learned later.
                                                              Relation Learning DORA uses comparison to isolate
                                                              shared properties of objects and to represent them as
                                                              explicit structures. DORA begins with simple feature-
                                                              vector representations of objects (i.e., a node connected
                                                              to a set of semantic features describing that object).
                                                              When DORA compares two objects, the two
  Figure 1. Complete relational proposition in DORA.
                                                              representations are activated simultaneously. For
    Units in different layers are coded using different
                                                              instance, if DORA compares a block that is larger than
          shapes for the purposes of exposition.
                                                              some object to a plate that is larger than some other
                                                              object (e.g., when the block is larger than a ball and the
   Propositions in DORA are divided into four mutually-
                                                              plate is larger than a fork), then the nodes representing
exclusive sets of layered networks: a driver, one or more
                                                              the block and plate fire together (Figure 2a). Semantic
recipients, long-term memory (LTM), and the emerging
                                                              features shared by the compared objects (i.e., features
recipient (EM). Each set consists of a layered network of
                                                              common to the block and the plate) receive twice as
PO, RBs, and P units (i.e., there are specific layers coding
                                                              much input and thus become roughly twice as active as
for PO, RB, and P units in the driver, and another set of
                                                              features connected to one but not the other (Figure 2b).
layers coding for PO, RB, and P units in the recipient).
                                                              DORA then learns connections between a newly
Semantic units are shared across all networks (i.e., driver
                                                              recruited PO unit and active semantic units via Hebbian
and recipient units are connected to the same pool of
                                                              learning (Figure 2c). In Hebbian learning the strength of
semantic units). The driver corresponds to the current
                                                              a learned connection is a function of unit activation (i.e.,
focus of attention and controls the flow of activation.
                                                              stronger connections are learned to more active units).
Units in the driver pass activation to the semantic units.
                                                              Consequently, the new PO unit becomes most strongly
Because the semantic units are shared by all sets,
                                                              connected to the highly active semantic units. The new
activation flows from the driver to the other three sets.
                                                              PO becomes an explicit representation of the feature
DORA operations (e.g., mapping and relation learning,
                                                              overlap between the block and plate. In this example,
detailed below) proceed as a product of units in the driver
                                                              DORA forms an explicit representation of the semantics
activating their semantic units, which in turn activates
                                                              of bigger things (i.e., the features common to both the
units in the various other sets.
                                                              block and plate). The new PO functions as a predicate
   When a relational representation enters the driver the
                                                              representation of bigger because it can be dynamically
binding of roles to their fillers must be represented
                                                              bound to fillers via an RB unit (Figure 2d).
                                                             1956

                                                                than a smaller item. There is a preponderance of evidence
       (a)                    (b)                               for this assumption. In visual processing, larger items
                   block                 block                  take up more space on the retina (e.g., Wandell, 1995)
                                                                and are coded by larger swaths of the visual cortex (e.g.,
                                                                Engel et al., 1994).
                                                                     (a)                                                                            (b)
                                                                                              bigger+plate                    smaller+fork                                bigger+plate                     smaller+fork
                   plate                 plate
                                                                                                            smaller                                                                     smaller
                                                                          bigger                 plate                           fork                bigger                   plate                           fork
        (c)                   (d)
                                                                          bigger                  block                          ball                bigger                   block                            ball
                     block                   block                                                        smaller                                                                     smaller
                                                                                              bigger+block                    smaller+ball                               bigger+block                      smaller+ball
                                                                     (c)                                                                             (d)
                                                                                                bigger+plate                   smaller+fork                                 bigger+plate                     smaller+fork
                                                                                                              smaller                                                                    smaller
                                                                            bigger                  plate                          fork                bigger                  plate                            fork
                                                   plate
          “bigger”   plate      “bigger”
                                               bigger+plate
                                                                            bigger                  block                          ball                bigger                   block   smaller                 ball
                                                                                                           smaller
    Figure 2. Comparison-based predication in DORA.                                             bigger+block                    smaller+ball                               bigger+block                      smaller+ball
DORA learns a representation of bigger by comparing a             (e)                                                  (f)                                                          (g)
  block that is bigger than some object to a plate that is                   bigger+plate               smaller+fork                    bigger+plate              smaller+fork                      bigger+plate             smaller+fork
  bigger than some other object. (a) DORA compares a              bigger        plate smaller
                                                                                                           fork
                                                                                                                       bigger              plate  smaller
                                                                                                                                                                     fork
                                                                                                                                                                                    bigger             plate smaller
                                                                                                                                                                                                                                fork
     block and a plate. Units representing both become
   active. (b) Feature units shared by the block and the          bigger        block
                                                                                     smaller
                                                                                                           ball        bigger              block
                                                                                                                                                 smaller
                                                                                                                                                                     ball           bigger             block
                                                                                                                                                                                                             smaller
                                                                                                                                                                                                                                 ball
      plate become more active than unshared features                        bigger+block                smaller+ball                   bigger+block               smaller+ball                     bigger+block              smaller+ball
 (darker grey). (c) A new PO unit learns connections to
    features in proportion to their activation (solid lines
   indicate stronger connection weights). The new unit                (h)                                                                                     (i)
  codes the featural overlap of the block and plate (i.e.,                  bigger
                                                                                             bigger+plate
                                                                                                plate smaller
                                                                                                                      smaller+fork
                                                                                                                         fork                                              bigger
                                                                                                                                                                                           bigger+plate
                                                                                                                                                                                              plate smaller
                                                                                                                                                                                                                     smaller+fork
 the role “bigger”). (d) This new PO unit functions as a
                                                                                                                                                                                                                        fork
         predicate when dynamically bound to fillers.                       bigger              block                    ball                                              bigger             block                     ball
                                                                                                      smaller                                                                                      smaller
                                                                                            bigger+block              smaller+ball                                                         bigger+block               smaller+ball
DORA learns representations of multi-place relations by
linking sets of co-occurring role-filler pairs into                                                                                                                                        larger (block, ball)
hierarchical relational structures. Continuing the                  Figure 3. DORA learns a representation of the whole
example, when DORA compares a plate that is larger              relation larger (block, ball) by mapping bigger(plate) to
than a fork to a block that is larger than a ball, it will map      bigger(block) and smaller(fork) to smaller(ball). (a)
larger (plate) to larger (block) and smaller (fork) to           The units coding bigger fire; (b) the units for plate and
smaller (ball) (Figure 3a). When constituent sets of role-       block fire; (c) the units for smaller fire; (d) the units for
filler pairs are mapped, a distinct pattern of firing                 fork and ball fire. (e) DORA recruits a P unit in the
emerges—namely, mapped RB units fire together and                recipient. (f-g) DORA learns a connection between the
out of synchrony with any other RB units; Figure 3b-d).            new P unit and the active RB unit (the unit coding for
This pattern is a reliable signal that DORA exploits to            bigger(block)). (h-i) The P unit learns connections to
combine sets of role-filler pairs into multi-place              the active RB unit (coding for smaller(ball)). The result
relations. In response to the pattern, DORA recruits a P                      is a structure coding for larger(block, ball).
unit that learns connections to any active RB units in the
recipient (Figure 3e-g) via Hebbian learning. The result              Basic magnitude calculation is accomplished by
is a P unit linking the RB units in the recipient into a        comparison. When the model attends to two
complete relational structure (larger (block, ball); Figure     representations with specific magnitude values (e.g., two
3i).                                                            POs attached to absolute size are present in the driver
                                                                together; Figure 4a), the representations of the absolute
Producing invariant responses for basic SDML                    magnitude semantics are co-activated and the PO units
    A comparison-based solution to the problem of               attached to these semantic units compete via lateral
learning an invariant feature coding for “more”, “less”,        inhibition (Figure 4b). The POs will eventually settle,
and “same” requires the assumption that initially               with either one PO becoming more active and inhibiting
available magnitude information is coded by a direct            the other to inactivity, or, when both POs code for the
neural proxy: All else being equal, higher magnitude            same absolute magnitude, with both POs in a steady state
items are coded (at least early in processing) by more          of co-activation. More semantic units can then respond
neurons than comparatively lower magnitude items. For           to the particular pattern of firing in the driver POs. Some
example, a larger item will be coded by more neurons            units are excited by two active POs in the driver, others
                                                               1957

are excited by a single highly active PO early in firing,    proxy (as in the human neural system) produces one of
or by a single highly active PO late in firing (these        three patterns. (1) Both units settle into a state of similar
regions of excitement are easily learnable via simple        co-activation—which occurs when two representations
neural threshold tuning). The active POs learn               of the same magnitude are compared. (2) One unit
connections to the active semantic unit by Hebbian           becomes more active and forces the second unit to
learning. If a single PO is active, that unit will learn     inactivity—which occurs when a unit codes for a greater
connections to the semantics that are activated by a         magnitude. (3) One unit becomes active after it has been
single highly active driver PO early in firing (which        inhibited by a winning unit—which occurs when a unit
becomes the invariant signal for “more”; Figure 4c).         codes for a lesser magnitude. Whatever units respond to
When the active PO becomes inhibited (because of             these patterns naturally or through tuning become
asynchronous binding), the second PO (the one inhibited      implicit invariant codes for the presence of “sameness”,
by the winning PO) will become active (Figure 4d). That      “moreness”, and “lessness”, respectively. Vitally, the
unit learns connections to the semantics that are activated  same patterns will emerge and the same codes will
by a single highly active driver PO late in firing (which    become active when specific relative magnitudes are
becomes the invariant signal for “less”; Figure 4d).         present even cross dimensionally. That is, the same
Otherwise, if two POs are co-active (i.e., they code the     patterns emerge and units become active during an
same magnitude), then they will learn connections to the     instance of different absolute height, or width, or colour.
semantics which are activated by two active driver POs       What is left for the system is to learn explicit
(which becomes the invariant signal of “sameness”.           representations of these invariant semantics that are not
                                                             tied to any specific magnitudes (e.g, a PO connected to
                                                             semantics encoding ‘more’ & ‘height’, without strong
                                                             connections to any specific height) and can take other
                                                             POs as arguments. In other words, exactly the learning
                                                             that DORA does.
                                                                                   Simulations
                                                             Simulation 1
                                                                We tested whether DORA could learn structured
                                                             representations of relative SDML relations starting with
                                                             information about sets of shapes with features
                                                             representing absolute values on dimensions. This
                                                             simulation mirrored what happens during development
                                                             when a child learns from experience without a teacher or
                                                             guide.
                                                                The model began with pixel images of basic shapes
                                                             (differing in shape, colour, size, width, and height).
                                                             These images were pre-processed with a feedforward
                                                             neural network that learned via back-propagation to
                                                             deliver absolute shape, colour, size, width, and height
                                                             information (akin to that information delivered by early
 Figure 4. The SDML detector working on POs coding           visual processing). Each processed image was
   different values on a dimension. For the purposes of      represented by a PO attached to the delivered features. In
 clarity, only the predicate POs and their semantics are     addition, each shape was also attached to a set of 10
depicted in this figure. (a) Two POs coding for different    extraneous features selected randomly from a set of 100
  heights are in the driver. (b) The semantics coding for    features, included as noise (as objects in the world
  absolute dimensional information become active and         contain several features extraneous to any particular
   the two POs compete to become active. (c) The unit        learning goal). Each shape was then randomly paired
    coding for the greater value on the dimension (here      with another to create pairs of shapes over which
     height-6) becomes active first, thus marking it as      relations were learned. We created 100 pairs of objects
“more”. The PO learns a connection to the semantic that      in this manner and placed them in DORA’s LTM.
  responds to winning the SDML competition (i.e., the           We then allowed DORA to attempt to learn from these
 invariant of “more”). (d) The unit coding for the lesser    basic representations. On each learning trial, DORA
   value on the dimension (here height-3) will become        selected one pair of objects from LTM at random and ran
  active last, thus marking it as “less”. The predicate is   (or attempted to run) retrieval, mapping, SDML
   connected to the semantic unit coding for losing the      comparison, predication, and multi-place relation
      SDML competition, or the invariant of “less”.          learning, and stored any representations that it learned in
                                                             LTM. In short, we are testing whether unguided learning
   In short, comparing different magnitudes in a network     from simple shape objects is sufficient for DORA to
in which magnitude information is coded by an absolute
                                                            1958

learn structured representations of relative SDML             mapping similar, but non-identical predicates; and (iii)
relations.                                                    form the basis of overcoming the n-ary restriction.
   We defined a relational quality metric as the mean of         During cross-mapping, an object (object1) is mapped
connection weights to relevant features (i.e., those          to a featurally less similar object rather than a featurally
defining a relative magnitude on some specific                more similar object because it (object1) plays the same
dimension (e.g., ‘more’+‘height’, or ‘less’+‘width’))         role as the less similar object. Cross-mappings serve as a
divided by the mean of all other connection weights + 1       stringent test of the structure sensitivity of a
(1 was added to the mean of all other connection weights      representation as they require violating featural or
to normalize the quality measure to between 0 and 1). A       statistical similarity.
higher quality denoted stronger connections to the               We tested the relations that DORA had learned in the
semantics defining a specific SDML relation relative to       previous simulations for their ability to support finding
all other connections. We measured the relational quality     cross-mappings. We selected two of the refined relations
of the last 100 items DORA had learned after each 100         that DORA had learned during the previous simulation
learning trials for 1000 total learning trials. Importantly,  at random. We bound the relations to new objects,
we tested all representations that the model learned (not     creating two new propositions, P1 and P2 such that the
just those that instantiated the relevant relations) and      agent of P1 was semantically identical to the patient of
included these in the relational selectivity calculation.     P2 and patient of P1 was semantically identical to the
   Figure 5 shows the quality of the representations that     agent of P2, and allowed DORA to attempt to map P1
DORA learned. DORA learned representations of whole           and P2. We repeated this procedure 10 times, each time
relational structures encoding relative magnitudes and        with a different randomly-chosen pair of relations. All 10
similarity on all the encoded dimensions. DORA learned        times DORA successfully mapped the agents and
representations of bigger (one predicate PO connected         patients of P1 and P2. The relations DORA learned in
most strongly to the semantics ‘more’ & ‘size’, the other     the first simulation satisfy the requirement of cross-
connected to ‘less’ & ‘size’), wider (predicate POs           mapping.
connected to ‘more’ & ‘width, and ‘less’ & ‘width’),             We also tested whether the relations that DORA has
taller (predicate POs connected to ‘more’ & ‘height, and      learned would support mapping to similar but non-
‘less’ & ‘height), same-size (predicate POs both              identical relations (such as mapping higher to greater-
connected most strongly to ‘same’ & ‘size;), same-width       than). Humans successfully map such relations (e.g.,
(predicate POs both connected most strongly to ‘same’         Bassok, Wu, & Olseth, 1995; Gick & Holyoak, 1983), an
& ‘width’), same-height (predicate POs both connected         ability that Hummel and Holyoak (1997, 2003) have
most strongly to ‘same’ & ‘height’), same-colour              argued depends on the semantic-richness of relational
(predicate POs both connected most strongly to ‘same’         representations. We selected one of the refined relations
& ‘colour’), and same-shape (predicate POs both               that DORA had learned during the previous simulation,
connected most strongly to ‘same’ & ‘shape’). The             R1, and constructed a new relation, R2, that shared 50%
results indicate that DORA can learn structured               of its semantics (in each role) with the selected relation.
representations of relative SDML relations from objects       So that mappings could not be based on object similarity,
that include only absolute values on dimensions even          none of the objects that served as arguments of the
with the addition of extraneous noise.                        relations had any semantic overlap. We repeated this
                                                              process 10 times. Each time, DORA mapped the agent
                                                              role of R1 to the agent role of R2 and the patient role of
                                                              R1 to the patient role of R2, and, despite their lack of
                                                              semantic overlap, corresponding objects always mapped
                                                              to one another (because of their bindings to mapped
                                                              roles).
                                                                 Finally, we tested the model’s ability to find mappings
                                                              that violate the n-ary restriction: the restriction that an n-
                                                              place predicate may not map to an m-place predicate
                                                              when n ≠ m. Almost all models of structured cognition
                                                              follow the n-ary restriction (namely, those that represent
                                                              propositions using traditional propositional notation and
                                                              its isomorphs; see Doumas & Hummel, 2005). However,
          Figure 5. Results of DORA’s learning.               the restriction does not appear to apply to human
                                                              reasoning, as evidenced by our ability to easily find
                                                              correspondences between bigger (Sam, Larry) on one
Simulation 2                                                  hand, and small (Joyce), big (Susan), on the other
   A crucial question remains: do the representations         (Hummel & Holyoak, 1997).
DORA learns meet the requirements of relational                  To test DORA’s ability to violate the n-ary restriction,
representations? Some hallmark of relational                  we randomly selected a refined relation (R1) that DORA
representations (see Holyoak, 2012) are that they, (i)        had learned in the previous simulation. We then created
form the basis of solving cross mappings; (ii) support        a single place predicate (r2) that shared 50% of its
                                                             1959

semantics with the agent role of R1 and none of its             Psychology: Learning, Memory, and Cognition, 21(6),
semantics with the patient role. The objects bound to the       1522.
agent and patient role of R1 each shared 50% of their         Doumas, L. A., & Hummel, J. E. (2005). Approaches to
semantics with the object bound to r2. DORA attempted           modeling human mental representations: What works, what
                                                                doesn’t and why. The Cambridge handbook of thinking and
to map R1 to r2. We repeated this process 10 times, and
                                                                reasoning, ed. KJ Holyoak & RG Morrison, 73-94.
each time DORA successfully mapped the agent role of          Doumas, L. A., & Hummel, J. E. (2013). Comparison and
R1 to r2, along with their arguments. We repeated the           mapping facilitate relation discovery and predication. PloS
simulation such that r2 shared half its semantic content        one, 8(6), e63889.
with the patient (rather than agent) role of R1. In 10        Doumas, L. A., Hummel, J. E., & Sandhofer, C. M. (2008). A
additional simulations, DORA successfully mapped the            theory of the discovery and predication of relational
patient role of R1 to r2 (along with their arguments). In       concepts. Psychological review, 115(1), 1-43
short, in all our simulations DORA overcame the n-ary         Engel, S.A., Rumelhart, D.E., Wandell, B.A., Lee, A.T.,
restriction, mapping the single-place predicate r2 onto         Glover, G.H., Chichilnisky, E.J., & Shadlen, M.N. (1994).
                                                                fMRI of human visual cortex, Nature, 369, 525.
the most similar relational role of R1.
                                                              Falkenhainer, B., Forbus, K. D., & Gentner, D. (1989). The
                                                                structure-mapping engine: Algorithm and examples.
                        Conclusion                              Artificial intelligence, 41(1), 1-63.
                                                              Gick, M. L., & Holyoak, K. J. (1983). Schema induction and
We have shown how structured relational representations
                                                                analogical transfer. Cognitive psychology, 15(1), 1-38.
of magnitude and similarity can be learned from objects       Goodale, M. A., Ellard, C. G., & Booth, L. (1990). The role of
with only absolute magnitude values. Our model exploits         image size and retinal motion in the computation of
regularities that emerge in a connectionist network when        absolute distance by the Mongolian gerbil (Meriones
distributed representations are compared or co-activated.       unguiculatus). Vision Research, 30(3), 399-413.
These regularities serve as invariant signals that the        Halford, G. S., Wilson, W. H., & Phillips, S. (1998).
model can learn to exploit to bootstrap the detection of        Processing capacity defined by relational complexity:
relative magnitude differences and similarities. When           Implications for comparative, developmental, and cognitive
linked with the DORA predicate learning algorithm, the          psychology. Behavioral and Brain Sciences, 21(06), 803-
                                                                831.
system learns structured predicate representations of
                                                              Holyoak, K. J. (2012). Analogy and relational reasoning. The
these relative magnitudes and similarities, and then can        Oxford handbook of thinking and reasoning, 234-259.
exploit the resulting representations to solve problems.      Holyoak, K. J., & Thagard, P. (1995). Mental leaps.
   Our account provides a trajectory for similarity           Hummel, J. E., & Holyoak, K. J. (2003). A symbolic-
cognition that maps to cognitive complexity across              connectionist theory of relational inference and
species and maturational trajectories in humans. This           generalization. Psychological review, 110(2), 220.
trajectory reveals three distinct levels of abstraction in    Hummel, J. E., & Holyoak, K. J. (1997). Distributed
SDML computation; (i) implicit detection of SDML                representations of structure: A theory of analogical access
(responding based on the regular firing that occurs when        and mapping. Psychological review, 104(3), 427.
                                                              Kemp, C. (2012). Exploring the conceptual universe.
absolute magnitudes are compared), (ii) implicit
                                                                Psychological review, 119(4), 685.
generalization of SDML (or learning based on the              Kemp, C., & Tenenbaum, J. B. (2009). Structured statistical
presence or absence of a particular feature; e.g., learning     models of inductive reasoning. Psychological review,
to respond based on the presence or absence of the              116(1), 20.
‘more’ feature), and (iii) predicate representations of       Kim, K. U., Huh, N., Jang, Y., Lee, D., & Jung, M. W.
SDML (or full-fledged relational representations that           (2015). Effects of fictive reward on rat's choice behavior.
support complex cognitive capacities like analogy and           Scientific reports, 5, 8040.
reasoning).                                                   Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015).
   This distinction may explain why humans solve some           Human-level concept learning through probabilistic
                                                                program induction. Science, 350(6266), 1332-1338.
tasks involving similarity judgments without the
                                                              Lu, H., Chen, D., & Holyoak, K. J. (2012). Bayesian analogy
extensive training that other animals require (e.g.,            with relational transformations. Psychological review,
Young, Wasserman, & Garner, 1997). Humans may                   119(3), 617.
solve the task relationally rather than relying on            Medin, D. L., Goldstone, R. L., & Gentner, D. (1993).
generalized implicit similarity judgments.                      Respects for similarity. Psychological review, 100(2), 254.
   Many cognitive architectures and task models rely on       Von der Malsburg, C. (1999). The what and why of binding:
stimulus recognition. This theory explains how stimulus         the modeler’s perspective. Neuron, 24(1), 95-104.
recognition might be computed. We believe that                Wandell, B. (1995). Foundations of Vision. Sinaur Associates
providing a computational account for a function                Inc.: Sunderland, MA.
                                                              Young, M. E., Wasserman, E. A., & Garner, K. L. (1997).
existing models depend on represents a significant
                                                                Effects of number of items on the pigeon's discrimination
architectural contribution.                                     of same from different visual displays. Journal of
                                                                Experimental Psychology: Animal Behavior Processes,
                        References                              23(4), 491.
Bassok, M., & Olseth, K. L. (1995). Object-based
   representations: Transfer between cases of continuous and
   discrete models of change. Journal of Experimental
                                                             1960

