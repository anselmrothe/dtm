          Inferring Human Interaction from Motion Trajectories in Aerial Videos
                      Tianmin Shu ∗ Yujia Peng ∗ Lifeng Fan Hongjing Lu Song-Chun Zhu
                         {tianmin.shu, yjpeng, lfan}@ucla.edu hongjing@ucla.edu sczhu@stat.ucla.edu
                        Department of Psychology and Statistics, University of California, Los Angeles, USA
                               Abstract
   People are adept at perceiving interactions from movements
   of simple shapes but the underlying mechanism remains un-
   known. Previous studies have often used object movements
   defined by experimenters. The present study used aerial videos
   recorded by drones in a real-life environment to generate de-
   contextualized motion stimuli. Motion trajectories of dis-
   played elements were the only visual input. We measured
   human judgments of interactiveness between two moving el-
   ements, and the dynamic change of such judgments over time.          Figure 1: Stimulus illustration. (Left) An example frame of an aerial
   A hierarchical model was developed to account for human per-         video recorded by a drone. Two people were being tracked (framed
   formance in this task, which represents interactivity using la-      by red and green boxes). (Right) A sample frame of an experimental
   tent variables, and learns the distribution of critical movement     trial. The two people being tracked in the aerial video are presented
   features that signal potential interactivity. The model provides     as two dots, one in red and one in green, in a black background. A
   a good fit to human judgments and can also be generalized to         video demonstration can be viewed on the project website: http://
   the original Heider-Simmel animations (1944). The model can          www.stat.ucla.edu/˜tianmin.shu/HeiderSimmel/CogSci17
   also synthesize decontextualized animations with controlled
   degree of interactiveness, providing a viable tool for studying      different contexts. These modeling studies illustrate the po-
   animacy and social perception.
                                                                        tential fruitfulness of using a Bayesian approach as a princi-
   Keywords: social interaction; motion; decontextualized ani-          pled framework for modeling human interaction shown in de-
   mation; hierarchical model; action understanding
                                                                        contextualized animations. However, these models have been
                                                                        limited to experimenter-defined movements, and by compu-
                          Introduction
                                                                        tational constraints imposed by the modelers for particular
People are adept at perceiving goal-directed action and infer-          application domains.
ring social interaction from movements of simple objects. In                The present study aims to generate Heider-Simmel-type
their pioneering work, Heider and Simmel (1944) presented               decontextualized animations using real-life videos of visual
video clips showing three simple geometrical shapes mov-                scenes. As a naturalistic example, imagine that you are
ing around, and asked human observers to describe what they             watching a surveillance video recorded by a drone from a
saw. Almost all observers described the object movements                bird’s eye view, as shown in Fig. 1. In such aerial videos,
in an anthropomorphic way, reporting a reliable impression              changes in human body postures can barely be seen, and the
of animacy and meaningful social interaction among the geo-             primary visual cues are the noisy movement trajectories of
metric shapes displayed in the decontextualized animation.              each person in the scene. This situation is analogous to the ex-
   Later studies (Dittrich & Lea, 1994; Scholl & Tremoulet,             perimental stimuli used in Heider and Simmel’s studies, but
2000; Tremoulet & Feldman, 2000, 2006; Gao, Newman, &                   the trajectories of each entity are directly based on real-life
Scholl, 2009; Gao, McCarthy, & Scholl, 2010) used more                  human movements.
controlled stimuli and systematically examined what factors                 In the present study, we first used real-life aerial videos to
can impact the perception of goal-directed actions in a decon-          generate decontextualized animations and to assess how hu-
textualized animation. The results provided converging evi-             man judgments of interactivity emerge over time. We devel-
dence that the perception of human-like interactions relies on          oped a hierarchical model to account for human performance.
some critical low-level motion cues, such as speed and mo-              One advantage of using aerial videos to generate decontex-
tion direction. However, it remains unclear how the human               tualized animations is that the technique provides sufficient
visual system combines motion cues from different objects to            training stimuli to enable the learning of a hierarchical model
infer interpersonal interactivity in the absence of any context         with hidden layers, which could illuminate the representa-
cues.                                                                   tions of critical movement patterns that signal potential inter-
   To address this fundamental question, Baker, Saxe, and               activity between agents. Furthermore, we assessed whether
Tenenbaum (2009) developed a Bayesian model to reason                   the learning component in the model can be generalized to
about the intentions of an agent when moving in maze-like               the original animations by Heider and Simmel (1944).
environments of the sort used by Heider and Simmel (1944).
Other studies (Baker, Goodman, & Tenenbaum, 2008; Ull-                                    Computational Model
man et al., 2009; Baker, 2012) developed similar models that
                                                                        We designed a hierarchical model with three layers. As
could be generalized to situations with multiple agents and
                                                                        shown in Fig. 2, the first layer (the X layer) estimates spa-
    ∗ These  two authors contributed equally.                           tiotemporal motion patterns within a short period of time.
                                                                    1066

              y1                    yk                  yK
                                                                                                    Coordinate
                                                                                                  Transformation
                                                                                              v2t                                      =            +
                                                                                                                     (0,0)
              s1           …        sk          …       sK                                    xt2
                                                                                                                    ṽt
                                                                                                                                         Ref. Agent
                                                                                                                                          Condition   Interactive Field
                                                                              (0,0)
                                                                                                                       t
                                                                                                                    x̃
              …                                          …
                             xt1            xt2                              Figure 3: Illustration of a conditional interactive field (CIF): after
                                                                             a coordinate transformation w.r.t. the reference agent, we model
                             t 2 Tk        t 2 Tk
                                                                             the expected relative motion pattern x̃t and ṽt conditioned on the
                                                                             reference agent’s motion.
Figure 2: Illustration of the hierarchical generative model. The solid
nodes are observations of motion trajectories of two agents, and the
remaining nodes are latent variables constituting the symbolic rep-           CIFs        +                               +                             +
resentation of an interaction, i.e., the original trajectories are coded
as a sequence of sub-interactions S and interaction labels Y .                            ⌧1                              ⌧2                            ⌧3
                                                                                S                                                                                     t
                                                                             Traj.                         Y
The second layer (the S layer) captures the involvement of
various motion fields at different stages of interactivity over a                                                        X
long period by temporally decomposing interactivity with la-                 Figure 4: Temporal parsing by S (middle). The top demonstrates
tent sub-interactions. The last layer (the Y layer) indicates the            the change of CIFs in sub-interactions as the interaction proceeds.
presence or absence of interactiveness between two agents.                   The bottom indicates the change of interactive behaviors in terms of
                                                                             motion trajectories. The colored bars in the middle depict the types
   The inputs to the model are motion trajectories of two                    of the sub-interactions.
agents, denoted as Γa = {xta }t=0,··· ,T , a = 1, 2. The position
of agent a (a = 1, 2) at time t is xta = (x, y). The total length
of the trajectory is T . Using the input of motion trajecto-                 linear dynamic system:
ries, we can readily compute the velocity sequence of agent a
                                                                                                      ṽt ∼ N (As x̃t + Bs , Σs ),                                 (1)
(a = 1, 2), i.e., Va = {vta }t=1,··· ,T , where vta = xta − xt−1
                                                              a .
   To capture the interactivity between two agents based on                  where As , Bs , and Σs = diag(σ2s1 , σ2s2 ) are the parameters
the observed trajectories of movements, the model builds on                  of the Gaussian distribution to be learned for each sub-
two basic components. (1) Interactivity between two agents                   interaction s. As x̃t + Bs can be interpreted as the expected
can be represented by a sequence of latent motion fields, each               motion at location x̃ in the field.
capturing the relative motion between the two agents who
perform meaningful social interactions. (2) Latent motion                    Temporal Parsing by Latent Sub-Interactions
fields can vary over time, capturing the behavioral change of                We assume that a long interactive sequence can be decom-
the agents over a long period of time. The details for quanti-               posed into several distinct sub-interactions each with a dif-
fying the two key components are presented in the next two                   ferent CIF. For example, when observing that two people
subsections.                                                                 walk towards each other, shake hands and walk together,
                                                                             we can decompose this interactive sequence into three sub-
Conditional Interactive Fields                                               interactions. We represent meaningful interactivity as a se-
                                                                             quence of latent sub-interactions S = {sk }k=1,...,K , where a
As illustrated in Fig. 3, we use conditional interactive fields              latent sub-interaction determines the category of the CIF in-
(CIFs) to model how an agent moves with respect to a refer-                  volved in a time interval Tk = {t : tk1 ≤ t ≤ tk2 }, such that
ence agent. We randomly select an agent to be the reference                  st = sk , ∀t ∈ Tk . sk is the sub-interaction label in the k-th in-
agent, and then model the partner agent’s movement by esti-                  terval representing the consistent interactivity of two agents
mating a vector field of the relative motion conditioned on a                in the interval. Fig. 4 illustrates the temporal parsing.
specific distribution of the reference agent’s motion.                           In each interval k, we define an interaction label yk ∈ {0, 1}
   To ensure that the fields are orientation invariant, we per-              to indicate the absence or presence of interactivity between
form a coordinate transformation as Fig. 3 illustrates. At each              the two agents. The interaction labels also constitute a se-
time point t, the transformed position of the reference agent                quence Y = {yt }t=1,··· ,T . We have yt = yk , ∀t ∈ Tk , where yk
is always located at (0, 0), and its transformed velocity di-                is the interaction label in interval Tk .
rection is always pointed to the norm of the upward vertical
direction. Consequently, the position and velocity of the sec-                                       Model Formulation
ond agent after the transformation, i.e., Γ̃ = {x̃t }t=0,··· ,T and          Given the input of motion trajectories Γ, the model infers the
Ṽ = {ṽt }t=1,··· ,T , can be used to model the relative motion.            posterior distribution of the latent variables S and Y ,
   For a sub-interaction s (interactivity in a relatively short
time sharing consistent motion patterns, e.g., approaching,                             p(S,Y |Γ) ∝ P(Γ | S,Y ) · P(S | Y ) · P(Y ) .                              (2)
                                                                                                          | {z } | {z } | {z }
walking together, standing together), we define its CIF as a                                                 likelihood      sub int. prior int. prior
                                                                         1067

   The likelihood assesses how well the motion fields under                            where,
corresponding CIFs of sub-interactions can account for rela-
tive motion observed in the video input, the spatial density of                             p(st | S1:t−1 , yt )
the relative position and the observed motion of the reference                                      p(τ ≥ |Tk | + 1 | st = st−1 , yt ) if st = st−1           .  (8)
                                                                                            =
agent:                                                                                              p(τ ≥ 1 | st , yt )p(st |st−1 )          otherwise
                            K                                                             Then the posterior probability of yt = 1 given st ∈ S is de-
     p(Γ | S,Y ) = ∏            ∏ p(ṽt , x̃t , vt1 | st = sk , yt = yk ),      (3)    fined as
                           k=1 t∈Tk
                                                                                              p(yt | st , Γ0:t , S1:t−1 ) ∝ p(st | Γ0:t , S1:t−1 , yt )p(yt ),   (9)
where
                                                                                          This computation makes it possible to perform the follow-
     p(ṽt , x̃t , vt1 | st = sk , yt = yk )                                           ing inferences and online prediction: i) we maximize (7) to
     = p(ṽt | x̃t , sk , yk ) · p(x̃t | sk , yk ) · p(||vt1 || | sk , yk ) .   (4)    obtain the optimal st ; ii) we use (9) to compute the posterior
        |           {z        } |         {z      } |          {z        }             probability of two agents being interactive at t under the CIF
              rel. motion        rel. spatial density     ref. motion
                                                                                       of st as an approximation of the judgment of interaction/non-
Note that vt1 is the reference agent’s velocity. When yk = 1,                          interaction provided by human observers; iii) the model can
the first term is defined in equation (1), the second term is                          synthesize new trajectories using the following computation,
learned by Gaussian kernel density estimation, and the third
                                                                                                              st+1 ∼ p(st+1 | S1:t , yt+1 ),                    (10)
term is defined as a Weibull distribution, which is suitable for
learning a long-tail distribution of a non-negative variable.                                xt+1     t+1
                                                                                                                    p(xt+1      t+1 t t t+1 t+1
                                                                                              1 , x2         ∼          1 , x2 |x1 , x2 , s        ,y )
When yk = 0, the first term is defined as a Gaussian distri-                                                 =      p(ṽt+1 , x̃t+1 , vt+1 | s t+1 , yt+1 ) ,   (11)
                                                                                                                                       1
bution N ([0, 0]> , Σ0 = diag(σ20 , σ20 )), and the remaining two
                                                                                                                                                  t+1 t         t+1
terms are uniform distributions in quantized spaces.                                   where ṽt+1 , x̃t+1 , and vt+1                        t
                                                                                                                     1 are given by x1 , x1 , x2 and x2 ,
                                                                                       and the last term is defined in (4). By setting y                  t+1 = 1 or
   We model the prior term of sub-interactions P(S|Y ) using
two independent components, i) the duration of each sub-                               yt+1 = 0 in (10) and (11).
interaction, and ii) the transition probability between two con-
secutive sub-interactions, as follows:                                                                                 Learning
                                                                                       Algorithm
                             K                       K
          p(S | Y ) = ∏ p(|Tk ||sk , yk ) ∏ p(sk |sk−1 , yk ) .                 (5)    To train the model, we used Gibbs sampling to find the S that
                            k=1
                                |       {z       } k=2 |       {z       }              maximizes the joint probability P(Y, S, Γ). The implementa-
                                      duration             transition                  tion details are summarized below:
When yk = 1, the two terms follow a log-normal distribution                            • Step 0: To initialize S, we first construct a feature vec-
and a multinomial distribution respectively; when yk = 0, uni-                            tor for each time t, i.e., [||vt1 ||, x̃t , ṽt ]> . A K-means clus-
form distributions are used for the two terms instead.                                    tering is then conducted to obtain the initial {st }, which
   Finally, we use a Bernoulli distribution to model the prior                            also gives us the sub-interaction parsing S after merging
term of interactions P(Y ),                                                               the same consecutive st .
                   K                            K                                      • Step 1: At each time point t of every training video, we
                                                           t               t
    p(Y ) = ∏          ∏    p(yt = yk ) = ∏          ∏ ρy (1 − ρ)1−y .          (6)       update its sub-interaction label st by
                 k=1 t∈Tk                      k=1 t∈Tk
                                                                                                     st ∼ p(Γ | S−t ∪ {st },Y )p(S−t ∪ {st } | Y ),             (12)
                      Inference and Prediction
                                                                                          where S−t is the sub-interaction temporal parsing exclud-
The model infers the current status of latent variables and pro-                          ing time t, and S−t ∪ {st } is a new sub-interaction sequence
duces an online prediction of future trajectories. Inference                              after adding the sub-interaction at t. Note that Y is always
and prediction are performed for each time point from 1 to                                fixed in the procedure; thus we do not need p(Y ) term for
T sequentially (rather than offline prediction, which gives the                           sampling purpose.
labels after watching the entire video).
   We denote trajectories from 0 to t as Γ0:t , and the sub-                           • Step 2: If S does not change anymore, go to next step;
interactions from 1 to t − 1 as S1:t−1 . Without loss of gen-                             otherwise, repeat step 1.
erality, we assume there are K sub-interactions in S1:t−1 with                         • Step 3: Since we do not include the non-interactive videos
TK being the last interval and st−1 = sK . We first infer st under                        in the training set, we selected 22 videos in the first human
the assumption of interaction (i.e., yt = 1) by maximizing                                experiment (a mixture of interactive and non-interactive
                                                                                          videos) as a validation set to estimate ρ and σ0 by maxi-
    p(st | Γ0:t , S1:t−1 , yt ) ∝ p(ṽt , x̃t , vt1 | st )p(st | S1:t−1 , yt ),           mizing the correlation between the model prediction of (9)
                                                                                (7)       and the average human responses in the validation set.
                                                                                   1068

Model Simulation Results                                               the midpoint at the center of the screen in the first frame. The
We tested the model using two sets of training data. The first         coordinates were temporally smoothed by averaging across
dataset is a UCLA aerial event dataset collected by Shu et al.         the adjacent 5 frames.
(2015), in which about 20 people performed some group ac-                 24 non-interactive stimuli were generated by interchanging
tivities in two scenes (a park or a parking lot), such as group        motion trajectories of two people selected from two irrelevant
touring, queuing in front of a vending machine or playing fris-        interactive videos (e.g., the motion of one dot in video 1 re-
bee. People’s trajectories and their activities are manually an-       combined with the motion of a dot in video 2). The starting
notated. The dataset is available at http://www.stat.ucla              distances between two dots in non-interactive stimuli were
.edu/˜tianmin.shu/AerialVideo/AerialVideo.html                         kept the same as in the corresponding interactive stimuli.
   We selected training videos including interactivity from the           The duration of stimuli varied from 239 frames to 500
database, so that the two agents always interact with each             frames (mean frame = 404), corresponding to 15.9 to 33.3
other in all training stimuli. Thus, for any training video,           seconds, with a recording refresh rate of 15 frames per sec-
yt = 1, ∀t = 1, · · · , T . During the training phase, we excluded     ond. The diameters of dots were 1◦ of visual angle. One
the examples used in human experiments. In total, there were           dot was displayed in red (1.8 cd/m2 ) and the other in green
131 training instances.                                                (30 cd/m2 ) on a black background (0 cd/m2 ). Among the 48
   In the implementation, we manually define the maximum               pairs of stimuli, four pairs of actions (two interactive and two
number of sub-interaction categories to be 15 in our full              non-interactive) were used as practice.
model (i.e., |S | = 15), which is over-complete for our train-
ing data according to learning (low frequency in the tail of           Participants
Fig. 6). With simulated annealing (Kirkpatrick, Gelatt, &              33 participants (mean age = 20.4; 18 female) were enrolled
Vecchi, 1983), Gibbs sampling converges within 20 sweeps               from the subject pool at the University of California, Los An-
(where a sweep is defined as all the latent sub-interaction la-        geles (UCLA) Department of Psychology. They were com-
bels have been updated once). The frequencies of the top 15            pensated with course credit. All participants had normal or
CIFs are highly unbalanced. In fact, the top 10 CIFs account           corrected-to-normal vision.
for 83.8% of the sub-interactions in the training data. The
first row of Fig. 5 provides a visualization of the top 5 CIFs.        Procedures
   The second dataset was created from the original Heider-            Participants were seated 35 cm in front of a screen, which
Simmel animation (i.e., two triangles and one circle). We              had a resolution of 1024×768 and a 60 Hz refresh rate. First,
extracted the trajectories of the three shapes, and thus ob-           participants were given a cover story: “Imagine that you are
tained 3 pairs of two-agent interactions. We truncated the             working for a company to infer whether two people carry out
movie into short clips (about 10 seconds) to generate a to-            a social interaction based on their body locations measured by
tal of 27 videos. The same algorithm was used to train the             GPS signals. Based on the GPS signal, we generated two dots
model with 15 types of CIFs. The most frequent five CIFs               to indicate the location of the two people being tracked.” The
are visualized in the second row of Fig. 5. Clearly, the richer        task was to determine when the two dots were interacting with
behavior in the Heider-Simmel animation yielded a variety              each other and when they were not. Participants were asked
of CIFs with distinct patterns compared to the CIFs learned            to make continuous responses across the entire duration of
from aerial videos. The frequencies of CIFs are also more              the stimuli. They were to press and hold the left-arrow or
distributed in this dataset, as shown in Fig. 6.                       right-arrow button for interactive or non-interactive moments
   We observed a few critical CIFs that signal common in-              respectively, and to press and hold the down-arrow button if
teractions from the two simulation results. For instance, in           they were unsure. If no button was pressed for more than one
aerial videos, we observed i) approaching, e.g., CIF 1 and ii)         second, participants received a 500 Hz beep as a warning.
walking in parallel, or following, e.g., the lower part of CIF            Participants were presented with four trials of practice at
2; the Heider-Simmel animation revealed additional patterns            the beginning of the session to familiarize them with the task.
such as i) orbiting, e.g., CIF 1, ii) walking-by, e.g., CIF 5, and     Next, 44 trials of test stimuli were presented. The order of
iii) leaving, e.g., CIF 4.                                             trials was randomized for each participant. No feedback was
                                                                       presented on any of the trials. The experiment lasted for about
                             Experiment                                30 minutes in total.
Stimuli
24 interactive stimuli were generated from different pairs of
                                                                       Results
human interactions in aerial videos. We selected two people            Interactive, unsure and non-interactive responses were coded
interacting with each other in each aerial video. We then gen-         as 1, 0.5, and 0, respectively. Frames with no responses were
erated the decontextualized animations by depicting the two            removed from the comparison. Human responses were shown
people as dots with different colors. The dots’ coordinates            in Fig. 8 (left). A paired-sample t-test revealed that the aver-
were first extracted from the aerial videos by human annota-           age ratings of non-interactive actions (M = 0.34, SD = 0.13)
tors. Note that the two dots were first re-centered to localize        were significantly lower than interactive actions (M = 0.75,
                                                                   1069

  Learned from
  aerial videos
Learned from
Heider-Simmel
    movie
Figure 5: Interactive fields of the top five frequent CIFs learned from aerial videos (top) and Heider-Simmel movie (bottom) respectively.
In each field, the reference agent (red dot) is at the center of a field i.e., (0,0), moving towards north; the arrows represent the mean relative
motion at different locations and the intensities of the arrows indicate the relative spatial density which increases from light to dark.
                                   0.5
                                                                                                  to this parameter. The results clearly show that i) only using
                  Heider- Aerial
                                                                                                  one type of sub-interaction provides reasonably good results,
                                    0
                                         1 2 3 4 5 6 7 8 9 101112131415                           r = .855, and ii) by increasing the number of sub-interactions
                  Simmel Videos
                                   0.5
                                                                                                  |S |, the fits to human ratings were further improved until
                                    0
                                                                                                  reaching a plateau with a sufficiently large number of sub-
                                         1 2 3 4 5 6 7 8 9 101112131415
                                                                                                  interactions.
Figure 6: The frequencies of learned CIFs with the training data
generated from aerial videos (top) and the Heider-Simmel movie                                       Fig. 7 shows results for a few videos, with both model pre-
(bottom). The numbers on the x axis indicate the IDs of CIFs, ranked                              dictions and human ratings. The model predictions accounted
according to the occurrence frequency in the training data.                                       for human ratings quite well in most cases. However, the
                                                                  Hierarchical Model              model predictions were slightly higher than the average hu-
        Method    HMM                One-Interaction
                                                       |S | = 5        |S | = 10   |S | = 15      man ratings, which may be due to the lack of negative exam-
          r       0.739                    0.855        0.882            0.911       0.921        ples in the training phase. We also observed high standard
        RMSE      0.277                    0.165        0.158            0.139       0.134        deviations in human responses, indicating the large variabil-
                                                                                                  ity of the online prediction task for every single frame in a
Table 1: The quantitative results of all methods in experiment 1 us-
ing aerial videos as training data.                                                               dynamic animation. In general, the difference between our
                                                                                                  model’s predictions and human responses are seldom larger
SD = 0.13), t(32) = 13.29, p < 0.001. This finding indi-                                          than one standard deviation of human responses.
cates that human observers are able to discriminate interac-
                                                                                                    We also tested the model trained from the Heider-Simmel
tivity based on decontextualized animations generated from
                                                                                                  movie on the same testing set (generated from the aerial
the real-life aerial videos.
                                                                                                  videos), yielding a correlation of 0.640 and RMSE of 0.227.
   To compare the model predictions with human continuous
                                                                                                  The reduced fitting result indicates the discrepancy between
judgments, we computed the average human ratings, and ran
                                                                                                  two types of videos. The CIFs learned from one dataset may
the model to simulate online predictions of sub-interaction
                                                                                                  be limited in generalization to the other dataset.
and interaction labels on the testing videos (excluding the
ones in the validation set). Specifically, we used (9) to com-                                        One advantage of developing a generative model is that it
pute the probability of two agents being interactive with each                                    enables the synthesis of new videos by (10) and (11), based
other at any time point t. The model simulation used the                                          on randomly sampled initial positions of the two agents (x01 ,
hyper-parameters ρ = 10−11 and σ0 = 1.26.                                                         x02 ) and the first sub-interaction s1 . By setting the interaction
   Table 1 summarizes the Pearson correlation coefficient r                                       labels to be 1 or 0, the synthesized stimuli can be controlled
and root-mean-square error (RMSE) between the model pre-                                          to vary the degree of interactiveness. We ran a second experi-
dictions and the human ratings using aerial videos as train-                                      ment using model synthesized animations (10 interactive and
ing data. We compare our hierarchical model with two base-                                        10 non-interactive clips). These synthesized videos were pre-
line models: i) Hidden Markov Model (HMM), where the                                              sented to human observers in random orders and the interac-
latent variables st and yt only depend on their preceding vari-                                   tive ratings were recorded. The interactiveness between the
ables st−1 and yt−1 ; ii) a model with only one type of sub-                                      two agents in the synthesized videos was judged accurately
interaction. Both models yielded poorer fits to human judg-                                       by human observers (mean rating of 0.85 for synthesized in-
ments (i.e., lower correlation and higher RMSE) than the hi-                                      teractive clips, and 0.15 for non-interactive clips), suggesting
erarchical model. In addition, we changed the number of sub-                                      that the model effectively captured the visual features that sig-
interaction categories to examine how sensitive our model is                                      nal potential interactivity between agents.
                                                                                               1070

Figure 7: Comparison of online predictions by our full model (|S | = 15) (orange) and humans (blue) over time (in seconds) on testing videos.
The shaded areas show the standard deviations of human responses at each moment.
                                            Interactive
                                                                                               man reasoning about beliefs, desires, goals, and social re-
                                                                                               lations. Unpublished doctoral dissertation, Massachusetts
                  Interactive ratings
                                              Not sure                                         Institute of Technology.
                                                                                             Baker, C. L., Goodman, N. D., & Tenenbaum, J. B. (2008).
                                                                                               Theory-based social goal inference. In Proceedings of the
                                        Non-interactive
                                                          Interactive   Non-interactive        thirtieth annual conference of the cognitive science society
                                                          actions       actions
                                                                                               (p. 1447-1452).
Figure 8: Mean ratings of the interactive versus non-interactive ac-                         Baker, C. L., Saxe, R., & Tenenbaum, J. B. (2009). Action
tions in the experiment. Error bars indicate +/- 1 SEM.                                        understanding as inverse planning. Cognition, 113(3), 329-
                                                                                               349.
                                                    Conclusion                               Dittrich, W. H., & Lea, S. E. (1994). Visual perception of
In this paper, we examined human perception of social in-                                      intentional motion. Perception, 23(3), 253-268.
teractions using decontextualized animations based on move-                                  Dollár, P., Rabaud, V., Cottrell, G., & Belongie, S. (2005).
ment trajectories recorded in aerial videos of a real-life en-                                 Behavior recognition via sparse spatio-temporal features.
vironment, as well as Heider-Simmel-type animations. The                                       In In proceedings of ieee international conference on com-
proposed hierarchical model built on two key components:                                       puter vision workshops (pp. 65–72).
conditional interactive fields of sub-interactions, and tem-                                 Gao, T., McCarthy, G., & Scholl, B. J. (2010). The wolfpack
poral parsing of interactivity. The model fit human judg-                                      effect: Perception of animacy irresistibly influences inter-
ments of interactiveness well, and suggests potential mech-                                    active behavior. Psychological Science, 21, 1845-1853.
anisms underlying our understanding of meaningful human                                      Gao, T., Newman, G. E., & Scholl, B. J. (2009). The psy-
interactions. Human interactions can be decomposed into                                        chophysics of chasing: A case study in the perception of
sub-interactions such as approaching, walking in parallel, or                                  animacy. Cognitive Psychology, 59(2), 154-179.
standing still in close proximity. Based on the transition prob-                             Heider, F., & Simmel, M. (1944). An experimental study of
abilities and the duration of sub-components, humans are able                                  apparent behavior. American Journal of Psychology, 57(2),
to make inferences about how likely the two people are inter-                                  243-259.
acting.                                                                                      Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Opti-
   The model could be extended to be applied to the field of                                   mization by simulated annealing. Science, 220(4598), 671-
behavioral recognition. While previous work has focused                                        680.
on actions of individuals based on detecting local spatial-                                  Scholl, B. J., & Tremoulet, R. D. (2000). Perceptual causality
temporal features embedded in videos (Dollár, Rabaud, Cot-                                    and animacy. Trends in Cognitive Sciences, 4(8), 299-309.
trell, & Belongie, 2005), the current work can deal with multi-                              Shu, T., Xie, D., Rothrock, B., Todorovic, S., & Zhu, S.-C.
agent interaction. Understanding of the relation between                                       (2015). Joint inference of groups, events and human roles
agents could facilitate the recognition of individual behav-                                   in aerial videos. In Proceedings of ieee conference on com-
iors by putting single actions into meaningful social contexts.                                puter vision and pattern recognition.
In addition, the current model is only based on visual motion                                Tremoulet, P. D., & Feldman, J. (2000). Perception of ani-
cues. The model could be enhanced by incorporating a cogni-                                    macy from the motion of a single object. Perception, 29(8),
tive mechanism (e.g., a theory-of-mind framework) to enable                                    943-951.
explicit inference of intentions.                                                            Tremoulet, P. D., & Feldman, J. (2006). The influence of spa-
                                                                                               tial context and the role of intentionality in the interpreta-
                                         Acknowledgement                                       tion of animacy from motion. Perception & Pyschophysics,
This research was funded by a NSF grant BCS-1353391 to                                         68(6), 1047-1058.
HL and DARPA MSEE project FA 8650-11-1-7149 and ONR                                          Ullman, T., Baker, C. L., Macindoe, O., Evans, O., Goodman,
MURI project N00014-16-1-2007 for SZ.                                                          N., & Tenenbaum, J. B. (2009). Help or hinder: Bayesian
                                                                                               models of social goal inference. In Proceedings of ad-
                                                    References                                 vances in neural information processing systems (p. 1874-
Baker, C. L. (2012). Bayesian theory of mind: modeling hu-                                     1882).
                                                                                          1071

