The statistical significance filter leads to overconfident expectations of replicability
                                             Shravan Vasishth (vasishth@uni-potsdam.de)
                               Department of Linguistics, University of Potsdam, Potsdam 14476, Germany.
                                            Andrew Gelman (gelman@stat.columbia.edu)
                               Department of Statistics, Columbia University, New York, NY 10027, USA.
                                Abstract                                   discusses this exaggeration of effects in epidemiological stud-
                                                                           ies in terms of the vibration ratio: the ratio of largest to small-
   We show that publishing results using the statistical signif-
   icance filter—publishing only when the p-value is less than             est observed effects.
   0.05—leads to a vicious cycle of overoptimistic expectation                These overestimates get published and fill the literature.
   of the replicability of results. First, we show analytically that       Now consider what happens when researchers design a new
   when true statistical power is relatively low, computing power
   based on statistically significant results will lead to overesti-       study. They read the literature and see all these big effects,
   mates of power. Then, we present a case study using 10 exper-           then plan their next study. They do a power calculation based
   imental comparisons drawn from a recently published meta-               on these big effects and get an exaggerated estimate of power,
   analysis in psycholinguistics (Jäger et al., 2017). We show that
   the statistically significant results yield an illusion of replica-     and can easily convince themselves that they have a high pow-
   bility. This illusion holds even if the researcher doesn’t con-         ered study. Alternatively—and this is probably the more com-
   duct any formal power analysis but just uses statistical signifi-       mon route in many fields, such as psychology—they don’t
   cance to informally assess robustness (i.e., replicability) of re-
   sults.                                                                  do a formal power analysis, but just rely on the informal ob-
   Keywords: Statistical significance; p-values; replicability             servation that most of the previously published results had a
                                                                           significant effect and so the effect must be present.
   “‘. . . in [an]. . . academic environment that only publishes              A related observation about overestimation comes from the
   positive findings and rewards publication, an efficient                 replication attempts reported by the Open Science Collabora-
   way to succeed is to conduct low power studies. Why?                    tion (2015). The authors report that the magnitude of the pub-
   Such studies are cheap and can be farmed for significant                lished p-values from the original studies were predictive of
   results, especially when hypotheses only predict differ-                replication success. As they put it (p. 943): “. . . correlational
   ences from the null, rather than precise quantitative dif-              evidence is consistent with the conclusion that variation in
   ferences and trends.” (Smaldino & McElreath, 2016, p.                   the strength of initial evidence (such as original P value)
   5)                                                                      was . . . predictive of replication success . . . ” From this, re-
                                                                           searchers might erroneously conclude that lower p-values are
                             Introduction                                  generally more predictive of replication success. In other
The statistical significance filter tells us that significant              words, an erroneous conclusion would be that a lower p-value
results—those findings in which the p-value is less than                   suggests a higher probability that the effect can be detected in
0.05—are positively biased. The statistically significant esti-            future repeated studies.
mate is, by definition, more than t standard errors away from                 We show that if statistical significance is used as a filter
zero, where t is some critical value determined by a statistical           for publishing a result, and the observed effect (or p-value) is
test (such as the t-test) and the pre-specified Type I error (the          used to determine replicability, this will lead the researcher
probability, under repeated sampling, of incorrectly rejecting             to overestimate replicability. We demonstrate this point ana-
the null hypothesis).                                                      lytically, and then present a case study involving 10 reading
   Statistical power is the probability, under repeated sam-               studies in psycholinguistics that illustrates this illusion.
pling, of correctly rejecting the null hypothesis assuming that
the parameter of interest has some true point value µ.1 It is                     The relationship between p-values and
well-known that when statistical power is low, the effect (the                                    estimated power
sample mean) will tend to be exaggerated. These are referred
to as Type M errors by Gelman and Carlin (2014) (also see                  Assume for simplicity the case that we carry out a one-sided
Gelman & Tuerlinckx, 2000). This exaggeration of effects                   statistical test where the null hypothesis is that the null hy-
has been noticed in previous work (Hedges, 1984; Lane &                    pothesis mean is µ0 = 0 and the alternative is that µ > 0.2
Dunlap, 1978), and most recently in neuroscience and epi-                  Given some continuous data x1 , . . . , xn , we can compute the
demiology, where Button et al. (2013) refer to the exagger-                t-statistic and derive the p-value from it. For a large sample
ation of effects in neuroscience as the “winner’s curse” and               size n, a normal approximation allows us to use the z-statistic,
“the vibration of effects.” In related work, Ioannidis (2008)              Z = σX̄−µ √0 , to compute the p-value. Here, X̄ is the mean, σX
                                                                                   X/ n
    1 In order to compute power, we need to have an estimate of the        the standard deviation, and n the sample size.
true effect, the sample size, and an estimate of the standard devia-
tion.                                                                          2 The presentation below generalizes to the two-sided test.
                                                                       1272

   The p-value is the probability of observing the z-statistic
or a value more extreme assuming that the null hypothesis is
true. The p-value is a random variable P with the probability
density function (Hung, O’Neill, Bauer, & Kohne, 1997):
                                                                                     1.0
                           φ(Z p − δ)
               gδ (p) =               ,   0< p<1               (1)
                             φ(Z p )                                                 0.8
where
                                                                                     0.6
• φ(·) is the pdf of the standard normal distribution, Nor-                  Power
  mal(0,1).                                                                          0.4
• Z p , a random variable, is the (1-p)th percentile of the stan-                    0.2
  dard normal distribution.
                                                                                     0.0
• δ = σµ−µ √0 is the true point value expressed as a z-score.
         X/ n
  Here, µ is the true (unknown) point value of the parameter                               0         1        2         3          4
  of interest.
                                                                                                    Observed Z−score
   Hung et al. (1997) further observe that the cumulative dis-
tribution function (cdf) of P is:
                                                                         Figure 1: The relationship between power and the observed z-
              Z p
                                                                         score. The larger z-scores are easier to publish due to the sta-
   Gδ (p) =         gδ (x) dx = 1 − Φ(Z p − δ),    0< p<1      (2)       tistical significance filter, and these published studies there-
               0
                                                                         fore give a mistaken impression of higher power.
where Φ(·) is the cdf of the standard normal.
   Once we have observed a particular z-statistic z p , the cdf
Gδ (p) allows us to estimate power based on the z-statistic              some scale and standard deviation is 1, then statistical power
(Hoenig & Heisey, 2001). To estimate the p-value given that              is 15%.3
the null hypothesis is true, let the true value be µ = 0. It                If we now re-run the same study, collecting 36 data points
follows that δ = 0. Then:                                                each time, and impose the condition that only statistically sig-
                                                                         nificant results with Type I error α = 0.05 are published, then
                           p = 1 − Φ(z p )                     (3)       only observed z-scores larger than 1.64 (for a one-sided test)
                                                                         would be published and the power estimate based on these
  To estimate power from the observed z p , set δ to be the              z-scores must have a lower bound of
observed statistic z p , and let the critical z-score be zα , where
α is the Type I error (typically 0.05). The power is therefore:                            GZα (α) = 1 − Φ(1.64 − 1.64) = 0.5            (5)
                                                                         Thus, in a scenario where the real power is 15%, and only z-
                     Gz p (α) = 1 − Φ(zα − z p )               (4)       scores greater than or equal to zα are published, power based
   In other words, power estimated from the observed statis-             on the z-score will be overestimated by at least a factor of
tic is a monotonically increasing function of the observed z-            0.5/0.15=3.33. Call this ratio the Power Inflation Index (PII).
statistic: the larger the statistic, the higher the power estimate          Now, lower p-values are widely regarded as more “reli-
based on this statistic (Figure 1). Together with the com-               able” than p-values near the Type I error probability of 0.05.4
mon practice that only statistically significant results get pub-        This incorrect belief, widely shared by editors, reviewers, and
lished, and especially results with a large z-statistic, this leads      authors in areas like psychology and linguistics, has the effect
to overestimates of power. As mentioned above, one doesn’t               that studies with lower p-values are more likely to be reported
need to actually estimate power in order to fall prey to the                3 This    can be confirmed by running the fol-
illusion; merely scanning the statistically significant z-scores         lowing command using R (R Core Team,                          2014):
gives an impression of consistency and invites the inference             power.t.test(delta=0.1,sd=1,n=36,alternative =
that the effect is replicable and robust. The word “reliable” is         "one.sided",type="one.sample").
                                                                            4 Treating lower p-values as furnishing more evidence against the
frequently used in psychology, presumably with the meaning               null hypothesis reflects a misunderstanding about the meaning of
that the result is replicable and represents the reality.                the p-value; given a continuous dependent measure, when the null
   A direct consequence of Equation 4 is that overestimates              hypothesis that µ = 0 is true, under repeated sampling the p-value
                                                                         has a uniform distribution (see proof in the Appendix). This has the
of the z-statistic will lead to overestimates of power. For ex-          consequence that, when the null is true, a p-value near 0 is no more
ample, if we have 36 data points and the true effect is 0.1 on           surprising than a p-value near 0.05.
                                                                      1273

and published, with the consequence that the PII will tend to       Using a Bayesian random-effects meta-analysis to
be even higher than the lower bound discussed here.                 estimate the power function
   We turn next to a case study involving psycholinguistic          In Table 1, we calculated power based on the individual stud-
data that illustrates the illusion of replicability.                ies. As discussed above, these will tend to be overestimates
                                                                    because there is a preference to publish effects with low p-
    Case study: Interference effects in reading                     values. How can we check this for the 10 studies? True power
                              studies                               is unknown so we have no basis for comparing the power es-
                                                                    timates from individual studies with a true value for power.
To illustrate the illusion of replicability, we consider the           One way to arrive at a conservative estimate of the true
10 experiments that were reviewed in the literature review          power given these 10 studies is to carry out a Bayesian
and meta-analysis presented in Jäger, Engelmann, and Va-           random-effects meta-analysis (Gelman et al., 2014). This
sishth (2017). These were psycholinguistic studies in which         hierarchical modelling approach allows us to determine the
the dependent measure was reading time in milliseconds of           posterior distribution of the effect, which can then be used
words. The experimental manipulation involved pairs of sen-         for computing an estimate of power. As discussed in Button
tence types where one type was easier to read than the other;       et al. (2013), using estimates from a meta-analysis yields a
the empirical phenomenon of interest here is interference in        more conservative estimate of power. In the random-effects
working memory. Here, an appropriate statistical test is the        meta-analysis, this conservativity arises due to the shrinkage
two-sided paired t-test (one could do a one-sided t-test, al-       property of hierarchical models: Larger sample studies re-
though this is less common in psycholinguistics).                   ceive a greater weighting in determining the posterior than
   We had the raw data from these 10 studies and so were            smaller sample studies. Note, however, that even here the
able to carry out the pairwise comparison. As discussed in          power may be an overestimate due to the fact that the studies
detail in Jäger et al. (2017), theory predicts an effect with a    that go into the meta-analysis are likely to have publication
negative sign. The original results as published were analyzed      bias. But as we show below, the estimates of power from
on the raw milliseconds scale, but here we analyze the data on      individual studies tend to be ever larger.
the log milliseconds scale because the reading time data were          The random-effects meta-analysis model was set up as fol-
log-normally distributed.                                           lows. Let yi be the effect size in log milliseconds in the i-th
                                                                    study, where i ranges from 1 to n. Let µ be the true (un-
   A summary of the pairwise t-test is shown in Table 1. From
                                                                    known) effect in log ms, to be estimated by the model, and
the table, it is clear that the studies consistently found neg-
                                                                    µi the true (unknown) effect in each study. Let σi log ms be
ative values for the coefficient; this consistent result raises
                                                                    the true standard deviation of the sampling distribution; each
our confidence in the reproducibility of the result. A formal
                                                                    σi is estimated from the sample standard error from study i.
power analysis based on these studies, also shown in the last
                                                                    The standard deviation parameter τ represents between-study
column of the table, leads to estimates of power ranging from
                                                                    variability.
17 to 60%.
                                                                       Then, our model for n studies is as follows. The model as-
                                                                    sumes the i-th data point (the effect observed on the log ms
               t      d       n    se      s   pval  power          scale) yi is generated from a normal distribution with mean
      1    -1.9    -0.1      40   0.0    0.2     0.1    0.3         µi and some standard error σ, estimated from the sample’s
      2    -3.1    -0.1      32   0.0    0.1     0.0    0.6         standard error. Each of the true underlying means µi are as-
      3    -1.5    -0.0      32   0.0    0.2     0.2    0.2         sumed to be generated from a normal distribution with true
      4    -2.1    -0.0      32   0.0    0.1     0.0    0.3         mean µ and between-study standard deviation τ. We assign
      5    -1.7    -0.0      32   0.0    0.1     0.1    0.2         Cauchy(0,2.5) priors to the parameters µ and µi , and a trun-
      6    -2.6    -0.1      28   0.0    0.2     0.0    0.4         cated Cauchy(0,2.5) prior for the between-study standard de-
      7    -1.6    -0.0      60   0.0    0.2     0.1    0.2         viation τ, truncated so that τ is greater than 0. The model can
      8    -3.2    -0.1      44   0.0    0.2     0.0    0.6         be stated mathematically as follows:
      9    -1.9    -0.1      60   0.0    0.2     0.1    0.3
     10    -2.6    -0.0    114    0.0    0.2     0.0    0.5
                                                                               Likelihoods:
Table 1: Results from the paired t-tests for the 10 experimen-
tal comparisons. Shown are the t-score, the effect d in log                      yi | µi , σ2i ∼Normal(µi , σ2i ) i = 1, . . . , n
ms, the sample size n, the standard error se, the standard de-                    µi | θ, τ2 ∼Normal(µ, τ2 ),
viation s, and the p-value. The t-tests were done on the raw                           Priors:                                     (6)
data from the original studies (the t-values reported here may
deviate slightly from the published t-values). Also shown is                                µ ∼Cauchy(0, 2.5),
the power estimated from each study.                                                       µi ∼Cauchy(0, 2.5),
                                                                                            τ ∼Cauchy(0, 2.5), τ > 0
                                                                1274

                                                                                    • one sample for the precision by sampling from the
                                                                                      Gamma(5.3, 0.3), and then converting this to a standard
                                                                                      deviation.
                           ^ log ms
                           µ                                ^τ log ms
              30
                                                                                    Such a Monte Carlo sampling procedure gives a probability
                                                40
                                                                                    distribution of power values and allows us to quantify our un-
    Density                           Density
              20                                                                    certainty about the estimated power by taking all sources of
              10                                20                                  uncertainty into account—the uncertainty regarding the ef-
                                                                                    fect, and the uncertainty regarding the standard deviation.
              0                                 0                                      Figure 3 shows the resulting power distributions for power
                   −0.15     −0.05                   0.00    0.04   0.08            given different sample sizes. These power distributions are of
                                                                                    course only estimates, not the true power; and as Button et
                                                                                    al. (2013) point out, are probably slight overestimates if the
                                                                                    studies themselves have publication bias.
                                                                                       The power distributions illustrate two important points.
Figure 2: Posterior distributions of the estimated effect (µ̂),                     First, the range of most likely power values is remarkably
and the standard deviation of estimate of the between-study                         low for typical sample sizes used in psycholinguistic read-
variability (τ̂) in the random-effects meta-analysis.                               ing experiments relating to interference effects (see Table 1).
                                                                                    As an aside, we note that our estimates are similar to those
                                                                                    from a recent review of 44 meta-analyses of research in so-
   We fit the model using Stan 2.14.2 (Stan Development
                                                                                    cial and behavioural sciences published between 1960-2011;
Team, 2016), running four chains with 4000 iterations (half
                                                                                    they report a mean power of 0.24 with most studies suggest-
of which were warm-ups). Convergence was successful, as
                                                                                    ing power to be below 0.4 (Smaldino & McElreath, 2016, p.
diagnosed using the R̂ diagnostic (Gelman et al., 2014). The
                                                                                    6, Fig. 1). The second observation is that the power values
posterior distributions of µ̂ and of the between-study standard
                                                                                    computed from individual studies (the red dots) tend to be
deviation τ̂ are shown in Figure 2. The posterior mean of
                                                                                    overestimates relative to the mean of each power distribution
the effect is -0.05 log ms, with 95% credible interval [-0.08,-
                                                                                    shown. The power from each study tends to be higher than the
0.03]. Next, we use this estimate of the posterior distribution
                                                                                    mean of each power distribution. Of course, if the statistical
to compute a power distribution.
                                                                                    power of the original studies were very high (approximately
Computing the power distribution using the posterior dis-                           80% or higher), then the overestimation problem would dis-
tribution of the effect An analysis of reading studies, in-                         appear or at least be negligible.
cluding the ones considered here, showed that the precisions                           We can quantify the overestimation of power by computing
(the inverse of the variance) in reading time studies have mean                     the Power Inflation Index: the ratio of the power computed
values 16.3 and standard deviation 7.07 (the unit for precision                     from individual studies to the power distribution computed
is 1/log ms2 ). Since precision can be modelled as a Gamma                          using Monte Carlo simulations. If power is overestimated,
distribution, we assumed that precisions are distributed as                         then the distribution of the PII will be such that the mean
Gamma(α = 5.3, β = 0.3). These parameters of the Gamma                              ratio will be greater than 1. These distributions of PIIs are
distribution were computed by taking the mean x̄ and stan-                          computed for a typical sample size used in psycholinguistic
dard deviation s of the precisions, and then deriving the pa-                       studies (n=20, 30, 40) in Table 2. Here, we can see that the
rameters of the Gamma distribution by solving for α and β.                          PII can be as high as 12.
We use the fact that for a random variable generated from a
Gamma distribution with parameters α and β, the expectation                                                 Discussion
µ and variance σ2 are:                                                              We have shown that if statistical significance is used to decide
                                                                                    whether to publish a result, overestimates of the effect will
                              α                                     α
               E(X) =           =µ        and Var(X) =                 = σ2   (7)   tend to be published, leading to an over-enthusiastic belief in
                              β                                     β2              the replicability of the effect.
   Having obtained the estimate of the effect (through the                             Recently, the replication project reported by Open Sci-
meta-analysis) and the distribution of the precisions, we used                      ence Collaboration (2015) showed that only 47% of the stud-
these estimates to carry out 100, 000 Monte Carlo simula-                           ies they investigated could be replicated. One factor causing
tions to derive a power distribution for different sample sizes                     these failures to replicate could have been low power in the
(n = 20, . . . , 50) in the following manner. For each sample                       original studies. Even before the replication project, Cohen
size, we repeatedly computed power after obtaining:                                 (1962, 1988) and others have repeatedly warned against run-
                                                                                    ning low-powered studies. Despite these injunctions, many
• one sample for the effect by sampling from the distribution                       researchers do not believe that there is a problem of low
  Normal(−0.05, 0.01); this is the posterior distribution of                        power. For example, Gilbert, King, Pettigrew, and Wilson
  the effect derived from the random-effects meta-analysis;                         (2016) contested the 47% replication rate and argued that
                                                                                1275

                                                                                              the replication rate may be much higher, perhaps even “sta-
                                                                                              tistically indistinguishable from 100%.” The objections of
                                                                                              Gilbert et al. (2016) were largely based on arguments about
                                                                                              the lack of fidelity to the original design, but it is possible that,
                                                                                              in addition to concerns about fidelity, Gilbert et al. are, like
                           n=20                                  n=30                         many researchers, generally overconfident about the replica-
             6                                     4
                                                                                              bility and robustness of their results. This overconfidence is
             5                                                                                also evident in reading research in psycholinguistics, where it
                                                   3
             4                                                                                is routine to run experiments with sample sizes ranging from
   density   3                           density   2
                                                                                              20 to 40 participants. Recent work has argued that sample
             2
             1
                                                   1                                          sizes of 20-40 partipants may be too low for reading studies
             0                                     0                                          on interference (Jäger et al., 2017). We are hopeful that fu-
                   0.0 0.2 0.4 0.6 0.8 1.0               0.0 0.2 0.4 0.6 0.8 1.0              ture work will take this finding into account when planning
                           n=40
                           power
                                                                 n=50
                                                                 power
                                                                                              studies.
             3.0
                                                                                                 Currently, the replication problems in psycholinguistics are
                                                   2.0                                        serious. For example, in recent work (Mertzen, Jäger, & Va-
             2.0                                                                              sishth, 2017) we carried out six replication attempts of two
   density                               density
                                                   1.0                                        eyetracking experiments published in the Journal of Mem-
             1.0
                                                                                              ory and Language. We were unable to replicate any of the
             0.0                                   0.0                                        claims in the paper. There is thus an urgent need to attempt to
                   0.0 0.2 0.4 0.6 0.8 1.0               0.0 0.2 0.4 0.6 0.8 1.0              replicate published results, and not just in psycholinguistics.
                            power                                 power                       For example, Makel, Plucker, and Hegarty (2012) present a
                                                                                              quantitative analysis of the low rate of successful replications
                                                                                              in psychology (1%). Other fields are also affected. For ex-
Figure 3: Power distributions for different sample sizes (log                                 ample, Button et al. (2013) have shown that in neuroscience
reading times). The histogram shows the power distribution                                    studies, power may also be quite low, ranging from 8 to 31%.
(generated through Monte Carlo sampling; see text for de-                                     Smaldino and McElreath (2016) have shown through a 50-
tails). The red dots show power estimates from the 10 individ-                                year meta-analysis in behavioural science that power has not
ual experimental comparisons considered in this case study.                                   improved (mean power: 24%). In biomedical sciences, ap-
The white dot shows the mean of each power distribution.                                      proximately 50% of studies have power in the 0-10%5 or 11-
                                                                                              20% range (Dumas-Mallet, Button, Boraud, Gonon, & Mu-
                                                                                              nafò, 2017).
                                                                                                 Despite these indications, many researchers remain over-
                                                                                              confident about the robustness of their results. This overcon-
                                                                                              fidence is in part due to the statistical significance filter.
                                                                                                                 Concluding remarks
                        n=20                      n=30                         n=40
                                                                                              We have shown that the statistical significance filter directly
 Study             2.5% 97.5%                2.5% 97.5%                   2.5% 97.5%
                                                                                              leads to over-optimistic expectations of replicability of pub-
     1              1.37     4.64             0.98     3.95                0.76     3.47
                                                                                              lished research. Even if the researcher doesn’t conduct any
     2              3.67   12.45              2.63   10.61                 2.04     9.30
                                                                                              formal power analyses, they can fall prey to this illusion be-
     3              1.08     3.67             0.78     3.13                0.60     2.74
                                                                                              cause of the informal assessment of replicability afforded by
     4              1.95     6.61             1.40     5.64                1.08     4.94
                                                                                              the statistical significance filter. We illustrated the illusion
     5              1.41     4.76             1.01     4.06                0.78     3.56
                                                                                              of replicability through a case-study involving 10 published
     6              3.06   10.37              2.19     8.83                1.70     7.75
                                                                                              experimental comparisons.
     7              0.76     2.59             0.55     2.20                0.42     1.93
                                                                                                 Many psychology journals are beginning to require that
     8              2.99   10.12              2.14     8.62                1.65     7.56
                                                                                              power analyses be included in submitted manuscripts. But
     9              0.98     3.34             0.71     2.84                0.55     2.49
                                                                                              our results, echoing those of others who have studied this
    10              1.02     3.47             0.73     2.96                0.57     2.59
                                                                                              problem, suggest that such analyses, which invariably are
Table 2: The 95% credible intervals of the Power Inflation In-                                based on previously published work, will tend to provide
dex for each of the 10 experimental comparisons, for different                                overestimates of power.
sample sizes. The Power Inflation Index can be as large as 12.                                   To resolve or at least reduce this problem, we offer two
                                                                                              pieces of advice. First, we recommend entirely abandon-
                                                                                                 5 Note that this range is an error; power cannot be less than 5% if
                                                                                              Type I error is set at 5%.
                                                                                           1276

ing the concept of power, which is based on the idea that            Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Ve-
“p < .05” is a win, an attitude that fails miserably when ef-           htari, A., & Rubin, D. B. (2014). Bayesian data analysis
fect sizes are small and measurements are noisy. Second,                (Third ed.). Chapman and Hall/CRC.
when performing design analysis, consider possible effect            Gelman, A., & Tuerlinckx, F. (2000). Type S error rates
sizes based on subject-matter understanding; see Gelman and             for classical and Bayesian single and multiple comparison
Carlin (2014) for further discussion of this point. It can make         procedures. Computational Statistics, 15(3), 373–390.
sense to consider a range of reasonable effect sizes.                Gilbert, D. T., King, G., Pettigrew, S., & Wilson, T. D.
                                                                        (2016). Comment on “estimating the reproducibility of
                          Appendix                                      psychological science”. Science, 351(6277), 1037–1037.
                                                                        doi: 10.1126/science.aad7243
Here, we review the well-known proof that for a point null
                                                                     Hedges, L. V. (1984). Estimation of effect size under non-
hypothesis and a continuous dependent variable, the distribu-
                                                                        random sampling: The effects of censoring studies yielding
tion of the p-value under the null is Uni f orm(0, 1).
                                                                        statistically insignificant mean differences. Journal of Ed-
   When a random variable Z comes from a Uni f orm(0, 1)
                                                                        ucational Statistics, 9(1), 61–85.
distribution, then the probability that Z is less than (or equal
                                                                     Hoenig, J. M., & Heisey, D. M. (2001). The abuse of power:
to) some value z is exactly z: P(Z ≤ z) = z.
                                                                        The pervasive fallacy of power calculations for data analy-
   The p-value is a random variable, call it Z. The p-value is          sis. The American Statistician, 55(1), 19–24.
computed by calculating the probability of seeing a t-statistic      Hung, H. J., O’Neill, R. T., Bauer, P., & Kohne, K. (1997).
or something more extreme under the null hypothesis. The                The behavior of the p-value when the alternative hypothesis
t-statistic comes from a random variable T that is a√transfor-          is true. Biometrics, 11–22.
mation of the random variable X̄: T = (X̄ − µ)/(σ/ n). This          Ioannidis, J. P. (2008). Why most discovered true associa-
random variable T has a CDF F.                                          tions are inflated. Epidemiology, 19(5), 640–648.
   We can establish that if a random variable Z = F(T ), then        Jäger, L. A., Engelmann, F., & Vasishth, S. (2017).
Z ∼ Uni f orm(0, 1), i.e., that the p-value’s distribution under        Similarity-based interference in sentence comprehension:
the null hypothesis is Uni f orm(0, 1). This is proved next.            Literature review and Bayesian meta-analysis. Jour-
   Let Z = F(T ). Then: P(Z ≤ z) = P(F(T ) ≤ z) =                       nal of Memory and Language, 94, 316-339.                 doi:
P(F −1 F(T ) ≤ F −1 (z)) = P(T ≤ F −1 (z)) = F(F −1 (z)) = z.           10.1016/j.jml.2017.01.004
   Since P(Z ≤ z) = z, Z is uniformly distributed, that is,          Lane, D. M., & Dunlap, W. P. (1978). Estimating effect size:
Uni f orm(0, 1).                                                        Bias resulting from the significance criterion in editorial
                                                                        decisions. British Journal of Mathematical and Statistical
                    Acknowledgements                                    Psychology, 31(2), 107–112.
We thank Lena Jäger, Reinhold Kliegl, Christian Robert, and         Makel, M. C., Plucker, J. A., & Hegarty, B. (2012). Repli-
Bruno Nicenboim for helpful discussions. For partial sup-               cations in psychology research: How often do they really
port of this research, we thank the Volkswagen Foundation               occur? Perspectives on Psychological Science, 7(6), 537–
through grant 89 953, and the U.S. Office of Naval Research             542.
through grant N00014-15-1-2541.                                      Mertzen, D., Jäger, L. A., & Vasishth, S. (2017). The
                                                                        importance of replication in psycholinguistics. In Pro-
                         References                                     ceedings of the 30th Annual CUNY Conference on Sen-
                                                                        tence Processing.        Boston, USA.       Retrieved from
Button, K. S., Ioannidis, J. P., Mokrysz, C., Nosek, B. A.,             https://osf.io/j66z5/
   Flint, J., Robinson, E. S., & Munafò, M. R. (2013). Power        Open Science Collaboration. (2015). Estimating the repro-
   failure: why small sample size undermines the reliability of         ducibility of psychological science. Science, 349(6251),
   neuroscience. Nature Reviews Neuroscience, 14(5), 365–               aac4716.
   376.                                                              R Core Team. (2014). R: A language and environment for
Cohen, J. (1962). The statistical power of abnormal-social              statistical computing [Computer software manual]. Vienna,
   psychological research: a review. The Journal of Abnormal            Austria. Retrieved from http://www.R-project.org
   and Social Psychology, 65(3), 145.                                Smaldino, P. E., & McElreath, R. (2016). The natural se-
Cohen, J. (1988). Statistical power analysis for the behav-             lection of bad science. Royal Society Open Science, 3(9),
   ioral sciences (2nd ed.). Hillsdale, NJ: Lawrence Erlbaum.           160384.
Dumas-Mallet, E., Button, K. S., Boraud, T., Gonon, F., &            Stan Development Team.             (2016).      Stan modeling
   Munafò, M. R. (2017). Low statistical power in biomedical           language users guide and reference manual, version
   science: A review of three human research domains. Royal             2.12 [Computer software manual].            Retrieved from
   Society Open Science, 160254. doi: 10.1098/rsos.160254               http://mc-stan.org/
Gelman, A., & Carlin, J. (2014). Beyond power calculations
   assessing Type S (sign) and Type M (magnitude) errors.
   Perspectives on Psychological Science, 9(6), 641–651.
                                                                 1277

