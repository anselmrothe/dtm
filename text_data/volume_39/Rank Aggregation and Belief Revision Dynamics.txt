                             Rank Aggregation and Belief Revision Dynamics
 Igor Volzhanin (ivolzh01@mail.bbk.ac.uk), Ulrike Hahn (u.hahn@bbk.ac.uk), Dell Zhang (dell.z@ieee.org)
                                                      Birkbeck, University of London
                                                          London, WC1E 7HX UK
                                              Stephan Hartmann (s.hartmann@lmu.de)
                                        Munich Center for Mathematical Philosophy, LMU München
                                          Geschwister-Scholl-Platz 1, D-80539 München, Germany
                             Abstract                                   judge (e.g., search engine). One, and only one, of the pos-
                                                                        sible ranking orders (permutations) r∗ is deemed to be true
   In this paper we compare several popular rank aggregation            (correct).
   methods by accuracy of finding the true (correct) ranked list.
   Our research reveals that under most common circumstances               Each judge is characterised by his “competence” which is
   simple methods such as the average or majority actually tend         defined as the probability of providing the true list.
   to outperform computationally-intensive distance-based meth-
   ods. We then conduct a study to compare how actual people               Our simulation takes the various generated lists and aggre-
   aggregate ranks in a group setting. Our finding is that individ-     gated them into a single list using one of the rules outlined
   uals tend to adopt the group mean in a third of all revisions,       further down in this section.
   making it the most popular strategy for belief revision.
                                                                           Unlike in some previous work, such as Fernández et al.,
   Keywords: rank aggregation; distance measure; probabilistic          for each item in a list we know only its rank position, vis-a-
   model
                                                                        vis other items, and not any other numeric properties. It is
                                                                        often impossible or unrealistic to obtain the scores of individ-
                         Introduction                                   ual items and only their relative positioning to each other is
The problem of rank aggregation, where ranked lists from a              available (Dwork et al., 2001; Renda & Straccia, 2003). More
diverse set of “judges” are combined into a single “consen-             importantly, a wealth of psychological research suggests that,
sus” ranked list, is an active research area in computer sci-           in many domains, humans represent faithfully only ranking
ence. Particularly, rank aggregation has found successful ap-           order information and more detailed information is unhelpful
plications in meta-search (Dwork, Kumar, Naor, & Sivaku-                (Stewart, Chater, & Brown, 2006)
mar, 2001; Renda & Straccia, 2003; Fernández, Vallet, &                   For the sake of simplicity our modeling considers that each
Castells, 2006), crowd-sourcing (Niu et al., 2015), and rec-            judge will produce a complete list and no ties are possible.
ommender systems (Baltrunas, Makcinskas, & Ricci, 2010).                So when ranking items, they will rank all of the choices and
   Although extensive studies have already been conducted               will rank them relative to each other in such a way that each
on this topic by computer scientists, these largely concern             item will occupy a unique position. Furthermore, every judge
only the algorithmic issues, i.e., how to produce the “optimal”         in our model has the same level of competence c ∈ [0, 1].
ranked list, without questioning the very concept of “opti-             Finally, when a certain rule produces multiple lists that are
mal”. Typically, a distance measure is chosen, and the ranked           equally optimal, one of them is selected at random. This work
list with the minimum total distance to all the given ranked            could be generalized straightforwardly in the future by relax-
lists is presumed to be the best one (Dwork et al., 2001). In           ing these constraints.
this paper, we challenge such a view and address the prob-                 Following rank aggregation methods have been proposed
lem from the perspective of cognitive science. Just as impor-           in previous studies and are widely used in practice, so we
tantly, much of the previous research has been theoretical in           will use them in our comparison:
nature and no empirical work has been conducted to deter-
mine how humans actually aggregate ranks. To that end, we               • majority: the consensus list is just the ranked list that ap-
went beyond the theoretical models described in section 1 and              pears most frequently.
conducted a group study to better understand real-world rank
belief revision. To the best of our knowledge, there has been           • average: the consensus list is generated by ranking the
no similar work to date.                                                   items according to their average rank positions, which is
                                                                           essentially same as the Borda’s count (Dwork et al., 2001).
                           Modeling
                                                                        • Spearman: the consensus list is the one with the mini-
In the first instance, we developed a theoretical simulation to            mum sum of Spearman’s footrule to all the given ranked
test the accuracy of various rank aggregation methods. The                 list. Spearman’s footrule is defined as the total number of
simulation can be thought of in terms of the most preferred                displacements needed to achieve parity between two lists.
order in which to display results of a web search.
   Given a set of m items (e.g., web pages), we consider n              • Kendall: the consensus list is the one with the minimum
ranked list of them, {r1 , . . . , rn }, each of which is given by a       sum of Kendall’s tau to all the given ranked list. Kendall’s
                                                                    3454

  tau is defined as the total number of inversions required to                                        Average Accuracy of Different Aggregation Methods
  achieve parity between two lists.
• Kemeny-Snell: the consensus list is the one with the
  minimum sum of Kemeny-Snell distance to all the given
                                                                                   0.75
  ranked list. The Kemeny-Snell (KS) distance is similar to
  Kendall’s tau, but more robust when dealing with ties.
                                                                                                                                                                            Method
   While the first two methods are simple and easy to com-                                                                                                                      Average
pute, the other three that are based on distance measures and
                                                                        Accuracy
                                                                                   0.50                                                                                         Majority
have a high computational complexity. It has been shown                                                                                                                         KemDist
                                                                                                                                                                                Spearman
that finding the optimally ranked list based on Kendall’s tau                                                                                                                   Kendall
(known as the Kemeney optimal aggregation) is an NP hard
problem with just four full lists(Dwork et al., 2001).                             0.25
   Our research question is then: “which rank aggregation
method is most accurate?” Here by accuracy, we mean
how often the consensus list returned by a rank aggregation
method is indeed the true list.                                                    0.00
                                                                                                          List Size: 4 Competence: 0.1 Error Method 3
                                                                                                 25                                   50                75            100
                Computer Simulations                                                                                                  Judges
We prepared a simulation in R, which samples a number of
judges and uses different aggregation methods to determine          Figure 1: The comparison of aggregation methods in the
the list reflective of the group of judges. The generated con-      linear-decay error model.
sensus lists are then compared with the true list to calculate
accuracy, which we used as our “performance” measure for
                                                                    Error Model One important consideration in the study was
the aggregation method. This procedure is repeated across
                                                                    the underlying error model that governed a judge’s probabil-
pools of judges of different sizes. In order to smooth out
                                                                    ity of picking the wrong list among all possible permutations.
effects of randomness, we performed bootstrapping at each
                                                                    Each judge had a competence measure which reflected the
number of judges and took the average value. Therefore each
                                                                    probability of picking the true list. The rest of the probability
set of judges was simulated several times, before adding ad-
                                                                    was distributed among the remaining possible choices. As-
ditional judges.
                                                                    suming that judges know anything about the domain in ques-
   Our simulation had a number of parameters that could be
                                                                    tion, the probability of picking a wrong list is likely to be an
altered:
                                                                    inverse function of the distance from that list to the true list.
• list size: number of unique items in a list                       Without loss of generality, we used the Kemeny-Snell dis-
                                                                    tance measure d(·, ·) to determine the probability of a given
• competence level: individual probability of picking the           list being selected as follows.
  correct list
                                                                                                    (
                                                                                                     c                                                       if ri = r∗ ,
• aggregation method: methods of aggregation described                                    Pr[ri ] =                                   1/d(ri ,r∗ )                                     (1)
  above                                                                                              (1 − c)                                                 otherwise.
                                                                                                                                 ∑ j6=∗ (1/d(r j ,r∗ ))
• number of runs: each run increased the number of judges           In effect, lists that are closer to the true list, would be more
  in a group by one                                                 likely to be drawn than the lists further away.
• number of simulations: a number of repeats of the same               We wanted to see relative performance of the various ag-
  simulation with the same conditions to smooth out any             gregation methods, as the number of judges increased. For all
  noise due to randomness                                           results, we maintained a constant competence level c = 0.1,
                                                                    which meant a 10% chance for a judge to pick the true list r∗ .
   We began with a list size of 4. With no ties there are 24        We selected the simulation range from 5 to 100 judges.
possible permutations. In the simulation k groups consisting
of n number of judges would draw a single list from the full        Results
list of permutations. Using one of the aggregation methods, a       After running several different simulations we produced a
single list would be selected for each group as the aggregate       number of interesting and insightful results. We present
product, and then compared to the true list. Each group of          our findings in a series of figures that illustrate the relative
judges would be re-sampled a number of times to boostrap            performance of the different aggregation methods (see fig-
the results to get a smoother result. Thus, scores reported         ures 1, 2, 3).
below are the average results sampled over multiple trials for         The majority rule performs significantly worse than the al-
the same group.                                                     ternatives and does not increase in accuracy as the number of
                                                                 3455

                       Average Accuracy of Different Aggregation Methods                                                        Average Accuracy of Different Aggregation Methods
           1.00
                                                                                                                     0.8
           0.75
                                                                                                                     0.6
                                                                                    Method                                                                                                   Method
                                                                                        Average                                                                                                  Average
Accuracy                                                                                                  Accuracy
                                                                                        Majority                                                                                                 Majority
           0.50                                                                                                      0.4
                                                                                        KemDist                                                                                                  KemDist
                                                                                        Spearman                                                                                                 Spearman
                                                                                        Kendall                                                                                                  Kendall
           0.25                                                                                                      0.2
           0.00                                                                                                      0.0
                           List Size: 4 Competence: 0.1 Error Method 4                                                              List Size: 4 Competence: 0.1 Error Method 1
                  25                                   50                75   100                                          25                                   50                75   100
                                                       Judges                                                                                                   Judges
Figure 2: The comparison of aggregation methods in the                                                Figure 3: The comparison of aggregation methods in the
fastest-decay error model.                                                                            none-decay error model.
judges increases. On the other hand, the other four methods                                           closest to the true list had a non-zero selection probability
perform similarly to each other and their accuracy increases                                          (with each list at that distance equally likely to be picked).
as the number of judges goes up, as can be observed in fig-                                               From the results of the simulation we see that the major-
ure 1. It is important to note that the Kemeny-Snell aggre-                                           ity method plummets almost immediately towards zero accu-
gation method does not perform significantly better than the                                          racy. This is due to the fact that the competence level, i.e.,
other distance-based methods, despite the fact that the un-                                           the probability of picking the true list (10%), is significantly
derlying error model is based on the Kemeny-Snell distance!                                           lower than the probability of picking any of those closest lists
Furthermore, average, which is a very simple method (both                                             (which is 30% in this example as there are three closest lists
computationally and cognitively), performs at least on par                                            in total).
with the distance-based methods.                                                                          Interestingly, the average method appears to outperform
   A minor comment regarding high competence is due at                                                the other methods, and quickly moves towards perfect accu-
this point. When the competence level is above a thresh-                                              racy as the number of judges increases. There appears to be
old, e.g., 0.2, we see a quick rise towards perfect accuracy of                                       little difference among the other distance-based methods and
all methods, which is not particularly interesting, or informa-                                       they all behave similarly to the average method.
tive. Therefore, we kept the competence level low and tried                                               Second, we consider the case where a judge is equally
to understand how robust different rank aggregation methods                                           likely to pick any of the wrong lists, regardless of their dis-
would be under the more challenging condition of lower in-                                            tance to the true list.
dividual competence.                                                                                      The results of this simulation stand in stark contrast to the
   The above linear-decay error model as described in Eq. (1)                                         other two simulations. The majority method performs signif-
is just one way of converting the underlying KS-distance to                                           icantly better and improves with the number of judges, which
the targeted true list into a probability of erroneous list se-                                       is exactly reverse of what was observed in the earlier simula-
lection. Actually any monotonic decaying transformations –                                            tions.
such as an exponential decay – of those distances could be                                                Although the observation was initially quite surprising, the
utilised to pick the non-true lists. To generalise our results we                                     explanation is fairly intuitive. Since the probability of pick-
considered two extreme cases of monotonic decay functions                                             ing the true list is 10%, the remaining probability would be
of distance: at the one end (fastest-decay), the selection prob-                                      distributed evenly over 23 other possible permutations, which
ability drops so rapidly as a function of distance that only the                                      leads to only 3.9% per permutation. Therefore, the ranked list
closest lists stand a chance of being selected; at the other end                                      occurred most frequently is almost guaranteed to be the true
(none-decay), the selection function is flat and the lists of all                                     list, and the majority method would always perform the best.
distances are equally likely to be selected. We have examined                                             Just as importantly, the other aggregation methods appear
both of these extreme cases.                                                                          to falter at this stage. Although there is some improve-
   First, let us consider the case where only the ranked lists                                        ment along with the increase in the number of judges, the
                                                                                                   3456

accuracy stays well below 0.5, even for groups with 100
judges. Notably, the average method performed the worst,
while the Spearman method performed the best among the
three distance-based methods.
Discussion
A few key insights emerge from our modeling efforts. The
first and most important one is that there appears to be lit-
tle benefit of using computationally-expensive distance-based
methods to conduct rank aggregation. Secondly, there is clear
robustness of adopting the group mean. Accuracy is con-
                                                                                   Figure 4: NetLogo participant interface
stantly increases in most scenarios and the method itself is
simple enough to calculate and act upon.
   The one research question that remains open, however,
is what real human subjects would do given a similar task.
While it may appear that taking the group mean is advanta-
geous from the accuracy point of view, it is also more diffi-
cult to determine than simply adopting the majority opinion
for example. To test, this we designed a study that looked at
individual rank revision in a group setting.
               Experiment - Rank Revision
This experiment was set up to test what rules, if any, indi-
viduals use to revise their beliefs in light of new informa-                       Figure 5: NetLogo participant interface
tion. Unlike similar studies on the topic which have mostly
looked at absolute answers and estimates, we were interested
in applying this in the context of rank revision. In other
words, our interest was to understand better how participants
revise ranked orders when presented with information from
their peers. From the modeling exercises above we knew that
adopting the group mean is the most beneficial strategy a per-
son can take in most situations, however, we could not locate                       Figure 6: Zoomed in participant view
any research that corroborated this in an empirical study.
Method
                                                                      the name of the city (see example in figure 4). In the drop
Participants Participants for this study were volunteers              down box ‘City A’ they were instructed to put the number
from the University of London community. Participants were            of the city they believed to be the largest, ‘City B’ were to
paid 5 for taking part in the study. There were 19 participants       contain the second largest, and so on. After all four boxed
who took part, which created three panels of five participants        were filled, participants had to submit their answers and wait
and one panel with four participants (n=19). Each group of            for everyone else in the room to finish. Once, all answers
participants took part at the same time and were hosted in the        were submitted, participants could see how everyone else had
same room. No particular exclusion criteria were used and             ranked the cities. At this point, everyone had an opportunity
participants were free to self select which of the time slots         to revise their answers in light of additional information (see
worked best for them to attend the study. It did not appear           figure 5 and zoomed in view in figure 6). They repeated this
that any participants knew each other prior to the study.             process three times for each question, resulting in four rounds
Materials & Procedure Participants were seated in a com-              - initial round, plus three revision rounds.
puter lab, spaced apart in a way that prevented them from                In total, each participant answered 21 questions. There is
seeing each others’ screens. Each participant had a computer          an initial practice question which participants did in a directed
in front of them that contained a NetLogo interface that was          manner, followed by 20 other questions, which were done
connected in a network to other computers in the room. See            independently and free from any additional instructions. Each
Figure 4 for a sample interface that each participant saw.            question contained a different set of cities and in different
   Initially, participants were read basic instructions regarding     order, but the task was the same. There was only a single
the task. The task involved each participant to rank four cities      experimental condition and all participants were treated the
from the largest to smallest by population size. Each city was        same; they were shown the same set of questions, in the same
presented in a text box and contained a number along with             order.
                                                                  3457

                                                                                      models had very interesting properties and were most likely
                                                                                      to be available and calculable to participants. Since ranked
                      25
                                                                                      lists were presented near each other identifying the majority
                                                                                      list, or calculating the mean list was a conceivable task that a
                      20
                                                                                      participant could engage in prior to revising their beliefs.
                                                                                         In order to test whether participants actually behaved in
Number of Revisions
                      15
                                                                                      a way predicted by a model, we generated an answer that a
                                                                                      participant would pick if they were guided by a model and
                      10
                                                                                      then compared the predicted answer with the actual answer in
                                                                                      a binary fashion. We used two models: mean - using simple
                      5
                                                                                      Borda count - and majority lists.
                                                                                         Table 2 demonstrates that there were significantly more re-
                      0
                           0               5                      10          15      visions that moved towards the mean than majority. In fact, of
                                                    Participant
                                                                                      the 196 total revisions, 62, or 31% were revisions that adopted
                                                                                      the group mean, and 44 or 22% that adopted the majority list.
                               Figure 7: Number of revisions per Participant          On average, the mean model was adopted 3.26 times per par-
                                                                                      ticipant, while majority model was adopted 2.32 times. The
                                                                                      rest of the revisions were not accounted by these two models
Results                                                                               and were being guided by unknown rules.
In the first instance we were interested in individual belief                            Naturally, there were instances where both models pre-
revision. We analyzed how often individuals changed their                             dicted the same list and the above numbers include revisions
answers and what rules they have have used to do so.                                  where the mean and majority lists coincide. There were 35
Individual Revision Discounting the first question, there                             revisions where both models predicted the same result.
were 60 opportunities for revision for each participant (20                              When removed from the total revision count for each
questions * 3 revision rounds). On average participants                               model, there were 27 revisions that adopted the group mean
changed their answers 10.3 (SD 7.51) times over the course                            and only 9 revisions that adopted the majority list. This pro-
of the simulation, or about 10% of the time. With some par-                           vides strong evidence to suggest that participants in our study
ticipants changed their answers as little as once, and others                         adopted the group mean much more readily than the majority
changed almost a third of their answers. In total there were                          list.
196 revision for all participants. See figure 7 for a visual rep-
resentation of the number of revisions per participant.                                                  Table 2: Model Revision
   Overall, most revisions occurred in the first round, where
almost as many revisions occurred as the subsequent two                                   Model      Total   Model Only     Average     Revision %
rounds. Table 1 breaks down revisions by round.                                           Mean       62      27             3.263       31
   Revisions occurred unevenly between questions. Seven                                   Majority   44      9              2.316       22
questions had between 13 and 15 revisions, while remaining
13 questions had between five and nine revisions.
   The number of revisions made by participants was rather                            Toward a Model of Rank Belief Revision Our findings
low, but the overall profile of the changes, i.e. mostly in the                       suggest that human participants are 3 times more likely to
first round and more for some questions than others, is con-                          adopt the group mean over the majority list in cases where the
sistent with some of the other studies in the field of decision-                      two do not coincide. This suggests that computational models
making.                                                                               that emphasize mean ranks may be closer to the way humans
                                                                                      make revisions given additional information in a ranked for-
                                                                                      mat.
                                        Table 1: Round Revisions
                                                                                         We did not test other, more complex models on the study
                                      Revision    Revision             Revision       dataset. Therefore, it is difficult to say at this point whether
                                      Round 1     Round 2              Round 3        adopting the group mean is the most preferred strategy. It
                                      96          58                   42             should also be noted that revisions represented only 11% of
                                                                                      all choices made by participants and most of the time partici-
                                                                                      pants did not change their answers and were not influenced by
Models of Revision We fitted several models presented in                              additional information. However, where revisions did occur,
the first part of the paper trying to predict individual belief                       in a third of all cases it was towards the group mean, which is
revision rules that induced the change (such as mean, me-                             a significant finding. Future models that seek to replicate hu-
dian and majority models). We decided to restrict our fitting                         man behaviour should take these findings into account when
to two models in particular: mean and majority. As these                              constructing more human-like models.
                                                                                   3458

                        Conclusions
Our research outlined a basic error model as well as two limit
cases. In all three scenarios, distance-based methods did not
produce significantly better results, suggesting that the prob-
lem of rank aggregation could be satisfactorily solved by sim-
pler methods such as taking the average or majority.
   As the performances of the two simple methods are diamet-
rically opposite, which method should be used depends on the
underlying error distribution in a population. Conversely, if
one is able to measure accuracy, the performances of various
rank aggregation methods can actually inform us the under-
lying error distribution and allow us to make inferences about
the cognitive process of ranking.
   In order to expand on our findings, we conducted a lab ex-
periment where we tested actual belief revision in a group set-
ting. Our findings suggest that when revising their answers,
participants most often adopted the group mean, suggesting
that human cognition gravitates towards this method of re-
vision. This is significant, in light of the fact that adopting
group mean is both computationally less strenuous and quite
advantageous in most situations. This suggests that human
cognition is adaptive in this sense, using a strategy that our
modeling shows to be robust in most cases.
                         References
Baltrunas, L., Makcinskas, T., & Ricci, F. (2010). Group
   recommendations with rank aggregation and collaborative
   filtering. In Proceedings of the 2010 ACM conference on
   recommender systems (recsys) (pp. 119–126). Barcelona,
   Spain.
Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001).
   Rank aggregation methods for the web. In Proceedings
   of the 10th international conference on world wide web
   (www) (pp. 613–622). Hong Kong, China.
Fernández, M., Vallet, D., & Castells, P. (2006). Probabilistic
   score normalization for rank aggregation. In Proceedings
   of the 28th european conference on IR research (ecir) (pp.
   553–556). London, UK.
Niu, S., Lan, Y., Guo, J., Cheng, X., Yu, L., & Long, G.
   (2015). Listwise approach for rank aggregation in crowd-
   sourcing. In Proceedings of the 8th ACM international con-
   ference on web search and data mining (wsdm) (pp. 253–
   262). Shanghai, China.
Renda, M. E., & Straccia, U. (2003). Web metasearch: Rank
   vs. score based rank aggregation methods. In Proceedings
   of the 2003 ACM symposium on applied computing (sac)
   (pp. 841–846). Melbourne, FL, USA.
Stewart, N., Chater, N., & Brown, G. D. (2006). Decision by
   sampling. Cognitive Psychology, 53(1), 1–26.
                                                                 3459

