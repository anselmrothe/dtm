   Information density of encodings: The role of syntactic variation in comprehension
           Les Sikos (sikos@coli.uni-saarland.de), Clayton Greenberg (claytong@coli.uni-saarland.de),
    Heiner Drenhaus (drenhaus@coli.uni-saarland.de) and Matthew W. Crocker (crocker@coli.uni-sb.de)
               Department of Language Science and Technology, Saarland University, 66123 Saarbrücken, Germany
                             Abstract                                   such as that-deletion (Jaeger, 2010; Levy & Jaeger, 2007),
   The Uniform Information Density (UID) hypothesis links
                                                                        contraction (Frank & Jaeger, 2008), and the use of single
   production strategies with comprehension processes,                  word equivalents that vary in word length (chimpanzee vs.
   predicting that speakers will utilize flexibility in encoding in     chimp; Mahowald, Fedorenko, Piantadosi & Gibson, 2013).
   order to increase uniformity in the rate of information              Although the above studies provide important and
   transmission, as measured by surprisal (Jaeger, 2010).               compelling support for the notion that UID modulates
   Evidence in support of UID comes primarily from studies              aspects of syntactic encoding, the generality of the findings
   focusing on word-level effects, e.g. demonstrating that              is limited by the observation that all the phenomena are
   surprisal predicts the omission/inclusion of optional words.
   Here we investigate whether comprehenders are sensitive to           instances of highly local syntactic reduction.
   the information density of alternative encodings that are more         The goal of the current study is to investigate whether
   syntactically complex. We manipulated the syntactic                  comprehenders are sensitive to the information density of
   encoding of complex noun phrases in German via meaning-              more complex alternative syntactic encodings. Consider the
   preserving pre-nominal and post-nominal modification in              following examples:
   contexts that were either predictive or non-predictive. We
   then used the G-maze reading task to measure online
                                                                          (1)    The journalist published…            Predictive context
   comprehension during self-paced reading. Results were
   consistent with the UID hypothesis. In predictive contexts,            (2)    The man evaluated…              Non-predictive context
   post-nominal encodings elicited a more uniform distribution
                                                                             a. …[the carefully written essay].                 Obj NPadj
   of processing effort. Conversely, in non-predictive contexts,
   more uniform effort was found for pre-nominal encodings.                  b. …[the essay that was carefully written].        Obj NPrel
   Keywords: Language comprehension; surprisal; uniform                 Each object noun phrase (NP) above arguably expresses the
   information density hypothesis; G-maze; self-paced reading.
                                                                        same message,1 however (a) uses a pre-nominal adjective
                                                                        phrase while (b) uses a post-nominal relative clause. While
                         Introduction                                   the head noun (essay) is more expected in the predictive (1)
Levy and Jaeger’s (2007) Uniform Information Density                    than non-predictive (2) contexts, the expectation for the
hypothesis postulates that speakers adjust their lexical and            adjective carefully presumably does not differ across
syntactic realization of a message for the benefit of                   contexts. One potential encoding strategy for increasing the
comprehenders. Specifically, they suggest that there is an              uniformity of information density would be to produce low-
overarching preference to produce message encodings that                surprisal words early in the sentence, as this may facilitate
distribute information as evenly as possible across the                 the processing of subsequent less predictable words. For
linguistic signal. This account fundamentally links encoding            instance, in the non-predictive context, the UID hypothesis
and decoding processes, asserting that language producers               predicts a processing advantage for the pre-nominal
will exploit the flexibility in encoding so as to increase              encoding because carefully written should reduce the
uniformity in the rate of information transmission, as                  surprisal of essay. In the predictive context, on the other
measured by surprisal (Hale, 2001; Levy, 2008). As such,                hand, the pre-nominal encoding may result in a trough in
the UID hypothesis can be viewed as part of a rational                  information density at essay, as it is highly expected (and
theory of communication — from an information theoretic                 thus not very informative) following the verb and modifiers.
perspective — in which encoding strategies take into                    In this case, the post-nominal relative clause may distribute
account resource limitations of the comprehender.                       the informational content more uniformly. The UID
   There is robust empirical evidence that surprisal accounts           hypothesis therefore, predicts a greater benefit for the
for cognitive load during comprehension — at least at the               relative clause encoding in more predictive contexts.
level of individual words in a sentence (Drieghe, Rayner &                We tested the above predictions using a self-paced
Pollatsek, 2005; Kliegl, Grabner, Rolfs & Engbert, 2004;                reading design to measure online differences in cognitive
Rayner, Aschby, Pollatsek & Reichle, 2004; Rayner &                     load during the critical object NP.
Well, 1996). However, there exists little direct online
evidence that comprehenders are indeed sensitive to the
surprisal and density profiles of alternative syntactic
encodings — a critical assumption underlying the UID
hypothesis. Furthermore, current support for UID in
                                                                           1
production is limited to relatively local encoding choices,                  Choices in linguistic encoding are known to be influenced by
                                                                        aspects of information structure, including contrastive focus,
                                                                    3168

                         Experiment                                          combinations were neutral with respect to the object noun in
                                                                             the non-predictive context (e.g., Der Mann bewertete, “The
Our primary goal was to test whether comprehenders are
                                                                             man evaluated”), but were semantically associated with the
sensitive during online processing to differences in the
                                                                             object in the predictive contexts (e.g., Der Journalist
information density of alternative syntactically-complex
                                                                             veröffentlichte, “The journalist published”). Importantly,
encodings that nevertheless convey a similar message. The
                                                                             however, highly expected object nouns (e.g., Artikel,
materials crossed two factors (context × encoding), as
                                                                             “article”) were avoided in order to increase the possibility of
illustrated in Table 1. Because this manipulation distributes
                                                                             detecting surprisal differences between predictive/pre-
information within the critical region differently across
                                                                             nominal and predictive/post-nominal conditions.
conditions, we assessed cognitive load using a variation of
                                                                                The information density of object NPs was manipulated
self-paced reading that is less susceptible to spill-over
                                                                             via pre- and post-nominal modification, affecting both the
effects than standard forms of self-paced reading. The
                                                                             linear ordering and length (in words) of the message. Pre-
grammaticality maze task (G-maze; Forster, Guerrera &
                                                                             nominal modifiers (e.g., sorgfältig verfassten, “carefully
Elliot, 2009) can precisely identify the word at which
                                                                             written”) were shorter, containing 2 to 4 words, but
processing time differences emerge during online
                                                                             positioned the head noun at the end of the NP. Post-nominal
comprehension (Witzel, Witzel & Forster, 2010) and is
                                                                             modifiers used a relative clause construction (e.g., der
therefore well-suited for our purposes. In this task sentences
                                                                             sorgfältig verfasst worden war, “that was carefully written”)
are presented word by word as a sequence of forced choices
                                                                             and were therefore longer, ranging from 4 to 6 words, and
between two alternatives, only one of which continues the
                                                                             constrained the head noun to the beginning of the NP. To
sentence grammatically. If the participant successfully
                                                                             avoid having any words within the critical object NP region
navigates the “maze” by choosing the correct word from
                                                                             be sentence-final, all items ended with an adverbial phrase.
each pair, the selected words form a coherent sentence
(Figure 1).
Methods                                                                                              The$$$$$$x&x&x$
Participants Twenty-seven native German speakers (age M
                                                                                                       went$$$$$$man$$$
= 24, SD = 2.6) with normal or corrected to normal vision
were recruited from the Saarland University community and
                                                                                                        evaluated$$$$$sink$
were compensated 8€ for their participation. Participants
that did not successfully navigate at least 70% of mazes in                                                    the$$$$$$$hosed$
all experimental conditions were excluded (n = 3).
                                                                                                                while$$$$carefully$
Materials Forty-eight sets of sentences were constructed in
German by crossing context (predictive, non-predictive) and
                                                                                                                     wri8en$$$$$river$
syntactic encoding of the object NP (pre-nominal
modification, post-nominal modification), resulting in four                                                           some9me.$$$$$essay.$
conditions per item (Table 1). In order to create the context
manipulation, the same object noun (e.g., Essay, “essay”)
                                                                                      Figure 1: Example trial structure of G-maze task.
was used in all conditions, but the object was designed to be
                                                                               Sentences (in German) were presented word by word as a
more expected in predictive contexts than non-predictive
                                                                               sequence of forced choices between two alternatives, only
contexts. This was accomplished by choosing different
                                                                                   one of which continued the sentence grammatically.
subject–verb combinations for each context. Subject–verb
    Table 1: Example stimulus item in four conditions with approximate English translations. The critical region of interest
        was the object NP. RTs were analyzed separately for the object noun (bold) and modification region (underlined).
Context        Encoding     Example
Predictive     Post-nominal Der Journalist veröffentlichte den Essay, der sorgfältig verfasst worden war, unter Einbeziehung des größeren Kontextes.
                            “The journalist published the essay that was carefully written, taking into account the larger context.”
Predictive     Pre-nominal  Der Journalist veröffentlichte den sorgfältig verfassten Essay unter Einbeziehung des größeren Kontextes.
                            “The journalist published the carefully written essay, taking into account the larger context.”
Non-predictive Post-nominal Der Mann bewertete den Essay, der sorgfältig verfasst worden war, unter Einbeziehung des größeren Kontextes.
                            “The man evaluated the essay that was carefully written, taking into account the larger context.”
Non-predictive Pre-nominal  Der Mann bewertete den sorgfältig verfassten Essay unter Einbeziehung des größeren Kontextes.
                            “The man evaluated the carefully written essay, taking into account the larger context.”
                                                                        3169

   Four counterbalanced lists were constructed from these             and test sections with the ratio 8:1:1. The resulting training
materials according to a Latin Square design such that each           section contained 666,561,150 tokens. The model was
list contained 12 items in each condition, but no item                trained using the SRILM toolkit (Stolcke, 2002) and
appeared more than once in the same list. An additional 48            achieved perplexities of 25 on the training section, 201 on
sentences with the same structures as above, but containing           the test section, and 1583 on our stimuli.4
highly predictable object nouns, were constructed as fillers
                                                                      Procedure Participants were randomly assigned to a
(e.g., Der Schneider zerschnitt den stark gemusterten Stoff
                                                                      stimulus list (6 per list). The G-maze task was implemented
am Mittwoch., “The tailor cut the heavily patterned fabric on
                                                                      in E-prime (Schneider, Eschman, & Zuccolotto, 2002). Each
Wednesday.”). Half of the filler sentences contained pre-
                                                                      trial began with two crosses (+) that remained on screen for
nominal modification of the object noun and the other half
                                                                      1000 ms, indicating where subsequent word pairs would
contained post-nominal modification. No object nouns were
                                                                      appear. Each word in the sentence (except the first word)
repeated across experimental or filler items.
                                                                      was then presented together with a foil word,5 which was
Cloze probability and contextual constraint An offline                not a grammatical continuation of the sentence. The first
Cloze completion study was conducted to confirm that                  word in every sentence was paired with “x-x-x”. The
object nouns were more expected following predictive than             presentation side (left, right) was randomized such that the
non-predictive contexts, but were not highly expected in              correct word appeared equally often on each side. Any
either context. A separate group of 58 native German                  punctuation (i.e., comma, period) that appeared with a word
speakers (age M = 22.0, SD = 2.9) were presented with                 also appeared with its foil. Participants were instructed to
sentence fragments from the 48 experimental items                     choose as quickly and as accurately as possible the word
described above. Fragments contained only the contexts,               that best continued the sentence. Participants indicated their
followed by a blank (e.g., Der Mann bewertete___; Der                 selection by pressing the left or right button on a button box
Journalist veröffentlichte___). Predictive and non-predictive         and the amount of time required for selecting the
contexts were counter-balanced across two lists. Participants         grammatical continuation was recorded as the response time
were asked to fill in the blank with the first determiner–            (RT). If the correct word was chosen, the next pair of words
noun combination that came to mind. Cloze probabilities               appeared automatically. However, if a foil word was
were computed as the percentage of participants who                   selected, negative feedback (Inkorrekt, “Incorrect”) was
provided the experimental object noun for a particular item.          displayed and the trial was aborted. Once the end of a
As expected, object nouns had low cloze probabilities in              sentence was reached, positive feedback (Korrect,
both contexts but were reliably more expected following               “Correct”) was given. Participants initiated each new trial
predictive (cloze = 0.06, SD = 0.18) than non-predictive              by button press.
contexts (cloze = 0.00, SD = 0.01), t(47) = -2.32, p < .05.              To confirm that participants read the sentences for
   The percentage of the most frequently occurring response           meaning, a Yes/No comprehension question appeared after
to each sentence fragment in the cloze test was also used to          1/3 of the items. Half of the questions asked about the
assess the contextual constraint of predictive and non-               subject noun and half about the object noun. The correct
predictive contexts. As expected, the mean constraint of              answer was Yes for 50% of questions. Participants used the
predictive contexts was reliably greater (51.3%) than that of         button box to respond. No feedback was given.
the non-predictive contexts (21.3%), t(47) = -8.46, p < .001.            In order to familiarize participants with the task, five
                                                                      practice items (three with comprehension questions) were
Surprisal profiles To compare our response time results
                                                                      presented before the experiment began. Participants took
against a more theoretical notion of predictability, we
                                                                      approximately 40 minutes to complete the experiment.
computed surprisals for all experimental stimuli using an
interpolated modified Kneser-Ney 5-gram language model
trained on a 2017-01-01 dump of German Wikipedia. To
obtain the corpus, we filtered the original XML dump using
the tool WikiExtractor, split the corpus into sentences using
the NLTK sentence splitter for German, and preprocessed
the resulting dataset.2 After replacing all types occurring              4
                                                                           The sharp difference in perplexity scores between the test
fewer than 15 times with <unk>, the vocabulary size was               corpus and stimuli suggests that the German Wikipedia corpus is
833,734.3 We split the corpus into training, development,             not an ideal match for our stimuli. We return to this point in the
                                                                      discussion.
                                                                         5
                                                                           Foils were created in a two-stage process. First, a custom
   2
     Lowercased, replaced punctuation with space, replaced digits     Python script randomly selected a foil candidate for each word in
with NUM, removed empty lines, replaced tabs with spaces,             each experimental and filler item. Foil candidates were constrained
removed multiple spaces, removed multiple NUMs, replaced              such that they did not appear in bigrams with the correct word at
umlauts by their conventional character bigrams, and added            the previous position in the sentence within a large German corpus.
sentence begin and end markers.                                       Second, each foil was then hand checked by at least two trained
   3
     A threshold of 15 was selected since this was the highest        native-German linguists to ensure that it was not a grammatical
possible while maintaining a less than 1% out of vocabulary rate      continuation of the sentence. The same foil was used for identical
on a different corpus (EUROPARL).                                     words (or derivationally related words) across conditions.
                                                                  3170

Results and Discussion                                                   Figure 3 (left panel) shows that object nouns were read
Completed mazes Overall performance on the G-maze task                more quickly in predictive than non-predictive conditions, β
was high, with participants successfully navigating 85.6%             = -161.01, SE = 29.59, t(44.22) = -5.44, p < .001. This
(SD = 0.08) of experimental and filler items to completion.           finding replicates previous work demonstrating that
However, because the critical region of interest was the              expected linguistic material is easier to process than
object NP, the RT analyses reported below were conducted              unexpected material.
on all experimental items that were completed through at                 Within the non-predictive conditions, pre-nominal
least the end of the critical region (M = 0.90, SD = 0.06).           modification clearly facilitated the processing of unexpected
                                                                      object nouns. Length-adjusted RTs for object nouns were
Comprehension question accuracy Performance on the                    faster for pre-nominal modification than for post-nominal
comprehension questions was near ceiling (M = 0.97, SD =              modification, β = -124.86, SE = 30.42, t(23.07) = -4.104, p
0.04), confirming that participants were reading the                  < .001. This result is consistent with the UID hypothesis,
sentences for meaning during the G-maze task.                         which predicts a processing advantage for the pre-nominal
Response time RTs were analyzed with linear mixed effects             encoding: pre-modification reduces the surprisal of the
models with participants and items as crossed, independent,           unexpected word and results in a more uniform distribution
random effects. All models included maximal random                    of processing effort across the linguistic signal.
effects structures (Barr, Levy, Scheepers & Tily, 2013).                 Within the predictive conditions, length-adjusted RTs for
Analyses were conducted using the lmer function (lme4                 object nouns were also faster for pre-nominal modification
library, version 1.1-10; Bates & Sarkar, 2007) in the                 than for post-nominal modification, β = -51.00, SE = 18.67,
statistics software package R, version 3.2.2 (R Development           t(29.26) = -2.73, p < .05. However, the facilitation effect
Core Team, 2013). Fixed effects were evaluated via                    was weaker for predictive conditions, resulting in a context
likelihood ratio tests implemented in lmerTest (Kuznetsova,           × encoding interaction, β = 73.67, SE = 33.02, t(53.56) =
Brockhoff & Christensen, 2015), where denominator df was              2.23, p < .05. The UID hypothesis predicts a trough in
estimated using the Satterthwaite method. We report                   information density for words that are both highly expected
estimates, standard errors, t and p values associated with            and pre-modified. Figure 2 (upper panel) is compatible with
likelihood ratio tests for significant results only.                  this prediction. RTs drop steeply in the predictive / pre-
   All raw RTs that were abnormally low (below 200 ms) or             nominal condition at the object noun. Note that this is true
abnormally high (above 5000 ms) were excluded (0.3%),                 despite the fact that object nouns were selected to be low-
and outliers exceeding 3 standard deviations by participant           cloze. Thus, as shown in Figure 3, the post-nominal
were then trimmed (1.8%). The remaining RTs were                      condition distributes the informational content more
adjusted for word length (Ferreira & Clifton, 1986) and               uniformly, resulting in a smoother RT profile.
punctuation using a linear mixed effects regression model                Modification region analysis. Length-adjusted RTs for the
with fixed effects for word length, punctuation (i.e., whether        modification region were analyzed using the same mixed
or not a comma or period was presented with the word), and            effects model as above. Figure 3 (right panel) shows that
their interaction. The residuals of this model, length-               encoding influenced the processing of the modification
adjusted RTs, served as the dependent variable in the                 region in a way that was complementary to its effect on
analyses reported below.6 Because the number of words                 object nouns (see also Table 2). Pre-nominal modification
used to modify object nouns varied across items and                   was read more slowly than post-nominal modification in
conditions, we computed the length-adjusted RT for the                both contexts, β = 106.04, SE = 15.02, t(75.75) = 7.06, p <
modification region by averaging across modifier words.               .001. However the magnitude of this effect was greater in
   The upper panel of Figure 2 presents the mean length-              the non-predictive context, reflected in a context × encoding
adjusted word-by-word RTs for each condition. Differences             interaction, β = -53.44, SE = 18.69, t(51.68) = -2.86, p < .01.
first emerge at the subject noun, where RTs were slower for
predictive than non-predictive contexts. This is not                     Table 2: Mean length-adjusted RT (ms) by condition for
surprising as these words (e.g., journalist) are less frequent              object noun (upper panel) and modification region
than their non-predictive counterparts (e.g., man). More                     (lower panel). Standard deviation in parentheses.
relevant to the research question, all four conditions diverge
within the critical object NP region (Table 2).                         Object Noun
                                                                                            Pre-nominal  Post-nominal      Mean
   Object noun analysis. Length-adjusted RTs for object                        Predictive       -54 (77)       -5 (54)   -30 (50)
nouns were regressed onto a model including fixed-effect                  Non-predictive         27 (70)       56 (98)    92 (46)
factors for context (predictive, non-predictive), encoding                          Mean        -13 (56)       76 (62)
(pre-nominal, post-nominal), and their interaction. In order            Modification Region
to control for task adaptation, a main effect of stimulus                                   Pre-nominal  Post-nominal      Mean
order was also included.                                                       Predictive        54 (46)        2 (39)    28 (31)
                                                                          Non-predictive       113 (53)        10 (30)    61 (34)
   6
     Qualitatively identical results are obtained when raw RTs are                  Mean         84 (30)        6 (22)
used.
                                                                  3171

                                                                                                                                                           Sentence Position
                                                                                                                                                                                                object noun /         object noun /
                                                                                         determiner                                subject noun           verb             determiner         modification region   modification region
                                                                                        Word−by−Word
                                                                                             1                                    Reading
                                                                                                                                        2 Times   (collapsed
                                                                                                                                                           3 over   modification)
                                                                                                                                                                               4                       5                    6
                                                                             200
                                                                                                                                                                                                      ●
                                                                             100
                                             Length-Adjusted
                                                            RTRT
                                                                 (ms)
                                                                                                                                                          ●                                                                               as.factor(shp)
                                                                                                                                                         ●
                                                                                                                                                                                                                           ●               ● 21
                                                                                                                                                                                                                            ●
                                            Length−Adjusted    (ms)
                                                                                    0                                                                                          ●                        ●                                     22
                                                                                                                                       ●                                        ●
                                                                                                                                       ●                                                                                                  as.factor(ltyp)
                                                                         −100                                                                                                                                                                 1
                                                                                                                                                                       ––– Predictive                                                         2
                                                                                                                                                                       ----- Non-predictive
                                                                                                                                                                           Post-nominal
                                                                         −200                                                                                              Pre-nominal
                                                                                            ●●
                                                                                        Word−by−Word Surprisal Values (collapsed over modification
                                                                                    6
                                                                                                                                       2●
                                                                                                                                                         ●●                       4                                          6
                                                                                    5                                                                         Sentence Position                                                           as.factor(shp)
                                                                                                                                      ●                                                             ●                                     ●   21
                                                                                                                                                                                                        ●
                                            Surprisal
                                                                                                                                                                                                                                              22
                                                                        Surprisal
                                                                                    4
                                                                                    3                                                                                                                                                     as.factor(ltyp)
                                                                                                                                                                                                                                              1
                                                                                    2
                                                                                                                                                                              ●●
                                                                                                                                                                                                                                              2
                                                                                                                                                                                                                          ●●
                                                                                    1
                                                                                            ●●
                                                                                                                                          2                                       4                                         6
                      Figure 2: Upper panel: Mean length-adjusted word-by-word
                                                                             SentenceRTs.   Lower panel: Surprisal profiles as determined by
                                                                                      Position
                        a language model trained on the German Wikipedia corpus. RTs and surprisals for the modification region were
                             calculated by averaging across all modifiers words. Error bars represent one standard error of the mean.
                                               ReadingNoun
                                       AdjustedObject  Times for Target word
                                                                    Adjusted Reading Times
                                                                        Modification Region     -0.06). There are at least two plausible explanations for
                                                                                                              (r =
                                                                                           for Modification Region
                                 200                                                   this divergence. First, the predictable contexts may not have
                                                                                                                            200
                               ––– Predictive
                                                                                       made our atypical (i.e., low cloze) object nouns statistically
                  ●
                                                                                       more predictable, given our German Wikipedia corpus. For
     Length-Adjusted  RT (ms)
                                                                                               Adjusted Reading Time (ms)
                               ----- Non-predictive
    Adjusted Reading Time (ms)
                                                                      ●                instance, verpackte (“boxed”, a verb in the predictive
        100                            100
                                                                                       context) and Geschenk (“gift”, the corresponding object
                                             as.factor(c(2, 2, 1, 1))     as.factor(c(2, 2, 1, 1))
                                                 1                     ●      1
                                                                                       noun) were both present in the corpus but never co-occurred
                                ●                2                            2
                                                                                       in the same sentence. We assessed this possibility and found
          0                              0
                                                    ●
                                                     ●                                 that, on average, subject nouns in predictive contexts did not
                   ●
                                                                                       substantially increase the predictability of object nouns
                                 ●
                                                                                       above the general case. Verbs, however, did so by a factor
                                                                                       of 6. Second, the dependencies that existed in the training
       −100                           −100                                             corpus may not have been fully captured by the language
                 post          pre
            Post-nominal Pre-nominal               post
                                            Post-nominal Pre-nominal  pre              model. To test this possibility, we calculated the mean gram
                      pre.post                           pre.post
                                    Linearization                                      size used for surprisal predictions in the object NP region
    Figure 3: Mean length-adjusted RTs for object nouns                                (M = 1.86). This finding indicates that despite being trained
    (left panel) and the modification region (right panel).                            on 5-grams, the model predictions in this region were based
     Error bars represent one standard error of the mean.                              predominantly on more local statistics (i.e., bigrams),
                                                                                       effectively modeling only the non-predictive conditions.7
  Surprisal profiles. The lower panel of Figure 2 shows the                                Despite these caveats, the results broadly confirm our
surprisal profiles produced by the language model.                                     assumptions about the distribution of surprisal across pre-
Surprisals at sentence positions 1-4 patterned closely with                            nominal and post-nominal encodings of the critical object
RTs, reflected in a positive correlation within this region (r
= 0.36). However, the surprisal pattern differed somewhat                                  7
                                                                                              Note, however, that the cloze results validate both the stimuli
from the pattern of RTs during the critical object NP region                           and the RT findings.
                                                                                                                                                               3172

NP: according to the language model, the pre-nominal               Ferreira, F., & Clifton, C. (1986). The independence of
encodings had more uniform information densities. To                 syntactic processing. Journal of Memory and Language,
capture the behavior found for reading times in the                  25(3), 348-368.
predictive conditions, either a closer domain match between        Forster, K. I., Guerrera, C., & Elliot, L. (2009). The maze
training corpus and stimuli would be required, or a language         task: Measuring forced incremental sentence processing
model architecture that is less sensitive to word position.          time. Behavior Research Methods, 41, 163-171.
                                                                   Frank, A. & Jaeger, F. (2008). Speaking rationally: Uniform
                   General Discussion                                information density as an optimal strategy for language
The UID hypothesis links production strategies with                  production. In Proceedings of the Annual Conference of
comprehension processes and predicts that speakers utilize           the Cognitive Science Society, (Vol. 30, No. 30).
flexibility in encoding to distribute information as evenly as     Hale, J. (2001). A probabilistic Earley parser as a
possible across the linguistic signal (Jaeger, 2010; Levy &          psycholinguistic model. In Proceedings of the second
Jaeger, 2007). While prior evidence for UID comes                    meeting of the North American Chapter of the Association
primarily from word-level effects (Frank & Jaeger, 2008;             for Computational Linguistics on Language technologies,
Jaeger, 2010; Levy & Jaeger, 2007; Mahowald et al., 2013),           (pp. 1-8). Association for Computational Linguistics.
a critical assumption underlying the UID hypothesis is that        Jaeger, T. F. (2010). Redundancy and reduction: speakers
comprehenders should also be sensitive to the information            manage syntactic information density. Cognitive
density of alternative syntactically-complex encodings. To           Psychology, 61(1): 23-62.
our knowledge, the current study is the first to investigate       Kliegl, R., Grabner, E., Rolfs, M., & Engbert, R. (2004).
this important and challenging question.                             Length, frequency, and predictability effects of words on
   We manipulated the syntactic encoding of complex noun             eye movements in reading. European Journal of
phrases via meaning-preserving pre-nominal and post-                 Cognitive Psychology, 16: 262-284.
nominal modification in contexts that were either predictive       Kuznetsova, A., Brockhoff, P. B., & Christensen, R. H. B.
or non-predictive. The results were consistent with the UID          (2016). lmerTest: Tests in linear mixed effects models
hypothesis. In predictive contexts, post-nominal encodings           (Version 2.0-30) http://cran.r-project.org/web/package=
elicited a more uniform distribution of processing effort            lmerTest
than pre-nominal encodings. This makes sense because the           Levy,      R.     (2008).     Expectation-based      syntactic
head noun is already expected in such contexts, thus pre-            comprehension. Cognition, 106(3): 1126-1177.
nominal modification could lead to a trough in information         Levy, R. & Jaeger, T. F. (2007). Speakers optimize
density at the noun. Conversely, in non-predictive contexts,         information density through syntactic reduction. In
a more uniform RT profile was found for pre-nominal                  Schöllkopf, B., Platt, J., and Hoffman, T., editors,
encodings, where pre-modification served to reduce the               Advances in Neural Information Processing Systems, pp.
surprisal of the unexpected head noun. This pattern of               849-856, Cambridge, MA. MIT Press.
comprehension results provides indirect support for UID as         Mahowald, K., Fedorenko, E., Piantadosi, S. T., & Gibson,
a rational strategy for producers to adopt. An important             E. (2013). Info/information theory: Speakers choose
question for further investigation is whether speakers are           shorter words in predictive contexts. Cognition, 126(2):
indeed attentive to such factors when making their encoding          313-318.
decisions.                                                         R Core Team (2015). R: A language and environment for
                                                                     statistical computing. R Foundation for Statistical
                                                                     Computing, Vienna, Austria. https://www.R-project.org
                    Acknowledgments
                                                                   Rayner, K., Aschby, J., Pollatsek, A., & Reichle, E. D.
This research was funded by the German Research                      (2004). The effects of frequency and predictability on eye
Foundation (DFG) as part of SFB 1102 Information Density             fixations in reading: Implications for the ez reader model.
and Linguistic Encoding.                                             Journal of Experimental Psychology: Human Perception
                                                                     and Performance, 30(12).
                        References                                 Rayner, K. & Well, A. (1996). Effects of contextual
Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013).          constraint on eye movements in reading: A further
   Random effects structure for confirmatory hypothesis              examination. Psychonomic Bulletin & Review, 3(4): 504-
   testing: Keep it maximal. Journal of Memory and                   509.
   Language, 68(3), 255-278.                                       Schneider, W., Eschman, A., & Zuccolotto, A. (2002). E-
Bates, D., & Sarkar, D. (2007). lme4: Linear mixed-effects           Prime: User's guide. Psychology Software Incorporated.
   models using S4 classes. ‘R’ package. Version 0.9975-12.        Stolcke, A. (2002). SRILM – An extensible language
   http://CRAN. R-project.org                                        modeling toolkit. In Interspeech (Vol. 2002, p. 2002).
Drieghe, D., Rayner, K., & Pollatsek, A. (2005). Eye               Witzel, N., Witzel, J., & Forster, K. (2012). Comparisons of
   movements and word skipping during reading revisited.             online reading paradigms: Eye tracking, moving-window,
   Journal of Experimental Psychology: Human Perception              and maze. Journal of Psycholinguistic Research, 41(2),
   and Performance, 31(5): 954–969.                                  105-128.
                                                               3173

