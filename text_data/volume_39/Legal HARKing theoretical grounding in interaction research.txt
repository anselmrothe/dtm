Legal HARKing: theoretical grounding in interaction research
Saul Albert (saul.albert@tufts.edu)
J.P. de Ruiter (jp.deruiter@tufts.edu)
Department of Psychology, Tufts University
Medford, MA 02155 USA
Abstract
In psychology, we tend to follow the general logic of falsificationism: we separate the ‘context of discovery’ (how
we come up with theories) from the ‘context of justification’
(how we test them). However, when studying human interaction, separating these contexts can lead to theories with low
ecological validity that do not generalize well to life outside
the lab. We propose borrowing research practices from formal inductive methodologies during the process of discovering new regularities and analyzing natural data without being
led by theory. From the perspective of experimental psychology, this approach may appear similar to the ‘questionable research practice’ of HARKing (Hypothesizing After The Results are Known). We argue that a carefully constructed form
of HARKing can be used systematically and transparently during exploratory research and can lead to more robust and ecologically valid theories. Keywords: HARKing; experimental
psychology; conversation analysis; methodology; interaction

Performance-enhancing questionable practices
Most discussions of the current ‘replication crisis’ in psychology and the social sciences (Pashler & Harris, 2012; Pashler
& Wagenmakers, 2012) focus on identifying and mitigating
the biases and incentives that lead researchers to adopt questionable research practices (QRPs): a range of methods for
manipulating experimental results and processes that John,
Loewenstein, and Prelec (2012) describe as “the steroids of
scientific competition, artificially enhancing performance”.
But science—at least ideally—is not about competition, and
the highest scientific achievements are of benefit to all. It
makes sense, therefore, to look at some QRPs and their underlying rationales in more detail: why are they so tempting?
What makes them ‘safe’ or ‘unsafe’ for science in specific
contexts? For example, qualitative, inductive methods often
used in cognitive science such as grounded theory (Glaser &
Strauss, 1967) are very useful for exploratory studies in many
research areas, but may produce misleading inferences when
used to code certain kinds of behavioral phenomena for confirmatory, quantitative research into language and human interaction (Stivers, 2015). However, rather than simply labeling all such methods as QRPs in the context of experimental, confirmatory research, we may be able to borrow from
them to enhance our research results without compromising
our methodological rigor. In many of the failed replications
reported in Open Science Collaboration (2015), it seems that
QRPs are used to increase the probability of ‘finding’ an effect predicted by the stated theory. Theorizing about an interactional phenomenon that has no grounding in interactional
reality makes QRPs attractive, simply because they make it
more likely that researchers will be able to report significant
effects that support their theory. The issue underlying the use

of QRPs in the study of human interaction, then, may be intrinsically related to the broader problem of groundless theorizing, where theories are formulated without being familiar with the situations they theorize about. We suggest that
this problem, in turn, stems from some uncritical assumptions
about science and human interaction.

The problem of groundless theorizing
A common assumption about falsificationism, still implicitly
or explicitly a major philosophical underpinning of empirical
science, is that as long as a theory can be falsified by testing
a hypothesis, the scientist is free to theorize any conceivable
causal relationship between any measurable variables. There
is nothing inherently wrong with this approach if all plausible
confounding variables can be controlled, and this theoretical
freedom of movement is tremendously powerful. Popper was
inspired by how the freedom to theorize raised the stakes for
cosmologists such as Einstein, whose entire theory of general
relativity could have been falsified if just one of his audacious predictions about electromagnetism and gravitational
potential had turned out to be false. However, in the context of human interaction research, it is notoriously difficult
to control for confounds because there are many human behaviors that are very difficult if not impossible to emulate in
controlled conditions (De Ruiter, 2013; Schegloff, 2006), and
just recording or observing people interacting may change the
ways they interact in unpredictable ways (Labov, 1972). In
this paper we describe a set of pre-theoretical research procedures that interaction researchers can use to constrain their
theories to match observable facts within the domain of interest. While this may sound like the questionable research
practice of HARKing (Hypothesizing After the Results are
Known) (Kerr, 1998), we argue that systematic, inductive
methods for analyzing social interaction can provide a principled and effective way to ground theorizing about human
interaction, leading to more robust and relevant theories.

Contexts of discovery and contexts of justification
The ‘context of discovery’ is the situation in which a phenomenon of interest is discovered. For example, when studying human interaction, a useful context of discovery would
be an otherwise naturalistic conversation that happened to
be recorded for analysis (Potter, 2002). ‘Contexts of justification’, in this example, might then include the laboratory, the conference paper, and the academic literature within
which the empirical details are reported, analyzed and formulated as a scientific discovery (Bjelic & Lynch, 1992). These

1525

describes as ‘mutual accountability’. For analysts, discovery
and justification use as many of the same resources as possible, but are motivated by different concerns i.e. to provide
causal explanations for the events and phenomena discovered
for the purposes of scientific research, but without the urgent
imperatives of mutual accountability. The challenge for analysts wishing to improve their theories by bringing together
contexts of discovery and contexts of justification is to constrain themselves to testing theories that deal with resources
and methods that are evidently available to both analysts and
participants.
Figure 1: Interactional resources and methods within participants’ or analysts’ contexts of discovery and justification.

Pre-experimental HARKing for better theories

are important distinctions for what we think of as theoretical grounding: drawing together the contexts of discovery
and contexts of justification in order to place principled limits on theorizing about interaction. However, drawing these
contexts together in interaction research requires analysts to
take account of critical distinctions between what kinds of
evidence is available to analysts and the kinds of interactional resources available to participants in the situation itself
(Garfinkel, 1964; Lynch, 2012). Figure 1 lists a few key resources participants and analysts can both use when discovering and justifying interactional phenomena, and (underneath,
in red) some of the resources for making sense of interaction that are only available from one perspective or the other.
Some of these resources are shared, for example, both participants and overlooking analysts can use observable features
of the setting and the visible actions of the people within it to
discover new phenomena. Both participants and analysts can
also observe when these visible actions are contiguous and
uninterrupted (Sacks, 1987), and can see if certain actions are
routinely matched into patterns of paired or ‘adjacent’ initiations and responses (Heritage, 1984, p.256). Similarly, both
analysts and participants can observe when contiguous flows
of initiation and response seem to break down, falter or require repair to re-establish orderliness and ongoing interaction (Schegloff, Jefferson, & Sacks, 1977). By contrast, many
other resources and methods for making sense of the situation
are exclusively available to one or another role. For example,
analysts can repeatedly listen to a recording, slow it down,
speed it up, and can precisely measure, quantify, and deduce cumulative facts that would be unimaginable to participants in the interaction. On the other hand, participants may
draw on their store of tacit knowledge and use introspection—
options which are not necessarily observable for overlooking
analysts—to make sense the current state and consequences
of the interaction. ‘Discovery’ for participants, then, is some
action or phenomenon observably discovered and treated as
mutually relevant with others in the situation. Justification is
the interactional work participants do with others in the setting to display and uphold the mutual intelligibility and rationale of their actions: an imperative that Garfinkel (1967)

When we advocate pre-experimental HARKing, it should be
clear that this proviso about using interactional resources
available to both participants and analysts excludes ‘exploratory data analysis’ (Jebb, Parrigon, & Woo, 2016) or
other uses of inferential statistics for pre-confirmatory theorizing since this is not something participants would be able
to use as a resource within their contexts of discovery or
justification. Rather, the research procedures recommended
here are inspired by conversation analysis (CA): an approach
to interaction research which exemplifies the use of empirical constraints on theorizing (Schegloff, 2007, pp. xii-xiii),
and which has tended to avoid engagement with experimental
studies that necessarily prioritize theorizing in order to arrive
at causal explanations (Kendrick, 2017). The ‘theoretical asceticism’ (Levinson, 1983, p. 295) of CA’s research practices
makes them very useful for drawing together contexts of discovery and justification in a principled and coherent way (De
Ruiter & Albert, 2017). In relation to theorizing, we call these
practices ‘pre-experimental HARKing’ to draw attention to
the distinction between HARKing as a QRP (after having
produced a theory and tested it with an experiment), and CA’s
“qualitative, inductive, and strictly empirical” (Haddington,
Mondada, & Nevile, 2013, p.7) research processes of systematic observation and ongoing informal peer review that takes
place before any theorizing is allowed. One of the ironies of
theories and experiments in interaction psychology that use
corpora is that the ‘results’ (i.e. what actually happened in
the interaction) usually are known before the hypotheses or
research questions are formulated. It therefore makes sense
to use these data to develop better theories and operationalizations before having to make key decisions about coding,
quantifying and analyzing interactional phenomena. The risk
otherwise is that what gets coded, quantified and tested may
not turn out to be observably relevant to the participants in the
interaction at all (Schegloff, 1993; Stivers, 2015). It should
be clear by now that the term HARKing is not used perjoratively here. Since existing data, intuitions, and past results
often provide the basis for theorizing at a pre-experimental
stage in any case, we advocate using CA’s systematic and
transparent procedures to constrain and ground those theories
empirically.

1526

Sharing contexts of discovery and justification
A ‘result’ in the participants’ context of discovery can be
thought of as the achievement of a reciprocal action in a social
situation such as successfully ordering a beer in a bar. This is
motivated quite differently from from the ‘results’ that might
be discussed in the analyst’s context if the researcher were,
for example, designing an experiment to try to figure out what
behaviors enable people to obtain beer in bars. Loth, Huth,
and De Ruiter (2013) show that going to a bar and systematically observing how beer-ordering is achieved through interaction provides very informative and somewhat surprising
results as the basis for formulating new theories. They found
that all customers have to do to initiate a successful beerordering interaction is to stand at the bar looking towards the
bartender and that any use of the stereotypical ordering-like
actions they had anticipated in fact proved to be unnecessary
and even potentially disruptive. The first step in drawing together contexts of discovery and justification, then, is to find
a setting where participants do observable interactional work
to achieve their results (getting a beer in a bar) in ways that
are informative for the analyst’s results (finding out how people get beer in bars). CA terms this kind of social situation
that can be used as a starting point for analysis a ‘perspicuous setting’ from the Latin perspicio ‘to see through’, denoting a situation that functions like a microscope that analysts
can use to examine the local organization of human affairs.
Garfinkel (1992, pp. 184-186) emphasizes that in perspicuous settings participants’ affairs are “locally produced, locally occasioned and locally ordered” and that these function
as contexts of discovery and justification of what is relevant
for the participants, whose interactions in those contexts are
conducted without reference to analyst’s concerns. The bar
is an obvious choice as a perspicuous setting for exploring
beer-ordering, but even if there is no specific domain of inquiry, new questions can also emerge from repeated viewing
and ‘unmotivated’ analysis of data. For example, a corpus of
video recordings of guided walking tours has provided a perspicuous setting for discovering questions about how people
organize themselves as mobile groups (De Stefani & Mondada, 2013), about the roles and procedures involved in getting the group to examine something (De Stefani, 2010), and
to then coordinate the process of walking away together interactionally (Broth & Mondada, 2013). The starting point for
Legal HARKing, then, is to find a perspicuous setting where
participants work together to achieve a given outcome in ways
that researchers can then observe and analyze as the basis for
formulating more interactionally grounded theories.

Transcribe interactionally relevant details
Conversational turn-taking is one of the most clearly observable systematic forms of organization in interaction (Sacks,
Schegloff, & Jefferson, 1974). In this sense conversation
is a useful example of a context of discovery and justification that is shared between participants and analysts alike. In
the context of conversation, participants discover things like

whose turn it is to talk next, and justify their discoveries using a clearly organized protocol for turn-allocation and turntransition. For conversation analysts, the turn-taking system
became a foundational context for discovery and justification
when Sacks et al. (1974) showed how it could explain systematic features of everyday interaction such as the tendency
for minimal gaps and overlaps in natural talk (a discovery that
has subsequently been tested experimentally and across multiple languages (Stivers et al., 2009)). CA’s transcription system was devised by Gail Jefferson to highlight the systematic
patterns of overlap and variations in prosody and intonation
(Hepburn & Bolden, 2012). Although phonetic transcription
in IPA notation provides a much higher degree of accuracy
than standard orthography, these objective levels of description are not necessarily available to participants themselves,
and in any case people in everyday interactions do not usually
make an issue of pronunciation. Jeffersonian transcription is
relatively simple to read and use, and is optimised to spatialize and represent the features of talk such as speed-ups,
stress, stretches, overlaps, and gaps that seem most relevant
to participants’ contexts of discovery and justification. Most
importantly, the activity of hand-transcribing conversational
data is a very useful pre-analytical activity in itself through
which researchers can become intimately familiar with their
data by watching repeatedly while trying to capture the fine
details of whatever features are observably relevant to the participants themselves (Bolden, 2015). While all transcription
systems introduce the analytic perspectives and assumptions
of the analyst doing the transcription (Ochs, 1979), it makes
sense to use a system designed specifically to capture the details of talk most demonstrably relevant to how participants
maintain the smooth operation of the turn-taking system.

Use intersubjective review of subjective judgments
Another way to draw together the participants’ and analysts’
contexts of discovery and justification is to use the interactional aptitudes of the analysts themselves as a heuristic device to explore what is going on in the interaction. This
may sound like an overly subjective form of judgment, but
since the object of inquiry for analysts is human interaction
where we have no better measuring device than our own social intelligence, it makes sense to use our skills as interactants, even if we may not understand how these abilities work.
This problems of reliance on subjective intuition can be mitigated through interaction itself. The conversation analytic
‘data session’ is a research practice where analysts present
their data, describe what they see, and have their observations
tested against the intuitions and reasoned arguments of other
analysts. This is one of the least well-documented aspects of
CA, and is barely mentioned in the research or training literature (Sidnell & Stivers, 2012), although Ten Have (1999, pp.
140-141) provides a brief explanatory description.
“[The data session] often involves playing (a part of)
a tape recording and distributing a transcript...The session starts with a period of seeing/hearing and/or read-

1527

ing the data, sometimes preceded by the provision of
some background information by the ‘owner’ of the
data. Then the participants are invited to proffer some
observations on the data, to select an episode which
they find ‘interesting’ for whatever reason, and formulate their understanding, or puzzlement, regarding that
episode. Then anyone can come in to react to these
remarks, offering alternatives, raising doubts, or whatever.”

of substantial observations, methodological discussions,
and also theoretical points.”
Even after the analyst’s painstaking transcripts and observations have run the gauntlet of multiple data sessions where
flaws in theory may be identified and discussed, the CA research cycle has just begun by finding candidate phenomena
for analysis.

The group often consists of both experienced and novice
analysts, so there is an element of ‘tradecraft’ and apprenticeship built into the structure of the data session (Jordan
& Henderson, 1995; Harris, Theobald, Danby, Reynolds, &
Rintel, 2012). Ten Have (1999) in fact attributes his learning CA to having attended data sessions with Gail Jefferson
and Emanuel Schegloff, so despite the lack of documentation,
the data session has clearly been central to CA from the start.
There is a scattering of advice about how to run such sessions in some textbooks, in a few short papers (Hindmarsh,
2012) and even within some reflexive studies that explore CA
data sessions as interactional situations in themselves using
CA (Antaki, 2008; Harris et al., 2012). These accounts also
provide some useful technical advice, for example, Jordan
and Henderson (1995) suggest that the ‘owner’ of the data
plays back short clips of up to twenty seconds, then discusses
each clip, but limits the discussion to 5 minutes before looking at more data, so that “no single participant can speculate for very long without being called upon to ground her
or his argument in the empirical evidence, that is to say, in
renewed recourse to the tape.” Heath, Hindmarsh, and Luff
(2010, pp. 156-157) have some similarly practical advice to
limit the session to 20 or fewer people, and not to “cheat and
look ahead, or rely on information exogenous to the clip itself”: essentially following the rule of thumb to avoid using resources or methods unavailable to participants themselves. Of course the frequent fast forwarding and rewinding of recordings and many of the other analytical methods
described here do, nonetheless, rely on resources not necessarily available within the participants’ contexts of discovery
and justification. However, since the data session is an interactional situation where peers are involved in grounding
one another’s assumptions about the interaction through interaction, there is also a degree of mutual accountability at
work that may compensate for the loosening of CA’s strict
methodological constraints. Through the data session, theories and assumptions are subjected to open and self-critical
debate. Ten Have (1999) sums up this analytic attitude neatly:
“What is most important in these discussions is that the
participants are, on the one hand, free to bring in anything they like, but, on the other hand, required to ground
their observations in the data at hand, although they may
also support them with reference to their own data-based
findings or those published in the literature. One often gets, then, a kind of mixture, or coming together,

The analytical phases of pre-experimental
HARKing
Having attended multiple data sessions to explore candidate
phenomena and findings, there are several further stages required to develop analytically grounded theories about interaction. Conversation analytic primers are now available
(Schegloff, 2007; Sidnell & Stivers, 2012; Ten Have, 1999),
so only a summary of analytic procedures is provided here.
After a series of data sessions, analysts collect multiple instances of a target phenomenon each with minor variations
in terms of their composition, sequential structure and their
range of uses in interaction. Analysts often then work on
‘single case analyses’ involving an extended study of a few
episodes of interaction featuring the target phenomenon in
great detail. Over time, the analyst may build up hundreds of
cases, organized into ‘collections’ (Schegloff, 1996), working
towards a more complete characterization of the phenomenon
and its specialized variations. For example, Schegloff (1968)
describes collecting 499 cases of telephone call openings, and
considering his collection almost complete and ready to be
analyzed. It was the 500th case, however, which provided
him with a single ‘deviant case’ that forced him to re-evaluate
his findings about the sequential order of ringing and greeting
exchanges in telephone call openings. This example is often
cited to demonstrate the difference between these approaches
and more conventional case studies. Each single case starts
from first (interactional) principles in trying to explore the
setting from a vantage point as close to the context of discovery and justification of the participants as possible. For
this reason, Schegloff’s (1968) example functions as a kind
of applied falsificationism: the only way the 500th case could
make sense from the analyst’s context was to (quite radically)
change the theory. Furthermore, long-standing collections
of often-analyzed phenomena become theory-like over time,
and can be subject to falsification and ongoing modification
through contradiction by subsequent CA findings, or through
changes in people’s patterns of behavior over time. For example, since the mid-2000s the most common telephone call
opening sequence has changed significantly due to the prevalence of caller-ID on mobile phones (Raudaskoski, 2009).
This process of careful, qualitative analysis is required before
CA researchers even consider developing a formal coding
scheme (Dingemanse, Kendrick, & Enfield, 2016; Stivers &
Enfield, 2010) with which to quantify their findings (Stivers,
2015) and run experiments—although these last few steps are
still not widely accepted, and remain controversial within CA

1528

References

(Kendrick, 2017). While this overall procedure is clearly extremely laborious, it does have the reassuring advantage that
the phenomena described are guaranteed to have actually occurred in reality, not only in our theoretical imagination.

Summary: Better theorizing after legal
HARKing
This paper argues for researchers of human interaction to devote attention and resources to systematically exploring the
context of discovery where their theories will be formulated
by extending the falsificationist paradigm. Before we theorize and then test our predictions experimentally, we suggest
researchers borrow methods from conversation analysis and
other formal inductive methods to enhance the performance
of our theories with a kind of pre-experimental ‘legal HARKing’. This procedure involves using detailed Jeffersonianstyle transcription, holding data sessions and subjecting our
qualitative findings to ongoing, critical analysis before developing theories. By proceeding with our analysis with a sensitivity to the kinds of resources that participants themselves
have at hand, we can identify interactional practices that are
psychologically relevant and consequential for participants
(and not just researchers), and empirically grounded in natural interaction. We expect this kind of grounding to improve
the relevance, robustness, and replicability of human interaction research by producing more theoretically grounded hypotheses that we can then test using traditional experimental
methods. As long as—at this stage—we pre-register our experiments, we can harness the performance-enhancing benefits of legal HARKing while excluding the dangerous possibility of ‘illegal’ post-experimental HARKing. More generally, since research practices are seen as ‘questionable’ in relation to the conventions of a specific methodological framework, we suggest that if we reconsider them at a critical distance from any one methodology, these practices may have
many potentially beneficial applications. From the perspective of the experimental research practices that predominate
within cognitive science and psychology (Toomela, 2014),
for example, the inductive categorizing and coding methods
of grounded theory may be seen as ‘questionable’. Similarly,
from the perspective of generalization-oriented experimental studies, the scope of theories derived from micro-analytic
methods such as CA (e.g. about turn-taking) can seem almost trivial (Heritage, 2008). However, taken together the
body of work derived from CA’s empirical studies constitutes
a very broad set of findings about interaction against which
generalized theories can be tested (De Ruiter & Albert, 2017).
While different research practices address different problems
and questions at different scales, they may also have some
useful practical and philosophical intersections. One scientists’ ‘questionable’ research practice can be another’s means
of rigorous inquiry, and perhaps remaining ‘questionable’—
in the sense of being open to critical review—is something
more researchers could aim for in their research practices.

Antaki, C. (2008). Accounting for moral judgments in academic talk : The case of a conversation analysis data session. Text & Talk, 28(1), 1–30.
Bjelic, D., & Lynch, M. (1992). The work of a (scientific)
demonstration: Respecifying Newton’s and goethe’s theories of prismatic color. In G. Watson & R. M. Seiler (Eds.),
Text in context: Contributions to ethnomethodology (pp.
52–78). Sage Publications Newbury Park, CA.
Bolden, G. B. (2015, jul). Transcribing as research: “manual” transcription and conversation analysis. Research on
Language and Social Interaction, 48(3), 276–280.
Broth, M., & Mondada, L. (2013, #feb#). Walking away:
The embodied achievement of activity closings in mobile
interaction. Journal of Pragmatics, 47(1), 41–58.
De Ruiter, J. P. (2013). Methodological paradigms in interaction research. In I. Wachsmuth, J. P. De Ruiter, P. Jaecks,
& S. Kopp (Eds.), Alignment in communication: Towards a
new theory of communication. John Benjamins Publishing
Company.
De Ruiter, J. P., & Albert, S. (2017, jan). An appeal for
a methodological fusion of conversation analysis and experimental psychology. Research on Language and Social
Interaction, 1–18.
De Stefani, E. (2010). Reference as an interactively and
multimodally accomplished practice. organizing spatial reorientation in guided tours. In M. Pettorino, A. Giannini,
I. Chiari, & F. M. Dovetto (Eds.), Spoken communication
(pp. 137–170). Newcastle: Cambridge Scholars Publishing.
De Stefani, E., & Mondada, L. (2013, #dec#). Reorganizing mobile formations: When “guided” participants initiate
reorientations in guided tours. Space and Culture, 17(2),
157–175.
Dingemanse, M., Kendrick, K. H., & Enfield, N. J. (2016,
jan). A coding scheme for other-initiated repair across languages. Open Linguistics, 2(1).
Garfinkel, H. (1964). Studies of the Routine Grounds of
Everyday Activities. Social Problems, 11(3), 225–250.
Garfinkel, H. (1967). Studies in ethnomethodology. Englewood Cliffs, New Jersey: Prentice-Halll.
Garfinkel, H., & Wieder, D. L. (1992). Two incommensurable, asymmetrically alternate technologies of social analysis. In G. Watson & R. M. Seiler (Eds.), Text in context:
Contributions to ethnomethodology (pp. 175–206). Newbury Park, CA: Sage New York.
Glaser, B. G., & Strauss, A. L. (1967). The discovery of
grounded theory: Strategies for qualitative research. New
York, NY: Aldine de Gruyter.
Haddington, P., Mondada, L., & Nevile, M. (2013). Interaction and mobility: Language and the body in motion
(P. Haddington, L. Mondada, & M. Nevile, Eds.). Berlin,
Boston: De Gruyter.
Harris, J., Theobald, M. A., Danby, S. J., Reynolds, E., &
Rintel, S. (2012). “What’s going on here?” The pedagogy

1529

of a data analysis session. In A. Lee & S. J. Danby (Eds.),
Reshaping doctoral education: International approaches
and pedagogies (pp. 83–96). London: Routledge.
Heath, C., Hindmarsh, J., & Luff, P. (2010). Video in qualitative research: analysing social interaction in everyday life.
London: Sage Publications.
Hepburn, A., & Bolden, G. B. (2012). The Conversation Analytic Approach to Transcription. In J. Sidnell & T. Stivers
(Eds.), The Handbook of Conversation Analysis (pp. 57–
76). Oxford: John Wiley & Sons.
Heritage, J. (1984). Garfinkel and ethnomethodology. Cambridge: Polity Press.
Heritage, J. (2008). Conversation analysis as social theory.
In Bryan Turner (Ed.), The new Blackwell companion to
social theory (pp. 300–320). London: Blackwell.
Hindmarsh, J. (2012). Heath’s natural habitat: The data session. In P. Luff, J. Hindmarsh, D. vom Lehn, & B. Schnettler (Eds.), Work, interaction and technology: A festschrift
for christian heath. (pp. 21–23). London: Dept. of Management, Kings College London.
Jebb, A. T., Parrigon, S., & Woo, S. (2016, aug). Exploratory
data analysis as a foundation of inductive research. Human
Resource Management Review.
John, L. K., Loewenstein, G., & Prelec, D. (2012, may).
Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532.
Jordan, B., & Henderson, A. (1995). Interaction analysis:
Foundations and practice. The journal of the learning sciences, 4(1), 39–103.
Kendrick, K. H. (2017, jan). Using conversation analysis
in the lab. Research on Language and Social Interaction,
1–11.
Kerr, N. L. (1998, aug). HARKing: Hypothesizing after
the results are known. Personality and Social Psychology
Review, 2(3), 196–217.
Labov, W. (1972). Sociolinguistic patterns. University of
Pennsylvania Press, Incorporated.
Levinson, S. C. (1983). Pragmatics. Cambridge: Cambridge
University Press.
Loth, S., Huth, K., & De Ruiter, J. P. (2013). Automatic
detection of service initiation signals used in bars. Frontiers
in Psychology, 4.
Lynch, M. (2012). Revisiting the cultural dope. Human
Studies, 35(2), 223–233.
Ochs, E. (1979). Transcription as theory. In E. Ochs &
B. B. Schieffelin (Eds.), Developmental pragmatics (pp.
43–72). New York: Academic Press.
Open Science Collaboration. (2015, August). Estimating the reproducibility of psychological science. Science,
349(6251), aac4716–aac4716.
Pashler, H., & Harris, C. R. (2012, #nov#). Is the replicability
crisis overblown? three arguments examined. Perspectives
on Psychological Science, 7(6), 531–536.
Pashler, H., & Wagenmakers, E. (2012, nov). Editors’ intro-

duction to the special section on replicability in psychological science. Perspectives on Psychological Science, 7(6),
528–530.
Potter, J. (2002, aug). Two kinds of natural. Discourse Studies, 4(4), 539–542.
Raudaskoski, S. (2009). Tool and machine: The affordances
of the mobile phone (Unpublished doctoral dissertation).
Sacks, H. (1987). On the preferences for agreement and contiguity in sequences in conversation. In G. Button & J. Lee
(Eds.), Talk and social organization (pp. 54–69). Clevedon:
Multilingual Matters.
Sacks, H., Schegloff, E. A., & Jefferson, G. (1974). A
simplest systematics for the organization of turn-taking for
conversation. Language, 50(4), 696–735.
Schegloff, E. A. (1968, #dec#). Sequencing in Conversational
Openings. American Anthropologist, 70(6), 1075–1095.
Schegloff, E. A. (1993). Reflections on Quantification in the
Study of Conversation. Research on Language & Social
Interaction, 26(1), 99–128.
Schegloff, E. A. (1996). Confirming allusions: Toward an
empirical account of action. American Journal of Sociology, 102(1), 161–216.
Schegloff, E. A. (2006). On possibles. Discourse Studies,
8(1), 141157.
Schegloff, E. A. (2007). Sequence organization in interaction: Volume 1: A primer in conversation analysis. Cambridge: Cambridge University Press.
Schegloff, E. A., Jefferson, G., & Sacks, H. (1977). The
preference for self-correction in the organization of repair
in conversation. Language, 53(2), 361–382.
Sidnell, J., & Stivers, T. (2012). The Handbook of Conversation Analysis. Oxford: John Wiley & Sons.
Stivers, T. (2015, #jan#). Coding social interaction: A heretical approach in conversation analysis? Research on Language and Social Interaction, 48(1), 119.
Stivers, T., & Enfield, N. J. (2010, #oct#). A coding scheme
for question–response sequences in conversation. Journal
of Pragmatics, 42(10), 2620–2626.
Stivers, T., Enfield, N. J., Brown, P., Englert, C., Hayashi, M.,
Heinemann, T., . . . Levinson, S. C. (2009, #jun#). Universals and cultural variation in turn-taking in conversation.
Proceedings of the National Academy of Sciences of the
United States of America, 106(26), 10587–92.
Ten Have, P. (1999). Doing conversation analysis: A Practical Guide (1st ed.). London: Sage Publications.
Toomela, A. (2014). Mainstream psychology. In T. Teo (Ed.),
Encyclopedia of critical psychology (1st ed., pp. 1117–
1125). New York: Springer-Verlag.

1530

