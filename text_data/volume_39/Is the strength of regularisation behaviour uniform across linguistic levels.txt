       Is the strength of regularisation behaviour uniform across linguistic levels?
       Carmen Saldana (C.C.Saldana@sms.ed.ac.uk), Kenny Smith, Simon Kirby & Jennifer Culbertson
                     Centre for Language Evolution, School of Philosophy, Psychology and Language Sciences,
                The University of Edinburgh, Dugald Stewart Building, 3 Charles Street, Edinburgh, EH8 9AD, UK
                               Abstract                                 periods when pidgins are highly inconsistent, linguistic lev-
                                                                        els might behave differently: morphologically complex traits
   Human languages contain very little unconditioned variation.
   In contexts where language learners are exposed to input that        such as inflectional morphology seem to be highly simpli-
   contains inconsistencies, they tend to regularise it, either by      fied whilst syntactic traits such as word order tend to repro-
   eliminating competing variants, or conditioning variant use on       duce the input complexity more closely (Good, 2015; Siegel,
   the context. In the present study we compare regularisation
   behaviour across linguistic levels, looking at how adult learn-      2004). Good (2015) argues that this asymmetry is given by a
   ers respond to variability in morphology and word order. Our         break in transmission from source languages for morphologi-
   results suggest similar strengths in regularisation between lin-     cal traits, which are only successfully transmitted if an entire
   guistic levels given input languages whose complexity is com-
   parable.                                                             contrasting paradigm is available to the learner, which is not
   Keywords: artificial language learning; statistical learning;        the case in periods of linguistic instability. However, word or-
   regularisation; variation; complexity; morphology; word order        der variation can be contrastive as well (e.g. S-Aux inversion
                                                                        to distinguish illocutionary forces). Alternatively, a more par-
                           Introduction                                 simonious hypothesis we could entertain is that a general ten-
While languages exhibit variation at all linguistic levels, in          dency for pidgins to comprise highly simplified morpholog-
the form of paraphrases, synonyms, allomorphs and allo-                 ical traits and more conservative word order is rooted in the
phones, that variation tends to be predictable: the choice              differing complexity of these traits in the source languages;
of variant is (at least partially) conditioned by some aspect           Hudson Kam and Newport (2009) show that learners are more
of the social or linguistic context. Occasionally, language             likely to regularise complex systems of variation.
learners are exposed to input that involves inconsistencies,               Recent experimental studies have separately explored the
for instance, when new variants are introduced into an es-              effect of learning biases on typological asymmetries found in
tablished system, or when conventions are still not estab-              morphology and word order respectively. In morphology for
lished, as in emerging languages (Senghas & Coppola, 2001;              example, St Clair, Monaghan, and Ramscar (2009) provide
Siegel, 2004). Learners under those circumstances tend to               evidence of a preference for suffixing over prefixing, mir-
reduce or remove such inconsistencies, i.e. they regularise             roring the cross-linguistic preference for suffixing. In word
their input. This can be achieved either by removing com-               order, Culbertson et al. (2012) show that learners prefer con-
peting variants, or conditioning variant choice on the context          sistent harmonic word order patterns (i.e. all modifiers either
(Ferdinand, Kirby, & Smith, 2017).                                      pre-nominal or post-nominal), also found more commonly in
   Regularisation has been documented extensively across                the world’s languages. Moreover, Culbertson et al. (2012)
linguistic levels (i.e. phonology, morphology, syntax and               show that this bias leads to different regularisation behaviour
the lexicon) in natural language; e.g. in language acquisi-             for different word order patterns. Nevertheless, no study has
tion, language change, and in emerging languages (Senghas               hitherto tried to systematically compare regularisation be-
& Coppola, 2001; Siegel, 2004; van Trijp, 2013). Experimen-             haviour across linguistic levels. Uncovering differences in
tal studies involving artificial language learning and statisti-        regularisation behaviour across linguistic levels could shed
cal learning techniques report regularisation behaviour during          light on the intriguing asymmetry found in pidgin languages:
the learning and production of probabilistic unconditioned              morphological paradigms seem to be highly simplified whilst
variation in different linguistic units, across different linguis-      input complexity is more closely reproduced in word order.
tic levels (Culbertson, Smolensky, & Legendre, 2012; Fehér,               In the present study we combine artificial language learn-
Wonnacott, & Smith, 2016; Hudson Kam & Newport, 2005,                   ing and statistical learning techniques to systematically com-
2009; Wonnacott & Newport, 2005). Nevertheless, it still                pare the strength of regularisation of inflectional morphology
remains an open question whether regularisation behaviour               and word order, controlling for asymmetries in the complex-
applies with uniform strength across linguistic levels and to           ity and variability of the input languages.
what extent level-specific biases interact with regularisation
during language learning and use.
                                                                                                Experiment 1
                                                                        We utilise the methodology developed in Culbertson et al.
Level-specific effects in regularisation behaviour                      (2012); Hudson Kam and Newport (2005). Adult learners
Research in second language acquisition and pidgin and cre-             are exposed to a miniature artificial language featuring an
ole studies has highlighted different developmental paths               inconsistent mixture of synonymous variants. We are inter-
for morphology and syntax cross-linguistically (Good, 2015;             ested in how learners restructure the probabilistic uncondi-
Slabakova, 2013). Studies in pidginisation suggest that, in             tioned variation in the input languages, and to what extent that
                                                                    1023

restructuring is comparable across linguistic levels (specifi-        Table 1: Probabilistic input languages in the Morphology and
cally, morphology and word order).                                    Word order conditions. Languages contain probabilistic un-
                                                                      conditioned variation in inflectional morphology or word or-
Method                                                                der respectively. All morphological variation resides in the
                                                                      suffixation of the modifiers. All word order variants conform
Participants Fifty-six native-English speakers (aged be-              to constituent structure [Num [Adj N]]. There are three types
tween 18 and 41, mean = 23.2) were recruited from the Uni-            of NPs: Num Only (single Num modifier) refer to objects in
versity of Edinburgh’s Careers Service database of vacancies.         pairs and in grey-scale, Adj Only (single Adj modifier) refer
Each was compensated £6. Twenty-six participants were as-             to a single object coloured in blue, and two-Mod(ifier) NPs
signed to the Morphology condition, and 26 to the Word Or-            (with both Num and Adj modifiers) correspond to objects in
der condition; the data from a further 4 participants (all in the     pairs coloured in blue. Languages include two different nouns
morphology condition) were excluded as they either failed to          (each corresponding to a different object) and thus comprise
learn the noun lexicon or failed to learn the associations be-        a total of 16 NPs (8 per noun) that correspond to a total of 6
tween phrases and pictures.                                           pictures (1 per NP type, 3 per object).
Input languages We designed two novel languages which                   NP TYPE     M ORPHOLOGY CONDITION     W ORD O RDER CONDITION
contained probabilistic unconditioned variation either in mor-
phology or word order. Their respective probabilistic gram-                          0.6 NP → N nefri           0.6 NP → N nefri
                                                                          N UM
mars are shown in Table 1. Both languages were used to                    O NLY      0.4 NP → N nezno           0.4 NP → nefri N
describe simple pictures featuring one of two objects. Each
object appeared either singly or in a pair; and could appear
either in greyscale or coloured in blue. Descriptions were                A DJ       0.6 NP → N kogla           0.6 NP → N kogla
noun phrases composed of a Noun plus a Num(eral) and/or                   O NLY      0.4 NP → N kospu           0.4 NP → kogla N
Adj(ective) modifier, which were presented orthographically
and aurally to participants during the experiment.
   All lexical items were 5 graphemes/phonemes long and had                          0.6 NP → N kogla nefri     0.6 NP → N kogla nefri
a neighbourhood density of 0 in the English lexicon. Nouns                           0.13̄ NP → N kogla nezno   0.13̄ NP → nefri kogla N
                                                                          T WO
and modifiers differed in their syllabic structure; while all             M OD       0.13̄ NP → N kospu nefri   0.13̄ NP → nefri N kogla
were bisyllabic, nouns (i.e. “mokte” and “jelpa”) conformed
to a CVC.CV pattern, and modifiers to CV.CCV (based on                               0.13̄ NP → N kospu nezno   0.13̄ NP → kogla N nefri
English phonotactics and the Maximal Onset Principle).
Procedure Participants worked through a six-stage training
and testing regime.                                                   empirical probability of P = 0.6, and minority variants with
   Stage 1, noun familiarisation Participants were trained on         P = 0.4, as shown in Table 1. This phase comprised 40 trials
the two bare nouns that corresponded to pictures of the two           in total, divided in 2 blocks of 20 trials; each block consisted
different objects in the artificial language. During this phase,      of 15 exposure trials followed by 5 picture-selection trials.
participants underwent a block of training consisting of 6 ex-        Participants saw each of the four different one-modifier pic-
posure trials and 4 picture-selection comprehension trials (in        tures 5 times per block (order randomised).
that order) —each noun-picture pair appeared 5 times (order              Stage 3, one-modifier testing Stage 3 of the experiment
randomised). Common to all training blocks to follow, on              tested the participants’ knowledge of the language. They saw
each exposure trial participants were presented with a picture        the same pictures used in Stage 2 without accompanying text
(in this block always of a single object in grey-scale) and a         or audio and were asked to type in an appropiate description.
corresponding description in the language (in this block, a           They had to describe 20 pictures in total; each of the four dif-
bare noun), displayed both visually and aurally. On compre-           ferent one-modifier pictures was presented 5 times in random
hension trials, participants were asked to select a picture out       order.
of an array of four (in this stage, the two objects seen during          Stage 4, full training In Stage 4 participants were trained
training plus two distractors) that corresponded to the dis-          on a mix of one-modifier (a noun plus Adj or Num) and two-
played description in the alien language, and received feed-          modifier NPs (a noun plus both Num and Adj). Two-modifier
back on their accuracy.                                               NPs were used to describe pairs of blue objects. For one-
   Stage 2, one-modifier training In Stage 2 participants             modifier phrases, variants were chosen in the same way as
were trained on one-modifier NPs, i.e. a Noun plus either             in Stage 2. For two-modifier phrases, variants were also se-
Num or Adj only. Pictures contained any of the two objects            lected randomly from the grammars assigned, with empirical
presented either in blue and singly (Adj only) or in greyscale        probabilities of P = 0.6 and P = 0.13̄ for the majority and
and in pairs (Num only). For each picture, a variant was              the three minority variants respectively (see Table 1). This
selected randomly from the grammar assigned to the partic-            stage comprised 100 trials (20 Num Only, 20 Adj Only and
ipant. Both grammars contained majority variants with an              60 two-Mod), divided into 4 block of 25 (15 exposure train-
                                                                  1024

ing trials followed by 10 picture-selection trials). Participants      Table 2: Central tendencies of the proportion of majority in-
saw each of the four one-modifier pictures 10 times, and each          put variants in production by condition and NP type. From
of the two two-modifier pictures 30 times.                             left to right, the mean, median and mode(s).
   Stage 5, full testing Stage 5 tested participants’ knowledge                            Proportion Majority Input Variant in Production
of the whole language. They saw all pictures they had been                                              mean median mode(s)
trained on and were asked to type in appropriate descriptions.                           Num Only       0.704 0.8           0.919
They had to describe 52 pictures in total: 10 Adj Only (5                Morphology      Adj Only       0.669 0.7           0.916
per object), 10 Num Only (5 per object), 30 two-modifier (15                             two-Mod        0.609 0.65          0.843
per object), and additionally, 2 pictures of bare objects by                             Num Only       0.580 0.65          0.094 & 0.96
themselves and in grey-scale (1 per object).                              Word Order     Adj Only       0.585 0.7           0.104 & 0.947
                                                                                         two-Mod        0.442 0.33          0.089 & 0.92
Results
Output variability Figure 1 shows the entropy of partici-              regularisation behaviour (dependent variable: entropy). As
pants’ production systems for both the Morphology and Word             fixed effects we entered Condition (two levels: Morphology
Order conditions. Analyses are run on Stage 5’s testing ex-            as reference, and Word Order), NP Type (reverse Helmert
clusively, i.e. participants’ final production sets. Words in the      coded with the 3 ordered levels: Num Only, Adj Only and
productions were corrected for typos (and only typos). Shan-           two-Mod) and System (two levels: Input as reference, and
non entropy measures how variable participants’ productions            Output). We also entered all interactions between fixed ef-
are; the higher the scores, the more variable and the lower the        fects. As random effects, we included intercepts for Subject
scores, the more regular. The Shannon entropy (H) of phrase            as well as by-Subject slopes for the effects of NP Type and
use for participant is given by                                        System type. P-values were obtained through the lmerTest
                                n                                      package (Kuznetsova, Bruun Brockhoff, & Haubo Bojesen
                  H(X) = − ∑ P(xi )log2 P(xi )                 (1)     Christensen, 2015). Results show a significant effect of Sys-
                              i=1                                      tem (β = −0.346, SE = 0.085, p < .001), suggesting that
                                                                       participants did indeed regularise their input in their output
   where the sum is over the different variants, and P(xi ) is the
                                                                       productions. We also found a significant interaction between
empirical probability of variant xi in the set of a participant’s
                                                                       System and Condition (β = −0.284, SE = 0.119, p = .021),
productions, X. We treated the two nouns for the different
                                                                       suggesting that participants regularised their input signifi-
objects as the same variant when we calculated the entropy
                                                                       cantly more in the Word Order condition. Results show the
of the phrase variants such that no variability is introduced
                                                                       expected effect of higher input entropies in two-Mod NPs
by the correct use of the different nouns. Entropy lower- and
                                                                       (β = 0.21, SE = 0.024, p =< .001), and no significant in-
upper- bounds will vary depending on the number of required
                                                                       teractions between NP Type and System (largest: β = 0.027,
and possible variants as well as on the number of production
                                                                       SE = 0.028, p = .324) or between NP Type, System and Con-
trials. The most regular expressive language contains only
                                                                       dition (largest: β = −0.041, SE = 0.039, p = .299). These
one-to-one picture-phrase mappings and therefore only three
                                                                       results suggest that participants regularised their input sys-
different variants, one Num Only (e.g. N nefri), one Adj Only
                                                                       tems across conditions and NP types, and that participants in
(e.g. N kogla) and one two-modifier (e.g. N kogla nefri). The
                                                                       the Word Order condition regularised them more than those
final production phase consisted of 50 trials (excluding the
                                                                       in the Morphology condition.
two bare noun trials), divided up into 20 one-modifier trials
(half Num Only and half Adj Only) and 30 two-modifier tri-             Variant production Table 2 provides the central tenden-
als: the entropy lower bound for the language overall is thus          cies for proportion use of the majority input variant for each
1.37 bits, and 0 bits for each of the NP types.                        NP type. We observe that all distributions in the Word Or-
   Figure 1 shows the entropy scores for the set of all partici-       der condition are bimodal, with modes of the distributions of
pants’ productions (i.e. the overall language), as well as those       majority variant use at P ≤ 0.1 and P > 0.9 across NP types,
for the production sets for specific NP types in isolation: one-       suggesting two opposite trends amongst participants: one to-
modifier Num (Num Only), one-modifier Adj (Adj Only),                  wards the over-production of the majority input word order
and two-modifier (two-Mod) NPs. Entropy lower bounds and               variants and another, towards their under-production.
input entropies are represented as solid and dotted vertical              Participants under-producing the majority word order vari-
lines respectively. A visual inspection of the Morphology              ant in one-modifier NPs are necessarily producing modifiers
and Word Order conditions in Figure 1 suggests that in many            pre-nominally. Figure 2 shows the overall proportions of the
cases participants failed to reproduce the full variability of the     variants produced for two-Mod NPs by all participants. The
input languages; entropy scores are generally lower.                   input proportions are represented by the yellow vertical lines.
   We used the stats and lme4 packages developed in R                  The word order produced the most is the majority input vari-
(Bates, Mächler, Bolker, & Walker, 2015; R Core Team,                 ant N Adj Num. Although the three remaining input vari-
2015) to run a linear mixed effects regression model (which            ants (below the grey solid line division) were equally frequent
we will call Model 1) to explore the effect of condition on            in the input language, the Num Adj N word order is overall
                                                                   1025

                         overall                      Num Only                        Adj Only                      two−Mod
          50
                                                                                                                                        Morphology
          40
          30                    overall                                  Num Only                                Adj Only                                                          two−Mod
          20                    overall                                  Num Only                                Adj Only                                                          two−Mod
          50
          10
          50
                                                                                                                                                                                                           Morphology
                                                                                                                                                                                                            Morphology
          40
           0
          40
            30
          30
          50
                                                                                                                                                                                                                              Experiment 1
          20
                                                                                                                                        Word Order
          20
          40
          10
 % ppts
          10
          30
            0
          200            overall                      Num Only                        Adj Only                      two−Mod
          50
          10
          50
          50
                                                                                                                                                                                                           Word
                                                                                                                                                                                                            WordOrder
                                                                                                                                                                                                                 Order
          40
           0
  %%ppts
     ppts
                                                                                                                                        Morphology
                                                                                                                                          NoL1 Word Order Word Order
          40
          40
          30
          50
          30
          30
          20
          40
          20
          20
          10
          30
          10
          10
            0
          20
           00
                                                                                                                                                                                                           NoL1
                                                                                                                                                                                                            NoL1Word
                                                                                                                                                                                                                 WordOrder
                                                                                                                                                                                                                      Order
          50
          10
                                                                                                                                                                                                                              Experiment 2
          50
          50
          40
           0
          40
          40
 % ppts
          30     1.5    2.0    2.5       3.0   0.0     0.4         0.8       0.0      0.4        0.8         0       1        2
          30
          30                                                  Shannon Entropy (bits)
          20
          20
          20
          10
          10
          10
            0
           00
                  1.5         2.0        2.5   3.0           0.0           0.5             1.0         0.0         0.5            1.0                                  0.0   0.5   1.0   1.5   2.0   2.5
           1.5   2.0   2.5    3.0        0.0      0.5          1.0 Entropy
                                                                        0.0 (bits) 0.5   1.0 0.0 0.5 1.0 1.5 2.0 2.5
                                                                                                                                        NoL1 Word Order
    50                                                   Shannon
    40                                                   Shannon Entropy (bits)
    30
Figure
    20
        1: Entropy scores of participants’ production systems. From top to bottom, scores for the Morphology (green) and Word
Order
    10 (red) conditions in Experiment 1 and for the NoL1 Word Order condition (orange) in Experiment 2. From left to right,
entropies
     0     of participants’ full production sets as well as entropies by NP type: one-modifier Num (Num Only), one-modifier
         1.5 2.0 2.5 3.0       0.0   0.4     0.8     0.0    0.4     0.8       0      1 2
Adj (Adj Only), and two-modifier (two-Mod)        NPs. (bits)
                                       Shannon Entropy  Input entropy scores are indicated by dashed vertical lines. Minimum
entropy scores are indicated by solid vertical lines. Minimum entropy is always 0 for each NP type in isolation but 1.37 for the
overall system as it necessitates a minimum of 3 variants, one per NP type.
                                                                                                          We ran a logistic regression model, which we will call
   Adj Num N              I                                                                            Model 2, to explore the average difference between the pro-
                                                                                                       portions of Num Adj N variants in input and output linguistic
   N Num Adj              I
                                                                                                       systems. We entered System (two levels: Input as reference,
                                                                                                       and Output) as the only fixed effect. Random intercepts for
                                                                                                       Subject as well as by-Subject random slopes for the effect of
   Adj N Num                         I                                                                 System were also included. Results show that the Num Adj N
                                                                                                       variant is produced significantly less in output languages than
   Num N Adj                         I                                                                 in the input language (β = −7.641, SE = 1.943, p < .001).
                                                                                                       Only a minority of participants overproduced this variant, the
   Num Adj N                         I                                                                 majority of participants were in fact under-producing it. On
                                                                                                       top of the observed preference for harmonic order, these re-
                                                                                                       sults confirm a tendency to avoid systems with two opposite
   N Adj Num                                                   I
                                                                                                       N-peripheral variants, i.e. N Adj Num and Num Adj N.
                        0.00             0.25         0.50        0.75              1.00
                                    overall output proportions (Word Order)                            Discussion of Experiment 1
Figure 2: Box plot displaying the output proportions of two-                                           Our results provide evidence that learners regularise proba-
modifier variants in the Word Order condition with individual                                          bilistic unconditioned variation in both morphology and word
participants’ data points overlaid. Seen (bottom) and unseen                                           order. Regularisation behaviour is in line with an overarching
(top) variants during training are divided by a solid grey line.                                       simplicity bias argued to be at play in language learning and
Vertical yellow lines indicate input proportions.                                                      use (Culbertson & Kirby, 2016). Though the input languages
                                                                                                       were similar in terms of overall system complexity, regulari-
more frequently used (although only by a minority as indi-                                             sation behaviour was slightly stronger in the Word Order con-
cated by the median value 0). Only 30% of participants pro-                                            dition than in the Morphology condition. A close analysis of
duced systems with both harmonic variants (Num Adj N and                                               the variant usage in the Word Order condition suggests that
N Adj Num) —and only 19% produced both variants more                                                   this difference is driven by a bias in favour of harmonic N
than once, suggesting that although both harmonic orders are                                           Adj Num and Num Adj N variants but against their coexis-
preferred overall, they do not generally coexist within the pro-                                       tence within a system. This bias could be the result of L1
ductions of a single participant.                                                                      transfer; participants may have overproduced the Num Adj
                                                                                                 1026

Table 3: Probabilistic input language in the NoL1 Word order
condition in contrast to the Word Order condition in Experi-             Adj Num N     I
ment 1. Changes in the variant set are indicated with boxes.
  NP TYPE          W ORD O RDER          N O L1 W ORD O RDER             Num Adj N     I
     N UM     0.6 NP → N nefri           0.6 NP → N nefri
                                                                         Adj N Num            I
     O NLY    0.4 NP → nefri N           0.4 NP → nefri N
                                                                         Num N Adj            I
     A DJ     0.6 NP → N kogla           0.6 NP → N kogla
     O NLY    0.4 NP → kogla N           0.4 NP → kogla N                N Num Adj            I
              0.6 NP → N kogla nefri     0.6 NP → N kogla nefri          N Adj Num                                    I
     T WO      0.13̄ NP → nefri kogla N   0.13̄ NP → N nefri kogla
                                                                                     0.00          0.25        0.50        0.75        1.00
     M OD                                                                                 overall output proportions (NoL1 Word Order)
              0.13̄ NP → nefri N kogla   0.13̄ NP → nefri N kogla
              0.13̄ NP → kogla N nefri   0.13̄ NP → kogla N nefri
                                                                       Figure 3: Box plot displaying the output proportions of two-
                                                                       modifier variants in the NoL1 Word Order condition with
                                                                       individual participants’ data points overlaid. Divided by a
                                                                       solid grey line, seen (bottom) and unseen (top) variants dur-
N word order because it is the most common order in their              ing training. Vertical light brown lines indicate input propor-
L1 grammar. To minimise the possible effects of this level-            tions.
specific word order bias, Experiment 2 investigated learning
in a second word order condition, removing the English-like
two-modifier harmonic pattern from the input.                          tion such that NoL1 Word Order was directly compared to
                                                                       the Morphology condition from Experiment 1, and the Word
                        Experiment 2                                   Order condition was compared to the average of the Morphol-
                                                                       ogy and NoL1 Word Order conditions. Results show a signif-
Experiment 2 follows the same design as the Word Order con-            icant effect of System (β = −0.483, SE = 0.051, p < .001)
dition described in Experiment 1, with one difference: the set         and a significant interaction between Word Order and System
of two-modifier NP input variants. As illustrated in Table 3,          (β = −0.073, SE = 0.036, p = .046), ratifying the results in
we replaced the Num Adj N variant with the N Num Adj pat-              Model 1. However, we did not find a significant interaction
tern, maintaining the number of harmonic word orders (two,             between NoL1 Word Order and System (β = −0.063, SE =
i.e. N Adj Num and N Num Adj) but eliminating the L1 vari-             0.063, p = .317), suggesting that participants in the Morphol-
ant and the presence of opposite N-peripheral patterns. For            ogy and the NoL1 Word Order conditions regularised their
ease of reference, we call Experiment 2 the NoL1 Word Or-              input to similar degrees, and on average they regularised it
der condition. We expect the change in the input language to           less than participants in the Word Order condition in Exper-
mitigate the effect of L1 transfer and to increase the coexis-         iment 1. As in Model 1, we did not find significant inter-
tence of both harmonic patterns.                                       actions between NP Type and System (largest: β = 0.016,
Participants Twenty-eight native-English speakers (aged                SE = 0.015, p = .288) or between NP Type, System and Con-
between 18 and 35, mean = 24.8) were recruited via the                 dition (largest: β = −0.015, SE = 0.011, p = .168). These re-
University of Edinburgh’s Careers Service advertisement                sults suggest that participants regularised their input systems
database. Participants received £6. Only the data from 26 par-         across conditions and NP types, and that whilst participants
ticipants were fit for analysis as two participants either failed      in the Word Order condition regularised more than those in
to learn the noun lexicon or failed to learn the associations          the Morphology condition, participants in the Morphology
between phrases and pictures.                                          and the NoL1 Word Order conditions regularised their input
                                                                       to similar degrees. Excluding the Num Adj N variant in the
Results                                                                input language thus eliminated the difference between levels.
Entropy scores obtained in the NoL1 Word Order condition               In other words, participants do not regularise probabilistic un-
are shown in Figure 1 (coloured in orange). We ran a linear            conditioned variation in word order more than in morphology.
mixed effects model as in Experiment 1 to explore the effect
of condition on regularisation behaviour (dependent variable:             Figure 3 shows the overall proportions of the variants pro-
entropy), including the conditions in Experiment 1 plus NoL1           duced for two-Mod NPs in the NoL1 Word Order condition.
Word Order. The mixed-effects structure was the same as                We observe that the most produced word order is the major-
in the Model 1 but with reverse Helmert coding of Condi-               ity input variant N Adj Num, and that the harmonic N Num
                                                                   1027

Adj word order is overall more frequent than any other mi-           Culbertson, J., & Kirby, S. (2016). Simplicity and specificity
nority input variant. Unlike in the Word Order condition               in language: Domain-general biases have domain-specific
where systems with both Num Adj N and N Adj Num pat-                   effects. Frontiers in Psychology, 6, 1964.
terns were not common, 65% of participants produced sys-             Culbertson, J., Smolensky, P., & Legendre, G. (2012). Learn-
tems with both N Adj Num and N Num Adj harmonic vari-                  ing biases predict a word order universal. Cognition,
ants in the NoL1 Word Order condition. We ran a logistic               122(3), 306–329.
regression model to test the difference between the propor-          Fehér, O., Wonnacott, E., & Smith, K. (2016). Structural
tions of N Num Adj variants in input and output linguistic             priming in artificial languages and the regularisation of un-
systems across participants. We used the same mixed-effects            predictable variation. Journal of Memory and Language,
structure as in Model 2. Results suggest that the proportion           91, 158–180.
of N Num Adj variants in the output languages is not signifi-        Ferdinand, V., Kirby, S., & Smith, K. (2017). The cog-
cantly different from the input proportion across participants         nitive roots of regularization in language. arXiv preprint
(β = −0.594, SE = 0.546, p = .277).                                    arXiv:1703.03442.
                                                                     Good, J. (2015). Paradigmatic complexity in pidgins and
                          Discussion                                   creoles. Word Structure, 8(2), 184–227.
Our experimental results reveal regularisation behaviour in          Hudson Kam, C. L., & Newport, E. L. (2005). Regularizing
the production of complex systems of variation in morphol-             unpredictable variation: The roles of adult and child learn-
ogy and word order. They also suggest that regularisation              ers in language formation and change. Language learning
behaviour is of similar strength between these linguistic lev-         and development, 1(2), 151–195.
els given input languages with comparable initial complex-           Hudson Kam, C. L., & Newport, E. L. (2009). Getting it
ities. In Experiment 1 we found higher levels of regulari-             right by getting it wrong: When learners change languages.
sation in word order than in morphology, apparently due to             Cognitive psychology, 59(1), 30–66.
the specific properties of the set of variants in the input lan-     Kuznetsova, A., Bruun Brockhoff, P., & Haubo Bojesen
guages. When both harmonic pre-nominal and post-nominal                Christensen, R. (2015). lmertest: Tests in linear mixed ef-
two-modifier variants were included, the coexistence of both           fects models [Computer software manual]. Retrieved from
variants in a single production system was rare. Although a            http://CRAN.R-project.org/package=lmerTest
preference for harmonic order and consistent head position           R Core Team. (2015). R: A language and environment for
may have been at play, the interference of L1 transfer can-            statistical computing [Computer software manual]. Vienna,
not be categorically rejected. Indeed previous research sug-           Austria. Retrieved from https://www.R-project.org/
gests that L2 learners tend to access their L1 knowledge if it       Senghas, A., & Coppola, M. (2001). Children creating lan-
matches the novel input (Weber, Christiansen, Petersson, In-           guage: How nicaraguan sign language acquired a spatial
defrey, & Hagoort, 2016). In Experiment 2, we showed that              grammar. Psychological science, 12(4), 323–328.
eliminating opposite N-peripheral positions in the subset of         Siegel, J. (2004). Morphological simplicity in pidgins and
two-modifier variants by replacing Num Adj N with N Num                creoles. Journal of Pidgin and Creole languages, 19(1),
Adj eliminates the difference in regularisation between lev-           139–162.
els. Our results do not suggest general level-specific learn-        Slabakova, R. (2013). What is easy and what is hard to ac-
ing biases that could straightforwardly predict a typological          quire in a second language. Contemporary approaches to
asymmetry between the strength and speed of regularisation             second language acquisition, 9, 5.
in morphology and word order hinted at in pidgin and creole          St Clair, M. C., Monaghan, P., & Ramscar, M. (2009). Re-
studies (Good, 2015). Instead, they suggest that asymme-               lationships between language structure and language learn-
tries in regularisation processes in language formation ought          ing: The suffixing preference and grammatical categoriza-
to be sought in asymmetries in the input complexity of traits          tion. Cognitive Science, 33(7), 1317–1329.
across levels, also taking into account the overlap of features      van Trijp, R. (2013). Linguistic assessment criteria for ex-
between contributing languages.                                        plaining language change: A case study on syncretism in
                                                                       german definite articles. Language Dynamics and Change,
                         Conclusion                                    3(1), 105–132.
Our results suggest similar strengths of regularisation be-          Weber, K., Christiansen, M. H., Petersson, K. M., Indefrey, P.,
tween linguistic levels given input languages with compara-            & Hagoort, P. (2016). fmri syntactic and lexical repetition
ble initial complexities. Nevertheless, preferences for cer-           effects reveal the initial stages of learning a new language.
tain patterns within a linguistic level might in fact vary the         Journal of Neuroscience, 36(26), 6872–6880.
strength of regularisation behaviour within a given level.           Wonnacott, E., & Newport, E. L. (2005). Novelty and regu-
                                                                       larization: The effect of novel instances on rule formation.
                         References                                    In Bucld 29: Proceedings of the 29th annual boston univer-
Bates, D., Mächler, M., Bolker, B., & Walker, S. (2015).              sity conference on language development (pp. 663–673).
   Fitting linear mixed-effects models using lme4. Journal of
   Statistical Software, 67(1), 1–48.
                                                                 1028

