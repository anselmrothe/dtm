                  Why Does Higher Working Memory Capacity Help You Learn?
                                               Kevin Lloyd (klloyd@gatsby.ucl.ac.uk)
                                                 Gatsby Computational Neuroscience Unit
                                                25 Howland Street, London, W1T 4JG, UK
                                         Adam Sanborn (A.N.Sanborn@warwick.ac.uk)
                                            Department of Psychology, University of Warwick
                                                University Road, Coventry, CV4 7AL, UK
                                               David Leslie (d.leslie@lancaster.ac.uk)
                                     Department of Mathematics and Statistics, Lancaster University
                                                           Lancaster, LA1 4YF, UK
                                Stephan Lewandowsky (stephan.lewandowsky@bristol.ac.uk)
                             School of Experimental Psychology and Cabot Institute, University of Bristol
                                                            Clifton, BS8 1TU, UK
                               Abstract                                   shared across representations and processes in working mem-
   Algorithms for approximate Bayesian inference, such as                 ory (e.g., Just & Carpenter, 1992).
   Monte Carlo methods, provide one source of models of how                   In the current work, we consider WMC limits within the
   people may deal with uncertainty in spite of limited cognitive         context of Bayesian inference, asking whether WMC may be
   resources. Here, we model learning as a process of sequential
   sampling, or ‘particle filtering’, and suggest that an individ-        usefully modelled as a constraint on inferential resources. In
   ual’s working memory capacity (WMC) may be usefully mod-               particular, we model the learning process as one of particle
   elled in terms of the number of samples, or ‘particles’, that are      filtering, in which a series of probability distributions is rep-
   available for inference. The model qualitatively captures two
   distinct effects reported recently, namely that individuals with       resented by a limited set of samples (‘particles’) which are
   higher WMC are better able to (i) learn novel categories, and          sequentially updated over time (Griffiths et al., 2012). Higher
   (ii) flexibly switch between different categorization strategies.      WMC is then assumed to be implemented as a greater num-
   Keywords: Bayesian inference; particle filter; working mem-            ber of particles. This approach is applied to two recent exper-
   ory; category learning; knowledge restructuring
                                                                          iments which indicate positive effects of higher WMC on two
                           Introduction                                   distinct aspects of categorization: (i) the facility with which
Humans often behave in a manner consistent with Bayesian                  novel categories are learned (Lewandowsky, 2011); and (ii)
principles (Chater & Oaksford, 2008) yet how they achieve                 the ability to flexibly switch between different category rep-
this is unclear. Though simple in principle, exact Bayesian               resentations or response strategies, referred to as knowledge
calculations are frequently intractable in real-world settings,           restructuring (Sewell & Lewandowsky, 2012). We show that
leading to a need for approximations. In statistics and com-              both of these effects are qualitatively captured by a single
puter science, this challenge has been met through the de-                model in which WMC is equated with the number of parti-
velopment of powerful, general-purpose techniques for ap-                 cles available for inference — i.e., the number of hypotheses
proximate Bayesian inference, such as Monte Carlo meth-                   about category structure that an individual can concurrently
ods, which allow practical application of Bayesian methods                entertain.
in complex domains. The practical success of these tech-
                                                                          WMC and Category Learning
niques has naturally prompted an interest in whether people
deal with uncertainty in an analogous manner (Griffiths, Vul,             Lewandowsky (2011) measured participants’ WMC before
& Sanborn, 2012). Importantly, such algorithms can approx-                testing category learning performance on the six classical
imate probabilistic inference arbitrarily well when sufficient            problem types of Shepard, Hovland, and Jenkins (1961)
time and memory are available, thereby providing a bench-                 (henceforth ‘SHJ’). Each involves learning to assign a set
mark for ideal performance, but also display systematic de-               of stimuli to category A or B based on their values on bi-
viations from the normative solution when resources are lim-              nary dimensions, but the problem types vary in the number of
ited. These latter ‘qualitative fingerprints’ may be particu-             stimulus dimensions required to correctly perform classifica-
larly illuminating when considering human cognition, where                tion. Consistent with the classical results, participants gener-
constraints on information-processing capacity are typically              ally learned the Type I problem fastest, Type VI the slowest,
assumed. A salient example is provided by limits on work-                 and Types II-V at an intermediate rate. Crucially, WMC score
ing memory capacity (WMC; Cowan, 2001). While the exact                   was found to be positively correlated with category learning
nature of these limits remain the subject of debate, one promi-           performance: higher WMC individuals tended to make fewer
nent conception is that they reflect a limited resource which is          errors across all problem types.
                                                                      767

WMC and Knowledge Restructuring                                            A                                                 B                                                          A B
                                                                           Session 1                    “switch”
                                                                                                                                 rectangle height
Sewell and Lewandowsky (2012) assessed the relationship                               Train      Transfer         Transfer
between WMC and performance in a knowledge restructur-                              Strategy 1    Test 1           Test 2
ing (KR) task. Participants were guided to use one particular                                                                                         B
categorization strategy in a binary classification task before                        Train        Transfer       Transfer                            A
                                                                                    Strategy 2      Test 3         Test 4
being instructed to switch to an alternative, equally-effective            Session 2
                                                                                                        “switch”                                                      bar position
strategy (Fig 1A). The stimuli, rectangles of varying height
with a vertical bar located at different locations along their
                                                                                       context 1              context 2
base, belonged to category A or B depending on their position              C                                                         D                       1
                                                                                                                                                                         CI-first condition
in category space (Fig 1B). Crucially, training stimuli (filled
                                                                                                                                      Context Sensitivity
                                                                                                                                                                         KP-first condition
                                                                               CI
circles) were clustered into two separate regions of category
                                                                                                                                                            0.5
space (as indicated by different colours), with categories ar-
ranged so that partial category boundaries (solid lines) could
not be integrated in a coherent manner; neither partial bound-                 KP
                                                                                                                                                             0
ary could be extended so as to allow accurate classification                                                                                                      1      2          3         4
of all stimuli in the other cluster. A third, binary ‘context’                                                                                                         Transfer Test
dimension was systematically mapped onto the two training
                                                                           Figure 1: (A) Knowledge restructuring (KR) task design. (B)
clusters so that stimuli belonging to distinct clusters appeared
                                                                           Experimental stimuli, depicted in category space: position of
in different colours (see example stimuli, lower Fig 1B).
                                                                           a vertically-oriented bar (x-axis) vs. height of rectangle (y-
   At the task outset, participants were given information de-             axis). Filled circles denote training stimuli; open squares de-
signed to guide them towards using one of two different                    note test stimuli; solid lines indicate the partial rule bound-
strategies for co-ordinating partial categorization rules: (1)             aries. Two example stimuli are shown underneath. (C) ‘Ideal’
a knowledge partitioning (KP) strategy was encouraged by                   predicted response profiles given exclusive use of a context-
imparting that the context variable (colour) could be used to              insensitive (CI; top row) or knowledge-partitioning (KP; bot-
determine which dimension to use (rectangle height or bar po-              tom row) strategy during test. Darker shading indicates a
sition) for categorization; (2) a context-insensitive (CI) strat-          higher probability of classifying as category A. (D) Aver-
egy was instead encouraged by highlighting that bar position               age context-sensitivity (CS) scores across participants during
could be used to determine which partial boundary to apply                 transfer tests, indicating use of CI (low) or KP (high) strategy.
(i.e., regardless of context). Both strategies could support per-          Figures B–D adapted from Sewell and Lewandowsky (2012).
fect performance but predicted different patterns of general-
ization when applied to new stimuli (open squares, Fig 1B)
in a transfer test, thereby revealing which strategy was in use            Category Representation
(Fig 1C). A summary ‘context sensitivity’ (CS) measure was
applied to participants’ test patterns to quantify the degree to           A number of representational formats for categories have
which they generalized in a manner consistent with the KP                  been discussed in the literature. Here, we opted to use classi-
(high CS) or CI (low CS) strategy (Fig 1D).                                fication and regression tree (CART) models (Breiman, Fried-
   Critically, Sewell and Lewandowsky found evidence that                  man, Olshen, & Stone, 1984). Firstly, these are well-suited
individuals with higher WMC were more adept at switch-                     to cases in which categories are readily described in terms
ing between these different categorization strategies when in-             of simple rules, particularly if an ordering on these rules
structed to do so, as measured by how much their CS scores                 is suggested (as in the KR task). Secondly, the classifica-
changed between tests. This was interpreted in terms of                    tion boundaries generated by CART models lead naturally to
greater ‘knowledge restructuring’, i.e., ability to coordinate             ‘axis-aligned’ generalization patterns like those observed in
different category representations or response requirements.               the KR task (participants’ response profiles were very similar
                                                                           to those shown in Fig 1C), whereas producing this behaviour
                  Modelling Approach                                       is non-trivial for other category models.
                                                                              Briefly, CART models provide a flexible method for spec-
Our model comprises three parts: 1) assumptions about how                  ifying the conditional distribution of a binary category la-
participants represent categories, specified in terms of an ex-            bel y given a p-dimensional stimulus feature vector x =
plicit generative process; 2) a procedure by which participants            (x1 , x2 , . . . , x p ). In the KR task, for a given stimulus on
are assumed to infer categories in light of prior assumptions              trial t, we have yt ∈ {A, B} and a 3-dimensional input xt =
and experimental stimuli; and 3) a means for translating par-              (xt,1 = bar positiont ∈ R, xt,2 = heightt ∈ R, xt,3 = contextt ∈
ticipants’ beliefs into choice (i.e., a predicted category label).         {0, 1}). The models work by recursively partitioning the
Our description focuses on how the modelling approach is                   input space into axis-aligned cuboids (similar to the partial
applied to the KR task; the SHJ tasks are simpler and easily               boundaries in Fig 1B) and applying a simple conditional
modelled with only minor modifications.                                    model to each region (e.g., probability that category label =
                                                                     768

A). The sequence of partitions can be represented as a binary                        assume that the probability of splitting on each dimension is
tree (Fig 2).                                                                        equal,
    Formally, a binary tree structure T consists of a hierar-                                           p(κ(η) = j) = 1/p,            j = 1, . . . , p,          (3)
chy of nodes η ∈ T. Nodes with children are internal nodes,
while nodes without children are leaf nodes (Fig 2A). Each                           and that split location is then drawn uniformly from the
node is associated with a block B(η) ⊆ R p of the input space                        node’s range,
as follows: the root node is associated with the entire input                                                                          η,−
                                                                                                         τ(η)|κ(η) = j ∼ U (R j , R j ).
                                                                                                                                             η,+
                                                                                                                                                                 (4)
space, while each further internal node splits its block into
two halves by selecting a single dimension κ(η) = {1, . . . , p}                     However, in the KR task, participants were guided towards
and location τ(η) on which to split (Fig 2B). The block of                           a particular strategy by being told in the first instance that
input space associated with a node η is determined by the                            stimulus colour (KP-first condition) or bar position (CI-first
ranges on each dimension j which it covers, and we denote                            condition) reliably indicated whether height or bar position
                                     η       η,− η,+
the corresponding range R j = [R j , R j ]. We call the tuple                        was diagnostic of stimulus category. To incorporate this ad-
T = (T, κ, τ) the decision tree.                                                     ditional information, we assume a bias term b ≤ 1 which as-
                                                                                     signs higher probability to splitting the root node η0 on the
 A                                           B                   Β(η)                dimension j∗ highlighted by instruction:
                          η
           Internal node                                                                                                (
                                                                                                                           b       if κ(η0 ) = j∗ ,
                                                                                                        p(κ(η0 )) = 1−b                                          (5)
                                         Dimension 2      B(η L )          B(ηR)                                             2     otherwise.
             ηL                      ηR                                              The generative model is completed by the conditional
Leaf nodes
                                                          Dimension 1 τ(η)
                                                                                     probabilities of stimulus labels given the tree structure,
                                                                                     p(y1:t |x1:t , T ). We assume that the kth leaf node has an as-
Figure 2: (A) Simple binary tree with (internal) root node                           sociated probability θk of generating label A,
η which splits into two ‘leaf’ nodes, ηL and ηR . (B) Cor-                                                                     y
                                                                                                           p(yt |θk , xt ) = θkt (1 − θk )1−yt ,                 (6)
responding split of a two-dimensional input space. The root
node η is associated with the full input space, B(η). Here,                          and that this probability is an i.i.d. draw from a Beta distri-
                                                                                                   iid
node η is split on dimension 1, κ(η) = 1, at a location τ(η).                        bution, θk ∼ Beta(a0 , b0 ). Standard analytical simplification
This splits the input space into two blocks, B(ηL ) and B(ηR ),                      then yields the marginal likelihood
associated with the leaf nodes ηL and ηR .                                                                
                                                                                                             Γ(a0 + b0 )
                                                                                                                            K   K   Γ(ntkA + a0 )Γ(ntk· − ntkA + b0 )
                                                                                     p(y1:t |T , x1:t ) =                       ∏                                      ,
                                                                                                            Γ(a0 )Γ(b0 )       k=1          Γ(ntk· + a0 + b0 )
In addition to a decision tree T with K leaf nodes, a parameter                                                                                                   (7)
Θ = (θ1 , θ2 , . . . , θK ) associates parameter value θk with the
kth leaf node. If a stimulus x lies in the region of the kth leaf                    where ntkA and ntk· are respectively the number of instances of
node, then y|x has distribution f (y|θk ) for some parametric                        category A and the total number of data points in the partition
family f . It is typically assumed that, conditional on (Θ, T ), y                   of leaf k up to trial t. Note that for a given tree, this likeli-
values within a leaf node are i.i.d. and that y values across leaf                   hood is higher for leaves assigned observations with homoge-
nodes are independent. Thus, letting nk denote the number                            nous labels, and these are exactly the partitions that constitute
of observations assigned to the kth leaf node and letting yk,i                       ‘good’ solutions to the categorization problem.
denote the ith observation of y assigned to leaf k,                                  Inference
                                             K    nk
                   p(y1:n |x1:n , Θ, T ) = ∏ ∏ f (yk,i |θk ),                (1)     Participants are assumed to approximate the sequence of pos-
                                                                                     terior distributions {p(T |x1:t , y1:t )}t=1     T over trials. Given the
                                            k=1 i=1
                                                                                     implausibility of enumerating all possible trees, participants
where n = ∑Kk=1 nk is the total number of observations.
                                                                                     are assumed to represent a relatively small number of sam-
    Prior beliefs about category structure can be formalized as
                                                                                     ples, i.e. hypotheses, from these posterior distributions which
a prior distribution on decision trees, specified via a stochastic
                                                                                     can be updated over time. In other words, we assume partici-
generative process. Following Chipman, George, and McCul-
                                                                                     pants perform particle filtering.
loch (1998), we set the prior probability of a node η in tree
structure T being split into children nodes to                                          Two aspects of the inference process which we now de-
                                                                                     scribe draw parallels with working memory. Firstly, simi-
                                                   α
                         pSPLIT (η, T) =                ,                    (2)     lar to the idea of a limit on the number of items that can be
                                             (1 + dη )β                              held in working memory (Cowan, 2001), we assume there is
where dη denotes the depth of the node, and α < 1 and β ≥                            a bounded number of hypotheses about category structure —
0 are parameters controlling expected tree size. Under this                          in this case, the particles which correspond to specific tree
specification, the probability pSPLIT is a decreasing function                       structures — that can be entertained at a given time. Sec-
of node depth, and decreases more steeply for large β.                               ondly, similar to the notion that working memory is active
    In addition to this prior on tree structure T, we generally                      (Baddeley, 1992), involving manipulation rather than merely
                                                                                 769

passive storage of items, we assume that inference involves                           and after t training sessions has in mind the set of weighted
                                                                                                       (l)
a continual process whereby local transformations to current                          trees {T (l) , wt }Ll=1 approximating the target distribution un-
hypotheses are proposed, and which may be accepted or re-                             der the prior appropriate to the CI strategy. We denote this
jected. The latter process promotes diversity in the hypothesis                       target distribution pCI (T |x1:t , y1:t ). The experimenter then
set and continuous exploration of the hypothesis space.                               instructs the participant to change to using the KP strategy.
   In detail, we assume that on trial t, a participant’s beliefs                      Assuming that the set of trees remains fixed, the associated
are represented by a small set of L possible trees {T (l) }Ll=1                       tree weights now need to be changed to reflect the new tar-
                                                      (l)                             get distribution pKP (T |x1:t , y1:t ). This can be achieved by
with associated importance weights {wt }Ll=1 . This set of
trees constitutes the limited set of hypotheses putatively                            an importance weighting step, treating pCI (T |x1:t , y1:t ) as the
maintained in a working memory of capacity L. With the ob-                            importance distribution. In particular, denoting a particle’s
                                                                                                                                                                   (l)−
servation of the stimulus and category label on the next trial                        weight before and after the instruction to switch as wt                            and
t + 1, a proper reweighting of the lth tree is given by the fol-                        (l)+
                                                                                      wt , respectively, the relevant reweighting is
lowing update (Chopin, 2002):
                                                                                                          (l)+        (l)−     pKP (T (l) |x1:t , y1:t )
                     (l)     (l)                                                                       wt       ∝ wt                                      .               (9)
                   wt+1  ∝ wt p(yt+1 |T (l) , xt+1 , y1:t ).                  (8)                                               pCI (T (l) |x1:t , y1:t )
As standard within particle filtering methods, this reweight-                         To switch in the reverse direction — from the KP to CI
ing process is alternated with a resampling stage in which                            strategy — the appropriate reweighting instead uses the ra-
very unlikely trees, i.e., those with very low weights, are dis-                      tio pCI (T (l) |x1:t , y1:t )/pKP (T (l) |x1:t , y1:t ).
carded and replaced by replicates of more probable trees. A
simple way of doing this is to sample L times with replace-                           Choice
ment from the set {T (l) } with probabilities proportional to                         Participants are assumed to predict category labels based on
                              (l)                                                     their current hypotheses. Assuming a newly-resampled par-
the updated weights {wt+1 }Ll=1 (Gordon, Salmond, & Smith,
1993). Following this resampling step, all particle weights                           ticle set with equal weights 1/L, a sample-based approxima-
are equalized to 1/L.                                                                 tion to the predictive probability that a stimulus xt+1 has label
   Additionally, this resampled particle set can then be re-                          yt+1 = A is given by
juvinated (Chopin, 2002), reintroducing diversity and allow-                                                                 1 L
ing continuous exploration of alternative solutions. This is the                       p(yt+1 = A|x1:t+1 , y1:t ) ≈              ∑ p(yt+1 = A|x1:t+1 , y1:t , T (l) )
                                                                                                                             L l=1
‘active’ step which, we suggest, recalls conceptions of work-
ing memory as involving active manipulation of currently-                                                                    1 L
stored items. Specifically, we may, without altering the tar-
                                                                                                                        =        ∑ Eθk |x1:t+1 ,y1:t ,T (l) [θk ].
                                                                                                                             L l=1
                                                                                                                                                                        (10)
geted posterior distribution, propose transformations of trees
                                                                                      Thus, an approximation to the predictive probability is given
from a Markov chain transition kernel qt+1 (·|T (l) ) with ap-
                                                                                      by an unweighted average of posterior means for θk , where k
propriate stationary distribution p(T |x1:t+1 , y1:t+1 ). Closely
                                                                                      for the lth particle is the index of the leaf node relevant to the
following the transition kernel suggested by Chipman et al.
                                                                                      input xt+1 in T (l) . In our case, the posterior mean is
(1998), we consider the scheme where for each tree {T (l) },
a new tree T (l)∗ is proposed by randomly choosing among                                                                                   ntkA + a0
                                                                                                      Eθk |x1:t+1 ,y1:t ,T (l) [θk ] =                      .           (11)
3 possible transformations: (1) grow: randomly select a leaf                                                                           ntk· + a0 + b0
node, then draw a splitting dimension and location from the
prior; (2) prune: randomly select an internal node, then turn it
                                                                                                                          Results
into a leaf node by deleting all nodes below it; or (3) change:                       Rate of Learning
randomly select an internal node, then reassign it a splitting                        Lewandowsky (2011) found that WMC was positively corre-
dimension and location by a draw from the prior. The pro-                             lated with category learning performance. We hypothesized
posed tree T (l)∗ is then accepted with probability                                   that a greater number of particles, i.e. increasing L, would
                                                                                      have a similar effect since, on average, one might expect the
                          (                                                   )
                            p(T (l)∗ |x1:t+1 , y1:t+1 )/qt+1 (T (l)∗ |T (l) )
 α(T (l) , T (l)∗ ) = min                                                       ,     search for a ‘good’ (i.e., more probable) category structure to
                            p(T (l) |x1:t+1 , y1:t+1 )/qt+1 (T (l) |T (l)∗ )
                                                                                      progress faster, and with less chance of getting stuck in local
as per the standard Metropolis-Hastings algorithm.                                    maxima, with a higher number of particles.
   We also need to model the effect of an instruction to switch                           Figure 3 displays average simulated learning curves for the
categorization strategy. We assume that the effect is to change                       SHJ tasks when the number of particles is increased from 1
the prior distribution over trees, which is then combined with                        (Fig 3A) to 100 (Fig 3B). Though the effect is subtle, there is
past observations to produce an updated posterior distribu-                           a general steepening of learning curves and a downward shift
tion. This update can be implemented via a simple reweight-                           in initial error rate for problem Type I. A more systematic
ing operation on the set of trees.                                                    gauge of the effect is obtained by fitting exponential func-
   To see how this works, consider the specific example where                         tions to such learning curves and comparing the size of the
a participant has initially been guided to use the CI strategy                        fitted coefficients as the number of particles is increased (a
                                                                                  770

      A                                                                                         B                                                                                  A                    1
                                                                                                                                                                                                                     CI-first                                                 1
                                                                                                                                                                                                                                                                                        KP-first
                                       1 particle                                                                                100 particles                Type I                                                                                        1 particle
                                                                                                                                                              Type II
                                                                                                                                                                                                      0.9                                                   5 particles     0.9
                     0.6                                                                                      0.6
                                                                                                                                                                               Context Sensitivity
                                                                                                                                                              Type III                                0.8                                                   10 particles    0.8
  Proportion Error                                                                         Proportion Error
                                                                                                                                                              Type IV                                 0.7                                                   20 particles    0.7
                                                                                                                                                              Type V                                                                                        40 particles
                                                                                                                                                                                                      0.6                                                                   0.6
                                                                                                                                                              Type VI                                                                                       80 particles
                     0.4                                                                                      0.4                                                                                     0.5                                                                   0.5
                                                                                                                                                                                                      0.4                                                                   0.4
                                                                                                                                                                                                      0.3                                                                   0.3
                     0.2                                                                                      0.2                                                                                     0.2                                                                   0.2
                                                                                                                                                                                                      0.1                                                                   0.1
                                                                                                                                                                                                        0                                                                     0
                                                                                                                                                                                                     -0.1                                                                  -0.1
                      0                                                                                        0                                                                                            1    2              3               4                                 1    2           3   4
                           1   2   3    4           5        6       7   8   9    10 11 12                          1   2    3     4    5    6   7   8   9   10 11 12
                                                                                                                                                                                                                Transfer Test                                                         Transfer Test
                                                              Block                                                                          Block
                                       C                    0.42                                                                                                                                                          B                1
                                                                                                                                                                                                                                          0.9
                                                                                                                                                                                                                                          0.8
                                                             0.4
                                            Learning Rate
                                                                                                                                                                                                                                          0.7
                                                                                                                                                                                                                              P(switch)
                                                                                                                                                                                                                                          0.6
                                                            0.38                                                                                                                                                                          0.5                          0    0.5   1
                                                                                                                                                                                                                                                                           ∆CS
                                                                                                                                                                                                                                          0.4
                                                            0.36                                                                                                                                                                          0.3
                                                                                                                                                                                                                                          0.2
                                                                                                                                                                                                                                                      0    0.5     1
                                                                                                                                                                                                                                          0.1             ∆CS
                                                            0.34
                                                                                                                                                                                                                                           0
                                                                                                                                                                                                                                            1   5   10 20 30 40 50 60 70 80 90 100
                                                                 0           20       40                       60           80         100                                                                                                                 Particles
                                                                                  Number of Particles
Figure 3: Increasing the number of particles leads to faster                                                                                                                    Figure 4: (A) In both the context-sensitive (CI)-first (left) and
category learning. Simulated learning curves for (A) 1 parti-                                                                                                                   knowledge-partitioning (KP)-first (right) conditions, increas-
cle, and (B) 100 particles. Learning curves are averages over                                                                                                                   ing the number of particles L leads to a greater change in
100 simulations with other model parameters fixed (a0 = b0 =                                                                                                                    context sensitivity (CS) score on average when prompted to
1; α = 0.95, β = 1). (C) Learning rate as a function of number                                                                                                                  change strategy (1500 simulation runs per condition). (B)
of particles. For each setting, the model is run 100 times and                                                                                                                  This is due to an increased probability P(switch) of a suc-
exponential curves fit to each individual learning curve. The                                                                                                                   cessful switch (∆CS > 0.5). Lower inset: with fewer particles
resulting coefficients are averaged over both simulation runs                                                                                                                   (L = 20), it will frequently occur that the model completely
and problem types to yield an aggregate ‘learning rate’.                                                                                                                        fails to switch (∆CS = 0). Upper inset: with more particles
                                                                                                                                                                                (L = 100), such failures are unlikely (3000 simulation runs;
                                                                                                                                                                                b = 0.9, a0 = b0 = 1, α = 0.95, β = 1).
larger coefficient indicates a steeper learning curve). Figure
3C shows that the learning rate does increase with more par-
ticles, though the effect is small beyond ≈ 20 particles.                                                                                                                       tribution (Eq. (9)). However, the success of this will depend
   Note that even without fitting the model parameters, the                                                                                                                     on how well the particle set covers the support of the updated
basic SHJ pattern of results — Type I easiest, Type VI hard-                                                                                                                    distribution. With a sufficiently large number of particles, at
est, and Types II-V clustered in between — is reproduced.                                                                                                                       least some should be allocated to (previously) lower probabil-
Briefly, this results from the preference for simpler, or more                                                                                                                  ity regions; if the new strategy corresponds to such a region,
parsimonious, hypotheses that arises naturally within the                                                                                                                       then appropriate reweighting can be applied. However, with
Bayesian framework. An advantage for the Type II problem                                                                                                                        a decreasing number of particles, representation of the pos-
relative to types III-V is not produced by the model here, but                                                                                                                  terior distribution may be so impoverished that such regions
we note that any such advantage was extremely marginal in                                                                                                                       of low probability may not contain any particles at all, and so
Lewandowsky (2011), and that the effect may arise only un-                                                                                                                      switching is not immediately possible.
der specific conditions (cf. Kurtz, Levering, Stanton, Romero,
& Morris, 2013).                                                                                                                                                                                                                                Discussion
                                                                                                                                                                                Experiments suggest that higher WMC benefits learning of
Knowledge Restructuring                                                                                                                                                         novel categories (Lewandowsky, 2011) and the ability to co-
Sewell and Lewandowsky (2012) found a positive association                                                                                                                      ordinate different category representations or response strate-
between WMC and knowledge restructuring. In the model,                                                                                                                          gies (Sewell & Lewandowsky, 2012). We framed such tasks
increasing the number of particles also has a beneficial effect                                                                                                                 in terms of inference, where individuals seek to infer the most
on the average degree of knowledge restructuring (Fig 4A),                                                                                                                      probable category structure(s) given their prior assumptions
with an increased probability of being able to successfully                                                                                                                     and experimental observations/instructions. Further, we as-
switch strategy (Fig 4B).                                                                                                                                                       sumed that individuals approximate inference by represent-
   This result arises from an enhanced ability to accurately                                                                                                                    ing and manipulating in working memory a relatively small
represent the posterior distribution with a greater number of                                                                                                                   number of hypotheses — samples, or ‘particles’ — about
particles. Recall that strategy-switching was modelled by a                                                                                                                     possible category structures. Our principal hypothesis was
change in posterior distribution, driven by the different priors                                                                                                                that by linking WMC with the number of such particles, we
underlying the distinct strategies; a simple way to track this                                                                                                                  would observe similarly positive effects of higher WMC on
change was by reweighting particles according to the new dis-                                                                                                                   performance. Simulation results were consistent with this hy-
                                                                                                                                                                         771

pothesis: more particles in the model enhanced both category                                        References
learning performance and the ability to switch between dif-            Baddeley, A. (1992). Working memory. Science, 255, 556-559.
ferent categorization strategies.                                      Bramley, N., Dayan, P., Griffiths, T., & Lagnado, D. (2017). For-
                                                                               malizing Neurath’s ship: Approximate algorithms for online
   These effects respectively arise due to increased search ef-                causal learning. Psychological Review, 124(3), 301–338.
ficiency and what we might call ‘representational adequacy’.           Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classi-
Conceptualized in terms of search for more probable cate-                      fication and regression trees. Belmont, CA: Wadsworth.
                                                                       Chater, N., & Oaksford, M. (Eds.). (2008). The probabilistic mind:
gories, the more resources (i.e., particles) available to search               Prospects for Bayesian cognitive science. Oxford University
this space — i.e., the greater the number of hypotheses that                   Press.
one can entertain and manipulate within working memory                 Chipman, H., George, E., & McCulloch, R. (1998). Bayesian CART
                                                                               model search. Journal of the American Statistical Associa-
— then the more likely it is that one will quickly discover                    tion, 93(443), 935-948.
good solutions, a process which draws natural parallels with           Chopin, N. (2002). A sequential particle filter method for static
the broader topic of problem-solving (Hambrick & Engle,                        models. Biometrika, 89(3), 539–552.
                                                                       Cowan, N. (2001). The magical number 4 in short-term memory: A
2003; Newell & Simon, 1972). Furthermore, a greater num-                       reconsideration of mental storage capacity. Behavioral and
ber of particles generally means that the posterior distribution               Brain Sciences, 24(1), 87–114.
over categories is more accurately represented — including             Dougherty, M., Thomas, R., & Lange, N. (2010). Toward an integra-
                                                                               tive theory of hypothesis generation, probability judgment,
those assigned lower probability — and this pluralism means                    and hypothesis testing. In B. Ross (Ed.), The psychology of
that the model can more easily express alternative hypothe-                    learning and motivation (Vol. 52, pp. 299–342). Burlington:
ses when instructed to switch strategy, as operationalized by                  Academic Press.
                                                                       Gordon, N., Salmond, D., & Smith, A. (1993). Novel approach
a reweighting of particles. This source of flexibility may also                to nonlinear/non-Gaussian Bayesian state estimation. Radar
be relevant to so-called ‘insight’ problem-solving (Murray &                   and Signal Processing, IEE Proceedings F, 140(2), 107–113.
Byrne, 2005; Ohlsson, 1992).                                           Griffiths, T., Vul, E., & Sanborn, A. (2012). Bridging levels of anal-
                                                                               ysis for probabilistic models of cognition. Current Directions
   The current work is preceded by a number of related lines                   in Psychological Science, 21(4), 263-268.
of research. The HyGene model (Dougherty, Thomas, &                    Hambrick, D., & Engle, R. (2003). The role of working memory in
                                                                               problem solving. In J. Davidson & R. Sternberg (Eds.), The
Lange, 2010; Thomas, Dougherty, Sprenger, & Harbison,                          psychology of problem solving (pp. 176–206). Cambridge
2008), which emphasizes the importance of hypothesis gen-                      University Press.
eration and testing, includes the assumption that the number           Just, M., & Carpenter, P. (1992). A capacity theory of comprehen-
                                                                               sion: Individual differences in working memory. Psycholog-
of hypotheses that can be entertained at a given time is lim-                  ical Review, 99, 122–149.
ited by working memory constraints. Similarly, in their study          Kurtz, K., Levering, K., Stanton, R., Romero, J., & Morris, S.
of ‘garden path’ effects in sentence processing, Levy, Reali,                  (2013). Human learning of elemental category structures:
                                                                               revising the classic result of Shepard, Hovland, and Jenk-
and Griffiths (2008) suggested that difficulties in parsing such               ins (1961). Journal of Experimental Psychology: Learning,
sentences correctly may be explained by constraints on the                     Memory, and Cognition, 39(2), 552–572.
resources (i.e., number of particles) available for incremental        Levy, R., Reali, F., & Griffiths, T. (2008). Modeling the effects of
                                                                               memory on human online sentence processing with particle
parsing; their demonstration that a decreasing number of par-                  filters. In D. Koller, D. Schuurmans, Y. Bengio, & L. Bottou
ticles increases the probability of parse failure is exactly anal-             (Eds.), Advances in Neural Information Processing Systems
ogous to the mechanism suggested here in relation strategy-                    21.
                                                                       Lewandowsky, S. (2011). Working memory capacity and catego-
switching.                                                                     rization: Individual differences and modeling. Journal of Ex-
   There are a number of avenues for future investigation.                     perimental Psychology: Learning, Memory, and Cognition,
We have focused on qualitative effects here, but fitting the                   37(3), 720-738.
                                                                       Murray, M. A., & Byrne, R. M. (2005). Attention and working
model to individual participants will be necessary for a more                  memory in insight problem solving. In B. Bara, L. Barsa-
quantitative assessment; the obvious prediction is that high-                  lou, & M. Bucciarelli (Eds.), Proceedings of the xxvii annual
WMC individuals should tend to be fit best by a larger num-                    conference of the cognitive science society (pp. 1571–1575).
                                                                               Lawrence Erlbaum Associates.
ber of particles. Decomposing the relative contributions of            Newell, A., & Simon, H. (1972). Human problem solving. Engle-
particular features of the model, such as resampling, should                   wood Cliffs, NJ: Prentice-Hall.
also be explored, and quality of fit directly compared with            Ohlsson, S. (1992). Information processing explanations of insight
                                                                               and related phenomena. In M. Keane & K. Gilhooly (Eds.),
‘single-particle’ approaches (e.g., Bramley, Dayan, Griffiths,                 Advances in the psychology of thinking. London: Harvester-
& Lagnado, 2017). How the approach fares in domains be-                        Wheatsheaf.
yond category learning is also of clear interest. More gener-          Sewell, D., & Lewandowsky, S. (2012). Attention and Working
                                                                               Memory Capacity: Insights From Blocking, Highlighting,
ally, Monte Carlo methods provide a rich source of ideas for                   and Knowledge Restructuring. Journal of Experimental Psy-
psychological models — exploring how such methods may                          chology: General, 141(3), 444-469.
succeed or fail to illuminate aspects of human cognition is a          Shepard, R. N., Hovland, C. I., & Jenkins, H. M. (1961). Learning
                                                                               and memorization of classifications. Psychological Mono-
substantial task for future research.                                          graphs: General and Applied, 75(13), 1–42.
                                                                       Thomas, R., Dougherty, M., Sprenger, A., & Harbison, J. (2008).
                    Acknowledgments                                            Diagnostic hypothesis generation and human judgment. Psy-
                                                                               chological Review, 115(1), 155–185.
This work was supported by the Gatsby Charitable Founda-
tion (KL), and by EPSRC grant EP/I032622/1 (DL).
                                                                   772

