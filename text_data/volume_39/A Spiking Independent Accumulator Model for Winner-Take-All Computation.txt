   A Spiking Independent Accumulator Model for Winner-Take-All Computation
                                              Jan Gosmann (jgosmann@uwaterloo.ca)
                                            Aaron R. Voelker (arvoelke@uwaterloo.ca)
                                            Chris Eliasmith (celiasmith@uwaterloo.ca)
                                      Centre for Theoretical Neuroscience, University of Waterloo
                                                      Waterloo, ON, Canada N2L 3G1
                               Abstract                                   To implement each model, we use the Neural Engineering
                                                                       Framework (NEF; Eliasmith & Anderson, 2003) to map the
   Winner-take-all (WTA) mechanisms are an important compo-
   nent of many cognitive models. For example, they are often          model’s dynamics onto populations of spiking neurons. In
   used to decide between multiple choices or to selectively di-       the remainder of the paper, we provide a short introduction to
   rect attention. Here we compare two biologically plausible,         the NEF, describe our implementation of the two WTA mech-
   spiking neural WTA mechanisms. We first provide a novel
   spiking implementation of the well-known leaky, competing           anisms, present our benchmarks, and finally discuss some re-
   accumulator (LCA) model, by mapping the dynamics onto a             sulting implications for cognitive modelling.
   population-level representation. We then propose a two-layer
   spiking independent accumulator (IA) model, and compare its
   performance against the LCA network on a variety of WTA                                          Methods
   benchmarks. Our findings suggest that while the LCA net-
   work can rapidly adapt to new winners, the IA network is bet-       The Neural Engineering Framework
   ter suited for stable decision making in the presence of noise.
                                                                       The Neural Engineering Framework (NEF; Eliasmith & An-
   Keywords: Neural Engineering Framework; Nengo; winner-              derson, 2003) is a method for mapping a cognitive model, de-
   take-all; decision making; mutual inhibition; neural competi-
   tion; dynamical systems                                             scribed using mathematical equations, onto a spiking neural
                                                                       network. We now describe the aspects of this framework that
                           Introduction                                are relevant to this work, by summarizing its three principles:
Winner-take-all (WTA) networks are mechanisms that se-                 representation, transformation, and dynamics.
lect the largest value among a number of inputs. More pre-             Principle 1: Representation We define the representation
cisely, given a D-dimensional vector corresponding to the              of a scalar value x(t) by an encoding and decoding with re-
non-negative utility of D different choices, the desired out-          spect to some population of neurons. The encoding of x(t)
put is positive for the dimension with highest utility (i.e., the      into a spike train ai (t) for neuron i is given by:
“winner”) and zero for all others. This mechanism is regu-                                             h                 i
larly employed as a component of cognitive models involving                              ai (t) = Gi αi ei x(t) + Jibias ,             (1)
selective attention (e.g., Itti, Koch, & Niebur, 1998; Standage,
Trappenberg, & Klein, 2005) and decision making, where                 where αi is a gain factor, ei is an encoder that determines a
the action with the highest utility is selected to drive be-           neuron’s tuning curve, Jibias a bias current, and Gi [·] is the
haviour (e.g., O’Reilly, 1998).                                        neural nonlinearity. Here, we use spiking, leaky integrate-
   A large body of literature examines the optimality of WTA           and-fire (LIF) neurons for Gi [·], and set the encoders to one.
mechanisms and their consistency with neurobiological and              Decoding weights di are then used to approximate the repre-
psychological data (e.g., Bogacz, Brown, Moehlis, Holmes,              sented value x̂(t) from the activity of the population of neu-
& Cohen, 2006; Gold & Shadlen, 2007; Smith & Ratcliff,                 rons by:
2004). Here, we investigate the suitability of two different
                                                                                            x̂(t) = ∑ di (ai ∗ h)(t) ,
                                                                                                                      
                                                                                                                                       (2)
WTA mechanisms in the context of neurally plausible cogni-                                           i
tive modelling. In particular, we map each mechanism onto
a network of spiking neurons, and then compare them using              where h(t) = τ−1 s exp(−t/τs ) is an exponentially decaying
a set of functional benchmarks that are normative in nature.           synaptic filter with time-constant τs , and ∗ is the convolution
The first mechanism we consider is an implementation of the            operator. The decoding weights are obtained by least-squares
leaky, competing accumulator (LCA) model from Usher and                optimization of the error Ex = |x − x̂|. For the transmission
McClelland (2001). The LCA model and variants have been                of a value from one population to another, the connection
widely used, for example in versions of the Temporal Context           weights are given by:
Model (Sederberg, Howard, & Kahana, 2008), and in work
on the Remote Associates Test models (e.g., Kajic, Gosmann,                                        Wi j = α j e j di .                 (3)
Stewart, Wennekers, & Eliasmith, 2017). The second mecha-
nism we consider is the independent accumulator (IA) model             Principle 2: Transformation By finding alternate decod-
                                                                                      f
that we propose here, which involves a secondary threshold-            ing weights di with the error given by E f (x) = f (x) − x̂ , arbi-
ing layer that is recurrently connected to a primary integrating       trary linear and nonlinear functions f (x) can be approximated
layer.                                                                 in the connections between neural populations.
                                                                   2125

Principle 3: Dynamics Given some desired nonlinear dy-                                                                                 (a) LCA
                                                                                                    1.0
namics for the state variable x(t):                                                                 0.8
                                                                          Decoded output
                           ∂x                                                                       0.6
                              = g(x),                         (4)                                   0.4
                           ∂t
                                                                                                    0.2
we can map these dynamics onto a recurrent transformation,                                          0.0
by harnessing the synaptic filter mentioned in Principle 1. In
                                                                                                          0.0           0.2          0.4                                 0.6         0.8       1.0
particular, for the exponentially decaying h(t) we may apply                                                                            Time [s]
Principle 2 to the recurrent transformation f (x) = τs g(x) + x
to ensure that x(t) obeys Equation 4.                                                                                                (b) IA
                                                                                                                                        Decoded output (layer 2)
                                                                                                    1.5                                                            1.5
                                                                          Decoded state (layer 1)
Leaky, competing accumulator model
Using the principles of the NEF, we have implemented the                                            1.0                                                            1.0
widely-used leaky, competing accumulator (LCA) model pro-
posed by Usher and McClelland (2001). The dynamics (see                                             0.5                                                            0.5
Fig. 1a) for each state variable xi (t), 1 ≤ i ≤ D, where D is
                                                                                                    0.0                                                            0.0
the number of choices, are given by:
                                                                                                          0.0 0.1 0.2 0.3 0.4                                             0.0 0.1 0.2 0.3 0.4
                                                                                                                   Time [s]                                                       Time [s]
           ∂xi                           1
               = ρi − kxi − β ∑ x j  , xi ≥ 0,            (5)
           ∂t                   j6=i      τ                           Figure 1: Example time course of the state variables in
                                                                      the LCA (top) and IA (bottom) networks with three choices
where ρi is each external input, k is the leak rate, β the lateral    (D = 3). The vector of inputs is [0.8, 0.7, 0.6].
inhibition, and τ the integration time-constant. This model
essentially integrates each input ρi with a leak term (−kxi ),
                                                                         The first layer consists of a separate integrator population
minus competition from every other variable (β ∑ j6=i x j ). Sup-
                                                                      (i.e., accumulator) for each state variable xi (t), 1 ≤ i ≤ D.
posing ρi > ρ j for all j 6= i, a WTA mechanism should indi-
                                                                      The second layer consists of independent, non-recurrent pop-
cate that i is the winning choice. Setting k = β = 1 will guar-
                                                                      ulations that receive input from the first layer in a one-to-
antee that the winning state xi converges to the value of the
                                                                      one fashion. From the second layer, we decode the func-
largest input ρi , while each losing state x j ( j 6= i) converges
                                                                      tion x̄i := Θ(xi − ϑ) where Θ is the Heaviside function, and
to zero. Other choices of k merely alter the effective τ and
                                                                      ϑ = 0.8 is a fixed threshold value that determines how much
the effective gain on the input, while other choices of β will
                                                                      evidence needs to be accumulated to produce an output. The
produce unwanted behaviours (see supplementary analysis).
                                                                      Heaviside decoded output of layer 2 projects back to layer 1
   We implement Equation 5 with the NEF by using one pop-
                                                                      to add x̄i − β̄ ∑ j6=i x̄ j to the input of xi . Since intuitively the
ulation of neurons for each xi , and applying Principle 3 to
                                                                      largest input will accumulate the fastest, once this reaches the
each population. By appropriately selecting the gain and bias
                                                                      threshold ϑ it will self-excite and inhibit all other state vari-
parameters from Principle 1, we ensure that each state vari-
                                                                      ables. Fixing β̄ = 2 ensures that the losing state variables will
able is rectified (xi ≥ 0) as required. We believe this imple-
mentation is novel as it does not interpret each xi as a distinct
neural firing rate, but rather as a population-level represen-                                             Inputs Layer 1                                           Layer 2
tation distributed across any number of spiking neurons. In
effect, heterogeneous spike trains are weighted by optimal                                                      ρ1              x1                                         x̄1
decoding weights to precisely implement the stated dynam-
ics. This allows us to attain greater biological realism without
                                                                                                                ρ2              x2                                         x̄2
altering the dynamics prescribed by Equation 5.
                                                                                                                                ..                                             ..
Independent accumulator model                                                                                                    .                                              .
The other WTA mechanism that we investigate is our pro-
posed independent accumulator (IA) model (see Fig. 2). We                                                       ρD              xD                                        x̄D
use the term ‘independent’ to refer to the fact that there are
no direct interactions between each accumulator, unlike in the
LCA model which has direct competition between states. To             Figure 2: Independent accumulator (IA) network. Neural
enable a form of competition, we add a second thresholding            populations are denoted by circles labelled with their repre-
layer that projects back to self-excite and mutually-inhibit the      sented state variable. Arrows denote excitatory connections,
first layer. We now provide the details of each layer – again         while lines ending in circles denote inhibitory connections.
implemented using the principles of the NEF.                          The second layer computes x̄i := Θ(xi − ϑ).
                                                                   2126

go to zero (see supplementary analysis). This is summarized
                                                                                 Table 1: Summary of parameter values.
more precisely by the following dynamics (see Fig. 1b):
                                                                       LCA time-constant                             τ = 0.1 s
                                                                     LCA recurrency parameters                     k=β=1
           ∂xi      1                     1                                                                         τ1 = 0.1 s, 0.5 s
               = ρi + x̄i − β̄ ∑ x̄ j  , xi ≥ 0.             (6)      IA accumulation time-constant
           ∂t       τ1            j6=i    τ 2                          IA feedback time-constant                     τ2 = 0.1 s
                                                                       IA threshold                                  ϑ = 0.8
Notably, this takes the form of Equation 5 after substituting          IA recurrency parameters                      β̄ = 2
τ = τ1 , k = −τ1 /τ2 , and β = β̄τ1 /τ2 , with the only remain-        Recurrent synaptic time-constant              τs = 0.1 s
ing difference being the Heaviside nonlinearity applied to the         Feed-forward synaptic time-constant           τs = 0.005 s
state feedback. Thus, in contrast to the continual competition         Output decoding synaptic time-constant τs = 0.01 s
occurring in the LCA model, the threshold ϑ is a free param-
eter that controls how much evidence needs to be integrated
before enabling any competition between states. Instead of           some models it is more desirable to produce a clear incorrect
directly manipulating ϑ, we opt to change τ1 , which has a           decision than an unstable incorrect decision. In other situa-
comparable effect due to the leak-free integration.                  tions, though, the correctness of the decision may be of higher
   Note that the decoded output from layer 2 of the IA net-          importance. Thus, we consider a trial ‘correct’ if the model
work has higher variance than the LCA network (Fig. 1), but          forms a clear decision, and the largest output corresponds to
the separation of the output for different choices is more rel-      the true largest input. Measurement of correct trials forms our
evant than the variance in interpreting the output.                  second benchmark.
                                                                        We use two additional benchmarks on the set of all trials
Benchmarks                                                           with a clear decision. First, it is important to consider the
To test and compare the two WTA mechanisms we provide                speed at which the network can make decisions. We there-
an input of ρi = u − s(1 − δ1i ) + ηi , where u is the magnitude     fore define the ‘decision time’ as the length of time it takes to
of the largest input, s > 0 is the target separation relative to     fulfil the conditions of a clear decision. Second, the correct-
all other inputs, δ is the Kronecker delta, and ηi is Gaussian       ness metric only considers the final averaged output during a
(white) noise with standard deviation σ. Thus, without loss          time interval. It is possible for a network to produce transient
of generality, the first state variable receives the largest in-     outputs before the final decision is reached. In the context of
put u plus noise, and all other state variables receive a noisy      a larger model, this can become a problem because the tran-
input that is smaller by s. It is important to note that u not       sient output might be prematurely interpreted as a decision.
only determines the size of the largest input, but also the gen-     Thus, we define it as the ‘highest output of a losing choice’
eral baseline of inputs. Since all of the runner-ups have equal      during the whole simulation.
magnitude, this represents the most difficult scenario for the
networks, where all potential choices must be considered. As                                      Results
s → 0 the problem also becomes more difficult because the            We find that the ability to form a clear decision of the LCA
utilities of the choices are closer together. We use u = 1 un-       network decreases with more noise and less target separation
less indicated otherwise, and set the number of choices to           (see Fig. 3). Also, the magnitude of the inputs has an im-
D = 10. Furthermore, we increment the noise variance to              portant influence. For a small inputs with u = 0.2 the winner
highlight successes and failures as the task becomes increas-        will mostly not exceed the 0.15 threshold with noise present.
ingly difficult with more noise. This allows us to determine         The best performance is achieved with medium inputs with
which functions are performed robustly by each network. In           u = 0.6. Higher inputs make it more likely that runner-ups
both WTA models we use 200 neurons per choice. In the IA             will exceed the 0.15 threshold, especially with a small target
network this is split into 150 neurons for each layer 1 popula-      separation. In contrast to the LCA network, the IA network
tion and 50 neurons for each layer 2 population. All remain-         manages to form a clear decision in every trial (not explicitly
ing network parameters are summarized in Table 1.                    shown in Figure 3, but all data points fall on the grey line).
   To evaluate the two mechanisms on the previously defined             Interestingly, for all clear decisions the correct winner was
input, we use a number of separate metrics. First, we deter-         determined by the LCA network. Thus, a plot of correct tri-
mine whether the model is able to form a clear decision within       als looks identical to Figure 3, with slightly different error
one second. To be counted as ‘clear’, at least one output must       bars. While always reaching a clear decision, the decisions of
remain above 0.15 across the time interval (1 s, 2 s] while all      the IA network are not always correct. Overall, the IA perfor-
other outputs remain below this threshold. This lower bound          mance tends to be worse than the LCA performance for a high
of 0.15 was chosen to ensure that noise on a zero output is          input magnitude, but better for smaller inputs (Fig. 4a, b). We
not mistaken for a non-zero output. Note that this metric re-        can greatly improve the IA performance by increasing τ1 to
quires that the decision does not change throughout the time         0.5 s which slows down the integration of evidence (Fig. 4c).
interval. This does not take into account whether the winning        This improves the IA performance to be above the LCA per-
output actually corresponds to the largest input. However, for       formance for high baselines, but it will also increase the de-
                                                                 2127

                                                         (a) LCA, u = 0. 2                      (b) LCA, u = 0. 6                       (c) LCA, u = 1. 0
    Fraction of trials with clear decision
                                             1.0                                    1.0                                   1.0
                                             0.8                    Separation s    0.8                                   0.8
                                             0.6                             0.05   0.6                                   0.6
                                                                             0.1
                                             0.4                             0.15   0.4                                   0.4
                                                                             0.2
                                             0.2                                    0.2                                   0.2
                                             0.0                                    0.0                                   0.0
                                                   0.0 0.01 0.02 0.03 0.04 0.05           0.0 0.01 0.02 0.03 0.04 0.05           0.0 0.01 0.02 0.03 0.04 0.05
                                                     Noise standard deviation σ             Noise standard deviation σ              Noise standard deviation σ
Figure 3: Fraction of trials with a clear decision for the LCA network (which also exactly matches the fraction of correct trials).
Each plot shows data for a different input magnitude u. Error bars denote bootstrapped 95% confidence intervals. The grey
horizontal line indicates the optimum, which coincides with the performance of the IA network.
cisions times of the IA network. For a low baseline with                                              ally arrive at a decision and produce a clear output. This is a
u = 0.2, it makes the IA network unable to decide within the                                          direct consequence of the thresholding on the state feedback,
allocated time frame, but given more time it would still reach                                        which prevents competition from occurring until a sufficient
a decision.                                                                                           amount of evidence has accumulated. This also enables the
   As shown in Fig. 5a, the time required to reach a decision                                         IA network to react to small inputs. In addition, the IA net-
in the LCA network depends mostly on the size of inputs and                                           work is easily extendible to allow dynamic control of the de-
target separation. We averaged over the different noise condi-                                        cision speed, by supplying an external bias to layer 2 to adjust
tions because the noise influence on the timing was minor. In                                         the ϑ threshold.
the IA network the largest input is the most important factor                                            The LCA network is especially well-suited for situations
(dashed vs. solid lines in Fig. 5b). Depending on this magni-                                         where a decision needs to be continuously adjusted. The dy-
tude, the network can either be faster or slower than the LCA                                         namics constantly push the winning state variable to the mag-
network, but it will need more time given a value of τ1 that                                          nitude of the largest input, while adapting to input changes
achieves the same fraction of correct responses as the LCA                                            along a time-scale of τ. This makes it quick to respond to
network. There is also a slight influence of target separation                                        changes in the input for smaller τ, but leads to randomly
and input noise, with an interaction of these two parameters                                          switching outputs due to noise.
(solid lines in Fig. 5).                                                                                 In contrast, the IA network is better suited for situations
   Finally, looking at the transient responses indicates that                                         where a discrete sequence of decisions is required. After se-
both models might produce outputs of losing choices. For the                                          lecting a winner, the model’s decision will persist due to self-
LCA network the magnitude of the transient response mainly                                            excitation, even in the absence of input. Thus, after making
increases with the amount of noise (Fig. 6a). For the IA net-                                         a decision, it is necessary to reset the model by inhibiting the
work, transient outputs are smaller in noisy conditions, but                                          winning accumulator. This limits how quickly successive de-
can be higher than for the LCA network in less noisy condi-                                           cisions can be made and reduces the ability to react to chang-
tions with small target separations. The magnitude of such                                            ing inputs. However, once a decision is made, the network
transient responses is reduced by adjusting τ1 to 0.5, at the                                         provides a stable output. For example, we intend to use the
cost of a slower decision.                                                                            IA network in a model of free recall to output a sequence of
                                                                                                      WTA responses. In this case, the model requires stable recall
                                                           Discussion                                 in the presence of noise, even if each response may not be the
We have shown that neither network performs better on all                                             “true” winner.
benchmarks, but rather each is better suited for different pur-                                          Both models might produce a transient response that may
poses. For instance, the LCA network can determine the cor-                                           be interpreted as a decision, which can make the detection
rect winner more quickly, and, given that a conclusive deci-                                          of decisions problematic. This is of special relevance when
sion was made, it always selects the correct winner. However,                                         incorporating the networks into larger cognitive models. For
under noisy conditions it may fail to produce a clear output at                                       the LCA network, the transient behaviour is inherent to its
all, and thus not make a decision. This can be problematic for                                        design; there will always be an initial rise of all state vari-
cognitive models that must form a clear decision (even when                                           ables (that receive input) before the mutual inhibition grows
it may be incorrect). The IA network might not be able to                                             strong enough to push them back to zero. In the IA network,
identify the correct winner as quickly or as reliably (depend-                                        however, such transient responses may be avoided by choos-
ing on the choice of τ1 ), but given enough time it will eventu-                                      ing appropriate τ1 and ϑ. It should be noted that other recur-
                                                                                                 2128

                                                                    (a) IA, u = 0. 2, τ1 = 0. 1                       (b) IA, u = 1, τ1 = 0. 1                         (c) IA, u = 1, τ1 = 0. 5
                                                        1.0                                       1.0                                                  1.0
                           Fraction of correct trials
                                                        0.8                                       0.8                                                  0.8
                                                        0.6                                       0.6                                                  0.6
                                                                                                                                                                Separation s
                                                        0.4                                       0.4                                                  0.4             0.05
                                                                                                                                                                       0.1
                                                        0.2                                       0.2                                                  0.2             0.15
                                                                                                                                                                       0.2
                                                        0.0                                       0.0                                                  0.0
                                                              0.0 0.01 0.02 0.03 0.04 0.05                    0.0 0.01 0.02 0.03 0.04 0.05                      0.0 0.01 0.02 0.03 0.04 0.05
                                                                    Noise standard deviation σ                   Noise standard deviation σ                        Noise standard deviation σ
       Figure 4: Fraction of correct trials for the IA network. Each plot shows data for a different combination of input magnitude u
       and integration time-constant τ1 . Error bars denote bootstrapped 95% confidence intervals. The grey horizontal line indicates
       the optimum.
                                                                                   (a) LCA                                                       (b) IA, u = 0.2 (solid) and u = 1 (dashed)
                                                                                                        Separation s
                         0.4
Mean decision time [s]
                                                                                                               0.05            0.4
                                                                                                               0.1
                         0.3                                                                                   0.15            0.3
                                                                                                               0.2
                         0.2                                                                                                   0.2
                         0.1                                                                                                   0.1                                                                u = 0.2
                                                                                                                                                                                                  u=1
                                                              0.2                     0.6               1.0                               0.0       0.01        0.02           0.03     0.04       0.05
                                                                              Input magnitude u                                                            Noise standard deviation
       Figure 5: (Left) Mean decision times for the LCA network. Shown data is averaged across all noise levels, since noise had
       minimal effect on decision times. (Right) Mean decision times for the IA network with input magnitude u = 0.2 (solid) and
       u = 1 (dashed). Error bars denote bootstrapped 95% confidence intervals.
       rently connected readout mechanisms have been proposed for                                                              of choices increases: for the LCA network decisions become
       the two choice case that produce bursting outputs somewhat                                                              slower due to the mutual interaction, but for the IA network
       comparable to the IA network (Lo & Wang, 2006). However,                                                                decisions will take less time because only a single accumula-
       the evidence integration in the Lo and Wang (2006) model                                                                tor needs to exceed the threshold. Nevertheless, evidence for
       uses competition via pooled inhibition, making it more simi-                                                            one network does not exclude the possibility that the other
       lar to the LCA then the IA network.                                                                                     network is employed for different tasks or by different brain
          We have not yet investigated the agreement of these mech-                                                            areas. Relatedly, it might be more plausible to distribute the
       anisms with neurobiological and behavioural data, although                                                              representation of all state variables over a single population of
       this has been done before for other WTA networks (Gold &                                                                neurons. This is directly supported by the NEF, but we chose
       Shadlen, 2007; Smith & Ratcliff, 2004). The implementation                                                              to leave this to future work to keep the current analysis free
       in spiking neurons, however, provides some basic biological                                                             from potential interactions of the state variables introduced
       plausibility and more readily permits comparisons with neu-                                                             by such a distributed representation.
       ral data. In particular, the IA network predicts that the firing                                                           We also did not look at at the influence of the number of di-
       rates for neurons in the first layer will rise up to a threshold,                                                       mensions D in detail. For higher D, we see reduced accuracy
       and that neurons in the second layer will not become active                                                             overall since each additional choice has a baseline chance to
       or inhibit the first layer until reaching this threshold. Neurons                                                       win due to noise. Nevertheless, the results that we discuss
       in the macaque lateral intraparietal area exhibit a similar step                                                        here are qualitatively similar.
       response (Latimer, Yates, Meister, Huk, & Pillow, 2015). In                                                                One critique of non-leaky accumulator models is that their
       contrast, the LCA network predicts that the firing of any neu-                                                          ability to discriminate the largest input increases indefinitely
       rons will proportionately inhibit all other neurons that do not                                                         with time (Usher & McClelland, 2001) and that there is no
       represent the same state. With regard to behavioural data, dif-                                                         sensible stopping criterion. However, this assumes that the
       ferent effects for decision times are predicted as the number                                                           time to reach a decision has no cost. If time-to-decision has a
                                                                                                                         2129

                                                (a) LCA                       (b) IA, τ1 = 0. 1                      (c) IA, τ1 = 0. 5
                           1.0   Separation s                    1.0                                   1.0
                                        0.05
      Transient response
                           0.8          0.1                      0.8                                   0.8
                                        0.15
                           0.6          0.2                      0.6                                   0.6
                           0.4                                   0.4                                   0.4
                           0.2                                   0.2                                   0.2
                           0.0                                   0.0                                   0.0
                                 0.0 0.01 0.02 0.03 0.04 0.05          0.0 0.01 0.02 0.03 0.04 0.05           0.0 0.01 0.02 0.03 0.04 0.05
                                    Noise standard deviation σ           Noise standard deviation σ             Noise standard deviation σ
Figure 6: Transient response (highest output of losing choices) for the LCA and IA network. Error bars denote bootstrapped
95% confidence intervals. The grey horizontal lines show the optimum.
cost, then it will at some point exceed the gain achieved from                      Gold, J. I., & Shadlen, M. N. (2007). The neural basis of
making a correct decision. Furthermore, this argument as-                              decision making. Annual Review of Neuroscience, 30(1),
sumes integration with perfect accuracy. But, with networks                            535–574. doi: 10.1146/annurev.neuro.29.051605.113038
built using the NEF, the representation of each state variable                      Itti, L., Koch, C., & Niebur, E. (1998, November). A model
has limited precision, and so an ideal trade-off must be found.                        of saliency-based visual attention for rapid scene analysis.
   To conclude, we investigated two spiking neural networks                            IEEE Transactions on Pattern Analysis and Machine Intel-
computing a winner-take-all function based on the leaky,                               ligence, 20(11), 1254–1259. doi: 10.1109/34.730558
competing accumulator model and a novel two-layer inde-                             Kajic, I., Gosmann, J., Stewart, T. C., Wennekers, T., & Elia-
pendent accumulator model. While both perform the same                                 smith, C. (2017). A spiking neuron model of word asso-
basic tasks, they fail in different ways as each task scales                           ciations for the Remote Associates Test. Frontiers in Psy-
in difficulty via increased noise or less separation between                           chology, 8(99). doi: 10.3389/fpsyg.2017.00099
choices. From a modelling perspective, this makes each net-                         Latimer, K. W., Yates, J. L., Meister, M. L. R., Huk, A. C.,
work more useful for different situations. The LCA model                               & Pillow, J. W. (2015, July). Single-trial spike trains
is better for continuous updating of decisions, whereas the                            in parietal cortex reveal discrete steps during decision-
IA network is better suited for more discrete decisions in                             making. Science, 349(6244), 184–187. doi: 10.1126/sci-
the presence of noise. It is left to future work to investigate                        ence.aaa4056
whether these two distinct mechanisms can be identified from                        Lo, C.-C., & Wang, X.-J. (2006, July). Cortico–basal gan-
either behavioural or neurophysiological data.                                         glia circuit mechanism for a decision threshold in reac-
                                                                                       tion time tasks. Nature Neuroscience, 9(7), 956–963. doi:
                                                Notes                                  10.1038/nn1722
Source code and supplementary analysis are available at                             O’Reilly, R. C. (1998, November). Six principles for bi-
https://github.com/ctn-waterloo/cogsci17-decide.                                       ologically based computational models of cortical cogni-
                                                                                       tion. Trends in Cognitive Sciences, 2(11), 455–462. doi:
                                    Acknowledgments                                    10.1016/S1364-6613(98)01241-8
                                                                                    Sederberg, P. B., Howard, M. W., & Kahana, M. J. (2008).
This work was supported by the Canada Research Chairs pro-                             A context-based theory of recency and contiguity in free
gram, the NSERC Discovery grant 261453, Air Force Office                               recall. Psychological Review, 115(4), 893–912. doi:
of Scientific Research grant FA8655-13-1-3084, CFI, OIT,                               10.1037/a0013396
and NSERC CGS-D.                                                                    Smith, P. L., & Ratcliff, R. (2004, March). Psychology and
                                                                                       neurobiology of simple decisions. Trends in Neurosciences,
                                          References                                   27(3), 161–168. doi: 10.1016/j.tins.2004.01.006
Bogacz, R., Brown, E., Moehlis, J., Holmes, P., & Cohen,                            Standage, D. I., Trappenberg, T. P., & Klein, R. M. (2005,
  J. D. (2006, October). The physics of optimal decision                               July). Modelling divided visual attention with a winner-
  making: A formal analysis of models of performance in                                take-all network. Neural Networks, 18(5–6), 620–627. doi:
  two-alternative forced-choice tasks. Psychological Review,                           10.1016/j.neunet.2005.06.015
  113(4), 700–765. doi: 10.1037/0033-295X.113.4.700                                 Usher, M., & McClelland, J. L. (2001). The time course
Eliasmith, C., & Anderson, C. H. (2003). Neural engineer-                              of perceptual choice: The leaky, competing accumulator
  ing: Computation, representation, and dynamics in neuro-                             model. Psychological Review, 108(3), 550–592. doi:
  biological systems. Cambridge, MA: MIT Press.                                        10.1037/0033-295X.108.3.550
                                                                              2130

