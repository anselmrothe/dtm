                                Object Representation in Multiattribute Choice
                                            Sudeep Bhatia (bhatiasu@sas.upenn.edu)
                                              University of Pennsylvania, Philadelphia, PA.
                                           Neil Stewart (neil.stewart@warwick.ac.uk)
                                                  University of Warwick, Coventry, UK.
                             Abstract                                 and resulting behaviors depend greatly on the ways in which
                                                                      attributes and objects are presented (e.g. Kleinmuntz &
   We propose a theoretical framework for understanding how
   everyday choice objects are represented and how decisions          Schkade, 1993) suggesting that real-world decisions, which
   involving these objects are made. Our framework combines           seldom involve actual attribute-by-object matrices, may be
   insights regarding object and concept representation in            different to the types of decisions observed in current
   semantic memory research with multiattribute choice rules          experimental work. More importantly, by using artificial
   proposed by scholars of decision making. We also outline           designs in which the attributes of objects are directly
   computational techniques for using our framework to                presented to decision makers, existing theoretical work has
   quantitatively predict naturalistic multiattribute choices. We
   test our approach in two-object and three-object forced choice     largely ignored the role of object representation. Storing,
   experiments involving common books, movies, and foods.             retrieving, and processing attribute information about the
   Despite using complex naturalistic stimuli, we find that our       objects in a given choice problem is a pivotal part of the
   approach achieves high predictive accuracy rates, and is also      decision process, and a complete account of choice requires
   able to provide a good account of decision time distributions.     an approach that is able to specify the mechanisms involved
   Keywords: Multiattribute choice, Semantic memory,                  at this stage in the decision, well as the relationship between
   Naturalistic decision making, Judgment and decision making         these mechanisms and the final outcomes of the decision
                                                                      (see Bhatia, 2013 for a discussion).
                         Introduction                                    This paper provides a theoretical framework capable of
Most choices that people make on a day-to-day basis, from             addressing these issues. It relies on insights in semantic
the books they read to the foods they eat, involve trading off        memory research which suggest that low-dimensional
attributes, so as to select the object whose attributes are           attribute spaces are used to represent objects and concepts.
overall the most desirable (Keeney & Raiffa, 1993). There             For example, multi-dimensional scaling (Shepard, 1962)
is, however, a disconnect between the way in which                    passes similarity ratings through a matrix decomposition
multiattribute choices are currently studied, and the way in          algorithm, resulting in the recovery of a small number of
which these day-to-day choices are typically made. Most               latent attributes that best describe the structure of similarity
multiattribute choice experiments explicitly present choice           for a given domain. Likewise, distributional models of
objects and their attributes to participants in a matrix of           semantic memory typically learn low-dimensional word
numerical quantities (e.g. Figure 1a). Everyday decisions, in         representations through natural language. Some approaches,
contrast, are not usually composed of objects with a small            like latent semantic analysis, use singular value
set of explicitly presented and quantified attributes. Rather         decomposition to perform dimensionality reduction on
the objects in these decisions are much richer and complex            word-context occurrence matrices (Landauer & Dumais,
(e.g. Figure 1b). Decision makers do have knowledge about             1997). Others use Bayesian statistics or convolution based
these objects and their attributes, but this knowledge is             associative memory, but also result in low-dimensional
represented in the decision makers’ minds after having been           representations for words (see Jones et al., 2015).
learnt through prior experience with the choice domain.                  We suggest that these insights extend to everyday
                                                                      multiattribute choice, so that decision makers can be seen as
                                                                      using the distribution of observable features across choice
                                                                      objects in the environment to uncover low-dimensional
                                                                      latent attributes for representing the objects. Furthermore,
                                                                      we propose that it is these latent attributes that are evaluated
                                                                      and aggregated during the decision process. For simplicity
                                                                      we suggest that the recovery of latent attributes can be
                                                                      approximated using singular value decomposition on the
Figure 1a and b. Stimuli presentation in standard multiattribute      observable feature space (as in e.g. Landauer & Dumais,
choice experiments (left) and in Study 1 (right).                     1997), and that the evaluation of the latent attributes can be
                                                                      approximated with a linear model with decision weights for
   The divergence between the stylized stimuli used in                each latent attribute (as in e.g. Keeney & Raiffa, 1993).
current research and the complex multiattribute choices                  We also propose computational techniques for uncovering
made in real-world settings is problematic. Choice processes          the latent attribute representations of common choice
                                                                  1635

objects. Particularly, keywords, tags, and other natural                        In this paper, we use three large online datasets:
language descriptors for choice objects on internet websites,                www.GoodReads.com, which contains user-generated
can be considered suitable proxies for the observable                        bookshelves for thousands of books; www.IMDB.com,
features of these objects. For a sufficiently rich online                    which contains user-generated keywords for thousands of
dataset, it is possible to train semantic models and learn the               popular movies; and www.AllRecipes.com which contains
latent attribute representations for the objects in a choice                 user-specified ingredients for thousands of dishes. We
environment, and subsequently examine peoples’ choices                       scrapped these websites in 2014, and for each website we
between these objects.                                                       attempted to obtain as much information (as many objects
                                                                             and associated features) as was technically feasible. We
                         Framework                                           obtained a total of 372,186 unique shelves for 15,737 books
   Let us consider a choice domain with N total objects.                     for the www.GoodReads.com dataset, a total of 160,322
Each of these objects has a set of observable features, and                  unique keywords for 44,971 movies for the
can be written as a vector of these features. If there are M                 www.IMDB.com dataset, and a total of 24,688 unique
total unique features in the environment, then each for                      ingredients for 39,979 recipes for the www.AllRecipes.com
object i we have xi = (xi1, xi2, … xiM), with xij = 1 or xij = 0             dataset. Using these user-generated descriptors as our
based on whether or not feature j is present in object i.                    observable features, each of the N objects in each of the
Singular value decomposition involves processing the                         three datasets can be written as an M-dimensional feature
matrix X = [x1, x2, … xN] to obtain L << M latent attributes,                vector xi = (xi1, xi2, … xiM), with xij = 1 if object i (a book, a
corresponding to the L largest singular values of X. Using                   movie, or a food dish) has observable feature j (a keyword, a
these singular values, we can represent an object i as zi =                  shelf, or an ingredient). A singular value decomposition on
(zi1, zi2, … ziL), with zij corresponding to the association                 X = [x1, x2, … xN] can be subsequently performed to obtain L
between the object and the jth latent attribute. Note that M                 << M latent attributes for the datasets.
can be very large in many naturalistic choice domains,
whereas L is typically much smaller.                                                                     Study 1
   The use of latent attributes for representing objects                        In Study 1 we tested whether our theoretical framework
implies that our approach retains the multiattribute structure               and the computational techniques for applying this
assumed by theoretical decision making research. Thus we                     framework,        actually     predict     peoples’      everyday
can take common multiattribute decision rules and apply                      multiattribute choices. This is the primary experiment in this
them very easily to latent attributes. We use a simple linear                paper: It involves incentivized choices in the laboratory with
rule, which specifies a decision weight for each attribute and               reaction time measures. In later studies we examine variants
aggregates weighted attributes into a measure of utility for                 of this design using non-incentivized online samples.
an object (Keeney & Raiffa, 1993). The object with the                          Method. In this study, 73 participant made binary choices
higher utility is the one that is most frequently chosen. In                 between pairs of popular books. Participants were recruited
the context of the latent attribute structure outlined here, this            from a university subject pool, and performed the study in a
involves specifying an L dimensional vector of weights w =                   behavioral laboratory on computer screens. Participants
(w1, w2, … wL), and multiplying the latent attributes for an                 were also incentivized, and one of their chosen books was
object i by these weights, so as to obtain the utility for the               selected at random and given to them at the end of the study.
object Ui = w ∙ zi. In order to permit random noise in the                      Unlike most existing multiattribute choice experiments,
choice process we embed our utilities in the logit choice rule               the choice objects were not presented alongside a set of
(Luce, 1959). In a two-object choice this specifies the                      quantifiable attributes (as in e.g. Figure 1a). Rather they
probability of choosing an object i over another object i' as                were shown to participants using just the covers of the
Pr[i chosen] = eUi)/(eUi + eUi’) = ew ∙ zi/(ew ∙ zi + ew ∙ zi’). For the     books and the accompanying titles (as in e.g. Figure 1b).
general case with N’ choice objects we have Pr[i chosen] =                   Overall, each of the 73 participants made 220 choices
eUi/(Σn = 1 .. N’ eUn).                                                      involving 150 unique books. The books used in this study
   In order to test our approach and illustrate its applicability            were obtained from 30 different popular genres on
we first need to uncover the actual attribute representations                www.GoodReads.com.
that characterize common choice objects. In related                             Model Fitting. We fit participant choices using the latent
domains, such representations are usually obtained by                        attributes recovered from a singular value decomposition
asking experimental participants to generate features that                   (SVD) on the www.GoodReads.com data. We allowed the
describe the meaning of a given word (e.g. McRae et al.,                     number of underlying latent attributes, L, to vary across
2005). However common choice domains are so vast                             participants. For a given value of L, we used the L latent
(involving thousands of features for thousands of objects)                   attributes with the highest singular values from the SVD on
that the experimental elicitation of these feature norms may                 the www.GoodReads.com dataset. In order to ensure
not practical. Thus we suggest that user-generated                           sufficient degrees of freedom for estimating decision
keywords, tags, and other descriptors for common choice                      weights, we restricted L to a maximum of L = 100 (and a
objects on online datasets can be seen capturing the                         minimum of L = 2). In essence this leads to a total of 99
observable features that best describe the various objects.                  unique models for each participant, corresponding to L = 2,
                                                                         1636

L = 3, … L = 100. with a separate set of best fitting
participant-level attribute weights for each model. The
values of the 150 books in our study on the two latent
attributes with the largest singular values are shown in
Figure 2. Figure 2 also shows the ten shelves with the
largest absolute weights for these two latent attributes.
Figure 2. The values of the 150 books in Study 1 on the two latent
attributes with the largest singular values, alongside the ten shelves
with the largest absolute weights for these two latent attributes.
   In order to avoid overfitting, we used ten-fold cross-
validation to test predictive accuracy and find the best
performing model (i.e. best performing value of L) for
describing each participant’s choices. For model training,
we recovered the weighting vector w that provided the best
fit to the training data, with the assumption of a linear                  Table 1. Summary of model fits. Mean”, “Std. Dev.” and
choice rule embedded in a logistic link function. This vector              “Median” indicate the distribution of best-fitting model accuracy
(whose dimensionality depended on the dimensionality of                    rates on test data across participants. “Best Fit” describes the
the model (value of L) in consideration), was recovered                    proportion of participants for which the model has the highest
using maximum likelihood estimation. For model testing we                  accuracy (these proportions sum to greater than one as models are
calculated the proportion of choices in the test data                      sometimes tied) and “Significant” indicates the proportion of
predicted accurately by the recovered w for each model. A                  participants that outperform the baseline model with p < 0.05.
choice is considered to be predicted accurately if the utility
assigned to the chosen option by the model in consideration                   One possibility is that our technique achieves its high
is higher than the utility assigned to its competitor.                     accuracy rates by allowing flexible weights across a large
Ultimately, the value of L and corresponding weight vector                 number of dimensions. In order to control for this, we
w with the highest accuracy on the test data was considered                attempted the above model-fits with randomly generated
to be the overall best fitting model.                                      attribute vectors. Particularly, for each participant and each
   Results. The mean accuracy of our approach for                          object offered to the participant, we artificially created a
predicting the test data is 83% (SD = 0.08), significantly                 100-dimensional vector with each dimension randomly and
above a baseline accuracy of 50% (p < 0.01). Additionally,                 uniformly distributed in the range [0,1]. We then performed
the average best fitting value of L across our participants is             a 10-fold cross validation procedure that examined the fits
39.67 (SD = 27.95. Table 1 summarizes statistics regarding                 of linear models with flexible weights for L dimensions of
model accuracy.                                                            the random vectors. With this approach we found the mean
                                                                           accuracy to be 69% (SD = 0.08) Additionally, 84% of
                                                                       1637

participants achieved a higher accuracy rate using the                option in the trial for the participant, based on the best
recovered latent attributes from www.GoodReads.com,                   fitting model for the participant. Likewise, UabR is the
compared to the randomly generated vectors (and 8% of                 predicted utility for the right option. β1 is a multiplier
participants had equal accuracy with both approaches). A              mapping this utility difference on to a drift rate, and β0 is an
participant-level paired t-test indicates shows that this             intercept term capturing an absolute bias in drift for the left
difference is significant (p < 0.01). Table 1 provides further        option. In this model, hitting the upper boundary leads to the
statistics involving the random vectors approach.                     left option being selected, whereas hitting the lower
   Another alternative to our SVD-based attributes involves           boundary leads to the right option being selected.
the use of the raw observable features for the books. Of                 We fit this modified drift diffusion model permitting trial-
course it is impossible to actually recover separate decision         to-trial variability in starting points and trial-to-trial
weights for each of these observable features. However, we            variability drift rates. For this purpose, we adopted a
can use well-known decision heuristics applied to these               hierarchical model fitting approach, as implemented by the
observable features. For example, using the lexicographic             HDDM toolbox (Wiecki et al., 2013). This approach
heuristic (Tversky, 1969) would involve considering only a            recovers group mean parameters for the decision threshold,
single feature, and choosing the object that is the most              non-decision time, drift rates, trial-to-trial variability in
desirable on this feature. Likewise, applying the tallying            starting points, trial-to-trial variability, and trial-to-trial
heuristic (Russo & Dosher, 1983) would involve counting               variability drift rates, while also permitting individual
up the positive and negative features of each choice object,          differences in these parameters. Importantly this toolbox
and choosing the object with highest number of positive               makes it easy to fit linear functions for drift rates as we wish
features relative to negative features. We applied these two          to do in this paper. The best fitting group mean parameters
heuristics to participant-level choice using 10-fold cross            from our specification, as recovered by the diffusion
validation. For the lexicographic heuristic we used the               analysis, are presented in Table 2. Again β1 represents the
training sample to determine which of the object’s features           weight on utility difference in the drift term. As can be seen,
has the highest absolute correlation with choice. We then             the bulk of the distribution of this parameter lies above 0,
used this single feature to predict the choices on our test           indicating that the best fitting utility difference has a strong
sample. For the tallying heuristic, we used the training              positive relationship with mean drift in the model. Table 2
sample to determine whether each of the features were                 also displays the deviance information criterion (DIC) value
positively or negatively correlated with choice. If they were         for this fits.
positively correlated with choice, they received a weight of
+1, and if they were negatively correlated with choice they
received a weight of -1. These weights were then applied to
the observable features in the test data to predict choices
according to the tallying heuristic.
   We found that the lexicographic heuristic achieved a
mean accuracy rate of exactly 50% (SD = 0.03), indicating
that it is not a suitable way of making multiattribute choices
with such large features spaces. In contrast, the tallying
heuristic achieved a mean accuracy rate of 72% (SD = 0.09).
When comparing these heuristics with our latent attribute
approach, we found that all participants were better fit by
our approach compared to the lexicographic heuristic, and
that 78% of participants were better fit by our approach
relative to the tallying heuristic (with another 16% tied).           Table 2. Summary of best fitting group mean parameters for the
The differences in accuracy rates shown here are                      drift diffusion model fits in Study 1. Here β1 represents the weight
statistically significant when evaluated with a paired t-test         on utility difference in the drift term, in the full model. The
(p < 0.01 for both heuristics). Table 1 provides further              restricted model sets this to 0. DIC indicates the deviance
statistics involving the lexicographic and tallying heuristics.       information criterion value for the fits.
   How well do our model fits predict decision time? We can
perform this test by embedding our best fitting utilities into           In a related analysis, we fitted a simplified version of this
a drift diffusion model (Ratcliff & Rouder, 1978). Our                model in which β1 = 0, and drift is independent of the
utilities are a measure of the desirability of the objects and,       predicted utility difference. As shown in Table 2, the fits for
within the drift diffusion framework, are likely to determine         this model, measured through the deviance information
the drift rate. We can formalize this by allowing the mean            criterion (DIC), are much lower than those for the extended
drift rate in the drift diffusion model to be a linear function       model, suggesting that the utility differences specified by
of the best fitting utility difference. Thus, for trial a for         our approach do improve reaction time predictions in
participant b, we can write this mean drift rate as vab = β0 +        naturalistic multiattribute choice tasks.
β1∙(UabL – UabR). Here UabL is the predicted utility for the left
                                                                  1638

                        Studies 2-5                                Overall, the average best fitting value of L (i.e. number of
                                                                   dimensions) across our participants is 31.95 (SD = 28.55)
   As a secondary demonstration we applied our approach to
                                                                   for Study 2, 56.02 (SD = 27.18) for Study 3, 50.05 (SD =
two other domains: food choice and movie choice. We
                                                                   28.12) for Study 4, and 52.64 (SD = 25.93) for Study 5.
conducted a series of online studies offering participants
                                                                      Table 1 also displays the results of a random vector model
two-object and three-object choices between various food
                                                                   for these studies. Again it shows that the majority of
dishes and between various movies, and we predicted these
                                                                   participants are better described by our approach relative to
choices using latent attributes obtained from user-generated
                                                                   the random vector approach. Finally, Table 1 shows the fits
ingredients on www.AllRecipes.com and user-generated
                                                                   of the lexicographic and tallying heuristics. For Study 2,
keywords on www.IMDB.com.
                                                                   these fits are performed similarly to Study 1. However,
   Method. In Study 2, 90 participants recruited from
                                                                   Studies 3-5 involve three object choice. Thus the weights
Amazon Mechanical Turk made 200 binary choices between
                                                                   for the individual features necessary for fitting these
various food dishes. The food dishes were obtained from
                                                                   heuristics cannot be obtained through a simple correlation
www.AllRecipes.com, and there were a total of 100 unique
                                                                   analysis between the relative presence or absence of a
food dishes used in the study (which were the most popular
                                                                   feature and the choice in a trial. Instead we calculated, for
dishes on www.AllRecipies.com). Choices in this study
                                                                   each feature in each trial, Relative Presence = C – 0.5[UC1
were presented on the screen using just the names of the
                                                                   + UC2]. Here C = 1 if the feature is present in the chosen
dishes. Participants had to click on the names in order to
                                                                   option and 0 otherwise. Likewise, UC1 = 1 if the feature is
indicate their choices. In Study 3, 88 participants recruited
                                                                   present in the first unchosen option and 0 otherwise, and
from Amazon Mechanical Turk made 200 three-object
                                                                   UC2 = 1 if the feature is present in the second unchosen
choices between various food dishes. The dishes used were
                                                                   option and 0 otherwise. For each feature, we summed
the same as those in Study 2, and their presentation was
                                                                   Relative Presence over all the observations in the training
identical to that in Study 2 (except that each screen offered
                                                                   data for the participant in consideration. This gave us a
three different choices, instead of two). Participants in both
                                                                   measure of the Total Relative Presence of the feature in the
Studies 2 and 3 were compensated with money.
                                                                   chosen options for the participant. For the lexicographic
   In Study 4, 75 participants recruited from an
                                                                   heuristic, we then selected the single feature with the
undergraduate student participant pool made 200 three-
                                                                   highest absolute Total Relative Presence for the participant
object choices between different movies. There were a total
                                                                   in the training data, and used this feature to predict the
of 100 unique movies used. These were the 100 most
                                                                   participant’s choices in the test data. For the tallying
popular movies on www.IMDB.com (Internet Movie Data
                                                                   heuristic we recoded the Total Relative Presence for a
Base). The choices were presented on the computer screen
                                                                   feature to generate a weight of +1 if Total Relative Presence
using just the names of the movies and their IMDB movie
                                                                   was positive and -1 if it was negative. These binary weights
posters. Participants had to click on the movie name or
                                                                   were then used to predict the participant’s choices according
poster in order to indicate their choices. Participants were
                                                                   to the tallying heuristic. Using this approach, we again
compensated with course credit. Study 5 was identical to
                                                                   found that the lexicographic and tallying heuristics were out
Study 4, except that participants were recruited from
                                                                   performed by the latent attribute approach, as shown in
Amazon Mechanical Turk. There were 223 total participants
                                                                   Table 1.
in this study, and they were compensated with a monetary
payment.
   Model Fitting. The model fitting in Study 2 was identical                                Discussion
to Study 1, except that the latent attributes were recovered       In this paper we have proposed that decision makers use
from a singular value decomposition on the                         low-dimensional latent attributes in order to make decisions
www.AllRecipes.com data. Study 3 used a very similar               in naturalistic multiattribute choice settings. We have
model fitting technique, except that instead of a binary logit     obtained latent attribute representations for various
choice rule, there was a three-object (multinomial) logit          everyday choice objects using user-generated object
choice rule. Studies 4 and 5 also used this choice rule,           descriptors in large online datasets, and in five experiments,
applied using latent attributes recovered from a singular          have predicted participant choices between these objects by
value decomposition on the www.IMDB.com data.                      fitting linear models with our latent attributes. Our fits
   Results. The accuracy rates from our analysis for the           reveal that our approach provides high accuracy rates, which
Studies 2-5 are displayed in Table 1. The mean accuracy            significantly outperform accuracy rates obtained through
for Study 2 is 78% (SD = 0.10), the mean accuracy for              other sophisticated methods (such as linear models with
Study 3 is 74% (SD = 0.13), the mean accuracy for Study 4          random attribute vectors, and lexicographic and tallying
is 79% (SD = 0.14) and the mean accuracy for Study 5 is            heuristics). The best fitting models in our analysis often
80% (SD = 0.12). All of these are significantly (p < 0.01)         have small or moderate number of dimensions.
higher than the baseline accuracy of 50% (for Study 2) and         Additionally, these models are able to quantitatively predict
33% (for Studies 3-5).                                             decision times, when their estimated utilities are embedded
   We also found that the best fitting latent attribute models     within a drift diffusion process.
have a relatively low dimensionality, for most participants.
                                                               1639

   Our primary theoretical contribution involves the formal         models may also facilitate the learning of such
characterization of the processes involved in choosing              representations.
between everyday choice objects. In doing so we extend                 Despite the need to test more sophisticated representation
insights from semantic memory research to the field of              and choice models, the success of our current approach
multiattribute decision making. The resulting framework             nonetheless opens up a new avenue for studying naturalistic
attempts to describe all key aspects of the decision process,       multiattribute choice. It can be applied to examine whether
from the learning of object representations for common              existing multiattribute choice effects also emerge in more
choice objects, to the use of these representations for             realistic choice settings, where attribute information is not
evaluation and decision making. This is in contrast to most         presented numerically (as in Figure 1a). It can also be used
theories of multiattribute choice, which specify the                to extend the psychological analysis of multiattribute choice
mechanisms involved in aggregating decision attributes but          beyond the laboratory and predict real world choice data.
seldom attempt to describe what these attributes actually are       Ultimately, by combining existing theories of semantic
(see Bhatia, 2013 for a discussion).                                representation and multiattribute choice with rigorous
   Our results suggest that dimensionality reduction is not         analysis of large-scale data, this paper has proposed tools to
only at play in representing words, concepts, and various           capture the large number of important decisions made in the
non-choice objects (as in e.g. Landauer & Dumais, 1997;             real-world, that are not currently within the scope of
Shepard, 1962) but is also a critical feature of multiattribute     decision making research. This has the potential to
choice object representation in preferential decision making.       significantly expand the theoretical, descriptive, and
There are many reasons why this would be the case. Firstly,         practical scope of this area of study.
common multiattribute choice objects involve a large
number of observable features, as well as systematic                                         References
relationships between the features. Good decision making            Bhatia, S. (2013). Associations and the accumulation of
involves understanding these feature relationships, and                preference. Psychological Review, 120(3), 522.
using these relationships to make inferences about the              Draper, N., and Smith, H. (1981), Applied Regression
objects. Even though the inferences in preferential choice             Analysis, New York: Wiley.
are primarily evaluative, knowledge is used in a very similar       Jones, M. N., Willits, J. A., Dennis, S., & Jones, M. (2015).
manner as in categorization, language comprehension,                   Models of semantic memory. Oxford Handbook of
object recognition, and other related tasks. Additionally, the         Mathematical and Computational Psychology, 232-254.
use of latent attributes also offers a number of distinct           Keeney, R. L., & Raiffa, H. (1993). Decisions with Multiple
advantages relative to the use of raw observable features.             Objectives: Preferences and Value Trade-offs. Cambridge
There are fewer latent attributes than there are observable            University Press.
features, and for this reason, latent attributes simplify the       Kleinmuntz, D. N., & Schkade, D. A. (1993). Information
decision process. These attributes also reduce redundancy in           displays and decision processes. Psychological Science,
object representation, and do so in the most efficient manner          4(4), 221-227.
possible. In fact, our approach is not unlike principle             Landauer, T. K., & Dumais, S. T. (1997). A solution to
components regression, which possesses a very similar set              Plato's problem: The latent semantic analysis theory of
of statistical benefits (see Draper & Smith, 1981).                    acquisition, induction, and representation of knowledge.
   That said, the approach presented in this paper is fairly           Psychological Review, 104(2), 211.
simplistic: It involves a linear technique for dimensionality       McRae, K., Cree, G. S., Seidenberg, M. S., & McNorgan, C.
reduction combined with a linear multiattribute utility                (2005). Semantic feature production norms for a large set
model. Both of these assumptions should be tested, it                  of living and nonliving things. Behavior Research
wouldn’t be surprising if more sophisticated and more                  Methods, 37(4), 547-559.
realistic approaches to building semantic representations (         Oppenheimer, D. M., & Kelso, E. (2015). Information
Jones et al., 2015) and making choices (Oppenheimer &                  processing as a paradigm for decision making. Annual
Kelso, 2015) outperform the current approach. It may also              Review of Psychology, 66, 2774.
be the case that the representations of choice objects depend       Russo, J. E., & Dosher, B. A. (1983). Strategies for
not only on feature co-occurrence, but also on the reward              multiattribute binary choice. Journal of Experimental
structure of the domain in consideration. Individuals may,             Psychology: Learning, Memory, and Cognition, 9(4), 676.
for example, learn object representations that best predict         Shepard, R. N. (1962). The analysis of proximities:
rewards, rather those that best predict feature occurrence. If         Multidimensional scaling with an unknown distance
this is the case then it would be necessary to train models of         function. Psychometrika, 27(2), 125-140.
object representation alongside models of evaluation and            Tversky, A. (1969). Intransitivity of preferences.
choice (rather than training the former separately, as is done         Psychological Review, 76(1), 31.
in this paper). This could be accomplished using neural             Wiecki, T. V., Sofer, I., & Frank, M. J. (2013). HDDM:
networks with backpropagation from a preference (reward)               hierarchical bayesian estimation of the drift-diffusion
layer to an object representation layer. Supervised topic              model in python. Frontiers in Neuroinformatics, 7, 14.
                                                                1640

