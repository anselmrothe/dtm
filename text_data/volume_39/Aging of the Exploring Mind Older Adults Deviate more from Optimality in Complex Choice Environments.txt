     Aging of the Exploring Mind: Older Adults Deviate more from Optimality in
                                              Complex Choice Environments
                                                              Job J. Schepens
                                                      (job.schepens@fu-berlin.de)
                                                  Center for Cognitive Neuroscience Berlin
                                                  Freie Universität, Berlin 14195, Germany
                                                  Ralph Hertwig, Wouter van den Bos
                                             ({hertwig,vandenbos}@mpib-berlin.mpg.de)
                                                       Center for Adaptive Rationality
                                 Max Planck Institute for Human Development, Berlin 14195, Germany
                               Abstract                                       It is unclear how such changes in learning mechanisms in
                                                                           OA depend on the relative complexity of a task. Such a de-
   Older adults (OA) need to make many important and difficult             pendency would be likely, however, from the perspective of
   decisions. Often, there are too many options available to ex-
   plore exhaustively, creating the ubiquitous tradeoff between            ecological rationality (Mata et al., 2012), which focusses on
   exploration and exploitation. How do OA make these com-                 adaptation effects between the mind and the environment. For
   plex tradeoffs? We investigated age-related shifts in solving           example, when OA need to explore among many options in
   exploration-exploitation tradeoffs depending on the complex-
   ity of the choice environment. Participants played four and             order to choose between them later, OA rely on more mini-
   eight option bandit problems with numbers of gambles and av-            mal exploration strategies than YA (Frey, Mata, & Hertwig,
   erage rewards available on the screen. OA reliably performed            2015). Here, we study such age-related performance changes
   worse in a more complex choice environment and were also
   more deviant from an optimality model (Thompson sampling),              in explore-exploit tradeoffs with varying cognitive demands.
   which keeps track of uncertainty beyond just the mean or last           Analyzing effects of the complexity of choice environments
   reward. OA seem to process important information in more                this way could help to better understand the effects of task
   complex choice environments sub-optimally, suggesting lim-
   ited representations of future rewards. This interpretation fits        demands on age-related changes in learning mechanisms.
   to multiple contexts in the complex cognitive aging literature,            We used typical N-armed bandit problems to study changes
   in particular to the context of challenges in the maintenance of        in learning mechanisms across choice environments. Partici-
   goal-directed learning.
                                                                           pants made inferences about risky options by sampling infor-
                                                                           mation from a four and eight option choice environment. Re-
                           Introduction                                    wards were consequential, ensuring that participants needed
In today’s aging societies, more and more older adults (OA)                to trade-off exploration and exploitation. Participants had to
are making cognitively demanding decisions about work, fi-                 find options that give them the most money while having to
nances, their health, etc. Many such decisions benefit from                minimize sampling from low reward options. N-armed ban-
thinking about future goals because the options available cre-             dit problems are well studied and afford detailed analysis of
ate explore-exploit tradeoffs. How do OA usually respond                   information processing in terms of continuation and switch-
to these cognitive challenges in increasingly complex choice               ing behavior. Theoretically, expectations of future reward
environments?                                                              should rise with adequate, but not excessive, exploration.
   Decision makers generally have access to a number of                    Such “smart” exploration requires one to have a good repre-
learning mechanisms, habitual experience-based learning,                   sentation of the task and its structure, which typically weighs
and goal-directed learning. Goal-directed learning depends                 already observed rewards by the degree to which an option
on some internal model, so that learning can be adapted flex-              has been explored. This would thus involve an internal model
ibly, for example like when managing a research project. Ha-               of the task contingency between average payoff and average
bitual learning has been related to a dorsolateral striatal to             payoff uncertainty (see also Worthy, Cooper, Byrne, Gorlick,
sensorimotor cortex control loop while goal-directed learn-                & Maddox, 2014). Good performance in the task thus de-
ing has been related to a dorsomedial striatal to ventromedial             pends on learning mechanisms that use adequate future re-
and lateral prefrontal cortex control loop (Daw & O’Doherty,               ward representations while performance anomalies will in-
2014). Importantly, goal-directed learning is impaired in OA               volve inadequate future reward representations.
and this impairment has been associated to lower activation                   We hypothesized that OA achieve lower performance and
in prefrontal cortex areas (Eppinger & Bruckner, 2015; Ep-                 arrive slower at the higher reward options, depending on the
pinger, Walter, Heekeren, & Li, 2013). OA rely relatively                  number of options. If OA focus more on reward in current
more often on experience-based learning, which may arise                   states, their rewards in future states should suffer. Such short-
from white matter integrity changes in the ventromedial and                term planning would more closely resemble experience-based
lateral prefrontal cortex (Chowdhury et al., 2013; Eppinger et             learning rather than goal-directed learning. OA not arriving
al., 2013; Samanez-Larkin & Knutson, 2015).
                                                                      3082

                                          Total regret of 6.2 points by a 30 year old male (id 5, gm 6)                                                                               Total regret of 23.4 points by a 69 year old male (id 125, gm 6)
                         .6               ●        ● ● ● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●        ●●●●●●●●●●●●●          ●●●●●●                             .6       ●●●●                            ●●     ●●●           ●                                     ●●●   ●
                       .525           ●                                                                                                                                   .525   ●                                     ●●●●●●          ●                                          ●●●●
Hidden probabilities                                                                                                                               Hidden probabilities
                        .45               ● ●●         ●●                                                ●●●●●●                        ●●                                  .45          ●●●                      ●●●            ●●●●       ●                                ●            ●                          ●●●
                       .375                   ●●                                                                                  ●●●●                                    .375              ●●●                 ●●                 ●●●●       ●                           ●●●             ●●●●                    ●
                         .3       ●                ●                                                                          ●●●                                           .3                    ●●    ●                                          ●●    ●       ●                                 ●●●      ●
                       .225       ●                                                                                                                                       .225                    ●         ●                                     ●●         ●       ●●                          ●●●          ●
                        .15           ●                                                                                                                                    .15                        ●●●                                              ●●●       ●                                     ●●●●
                       .075   ●                                                                                                                                           .075                ●             ●                                  ●             ●        ●                        ●                ●
                              0               10            20     30        40        50        60          70        80         90        100                                  0          10          20             30         40          50             60             70            80           90           100
                                                                               Trial number                                                                                                                                        Trial number
                         (a) A more effective strategy that seems to take into account                                                                                      (b) A less effective strategy that seems to depend almost only
                         uncertainty.                                                                                                                                       on the outcome of the last trial.
                              Figure 1: Example eight-option choice profiles. Green indicates rewards and red indicates no rewards on a trial.
at the higher reward options at all would show a lack of an                                                                                                               far and the probability of a reward based on the observed re-
explore-exploit trade-off. We assumed that problems with a                                                                                                                wards so far. This information is sufficient to make an opti-
larger number of options are relatively more cognitively de-                                                                                                              mal choice at any point in time, reducing the role of working
manding because of a larger search space, which increases the                                                                                                             memory. Of course, participants still need to figure out how
amount of necessary information processing and representa-                                                                                                                they want to trade off exploration and exploitation. 89% of
tion.                                                                                                                                                                     YA and 70% of OA (p = .14, test of equal proportions) indi-
   Next, we describe methods and results from six kinds of                                                                                                                cated in a post-task questionnaire that “the extra information
data analyses: choice proportions statistics, choice propor-                                                                                                              regarding the options” was helpful.
tions over trials, regret over trials, comparisons to an opti-
mality model, comparisons to a fitted optimality model, and                                                                                                               Procedure Participants were instructed 1) to maximize the
one-step ahead predictions of a fitted optimality model. We                                                                                                               sum of rewards, 2) how the task looked and worked, 3) that
end with a discussion.                                                                                                                                                    each trial is independently generated, and 4) that the best
                                                                                                                                                                          gambling machine in every individual problem had popt = .6
                                                                         Methods                                                                                          (to help comparability across problems). All participants had
Participants 32 older adults (OA, Mage = 70.5, 65-74, 38%                                                                                                                 taken part in an unrelated fMRI study on risk-taking prefer-
female) and 29 younger adults (YA, Mage = 24.3, 19-30,                                                                                                                    ence several weeks beforehand. Ethics approval was granted
45% female) participated in this study. All participants were                                                                                                             by the Institutional Review Board of the Max Planck Institute
healthy, right-handed, native German speakers with normal                                                                                                                 for Human Development.
or corrected to normal vision, and without a history of psy-
chiatric or neurological disorders. There were no group dif-                                                                                                              Design The experiment made use of a repeated within-
ferences in gender proportion, educational level, and socio-                                                                                                              subject condition (four vs eight options), and a between-
economic status. Compensation amounted to about 10 Euro                                                                                                                   subject condition (age group). We chose the other hidden
per hour, plus on average 2 Euro performance-dependent                                                                                                                    probabilities in steps of .075 below .6. Reliably finding the
bonus. Participants were recruited using advertisements.                                                                                                                  better options thus required a significant part of exploration
                                                                                                                                                                          out of the 100 available trials. See also Figure 1 for exam-
Task The task of the participants was to maximize the sum                                                                                                                 ple choice profiles and the unique hidden probabilities. All
of rewards in a total of 16 alternating four and eight-armed                                                                                                              participants saw the same randomly generated rewards for all
bandit problems. Rewards could be earned by selecting pic-                                                                                                                16 problems. This allowed comparison of the problem dif-
tures of casino-style gambling machines presented on a com-                                                                                                               ficulty across participant groups as well as a reduction of an
puter screen using a keyboard or mouse. The gambling ma-                                                                                                                  unnecessary source for variance in performance while keep-
chines provided random rewards (1 or 0) with a hidden prob-                                                                                                               ing the probabilistic character of the task intact. Four dif-
ability that was specific to each machine. The rewards were                                                                                                               ferent problem orders were generated and counterbalanced
displayed on the respective bandit after each play. Partici-                                                                                                              across participants. Two different orders started with four
pants had 100 trials for every problem to explore the hidden                                                                                                              options and two different orders started with eight options.
reward probabilities and to exploit those machines that give                                                                                                              Between problems, performance was displayed on the screen
rewards most often. Remaining trials were displayed on the                                                                                                                and a keypress was required to continue with the next prob-
screen. Also, every bandit showed the number of plays so                                                                                                                  lem. Participants in both groups took about half an hour to
                                                                                                                                                  3083

                                     female                            male                                                                            4                               4
                                                                                                                                                       old                            young
                                                                                                                                       1.00
                     0.50                                                                                                              0.75
                                                                                         Block size
Reward probability
                                                                                            4                                          0.50
                                                                                            8
                                                                                                        Predicted switch probability
                     0.45
                                                                                                                                       0.25
                                                                                         Age                                                                                                             Hidden
                                                                                                                                       0.00                                                              Probability
                                                                                            YA                                                                                                             0.075
                                                                                            OA                                                                                                             0.15
                     0.40                                                                                                                              8                               8                   0.225
                                                                                                                                                                                                           0.3
                                                                                                                                                       old                            young                0.375
                                                                                                                                       1.00                                                                0.45
                                                                                                                                                                                                           0.525
                                                                                                                                                                                                           0.6
                     0.35                                                                                                              0.75
                            0   25     50     75     100   0      25    50    75   100
                                                   Trial number                                                                        0.50
                                                                                                                                       0.25
Figure 2: Predictions from a mixed effects model with the
hidden reward probabilities of participant’s choices as depen-                                                                         0.00
                                                                                                                                              0   25   50    75     100   0      25    50     75   100
dent variable and interaction effects between age, number of                                                                                                      Trial number
options, trial, and gender.
                                                                                                      Figure 3: Predictions from a mixed effects model with
                                                                                                      switching as dependent variable and three-way interaction ef-
finish the experiment. The minimum response time was set                                              fects between age, number of options, trial, reward, and the
to 200 milliseconds.                                                                                  hidden probabilities of participant’s choices.
                                                   Results
We first investigated age-related differences in task perfor-                                         preted in the light of all data included in the model. Besides
mance. Proportions of choices for each option revealed that                                           performance, we used a similar statistical analysis to test age
OA chose the option with the highest hidden probability                                               differences in switching probability over time. The resulting
about 5% less often than YA did in both four and eight option                                         logistic regression model included age, number of options,
conditions (four option 95% HDI: .003 - .093; eight option                                            trial number, the hidden probability, the reward for the partic-
95% HDI: .018 - .096; Bayesian ANOVA with logit function                                              ipant’s choices, and all three-way interactions. Together, the
and broad prior), see Figure 4 for the differences for all op-                                        estimated effects on switching indicated that OA switch less
tions. We also tested a linear mixed effects model with the                                           often (**), OA switch away less often after sampling from
hidden probability of every chosen bandit as dependent vari-                                          an option with a relativey low hidden probability (***), espe-
able and with participant ID, problem ID, and bandit position                                         cially in eight options over trials (**). Beta’s were not easily
ID as random effects. We used Satterthwaite’s approxima-                                              comparable for this model. We again generated predictions
tions of p-values (*** indicating p < .001). We found nega-                                           from this model to visualize these switching patterns, see Fig-
tive interaction effects for OA in eight options (B = -.021***)                                       ure 3.
and for OA in eight options over trials (B = -.011***). To-                                              Second, we examined development of age-differences in
gether, these indicated a lower performance for OA in eight                                           choice proportions over trials, see Figures 6a and 6b. For
options, as well as an increasingly lower performance over                                            every trial, the solid lines represent the average number of
trials. We also found a positive interaction effect for both YA                                       times that participants chose an option. The local instabilities
and OA in eight options over trials (B = .021***), as partic-                                         in the trajectories may result from individual differences and
ipants could improve relatively more over time for eight op-                                          variation across the several problems. On average, the third
tions. There still was a main negative effect of eight options                                        best option stops overlapping with the second best option af-
(B = -.092***) and a main positive effect of trial number (B                                          ter about 25 trials for YA. For OA, the same separation ex-
= -.009***). No significant difference or decrease over trials                                        ists between the second and third option after twice as many
for OA remained, so the age effect is captured only by the                                            trials, see the right panel of Figure 6a. In the eight option
higher-level interactions. Furthermore, we also controlled for                                        condition, YA separate between the better three options and
gender effects, which indicated that male OA performed bet-                                           the worse five options after about 50 trials. OA do this after
ter over trials (B = .010***) and that male OA performed                                              about 75 trials. These 2-2 and 3-5 separations could reflect
better with 8 options (B = .024***), and that males generally                                         the participants’ psychologically most salient explore-exploit
performed better (B = .003**). For visualizing these high                                             representations. Together, the choice trajectories show that
level interactions, we generated predictions from this model                                          already from the beginning onwards, YA choose more often
using the package merTools, see Figure 2. Note that the vi-                                           from the better options.
sualization does not show raw data and that the differences in                                           Third, we analyzed another measure of performance to
intercepts and slopes for the lines displayed should be inter-                                        compare performance across all of the options at once. We
                                                                                                 3084

                                             4                                      8                                                                       4                                      8
                        50
                                                                                                                              30
                                                                                                    ●                                                                            ●
                                                                                                                                                                                 ●
Proportion of choices
                        40
                                                                                                ●
                                                                                        ●
                                                                                                                                   ●
                        30                                                                                                    20
                                                                                                                     Regret
                                                                                            ●
                                                                                            ●
                                                                                                        Age                                                                                                                Age
                                                                                                         YA                                                                                                                 YA
                                                                                                         OA                                         ●                                                                       OA
                        20
                                                                                ●
                                                         ●                                                                    10
                                                                 ●                                                                                                                                             ●
                                                                 ●    ●                                                                                                                                        ●
                                                                 ●
                        10                                       ●                                                                                                                                         ●
                                                                                                                                                                                          ●
                                                                                                                                                                             ●
                                                                                                                                                        ●
                                                                                                                                       ●                ●                    ●
                                                                                ●                                                          ●                    ●
                        0                                                                                                     0                                                           ●●
                             .075 .15 .225 .3 .375 .45 .525 .6   .075 .15 .225 .3 .375 .45 .525 .6                                 1       2    3   4           5   6   7   8    1    2   3    4       5   6       7   8
                                                     Hidden probability                                                                                                     Problem
Figure 4: Boxplots of variation in average choice proportion                                                     Figure 5: Variation in performance across the 16 different
across participants for both choice environments.                                                                problems in the task.
                                                                                                                 bandit gets overtaken by another mean, but the whole pos-
choose to measure regret (a common measure in machine                                                            terior probability distribution. Conceptually, the algorithm
learning) as it generalizes over the specific outcomes of the                                                    keeps track of beliefs about the hidden probabilities of the
random number generation process. Regret can be computed                                                        bandits and then updates these beliefs each time after seeing
as RT = ∑100i=1 popt − pB(i) , where popt = .6 and pB(i) is the                                                  an outcome. The algorithm is initialized by setting a uniform
hidden probability of a reward for the chosen bandit. It fol-                                                    prior for all options. The algorithm then plays option x pro-
lows that randomly behaving agents get a total regret of 11.25                                                   portional to the probability of it being the best. Finally, it
points for four options and 26.25 points for eight options.                                                      updates its priors using the newly collected information. See
Overall, the age effect on regret was large (p < .01, Cohen’s d                                                  also Table 1. Regret as computed from applying Thompson
.707) for eight options (MOA = 19.87, SE = .79, MYA = 16.84,                                                     sampling 29 times to the same games as participants played
SE .75) and medium (p < .05, Cohen’s d .550) for four op-                                                        was significantly worse compared to participants (four op-
tions (MOA = 9.16, SE = .32, MYA = 8.17, SE .33). These age-                                                     tions: YA after trial 14, OA 11; eight options: YA after trial
related differences varied slightly across the unique problems,                                                  16, OA 16). Expected regret was 6.5 points for four options
which only differed by random number generation, see Figure                                                      and 11.0 points for eight options, which is considerably bet-
5. We also investigated how regret differences emerged using                                                     ter than YA performed on average (170% larger than the gap
the shapes of the exploration-exploitation trade-offs over tri-                                                  between YA and OA for four options and 193% for eight op-
als within the choice profiles. We observed a slowing increase                                                   tions). Interestingly, 5 out of 32 OA (16%) and 9 out of 29 YA
in regret over time in general but increasing age-related dif-                                                   (31%) achieved a median regret score within 10% of Thomp-
ferences for both conditions, see Figure 7. Age-differences                                                      son sampling for four options, while this was 1 (3%) and 4
became significant after trial 24 in eight options and 23 trials                                                 (14%) for eight options. Some individuals were thus able to
in four options. It seems that exploration in OA happens less                                                    achieve regret scores similar to Thompson sampling.
effectively. Regret was significantly (p < .05, t.test) better
compared to a random agent (four options: YA after trial 17,                                                     Table 1: Thompson sampling in r pseudocode, with n being
OA 32; eight options: YA after trial 15, OA 16).                                                                 the number of bandits, x a randomly generated probability,
   Fourth, we wanted to know how participant performance                                                         and qbeta for looking up quantiles from the Beta distribution.
differed from optimality. We used Thompson sampling as an
optimality model (Thompson, 1933), but we observed that
differences in regret for similar algorithms are small in the                                                         Step                     Computation
context of the present task. Thompson sampling uses an in-                                                            Init                     wins = rep(0, n)
verse cumulative distribution function (also known as per-                                                                                     pulls = rep(0, n)
centage point function or quantile function) that is used to                                                          Choose                   so f tmax(q, θ)
choose the bandit with the highest certainty that the hidden                                                                                   q = max(qbeta(x, α, β))
probability of a bandit is smaller or equal than some ran-                                                            Update                   wins = wins + reward
domly generated value. This way, the algorithm minimizes                                                                                       pulls = pulls + 1
uncertainty that there exists a better option by making sure                                                                                   α = 1 + wins
that the probability of choosing a certain bandit is propor-                                                                                   β = 1 + pulls − wins
tional to the probability of it being the best bandit. By taking
uncertainty into account, the algorithm affords a way of more                                                     Fifth, we wanted to know how well a fitted optimality
rapidly adapting its decision if not only the mean of a certain                                                  model predicted participant’s decisions. Thompson sampling
                                                                                                              3085

                                      YA                                                    OA                                                    YA                                                    OA
                        0.50          Thompson                                   0.50                                                             Thompson
                                      observed                                                                                                    observed
Proportion of choices                                    Proportion of choices                             Proportion of choices                                     Proportion of choices
                                                                                                                                   0.25                                                      0.25
                        0.25                                                     0.25
                        0.00                                                     0.00                                              0.00                                                      0.00
                               0      50           100                                  0    50     100                                   0       50           100                                  0    50     100
                                     Trial                                                  Trial                                                Trial                                                  Trial
                                                 (a) Four options                                                                                            (b) Eight options
Figure 6: Observed choice proportions and one-step ahead predictions for fitted Thompson sampling. The color of the lines
corresponds to the value of the hidden probabilities, where blue colors represent lower probabilities. Local polynomial regres-
sion was used as a moving window to smooth the trajectories (using a neighborhood of 40% of all points where neighboring
points are also being weighted by their distance tricubically)
was fitted to individual games by scaling predicted choices                                                                        choice environment. We found a large age-related effect
using a softmax function with a fitted inverse temperature pa-                                                                     on performance in a typical eight-armed bandit task and a
rameter θ, ranging from 0.003 to 30 (higher θ values pro-                                                                          smaller effect in a four-armed bandit task. YA also devi-
duced more randomness). OA deviated more from fitted                                                                               ated less from optimality than OA did. Choice trajectories
Thompson sampling than YA did (p < .05, Wilcoxon tests,                                                                            showed that age effects were already observable in the early
Cohen’s d = .57 for four options and Cohen’s d = .53 for                                                                           exploration stage, suggesting that OA explore longer or less
eight options). θ was also significantly lower for YA than                                                                         efficiently. Theoretically, the early stages require fast explo-
for OA (p < .05, Wilcoxon test, Cohen’s d = .96 for four                                                                           ration using not only average rewards but also their associ-
and 2.68 for eight options), indicating more randomness and                                                                        ated uncertainty. This was illustrated here using Thompson
worse matches to predictions of Thompson sampling in OA                                                                            sampling, which is a kind of randomized probability match-
than in YA. OA and YA both significantly decreased their me-                                                                       ing algorithm. Participants diversified their choices similar
dian θ for eight options compared to the four option condition                                                                     to Thompson sampling, in line with previous work (Konstan-
(p < .05, Wilcoxon tests), see Figure 8. θ for OA signifi-                                                                         tinidis, Ashby, & Gonzalez, 2015; Speekenbrink & Konstan-
cantly varied more in both conditions than for YA (p < .01 for                                                                     tinidis, 2015). Furthermore, OA had higher and more variable
both conditions, Wilcoxon tests) and average variation across                                                                      inverse temperature parameter estimates across choice envi-
games was significantly lower in four options for YA, but for                                                                      ronments, indicating more randomness in OA. OA thus rely
OA this was similar in both conditions (p < .01 vs. p = .4,                                                                        on less effective learning strategies that consider important in-
Wilcoxon tests). In all, OA adults were more random and less                                                                       formation less effectively, in particular in the more complex
homogeneous, possibly indicating more strategy changes.                                                                            environments.
   Finally, we compared the shapes of the mean observed                                                                               Why would OA fail to represent important information like
exploration-exploitation trade-off trajectories to shapes from                                                                     uncertainty or a specific task model? The role of working
one-step-ahead predictions across all trials. These predictions                                                                    memory influences should be minimal as this is not strictly
are plotted in Figures 6a and 6b using dashed lines. We used                                                                       necessary to perform well in the task. General “slowing”,
the median θ of every participant for both conditions as data                                                                      gender effects, and more cautious risk taking, all of which
scaling parameter. The predictions from this fitted Thompson                                                                       could favor exploitation of short-term rewards, also mark
sampling model resulted in accurately ordered trajectories for                                                                     cognitive aging. We did indeed observe gender interactions,
both groups and for both conditions: The orderings of solid                                                                        age-differences in reaction times, and in standard neuropsy-
and dashed lines were identical for all four graphs for most                                                                       chological test results (working memory, fluid intelligence,
trials, except in the first few trials. The latter may indicate                                                                    and risk-taking). However, as performance is mainly deter-
more rapid exploration in Thompson sampling and that both                                                                          mined by a cognitively costly explore-exploit tradeoff and ad-
YA and OA explore less rapidly, with OA taking the longest.                                                                        equate future reward representations, our findings specifically
                                                                                                                                   point towards underreliance on goal-directed learning.
                                   Discussion and Conclusions                                                                         A logical next step is to assess if fits of simple learning
                                                                                                                                   strategies can indeed better accommodate OA. Specifically,
We aimed to identify changes in the ways OA and YA make
                                                                                                                                   the exploration phase seems to happen sub-optimally in OA
goal-directed choices depending on the complexity of the
                                                                                                          3086

                       4                         8
                                                                                       Age
                                                                                         YA
                                                                                         OA
         20
                                                                 Pattern
Regret
                                                                   YA
                                                                   OA
                                                                   Random
         10                                                        Thompson
         0
              0   25   50    75    100 0    25   50   75   100                    50           100            150           150          200
                                    Trial                                              Four Option Deviance           Eight option Deviance
Figure 7: Age-differences in the increase in regret at every                  Figure 8: Histogram and medians of median deviance per in-
trial with standard deviations displayed around the means                     dividual across games from fitted Thompson sampling.
(standard errors were too narrow to visualize). Regret in-
creased quickly first and increased slower later on, but slower
for YA.                                                                         Decision Making: A Neuro-Computational Approach. In
                                                                                T. M. Hess & J. S. E. Lckenhoff (Eds.), Aging and Deci-
                                                                                sion Making (pp. 61–77). San Diego: Academic Press.
and in a more varied way. Favoring short-term rewards could                   Eppinger, B., Walter, M., Heekeren, H. R., & Li, S.-C. (2013).
be a sign of a learning mechanism that sub-optimally repre-                     Of goals and habits: Age-related and individual differ-
sents future rewards. More varied or reduced processing of                      ences in goal-directed decision-making. Frontiers in Neu-
important information such as uncertainty would be able to                      roscience, 7.
account successfully for the observed age-related changes.                    Frey, R., Mata, R., & Hertwig, R. (2015). The role of cogni-
Furthermore, if the task indeed probes OA to rely less on                       tive abilities in decisions from experience: Age differences
goal-directed learning, we may also expect differences in                       emerge as a function of choice set size. Cognition.
connectivity to prefrontal regions (pending analyses). In                     Konstantinidis, E., Ashby, N. J., & Gonzalez, C. (2015). Ex-
all, OA may be using less effective learning strategies the                     ploring Complexity in Decisions from Experience: Same
more demanding the choice environment becomes. Identi-                          Minds, Same Strategy. In 37th annual meeting of the Cog-
fying such task-dependent differences is typically neglected                    nitive Science Society (CogSci 2015) (pp. 23–25).
in neuro-computational models of decision-making. In the                      Mata, R., Pachur, T., Von Helversen, B., Hertwig, R.,
context of cognitive aging, this may be useful for empower-                     Rieskamp, J., & Schooler, L. (2012). Ecological rational-
ing aging decision makers to navigate cognitively demanding                     ity: A framework for understanding and aiding the aging
choice environments.                                                            decision maker. Decision Neuroscience, 6, 19.
                                                                              Mata, R., Schooler, L. J., & Rieskamp, J. (2007). The aging
                            Acknowledgments                                     decision maker: Cognitive aging and the adaptive selection
                                                                                of decision strategies. Psychology and Aging, 22, 796–810.
JS is supported by European Commission Horizon 2020
                                                                              Samanez-Larkin, G. R., & Knutson, B. (2015). Decision
MSCA No 708507. Robert Lorenz, Jann Wscher, Da-
                                                                                making in the ageing brain: Changes in affective and mo-
nia Esch, and Lukas Nagel supported with participant re-
                                                                                tivational circuits. Nature Reviews Neuroscience. 16(5),
cruitment, organization, and data collection. Materials,
                                                                                278-289.
data, and analysis can be found on https://github.com/
                                                                              Speekenbrink, M., & Konstantinidis, E. (2015). Uncertainty
jobschepens/Bandits/.
                                                                                and Exploration in a Restless Bandit Problem. Topics in
                                  References                                    Cognitive Science, 7, 351–367.
                                                                              Thompson, W. R. (1933). On the likelihood that one un-
Chowdhury, R., Guitart-Masip, M., Lambert, C., Dayan, P.,                       known probability exceeds another in view of the evidence
  Huys, Q., Dzel, E., & Dolan, R. J. (2013). Dopamine re-                       of two samples. Biometrika, 285–294.
  stores reward prediction errors in old age. Nature Neuro-                   Worthy, D. A., Cooper, J. A., Byrne, K. A., Gorlick, M. A.,
  science, 16, 648–653.                                                         & Maddox, W. T. (2014). State-based versus reward-based
Daw, N. D., & O’Doherty, J. P. (2014). Multiple Systems for                     motivation in younger and older adults. Cognitive, Affec-
  Value Learning. In P. W. G. Fehr (Ed.), Neuroeconomics                        tive, & Behavioral Neuroscience, 14, 1208–1220.
  (Second Edition) (pp. 393–410). San Diego: Academic
  Press.
Eppinger, B., & Bruckner, R. (2015). Towards a Mechanis-
  tic Understanding of Age-Related Changes in Learning and
                                                                           3087

