     Moral Action Changes Mind Perception for Human and Artificial Moral Agents
                                             Evgeniya Hristova (ehristova@cogs.nbu.bg)
                                              Maurice Grinberg (mgrinberg@nbu.bg)
                      Research Center for Cognitive Science, Department of Cognitive Science and Psychology
                                                            New Bulgarian University
                                                  21 Montevideo Str., Sofia 1618, Bulgaria
                               Abstract                                    memory, emotion recognition, planning, communication, and
   Mind perception is studied for three different agents: a human,
                                                                           thought. Further, the authors establish that moral judgments
   an artificial human, and a humanoid robot. The artificially             about punishment correlate more with the Agency dimension
   created agents are presented as being undistinguishable from a          than with the Experience dimension: perceived agency is
   human. Each agent is rated on 15 mental capacities. Three mind          correlated with moral agency and responsibility. On the other
   perception dimensions are identified - Experience, Agency,              hand, desire to avoid harming correlates with the experience
   and Cognition. The artificial agents are rated higher on the            dimension: perceived experience is connected with moral
   Cognition dimensions than on the other two dimensions. The              patience, rights and privileges. One result of Gray et al.
   humanoid robot is rated lower than the human on the
   Experience dimension. These results show that people ascribe            (2007), relevant for the present paper, is the evaluation of a
   to artificial agents some mental capacities more than others. In        human as having the highest scores in experience and agency
   a second experiment, the effect of agent’s moral action on mind         and the evaluation of the robot to have practically zero score
   perception is explored. It is found that when the artificial agents     on the experience dimension and half the maximal score on
   have undertaken a moral action, they are perceived to be                the agency dimension. This will mean that following the
   similar to the human agent. More interestingly, the presentation        interpretation given by Gray et al. (2007), robots will be
   of the moral action leads to a restructuring of the dimensions
                                                                           judged as less morally responsible for their actions. On the
   of mind perception.
                                                                           other hand, the opposite should be also true. If an agent is
   Keywords: mind perception; moral agency; artificial agents;             judged to be able to be a moral agent, this will reflect in her
   utilitarian moral actions; moral dilemmas                               score on the mind perception dimensions. The latter is
                                                                           explored in the present paper.
                           Introduction                                      In a recent study (Takahashi et al., 2014), the perception of
                                                                           the participants about five agents – a human, a human-like
Mind Perception and Artificial Cognitive Agents                            android, a mechanical robot, an interactive robot, and a
The problem of mind perception is central to many debates in               computer – was investigated. The study found that
psychology and philosophy and has been extensively studied                 participants position the agents in a two dimensional space
in cognitive science in the last years (see e.g. Arico et al.,             spanned by “Mind-holderness” (the possibility for the agent
2011; Gray et al., 2007). The questions of how people know                 to have a mind) and “Mind-readerness” (the capability to
that other people are conscious or what are their intentions,              “read” other agent minds). The results showed that the
feelings and thoughts have large implications in the way                   appearance and the capability for communication lead to
people make judgments and decisions, and act. This problem                 different beliefs about the agents’ closeness to human social
is so interesting and difficult because mental states are not              agents. The humanoid robot was very close to the human
observable. Moreover, mind attribution and mind perception                 agent, while the computer was at the same level in terms of
concern not only human or animal agents but also inanimate                 “Mind-readerness” but very low relative score on “Mind-
entities, e.g. geometrical shapes moving in at various speeds              holderness”. An interesting result for the present study is fact
and in various directions (Heider & Simmel, 1944).                         that the ordering in terms of “Mind-holderness” is based on
   The question of how people attribute mental states to others            appearance of the agent – the human and the human-like
– humans and other entities is also related to whether there is            android having the highest score and the mechanical robot
a single continuum of mind perception and what are its                     having the lowest.
dimensions.                                                                  The results of Takahashi et al. (2014) show that social
   In the influential study of Gray et al. (2007), participants            interaction with human-like or potentially intelligent agents
had to evaluate several characters including a human, a robot,             could activate selectively our social brain and lead to
and a computer with respect to the degree of possessing                    behavior similar to the one people have with other humans.
various cognitive capacities. Using factor analysis, they                  Thus, Takahashi et al. (2014) demonstrated that people can
found two dimensions, which correlate with mind perception:                infer different characteristics related to various cognitive
'Agency' (exhausting 88% of the variance) and 'Experience'                 abilities based on short communication sessions and act
(exhausting 8 % of the variance).                                          accordingly. One can ask the question addressed in the
   The Experience dimension includes the following                         present paper: can people be influenced by short stories of
capacities: hunger, fear, pain, pleasure, rage, desire,                    moral action of agents, instead of actual interaction with an
personality, consciousness, pride, embarrassment, and joy.                 agent, in their mind perception?
The Agency dimension includes self-control, morality,
                                                                       2236

Moral Agency and Mind Perception                                        The goals of the present paper are the following. First, to
As discussed in the previous section, mind perception is             explore the dimensions of mind perception for human agents
based on a number of dimensions, which depend on the                 and fictitious artificial agents that are identical to humans.
specific experimental settings – ‘Agency’ and ‘Experience’           Here, the artificial agents are described as undistinguishable
in Gray et al. (2007), when agents are directly evaluated and        from a human, but as being created from organic materials -
‘Mind-readiness’ and ‘Mind-holderness’ in Takahashi et al.           one of them is labeled as an artificially created human and the
(2014), following a similar procedure but after interacting          other one - as a robot. The rationale of using artificial agents
with the agents. Both papers discuss the relation of mind            is that in such a way dimensions of mind perception can be
perception to social interaction, which includes moral agency        better explored as people do not have previous knowledge or
to various degrees.                                                  experience with those agents.
   In law and philosophy, moral agency is taken to be                   The second goal is to explore the moral judgments about
equivalent to moral responsibility, and is not attributed to         utilitarian moral action undertaken by of those three agents.
individuals who do not understand or are not conscious of            This goal is a continuation of previous research (Hristova &
what they are doing (e.g. to young children). Sullins (2011)         Girnberg, 2015; Hristova & Grinberg, 2016) on moral
states that moral agency can be attributed to a robot when it        judgments about the actions of artificial cognitive agents.
is autonomous, and it has intentions to do good or harm. The         Moral judgments can be studied in their purest form using
latter is related to the requirement that the robot behaves with     hypothetical situations in which there is a conflict between
understanding and responsibility with respect to other moral         moral values, rules, rights, and agency (Foot, 1967;
agents. If the perceived action are morally harmful or               Thomson, 1985). Such moral dilemma is used in the paper -
beneficial and are “seemingly deliberate and calculated”, the        a hypothetical situation in which several people will die if the
robot can be regarded as a moral agent.                              agent does not intervene in some way. The intervention will
   On the other hand, it is well known that people easily            lead to the death of another person but also to the salvation of
anthropomorphize nonhuman entities like animals and                  the initially endangered people. The moral actions used in the
computers and thus would ascribe to some degree moral                presented experiments are decisions of the agents to sacrifice
agency, intentions, and responsibilities to them (Waytz,             one person and save five.
Gray, Epley, & Wegner, 2010). Several studies, explore the              The third goal of the research is to test the influence of a
attribution of mind and moral agency to artificial cognitive         moral action of an agent on mind perception for that agent.
systems. In Arico et al. (2011), it is shown that entities           The expectation is that an agent performing a moral action
displaying simple features like eyes, distinctive motions, and       will be perceived as possessing mental capacities to a higher
interactive behavior, are categorized as agents and that             degree. This especially applies to the artificial agents which
categorization triggers the attribution of conscious mental          are expected to be perceived as more human-like when they
states to those entities, including individuals.                     have undertaken an utilitarian action.
   In Ward, Olsen, Wegner (2013), it was shown that people
can perceive mind in entities like corpses, people in a                                      Experiment 1
persistent vegetative state, or robots, if they are subject to                         Goals and Hypothesis
intentional harm. According to the authors, the evidence of          Experiment 1 aims to achieve the first two goals described
mind can be related to observation or interaction with entities,     above. First, to test the dimensions of mind perception of
which exhibit intention, emotion or behavior but also to             artificial agents (described as being undistinguishable from a
indirect evidence related to the moral or social interaction         human, but as being created from organic materials) and to
surrounding those entities.                                          compare them to the mind perception of a human being.
                                                                     Second, to explore the moral judgments about utilitarian
Current research                                                     actions undertaken by those agents. The hypothesis is that
The results summarized above show that moral agency is               although described as being identical to a human, the
closely related to mind perception and give evidence that            artificial agents will be perceived as equal to humans on more
perceived mental capacities or actions influence moral               cognitive dimensions (e.g. perception and planning) but
agency evaluation. Some of the results suggest that the              lower than humans on the experiential dimensions (e.g.
inverse influence is also taking place, namely from perceived        emotions and consciousness).
moral agency to infer mental capacities.
   Recently, the behaviour of artificial cognitive agents                                        Method
became central to research and public debate in relation to the
rapidly increasing usage of robots and intelligent systems in        Design and Procedure
our everyday life. Several important questions must find their       Mind perception is studied for three different agents: a
answers as the use of artificial cognitive agents has many           human, an artificial human, and a humanoid robot. The
benefits but also many risks. Some of those questions concern        artificially created agents (the artificial human and the
moral agency - if those agents should be allowed to make             humanoid robot) were presented to participants as being
moral decisions and how such decisions are judged and                undistinguishable from a human, but as being created from
evaluated.                                                           organic materials). Their descriptions are provided in
                                                                 2237

Table 1. The only difference between the artificial human               condition, 17 for the artificial human condition, 18 for the
and the humanoid robot conditions is in the word used to                humanoid robot condition.
label the created individual – a human or a robot. The identity
of the agent is varied in a between-subjects design – each               Table 1. Descriptions of the agents used in the experiments.
participant was presented with only one description of an                   Human:
agent (human, artificial human, or humanoid robot). The data                The year is 2100. Mark is a young man.
was collected using web-based questionnaires. The                           Artificial Human:
questionnaires had two parts – a mind perception task and a                 The year is 2100. Technology has advanced so much that all
moral judgment task. Participants were not informed                         parts and organs of the human body, including the brain, can
                                                                            be created from organic materials and are identical to natural
beforehand that there are two different tasks.
                                                                            ones. Mark is a human created like this. All his organs are
Mind perception task. After the description of the agent, the               created from organic matter and are the same as those of a real
participants had to rate the mental capacities and mental                   human. His brain is also created from organic matter and is
states of the agent on 32 Likert scales (ranging from ‘1 –                  functioning as the brain of a real human. Mark could not be
completely disagree’ to ‘7 – completely agree’). Questions                  distinguished by anything from a human.
assessed 15 mental capacities: psychobiological (hunger &                   Humanoid robot:
thirst; physical pain; physical pleasure), perception (vision &             The year is 2100. Technology has advanced so much that all
hearing; taste & smell; touch), cognitive functions (thinking               parts and organs of the human body, including the brain, can
& reasoning; learning, memory & knowledge; judgment &                       be created from organic materials and are identical to natural
choice), planning (goal formulation, action planning);                      ones. Mark is a robot like this. All his organs are created from
                                                                            organic matter and are the same as those of a real human. His
emotional experience (emotional pain; emotional pleasure),                  brain is also created from organic matter and is functioning as
affective states (feels emotions like anger, joy, happiness,                the brain of a real human. Mark could not be distinguished by
sadness, fear; feels love; feels sympathy and compassion),                  anything form a human.
agency (intentions; autonomous decisions; understanding
consequences of own actions), moral agency (knows right                         Table 2. Moral dilemma used in the experiments
from wrong; tries to do the right thing; responsible for own                 Mark is responsible for a system controlling the movement
actions), beliefs (beliefs, expectations), desires (desires;              of containers with cargo in a metallurgical plant. Mark notices
dreams), theory of mind (understanding others’ thoughts;                  that the system is faulty and a heavy container had become
understanding others’ feelings), communication (ability to                uncontrollable and headed at high speed toward five
communicate thoughts and feelings to others), conscious                   technicians who are in a tunnel. They do not have time to get
experience (conscious experience), self-control (control of               out of there and are going to die, crushed by container.
                                                                             No one but Mark can do anything in this situation.
desires, emotions, impulses), and personality (unique
                                                                             The only thing that Mark can do, is to activate a control
personality).                                                             button and to switch off the security system of another
Moral Judgment task. In the second part of the survey, each               technician who is on a high platform. The technician will fall
participant is again presented with the description of the agent          down in front of the container. Together with his equipment,
followed by a description of a moral dilemma in which the                 the technician is heavy enough to stop the moving container.
protagonist is the same agent as in the previous task. The agent          He will die crushed by the container, but the other five
has to make the moral decision whether to push a control button           technicians will remain alive.
and kill a person in order to save five people. The full text of the         Mark decides to activate the control button and to switch
dilemma is given in Table 2. The agent is described to make the           off the security system of the technician who is on the
                                                                          platform. The technician falls on the path of the container and
utilitarian decision and to undertakes the utilitarian action (the
                                                                          as the technician, together with his equipment, qis heavy
agent pushes the control button and kills one person but saves            enough, he stops the moving container. He dies, but the other
five other). After that the participants judged the moral rightness       five technicians stay live.
of the action (‘yes’ or ‘no’), rated the moral permissibility of the
action (on a scale ranging from ‘1 = not permissible at all’ to ‘7                                     Results
= it is mandatory’) and the blameworthiness of the agent (on a
scale ranging from ‘1 = not at all blameworthy’ to ‘7 = extremely       Dimensions of Mind Perception
blameworthy’).
                                                                        Mind perception is assessed with respect to 15 mental
Participants                                                            capacities involving 32 rating scales. When a mental
                                                                        capacity is assessed using more than one rating scales, the
70 participants filled in the questionnaires online. They were          average value from the ratings is calculated. The ratings on
randomly assigned to one of the three experimental                      these 15 capacities were subjected to a principal components
conditions. Data of 13 participants were discarded as they              factor analysis with varimax rotation (Kaiser normalization).
failed to answer correctly the question assessing the reading           The rotated solution yielded 3 factors with eigenvalues
and the understanding of the presented scenario. So,                    greater than 1 that explained 77.4% of the variance.
responses of 57 participants (47 female, 10 male; 36 students,             The first factor accounted for 31.7% of the variance and
21 non-students) were analyzed – 22 for the human agent                 included 7 capacities – desires, affective states, emotional
                                                                    2238

experience, beliefs, psychobiological, personality, conscious          For the human agent, there is a significant effect of DMP on
experience. This factor is further named Experience.                the ratings (F (2, 42) = 5.59, p = .007). The human agent received
  The second factor accounted for 24.1% of the variance and         lower ratings on the Agency dimension (M = 4.5) than on the
included 5 capacities - self-control, communication, theory of      Experience (M = 5.3, p = .02) or on the Cognition dimension (M
mind, moral agency, agency – and is called Agency.                  = 5.2, p = .02). The effect of DMP is also significant for the
  The third factor accounted for 21.6% of ratings variance          artificial human (F(2, 32) = 13.03, p < .001): the artificial human
and included 3 of the capacities – perception, cognitions,          is rated higher on the Cognition dimension (M = 5.3) than on the
planning - and is named Cognition.                                  Experience dimension (M = 4.5, p = .008) or on the Agency
  Those factors are considered as Dimensions of Mind                dimension (M = 4.4, p < .001). For the humanoid robot, the
Perception (DMP).                                                   effect of DMP is also significant (F(2, 34) = 8.44, p = .001): the
  To obtain ratings for each DMP, the ratings of all capacities     humanoid robot is rated higher on the Cognition dimension (M=
that load on that DMP were averaged. Those average ratings          5.3) than on the Experience dimension (M = 3.8, p = 0.008) or
were subjected to a 3 x 3 Repeated-Measures ANOVA with              on the Agency dimension (M = 4.3, p = .039).
DMP (Experience vs. Agency vs. Cognition) as a within-
subjects factor and identity of the agent (human vs. artificial     Moral Judgments
human vs. humanoid robot) as a between-subjects factor. The         The proportion of participants choosing the option that the
results are presented on Figure 1.                                  agent’s utilitarian action (activating a control button, thus
                                                                    sacrificing one person, and saving five people) is morally
                                                                    right is 0.55 for the human, 0.53 for the artificial human, 0.5
                                                                    for the humanoid robot. Chi-square test shows that the
                                                                    differences are not significant. The effect of the identity of
                                                                    the agent is not significant neither for the moral permissibility
                                                                    ratings (p = .71) nor for the blameworthiness ratings (p =
                                                                    .74). The data is presented in Table 3.
                                                                    Table 3: Mean and standard deviation of the ratings about
                                                                    moral permissibility of the action (‘1 = not permissible at all’ to
                                                                    ‘7 = it is mandatory’) and the blameworthiness of the agent (‘1
                                                                    = not at all blameworthy’ to ‘7 = extremely blameworthy’).
                                                                            Agent            Moral permissibility    Blameworthiness
                                                                           Human                   4.3 (1.8)             3.1 (1.6)
Figure 1: Average ratings on each Dimension of mind                   Artificial human             4.2 (1.8)             3.1 (1.9)
perception (Experience, Agency, Cognition) for each agent
(human, artificial human, humanoid robot) on 7-point scales           Humanoid robot               3.8 (1.9)             3.8 (1.9)
(1 = ‘completely disagree’, 7 = ‘completely agree’). Error
bars represent standard errors.                                     Summary of the Results in Experiment 1
  The main effect of identity of the agent is not statistically     In Experiment 1, three dimensions of mind perception are
significant.                                                        identified – Experience (desires, affective states, emotional
  The analysis revealed a main effect of DMP, F(2, 108) =           experience, beliefs, psychobiological, personality, conscious
15.94, p < .001. A Bonferroni post-hoc test revealed that           experience), Agency (self-control, communication, theory of
agents receive higher ratings on the Cognition dimension (M         mind, moral agency, agency), Cognition (perception,
= 5.28) than on the Experience dimension (M = 4.54, p =             cognitions, planning).
.001) or on the Agency dimension (M = 4.36, p < .001).                 The artificial human and the humanoid robot are rated as
  The effect was qualified by a significant interaction             similar to the human agent on Agency and Cognition
between DMP and identity of the agent, F(4, 108) = 4.29, p =        dimension. The humanoid robot is rated lower on the
.003. The interaction is as follows. There is no significant        Experience dimension than the human agent.
difference between the ratings of the agents on the Agency             The identified dimensions of mind perception are ascribe
dimension – human (M = 4.5), artificial human (M = 4.3),            to different agents in a different pattern. Human agent is
humanoid robot (M = 4.3). There is also no significant              judged higher on the Experience and Cognition dimensions
difference between the ratings of the agents on the Cognition       than on the Agency dimension. The artificially created agents
dimension – human (M = 5.2), artificial human (M = 5.3),            (artificial human and humanoid robot) are judged higher on
humanoid robot (M = 5.3). Only for the Experience                   the Cognition dimension than on the Agency or Experience
dimension there is a significant effect of the identity of the      dimensions. People more readily ascribe cognitive mental
agent (F(2, 54) = 4.07, p = .023) – the humanoid robot is rated     capacities to artificially created agents than mental capacities
lower (M = 3.8) than the human (M = 5.3) on the Experience          belonging to the Experience or Agency dimensions.
dimension (p = .019).
                                                                2239

   No differences among the agents were found with respect               The third factor (Factor 3) accounted for 21.2% of ratings
to moral judgments. This result is not surprising as all agents        variance and included 3 of the capacities – cognitions,
are perceived as having similar agency (p = .93) and moral             emotional experience, perception, psychobiological.
agency (p = .38).                                                        The average ratings on each factor were calculated and
                                                                       subjected to a 3 x 3 Repeated-Measures ANOVA with DMP
                          Experiment 2                                 (Factor1 vs. Factor2 vs. Factor3) as a within-subjects factor
                   Goals and Hypothesis                                and identity of the agent (human vs. artificial human vs.
                                                                       humanoid robot) as a between-subjects factor. The analysis
As stated above, the third goal of the current research is to
                                                                       revealed a main effect of DMP, F(2, 114) = 10.52, p < .001.
test the influence of a moral action of an agent on mind
                                                                       A Bonferroni post-hoc test revealed that agents receive lower
perception for that agent. In order to accomplish this goal, a
                                                                       ratings (M = 4.55) on the second dimension than on the first
second experiment is conducted. In that experiment, the
                                                                       dimension (M = 5.25, p < .001) and on the third dimension
ratings of mental capacities are preceded by the moral
                                                                       (M = 5.31, p = .003).
judgment task in which the agent is described as undertaking the
                                                                         The main effect of identity of the agent is not statistically
utilitarian action of killing one person in order to save five. The
                                                                       significant. The interaction is aslo not significant.
hypothesis is that an agent performing a moral action will be
perceived as possessing a higher degree of mental capacities.          Moral Judgments
This especially applies to the artificial agents.
                                                                       Proportion of the participants answering that the utilitarian
                              Method                                   action undertaken by the agent, is morally right is 0.5 for the
                                                                       human agent, 0.41 for the artificial human, 0.5 for the
Design and Procedure                                                   humanoid robot. Human, artificial human, and humanoid
                                                                       robot receive mean moral permissibility ratings of 3.1, 3.7,
The design of Experiment 2 is similar to that of Experiment            and 3.7 and blameworthiness ratings of 3.4, 3.3, and 3.0,
1, the only difference being the inverse order of task                 respectively. No significant differences are found.
presentation: the moral judgment task was presented first and
then – the mind perception task.                                       Summary of the Results in Experiment 2
Participants                                                           In Experiment 2, again three dimensions of mind perception
                                                                       are revealed, but they are different from the dimensions
64 participants filled in the questionnaires online. They were         identified in Experiment 2. The difference is attributed to the
randomly assigned to one of the three experimental conditions.         utilitarian moral action undertaken by the agent before the
Data of 4 participants were discarded as they failed to answer         mind perception ratings being made. First dimension
correctly the control question. So, responses of 60 participants       identified here combines mental capacities from Experience
(48 female, 12 male; 36 students, 24 non-students) are analyzed        and Agency dimensions identified in Experiment 1.
– 20 for the human agent condition, 22 for the artificial human          No differences are found between agent’s ratings on each
condition, 18 for the humanoid robot condition.                        of the identified dimensions in Experiment 2. It seems that
                                                                       undertaking the utilitarian moral action makes the artificial
                              Results                                  agents to be perceived as similar to the human agent.
Dimensions of Mind Perception
As in Experiment 1, mind perception is assessed with respect            Influence of Moral Action on Mind Perception
to 15 mental capacities with 32 rating scales. Again, when a           In order to explore further the influence of moral action on
mental capacity was assessed using several questions, the              mind perception, we compared the ratings for each of the 15
average value from the ratings was calculated. The ratings on          mental capacities between Experiment 1 and Experiment 2.
these 15 capacities were subjected to a principal components           Description of the agent undertaking the utilitarian moral
factor analysis with varimax rotation (Kaiser normalization).
                                                                       action preceded mind perception ratings in Experiment 2 so
The rotated solution yielded 3 factors with eigenvalues
                                                                       this is considered as moral-action condition.
greater than 1 that explained 80% of the variance.
                                                                         Ratings for each mental capacity were analyzed in a 3x2
   The first factor (Factor 1) accounted for 32% of the
                                                                       ANOVA with identity of the agent (human vs. artificial
variance and included 7 capacities – beliefs, conscious
                                                                       human vs. humanoid robot) and agent’s moral action (‘no’ or
experience, agency, desires, planning, affective state, moral
                                                                       ‘yes’) as between-subjects factors. Only the significant
agency. It seems that the first dimension is a combined
                                                                       results are reported here.
Experience-Agency dimension.                                             Identity of the agent had an effect on the ratings of the
   The second factor (Factor 2) accounted for 26.7% of the             following mental capacities: conscious experience (p = .016),
variance and included 5 capacities – personality,                      affective states (p = .002), emotional experience (p = .020)
communication, self-control, theory of mind.                           and desires (p < .001). The human agent was rated higher
                                                                       than the humanoid robot on all of those mental capacities (all
                                                                       p’s < .05). The human agent was rated higher than the
                                                                   2240

artificial human on affective states (p = .051) and desires (p        to a lesser extend to non-human agents. They also give
= .017). For beliefs the effect was marginally significant (p =       evidence that mind perception space is sensitive to and
.054): human agent was rated higher than the humanoid robot           dependent on the actions performed by an agent.
(p = .065).
  Agent’s moral action had an effect on the ratings of the
following mental capacities: agency (p = .014), moral agency                                    References
(p = .029), beliefs (p = .024), conscious experience (p = .056),
and planning (p = .058). The result is interesting, as it             Arico, A., Fiala, B., Goldberg, R. F., & Nichols, S. (2011).
demonstrated that the agent’s moral action have an effect not               The Folk Psychology of Consciousness. Mind &
only on his agency and moral agency ratings, but also on the                Language, 26(3), 327–352.
rating of other mental capacities.                                    Foot, P. (1967). The Problem of Abortion and the Doctrine of
                                                                            Double Effect. Oxford Review, 5, 5–15.
               Discussion and Conclusions                             Gray, H. M., Gray, K., & Wegner, D. M. (2007). Dimensions of
The paper investigates the dimensions of mind perception for                mind perception. Science, 315(5812), 619.
human agents and fictitious artificial agents (an artificial          Heider, F., & Simmel, M. (1944). An experimental study of
human and a humanoid robot) that are identical to humans                    apparent behavior. The American Journal of
and how mind perception is affected by the agent being                      Psychology, 57(2), 243.
presented as moral agent.                                             Hristova, E., & Grinberg, M. (2015). Should Robots Kill? Moral
  In Experiment 1, three dimensions of mind perception are                  Judgments for Actions of Artificial Cognitive Agents. In
identified – Experience, Agency, and Cognition. The                         Proceedings of EAPS 2015.
identified dimensions of mind perception are ascribed to              Hristova, E., Kadreva, V., & Grinberg, M. (2014). Moral
different agents in a different pattern. The artificially created           Judgments and Emotions: Exploring the Role of
agents are judged higher on the Cognition dimension than on                 'Inevitability of Death'and “Instrumentality of Harm” (pp.
the Agency or Experience dimensions. The human is judged                    2381–2386). Austin, TX: Proceedings of the Annual
higher on the Experience and Cognition dimensions than on                   Conference of the Cognitive Science Society.
the Agency dimension. The artificial agents are rated as              Sparrow, R. (2007). Killer Robots. Journal of Applied
similar to the human agent on Agency and Cognition                          Philosophy, 24(1), 62–77.
dimension but not on the Experience dimension.                        Strait, M., Briggs, G., & Scheutz, M. (2013). Some correlates of
   People more readily ascribe cognitive mental capacities to               agency ascription and emotional value and their effects on
artificially created agents than mental capacities belonging to             decision-making. In Affective Computing and Intelligent
the Experience or Agency dimensions.                                        Interaction, 505-510. IEEE.
  In Experiment 2, the goal was to explore the influence of a         Sullins, J. (2006). When is a robot a moral agent? International
utilitarian moral action undertaken by the agent on mind                    Review of Information Ethics, 6, 23-30.
perception for that agent. The three dimensions of mind               Takahashi, H., Terada, K., Morita, T., Suzuki, S., Haji, T.,
perception here are restructured – the first dimension                      Kozima, H., et al. (2014). Different impressions of other
regroups mental capacities that seem influenced by the                      agents obtained through social interaction uniquely
preceding agents’ moral action description like agency,                     modulate dorsal and ventral pathway activities in the
moral agency, consciousness, planning and affective states.                 social human brain. Cortex, 58(C), 289–300.
The second factor is related to communication and social              Thomson, J. J. (1985). The Trolley Problem. The Yale Law
interaction, while the third to cognition and psychobiological              Journal, 94(6), 1395–1415.
capacities. Now the artificial agents are rated to be similar to      Wallach, W., & Allen, C. (2008). Moral Machines: Teaching
a human.                                                                    Robots Right from Wrong. Oxford University Press.
  The results of the two experiments show that a utilitarian          Waytz, A., Gray, K., Epley, N., & Wegner, D. M. (2010). Causes
moral action undertaken by an agent has a strong effect no                  and consequences of mind perception. Trends in Cognitive
only on the evaluation of moral agency but also other mental                Sciences, 14(8), 383–388.
capacities.                                                           Ward, A. F., Olsen, A. S., & Wegner, D. M. (2013). The Harm-
  Another goal of the study was to explore the moral                        Made Mind: Observing Victimization Augments
judgments about utilitarian moral action undertaken by those                Attribution of Minds to Vegetative Patients, Robots, and
three agents. It turns out that there are no differences in the             the Dead. Psychological Science, 24(8), 1437–1445.
moral judgments for the human or the artificially created
agents. This result is in line with the finding that similar
agency and moral agency is ascribed to the human and to the
artificial agents.
  In conclusion, our results provide support for the idea that
some mental states and capacities (especially cognitive ones)
are more readily ascribed to non-human agents; while other
mental states (related to conscious experience) are ascribed
                                                                  2241

