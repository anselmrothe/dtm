                   Semantic vector evaluation and human performance on a new
                                                    vocabulary MCQ test
                                          Joseph P. Levy (j.levy@roehampton.ac.uk)
                                       Department of Psychology, University of Roehampton
                                                               London, UK
                                     John A. Bullinaria (j.a.bullinaria@cs.bham.ac.uk)
                                      School of Computer Science, University of Birmingham,
                                                            Birmingham, UK
                          Samantha McCormick (Samantha.McCormick@roehampton.ac.uk)
                                       Department of Psychology, University of Roehampton
                                                               London, UK
                             Abstract                                 to measure how well they perform in a vocabulary multiple
  Vectors derived from patterns of co-occurrence of words in
                                                                      choice question (MCQ) test where a participant is asked to
  large bodies of text have often been used as representations of     choose which of a number of choice words is closest in
  some aspects of the meanings of different words. Generally,         meaning to a stem word (not to be confused with a
  the distance between such vectors is used as a measure of the       morphological stem). Often this amounts to choosing a
  semantic similarity between the word meanings they                  (“key”) synonym, or the word that is closest to being a
  represent. One important way of evaluating the performance          synonym, from the other choice words that act as
  of these vectors has been to use them to answer vocabulary          distractors. For human participants, these tests are used to
  multiple choice questions (MCQs) where the participant is
  asked to judge which of several choice words is closest in          measure vocabulary knowledge. Such tests are ideal
  meaning to a stem word. The existing vocabulary MCQ tests           methods to use to evaluate co-occurrence techniques which
  used in this way have been very useful but there are some           construct semantic vectors for each word such that their
  practical problems in their use as general evaluation               distances are related to how substitutable the words are for
  measures. Here, we discuss why such tests remain useful             each other. A linguistic intuition would be that if two words
  evaluation measures, introduce a new vocabulary test,               are substitutable for each other in everyday language then
  evaluate several current sets of semantic vectors using the
                                                                      they are synonyms or at least very closely semantically
  new test and compare their performance to human data.
                                                                      related.
                                                                         Landauer and Dumais (1997) used the performance of
   Keywords: Distributional semantics; vocabulary MCQ.                their LSA (Latent Semantic Analysis) method on the retired
                                                                      items of a once commercially available test of English
                         Introduction                                 vocabulary the TOEFL (Teaching of English as a Foreign
There are many potential applications for a method that can           Language). They reported a performance of 64.4%, well
reliably form the basis for measuring the semantic distance           above chance and equivalent to acceptable performance for
between words or concepts. Many methods achieve this by               entry to a US University. This MCQ test has been the most
placing each word/concept in a multidimensional space                 widely used vocabulary test to date for evaluating
where the dimensions are defined by the way in which                  distributional semantic vectors1. Turney (2001) also
words co-occur in corpora of real language use (Schütze,              describes a commercially available test, the ESL, and this
1993; Bullinaria & Levy, 2007; Turney & Pantel, 2010).                has been used to evaluate such methods. Another candidate
The simplest such methods place a target word in a space              MCQ test would be the one used by Adelman et al. (2014)
defined by the count of how many times this word co-occurs            from Shipley (1940). This is a 40 item vocabulary MCQ
with other words in the corpus with each of these context             using some now somewhat archaic US English usages. It
words defining a dimension. The resulting semantic vectors            has the advantage of being freely available in the appendix
may also be smoothed by some kind of dimensionality                   of a historic journal article. Such tests are valuable
reduction technique. Most current techniques retain only a            evaluation measures for semantic representations in that
small proportion of the number of initial dimensions (often           they are independently designed to measure the performance
300) and refer to these dense vector sets as “word                    of human participants. However, they have a number of
embeddings” (e.g., Mikolov et al., 2013). Two words are               disadvantages if they are used as the only evaluation:
then judged to be semantically similar if the vector distance
between them is small.
  Various validation approaches have been used, but a                    1
                                                                           The ACL Wiki lists the performance of some key approaches:
particularly convenient way of evaluating such techniques is          https://www.aclweb.org/aclwiki/index.php?title=TOEFL_Synony
                                                                      m_Questions_(State_of_the_art)
                                                                  2549

   •    They are at least relatively commercially sensitive as     relative question difficulty and that word familiarity (and
making a real-word test freely available would render it           proxies for this such as level of education or experience
useless as a fair measure of human vocabulary knowledge.           with English in the case of the data described here) is likely
We have always found that researchers who report the use           to dominate models of individual differences in
of such tests have been helpful and generous in making the         performance on these tasks. If a participant has never or
test items available to other researchers but, inevitably, the     very rarely come across a stem or synonym then, apart from
commercial/practical sensitivity of the items is an obstacle       the possibility of sensitivity to form-meaning symbolism
for widespread open publication of them. This can prevent          (e.g., Levy & Thompson, 2008; Monaghan et al., 2014),
the clear reporting of slight changes made to the items due        they are unlikely to perform well on test items containing
to low frequency word forms (or USA/UK spelling variants)          these words. However, there remains the strong possibility
not appearing, or not appearing frequently enough, in the          that semantic distances between stem and synonym, stem
corpora used, which can lead to papers reporting results of        and distractors and synonym and distractors will affect
slightly different tests.                                          question difficulty and the choice of distractor when an
   •    They are relatively small as they are designed to be       MCQ item is answered incorrectly. Semantic vector
completed in a reasonably short period of time by the              techniques are a good resource for calculating these
human testees.                                                     distances. Thus, vocabulary MCQs are useful measures for
   •    The individual questions may not be uniform in             evaluating the competence of semantic vector techniques,
terms of difficulty or kind of semantic relationship being         and semantic vectors are likely to be components of any
tested. A question may test knowledge of near-synonymy or          complete model of human vocabulary MCQ performance.
one of a degree of some other kind of semantic similarity             In the rest of this paper, we outline how we constructed a
such as category membership.                                       new 200 item vocabulary MCQ test, show how well three
   In this paper, we describe a new 200 item MCQ                   recent methods for generating distributional semantics
vocabulary which we will make freely available. The test           vectors perform on the test, and compare the performance of
has been constructed using the lexicographical judgements          the semantic vectors with human performance on the same
implicit in the construction of the noun entries in WordNet        test items. We intend to make the MCQ items and human
(Fellbaum, 1998). Half of the stem words in the test are           data freely available as a research resource.
relatively high frequency (in the psycholinguistically
relevant sense) and half are low frequency. Word frequency                Construction of New Vocabulary Test
is a dominant lexical variable for human language                  We constructed a set of 200 vocabulary MCQ items. This is
processing and especially so in instruments, such as this          larger than most of the evaluation benchmarks that have
one, that are designed to measure vocabulary knowledge.            been suggested (allowing the set to be potentially split into
The 200 MCQ set is large enough to be performed                    independent training and testing components for reliable
comfortably by human participants and to be split into             model selection purposes) but still a manageable number of
subsets for training and testing when using machine learning       items for individual human participants.
techniques.                                                            The words in the MCQs were chosen by using the entries
   As noted above, vocabulary MCQ tests have frequently            for nouns in WordNet. All words considered for selection
been used as evaluation measures for distributional semantic       appeared in both the SUBTLEX-UK (Van Heuven et al.,
vectors. However, some of the most recent methods for              2014) and WordNet databases, and were dominantly tagged
generating such semantic vectors (e.g., Mikolov et al., 2013;      in their noun form in both databases. SUBTLEX-UK is a
Pennington et al., 2014) have emphasised evaluation using          database constructed from BBC TV subtitling records and
sets of analogy problems. We would argue that both                 so this ensured that the words chosen were in common
vocabulary and analogy tests are important in evaluating the       usage in the UK.
semantic competence of distributional semantic vectors, as            We chose to generate stem-synonym pairs by using the
well as being useful in models of human semantic                   synsets in WordNet because this gives us an independent
performance. Here, we therefore evaluate three promising           benchmark for lexical semantic relations. By dividing the
recent semantic vector techniques using our new vocabulary         MCQ items into two subsets where one has relatively high
test.                                                              stem frequencies and the other has relatively low ones, the
   In constructing our new test, we use WordNet (Fellbaum,         vocabulary test controls one of the most important
1998), a freely available lexicographic database organised         influences on human linguistic performance.
around lists of synonyms (synsets) for the different senses           The potential candidate list of stem words was divided
of each word in the database. This allows us to use the            into lower-frequency (LF) and higher-frequency (HF)
independent linguistic judgements of the WordNet team as a         subsets using the “Zipf” scale (van Heuven et al., 2014),
standard for competence in tests of synonymy judgements            which is based on the log10 transform of word frequency.
on the vocabulary MCQ items that we construct.                     Those authors argue that this scale is a better way to control
   We consider that MCQ vocabulary tests are interesting           frequency in a psycholinguistically relevant way than
psychological tasks in their own right. It is likely that word     frequency per million word (fpmw) counts. For example,
frequency measures will dominate any quantitative model of         using these counts to select stimuli results in an
                                                               2550

underrepresentation of relatively low frequency words that         synonym and distractor frequencies between the low- and
are familiar to human participants.                                high-frequency subsets. The correlation between MCQ
   These candidate lists were randomly sorted. Stems and           question item difficulty (as measured by overall percentage
synonyms (taken from the synsets associated with each stem         correct) and stem SUBTLEX-UK frequency is 0.41,
noun) were selected from this list such that the final HF and      suggesting that factors other than word frequency may be
LF subsets consisted of pairs that were matched for stem           affecting human vocabulary test performance.
word length, synonym frequency and synonym length.                    Three participants achieved overall scores of 97% - six
Hyphenated stem words or synonyms were excluded from               errors from the 200 MCQ items. These few errors were
selection. Three distractor words were selected at random          sometimes made on the same question by more than one
from the remaining nouns in WordNet and SUBTLEX-UK                 participant and were also sometimes also made by the
with a Zipf frequency greater than two. The mean distractor        semantic vector methods.
length and frequency (over the three distractors) was
pairwise matched to the synonym. Mean stem, synonym and                       Semantic Vectors for Evaluation
distractor characteristics are shown in Table 1.                      In addition to testing our new MCQ vocabulary test on
                                                                   human participants, we also used it to evaluate three
 Table 1: Mean Zipf Frequency (F) and word length (L) for          available sets of semantic vectors, all derived from large
     Stem, Synonym (Syn) and Distractor (Distr) words              text corpora but using contrasting methods to capture the
                                                                   patterns of word usage regularities. Our aim here is to
   MCQs      Stem    Stem     Syn    Syn    Distr     Distr        illustrate how well a few recent methods that have been
             F       L        F      L      F         L            most successful on other semantic tasks are able to perform
   LF        3.0     6.3      3.6    6.4    3.6       6.3          on this task, and not to make any claims about optimal
   HF        4.8     6.4      3.7    6.5    3.7       6.6          methods or parameters.
                                                                      We compared the levels of success on the new vocabulary
                                                                   test using three different kinds of semantic vectors that span
  Human Performance on the Vocabulary Test                         the range of approaches available: vectors derived from the
The vocabulary MCQ test was given to 85 monolingual                methods described by Bullinaria & Levy (2012), the
English speaking undergraduate student participants and 77         publicly available semantic vectors that were generated
non-monolingual       students.    Their    performance     is     using one of the word2vec (Mikolov et al., 2013) methods
summarised in Table 2.                                             (available at: https://code.google.com/archive/p/word2vec/)
                                                                   and the GloVe (Pennington, Socher & Manning, 2014)
     Table 2: Performance (% correct) of monolingual and           vectors derived from the co-occurrence matrix from 6, 42
                 non-monolingual participants                      and      840    billion    word     corpora   available     at:
                                                                   http://nlp.stanford.edu/projects/glove/.
             MCQs       mean SD        range                          The Bullinaria & Levy (B&L) vectors are derived from
                          Monolingual                              the ukWaC (Baroni et al., 2009) (2 billion words) corpus by
             All 200    79% 10%        56% - 97%                   counting word co-occurrences in a context window of size
             100 LF     72% 13%        46% - 96%                   one and using those counts to generate a starting matrix of
             100 HF     86% 8%         63% - 98%                   positive pointwise mutual information (PPMI) values for
                                                                   about 50,000 target words and 50,000 context words. This
                       Non-monolingual
                                                                   choice of window size and co-occurrence statistic was
             All 200    71%      10%   49% - 92%                   previously shown to be optimal for a range of semantic
             100 LF     61%      12%   32% - 88%                   tasks (Bullinaria & Levy, 2007) and is now widely used.
             100 HF     81%      10%   52% - 96%                   Singular Value Decomposition (SVD) dimensionality
                                                                   reduction is then used to generate orthogonal matrices U
   Mean performance is high but does not appear to be close        and V and diagonal singular value matrix S such that M =
to ceiling. The very best performance is close to maximal          USVT, and dimensionality reduction is performed by taking
demonstrating that it is possible for humans to achieve a          the principal components of USP = MVSP-1 to produce
close to perfect score.                                            semantic vectors with a dimensionality of 5,000 with the
  The mean scores for monolingual participants are higher          components weighted using a Caron (2001) P value of 0.25.
than that for non-monolingual participants for the complete        These parameter values were optimised so as to perform
MCQ set and the two subsets.                                       well on four different semantic evaluation measures
   As would be expected for human performance,                     including a version of the Landauer & Dumais (1997)
performance for the high frequency stems exceeds that for          TOEFL MCQ vocabulary test, and achieved the current
the low frequency ones with the non-monolingual                    state-of-the art performance on the TOEFL MCQ1 test.
participants demonstrating a larger deficit for low frequency         The word2vec (W2V) vectors were generated using a 100
stems. Clearly, human performance has been affected by our         billion word sample of the Google News dataset. Word2vec
manipulation of stem frequency whilst matching the                 uses supervised learning algorithms to train a simple but
                                                               2551

very large neural network model to predict either which           the WordNet-based MCQ test. The GloVe vectors trained
words will appear in a window around the current word (the        on an 840 billion word corpus comes closest to matching the
context given the current word) or which word will appear         very best performance of human participants. However, all
given the current context words. There are a number of            the vector sets exceed the mean performance of the human
different methods and parameters that can be varied in what       participants by large margins.
amounts to a family of techniques. We made use of the                The LF and HF subsets are distinguished by the
publicly available vectors which have 300 dimensions.             SUBTLEX-UK frequencies of their stem words. The
  The GloVe (G6, G42, G840) vectors were extracted from           frequencies (and word lengths) of synonyms and distractors
the files linked to on the GloVe website. The G6 vectors          were matched between subsets. Unsurprisingly, the human
were generated from a 6-billion-word corpus derived from          participants performed better on the HF subset than on the
Wikipedia. The 42B and 840B vectors were generated from           LF subset, presumably reflecting the association between
42 billion and 840 billion word corpora derived from              word frequency and the familiarity that participants had
Common Crawl archives (obtained by an automated process           with the stem words. However, in three of the five sets, the
of systematically browsing the web). All the GloVe vectors        semantic vectors performed better on the LF subset than the
used here have 300 dimensions. The GloVe method uses              HF one. Since even the smallest corpus used for generating
regression modelling to learn semantic vectors from the           the semantic vectors was 2 billion words in size, it is likely
non-zero entries of a word co-occurrence matrix such that         that all the words used in the vocabulary MCQ test were
the dot product between the vectors for a pair of words           sampled a great many times and that this overcame any
equals the logarithm of their probability of co-occurrence.       difference in the reliability of the semantic representations
Pennington et al. (2013) show that their vectors perform          due to frequency differences. For the 2 billion word ukWaC
well on the analogy problem set that was also used to             corpus that we used (by far the smallest of the corpora used
evaluate the word2vec methods.                                    to train the methods compared here), Table 4 gives the
  Levy, Goldberg and Dagan (2015) have argued that the            frequency statistics for the vocabulary MCQ test synonyms.
three broad semantic vector techniques used here have             There is a very large variance in frequency values within
similar levels of overall performance when appropriately          each subset. The mean frequency for the LF subset is 3993
tuned.                                                            with the lowest stem frequency being sampled 98 times in
  17 of the 1,000 words within the 200 MCQs did not               the corpus. The correlation between the log10 ukWaC corpus
appear in the word2vec vector sets due to differences in UK       synonym frequencies and their SUBTLEX-UK Zipf
and USA English. For these words, we used the vectors             frequencies is 0.93.
derived from the USA spelling variants. 7 words did not
appear in the GloVe vectors derived from their 6 billion               Table 4: ukWaC frequencies for the stem words in the
word corpus. For these words, we substituted related words                              vocabulary MCQ test
that did appear in the vector set. The other semantic vector
sets contained all the 1,000 words used in this MCQ                    MCQs       mean        SD         range
vocabulary set.                                                        All 200    119,809     198,796    98 – 1,057,891
  Clearly, there are a number of differences in the corpora
                                                                       100 LF     3993        5099       98 – 36,571
and parameters used for the three methods and so this
exercise cannot reliably compare the success of the different          100 HF     235,625     228,725    12,291 – 1,057,891
methods in general, but serves as a comparison of a number
of different off-the-shelf semantic vector sets.                     We suppose that any differences in performance for the
                                                                  semantic vectors on the LF and HF subsets is due to an
       Semantic Vector Performance on the                         inadvertent bias in the distribution of semantic distances
                     Vocabulary Test                              between the MCQ words that is revealed when the statistical
                                                                  reliability related to word frequency differences is made
We compared the performance of the five different vector
                                                                  irrelevant due to very high degrees of corpus sampling. It is
sets on all 200 items and on the LF and HF subsets. Mean
                                                                  likely that there is a higher degree of semantic ambiguity for
performance is summarised in Table 3.
                                                                  the high frequency stems and this may have affected the
     Table 3: Performance (% correct) of the five different       MCQ results. We will explore these issues in further detail
                           vector sets.                           in modelling work in a future paper.
                                                                     The corpora used to train the vector generation methods
        MCQs       B&L     W2V      G6    G42    G840             ranged from 2 to 840 billion words. Although performance
        All 200    91.0    87.0     86.5  89.5   92.0             of the different methods differed by only a few percent, it is
        100 LF     93.0    87.0     86.0  92.0   95.0             notable that the B&L vectors achieved one of the higher
        100 HF     89.0    87.0     87.0  87.0   89.0             scores using a corpus of 2 billion words and that the GloVe
                                                                  vectors achieved higher scores as the corpus size used
                                                                  increased from 6 to 42 and then 840 billion words. The
  All three types of semantic vector perform well but not         B&L method was tuned for previous work on a different
perfectly. None of them match the competence standard of
                                                              2552

vocabulary MCQ test whilst W2V and GloVe have been                2015; Mandera et al., 2017) and human brain activity (e.g.,
reported as having notable success on the rather different        Mitchell et al., 2008; Bullinaria & Levy, 2013) is becoming
domain of analogy problems. It is likely that further             more widespread due to better and more easily available
parameter tuning would increase the scores of all three           semantic vectors. However, it remains unclear how the
methods on this specific task if not in general for other         balance between idealised competence and realistic human
tasks.                                                            performance can be modelled by such techniques and which
   Landauer & Dumais (1997) reported that their LSA               corpora and parameter settings should be used. Some of
semantic vectors performed at a level of 64.4% on a TOEFL         these issues can be addressed in the straightforward arena of
vocabulary MCQ test. This matched the performance of a            vocabulary MCQ tests.
large sample of applicants to colleges in the USA from non-          In this paper, we introduce a vocabulary test, based on
English speaking countries who averaged a score of 64.5%.         WordNet synsets, that is both challenging enough for
This is close to the performance of our non-monolingual           human participants not to be performed at ceiling and large
group on the 100 low frequency MCQs (61.4%). These LSA            and uniform enough to be useful as an evaluation measure
vectors were trained on a much smaller corpus than the            for corpus-derived semantic vectors.
other semantic vectors describe here (6.4 million words) and         We argue that we are within reach of developing
so, arguably, are a better psychological model of attaining a     distributional semantic vectors that can demonstrate
degree of semantic competence from a realistic scale of           competence in the important, if narrow, domain of
linguistic input. However, they did not approach the high         synonymy judgement. However, there is much to be done in
scores required to claim to be a model of idealised semantic      using such representations as components of models that
competence.                                                       can successfully account for actual human performance on
   As noted above, there were several MCQs where the same         these same tasks.
errors were made by some of the highest performing human             Although the semantic vectors we tested were close to an
participants and some of the vector methods. In some of           idealised level of competence, they do not reflect the clear
these cases, it appears that the questions were made more         effect of synonym frequency in the human data. However,
difficult than expected by the random choice of distractor        the ability of the semantic vectors to provide reliable
items leading to one of the distractors potentially being         measures of semantic similarity does show promise for
more closely semantically related to the stem than the            modelling aspects of vocabulary MCQ question difficulty
intended synonym. An example is the intended stem,                that are left after the influence of word frequency is
synonym, distractor1, distractor2, distractor3 MCQ: benefit,      accounted for.
welfare, flask, advantage, lipstick. Here, two of the three          A single set of semantic vectors cannot both account for
highest performing participants and four of the five              idealised semantic competence as defined by a resource
semantic vector methods made an error. Mean human                 such as WordNet and provide a model of average imperfect
accuracy was at below chance level. Because the vector            human performance. For tasks such as vocabulary MCQ
methods have captured synonymy well, they show the                tests, it may be necessary to use semantic vectors as models
potential for automatically measuring vocabulary MCQ              of competence and account for varying performance using
difficulty in terms of semantic similarity over and above the     simple psychologically valid variables such as word
effect of word frequency.                                         frequency or familiarity. Certainly, current methods for the
                                                                  generation of semantic vectors only obtain very high
                         Discussion                               performance scores after training on enormous corpora,
   In cognitive science, we are often interested in building      orders of magnitude larger than any human would
idealised or technologically useful models of intelligent         experience. This may make them poor or partial models of
behaviour as well as psychologically valid ones. Ideally,         human semantic learning but useful technological tools and
these are complementary aims. The development of                  cognitive modelling components. It remains to be seen
methods to generate distributional semantic vectors over the      whether semantic vectors with somewhat lesser levels of
                                                                  competence, perhaps trained on much smaller corpora, are
past 20 years is an interesting example of the possible
tensions between these two types of model. Landauer &             better tools for modelling ordinary levels of human
Dumais (1997) proposed LSA as a model of human                    performance.
semantic performance. LSA was partly validated by its                Vocabulary MCQ tests have been useful measures of
success in matching human non-native performance in the           human word knowledge. In the past they have proved their
TOEFL MCQ test. However, LSA was not capable of                   worth as evaluation measures for semantic vector
approaching perfect performance on the task. Current              generation. They are psychological tasks in themselves and
techniques have achieved very high levels of success on that      we have suggested here that semantic vector methods may
                                                                  allow us to model aspects of question difficulty that are
task and similar ones such as the test proposed here.
However, the amount of data used to train these models is         related to relative semantic distances and this may also
very far in excess of the amount of text that a human could       prove useful for the design of such instruments.
read in a lifetime. The use of distributional semantic vectors       Vocabulary MCQ tests are an important component in the
in the modelling of human performance (e.g., Pereira et al.,      evaluation of representations of lexical semantics. We have
                                                              2553

argued that it is important that such representations can         Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving
account for idealised performance and so reach perfect              distributional similarity with lessons learned from word
performance in these tests. Current techniques have not yet         embeddings. Transactions of the Association for
reached this level of competence. It would also be highly           Computational Linguistics, 3, 211-225.
desirable if these techniques contributed to our ability to       Levy, J. P., & Thompson, N. (2008). Using distributional
model the imperfect performance of human participants on            methods to explore the systematicity between form and
semantic tasks. We argue that vocabulary MCQ tests serve            meaning in British Sign Language. From Associations to
as useful psychological tasks to model. By making our new           Rules - Connectionist Models of Behavior and Cognition -
test freely available along with human data, we hope to             Proceedings of the Tenth Neural Computation and
stimulate further research.                                         Psychology Workshop, 100–111.
                                                                  Mandera, P., Keuleers, E., & Brysbaert, M. (2017).
                    Acknowledgments                                 Explaining human performance in psycholinguistic tasks
   We acknowledge the funding given by our Universities             with models of semantic similarity based on prediction
and thank the student participants in our vocabulary MCQ            and counting: A review and empirical validation. Journal
experiment and our colleagues who facilitated this data             of      Memory        and    Language,      92,    57–78.
collection. We also thank the authors of the ukWaC corpus,          http://doi.org/10.1016/j.jml.2016.04.001
word2vec methodology and GloVe methodology for making             Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013).
their data and code available to us.                                Efficient estimation of word representations in vector
                                                                    space. arXiv preprint arXiv:1301.3781.
                                                                  Mitchell, T. M., Shinkareva, S. V, Carlson, A., Chang, K.-
           Availability of the MCQ word list                        M., Malave, V. L., Mason, R. A., & Just, M. A. (2008).
   We have made the new vocabulary MCQ test words, and              Predicting human brain activity associated with the
various earlier test sets, available to be downloaded from:         meanings of nouns. Science, 320(5880), 1191–5.
http://www.cs.bham.ac.uk/~jxb/corpus.html                           http://doi.org/10.1126/science.1152876
                                                                  Monaghan, P., Shillcock, R. C., Christiansen, M. H., &
                        References                                  Kirby, S. (2014). How arbitrary is language?
Adelman, J. S., Johnson, R. L., McCormick, S. F.,                   Philosophical Transactions of the Royal Society B. 369:
   McKague, M., Kinoshita, S., Bowers, J. S., et al. (2014).        20130299. http://dx.doi.org/10.1098/rstb.2013.0299
   A behavioral database for masked form priming. Behavior        Pennington, J., Socher, R., & Manning, C. D. (2014).
   Research Methods, 46(4), 1052-1067.                              Glove: Global vectors for word representation.
Baroni, M., Bernardini, S., Ferraresi, A., & Zanchetta, E.          Proceedings of the Conference on Empirical Methods in
   (2009). The waCky wide web: A collection of very large           Natural Language Processing (EMNLP), 1532–1543.
   linguistically processed web-crawled corpora. Language         Pereira, F., Gershman, S., Ritter, S., & Botvinick, M.
   Resources       and    Evaluation,     43(3),   209–226.         (2015). A comparative evaluation of off-the-shelf
   http://doi.org/10.1007/s10579-009-9081-4.                        distributed semantic representations for modelling
Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic        behavioural data. Cognitive Neuropsychology, 33, 175-
   representations from word co-occurrence statistics: A            190. http://dx.doi.org/10.1080/02643294.2016.1176907
   computational study. Behavior Research Methods, 39(3),         Shipley,W. C. (1940). A self-administering scale for
   510–526.                                                         measuring intellectual impairment and deterioration. The
Bullinaria, J. A., & Levy, J. P. (2012). Extracting semantic        Journal of Psychology, 9, 371–377.
   representations from word co-occurrence statistics: stop-      Schütze, H. (1993). Word space. In: S. J. Hanson, J. D.
   lists, stemming, and SVD. Behavior Research Methods,             Cowan & C. L. Giles (Eds.) Advances in Neural
   44(3), 890-907.                                                  Information Processing Systems 5, 895-902. San Mateo,
Bullinaria, J. A., & Levy, J. P. (2013). Limiting factors for       CA: Morgan Kauffmann.
   mapping corpus-based semantic representations to brain         Turney, P.D. (2001). Mining the Web for synonyms: PMI-
   activity.       PLoS       ONE,        8(3),      e57191.        IR versus LSA on TOEFL. Proceedings of the Twelfth
   http://doi.org/10.1371/journal.pone.0057191                      European Conference on Machine Learning (ECML-
Caron, J. (2001). Experiments with LSA scoring: Optimal             2001), Freiburg, Germany, pp. 491-502.
   rank and basis. In: M. W. Berry (Ed.), Computational           Turney, P. D., & Pantel, P. (2010). From frequency to
   Information Retrieval, 157-169. Philadelphia, PA: SIAM.          meaning: Vector space models of semantics. Journal of
Fellbaum, C. (1998). WordNet: An electronic lexical                 Artificial    Intelligence   Research,    37,    141–188.
   database. Cambridge, MA: MIT Press..                             http://doi.org/10.1613/jair.2934
Landauer, T. K., & Dumais, S. T. (1997). A solution to            Van Heuven, W. J., Mandera, P., Keuleers, E., & Brysbaert,
   Plato's problem: The latent semantic analysis theory of          M. (2014). SUBTLEX-UK: A new and improved word
   acquisition, induction, and representation of knowledge.         frequency database for British English. The Quarterly
   Psychological Review, 104(2), 211.                               Journal of Experimental Psychology, 67(6), 1176-1190.
                                                              2554

