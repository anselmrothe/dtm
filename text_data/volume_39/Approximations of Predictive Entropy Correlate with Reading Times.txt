  Approximations of Predictive Entropy Correlate with Reading Times
                                Marten van Schijndel (van-schijndel.1@osu.edu)
                                                  Department of Linguistics
                                                  The Ohio State University
                                      William Schuler (schuler.77@osu.edu)
                                                  Department of Linguistics
                                                  The Ohio State University
                          Abstract                                2008), has also demonstrated predictive processing ab-
                                                                  sent a prediction violation, but the present work demon-
   The lexical frequency of an upcoming word affects read-
   ing times even when the upcoming word is masked from           strates that such an effect is also observable in a broad-
   readers (Angele et al., 2015). One explanation for this        coverage self-paced reading corpus such as can be col-
   observation is that readers may slow down if there is high     lected via Mechanical Turk. Finally, Roark, Bachrach,
   uncertainty about upcoming material. In line with this
   hypothesis, this study finds a positive correlation be-        Cardenas, and Pallier (2009) have previously shown that
   tween predictive entropy and self-paced reading times.         the entropy of upcoming syntactic categories influences
   This study also demonstrates that such predictive en-          self-paced reading times, but their entropy measure is ex-
   tropy can be effectively approximated by the surprisal
   of upcoming observations and that this future surprisal        tremely expensive to compute, they used a much smaller
   estimate is more predictive of reading times when the          corpus,1 and they did not find an influence of upcoming
   grammar is more granular, which would be prohibitively         lexical uncertainty on reading times, unlike the present
   expensive for predictive entropy. These results suggest
   readers engage in fine-grained predictive estimations of       work.
   certainty about upcoming lexical and syntactic material,          In addition, this work demonstrates that surprisal
   that such predictions influence reading times, and that        (Hale, 2001; Levy, 2008), typically only used to esti-
   estimating that uncertainty can be done less expensively
   and more robustly with information-theoretic surprisal.        mate responses to observed stimuli, can be used to quan-
   Keywords: Self-Paced Reading; Information Theory;              tify predictive influences as well. From a computational
   Language Modeling; Corpus Studies                              perspective, this work provides an inexpensive way to
                                                                  estimate the uncertainty experienced by readers, which
                       Introduction                               will allow future studies to test the cognitive plausibility
The lexical frequencies of upcoming words affects read-           of various grammars and parsing algorithms, providing
ing times even when the upcoming word is masked from              a tool with which to probe predictive human sentence
readers (Angele et al., 2015). Angele et al. suggest that         processing outside of highly constraining experimental
the driving factor behind their result may be anticipation        stimuli.
of upcoming difficulty. For example, a less constrain-
ing context (i.e. less predictable upcoming words) may                                   Background
produce slower reading. This study uses information-              Angele et al. (2015) wanted to test whether lexical suc-
theoretic entropy to test their hypothesis and to investi-        cessor effects (influences of upcoming material) could
gate the level of linguistic detail predicted by readers.         be elicited even when readers were unable to view the
   This work is scientifically important because it uses          upcoming words. They used a moving mask to hide
a large self-paced reading corpus to show that reading            upcoming words from readers but still found that the
times are influenced both by uncertainty over upcom-              trigram predictability of the next hidden word was a
ing syntactic constructions and by uncertainty over up-           significant predictor of reading times. Angele et al.
coming lexical items, which supports the hypothesis of            (2015) hypothesized that readers may anticipate upcom-
Angele et al. (2015) that anticipation of upcoming diffi-         ing difficulty and slow down. That is, an unconstrained
culty influences reading times. While previous work has           context with several plausible continuations might pro-
found evidence of prediction during language process-             duce slower reading (due to each continuation’s low pre-
ing through responses to violated predictions (Wicha,             dictability) than a highly constraining context with a
Moreno, & Kutas, 2004; Van Berkum, Brown, Zwitser-                smaller number of plausible continuations. To test this
lood, Kooijman, & Hagoort, 2005; Fine, Jaeger, Farmer,            hypothesis, we use information-theoretic entropy to pre-
& Qian, 2013; DeLong, Troyer, & Kutas, 2014), the                 dict reading times.
present work demonstrates that the influence of predic-              Under information theory (Shannon, 1948), the en-
tion can be reliably detected in reading times prior to           tropy (H) of a random variable (X) is defined by the
any violation of that prediction. Other work, for exam-           component probabilities of each possible value (x) of that
ple using a visual world paradigm (Altmann & Kamide,
                                                                      1
1999; Kamide, Altmann, & Haywood, 2003; Ito & Speer,                    The corpus in this work is about 25 times larger.
                                                              1260

variable:                     X                                     failure of lexical entropy to predict reading times may
                  H(X) = −         P(x) log P(x)             (1)    be due to the fact that their grammar was trained on
                              x∈X                                   the relatively small Brown portion of the Penn Treebank
In the case of language processing, the possible values             (Marcus, Santorini, & Marcinkiewicz, 1993), so their lex-
are words that have yet to be observed, and entropy                 ical probabilities may not have been robust enough.
is typically computed from the conditional probability                 It is interesting to note that ‘single-step prediction’
of each possible value given the observations that have             was defined slightly differently for these two sets of au-
already been made.                                                  thors. Roark et al. (2009) define it as a prediction over
   Linzen and Jaeger (2015) distinguished single-step               the next word in a lexical sequence, while Linzen and
predictive entropy (uncertainty about the next process-             Jaeger (2015) define it as a prediction over the next syn-
ing step) from full entropy (uncertainty about the rest             tactic category (e.g., noun phrase) that will branch from
of the sentence). Since Angele et al. (2015) found that             a partial derivation ending in a verb phrase. To avoid
lexical frequency successor effects were only dependent             making a commitment as to the particular parsing strat-
on the word following a fixation, the present work is               egy adopted by readers, this paper will use the definition
concerned with single-step predictive entropy. Linzen               of ‘single-step prediction’ from Roark et al. (2009) to
and Jaeger (2015) found that when single-step predic-               mean uncertainty about the next lexical observation.
tive entropy was computed over upcoming syntactic con-
stituents based on verb subcategorization biases, it was                                        Data
not predictive of self-paced reading times. However, they           This study makes use of the Natural Stories self-paced
hypothesize that the fit of entropy may improve when                reading corpus (Futrell et al., in prep). The corpus is
computed over finer-grained categories (they only com-              a set of 10 texts (485 sentences) written to sound flu-
puted probabilities for 6 subcategorization classes). The           ent but still containing many low-frequency and marked
results in Analysis 4 of this paper support their hypoth-           syntactic constructions. The sentences within each text
esis.                                                               were presented in order, and self-paced reading time data
   Roark et al. (2009) defined two variants of single-step          was collected from 181 native English speakers. Reading
predictive entropy to distinguish syntactic uncertainty             times were excluded if they occurred at the beginning or
from lexical uncertainty. Syntactic entropy is computed             end of a sentence, or if they were less than 100 ms or
over the conditional probability of each preterminal (p)            greater than 3000 ms. Approximately one third of the
in the grammar (G) given the previously observed lexical            sentences (255,554 events) were used for exploration and
sequence (w1..i−1 ):                                                two thirds of the sentences (512,469 events) were used as
                                                                    a confirmatory partition for significance testing to reduce
            1            def
    SynHG     (w1..i−1 ) =                                          the risk of false positives due to multiple comparisons.
              X                                                     All significance results reported in this paper are from
          −        PG (pi | w1..i−1 ) log PG (pi | w1..i−1 ) (2)
                                                                    the confirmatory partition.
             pi ∈G
Syntactic entropy is computed in practice by generating                                        Models
all possible syntactic derivations2 that can generate each          This study fits reading times using linear mixed effects
possible upcoming word (wi ) in the vocabulary (V ) and             models computed with the lme4 (version 1.1-7) R pack-
then subtracting from each derivation’s probability the             age (Bates, Maechler, Bolker, & Walker, 2014). All mod-
emission probability of generating wi from the chosen               els include a baseline of fixed effect predictors for word
preterminal (pi ).                                                  length, sentence position, and 5-gram surprisal.3 The
   Lexical entropy is computed over the conditional prob-           models also include random intercepts for each word,
ability of each possible upcoming lexeme, given the pre-            each subject, and each subject/sentence pair. The last
viously observed lexical sequence:                                  random intercept corrects for the fact that multiple non-
                        def
                                                                    independent observations are drawn from each sentence.
           1
   LexHG     (w1..i−1 ) =                                           Finally, each model includes by-subject random slopes
                                                                    for all the fixed effects. All predictors were z-transformed
             X
         −         PG (wi | w1..i−1 ) log PG (wi | w1..i−1 ) (3)
            wi ∈V
                                                                    prior to fitting. Significance values for each predictor
                                                                    were obtained using a likelihood ratio test between two
   Roark et al. (2009) found that syntactic entropy was
                                                                        3
predictive of self-paced reading times but that lexical en-               5-gram surprisal predicts conditional frequency effects
                                                                    based on n-gram co-occurrence counts. Previous work has
tropy was not, which we were able to replicate on the cor-          shown that 5-gram frequency controls are sufficiently able to
pus in this study as well. Roark et al. suggested that the          control for frequency effects that syntactic frequency controls
                                                                    are sometimes unable to predict reading times over them (van
    2
      In fact, the number of possible syntactic derivations is      Schijndel & Schuler, 2016), so 5-grams create a strong base-
constrained by a very large beam.                                   line with which to test other frequency influences.
                                                                1261

mixed models: one of which contained both a by-subject             where wi is the current lexical item, w1..i−1 is the se-
random slope and a fixed effect for the predictor of in-           quence of previously observed lexical items and V is the
terest, and the other of which omitted the fixed effect            vocabulary of the language.
for that predictor.                                                   Therefore, surprisal is a single sample from the con-
                                                                   ditional probability distribution over which single-step
                          Analyses                                 lexical entropy is computed, where the sampled observa-
Analysis 1: Single-Step Predictive Entropy                         tion is the occurrence that ultimately is observed. Over
First, we test whether the original finding of Roark et            several trials, then, future surprisal should approximate
al. (2009) that syntactic predictive entropy positively            entropy since each observed occurrence should happen
correlates with reading times holds up on the Natural              proportionately to its expected occurrence frequency. As
Stories corpus (Futrell et al., in prep). We compute               a moving window self-paced reading corpus, participants
single-step predictive syntactic and lexical entropy us-           were physically unable to see upcoming words, similar to
ing the Roark (2001) top-down incremental parser. Our              the masked condition used by Angele et al. (2015).
findings are consistent with those of Roark et al. (2009):            To test surprisal as an approximation of entropy, we
syntactic entropy has a significant positive effect on self-       use the Roark (2001) parser’s estimate of surprisal of
paced reading times in the Natural Stories confirmatory            each observation to predict the reading time of the pre-
partition over the baseline model (β̂ = 4.53, σ̂ = 0.54,           ceding observation. This measure (future surprisal )
p-value < 0.001), and lexical entropy is not a significant         also has a significant positive effect on reading times
predictor of reading times.                                        (β̂ = 4.96, σ̂ = 0.63, p-value < 0.001). This measure
   As Roark et al. (2009) point out, the lack of predic-           may be thought of as an aggregate approximation to en-
tivity of lexical entropy may stem from the sparseness of          tropy, whereas the lexical entropy output by the Roark
the training data. Unfortunately, computing predictive             (2001) parser may be thought of as a point-wise approx-
entropy is very expensive since it requires predictively           imation to entropy. That is, Roark lexical entropy ap-
running the parser over a large set of hallucinated obser-         proximates the true lexical entropy for each new obser-
vations whose cardinality is the size of the vocabulary            vation as the weighted average of the conditional proba-
for for each actual observation. Therefore, meaningfully           bility distribution at that point according to the parser’s
increasing the vocabulary is not generally practical.4             grammar, while future surprisal approximates the true
                                                                   lexical entropy over the entire corpus (aggregated over
Analysis 2: Surprisal as Entropy                                   all observations) by sampling from the conditional prob-
Approximation                                                      ability distribution for each observation. The fact that
Angele et al. (2015) found that the trigram surprisal of           future surprisal is able to fit reading times more con-
an upcoming word is predictive of reading times and                sistently than point-wise lexical entropy gives hope that
speculated that such an effect could be driven by uncer-           this less expensive aggregate approximation of entropy
tainty over future events, so this section tests whether           is a more robust means of computing entropy than a
the predictive entropy effect observed in Analysis 1 can           point-wise approximation.
be approximated by the PCFG surprisal of the upcoming
                                                                   Analysis 3: N -grams as Better Entropy
word.
                                                                   Approximation
   Roark (2011) showed that single-step predictive lexi-
cal entropy is mathematically equivalent to the expected           Since the Roark (2001) parser computes surprisal based
value of total surprisal S:                                        on a relatively small and coarse-grained Penn Treebank
                                                                   grammar, the previous results may be skewed by the
                            def
         SG (wi , w1..i−1 ) = −log PG (wi | w1..i−1 )      (4)     small amount of training data. In order to obtain con-
                                                                   ditional probabilities based on more data, we use a 5-
         1                                                         gram back-off model computed with the KenLM toolkit
  LexHG    (w1..i−1 )
                                                                   (Heafield, Pouzyrevsky, Clark, & Koehn, 2013) on the
      def X
       =        −PG (wi | w1..i−1 ) log PG (wi | w1..i−1 ) (5)     Gigaword 4.0 corpus (Graff & Cieri, 2003), which con-
          wi ∈V                                                    sists of 2.96 billion words from English newswire text.
          X                                                        Again, the 5-gram surprisal of each word was used to pre-
      =        PG (wi | w1..i−1 ) SG (wi , w1..i−1 )       (6)
                                                                   dict the reading time of the preceding word. Similar to
         wi ∈V
                                                                   future Roark surprisal, future 5-gram surprisal has a sig-
      = E[SG (wi , w1..i−1 )]                              (7)     nificant positive correlation to reading times (β̂ = 4.49,
   4
     An alternative to the approach taken in this paper would      σ̂ = 0.57, p-value < 0.001), and when future 5-gram sur-
be to maintain a constant vocabulary size but to train the         prisal is in the model, future Roark surprisal ceases to
conditional probabilities of that vocabulary over a much           be a significant predictor of reading times.
larger training set. Such an approach would only help if
the weakness of lexical entropy is due to poor probability            This result aligns with work by van Schijndel and
estimates rather than to unknown words.                            Schuler (2016) who found that future PCFG surprisal,
                                                               1262

computed with a Penn Treebank PCFG, is an effective                                                    β̂     σ̂      t
predictor of reading times in eye-tracking, but that it            Syntactic Entropy                4.53  0.54    8.36
ceased to be predictive when future n-gram surprisal was           Future Roark Surprisal           4.96  0.63    7.85
included in their model. They also found that future n-            Future 5-gram Surprisal          4.49  0.57    7.89
gram surprisal was only predictive for one or two words            Future Fine PCFG Surprisal       4.10  0.74    5.58
following a fixation, similar to the finding of Angele et
al. (2015) that only the frequency of the word following         Table 1: Effect sizes for each predictor of interest over
a fixation was predictive of reading times.                      the baseline described in the Models section. Each pre-
                                                                 dictor was tested over the baseline factors and all predic-
Analysis 4: Fine-Grained Syntactic                               tors listed above it in the table. Future Roark Surprisal
Prediction                                                       is not significant once Future 5-gram surprisal is added.
Although future n-gram surprisal seems to account for a
lexical entropy effect, it is unable to account theoret-         and predictive entropy are consistent with that hypoth-
ically for the effect of Roark syntactic entropy, since          esis and suggest that, in particular, readers slow due to
n-gram surprisal reflects lexical probabilities and syn-         increased probabilistic uncertainty over upcoming mate-
tactic entropy reflects syntactic probabilities (without         rial.
lexical emission probabilities). However, future Roark              Previous studies have claimed that a positive corre-
PCFG surprisal using the default set of Penn Treebank            lation between entropy and reading times would indi-
syntactic categories was unable to predict reading times         cate that there is a competition cost between multiple
when future n-gram surprisal was in the model. Pre-              parse hypotheses (Linzen & Jaeger, 2015), but this is not
vious work on predictive processing has suggested that           the only possible explanation for such a correlation. For
predictions can be relatively fine-grained (Luke & Chris-        example, similar reasoning to the Uniform Information
tiansen, 2015; Kim & Lai, 2012), so this section explores        Density hypothesis (UID; Jaeger, 2010) might apply to
whether humans predict upcoming material with fine-              readers. That is, if readers have more uncertainty about
grained syntactic specificity.                                   upcoming material, they may anticipatorily slow their
   Whereas the above experiments used the Roark (2001)           reading in order to better process the less expected in-
parser with the default Penn Treebank tag set, this sec-         formation (reducing their expected per-millisecond sur-
tion uses the van Schijndel, Exley, and Schuler (2013)           prise to channel capacity). If, instead, readers are rea-
parser, which computes surprisal using the Petrov, Bar-          sonably confident about what words they are about to
rett, Thibaux, and Klein (2006) latent-variable grammar          encounter, they may speed up in order to maximize the
computed from sections 2-21 of the Wall Street Jour-             per-millisecond informativity of their observations. This
nal portion of the Penn Treebank and thereby achieves            sort of tuning may be exaggerated in the moving win-
higher parsing accuracy than the Roark parser (van Schi-         dow self-paced reading paradigm, where readers will be
jndel et al., 2013). The latent-variable grammar is de-          unable to regress if they speed past an unexpected ob-
rived from a split-merge algorithm that creates fine-            servation, which could be why previous work using eye
grained subcategory tags from the basic Penn Treebank            tracking has only been able to find an effect of future n-
category tags. For this experiment, the grammar under-           gram surprisal on reading times (Angele et al., 2015; van
went 5 split-merge operations to obtain optimally tuned          Schijndel & Schuler, 2016), while the present self-paced
tags, following the recommendations of Petrov et al.             reading study also found an effect for future PCFG sur-
   When future surprisal is computed with a finer-               prisal.
grained tag set, it is able to obtain a significant positive        The fact that both future 5-grams and future PCFG
correlation with reading times, even in the presence of          surprisal are predictive of reading times suggests that
future 5-gram surprisal and syntactic entropy (β̂ = 4.10,        predictions of upcoming difficulty are being made both
σ̂ = 0.74, p-value < 0.001).                                     about lexical items and syntactic constructions. Sur-
                                                                 prisal is computationally much less expensive than en-
                       Discussion                                tropy, and therefore it can provide samples from a much
Much previous psycholinguistic and neurolinguistic work          finer-grained conditional probability distribution over
has shown that prediction plays a role in language pro-          possible analyses than would be practical for entropy
cessing (DeLong et al., 2014; Kuperberg & Jaeger, 2015).         calculation.
Angele et al. (2015) observed that even when upcom-                 The present results show that future latent-variable
ing material is masked, its predictability can affect read-      PCFG surprisal can fit reading times even when the
ing times. They suggest that their observation is likely         coarser Roark et al. (2009) surprisal and lexical entropy
driven by readers predicting difficult material and slow-        cannot, which suggests that humans predict upcoming
ing in anticipation of it. The findings in this paper of         material at a relatively fine-grained level (both syntac-
a positive correlation between self-paced reading times          tic and lexical) as suggested by previous work (Luke
                                                             1263

& Christiansen, 2015; Kim & Lai, 2012). These re-               coverage correlation of fine-grained predictive entropy to
sults further indicate that the fit of entropy to reading       self-paced reading times.
times improves as the granularity of the grammar be-
comes finer, which supports the hypothesis of Linzen and                              Conclusion
Jaeger (2015) that their subcategorization entropy was          This paper has replicated previous findings that single-
likely too coarse-grained to reveal entropy’s influence.        step predictive entropy is positively correlated with self-
   The finding that Roark syntactic entropy retains its         paced reading times and presented new results that show
reading time predictivity in the presence of future 5-          this correlation can be inexpensively approximated using
gram surprisal and future latent-variable surprisal sug-        both future n-gram surprisal and future latent-variable
gests that humans estimate certainty about upcoming             PCFG surprisal. The present results also demonstrate
parses based on multiple samples from the distribution          that such approximations improve as the granularity of
over upcoming observations. Such a finding is consistent        the approximation increases. By showing that greater
with parallel models of sentence processing but may be          uncertainty over upcoming words and syntactic con-
problematic for serial processing models. Another inter-        structions slows reading times, these results support the
pretation of this finding is that a point-wise entropy ap-      hypothesis of Angele et al. (2015) that anticipation of
proximation is more stable and so can serve as a back-off       upcoming difficulty affects reading.
for the less stable but more nuanced aggregate approxi-
mations provided by both the n-gram and latent-variable                         Acknowledgements
surprisal models. It is left to future work to differentiate    Thanks to the computational linguistics and cognitive
between these two possibilities.                                modeling discussion groups at OSU (Clippers and CaCL)
   It may seem strange that total latent-variable surprisal     for helpful feedback on this work. This work was sup-
was used in this study instead of syntactic latent-variable     ported by a National Science Foundation Graduate Re-
surprisal (without lexical probabilities) since the goal of     search Fellowship under Grant No. DGE-1343012.
moving beyond future n-gram surprisal was to capture
something of syntactic entropy, which omits lexical emis-                             References
sion probabilities; however, explorations on the devel-         Altmann, G. T. M., & Kamide, Y. (1999). Incremental
opment partition revealed that total surprisal generally              interpretation at verbs: Restricting the domain of
provides better fits to reading times than syntactic sur-             subsequent reference. Cognition, 73 (3), 247–264.
prisal even in the presence of future 5-gram surprisal. In      Angele, B., Schotter, E. R., Slattery, T. J., Tenenbaum,
any case, the goal was not necessarily to approximate                 T. L., Bicknell, K., & Rayner, K. (2015). Do suc-
Roark syntactic entropy but to capture an aspect of the               cessor effects in reading reflect lexical parafoveal
uncertainty experienced by readers, of which Roark lex-               processing? evidence from corpus-based and ex-
ical entropy and Roark syntactic entropy are themselves               perimental eye movement data. Journal of Mem-
approximations. In fact, the consistent correlation be-               ory and Language, 79–80 , 76–96.
tween future surprisal (both n-gram and latent-variable)        Bates, D., Maechler, M., Bolker, B., & Walker, S. (2014).
and reading times compared to Roark lexical entropy                   lme4: Linear mixed-effects models using eigen and
suggests that fine-grained aggregate entropy approxima-               s4 [Computer software manual]. Retrieved from
tion via future surprisal is more robust than the coarser             http://CRAN.R-project.org/package=lme4 (R
but more intuitive point-wise lexical entropy approxima-              package version 1.1-7)
tion output by the Roark (2001) parser.                         DeLong, K. A., Troyer, M., & Kutas, M. (2014). Pre-
   The entropy findings in this paper are distinct from               processing in sentence comprehension: Sensitivity
those in the entropy reduction literature. The Entropy                to likely upcoming meaning and structure. Lan-
Reduction Hypothesis states that readers slow accord-                 guage and Linguistics Compass, 8 (12), 631–645.
ing to the informativity of the words they encounter (as        Fine, A. B., Jaeger, T. F., Farmer, T. A., & Qian, T.
measured by a decrease in entropy; Hale, 2006). It is                 (2013). Rapid expectation adaptation during syn-
possible that the two effects are independent and that                tactic comprehension. PloS ONE , 8 (10), 1–18.
people slow down before areas of greater uncertainty,           Futrell, R., Gibson, E., Tily, H., Vishnevetsky, A., Pi-
while also slowing down due to larger information gains.              antadosi, S., & Fedorenko, E. (in prep). Natural
These effects are not necessarily mutually exclusive be-              stories corpus.
cause entropy reduction deals with changes in entropy           Graff, D., & Cieri, C. (2003). English Gigaword
while predictive entropy deals with the overall level of              LDC2003T05 [Computer software manual]. Lin-
uncertainty in a text. That is, an entropy reduction of               guistic Data Consortium.
k may predict the same k · β∆H ms effect on reading             Hale, J. (2001). A probabilistic earley parser as a psy-
times whether the resulting entropy is low or high. In                cholinguistic model. In Proceedings of the second
contrast, the experiments in this paper highlight a broad-            meeting of the north american chapter of the as-
                                                            1264

      sociation for computational linguistics (pp. 159–             Science University.
      166). Pittsburgh, PA.                                    Roark, B., Bachrach, A., Cardenas, C., & Pallier, C.
Hale, J. (2006). Uncertainty about the rest of the sen-             (2009). Deriving lexical and syntactic expectation-
      tence. Cognitive Science, 30 (4), 609–642.                    based measures for psycholinguistic modeling via
Heafield, K., Pouzyrevsky, I., Clark, J. H., & Koehn,               incremental top-down parsing. Proceedings of the
      P. (2013, August). Scalable modified Kneser-Ney               2009 Conference on Empirical Methods in Natural
      language model estimation. In Proceedings of the              Langauge Processing, 324–333.
      51st annual meeting of the association for compu-        Shannon, C. (1948). A mathematical theory of com-
      tational linguistics (pp. 690–696). Sofia, Bulgaria.          munication. Bell System Technical Journal , 27 ,
Ito, K., & Speer, S. R. (2008). Anticipatory effect of              379–423, 623–656.
      intonation: Eye movements during instructed vi-          Van Berkum, J. J. A., Brown, C. M., Zwitserlood, P.,
      sual search. Journal of Memory and Language,                  Kooijman, V., & Hagoort, P. (2005). Anticipating
      58 , 541–573.                                                 upcoming words in discourse: evidence from erps
Jaeger, T. F.          (2010, August).        Redundancy            and reading times. Journal of Experimental Psy-
      and reduction: Speakers manage information                    chology: Learning, Memory, and Cognition, 31 (3),
      density.     Cognitive Psychology, 61 (1), 23–62.             443.
      Retrieved from http://dx.doi.org/10.1016/j               van Schijndel, M., Exley, A., & Schuler, W. (2013). A
      .cogpsych.2010.02.002                                         model of language processing as hierarchic sequen-
Kamide, Y., Altmann, G. T. M., & Haywood, S. L.                     tial prediction. Topics in Cognitive Science, 5 (3),
      (2003). The time-course of prediction in incremen-            522–540.
      tal sentence processing: Evidence from anticipa-         van Schijndel, M., & Schuler, W. (2016). Addressing
      tory eye movements. Journal of Memory and Lan-                surprisal deficiencies in reading time models. In
      guage, 49 (1), 133–156.                                       Proceedings of the computational linguistics for lin-
Kim, A., & Lai, V. (2012). Rapid interactions between               guistic complexity workshop. Association for Com-
      lexical semantic and word form analysis during                putational Linguistics.
      word recognition in context: Evidence from erps.         Wicha, N. Y. Y., Moreno, E. M., & Kutas, M. (2004).
      Journal of Cognitive Neuroscience, 24 (5), 1104–              Anticipating words and their gender: an event-
      1112.                                                         related brain potential study of semantic integra-
                                                                    tion, gender expectancy, and gender agreement in
Kuperberg, G. R., & Jaeger, T. F. (2015). What do
                                                                    spanish sentence reading. Journal of Cognitive
      we mean by prediction in language comprehension?
                                                                    Neuroscience, 16 (7), 1272–1288.
      Language, Cognition and Neuroscience, 1–29.
Levy, R. (2008). Expectation-based syntactic compre-
      hension. Cognition, 106 (3), 1126–1177.
Linzen, T., & Jaeger, T. F. (2015). Uncertainty and
      expectation in sentence processing: Evidence from
      subcategorization distributions. Cognitive Science,
      1–30.
Luke, S. G., & Christiansen, K. (2015). Predicting inflec-
      tional morphology from context. Language, Cog-
      nition and Neuroscience, 1–14.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A.
      (1993). Building a large annotated corpus of En-
      glish: the Penn Treebank. Computational Linguis-
      tics, 19 (2), 313–330.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).
      Learning accurate, compact, and interpretable tree
      annotation. In Proceedings of the 44th annual
      meeting of the association for computational lin-
      guistics (COLING/ACL’06).
Roark, B. (2001). Probabilistic top-down parsing and
      language modeling. Computational Linguistics,
      27 (2), 249–276.
Roark, B. (2011). Expected surprisal and entropy (Tech.
      Rep. No. CSLU-11-004). Portland, OR: Center for
      Spoken Language Processing, Oregon Health and
                                                           1265

