     An information-seeking account of eye movements during spoken and signed
                                                 language comprehension
                   Kyle MacDonald1 (kylem4@stanford.edu), Aviva Blonder2 (aviva.blonder@oberlin.edu),
                  Virginia Marchman1 (marchman@stanford.edu), Anne Fernald1 (afernald@stanford.edu),
                                             Michael C. Frank1 (mcfrank@stanford.edu)
                   1 Department of Psychology Stanford University, 2 Department of Psychology Oberlin College
                              Abstract                                How do we decide where to look? We propose that people
                                                                      modulate their eye movements during language comprehen-
   Language comprehension in grounded contexts involves in-
   tegrating visual and linguistic information through decisions      sion in response to tradeoffs in the value of gathering different
   about visual fixation. But when the visual signal also con-        kinds of information. We test this adaptive tradeoff account
   tains information about the language source – as in the case       using two case studies that manipulate the value of different
   of written text or sign language – how do we decide where to
   look? Here, we hypothesize that eye movements during lan-          fixation locations for language understanding: a) a compari-
   guage comprehension represent an adaptive response. Using          son of processing sign vs. spoken language in children (E1),
   two case studies, we show that, compared to English-learners,      and b) a comparison of processing printed text vs. spoken lan-
   young signers delayed their gaze shifts away from a language
   source, were more accurate with these shifts, and produced a       guage in adults (E2). Our key prediction is that competition
   smaller proportion of nonlanguage-driven shifts (E1). Next,        for visual attention will make gaze shifts away from the lan-
   we present a well-controlled, confirmatory experiment, show-       guage source less valuable than fixating the source of the lin-
   ing that English-speaking adults produced fewer nonlanguage-
   driven shifts when processing printed text compared to spoken      guistic signal, leading people to generate fewer exploratory,
   language (E2). Together, these data suggest that people adapt      nonlanguage-driven eye movements.
   to the value of seeking different information in order to in-
   crease the chance of rapid and accurate language understand-
   ing.                                                                                       Experiment 1
   Keywords:         eye movements;       language   processing;      E1 provides an initial test of our adaptive tradeoffs account.
   information-seeking; American Sign Language
                                                                      We compared eye movements of children learning ASL to
                                                                      children learning a spoken language using parallel real-time
                          Introduction                                language comprehension tasks where children processed fa-
The study of eye movements during language comprehen-                 miliar sentences (e.g., “Where’s the ball?”) while looking at a
sion has provided fundamental insights into the interaction           simplified visual world with 3 fixation targets (a center stim-
between conceptual representations of the world and the in-           ulus that varied by condition, a target picture, and a distracter
coming linguistic signal. For example, research shows that            picture; see Fig 1). The spoken language data are a reanalysis
adults and children will rapidly shift visual attention upon          of three unpublished data sets, and the ASL data are reported
hearing the name of an object in the visual scene, with a             in MacDonald et al. (under review). We predicted that, com-
high proportion of shifts occurring prior to the offset of the        pared to spoken language processing, processing ASL would
word (Allopenna, Magnuson, & Tanenhaus, 1998; Tanen-                  increase the value of fixating on the language source and
haus, Spivey-Knowlton, Eberhard, & Sedivy, 1995). More-               decrease the value of generating exploratory, nonlanguage-
over, researchers have found that conceptual representations          driven shifts even after the target linguistic item began un-
activated by fixations to the visual world can modulate subse-        folding in time.
quent eye movements during language processing (Altmann                  To test this prediction, we present traditional behavioral
& Kamide, 2007).                                                      analyses of first shift Accuracy and RT. We also present
   The majority of this work has used eye movements as a              two model-based analyses. First, we use an exponentially
measure of the output of the underlying language comprehen-           weighted moving average (EWMA) method (Vandekerck-
sion process, often using linguistic stimuli that come from a         hove & Tuerlinckx, 2007) to categorize participants’ gaze
disembodied voice. But in real world contexts, people also            shifts as language-driven or random. In contrast to the stan-
gather information about the linguistic signal by fixating on         dard RT/Accuracy analysis, the EMWA allows us to quan-
the language source. Consider a speaker who asks you to               tify differences in the accuracy of gaze shifts as a function of
“Pass the salt” but you are in a noisy room, making it difficult      when that shift occurred in time. Next, we use drift-diffusion
to understand the request. Here, comprehension can be facil-          models (DDMs) (Ratcliff & Childers, 2015) to quantify dif-
itated by gathering information via (a) fixations to the nonlin-      ferences in the underlying psychological variables that might
guistic visual world (i.e., encoding the objects that are present     drive behavioral differences in Accuracy and RT. For exam-
in the scene) or (b) fixations to the speaker (i.e., reading lips     ple, the DDM uses the shape of both the correct and incorrect
or perhaps the direction of gaze).                                    RT distributions to provide a quantiative estimate of whether
   But, this situation creates a tradeoff where the listener must     higher accuracy is driven by more cautious responding or by
decide what kind of information to gather and at what time.           more efficient information processing.
                                                                  780

                                                                           English linguistic stimuli. All three tasks (Object, Bulls-
                                                                        eye, and Face) featured the same female speaker who used
                                                                        natural child-directed speech and said: “Look! Where’s the
                                                                        (target word)?” The target words were: ball, banana, book,
                                                                        cookie, juice, and shoe. For the Face task, a female native
                                                                        English speaker was video-recorded as she looked straight
                                                                        ahead and said, “Look! Where’s the (target word)?” Mean
                                                                        word length was 0.79 sec, ranging from 0.6 sec to 0.94 sec.
                                                                           ASL and English visual stimuli. The image set consisted
                                                                        of colorful digitized pictures of objects presented in fixed
                                                                        pairs with no phonological overlap (ASL task: cat—bird,
                                                                        car—book, bear—doll, ball—shoe; English tasks: book-
                                                                        shoe, juice-banana, cookie-ball). Side of target picture was
Figure 1: Stimuli for E1 and E2. Panel A shows the layout of            counterbalanced across trials.
the fixation locations for all tasks: the center stimulus, the tar-
get, and the distracter. Panel B shows the five center stimulus         Design and procedure Children sat on their caregiver’s lap
items: a static geometric shape (Bullseye), a static image of           and viewed the task on a screen while their gaze was recorded
a familiar object (Object), a person speaking (Face), a person          using a digital camcorder. On each trial, children saw two im-
signing (ASL), and printed text (Text).                                 ages of familiar objects on the screen for two seconds before
                                                                        the center stimulus appeared (see Fig 1). Then they processed
                                                                        the target sentence – which consisted of a carrier phrase, a tar-
Method                                                                  get noun, and a question – followed by two seconds without
                                                                        language to allow for a response. Participants saw 32 test tri-
Participants Table 1 contains details about the age distri-             als with several filler trials interspersed to maintain interest.
butions of children in all of four samples.                                Coding. Participants’ gaze patterns were coded (33-ms res-
   Spoken English samples. Participants were 80 native,                 olution) as being fixated on either the center stimulus, one
monolingual English-learning children divided across three              of the images, shifting between pictures, or away. To as-
samples. Participants had no reported history of developmen-            sess inter-coder reliability, 25% of the videos were re-coded.
tal or language delay.                                                  Agreement was scored at the level of individual frames of
   ASL sample. Participants were 30 native, monolingual                 video and averaged 98% on these reliability assessments.
ASL-learning children (18 deaf, 12 hearing). All children,
regardless of hearing status, were exposed to ASL from birth            Results and Discussion
through extensive interaction with at least one caregiver fluent        Analysis plan First, we present behavioral analyses of First
in ASL and were reported to experience at least 80% ASL in              shift accuracy and Reaction Time (RT). RT corresponds to
their daily lives. The ASL sample included a wider age range            the latency to shift away from the central stimulus to either
compared to the spoken English samples because this is a rare           picture measured from target-noun onset. Accuracy was the
population.                                                             mean proportion of first gaze shifts that landed on the tar-
Stimuli ASL linguistic stimuli. We recorded two sets of                 get picture out of the total number of shifts. We log trans-
ASL stimuli, using two valid ASL sentence structures for                formed all RTs and used the lme4 R package (Bates, Maech-
questions: 1) Sentence-initial wh-phrase: “HEY! WHERE                   ler, Bolker, & Walker, 2013) to fit mixed-effects regression
[target noun]?” and 2) Sentence-final wh-phrase: “HEY! [tar-            models that included a random intercept for each participant
get noun] WHERE?” Two female native ASL users recorded                  and item. Since children’s age varied across conditions, we
several tokens of each sentence in a child-directed regis-              included age in months as a covariate in all models. All analy-
ter. Before each sentence, the signer produced a common                 sis code can be found in the online repository for this project:
attention-getting gesture. Mean sign length was 1.25 sec,               https://github.com/kemacdonald/speed-acc.
ranging from 0.69 sec to 1.98 sec.                                         Next, we present two exploratory model-based analyses to
                                                                        quantify differences in eye movements across the four sam-
                                                                        ples. First, we use an EWMA method to model changes in
     Task         Mean Age       Min Age      Max Age        n          accuracy as a function of increases in RT. For each RT, the
     ASL              27.90             16           53     30          model generates two values: a “control statistic” (CS, which
     Face             26.00             25           26     24          captures the running average accuracy of first shifts) and an
     Object           31.90             26           39     40          “upper control limit” (UCL, which captures the pre-defined
     Bullseye         26.10             26           27     16          limit of when accuracy would be categorized as above chance
                                                                        level). Here, the CS is an expectation of random shifting to
Table 1: Age distributions of children in Experiment 1. All             either the target or the distracter image (nonlanguage-driven
ages are reported in months.                                            shifts), or a Bernoulli process with probability of success 0.5.
                                                                    781

     A           1.5
                                                                        Behavioral analyses RT. Visual inspection of the Fig 2,
                            correct        incorrect                    panel A suggests that there was a speed accuracy trade-
     RT (sec)
                 1.0
                                                                        off in the ASL, Face, and Bullseye conditions, with incor-
                                                                        rect shifts tending to be faster than correct shifts. To quan-
                                                                        tify differences across the groups, we fit a linear mixed-
                 0.5
                                                                        effects regression predicting first shift RT as a function
                                                                        of center stimulus type, controlling for age, and including
                 0.0
                       Bullseye   Object        Face   ASL              user-defined contrasts to test specific comparisons of inter-
                                                                        est: Log(RT) ∼ center stimulus type + age + (1 |
     B          1.00                                                    subject) + (1 | item). We found that (a) ASL learners
                0.75
                                                                        generated slower RTs compared to all of the spoken English
     Accuracy
                                                                        samples (β = -0.97, p < .001), (b) ASL learners’ shifts were
                0.50                                                    slower compared directly to participants in the Face task (β =
                                                                        -0.42, p < .001), and (c) participants in the Face task shifted
                0.25
                                                                        slower compared to participants in the Object and Bullseye
                0.00                                                    tasks (β = -0.73, p < .001).
                       Bullseye   Object        Face   ASL
                                                                           Accuracy. Next we compared the accuracy of first shifts
Figure 2: First shift accuracy and RTs from E1. Panel A                 across the different tasks by fitting a mixed-effects logistic
shows a boxplot representing the distribution of RTs for cor-           regression with the same specifications and contrasts as the
rect (orange) and incorrect (blue) shifts for each center stim-         RT model. We found that (a) ASL learners were more ac-
ulus type. Panel B shows the distribution of mean first shift           curate compared to all of the spoken English samples (β =
accuracy scores for each center stimulus type. The solid lines          -0.78, p < .001), (b) ASL learners were more accurate when
represent median values, the boundaries of the box show the             directly compared to participants in the Face task (β = -0.62,
upper and lower quartiles, and the whiskers show the full               p = 0.001), and (c) participants in the Face task were numer-
range of the data excluding outliers.                                   ically more accurate compared to participants in the Object
                                                                        and Bullseye tasks (β = -0.73) but this effect was not signifi-
                                                                        cant (p = 0.089).
                                                                        Model-based analyses EWMA. Figure 3 shows changes in
As the RTs get longer, we assume that participants have gath-           the control statistic (CS) and the upper control limit (UCL)
ered more information and should become more accurate, or               as a function of participants’ RTs. Each CS starts at chance
a Bernoulli process with probability success > 0.5. Using               performance and below the UCL. In the ASL and Face tasks,
this model, we can quantify and compare: a) the cutoff point            the CS value begins to increase with RTs around 0.7 seconds
when the CS exceeds the UCL, indicating that participants               after noun onset and eventually crosses the UCL, indicat-
started to generate language-driven shifts and b) the propor-           ing that responses > 0.7 sec were on average above chance
tion of shifts that the model categorizes as language-driven            levels. In contrast, the CS in the Object and Bullseye tasks
vs. nonlanguage-driven.                                                 never crossed the UCL, indicating that children’s shifts were
                                                                        equally likely to land on the target or the distracter, regard-
   Finally, we took the shifts that were categorized as                 less of when they were initiated. This result suggests that first
language-driven by the EWMA and fit a hierarchical                      shifts in the Bullseye/Object tasks were not language-driven
Bayesian drift-diffusion model (HDDM) to quantify differ-               and may instead have reflected a different process such as
ences in the speed and accuracy of language-driven eye move-            gathering more information about the referents in the visual
ments. We chose to implement a hierarchical Bayesian ver-               world.
sion of the DDM using the HDDM Python package (Wiecki,
Sofer, & Frank, 2013) since we had relatively few trials from              Next, we compared the EWMA output for participants in
child participants and recent simulation studies have shown             the ASL and Face tasks. We found that ASL learners gener-
that the HDDM approach was better than other DDM fit-                   ated fewer shifts when the CS was below the UCL (β = -1.61,
ting methods for small data sets (Ratcliff & Childers, 2015).           p < .001), indicating that a larger proportion of their initial
The model assumes that people accumulate noisy evidence                 shifts away were language-driven (see the differences in the
in favor of one alternative with a response generated when              red shaded area in Fig 3). We did not find evidence for a dif-
the evidence crosses a pre-defined decision threshold. Here             ference in the timing of when the CS crossed the UCL (β =
we focus on two parameters of interest that map onto mean-              -0.04, p = 0.387), indicating that both groups began to gen-
ingful psychological variables: boundary separation, which              erate language-driven shifts about the same time after noun
indexes the amount of evidence gathered before a response               onset.
(higher values suggest more cautious responding) and drift
rate, which indexes the amount of evidence accumulated per                 HDDM. Using the output of the EWMA, we compared the
unit time (higher values suggest more efficient processing).            timing and accuracy of language-driven shifts for participants
                                                                  782

in the ASL and Face tasks.1 We found that ASL learners had
a higher estimate for the boundary separation parameter com-
pared to the Face participants (ASL boundary = 1.77, HDI =
[1.64, 1.9]; Face boundary = 1.35, HDI = [1.21, 1.49]), with
no overlap in the credible values (see Fig 4). This suggests
that ASL learners accumulated more evidence about the lin-
guistic signal before generating an eye movement. We found
high overlap for estimates of the drift rate parameter, indicat-
ing that both groups processed the linguistic information with
similar efficiency (ASL drift = 0.64, HDI = [0.44, 0.83]; Face
drift = 0.57, HDI = [0.33, 0.83]).
   Taken together, the behavioral analyses and the
EWMA/HDDM results provide converging support that
ASL learners were sensitive to the value of eye movements,
producing fewer nonlanguage-driven shifts and prioritizing
accuracy over speed, but accumulating information at
roughly the same rate. This behavior seems reasonable since
the potential for missing subsequent linguistic information
is high if ASL users shifted prior to gathering sufficient
information. It is important to point out that these were                 Figure 3: Output for the EWMA guessing model in E1. The
exploratory findings and that there were several, potentially             black curve represents the evolution of the control statistic
important differences between the stimuli, apparatus, and                 (CS) as a function of reaction time. The grey curve represents
populations. In E2, we set out to perform a well-controlled,              the upper control limit (UCL). The vertical dashed line is the
confirmatory test of our adaptive tradeoffs account.                      median cutoff value (point when the control process shifts
                                                                          out of a guessing state). The grey shaded area represents the
                           Experiment 2                                   95% confidence interval around the estimate of the median
                                                                          cutoff point. And the shaded areas represents the proprotion
In E2, we attempt to replicate a key finding from E1:                     of responses that were flagged as guesses (red) and language-
that increasing the competition between fixating the lan-                 driven (green).
guage source and the nonlinguistic visual world reduces
nonlanguage-driven eye movements. Moreover, we con-
ducted a confirmatory test of our hypothesis that also con-               Participants 25 Stanford undergraduates participated (5
trolled for the population differences present in E1. We                  male, 20 females) for course credit. All participants were
tested a sample of English-speaking adults using a within-                monolingual, native English speakers and had normal vision.
participants manipulation of the center stimulus type. We
                                                                          Stimuli Audio and visual stimuli were identical to the Face
used the Face and Bullseye stimulus sets from E1 and added
                                                                          and Bullseye tasks in E1. We included a new center fixation
two new conditions: Text, where the verbal language in-
                                                                          stimulus type: printed text. The text was displayed in a white
formation was accompanied by a word-by-word display of
                                                                          font on a black background and was programmed such that
printed text (see Fig 1), and Text-no-audio, where the spoken
                                                                          only a single word appeared on the screen, with each word
language stimulus was removed. We chose text processing
                                                                          appearing for the same duration as the corresponding word in
since, like sign language comprehension, the linguistic infor-
                                                                          the spoken language stimuli.
mation is gathered via fixations to the visual world.
   Our key behavioral prediction is that participants in the              Design and procedure The design was nearly identical to
Text conditions should produce a higher proportion of                     E1, with the exception of a change to a within-subjects ma-
language-driven shifts as indexed by the EWMA model out-                  nipulation where each participant completed all four tasks
put. We did not have strong predictions for the DDM pa-                   (Bullseye, Face, Text, and Text-no-audio). In the Text con-
rameter fits since the goal of the Text manipulation was to               dition, spoken language accompanied the printed text. In the
modulate participants’ strategic allocation of visual attention           Text-no-audio condition, the spoken language stimulus was
and not the accuracy/efficiency of information processing.                removed. Participants saw a total of 128 trials while their eye
                                                                          movements were tracked using automated eye-tracking soft-
Method                                                                    ware.
    1 We report the mean and the 95% highest density interval (HDI)       Results and Discussion
of the posterior distributions for each parameter. The HDI represents     Behavioral analyses RT. Visual inspection of Figure 5,
the range of credible values given the model specification and the
data. We chose not to interpret the DDM fits for the Bullseye/Face        panel A suggests that there was a speed-accuracy tradeoff for
tasks since there was no suggestion of any non-guessing signal.           all conditions: incorrect gaze shifts tended to be faster than
                                                                      783

                                                                                A          1.5
                                                                                                     accuracy          correct          incorrect
                                                                                RT (sec)
                                                                                           1.0
                                                                                           0.5
                                                                                           0.0
                                                                                                 bullseye       face             text   text−no−audio
                                                                                                                 Condition
                                                                                B          1.0
                                                                                Accuracy
                                                                                           0.8
                                                                                           0.6
                                                                                           0.4
                                                                                                 bullseye       face             text   text−no−audio
                                                                                                                 Condition
                                                                           Figure 5: Behavioral results from E2. All plotting conven-
Figure 4: Posterior distributions for the boundary and drift               tions are the same as in Figure 2.
rate parameters for children in E1 (Panel A) and adults in E2
(Panel B).
                                                                           prediction: that increasing the value of fixating the language
                                                                           source reduces exploratory gaze shifts to the nonlinguistic vi-
                                                                           sual world.
correct shifts. We fit a linear mixed-effects regression with                 HDDM. Using the output of the EWMA, we fit the same
the same specification as in E1, but we added by-subject in-               HDDM as in E1. There was high overlap of the posterior dis-
tercepts and slopes for each center stimulus type to account               tributions for the drift rate parameters (see Fig 4, panel B),
for our within-subjects manipulation. We did not find evi-                 suggesting that participants gathered the linguistic informa-
dence that RTs were different across conditions (all p > .05).             tion with similar efficiency. We also found high overlap in
   Accuracy. Next, we modeled accuracy using a mixed-                      the distribution of credible boundary separation estimates for
effects logistic regression with the same specifications (see              the Bullseye, Text, and Text-no-audio conditions. Interest-
Panel B of Fig 5). We found that adults’ first shifts were                 ingly, we found some evidence for a higher boundary separa-
highly accurate, and, in contrast to the children in E1, their re-         tion in the Face condition compared to the other three center
sponses were above chance level even in the Bullseye condi-                stimulus types (Face boundary = 1.72, HDI = [1.47, 1.97];
tion when the center stimulus was not salient or informative.              Bullseye boundary = 1.42, HDI = [1.21, 1.65]; Text boundary
We also found that participants tended to be less accurate in              = 1.38, HDI = [1.16, 1.6]; Text-no-audio boundary = 1.36,
the Text conditions compared to conditions without text (β                 HDI = [1.15, 1.58]), suggesting that adults higher accuracy in
= 1.18, p = 0.002). We did find not any other statistically                this condition was driven by accumulating more information
significant differences.                                                   before generating a response.
Model-based analyses EWMA. For all four conditions, the                       Together, these results suggest that adults were sensitive to
CS crossed the UCL (see Fig 6), suggesting that for all tasks              the tradeoff between gathering different kinds of information.
some proportion of adults’ shifts were language-driven. Inter-             When processing text, people generated fewer nonlanguage-
estingly, we found a graded effect of condition (see the shift             driven shifts (EWMA results) but their processing efficiency
in the vertical dashed lines in Fig 5) on the point when the CS            of the linguistic signal itself did not change (HDDM results).
crossed the UCL such that the Text-no-audio condition oc-                  Interestingly, we found a graded difference in the EWMA re-
curred earliest (Mtext−no−audio = 0.39), followed by the Text              sults between the Text and Text-no-audio conditions, with the
and Face conditions that were not different from one another               lowest proportion of early, nonlanguage-driven shifts occur-
(Mtext = 0.44, M f ace = 0.45, p > .05), and finally the Bullseye          ring while processing text without the verbal stimuli. This
condition (Mbullseye = 0.54). We also found the same graded                behavior makes sense; if the adults could rely on the auditory
difference in the proportion of shifts that occurred while the             channel to gather the linguistic information, then the value of
CS was below the UCL (see the red vs. green shaded area                    fixating the text display decreases. In contrast to the children
in Fig 5), indicating a higher proportion of first shifts were             in E1, adults were highly accurate in the Bullseye condition,
language-driven in the Text conditions, with the highest pro-              perhaps because they construed the Bullseye as a center fix-
portion in the Text-no-audio condition when tested against                 ation that they should fixate, or perhaps they had better en-
the three other conditions (Mtext−no−audio = 3.88, β = 1.74, p             coded the location/identity of the two referents prior to the
< .001). These results provide strong evidence for our key                 start of the target sentence.
                                                                     784

                                                                     plex language information and visual environments.
                                                                        This work attempts to integrate top-down, goal-based
                                                                     models of vision (Hayhoe & Ballard, 2005) with work on
                                                                     language-driven eye movements (Allopenna et al., 1998).
                                                                     While we chose to start with two case studies – ASL and
                                                                     text processing – we think the account is more general and
                                                                     that there are many real world situations where people must
                                                                     negotiate the tradeoff between gathering more information
                                                                     about language or about the world: e.g., processing spoken
                                                                     language in noisy environments or at a distance; or early in
                                                                     language learning when children are acquiring new words and
                                                                     often rely on nonlinguistic cues to reference such as point-
                                                                     ing or eye gaze. Overall, we hope this work contributes to
                                                                     a broader account of eye movements during language com-
                                                                     prehension that can explain fixation behaviors across a wider
                                                                     variety of populations, processing contexts, and during differ-
                                                                     ent stages of language learning.
                                                                                        Acknowledgements
Figure 6: EWMA model output for E2. All plotting conven-             We are grateful to the families who participated in this re-
tions are the same as Figure 3.                                      search. This work was supported by an NSF GRFP to KM
                                                                     and an NIDCD grant to Anne Fernald and David Corina
                                                                     (DC012505).
                    General Discussion
Language comprehension can be facilitated by fixating on                                       References
relevant features of the nonlinguistic visual world or on the         Allopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K.
speaker. But how do we decide where to look? We pro-                   (1998). Tracking the time course of spoken word recogni-
pose that eye movements during language processing reflect             tion using eye movements: Evidence for continuous map-
a sensitivity to the tradeoffs of gathering different kinds of         ping models. Journal of Memory and Language, 38(4),
information. We found that young ASL-learners generated                419–439.
slower but more accurate shifts away from a language source           Altmann, G., & Kamide, Y. (2007). The real-time media-
and produced a smaller proportion of nonlanguage-driven                tion of visual attention by language and world knowledge:
shifts compared to spoken language learners. We found the              Linking anticipatory (and other) eye movements to linguis-
same pattern of behavior within a sample of English-speaking           tic processing. Journal of Memory and Language, 57(4),
adults processing displays of printed text compared to spoken          502–518.
language. These results suggest that as the value of fixating         Bates, D., Maechler, M., Bolker, B., & Walker, S. (2013).
on a location to gather information about the linguistic sig-          Lme4: Linear mixed-effects models using eigen and s4. r
nal increases, eye movements to the rest of the visual world           package version 1.0-5.
become less useful and occur less often.                              Hayhoe, M., & Ballard, D. (2005). Eye movements in natural
   Our work here attempts to synthesize results from differ-           behavior. Trends in Cognitive Sciences, 9(4), 188–194.
ent populations and stimuli in a single framework, but it has         MacDonald, K., LaMarr, T., Corina, D. and, Marchman, V.,
several limitations that we hope will pave the way for future          & Fernald, A. (under review). Real-time lexical compre-
work. First, we have not performed a confirmatory test of              hension in young children learning american sign language.
the DDM findings: both ASL-learners (E1) and adults pro-              Ratcliff, R., & Childers, R. (2015). Individual differences
cessing language from a person (E2) prioritize accuracy over           and fitting methods for the two-choice diffusion model of
speed. So these findings, while interesting, are preliminary.          decision making. Decision, 2(4), 237–279.
Second, we do not know what might be driving the popula-              Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M.,
tion differences in E1. It could be that ASL-learners’ massive         & Sedivy, J. C. (1995). Integration of visual and linguistic
experience dealing with competition for visual attention leads         information in spoken language comprehension. Science,
to changes in the deployment of eye movements during lan-              268(5217), 1632.
guage comprehension. Or, it could be that the in-the-moment           Vandekerckhove, J., & Tuerlinckx, F. (2007). Fitting the rat-
constraints of processing a visual language cause different            cliff diffusion model to experimental data. Psychonomic
fixation behaviors. Finally, we used a very simple visual              Bulletin & Review, 14(6), 1011–1026.
world, with only three places to look, and very simple linguis-       Wiecki, T. V., Sofer, I., & Frank, M. J. (2013). HDDM: Hi-
tic stimuli, especially for the adults in E2. Thus it remains an       erarchical bayesian estimation of the drift-diffusion model
open question how these results might scale up to more com-            in python. Frontiers in Neuroinformatics, 7, 14.
                                                                 785

