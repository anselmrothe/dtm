                                 A computational model for decision tree search
                         Bas van Opheusden, Gianni Galbiati, Zahy Bnaya, Yunqi Li & Wei Ji Ma
                             Center for Neural Science & Department of Psychology, New York University
                                {basvanopheusden,gianni.galbiati,zahy.bnaya,yunqi.li,weijima}@nyu.edu
                               Abstract                                                          Experiments
   How do people plan ahead in sequential decision-making               Task. To investigate the computations underlying sequen-
   tasks? In this article, we compare computational models of hu-
   man behavior in a challenging variant of tic-tac-toe, to inves-      tial decision-making, we collected data from people playing
   tigate the cognitive processes underlying sequential planning.       a variant of tic-tac-toe, in which players need to make 4-in-a-
   We validate the most successful model by predicting choices          row on a 4-by-9 board (figure 1A). Despite these simple rules,
   during games, two-alternative forced choices and board evalu-
   ations. We then use this model to study individual skill differ-     the game is surprisingly challenging and fun to play. Because
   ences, the effects of time pressure and the nature of expertise.     the game is deterministic without hidden information, it is
   Our findings suggest that people perform less tree search un-        theoretically solvable. Using alpha-beta pruning and threat
   der time pressure, and that players search more as they improve
   during learning.                                                     tree search (Allis et al., 1994), we were able to derive a weak
   Keywords: Sequential decision-making, Behavioral model-              solution: the first player can force a win by opening on the
   ing, Expertise                                                       central square. However, with perfect defense, the second
                                                                        player can delay the win for 17 moves.
                            Introduction
Imagine you are deciding if you should run for President of
the United States in 2020. To make that choice, you have
to consider a sequence of future decisions. Will you run as
a Republican, Democrat or Independent? If Democrat, will
you run as a moderate or progressive candidate? What posi-
tions will you take on abortion or gun control? How will you
distinguish yourself during the primaries? What line of attack          Figure 1: Task. A. Two players take turns placing black or
will you choose in the Presidential Debates? You face a se-             white pieces on a 4-by-9 board, and the first to achieve 4-in-
quence of decisions, which together determine your electoral            a-row (horizontal, diagonal or vertical), wins the game. B.
success. In short, you have to explore a decision tree.                 In the 2AFC task, participants see a board and two candidate
   Although the computations underlying human decision-                 moves, and indicate their preferences. C. In the evaluation
making are extensively studied, the process by which peo-               task, participants see a board position and report their esti-
ple explore decision trees is less understood. Most work fo-            mated winning chances on a 7-point scale.
cuses on the neural implementation of learning and decision-
making in small decision trees (Solway & Botvinick, 2015;               Participants. We conducted four experiments: human-vs-
Simon & Daw, 2011). However, with more choices and more                 human (N = 40 participants), generalization (N = 40), time
available options, the decision tree grows exponentially, and           pressure (N = 30), and learning (N = 30). We recruited par-
people need to prune the tree (Huys et al., 2012).                      ticipants through the NYU psychology research participant
   There exists a large literature exploring human decision-            system, flyers, a sign-up link on our lab webpage or personal
making in chess, starting with de Groot’s seminal arti-                 communication. We did not collect demographic data. We
cle (A. D. de Groot, 1946). One central question in this lit-           compensated participants 12 per hour, but did not incentivize
erature is whether the superior performance of experts relies           task performance.
primarily on enhanced pattern recognition (Chase & Simon,               Procedure. In the human-vs-human experiment, we divided
1973), increased tree search (Holding, 1985), or both. The              participants into pairs. Participants in each pair played games
relation between tree search and expertise is especially con-           against each other without time constraints for 50 minutes,
troversial, with both positive (Campitelli & Gobet, 2004) and           switching colors every game. In the generalization experi-
negative (A. D. de Groot, 1946) results.                                ment, participants performed three tasks: playing the game
   In this article, we investigate sequential decision-making in        against a computer opponent for 30 minutes, 82 trials of a
a two-player board game, which is much simpler than chess,              two-alternative forced-choice (2AFC) between moves in a
but much more complex than traditional decision-making                  given board position (figure 1B), and 82 board evaluation tri-
tasks. We develop a computational model that predicts peo-              als, in which they rated their winning chances in given board
ple’s choices on individual trials, and fit this model to data          positions on a 7-point scale (figure 1C). The time pressure
from individual participants. We then ask whether the compu-            experiment was identical to the human-vs-computer compo-
tations performed by our model mimic the process by which               nent of the generalization experiment, except that for each
people arrive at their decisions. Finally, we use our model to          game, we added a time limit randomly selected between 5,
investigate the nature of expertise in our game.                        10 or 20 seconds per move. If participants exceeded the time
                                                                    1254

limit, they lost the game. The learning experiment consisted          legal move, and prunes all moves with value below that of
of 5 sessions, no more than 3 days apart. In sessions 1, 3 &          the best move minus a threshold. After each iteration, the al-
5, participants played against computers for 30 minutes, then         gorithm stops with a probability γ, resulting in a geometric
completed 60 trials each of the 2AFC and evaluation tasks.            distribution over the total number of iterations.
In session 2 & 4, they played against computers for the entire           Noise. To account for variability in people’s choices, we
50-minute session.                                                    add three sources of noise. Before constructing the decision
   In all human-vs-computer games, the computer opponents             tree, we randomly drop features (at specific locations and ori-
implemented an early version of our computational model for           entations), which are omitted during the calculation of V (s)
people’s decision-making process, with parameters adapted             anywhere in the tree. During tree search, we add Gaussian
from fits on human-vs-human games. We created 30 AI                   noise to V (s) in each node. Finally, we include a lapse rate λ.
agents, grouped by playing strength into 6 groups of 5 agents            The components of our computational model are inspired
each, and matched participants with AI opponents through a            by behavioral studies of human decision-making. Tree
one-up, one-down staircase procedure.                                 search, as a mechanism whereby people mentally simulate
   In the 2AFC and evaluation task, each participant com-             the consequences of available actions, is similar to “level-
pleted the same trials in shuffled order. We selected board           K reasoning” (Arad & Rubinstein, 2012) in behavioral eco-
positions and move options that maximize an approximation             nomics. In other decision-making tasks, people have been
to mutual information between model parameters and move               shown to prune away options leading to immediate losses but
choice, in order to present participants with interesting and         long-term gains (Huys et al., 2012). Feature dropping reflects
informative choices.                                                  shift in endogenous attention (to spatial locations, orientation
                                                                      or feature types), corresponding to participants overlooking
                               Model                                  relevant features on the board. Finally, feature-based evalua-
Value function. The core component of our model is an eval-           tion functions, value noise and lapse rates are all common in
uation function V (s) which assigns values to board states s.         reinforcement learning.
We use a weighted linear sum of 5 features: center, connected            There also exists neural evidence consistent with our
2-in-a-row, unconnected 2-in-a-row, 3-in-a-row and 4-in-a-            model. In rats, dynamic search and exploration of possible
row. The center feature assigns a value to each square, and           paths at junctions in a T-maze have been linked to preplay
sums up the values of all squares occupied by the player’s            sequences in hippocampal place cells (Johnson & Redish,
pieces. This value of each square is inversely proportional to        2007). In humans, tree search is associated with neural activ-
its Euclidean distance from the board center. The other fea-          ity in the ventral striatum (Simon & Daw, 2011) and ventro-
tures count how often particular patterns occur on the board          medial prefrontal cortex (Lee, Shimojo, & ODoherty, 2014).
(horizontally, vertically, or diagonally):
Connected 2-in-a-row: two adjacent pieces with enough                                            Methods
empty squares around them to complete 4-in-a-row.
Unconnected 2-in-a-row: two non-adjacent pieces which lie             Estimating task performance. To quantify task perfor-
on a line of four contiguous squares, with the remaining two          mance in human-vs-computer games, we use the Elo rating
squares empty.                                                        system (Elo, 1978), which estimates playing strength from
3-in-a-row: three pieces which lie on a line of four contigu-         game results, independent of the moves played. We append
ous squares, with the remaining square empty. This pattern            the results of games from all 4 experiments to a computer-
represents an immediate winning threat.                               vs-computer tournament, and estimate ratings jointly for all
4-in-a-row: four pieces in a row. This pattern appears only in        humans and computers with a Bayesian optimization al-
board states where a player has already won the game.                 gorithm (Hunter, 2004). To calculate performance in the
   We associate weights wi to these features, and write               2AFC task, we calculate the agreement between a partici-
                                                                      pant’s choices and those of an optimal agent with random
                   4                        4                         tie-breaking. In the evaluation task, we define performance
    V (s) = cself ∑ wi fi (s, self) − copp ∑ wi fi (s, opponent)      as the correlation between a participant’s choices and the op-
                  i=0                      i=0
                                                                      timal rankings.
where cself = C and copp = 1 whenever the player is to move              Estimating model parameters The model has 10 param-
in state s, and cself = 1 and copp = C when it is the oppo-           eters: the 5 feature weights, the active-passive scaling con-
nent’s move. The scaling constant C captures value differ-            stant C, the pruning threshold, stopping probability γ, fea-
ences between “active” and “passive” features. For exam-              ture drop rate δ and the lapse rate λ. We infer these param-
ple, a three-in-a-row feature signals an immediate win on the         eters for individual participants and individual learning ses-
player’s own move, but not the opponent’s.                            sions or time limit conditions with maximum-likelihood es-
   Tree search. The evaluation function guides the construc-          timation. We estimate the log probability of a participant’s
tion of a decision tree with an iterative best-first search algo-     move in a given board position with inverse binomial sam-
rithm. Each iteration, the algorithm chooses a board position         pling (M. H. de Groot, 1959), and optimize the log-likelihood
to explore further, evaluates the positions resulting from each       function with multilevel coordinate search (Huyer & Neu-
                                                                  1255

maier, 1999). We account for potential overfitting by re-               Modifications. We modify the model by fixing the num-
porting 5-fold cross-validated log-likelihoods, with the same        ber of iterations of the search algorithm to a constant instead
testing-training splits for all models.                              of the geometric distribution prescribed by the main model.
                                                                     Alternatively, we amend the search process to explore each
                    Model comparison                                 branch of the tree up to fixed depth, or the pruning rule to
To test how well our model predict participants’ choices,            keep only the K best moves (according to the evaluation func-
we compare its log-likelihood on human-vs-human games to             tion), where the branching factor K is again fixed. For a more
that of 25 alternative models (figure 2). We test four cate-         drastic modification, Monte Carlo Tree Search (MCTS) esti-
gories of alternative models: lesions, generated by remov-           mates state values not by calling the evaluation function V (s),
ing model components; extensions, generated by adding new            but by aggregating outcomes of simulated games between no-
model components; modifications, generated by replacing a            tree agents. It also extends the best-first search algorithm
model component with a similar implementation; and con-              by adding a term that favors exploration (investigating unex-
trols, which are structurally different from the main model.         plored moves) over exploitation (further investigating already
                                                                     explored moves). We consider fixing the feature weights to
                                                                     the optimal solution, i.e. those weights that maximize the
                                                                     correlation between V (s) and the game-theoretic value of the
                                                                     position s. Finally, we modify the attention mechanism from
                                                                     dropping random features from the evaluation function to
                                                                     dropping random branches from the decision tree.
                                                                        Controls. We consider MCTS with completely random
                                                                     playouts, or a mixture model between optimal and random
                                                                     play. The optimal agent enumerates all candidate moves
                                                                     that preserve the game-theoretic value of the position, and
                                                                     chooses randomly between them. Another control model, la-
                                                                     beled soft-max, assigns a value to each square on the board
                                                                     (enforced to obey reflection/rotation symmetry), and chooses
                                                                     a move with a softmax decision rule, constrained to unoccu-
                                                                     pied squares.
                                                                        All lesioned models fit worse than the full model. The most
                                                                     impactful lesions are specific features (3-in-a-row, connected
                                                                     2-in-a-row and center) and sources of variability (value noise
Figure 2: Cross-validated log-likelihood/move for our main           and feature dropping). Lesioning the pruning mechanism or
model and 25 alternatives on the human-vs-human data. The            the entire tree search algorithm has a less dramatic effect,
bars show mean and s.e.m. across participants (N = 40). The          which can be partially explained by parameter trade-offs. Fi-
main model fits better than lesions, most controls and some          nally, some lesions (active-passive scaling, unconnected 2-
modifications, and approximately equally good as extensions          in-a-row and 4-in-a-row) cause only small reductions in log-
or some other modifications.                                         likelihood. Most modifications also worsen the main model,
                                                                     but the Monte Carlo Tree Search model is equally good and
Lesions. We create lesion models by forcing either one of            the “fixed iterations” model slightly outperforms it. The
the feature weights to zero, or removing the feature dropping,       model extensions also slightly increase the main model’s per-
pruning, value noise, active-passive scaling or the entire deci-     formance. Finally, all control models fit much worse than the
sion tree. The no-tree model evaluates the positions after each      main model.
possible move, and chooses the one with maximum value. It
contains feature dropping and value noise but no pruning.               Unfortunately, the model comparison does not reveal a
   Extensions. We consider extending the model with a fea-           unique best-fitting model, meaning that we did not collect
ture that recognizes to a three-piece pattern arranged in a tri-     enough data to determine precise details of people’s thought
angle, or multiplying the weights for diagonally and vertically      process. For example, we cannot distinguish between tree
oriented features by scaling constants cdiag or cvert , respec-      search algorithms (best-first search or MCTS) or determine
tively. Alternatively, we extend the main model by allowing          specifics of the best-first search algorithm (pruning and num-
feature drop rates to differ between features of different types     ber of iterations). Alternatively, different participants may
(2-in-a-row, 3-in-a-row, etc) or orientations. Finally, we test      use different strategies. However, the model comparison does
a model in which all weights for the opponent’s features are         suggest that any model that can predict human choices needs
scaled by a factor copp , which thereby controls the balance         to contain a feature-based evaluation function, and mecha-
between attack and defense.                                          nisms for attentional oversights and tree search.
                                                                 1256

      Generalization to 2AFC and Evaluation                         ulations. As our first manipulation, we introduce time con-
Next, we show that the model can generalize by estimating           straints of 5, 10 or 20 seconds per move. Second, we conduct
parameters from subjects’ choices in games against comput-          an experiment in which participants play the game for 5 ses-
ers and predicting their choices in the 2AFC or evaluation          sions.
tasks with minimal additional assumptions. To select an op-            Given a set of parameters for an individual participant in a
tion on a 2AFC trial, the only change we make is to initial-        time limit condition or learning session, we simulate moves
ize the tree search algorithm with a three-node decision tree       made by the model in a database of pre-determined posi-
with the current board position as the initial node and the two     tions and measure 3 statistics of its process: the percentage
available candidate moves as children. On an evaluation trial,      of dropped features, the value quality (correlation between
we execute the tree search algorithm as usual, then measure         V (s) and the game-theoretic value V ∗ (s)) and the mean tree
the value of the root node. We then convert this value to a         size (number of nodes in its decision tree). Note that tree size
seven-point scale by transforming v → 3 + 4 tanh(v/20) and          incorporates both the width and depth of the decision tree.
rounding to the nearest integer.                                       Based on the literature on expertise and time pressure in
                                                                    chess, we expected that time constraints would reduce tree
                                                                    size but not affect value function quality. In the learning ex-
                                                                    periment, we expected the value function quality to increase
                                                                    across sessions and the tree size to remain constant or in-
                                                                    crease only slightly. Since chess algorithms often do not
                                                                    explicitly include feature dropping or similar mechanisms,
                                                                    we made no predictions for its trajectory. Finally, we pre-
                                                                    dict that experience increases participants’ task performance
                                                                    while time pressure reduces it.
                                                                    Time pressure To test the effectiveness of time constraints to
                                                                    manipulate participants’ behavior, we first plot the distribu-
                                                                    tion of response times in the three conditions, as well as the
Figure 3: A. Histogram of the percentage of correctly pre-          response times from the unconstrained (generalization) ex-
dicted 2AFC choices by our model across N = 40 partici-             periment (figure 4A). Adding time pressure causes an overall
pants. We fit parameters for each participant on their choices      shift in the response time distribution regardless of the time
in games against computers. The dashed line indicates the           limit. Additionally, participants play faster with shorter time
accuracy of a random prediction. B. Same for the evalua-            constraints. Surprisingly, there is no consistent effect of time
tion task, where we quantify goodness-of-fit as the correla-        constraints on participants’ performance (figure 4B).
tion across trials between rankings predicted by the model
and reported by a participant.
   The average accuracy of the model prediction on 2AFC
data is 58.6 ± 1.0% (figure 3A), the average correlation be-
tween predicted and observed evaluations is ρ = 0.38 ± 0.04
(figure 3B). The prediction is better than chance for 36/40
participants in the 2AFC task, and 37/40 for evaluation.
   Even though our model predicts participants’ choices in
these additional tasks well on average, the goodness-of-fit
is highly variable across participants. This variability in
goodness-of-fit is correlated across subjects between the three
tasks (2AFC-evaluation: ρ = 0.54, p < 0.001; 2AFC-games:            Figure 4: A. Empirical cdf of response times in the three con-
ρ = 0.35, p < 0.05; evaluation-games: ρ = 0.24, p = 0.12).          ditions of the time pressure experiment (red), and the gener-
Moreover, on the 2AFC and evaluation task, goodness-of-             alization experiment (blue). In the latter experiment, players
fit correlates with participants’ objective task performance        could take arbitrary amounts of time, which we denote as an
(2AFC: ρ = 0.56, p < 0.001; evaluation: ρ = 0.96, p <               infinite time limit. People play faster with shorter time lim-
0.001). This suggests that the variability in goodness-of-fit       its. B. Task performance, quantified by Elo rating, for the
can at least partially be explained by differences in intrinsic     same experiments and conditions. Error bars indicate mean
variability across participants.                                    and s.e.m. across participants (N = 30). The effect of time
                                                                    limits on performance is unclear.
 How experimental manipulations affect model
                         parameters                                    In figure 5 (top), we show the feature drop rate, value func-
To further support the model, we investigate whether its pa-        tion quality and tree size in different time limit conditions.
rameters respond in predictable ways to experimental manip-         Compared to the unconstrained experiment, participants build
                                                                1257

smaller trees and drop more features, while the value function
quality is similar. The impact of the time constraint on tree
size becomes larger with shorter time limits, but the feature
drop rate shows the opposite trend and is at its highest in the
20-second condition. We speculate that the stress of poten-
tially losing on time causes participants to pay more attention
with shorter time limits, whereas with 20 seconds, they are
more relaxed and make more attentional lapses.
                                                                     Figure 6: Elo rating of N = 30 participants in the learning
                                                                     experiment (mean and s.e.m. across participants). As partici-
                                                                     pants gain expertise, they play stronger.
Figure 5: Top row. Estimated model parameters in the time
pressure and generalization experiments. Error bars denote
mean and s.e.m. across participants. The model infers a re-
lation between time limit and tree size, but unclear effects
on feature dropping and the value function quality. Bottom
                                                                     Figure 7: Top: Model parameters as a function of sessions
row. Model parameters and Elo rating for each participant in
                                                                     completed in the learning experiment. Over the course of
each time limit condition. The tree size and feature drop rate
                                                                     learning, tree size is estimated to increase while feature drop-
correlate with Elo rating, but value function quality does not.
                                                                     ping decreases. The value function quality decreases, but
                                                                     only slightly. Bottom: Model parameters and Elo ratings for
   To understand the surprising negative result of figure 4, we      each participant in each session of the learning experiment.
investigate how Elo rating and parameter estimates correlate         Both tree size and feature dropping correlate with Elo, but
across both individuals and time limit conditions (figure 5,         value function quality does not.
bottom). Stronger players (in all time limit conditions) are es-
timated to build larger decision trees and drop fewer features.
Therefore, the increased tree size with longer time limit pre-                                  Discussion
dicts a performance increase, but the increased feature drop         Limitations. Our model has three conceptual limitations.
rate predicts decreased performance. These opposite effects          First, although its parameters shift as participants acquire ex-
happen to be approximately equal, which explains the lack of         pertise, the model does not describe how these shifts arise
correlation between time limit and Elo rating.                       from their experience (their specific move choices and re-
Learning We first validate that experience affects partici-          wards). Instead, model parameters are stationary within each
pants’ behavior by plotting Elo rating as a function of session      session. Moreover, because model parameters are constant
number (figure 6). Next, we investigate changes in parame-           while participants play against multiple AI opponents per ses-
ters across sessions (figure 7, top). Tree size increases across     sion, the model cannot capture strategic adaptations based on
sessions, feature drop rate decreases and value function qual-       an opponent’s game play. Finally, the model assumes that
ity remains constant. As in the time pressure experiment, tree       people make decisions independently on every move, ignor-
size and feature drop rate correlate with Elo rating on an in-       ing potential long-term planning or caching of partial game
dividual level (figure 7, bottom), and the change in parame-         trees between moves. We make these assumptions out of ne-
ter estimates across sessions explains changes in task perfor-       cessity, because parameter inference is already challenging.
mance. Experienced players build larger decision trees and           Relation with chess literature. Contrary to the chess
drop fewer features, both of which predict increased playing         literature, in which the superior pattern recognition of
strength, which matches the data.                                    chess experts is evident from board reconstruction experi-
                                                                 1258

ments (Chase & Simon, 1973) and eye movements (Reingold,              by using it to predict response times and eye movements.
Charness, Pomplun, & Stampe, 2001), we find no changes in
value function quality with expertise or individual skill dif-                             Acknowledgments
ferences. Stronger players might use features outside our             This work was supported by grant IIS-1344256 from the Na-
model space, and the lack of correlation could be a false nega-       tional Science Foundation
tive. Alternatively, perhaps chess and 4-in-a-row are qualita-
tively different domains of expertise. Chess contains many                                       References
non-obvious features (pawn structure, the bishop pair) or             Allis, L. V., et al. (1994). Searching for solutions in games
non-obvious feature weights (bishops and knights are equally             and artificial intelligence. Ponsen & Looijen.
strong). By contrast, in our task, people’s intuitive priors          Arad, A., & Rubinstein, A. (2012). The 11–20 money request
(three-in-a-row is good) happen to be correct.                           game: a level-k reasoning study. The American Economic
   Our finding of increased tree search with longer time con-            Review, 102(7), 3561–3573.
trols is consistent with chess studies that conceptualize pat-        Campitelli, G., & Gobet, F. (2004). Adaptive expert decision
tern recognition and tree search as fast and slow processes,             making: Skilled chess players search more and deeper.
respectively (Chabris & Hearst, 2003). However, the strong            Chabris, C. F., & Hearst, E. S. (2003). Visualization, pattern
dependence between expertise and tree search is unexpected.              recognition, and forward search: Effects of playing speed
We first investigate whether this effect could have arisen from          and sight of the position on grandmaster chess errors. Cog-
incorrect model assumptions. Specifically, players may use               nitive Science, 27(4), 637–648.
unmodeled features, stronger players may assign those fea-            Chase, W. G., & Simon, H. A. (1973). Perception in chess.
tures higher weights, and those feature weights may trade                Cognitive psychology, 4(1), 55–81.
off with additional tree search in our model. However, by             de Groot, A. D. (1946). Het denken van den schaker: een
analyzing parameter estimates in lesion models, we find no               experimenteel-psychologische studie. Noord-Hollandsche
such trade-offs. Therefore, our results reflect differences be-          Uitgevers Maatschappij.
tween 4-in-a-row and chess, or a methodological improve-              de Groot, M. H. (1959). Unbiased sequential estimation for
ment. Conclusions about tree search in chess derive almost               binomial populations. The Annals of Mathematical Statis-
solely from verbal reports, whereas we use the more princi-              tics, 80–101.
pled method of parameter inference in a behavioral model.             Elo, A. E. (1978). The rating of chessplayers, past and
                                                                         present. Arco Pub.
                         Conclusion                                   Holding, D. H. (1985). The psychology of chess skill.
                                                                         Lawrence Erlbaum.
We built a computational model that predicts people’s choices         Hunter, D. R. (2004). Mm algorithms for generalized
in a two-player board game. The model posits three compu-                bradley-terry models. Annals of Statistics, 384–406.
tational principles for sequential decision-making: a feature-        Huyer, W., & Neumaier, A. (1999). Global optimization by
based evaluation function, attentional oversights and tree               multilevel coordinate search. Journal of Global Optimiza-
search. All three components are necessary to explain par-               tion, 14(4), 331–355.
ticipants’ behavior, but the data does not constrain details of       Huys, Q. J., Eshel, N., O’Nions, E., Sheridan, L., Dayan, P.,
their implementation such as the order by which nodes are                & Roiser, J. P. (2012). Bonsai trees in your head: how the
visited during search, or how long the search process contin-            pavlovian system sculpts goal-directed choices by pruning
ues before players finalize their decision.                              decision trees. PLoS Comput Biol, 8(3), e1002410.
   The model generalizes to predict choices in a two-                 Johnson, A., & Redish, A. D. (2007). Neural ensembles
alternative forced-choice task and a board evaluation task.              in ca3 transiently encode paths forward of the animal at a
This suggests that the model doesn’t just fit a mapping from             decision point. Journal of Neuroscience, 27(45), 12176–
boards to moves, but that it captures aspects of the compu-              12189.
tational process that underlies decision-making in all three          Lee, S. W., Shimojo, S., & ODoherty, J. P. (2014). Neural
tasks. Furthermore, the feature drop rate and tree size change           computations underlying arbitration between model-based
in predictable ways when we expose participants to manipula-             and model-free learning. Neuron, 81(3), 687–699.
tions in time pressure and experience. These changes account          Reingold, E. M., Charness, N., Pomplun, M., & Stampe,
for participants’ task performance, suggesting that these spe-           D. M. (2001). Visual span in expert chess players: Evi-
cific parameters reflect some task-relevant characteristic of            dence from eye movements. Psychological Science, 12(1),
participants’ cognitive process. Furthermore, these two be-              48–55.
havioral characteristics are dissociable, since in the time pres-     Simon, D. A., & Daw, N. D. (2011). Neural correlates of for-
sure experiment, both tree size and feature dropping increase            ward planning in a spatial decision task in humans. Journal
across conditions, whereas in the learning experiment, tree              of Neuroscience, 31(14), 5526–5539.
size increases while feature dropping decreases. In the fu-           Solway, A., & Botvinick, M. M. (2015). Evidence integration
ture, we aim to further support our model as a description of            in model-based tree search. Proceedings of the National
the computational process underlying people’s move choices               Academy of Sciences, 112(37), 11708–11713.
                                                                  1259

