             Mapping the unknown: The spatially correlated multi-armed bandit
               Charley M. Wu1 , Eric Schulz2 , Maarten Speekenbrink2 , Jonathan D. Nelson1 & Björn Meder1
                      1 Max  Planck Institute for Human Development, Lentzeallee 94, 14195 Berlin, Germany
                    2 Department   of Experimental Psychology, University College London, London, WC1H0AP
                                                                                 Smooth Environment            Rough Environment
                              Abstract
   We introduce the spatially correlated multi-armed bandit
   as a task coupling function learning with the exploration-
   exploitation trade-off. Participants interacted with bi-variate
   reward functions on a two-dimensional grid, with the goal of
   either gaining the largest average score or finding the largest
   payoff. By providing an opportunity to learn the underly-
   ing reward function through spatial correlations, we model
   to what extent people form beliefs about unexplored payoffs
   and how that guides search behavior. Participants adapted to
   assigned payoff conditions, performed better in smooth than          Figure 1: Examples of the underlying reward functions for the two
   in rough environments, and—surprisingly—sometimes per-               classes of environments.
   formed equally well in short as in long search horizons. Our
   modeling results indicate a preference for local search options,     maximize cumulative payoffs by sequentially choosing one of
   which when accounted for, still suggests participants were           the N-arms of the bandit that stochastically generate rewards
   best-described as forming local inferences about unexplored
   regions, combined with a search strategy that directly traded        (Steyvers, Lee, & Wagenmakers, 2009), with learning hap-
   off between exploiting high expected rewards and exploring to        pening independently for each arm (i.e., reinforcement learn-
   reduce uncertainty about the spatial structure of rewards.           ing). In our case, because proximate arms generate similar re-
   Keywords: Exploration-exploitation; Multi-armed bandits;             wards, there is the opportunity to form inductive beliefs about
   Active Learning; Gaussian Processes;
                                                                        unobserved rewards (i.e., function learning). This allows us to
                                                                        study how people generate beliefs about unobserved rewards
                          Introduction                                  and how this influences their search behavior.
Modern humans descend from capable foragers and hunters,                   The spatially correlated MAB is related to the optimal for-
who have migrated and survived in almost every environment              aging context (Krebs, Kacelnik, & Taylor, 1978), whereby a
on Earth. Our ancestors were able to adaptively learn the               forager is not only guided by the search for resources, but also
distribution of resources in new environments and make good             by the need to acquire information about the distribution of
decisions about where to search, balancing the dual goals of            resources in the environment (Schulz, Huys, Bach, Speeken-
exploring to acquire new information and exploiting existing            brink, & Krause, 2016). This creates a natural trade-off be-
knowledge for immediate gains. What strategies do humans                tween exploration and exploitation (March, 1991), where an
use to search for resources in unknown environments?                    effective search policy needs to adequately balance exploring
   We present a new framework for studying human search                 areas with higher uncertainty, while also exploiting existing
behavior using a spatially correlated multi-armed bandit task,          information to obtain rewards. One key difference in our task
where nearby arms (i.e., search options) have correlated re-            is that the decision-maker must determine where to search,
wards. Spatial correlations provide an opportunity to learn             and not only whether to stay or to leave a patch.
about the underlying reward function, extending the tra-
ditional reinforcement learning paradigm (Sutton & Barto,                       Modeling Adaptive Search Behavior
1998) to allow for generalization of learned rewards to un-             We consider various computational models for describing hu-
observed actions using spatial context. We compare search               man behavior, which all make sequential predictions about
behavior across different payoff conditions, search horizons,           where people are likely to search. We present both simple
and types of environments, finding that participants adapt to           strategies without an explicit representation of the environ-
their environment, tend to perform very local inferences about          ment, along with more complex function generalization mod-
unexplored regions and choose arms based on a trade-off be-             els representing the task as a combination of (i) a function
tween expectations and their attached uncertainties.                    learning model and (ii) a decision strategy. We use a form of
                                                                        Gaussian Process regression as a flexible and universal func-
    Spatially Correlated Multi-Armed Bandits                            tion learning model, which forms inferential beliefs about the
We adapt the multi-armed bandit (MAB) setting by adding                 underlying reward function, conditioned on previous obser-
spatial correlation to rewards and placing the arms in a two-           vations of rewards. Decision strategies are used to transform
dimensional grid (Fig. 1). Each tile represents a playable              beliefs into predictions about where to search next. The re-
arm of the bandit, which are initially blank and display the            covered parameter estimates of our models describe the ex-
numerical reward value (along with a color aid) after an arm            tent to which people make spatial inferences and how they
has been chosen. Traditionally, the goal in an MAB task is to           trade off between exploration and exploitation.
                                                                    1357

Simple Strategies                                                                   a GP with the following mean and covariance:
Local search. While simple, a tendency to stay local to the
previous search decision—regardless of outcome—has been                                      µT (x) = kT (x)> (KT + σ2 I)yT                             (5)
                                                                                                  0          0           >            2   −1      0
observed in many different contexts, such as semantic forag-                              kT (x, x ) = k(x, x ) − kT (x) (KT + σ I) kT (x )             (6)
ing (Hills, Jones, & Todd, 2012), causal learning (Bramley,
Dayan, Griffiths, & Lagnado, 2017), and eye movements                               where kT (x) = [k(x1 , x), . . . , k(xT , x)]> and KT is the posi-
(Hoppe & Rothkopf, 2016). We use inverse Manhattan dis-                             tive definite kernel matrix [k(xi , x j )]i, j=1,...,T . This posterior
tance (IMD) to quantify locality:                                                   distribution is used to derive normally distributed predictions
                                                                                    about the rewards for each arm of the bandit (Fig. 2).
                                                   1
                      IMD(x, x0 ) =                                         (1)        The kernel function k(x, x0 ) encodes prior assumptions
                                       |x1 − x10 | + |x2 − x20 |                    about the underlying function. We use the radial basis func-
which compares the location of two arms x and x0 , where x1                         tion (RBF) kernel
and x2 are the grid coordinates. For the special case where
                                                                                                                              ||x − x0 ||2
                                                                                                                                           
x = x0 , we set IMD(x, x0 ) = 1. At each time t, we compute the                                      kRBF (x, x0 ) = exp −                              (7)
IMD for each arm based on the choice at xt−1 , and then use                                                                       2λ2
a softmax function (Eq. 11) to transform locality into choice                       which is a universal function learner and assumes infinitely
probabilities, such that arms closer to the previous search de-                     smooth functions (i.e., correlations between two points x and
cision have a higher probability of being chosen.                                   x0 slowly decay as an exponential function of their distance).
Win-stay lose-shift. We also consider a form of the win-                            The RBF kernel uses λ (length-scale) as a free parameter,
stay lose-shift (WSLS) heuristic (Herbert, 1952), where a win                       which determines how far correlations extend: larger values
is defined as finding a payoff with a higher or equal value                         of λ result in longer spatial correlations, whereas λ → 0+ as-
than the previous best. When the decision-maker “wins”, we                          sumes complete independence of spatial information. We use
assume that any tile with a Manhattan distance ≤ 1 is chosen                        recovered parameter estimates of λ to learn about the extent
(i.e., a repeat or any of the four cardinal neighbors) with equal                   to which humans make inferences about unobserved rewards.
probability. Losing is defined as the failure to improve, and                       Decision strategies. The GP learning model generates nor-
results in choosing any unrevealed tile with equal probability.                     mally distributed predictions about the expectation µ(x) and
Function Generalization Models                                                      the uncertainty σ(x) for each arm, which are available to the
                                                                                    decision strategies1 for evaluating the quality, q(x), and ulti-
We use a combination of (i) Gaussian Process (GP ) regres-
                                                                                    mately making a prediction about where to sample next.
sion as a model of how people form beliefs about the un-
                                                                                       The Variance Greedy (VG) strategy values an arm using
derlying reward function conditioned on previous observa-
                                                                                    only the estimated uncertainty
tions (Lucas, Griffiths, Williams, & Kalish, 2015), and (ii)
a decision strategy that transforms beliefs into predictions                                                   qV G (x) = σ(x)                          (8)
about where a participant will sample next. This approach
has recently been applied to human behavior in contextual                           and is an efficient step-wise (greedy) approximation of infor-
multi-armed bandits (Schulz, Konstantinidis, & Speeken-                             mation gain (Srinivas et al., 2010), which seeks to learn the
brink, 2016) and is the only known computational algorithm                          global reward function as rapidly as possible. VG achieves at
to have any guarantees in a bandit setting (i.e., bounded re-                       least a constant fraction of the optimal information gain value
gret; Srinivas, Krause, Kakade, & Seeger, 2010).                                    (Krause & Guestrin, 2005); however, it fails to adequately
                                                                                    trade-off between exploration and exploitation, because ef-
Gaussian process learning. A GP defines a distribution
                                                                                    fort is wasted exploring the function where f (x) is small.
P( f ) over possible functions f (x) that map inputs x to output
                                                                                       The Mean Greedy (MG) strategy is also step-wise greedy,
y, in our case, grid location to reward. A GP is completely
                                                                                    valuing arms using only the estimated mean reward
defined by a mean µ(x) and a kernel function, k(x, x0 ):
                    µ(x) = E [ f (x)]                                       (2)                                qMG (x) = µ(x)                           (9)
               k(x, x0 ) = E ( f (x) − µ(x))( f (x0 ) − µ(x0 ))
                                                                  
                                                                            (3)     although this strategy carries no known guarantees and is
                                                                                    prone to getting stuck in local optima.
Here, we fix the prior mean to the median value of payoffs,
                                                                                       Upper confidence bound sampling (UCB) combines the
µ(x) = 50 and use a radial basis function kernel (Eq. 7).
                                                                                    VG and MG strategies
   Suppose we have collected observations yT =
[y1 , y2 , . . . , yT ]> at inputs XT = {x1 , . . . , xT }, and assume                                   qUCB (x) = µ(x) + βσ(x)                      (10)
                       yt = f (xt ) + εt      εt ∼ N (0, 1)                 (4)          1 We also considered Probability of Improvement and Probabil-
                                                                                    ity of Maximum Utility (Speekenbrink & Konstantinidis, 2015) as
Given a GP prior on functions f (x) ∼ GP                      (µ(x), k(x, x0 )),    alternate decision strategies, but have omitted them because they
the posterior distribution over f (xT ) given inputs XT is also                     failed to reach performance comparable to UCB.
                                                                                1358

where the exploration factor β determines how the reduction                                                       Experiment
of uncertainty trades off against exploiting high expected re-                           We present a bi-variate MAB problem with spatially corre-
wards. This is sometimes referred to as optimistic “sampling                             lated rewards. The problem space was represented by a two-
with confidence” as it inflates expectations with respect to                             dimensional grid, measuring 11×11, resulting in 121 unique
the upper confidence bounds (Srinivas et al., 2010), creating                            tiles in total. Participants could click to reveal unexplored
a natural balance between exploration and exploitation.                                  tiles or re-click previously uncovered tiles to exploit known
                                                                                         rewards (see Fig. 2 top row for screenshots).
                                                                                         Methods
                                                                                         Participants. We recruited 80 participants from Amazon
                                                                                         Mechanical Turk (25 Female; mean age ± SD 32 ± 9). Each
                                                                                         participant was paid a participation fee of $0.50 and a per-
                                                                                         formance contingent bonus up to $1.50. Subjects earned on
                                                                                         average $1.64 ± 0.20 and spent 8 ± 4 minutes on the task.
           GP Posterior                             GP Posterior
                                                                                         Design. We used a 2×2 between subject design, where par-
                                                                                         ticipants were randomly assigned to one of two different
                                 μ(x)                                     μ(x)
                                                                                         pay-off structures (Average Reward vs. Maximum Reward)
                                      75                                       75
                                                                                         and one of two different classes of environments (Smooth
                                      50                                       50
                                                                                         vs. Rough). Each grid represented a bi-variate function,
                                      25                                       25        with each observation including normally distributed noise,
                                                                                         ε ∼ N (0, 1). The task was presented over 8 blocks on differ-
          UCB Prediction                           UCB Prediction
                                                                                         ent grid worlds drawn from the same class of environments.
                                                                                         In each block, participants had either a Short (20 clicks) or
                                 q(x)                                     q(x)           Long (40 clicks) search horizon to interact with the grid. The
                                      0.225
                                                                               0.3       search horizon alternated between blocks (within subject),
                                      0.200
                                      0.175
                                                                               0.2       with initial horizon length counterbalanced between subjects.
                                      0.150                                    0.1       Per block, observations were scaled to a uniformly sampled
                                      0.125                                              maximum value in the range of 65 to 85, so that the value of
                                                                                         the global optima could not be easily guessed (e.g., a value of
    Softmax Choice Probabilities             Softmax Choice Probabilities                100).
                                 P(x)                                     P(x)
                                                                                         Materials and procedure. Before starting, participants
                                                                                         were shown four fully revealed grids in order to familiarize
                                                                               0.75
                                      0.10                                               themselves with the task. Example environments were drawn
                                                                               0.50
                                      0.05                                               from the same class of environments assigned to the partic-
                                                                               0.25
                                                                                         ipant (Smooth or Rough) and underwent the same random
                                                                                         scaling of observations. Additionally, three comprehension
Figure 2: Modeling human performance. Column left represents the                         questions were used to ensure full understanding of the task.
initial state of the task and column right is after 10 clicks. Top row:                     At the beginning of each of the 8 blocks, one random
screenshots from the experiment. 2nd row: posterior predictions of                       tile was revealed and participants could use their mouse to
expected reward µ(x), from a GP with an RBF kernel (not shown:
the estimated variance). 3rd row: the values of each tile q(x) using                     click any of the 121 tiles in the grid until the search hori-
the UCB acquisition function. Bottom row: the softmax prediction                         zon was exhausted, including re-clicking previously revealed
surface transforming the UCB values into choice probabilities.                           tiles. Clicking an unrevealed tile displayed the numerical
Choice Probabilities                                                                     value of the reward along with a corresponding color aid,
For all models, we use a softmax function (Fig. 2 bottom row)                            where darker colors indicated higher point values (Fig. 1).
to convert the value of an option q(x) into a choice probability                         Previously revealed tiles could also be re-clicked, although
                                                                                         there were variations in the observed value due to noise. For
                                         exp(q(x)/τ)                                     repeat clicks, the most recent observation was displayed nu-
                       P(x) =                                                   (11)
                                  ∑Nj=1 exp(q(x j )/τ)                                   merically, while hovering over the tile would display the en-
                                                                                         tire history of observations. The color of the tile corresponded
where τ is the temperature parameter. As τ → 0 the highest-
                                                                                         to the mean of all previous observations.
value arm is chosen with a probability of 1 (i.e., argmax), and
when τ → ∞, all options are equally likely, with predictions                             Payoff conditions. We compared performance under two
converging to random choice. We use τ as a free parame-                                  different payoff conditions, requiring either a balance be-
ter, where lower estimates can be interpreted as more precise                            tween exploration and exploitation (Average Reward) or a
predictions about choice behavior.                                                       pure exploration context (Maximum Reward). Previous work
                                                                                     1359

has shown that people can adapt (sometimes with great dif-               displaying more variance in the locations sampled (t(78) =
ficulty) to different payoff conditions in information acquisi-          −2.48, p = .02). There were some differences in the number
tion tasks (Meder & Nelson, 2012).                                       of unique tiles revealed (Fig. 3C) and the number of repeat
   In each payoff condition, participants received a perfor-             clicks across the payoff conditions (Fig. 3D), although the ef-
mance contingent bonus of up to $1.50. Average Reward par-               fect size is largest for smooth environments given long search
ticipants were told to “gain as many points as possible across           horizons. However, these behavioral differences did not man-
all 8 grids” and were given a bonus based on the average                 ifest in terms of performance, with no systematic differences
value of all clicks as a fraction of the global optima, T1 ∑( yy∗t ),    across payoff conditions in terms of the average reward ob-
where y∗ is the global optimum. Maximum Reward partic-                   tained t(78) = 1.32, p = .2) or in the maximum revealed re-
ipants were told to “learn where the largest reward is” and              ward (t(78) = .001, p = .99).
were giving a bonus using the ratio of the highest observed              Environment and horizon. Independent of the payoff
reward to the global optimum, ( max    yt 4
                                     y∗ ) , taken to the power of
                                                                         condition, participants assigned to Smooth environments
4 to exaggerate differences in the upper range of performance            achieved higher average rewards (t(78) = 6.55, p < .001)
and for parity in expected earnings across payoff conditions.            and higher maximum rewards (t(78) = 5.45, p < .001), than
All 8 blocks were weighted equally, using noisy but unscaled             those assigned to the Rough environments (Fig. 3E), suggest-
observations to assign a bonus of up to $1.50. Subjects were             ing that stronger correlations of payoffs make the task easier.
informed in dollars about the bonus earned at the end of each            Interestingly, longer horizons did not lead to better overall
block.                                                                   performance in the Average Reward condition (t(80) = .34,
                                                                         p = .73), although participants given longer horizons found
Smoothness of the environment. We used two different                     larger maximum rewards for all payoffs and environment
classes of environments, corresponding to different levels of            conditions (t(158) = 7.62, p < .001). There may be a less-is-
smoothness (Fig. 1). All environments were sampled from a                more-effect, with longer horizons leading to over-exploration,
GP prior parameterized with a RBF kernel, where the length-              given the goal of maximizing average rewards.
scale parameter (λ) determines the rate at which the correla-
tions of rewards decay over distance. We sampled 20 Smooth
environments using λ = 2 and 20 Rough environments us-                    (A)                         Average Reward Earned                                 (B) Maximum Reward Revealed
ing λ = 1. Subjects performed the task on 8 grids randomly                                                  Rough                    Smooth                                                 Rough                    Smooth
                                                                         Avg. Reward (±SE)                                                                 Max. Reward (±SE)
                                                                                         80                                                                               100
drawn (without replacement) from their assigned class of en-                             70                                                                                    90
vironments, while the four fully revealed environments used                              60                                                                                    80
to familiarize subjects with the task were drawn (without re-                            50                                                                                    70
placement) from the remaining 12 environments.                                           40                                                                                    60
                                                                                                  0    10     20    30    40 0     10     20    30    40                             0    10 20 30 40 0            10 20 30 40
Search horizons. The length of the search horizon influ-                                                                  Trial                                                                           Trial
ences the value of information learned about the environment,                       Horizon                   Long        Short     Payoffs            Avg. Reward Condition                         Max. Reward Condition
with respect to the assigned payoff condition. Longer hori-               (C)                                      Unique Tiles                             (D)                                   Repeat Clicks
zons provide more opportunities for exploiting acquired in-                                                 Rough                    Smooth                                                 Rough                    Smooth
                                                                         Unique Tiles (±SE)                                                             Repeat Clicks (±SE)
                                                                                          40                                                                              12.5
formation, thereby making early exploration more valuable.                                30                                                                              10.0
                                                                                                                                                                               7.5
We chose two horizon lengths (Short= 20 and Long= 40) that                                20
                                                                                                                                                                               5.0
were fewer than the total number of tiles on the grid (121),                              10                                                                                   2.5
and varied within subject (alternating between blocks).                                       0                                                                                0.0
                                                                                                      Short        Long           Short        Long                                      Short     Long           Short     Long
                                                                                                                   Search Horizon                                                                 Search Horizon
                             Results                                                                               Payoffs         Avg. Reward Condition                             Max. Reward Condition
Figure 3 shows task performance. In all conditions, perfor-               (E)                                                           Distribution of Rewards
                                                                                                              Rough                        Rough                                         Smooth                    Smooth
mance improved as a function of the trial number (i.e., with                                                  Short                        Long                                           Short                     Long
each additional click), as measured by both the overall cor-                           0.020
                                                                                       0.015
                                                                        Density
relation between average reward and trial number (r = .32,                             0.010
p = .04) and between the maximum observed reward and                                   0.005
                                                                                       0.000
trial number (r = .83, p < .001). There were no learning                                              0     25 50 75 100 0                25 50 75 100 0                             25 50 75 100 0               25 50 75 100
effects across blocks (i.e., over successive grids), indicated                                                                                        Payoff value
by a lack of correlation between average reward and block                                                          Payoffs          Avg. Reward Condition                            Max. Reward Condition
number (r = .19, p = .65), or between maximum reward and                 Figure 3: Overview of task performance. (A) Average reward earned
block number (r = −.37, p = .36). Performance improved as                and (B) maximum reward revealed, where colors correspond to pay-
more information was revealed (i.e., over trials), but not over          off condition and line-types to horizon length. Black lines show
                                                                         simulated performance of a random model averaged over 10,000
additional blocks of identically parameterized environments.             randomly sampled grids. (C) The average number of unique tiles
Payoff conditions. Payoff conditions influenced search be-               clicked in each block and (D) the average number of repeat clicks
                                                                         in each block. (E) The distribution of rewards earned during each
havior, with participants in the Maximum Reward condition                block, grouped first by environment type and then by horizon length.
                                                                    1360

Model Comparison                                                                             local variant of each GP model (Local GP), which weighs the
We describe each model’s ability to predict participant behav-                               q(x) for each arm by the inverse Manhattan distance to the
ior using leave-one-block-out cross validation. For each par-                                previous choice, qLocal (xt ) = q(xt ) · IMD(xt , xt−1 ). Adding
ticipant, we analyzed the four short and the four long horizon                               locality to the GP models only improved prediction accuracy
blocks separately. Cross-validation was performed by hold-                                   (Fig. 4 right), with the Local GP-UCB model having the high-
ing out a single block as a test set, and fitting the model pa-                              est overall out-of-sample prediction accuracy (R2 = .38).
rameters using a maximum likelihood estimate (MLE) on the                                       Overall, the modeling results show that humans display
remaining three blocks. Iterating through each of the four                                   a preference for local search, but that locality alone fails to
hold-out blocks, for both short and long horizons, we calcu-                                 achieve comparable performance levels. The best model (Lo-
lated a model’s out-of-sample log loss (i.e., test set prediction                            cal GP-UCB) incorporated this tendency for local search into
accuracy) and then summed up the results over all blocks. We                                 a computational model that combines function learning with
use McFadden’s R2 values (McFadden, 1974) to compare the                                     a decision strategy explicitly trading off between both high
out-of-sample log loss for each model to that of a random                                    expected rewards and high uncertainty.
model (Fig. 4), where R2 = 0 indicates chance performance
                                                                                                                                                   Model Performance
and R2 = 1 is a perfect model.                                                                                                                                    100
                                             Model Comparison                                              70                                                                                         ●
                                                                                                                                                                                                           ●   ●
                                                                                                                                                                                                                    Environment
                                                                                                                                                        Max. Reward
                                                                                                                                                                                                               ●
                                                                                                 Avg. Reward
                                                                                                                                                                                                  ●
                                                                                                                                                                      90                                   ●
                         Simple Strategies         GP                    Local GP
                                                                                                                                                                                             ●        ●
                                                                                                                                                                                                  ●
                                                                                                                                                                                                                        Smooth
McFadden's R 2 ( ± SE)
                                                                                                                                                                                         ●
                                                                                                                                                                                             ●
                                                                                                                                                                                    ●    ●                              Rough
                     0.4                                                                                   60                                                         80            ●
                                                                                                                                                                                                                    Model
                     0.3
                                                                                                                                                                                ●                                   ●   Random
                                                                                                                                                                      70        ●                                       Local Search
                     0.2                                                                                   50   ●   ●   ●    ●   ●    ●   ●    ●   ●                                                                    GP−UCB
                                                                                                                                                                                                                        LocalGP−UCB
                                                                                                                ●   ●   ●    ●   ●    ●   ●    ●   ●
                                                                                                                                                                                                                        Human
                     0.1                                                                                                                                              60
                                                                                                                0       10       20       30       40                      0●       10       20       30       40
                     0.0                                                                                  Trial              ●
                                                                                                                                   Trial
                           Local WSLS        VG    MG    UCB        VG     MG       UCB
                                                                                             Figure 5: Comparison of simulated model performance over
                           Reward Condition        Average Reward   Maximum Reward           10,000 replications, where parameters were sampled from the cross-
                                                                                             validated MLEs of the subject population. Human results are aver-
Figure 4: Model Comparison. The height of the bars show the                                  aged across payoff conditions and horizon length.
group mean and error bars indicate standard error. McFadden’s R2
is a goodness of fit measure comparing each model Mk to a ran-                               Parameter Estimation
dom model Mrand . Using the out-of-sample log loss for each model,                           Figure 6 shows the cross-validated parameter estimates of the
R2McF = 1 − log L (Mk )/ log L (Mrand ).                                                     best predicting Local GP-UCB model. The estimates indi-
   A large amount of the variance in participant behavior is                                 cate subjects systematically under-estimated the smoothness
explained by local search (R2 = .28; all conditions); how-                                   of the underlying environments, with λ values lower than the
ever, locality alone fails to achieve similar task performance                               true underlying function (λSmooth = 2, λRough = 1), for both
as humans, with performance almost identical to random in                                    Rough environments (t(36) = −4.80, p < .001) and Smooth
terms of average reward and worse than random in maximum                                     environments (t(42) = −18.33, p < .001), using the median
reward (Fig. 5). WSLS by comparison, was a poor approxi-                                     parameter estimate for each subject. Participants not only had
mation of search behavior (R2 = .05), and was excluded from                                  a tendency towards selecting local search options, but also
the model performance comparison.                                                            made local inferences about the correlation of rewards.
   Among the GP models, UCB performed best (R2 = .23),                                          All participants valued the reduction of uncertainty (β >
with MG showing comparable results (R2 = .17) and VG per-                                    0), with long horizons often yielding larger β estimates than
forming poorly (R2 = .01). Interestingly, the performance of                                 short horizons (51 out of 80 subjects; t(79) = −2.02, p =
the GP-UCB model was remarkably similar to human sub-                                        .047)3 . There were no differences between payoff conditions
jects in terms of both average and maximum reward (Fig. 5).                                  (t(78) = −1.65, p = .1) or environments (t(78) = .5, p > .1).
Both humans and the GP-UCB model explore beyond what                                            Subjects in the average reward condition yielded smaller
is adaptive in the average reward context as evidenced by the                                estimates of the softmax temperature parameter (τ) than those
peak around t = 15, continuing to explore after most high-                                   in the maximum reward condition (t(78) = −2.66, p = .009),
value rewards have been revealed and thus failing to consis-                                 This is consistent with almost all models making better pre-
tently improve average rewards2 .                                                            dictions for average reward than for maximum reward sub-
   To harmonize the different aspects of human behavior cap-                                 jects (Fig. 4), since smaller values of τ indicate more precise
tured by local search and by the GP-UCB model, we added a                                    predictions. The larger number of unique tiles searched in the
                                                                                             maximum reward condition (Fig. 3C) may indicate a more
    2 Note that the peak in average reward for the GP-UCB is due to                          difficult prediction problem.
the use of human parameter estimates, whereas a GP-UCB model
with optimized hyper-parameters and a dynamic β is known to
achieve sublinear regret bounds (i.e., monotonically increasing av-                             3 Because horizon length varied within subjects, we compare the
erage reward; Srinivas et al., 2010)                                                         aggregate mean of the cross-validated parameter estimates for β.
                                                                                          1361

                              Parameter Estimates: Local GP−UCB
                                                                                            extent of spatial correlation of rewards, preferring instead to
  Estimate (log scale)
                     1.0                                                                    make very local inferences about unexplored rewards.
                                                                                                                  Acknowledgments
                     0.1                                                                    CMW is supported by the International Max Planck Research
                                                                                            School on Adapting Behavior in a Fundamentally Uncertain World.
                                                                                            ES is supported by the UK Centre for Financial Computing and
                           λ (Length−Scale)    β (Exploration Bonus)   τ (Temperature)
                                                                                            Analytics. BM was supported by Grant ME 3717/2-2, JDN was
Figure 6: Cross-validated parameter estimates for the Local GP-                             supported by Grant NE 1713/1-2 from the Deutsche Forschungs-
UCB model, showing the median estimate for each participant.                                gemeinschaft (DFG; SPP 1516). Code is available at https://
                                                                                            github.com/charleywu/gridsearch
                                     General Discussion
The results presented here can be seen as a first step towards                                                         References
uncovering how people search to acquire rewards in the pres-                                Bramley, N. R., Dayan, P., Griffiths, T. L., & Lagnado, D. A. (2017).
ence of spatial correlations. We have re-cast the multi-armed                                 Formalizing neuraths ship: Approximate algorithms for online
                                                                                              causal learning. Psychological Review, 124(3), 301.
bandit problem as a framework for studying both function-                                   Herbert, R. (1952). Some aspects of the sequential design of ex-
learning and the exploration-exploitation trade-off by adding                                 periments. Bulletin of the American Mathematical Society, 58,
spatial correlations to rewards. Within a simple experiment                                   527–535.
                                                                                            Hills, T. T., Jones, M. N., & Todd, P. M. (2012). Optimal foraging
about searching for rewards on a two-dimensional grid, we                                     in semantic memory. Psychological Review, 119, 431-40.
found that participants adapt to the underlying payoff condi-                               Hoppe, D., & Rothkopf, C. A. (2016). Learning rational temporal
tion, perform better in smooth than in rough environments,                                    eye movement strategies. Proceedings of the National Academy
                                                                                              of Sciences, 113, 8332-8337.
and—surprisingly—sometimes seem to perform as well in                                       Krause, A., & Guestrin, C. (2005). Near-optimal nonmyopic value
short as in long horizon settings.                                                            of information in graphical models. In Proceedings of the Twenty-
   Our modeling results show a tendency to prioritize local                                   First Conference Annual Conference on Uncertainty in Artificial
                                                                                              Intelligence (UAI-05) (pp. 324–331).
search options, which may indicate the presence of innate                                   Krebs, J. R., Kacelnik, A., & Taylor, P. (1978). Test of optimal
search costs (e.g., mouse movements or some additional cog-                                   sampling by foraging great tits. Nature, 275, 27–31.
nitive processing). Even accounting for this local search be-                               Lawrence, N., Seeger, M., & Herbrich, R. (2003). Fast sparse Gaus-
                                                                                              sian process methods: The informative vector machine. In Pro-
havior, our best predicting model (Local GP-UCB) indicates                                    ceedings of the 16th Annual Conference on Neural Information
that people still systematically underestimate the extent of                                  Processing Systems (pp. 609–616).
spatial correlation of rewards, preferring instead to make very                             Lucas, C. G., Griffiths, T. L., Williams, J. J., & Kalish, M. L. (2015).
                                                                                              A rational model of function learning. Psychonomic Bulletin &
local inferences about unexplored rewards. Additionally, we                                   Review, 22, 1193–1215.
also found that search behavior was best predicted by a com-                                March, J. G. (1991). Exploration and exploitation in organizational
bination of both high expected reward and high uncertainty,                                   learning. Organization Science, 2, 71–87.
                                                                                            McFadden, D. (1974). Conditional logit analysis of qualitative
embodied in the UCB decision strategy, which implicitly ne-                                   choice behavior. Frontiers in Econometrics, 1(2), 105–142.
gotiates the exploration-exploitation trade-off.                                            Meder, B., & Nelson, J. D. (2012). Information search with
   Future studies could expand on this work by assessing a                                    situation-specific reward functions. Judgment and Decision Mak-
                                                                                              ing, 7, 119–148.
more diverse and perhaps combinatorial set of kernel func-                                  Neal, R. M. (2012). Bayesian learning for neural networks
tions (Schulz, Tenenbaum, Duvenaud, Speekenbrink, & Ger-                                      (Vol. 118). Springer Science & Business Media.
shman, 2016) or by speeding up GP-inference using approxi-                                  Schulz, E., Huys, Q. J., Bach, D. R., Speekenbrink, M., & Krause,
                                                                                              A. (2016). Better safe than sorry: Risky function exploitation
mation methods such as sparse inference (Lawrence, Seeger,                                    through safe optimization. In Proceedings of the 38th Annual
& Herbrich, 2003) or more parsimonious neural network rep-                                    Conference of the Cognitive Science Society (pp. 1140–1145).
resentations (Neal, 2012). Indeed, the result that participants                             Schulz, E., Konstantinidis, E., & Speekenbrink, M. (2016). Putting
                                                                                              bandits into context: How function learning supports decision
formed only very local beliefs about spatial correlations could                               making. bioRxiv. Retrieved from http://www.biorxiv.org/
be used to find heuristic approximations to GP models in the                                  content/early/2016/10/14/081091
future, which could effectively trade-off a small loss in accu-                             Schulz, E., Tenenbaum, J., Duvenaud, D. K., Speekenbrink, M., &
                                                                                              Gershman, S. J. (2016). Probing the compositionality of intuitive
racy for reduced computational complexity.                                                    functions. In Advances in neural information processing systems
                                                                                              (pp. 3729–3737).
                                              Conclusion                                    Speekenbrink, M., & Konstantinidis, E. (2015). Uncertainty and
                                                                                              exploration in a restless bandit problem. Topics in Cognitive Sci-
We compared both simple strategies and more complex func-                                     ence, 7, 351–367.
tion generalization models in their ability to make out-of-                                 Srinivas, N., Krause, A., Kakade, S. M., & Seeger, M. (2010). Gaus-
sample predictions about participant sampling behavior. Our                                   sian Process Optimization in the Bandit Setting: No Regret and
                                                                                              Experimental Design. Proceedings of the 27th International Con-
modeling results indicate that there may be innate search                                     ference on Machine Learning (ICML 2010), 1015–1022.
costs, creating a tendency to prioritize local search options.                              Steyvers, M., Lee, M. D., & Wagenmakers, E.-J. (2009). A bayesian
Furthermore, even accounting for this local search behavior,                                  analysis of human decision-making on bandit problems. Journal
                                                                                              of Mathematical Psychology, 53, 168–179.
our best performing model (Local GP-UCB) indicates that                                     Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An
people also have a systematic tendency to underestimate the                                   introduction (Vol. 1) (No. 1). Cambridge, MA: MIT Press.
                                                                                         1362

