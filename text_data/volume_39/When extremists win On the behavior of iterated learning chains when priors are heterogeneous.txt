When extremists win: On the behavior of iterated learning chains when priors are
                                                           heterogeneous
                                           Daniel J. Navarro (d.navarro@unsw.edu.au)
                                          School of Psychology, University of New South Wales
                                           Amy Perfors (amy.perfors@adelaide.edu.au)
                                               School of Psychology, University of Adelaide
                                                 Arthur Kary (art.kary@gmail.com)
                                          School of Psychology, University of New South Wales
                                           Scott Brown (scott.brown@newcastle.edu.au)
                                              School of Psychology, University of Newcastle
                                          Chris Donkin (christopher.donkin@gmail.com)
                                          School of Psychology, University of New South Wales
                               Abstract
   How does the process of information transmission affect the
   cultural products that emerge from that process? This question
   is often studied experimentally and computationally via iter-
   ated learning, in which participants learn from previous partic-
   ipants in a chain. Much research in this area builds on math-        Figure 1: Schematic illustration of a typical iterated learning
   ematical analyses suggesting that iterated learning chains con-      paradigm, which assumes that learner n learns on the basis of the
   verge to people’s priors. We present three simulation studies        data provided by learner n − 1.
   suggesting that when the population of learners is heteroge-
   neous, the behavior of the chain is systematically distorted by
   the learners with the most extreme biases. We discuss implica-          Importantly, the theoretical proofs about how iterated
   tions for the use of iterated learning as a methodological tool      learning chains converge depend critically on the assump-
   and for the processes that might have shaped cultural products       tions made. For example, if learners select the hypothesis
   in the real world.
                                                                        with the highest posterior probability rather than sample from
   Keywords: Iterated learning; language evolution; cultural
   evolution; inductive biases; Bayesian cognition                      their posterior, an iterated learning chain will tend to exag-
                                                                        gerate the prior (Kirby, Dowman, & Griffiths, 2007). Simi-
   Which aspects of our language or culture are shaped by the           larly, we use language to talk about things and events in the
inductive biases possessed by people, and which aspects are             world. If one changes the mathematical assumptions to re-
shaped by the process of transmission from one learner to the           flect this insight, then the stationary distribution of the chain
next? A key framework for thinking about and disentangling              more closely resembles the posterior distribution (Perfors &
these factors is known as iterated learning, shown schemat-             Navarro, 2014). In this paper we consider the role played
ically in Figure 1. Iterated learning is a particular kind of           by individual differences. Such differences are robustly ob-
cultural transmission in which behavior arises in one individ-          served in many areas of cognition, yet theoretical results typ-
ual (or generation) by learning from the observations of the            ically assume that all learners share the same biases.
previous person (generation), forming a chain of learners.                 When individual differences exist, what should we expect
   An appealing characteristic of iterated learning is that the         to observe? One possibility is that the chain converges to a
behavior of iterated learning chains can be characterized               distribution that reflects the “average prior belief” in some
mathematically: under certain assumptions, iterated learn-              sense. For instance, if 10% strongly believe in hypothesis
ing chains with Bayesian learners will converge to a distri-            A and 90% of people strongly believe in hypothesis B, one
bution that depends on the learners’ priors and the size of the         might hope that an iterated learning chain reflects 10% A and
bottleneck (Griffiths & Kalish, 2007; Rafferty, Griffiths, &            90% B hypotheses. Alternatively, perhaps the chain will pro-
Klein, 2014). These results have allowed researchers to ex-             duce some other reasonable compromise between A and B
plore inductive biases in different tasks, including function           that weights each learner in equal proportion. Our findings
learning (Kalish, Griffiths, & Lewandowsky, 2007), visual               indicate that neither of these situations necessarily occurs: if
working memory (Lew & Vul, 2015), reasoning about every-                people do not share the same priors, iterated learning is not
day events (Lewandowsky, Griffiths, & Kalish, 2009), and                guaranteed to converge to the prior in any meaningful sense.
category learning (Canini, Griffiths, Vanpaemel, & Kalish,              Instead, the distribution to which it does converge is dispro-
2014). They have been especially useful in studying language            portionately influenced by the most biased learners. We illus-
evolution (Kirby, Griffiths, & Smith, 2014).                            trate this using three simulation studies.
                                                                    847

         Case study 1: Language evolution                                                     10
                                                                                                                Weak Learners
                                                                                                                                                                             10
                                                                                                                                                                                           Strong Learners
                                                                                                                                                                                                                                                            10
                                                                                                                                                                                                                                                                              Equal Mixture
                                                                           Average Response                                                               Average Response                                                               Average Response
                                                                                              8                                                                              8                                                                              8
Do all learners have equal influence on the process of lan-                                                         ●●
                                                                                                                       ●●●●
                                                                                                                            ●●●●●●●●●●●
                                                                                              6                 ●                                                            6                                                                              6
guage evolution? Consider the pressures on a language to in-                                  4
                                                                                                    ●
                                                                                                        ●
                                                                                                            ●
                                                                                                                                                                             4
                                                                                                                                                                                   ●
                                                                                                                                                                                                                                                            4
                                                                                                                                                                                                                                                                  ●
                                                                                                                                                                                                                                                                      ●
corporate a particular grammatical rule or not. Some learners                                 2                                                                              2
                                                                                                                                                                                       ●
                                                                                                                                                                                                                                                            2
                                                                                                                                                                                                                                                                          ●
                                                                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                                                                                  ●●
                                                                                                                                                                                                                                                                                     ●●●●●●●●●●●●●●●
                                                                                                                                                                                           ●
may have STRONG opinions about a particular rule or con-                                      0                                                                              0
                                                                                                                                                                                               ●
                                                                                                                                                                                                   ●●●●●●●
                                                                                                                                                                                                           ●●●●●●●●●●
                                                                                                                                                                                                                                                            0
                                                                                                                                                                                                                                                                                  ●                Iterated
                                                                                                                                                                                                                                                                                                   Prior
struction, whereas others might have WEAK opinions. Ex-                                                             5       10          15           20                                            5       10          15           20                                            5        10          15           20
actly who has which might might vary with the particular lin-                                                           Iteration                                                                      Iteration                                                                      Iteration
guistic context and construction involved: for instance, chil-                                0.6                                                                            0.6                                                                            0.6               ●           Final Iterated
                                                                                                                                                                                                                                                                                          Prior
dren may to have a bias for regularization that adults do not                                                                                                                          ●
                                                                                              0.4                                                                            0.4                                                                            0.4
share (Hudson Kam & Newport, 2005), but adult second-
language learners may have biases based on transfer from                   Probability                                                                    Probability                      ●
                                                                                                                                                                                                                                         Probability
                                                                                              0.2                                                                            0.2                                                                            0.2       ●
                                                                                                                                                                                                                                                                          ●
their first language while children do not (Ellis, 2015). We are                                                                ●
                                                                                                                                    ●
                                                                                                                                        ●
                                                                                                                                             ●
                                                                                                                                                 ●   ●
                                                                                                                                                                                                   ●
                                                                                                                                                                                                                                                                                  ●
                                                                                                                                                                                                                                                                                      ●
                                                                                                                                                                                                                                                                                           ●
                                                                                                                            ●                                                                                                                                                                  ●
                                                                                                                        ●                                                                              ●
fairly agnostic at this point about what such biases might be;                                0.0
                                                                                                        ●
                                                                                                            ●
                                                                                                                    ●
                                                                                                                                                                             0.0
                                                                                                                                                                                                           ●
                                                                                                                                                                                                               ●   ●   ●    ●   ●   ●
                                                                                                                                                                                                                                                            0.0
                                                                                                                                                                                                                                                                                                   ●
                                                                                                                                                                                                                                                                                                       ●    ●   ●   ●
                                                                                                        0           2       4       6        8       10                                0           2       4       6        8       10                                0           2        4       6        8       10
all that matters for the present purposes is that it is plausible                                                       Response                                                                       Response                                                                       Response
that there are individual differences in at least some language
learning biases. Our question is what effect this might have              Figure 2: Simulating an iterated learning investigation of language
on the nature of the evolved language.                                    evolution. When the learners all share the same bias (left and mid-
                                                                          dle columns) the average proportion of responses converges to the
   To study this, consider the following experimental design.             prior mean (top row), and the distribution of responses converges to
Participants are presented with sentences in an artificial lan-           the prior distribution (bottom row). When the chain is a mixture of
guage that may incorporate a construction (e.g., pluralization            STRONG and WEAK learners, the average proportion of responses
                                                                          does not correspond to the average prior expectation, nor does the
rule, morphological marking, etc). After training, participants           distribution converge to the average prior in the population.
are asked to produce new sentences, which are presented as
the input to the next learner in the chain. This is a relatively
                                                                          Simulation
typical design, and a simple Bayesian model for this learning
problem can be constructed as follows.                                    We simulate the results of three different kinds of iterated
   If θ denotes the probability that the grammatical rule                 learning experiments. In all cases, the first person is taught
should be followed, a Bayesian learner specifies a prior distri-          ten sentences in an artificial language, five consistent with a
bution P(θ). For simplicity we assume a Beta(a, b) distribu-              grammatical rule; they then generate ten sentences used as in-
tion in which P(θ) ∝ θa−1 (1 − θ)b−1 . In our simulations we              put to the next learner. In the first experiment all learners have
assume that some learners enter with a STRONG bias about the              a STRONG bias about the rule, and in the second experiment
grammatical rule, formalized via a Beta(1,10) prior. In con-              all of them have a WEAK bias in the opposite direction. In
trast, a WEAK learner might have the opposite bias, but not a             the third experiment, half of the learners have STRONG biases
strong one, which can be formalized with a Beta(2,1) prior.               and half have WEAK opposing ones. In each case results are
Regardless of the biases the learner possesses, it is assumed             aggregated across 100,000 simulated iterated learning chains.
that belief updating follows Bayes’ rule. After a training ses-              The results are shown in Figure 2. As predicted by previ-
sion in which x of n sentences follow the rule, the posterior             ous work, in both of the homogeneous cases iterated learning
distribution P(θ | x) is                                                  experiment transparently reveals the learner biases: the chain
                                                                          converges to the prior. However, when we consider the iter-
                    P(θ | x) ∝ P(x | θ)P(θ)                  (1)          ated learning experiment conducted with a mixed population
                                                                          (right panels of Figure 2) we observe a strikingly different re-
where P(x | θ) ∝ θx (1 − θ)n−x is the probability of observ-
                                                                          sult. In this situation – where half of the learners are STRONG
ing x out of n rule-consistent cases if the true probability
                                                                          and half are WEAK – the average bias in the population is to
is θ. Under these assumptions, the posterior over θ is a
                                                                          expect 38% of sentences to be rule-consistent. Yet, as the
Beta(a + x, b + n − x) distribution. When asked to generate
                                                                          top right panel shows, the iterated learning chain converges
a novel sentence, a Bayesian learner might sample a value
                                                                          to a smaller number, with only 27% of responses following
of θ from their posterior, and their output satisfies the rule
                                                                          the rule. More importantly, as the bottom right panel reveals,
with probability θ. The number of rule-consistent sentences
                                                                          the distribution of responses bears very little resemblance to
y generated by the learner is thus sampled from the posterior
                                                                          the underlying population biases. One might have hoped that,
predictive distribution P(y|x):
                                                                          when learners bring different priors to an iterated learning ex-
                            Z 1                                           periment, the chain would converge to a weighted average of
                 P(y|x) =         P(y|θ)P(θ|x)dθ             (2)          their priors. In this case, this weighted average would be a
                             0
                                                                          50-50 mixture of the priors of STRONG learners and WEAK
This kind of model is often used to study regularization in               learners (plotted as a histogram). As the figure illustrates, the
iterated learning designs (Ferdinand, Thompson, Kirby, &                  iterated learning chain (lines) does not converge to anything
Smith, 2013; Reali & Griffiths, 2009).                                    even remotely similar to this mixture distribution.
                                                                    848

                0.6
                      Weak Learners in Mixed Chain
                                                                                    0.6
                                                                                          Strong Learners in Mixed Chain                         tations left by the previous one, forming an iterated learning
                                                                                                                  ●   Final Iterated
                                                                                                                      Prior                      chain. A Bayesian juror might reason about this by consider-
                0.4                                                                 0.4                                                          ing two hypotheses, namely that the evidence favors the plain-
  Probability                                                         Probability
                                                                                             ●
                                                                                                 ●
                                                                                                                                                 tiff (e = 1) or the defendant (e = 0). The trial evidence sets
                0.2                                                                 0.2              ●                                           the juror’s prior belief that P(e = 1) = θ, which is updated
                            ●   ●   ●
                                         ●                                                               ●
                                             ●
                        ●
                                                 ●
                                                     ●
                                                         ●   ●   ●
                                                                                                              ●
                                                                                                                  ●
                                                                                                                      ●
                                                                                                                                                 when the vote v of the preceding juror is revealed. The juror
                0.0                                                                 0.0                                   ●   ●   ●   ●
                        0       2        4       6       8       10                          0       2        4       6       8       10
                                                                                                                                                 unconsciously assigns a reliability value r to this information,
                                        Response                                                             Response
                                                                                                                                                 such that P(v = 1|e = 1) = P(v = 0|e = 0) = r. If the preced-
                                                                                                                                                 ing juror voted for the plaintiff, the juror’s posterior degree of
                                                                                                                                                 belief that the verdict should favor the plaintiff becomes
Figure 3: Distribution of responses in a mixed chain plotted as a
function of the type of learner generating the response.                                                                                                                                  rθ
                                                                                                                                                             P(e = 1 | v = 1) =                                (3)
                                                                                                                                                                                  rθ + (1 − r)(1 − θ)
Discussion
                                                                                                                                                 and the posteriors are calculated similarly when the earlier
Why does the iterated learning procedure behave this way                                                                                         vote favored the defendant. For simplicity, we assume that
when the population is heterogeneous? The answer can                                                                                             jurors generate their vote probabilistically by sampling from
be found by separating the responses on the last iteration                                                                                       the posterior.
by learner type, shown in Figure 3. As is clear from in-                                                                                            As these equations illustrate, when r = 0.5 the current juror
spection, the WEAK bias learners (left) are greatly influ-                                                                                       completely ignores the vote provided by the previous one and
enced by the STRONG bias learners: their responses are rule-                                                                                     the posterior probability is identical to the prior. This arises
consistent 36% of the time, rather than 67% as one might                                                                                         naturally when the current juror is confident that their exist-
expect given their Beta(2,1) prior, and the distribution of                                                                                      ing beliefs incorporate all relevant information about the case,
responses (lines) deviates markedly from their prior (his-                                                                                       and as such the opinions of other jurors can have no influence
togram). The opposite effect occurs too (right panel), but it is                                                                                 upon their own beliefs. We refer to such a juror as a GOAT –
much smaller: the STRONG bias learners increase the propor-                                                                                      someone who forms their own view and is not led to conclu-
tion of rule-consistent responses from the 9% rate implied by                                                                                    sions by the opinions of others. In contrast, suppose the juror
the Beta(1,10) prior to 17.5% in the iterated learning chain.                                                                                    is underconfident or unsure about their beliefs, perhaps sus-
Similarly, their distribution of responses is not markedly dif-                                                                                  pecting that other jurors have access to different information.
ferent from their prior.                                                                                                                         Such a juror will set r > 0.5, because they attribute eviden-
   As this example illustrates, when individual differences ex-                                                                                  tiary value to the opinions of others. We refer to this kind of a
ist an iterated learning procedure is not guaranteed to reveal                                                                                   juror as a SHEEP because they are more likely to adjust their
the inductive biases of the learner. The STRONG learners ap-                                                                                     vote to agree with the votes of others.
ply a strong inductive bias, and these learners require a lot of
evidence before they are willing (or able) to apply the gram-                                                                                    Simulations with homogeneous chains
matical rule in question. As a consequence, data generated                                                                                       We consider three scenarios. In the first scenario all jurors are
by a WEAK learner will have minimal ability to sway such a                                                                                       GOATS who set r = 0.5 and have a modest opinion in favor of
person. The reverse does not hold: the WEAK learners in this                                                                                     the defendant (θ = 0.4). In the second scenario all jurors are
scenario are very responsive to external input. As a result, a                                                                                   SHEEP who set r = 0.95 and have a modest opinion favoring
WEAK bias participant makes a much larger adjustment from                                                                                        the plaintiff (θ = 0.6). Finally we consider a situation where
the prior than does a STRONG bias one, with the consequence                                                                                      half of the jurors are SHEEP and the other half are GOATS. To
that the overall behavior of the mixed chain is much more                                                                                        illustrate what happens in these situations we simulated each
heavily driven by the group with the strongest bias.                                                                                             scenario 100,000 times. The results are plotted in Figure 4.
                                                                                                                                                 Not surprisingly, because the GOAT jurors ignore the input
                      Case study 2: Group decision making                                                                                        and generate responses directly from their own prior beliefs,
Groups of people often arrive at beliefs that seem to lack any                                                                                   the “chain” starts at their prior (on average, 40% of jurors
evidentiary basis, famously described by the “groupthink”                                                                                        vote for the plaintiff) and the total number of votes in favor of
phenomenon (Janis, 1982). How do these false beliefs arise?                                                                                      the plaintiff follows a binomial distribution.
Do they necessarily reflect a bias shared by all reasoners, or                                                                                      What should we expect to see if all jurors are SHEEP? One
can an entire community be misled by a small number of                                                                                           reading of the literature suggests that, since iterated learning
highly biased learners?                                                                                                                          chains of Bayesian learners converge to the prior, and since
   To examine this question, we consider a scenario in which                                                                                     the first SHEEP samples from their own prior, we should see
a jury of 12 people begin their deliberations with a straw poll.                                                                                 a result not dissimilar to the one we see for GOATS. That
A notepad is passed around the room, with each person writ-                                                                                      is – while we might expect to see non-independence among
ing down whether they would decide in favor of the plaintiff                                                                                     successive jurors – we should find that on average a SHEEP
before removing their sheet of paper and passing the pad to                                                                                      juror should vote for the plaintiff 60% of the time, in accor-
the next juror. Unfortunately, each juror can read the inden-                                                                                    dance with their priors. However, as the middle column of
                                                                                                                                           849

                                                         100% Goats                                                                                         100% Sheep                                                                                50% Sheep, 50% Goat                                       SHEEP chain violates the assumptions of the original proof,
                                      0.7                                                                                                0.7                                                                                                0.7
Probability of Voting for Plaintiff                                                                Probability of Voting for Plaintiff                                                                Probability of Voting for Plaintiff
                                                                                                                                                            ●
                                                                                                                                                                ●   ●       ●   ●   ●   ●    ●   ●                                                                                                              because the SHEEP jurors use the wrong likelihood function
                                                                                                                                                        ●
                                      0.6                                                                                                0.6                                                                                                0.6
                                                                                                                                                   ●
                                                                                                                                               ●
                                                                                                                                                                                                                                                                                                                for the learning problem. The SHEEP juror assigns eviden-
                                      0.5                                                                                                0.5                                                                                                0.5   ●
                                                                                                                                                                                                                                                      ●
                                                                                                                                                                                                                                                                                                                tiary value to the opinions of other jurors when they should
                                                                                                                                                                                                                                                           ●
                                                                                                                                                                                                                                                               ●   ●
                                                                                                                                                                                                                                                                                                                not, because all jurors have seen the same facts at trial. This
                                                                                                                                                                                                                                                                       ●       ●   ●   ●   ●    ●   ●
                                      0.4   ●   ●    ●   ●   ●   ●       ●   ●   ●   ●    ●   ●                                          0.4                                                                                                0.4
                                      0.3                                                                                                0.3                                                                                                0.3                                                                 miscalibration creates the “groupthink” behavior: the SHEEP
                                                2        4       6           8       10       12                                                   2        4       6           8       10       12                                                   2        4       6           8       10       12
                                                         Juror Number                                                                                       Juror Number                                                                                       Juror Number
                                                                                                                                                                                                                                                                                                                jurors “double count” the evidence, and the iterated learning
                                                         100% Goats                                                                                         100% Sheep                                                                                50% Sheep, 50% Goat
                                                                                                                                                                                                                                                                                                                chain exaggerates their prior bias.
                                      0.4                                                                                                0.4                                                                                                0.4
                                                                                                                                                                                                                                                                                                                Simulations with mixed chains
                                      0.3                                                                                                0.3                                                                                                0.3
                                                                                                                                                                                                                                                                                                                Now consider what happens when SHEEP and GOATS are
Probability                           0.2                                                          Probability                           0.2                                                          Probability                           0.2
                                                                                                                                                                                                                                                                                                                mixed together in equal proportions (Figure 4, right). The
                                      0.1                                                                                                0.1                                                                                                0.1
                                                                                                                                                                                                                                                                                                                SHEEP assign prior probability of 0.6 to the plaintiff, whereas
                                      0.0
                                            0        2       4       6       8       10 12
                                                                                                                                         0.0
                                                                                                                                               0        2       4       6       8       10 12
                                                                                                                                                                                                                                            0.0
                                                                                                                                                                                                                                                  0        2       4       6       8       10 12
                                                                                                                                                                                                                                                                                                                the GOATS assign prior 0.4, so the population average prior is
                                                    Total Votes for Plaintiff                                                                          Total Votes for Plaintiff                                                                          Total Votes for Plaintiff
                                                                                                                                                                                                                                                                                                                0.5. Alternatively, if we consider the behavior of the two ho-
                                                                                                                                                                                                                                                                                                                mogeneous iterated learning chains, the SHEEP on their own
Figure 4: The jury straw poll. The top row plots the probability that
each juror votes for the plaintiff, as a function of their position in                                                                                                                                                                                                                                          would be expected to converge to 0.67 and the GOATS would
the chain (the dashed line plots the population average prior), and                                                                                                                                                                                                                                             converge to 0.4, so the average of these two long run proba-
the bottom row plots the distribution of votes for the plaintiff. The                                                                                                                                                                                                                                           bilities is 0.54. If one did not know the detail of the models,
left and middle plots show juries composed entirely of GOATS and
SHEEP respectively. The plots on the right depict a scenario when                                                                                                                                                                                                                                               it would be reasonable to expect a mixed chain to produce
50% of jurors are SHEEP and 50% are GOATS.                                                                                                                                                                                                                                                                      an average probability of voting for the plaintiff somewhere
                                                                                                                                                                                                                                                                                                                between 50% and 54%. Unsurprisingly, it does nothing of
Figure 4 illustrates, this is not what happens. The first ju-                                                                                                                                                                                                                                                   the sort. Because GOATS are insensitive to the opinions of
ror votes in accordance with their priors, but by the time the                                                                                                                                                                                                                                                  others and SHEEP are highly sensitive, the GOATS dominate
12th juror is polled, the probability of voting for the plain-                                                                                                                                                                                                                                                  the mixed chain, and the long run behavior converges to a
tiff has risen to 67%. Moreover, it is simple to prove that                                                                                                                                                                                                                                                     43% probability of voting for the plaintiff. That is, the SHEEP
this reflects the true stationary distribution of the chain. To                                                                                                                                                                                                                                                 “learn” to mimic GOATS but the GOATS make no such accom-
see this, let p = P(vi = 1|vi−1 = 0) denote the probability                                                                                                                                                                                                                                                     modation.
that the ith juror in the chain votes for the plaintiff given that
the previous juror voted for the defendant, and similarly let                                                                                                                                                                                                                                                   Discussion
d = P(vi = 0|vi−1 = 1) denote the probability that the ith ju-                                                                                                                                                                                                                                                  The implications of the jury scenario are twofold. First, the
ror switches the other direction. The transition matrix for the                                                                                                                                                                                                                                                 SHEEP -only chain illustrates that it is possible for an iter-
strawpoll is thus                                                                                                                                                                                                                                                                                               ated learning chain to exaggerate biases even when Bayesian
                                                                                                                                                                                                                                                                                                                learners sample hypotheses from the posterior. The result
                                           
                             1− p      p
                     T=                                        (4)                                                                                                                                                                                                                                              complements an earlier result by Perfors and Navarro (2014),
                               d     1−d
                                                                                                                                                                                                                                                                                                                which showed that the convergence of iterated learning chains
A chain with this transition matrix converges to a stationary
                                                                                                                                                                                                                                                                                                                is affected when there is an additional input to the chain (i.e.,
distribution π in which the (marginal) probability of voting
                                                                                                                                                                                                                                                                                                                the world passes new information to learners). In the SHEEP
for the defendant and plaintiff is proportional to d and p re-
                                                                                                                                                                                                                                                                                                                chain we find that convergence is even influenced when learn-
spectively. To verify this, note that
                                                                                                                                                                                                                                                                                                              ers mistakenly believe there is additional information being
                              1− p     p                                                                                                                                                                                                                                                                        passed into the chain. This miscalibration drives a kind of
           π T ∝ [d, p]
                                d     1−d                                                                                                                                                                                                                                                                       groupthink, in which a collection of individually underconfi-
                                                                                         = [d(1 − p) + pd, d p + p(1 − d)]                                                                                                                                                                                      dent learners becomes overconfident as a group.
                                                                                         = [d, p] ∝ π                                                                                                                                                                                               (5)            Second, the behavior of a heterogenous chain is not easily
                                                                                                                                                                                                                                                                                                                predicted by considering the behavior of the corresponding
For a SHEEP juror, the probability of switching the vote from                                                                                                                                                                                                                                                   homogeneous chains, or the priors of individual learners. The
the plaintiff to the defendant is d = (.1 × .4)/(.1 × .4 + .9 ×                                                                                                                                                                                                                                                 mixed chain of SHEEP and GOATS is mostly driven by the
.6) = .069, and similarly the probability of switching the vote                                                                                                                                                                                                                                                 GOATS, even though a homogenous chain of GOATS produces
towards the plaintiff is p = (.1×.6)/(.1×.6+.9×.4) = .142.                                                                                                                                                                                                                                                      a much less extreme outcome than the a chain of pure SHEEP.
In the long run, a chain of SHEEP converges to a 67% prob-                                                                                                                                                                                                                                                      The reason for this is obvious when we consider the decision
ability of voting for the plaintiff even though each individual                                                                                                                                                                                                                                                 making strategies used by the two learner types, but we rarely
SHEEP only assigns a 60% prior probability to the plaintiff.                                                                                                                                                                                                                                                    have access to such information in real life.
   On the surface, the SHEEP result seems at odds with the
convergence proof in Kalish et al. (2007) - Bayesian learners                                                                                                                                                                                                                                                               Case study 3: Categorization
sampling from their posterior do not (in this instance) con-                                                                                                                                                                                                                                                    Our third case study considers a categorization problem with
verge to the prior. To that end, it is useful to note that the                                                                                                                                                                                                                                                  non-Bayesian learners. We consider stimuli that vary along
                                                                                                                                                                                                                                                                                                          850

                                                                                                                                                               Category Coherence
                                                                                                                                   Homogeneous Priors                                                                        Heterogenous Priors
                                                                                                                     6.0                       ● ● ● ●
                                                                                                                                                       ● ● ● ● ● ●                                             6.0
                                                                                                                                           ● ●
                                                                                                                                       ●                      λ = 10
                                                                                   Adjacent Items in Same Category                                                           Adjacent Items in Same Category
                                                                                                                                   ●                                                                                                   ● ● ● ● ● ● ● ● ● ●
                                                                                                                                                                                                                                 ● ● ●
                                                                                                                     5.5                                                                                       5.5           ●                        λ = 10
                                                                                                                               ●               ● ● ● ● ● ● ● ● ● ●                                                       ●
                                                                                                                                           ● ●
                                                                                                                                                                                                                                     ● ● ● ● ● ● ● ● ● ● ●
                                                                                                                                       ●
                                                                                                                                                              λ=1                                                                ● ●
                                                                                                                                                                                                                                                      λ=1
                                                                                                                     5.0           ●
                                                                                                                                                                                                               5.0           ●
Figure 5: Categorization with eight items that vary along one di-                                                              ●                                                                                         ●
                                                                                                                     4.5                                                                                       4.5
mension (top panel). Items can be organized into categories that are                                                                                                                                                             ● ● ● ● ● ● ● ● ● ● ● ● ●
coherent (left panel) or incoherent (right panel).                                                                                                                                                                           ●
                                                                                                                                                                                                                                                      λ = 0.1
                                                                                                                     4.0       ●
                                                                                                                                   ● ● ● ● ● ● ● ● ● ● ● ● ● ●                                                 4.0       ●
                                                                                                                                                              λ = 0.1
a single dimension, with 8 exemplars spaced evenly across                                                            3.5   ●                                                                                   3.5   ●
the range (i.e., at x = 1, . . . , 8): an example is shown at the                                                    3.0                                                                                       3.0
top of Figure 5. Each stimulus can be assigned to one of two                                                               0                5            10             15                                           0              5            10             15
categories (A or B), and we are interested in the inductive                                                                                     Iterations                                                                              Iterations
biases that people bring to this categorization problem.
                                                                                   Figure 6: Exploring the “category coherence” bias using iterated
   An iterated learning design can be used to explore these bi-                    learning. The y axis plots category coherence (defined in main text).
ases. During category learning, each learner is shown training                     Left panel: Category coherence assuming all participants share the
items that consist of four exemplars and their category labels,                    same prior (λ). Here there are three chains each reflecting one of
                                                                                   the three λ values. As λ grows higher, iterated learning produces
selected randomly subject to the constraint that there must                        more coherent categories. The grey dashed line reflects the average
be one exemplar of each category in the training set. During                       of the three chains on iteration 15. Right panel: When there are
the test phase the learner must classify the remaining four ex-                    individual differences within participants, the learners all become
                                                                                   somewhat more similar to one another but the effect is small.
emplars. An iterated learning chain is constructed by using
a random subset of responses from one learner as the train-
ing data for the next, again subject to the constraint that the                    we mixed learners that varied in their λ values (sampling uni-
learner must be shown at least one example of each category.                       formly at random from 0.1, 1 and 10) into a single chain
   In our simulations we assume each participant applies the                       to investigate the effect heterogeneity has on each learner
Generalized Context Model (GCM: Nosofsky, 1986). In the                            type. Unlike our previous simulations, the heterogeneity of
GCM, the probability of assigning a test item located at y to                      the chain did not distort any of the three GCM learner types
category A, given training items x = (x1 , . . . , xn ) with labels                to a large extent: the right hand side of Figure 6 is not too dis-
l = (l1 , . . . , ln ) is proportional to the summed similarities be-              similar to the left. Based on this, one might conclude that the
tween y and the category A exemplars:                                              heterogeneity of the population has done very little to distort
                                                                                   the categorization schemes produced by the various different
                                       ∑i|li =A S(xi , y)                          learners. Unfortunately, this conclusion is unwarranted.
       P(y ∈ A | x , l ) =                                             (6)
                             ∑i|li =A S(xi , y) + ∑i|li =B S(xi , y)
                                                                                   Category size bias
where similarity decays exponentially with distance, S(x, y) =
exp(−λ|x−y|). This model has one free parameter: the speci-                        Categorization is complex, and even this simple problem in-
ficity parameter λ that describes how rapidly similarity de-                       volves multiple biases. A preference for coherent categories
cays. When λ is large, similarity falls away very quickly with                     is one kind of bias that a learner might express, but one might
distance, and when λ is small it diminishes more slowly.                           be just as interested in exploring the extent to which learners
                                                                                   prefer categories to be of similar size. Does the GCM have a
Category coherence bias                                                            bias to split items evenly or unevenly? Does it depend on λ?
Although not framed as a Bayesian model, the GCM imposes                              To that end, we counted the number of exemplars assigned
biases on how learners categorize, and these biases depend                         to the smaller category in our previous simulations. Figure 7
on λ. For instance, the GCM prefers “coherent” categories                          plots this for the three homogeneous chains (left) and the sin-
that assign similar items to the same category. A simple mea-                      gle heterogeneous chain (right). The left panel shows that the
sure of “coherence” counts the number of times that adjacent                       GCM has a bias to prefer unevenly sized categories: this bias
items are assigned to the same category: the categories on                         is weak when the learner generalizes narrowly (λ = 10), and
the left of Figure 5 have maximal coherence of six, whereas                        strong when the learner generalizes widely (λ = 0.1). Unfor-
the incoherent categories on the right have coherence zero.                        tunately, almost none of this differentiation is evident when
To investigate GCM biases, we simulated the iterated learn-                        we look at the heterogeneous chains: the average response is
ing experiment described above 100,000 times using differ-                         substantially different from when the three learner types were
ent values of λ, assuming that all learners in a chain have the                    taken separately, and there are almost no individual differ-
same λ. The results (Figure 6, left) show that the GCM bias                        ences to be found, with all three learner types producing sim-
for coherent categories is strongest for large values of λ.                        ilar responses. With respect to the category size bias, mixing
   Given that individual differences in categorization exist,                      different learners into the iterated learning chain has almost
we ran a second simulation study (Figure 6, right). This time                      completely erased their differences.
                                                                             851

                                                                           Category Size                                                                               been used as a tool for exploring the inductive biases of in-
                                                                                                                                                                       dividuals. Based on formal results suggesting that the sta-
                                      Homogeneous Priors                                                                  Heterogeneous Priors
                                                                                                                                                                       tionary distribution of an iterated learning chain is the prior,
                           3.0                                                                                 3.0                                                     researchers in cognitive science have sometimes used these
                                 ●                                                                                   ●
                                                                     λ = 10                                                                                            designs as a form of elicitation task, in which the (between-
Size of Smaller Category                                                            Size of Smaller Category
                                                                     ● ● ●
                                                           ● ● ● ● ●
                           2.8                     ●
                                                       ● ●                                                     2.8                                                     subject) distribution of responses is taken to be reflective
                                               ●
                                           ●                                                                                                                           of some (within-subject) latent mental representation of the
                           2.6         ●                             λ=1                                       2.6                                                     world. In this context, our results suggest that some care is
                                                                                                                                                  λ = 10
                                           ● ●
                                               ● ● ● ● ● ● ● ● ● ●                                                             ● ● ● ● ● ● ● ● ● ● ● ●
                                     ●                                                                                   ●   ●
                                     ●
                                     ● ● ●                                                                               ● ● ● ● ● ● ● ●
                                                                                                                         ●
                                                                                                                           ●
                                                                                                                                         ● ● ● ● ●
                                                                                                                                                   ● ●
                                                                                                                                                  λ=1                  required. When people bring different priors to a task, there
                           2.4         ●
                                                                                                               2.4         ● ● ● ● ● ● ● ● ● ● ● ● ● ●
                                           ● ● ● ●
                                                   ● ● ● ● ● ● ● ● ●
                                                                     λ = 0.1
                                                                                                                                                  λ = 0.1              is no inherent reason to think that the stationary distribution
                                                                                                                                                                       of an iterated learning chain reveals those priors. The distor-
                           2.2                                                                                 2.2
                                                                                                                                                                       tions are both systematic and difficult to predict. The latter
                                 0                 5            10             15                                    0          5            10             15         point is especially troublesome from a methodological per-
                                                       Iterations                                                                   Iterations                         spective. In our third case study, it was not obvious to us that
Figure 7: Exploring the “category size” bias using iterated learn-                                                                                                     heterogeneity among category learners would produce a large
ing. The y axis plots the number of items assigned to the smaller                                                                                                      distortion of “category size” biases, but almost no distortion
category. Left panel: Homogenous iterated learning chains when                                                                                                         to the bias for “coherent” categories. In this context, we sug-
all learners use the same value of λ. The three plots in the figure
are quite dissimilar: when λ is small the GCM strongly prefers an                                                                                                      gest that the interpretation of iterated learning experiments is
unequal allocation of items to categories, but when λ is large the                                                                                                     difficult when individual differences exist.
preference is weak. The grey dashed line reflects the average of the
three chains on iteration 15. Right panel: When the same GCM                                                                                                                                      References
learners are mixed into a heterogenous iterated learning chain, most
of this variation is suppressed (the curves are close to each other),                                                                                                  Canini, K., Griffiths, T., Vanpaemel, W., & Kalish, M. (2014). Re-
and the average size of the smaller category (grey dashed line) has                                                                                                        vealing human inductive biases for category learning by simulat-
substantially decreased.                                                                                                                                                   ing cultural transmission. Psychonomic Bulletin & Review.
                                                                                                                                                                       Ellis, R. (2015). Understanding second language acquisition (2nd).
                                                                                                                                                                           Oxford University Press.
                                                              General discussion                                                                                       Ferdinand, V., Thompson, B., Kirby, S., & Smith, K. (2013). Regu-
                                                                                                                                                                           larization behavior in a non-linguistic domain. In Proceedings of
The three case studies all display the same pattern. When                                                                                                                  the 35th Annual Conference of the Cognitive Science Society.
all learners bring the same inductive bias to the problem, it-                                                                                                         Griffiths, T. & Kalish, M. (2007). Language evolution by iterated
erated learning behaves in the way that previous theoretical                                                                                                               learning with Bayesian agents. Cognitive Science, 31(3), 441–
                                                                                                                                                                           480.
proofs suggest it should (Griffiths & Kalish, 2007). In par-                                                                                                           Hudson Kam, C. & Newport, E. (2005). Regularizing unpredictable
ticular, when learners are Bayesians with identical priors and                                                                                                             variation: the roles of adult and child learners in language for-
correctly specified likelihoods, iterated learning reveals those                                                                                                           mation and change. Language Learning and Development, 1(2),
                                                                                                                                                                           151–195.
priors. For a non-Bayesian learner an analogous inductive                                                                                                              Janis, I. L. (1982). Groupthink: psychological studies of policy deci-
bias is uncovered. However, when learners bring different bi-                                                                                                              sions and fiascoes. Houghton Mifflin Boston.
ases to the problem there is no guarantee that the responses of                                                                                                        Kalish, M., Griffiths, T., & Lewandowsky, S. (2007). Iterated learn-
                                                                                                                                                                           ing: Intergenerational knowledge transmission reveals inductive
any one participant genuinely reflects their prior biases, nor is                                                                                                          biases. Psychonomic Bulletin & Review, 14(2), 288–294.
there any guarantee that the average responses reflect the av-                                                                                                         Kirby, S., Dowman, M., & Griffiths, T. (2007). Innateness and cul-
erage bias in the population. To the contrary, our case studies                                                                                                            ture in the evolution of language. Proceedings of the National
                                                                                                                                                                           Academy of Sciences, 104(12), 5241–5245.
suggest that those learners with the most extreme biases exert                                                                                                         Kirby, S., Griffiths, T., & Smith, K. (2014). Iterated learning and
a disproportionate influence on the chain. We briefly consider                                                                                                             the evolution of language. Current Opinion in Neurobiology,
the implications if this pattern holds more generally.                                                                                                                     28C(108-114).
                                                                                                                                                                       Lew, T. & Vul, E. (2015). Structured priors in visual working mem-
   Iterated learning leads a double life within the psychologi-                                                                                                            ory revealed through iterated learning. In Proceedings of the 37th
cal literature. As a theoretical tool, the underlying dynamics                                                                                                             Annual Conference of the Cognitive Science Society, Austin, TX.
of the chain provide valuable insights into how cultural and                                                                                                               Cognitive Science Society.
                                                                                                                                                                       Lewandowsky, S., Griffiths, T., & Kalish, M. (2009). The wisdom of
linguistic evolution works. From that perspective, our results                                                                                                             individuals: exploring people’s knowledge about everyday events
open up new questions: for instance, does language evolution                                                                                                               using iterated learning. Cognitive Science, 33, 969–998.
reflect the cognitive biases of all speakers, or do some sub-                                                                                                          Nosofsky, R. M. (1986). Attention, similarity, and the identification-
                                                                                                                                                                           categorization relationship. Journal of Experimental Psychology:
populations (e.g., children) exert stronger influences on the                                                                                                              General, 115(1), 39–57.
process? Similarly, learners with the most confidence in their                                                                                                         Perfors, A. & Navarro, D. J. (2014). Language evolution can be
own beliefs will exert a disproportionate influence on others,                                                                                                             shaped by the structure of the world. Cognitive Science, 38(4),
                                                                                                                                                                           775–793.
providing a justification for expressing overconfidence: if the                                                                                                        Rafferty, A., Griffiths, T., & Klein, D. (2014). Analyzing the rate at
goal is to have cultural influence rather than be correct, strong                                                                                                          which languages lose the influence of a common ancestor. Cog-
biases are better than weak ones. Regardless, the effect of het-                                                                                                           nitive Science, 38, 1406–1431.
                                                                                                                                                                       Reali, F. & Griffiths, T. (2009). The evolution of frequency distribu-
erogeneity in this context need not be a reason for concern so                                                                                                             tions: relating regularization to inductive biases through iterated
much as a reason to ask new questions.                                                                                                                                     learning. Cognition, 111, 317–328.
   On the methodological side, iterated learning has often
                                                                                                                                                                 852

