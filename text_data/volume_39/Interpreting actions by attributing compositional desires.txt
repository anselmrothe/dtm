                          Interpreting actions by attributing compositional desires
Joey Velez-Ginorio1 (joeyv@mit.edu), Max Siegel1 (maxs@mit.edu), Joshua B. Tenenbaum1 (jbt@mit.edu),
                                    & Julian Jara-Ettinger2 (julian.jara-ettinger@yale.edu)
                              1
                               Department of Brain and Cognitive Sciences, MIT. Cambridge, MA. 02139
                                 2
                                  Department of Psychology, Yale University. New Haven, CT . 06520
                                Abstract                                buy coffee and milk, but the order in which he buys them does
   We cannot see others‚Äô mental states, so we infer them by             not matter. Finally, before Bob has had breakfast, many states
   watching how people behave. Bayesian inference in a model of         of the world are rewarding (eating at the caf√© or having a
   rational action ‚Äì called inverse planning ‚Äì captures how             scone at home, for example), but once he eats something, all
   humans infer desires from observable actions. These models
   represent desires as simple associations between agents and          rewards associated with breakfast disappear.
   world states. In this paper we show that by representing desires        These examples reveal three key properties of desires.
   as probabilistic programs, an inverse planning model can infer       First, desires can often be fulfilled in more than one way. So
   complex desires underlying complex behaviors‚Äîdesires with
   temporal and logical structure, which can be fulfilled in            from an observer‚Äôs standpoint, goals cannot be equated with
   different ways. Our model, which combines basic desires via          desires. Second, desires can have logical and temporal
   logical primitives, is inspired by recent probabilistic grammar-     structure: they can be fulfilled in different ways (get tea or
   based models of concept learning. Through an experiment
   where we vary behaviors parametrically, we show that our             coffee), they can break into subgoals (get coffee and milk),
   model predicts with high accuracy how people infer complex           and they can have temporal structure (go to the caf√© and then
   desires. Our work sheds light on the representations underlying      buy a scone). Finally, the logical and temporal structure of
   mental states, and paves the way towards algorithms that can
   reason about others‚Äô minds as we do.                                 desires interacts with the underlying rewards. If Bob is
                                                                        thirsty, then both soda and water are rewarding. But once he‚Äôs
    Keywords: social cognition; theory of mind; computational
                                                                        had one of them, the other loses its immediate appeal. If Bob
    modeling; Bayesian inference.                                       wants to exercise and then bathe before work, he has to do
                                                                        them in that specific order; doing them in the wrong order
                           Introduction                                 does not suffice. In other cases, the order does not matter, but
                                                                        the reward is only achieved once all the necessary
                                                                        prerequisites are fulfilled. If Bob likes his coffee with milk,
   As social creatures, humans routinely have to make sense
                                                                        then having coffee and milk together is rewarding, but having
of what other people are doing, and we do so by appealing to
                                                                        only one of them is not.
mental states such as beliefs, desires, and intentions. Because
                                                                           Computational models of mental-state attribution that
we cannot see these internal mental states we need to infer
                                                                        successfully explain human mental-state inferences assume a
them by watching how people act.
                                                                        relatively simplistic representation of desires: each desire can
   Research into this capacity, called a Theory of Mind
                                                                        only be fulfilled in one way, and it is fulfilled by reaching one
(Gopnik & Meltzoff, 1997; Dennett, 1989), suggests that
                                                                        and only one physical state of the world (e.g. Baker et al.,
mental state inferences are driven by the assumption that
                                                                        2017; Baker et al., 2012). This assumption implicitly blurs
agents act efficiently, subject to constraints imposed by their
                                                                        together desires, intentions, goals, and physical states of the
environment (Gergely & Csibra, 2003). If, for instance, an
                                                                        world. As our examples show, this is overly limiting; people
agent takes a straight path towards a cookie jar, we can guess
                                                                        may require conjunctions (A and B) or disjunctions of goals
that her goal is to get a cookie, even before she has reached
                                                                        (A or B), with temporal properties (A then B).
it. By contrast, if she gets there after wandering around for a
                                                                           In this paper we develop a richer representation of desires,
while, we may infer that she found it without having
                                                                        and clarify the multiple computational levels that transform
deliberately searched.
                                                                        desires into actions. To solve the representational challenges,
   In such scenarios, it makes sense to equate goals with
                                                                        we draw on advances in concept learning that support
desires. But in more complex scenarios it is important to
                                                                        concepts of unbounded complexity (Piantadosi et al., 2012;
distinguish between the two: a one-to-one correspondence
                                                                        Goodman et al., 2008, 2014). To solve the inferential
between desires and goals is rare. Consider, for instance, if
                                                                        challenges that arise with more sophisticated representations,
Bob wants to have breakfast. He can do this in several
                                                                        we draw on advances in mental-state attribution beyond goal
different ways that each require a different plan: he can stay
                                                                        inference (Lucas et al., 2014; Jara-Ettinger et al., 2016, under
home and prepare breakfast; he could go to the local caf√© near
                                                                        review). In the remainder of the paper, we sketch out the
his house; or he could go to a coffee shop that is out of the
                                                                        computational framework and we present an experiment
way. If he chooses to eat at the local caf√©, he can show up and
                                                                        testing quantitative predictions of our model.
request food. By contrast, if he chooses to cook, he may have
to go to the grocery store first and then go to his kitchen, in
that order. While Bob is at the grocery store, he may need to
                                                                    1284

                                                                     (Fig 1b), her space of intentions is {get tea and then a scone;
                                                                     get milk, coffee, and then a scone; and get coffee, milk, and
                                                                     then a scone}.
                                                                        To model how the agent selects an intention and transforms
                                                                     it into an action plan, we rely on advances in commonsense
                                                                     psychology that suggest that we interpret other people‚Äôs
                                                                     behavior through the assumption that they act to maximize
                                                                     their subjective utilities ‚Äì the difference between the rewards
                                                                     they obtain and the costs they incur (Jara-Ettinger et al., 2016,
  Figure 1. (a) schematic of the generative model. (b)               under review; Lucas et al., 2014). This assumption operates
  example of how an expression combines primitives and               at two levels: given a space of intentions, the agent will
  objects to determine how to satisfy a desire. This                 choose the one that maximizes her subjective utilities, and
  expression corresponds to an agent who first wants either          given an intention, the agent will attempt to complete it as
  coffee and milk, or just tea, and then a scone afterwards.         efficiently as possible (for an agent to maximize utilities, they
  The tree below shows the space of possible intentions that         must also minimize costs).
  can fulfill the desire.
                   Computational model
We take as a starting point the idea that social cognition is
supported by a probabilistic generative model that determines
how mental states lead to actions (Baker, et al., 2017). We
expand on this approach by building a more powerful
representation of desires, and how they relate to behavior.
   Figure 1a shows the overall schematic of our model. We
argued that a realistic model of commonsense psychology
should distinguish between desires, goals, intentions, and
actions, and our model attempts to do so.
   At the top level we place desires, which combine logical
(and/or) and temporal (then) primitives with simple goals
(such as arriving to certain physical locations). This approach
enables us to represent desires that directly map onto a single
goal (e.g. ‚Äúgo to get coffee‚Äù) as well as desires that can be
fulfilled in different ways (e.g. ‚Äúeat breakfast first, and then
either get coffee and milk, or alternatively get tea‚Äù). This
representation is inspired and based on computational models
that combine logical primitives with unitary concepts to               Figure 2: Examples of the experimental stimuli. (a-b)
explain the productivity and compositionality of conceptual            examples of stimuli that consist of a single event. (c)
knowledge (Piantadosi et al., 2012; Goodman et al., 2008,              example of stimuli that consists of two events.
2014).
   Following Goodman et al. (2008), we model the space of               To compute each intention‚Äôs utility, we rely on planning
desires with a probabilistic grammar, which builds arbitrarily       algorithms developed in the robotics literature (Puterman,
complex desires by composing simple ones. The grammar                2014) that have been successfully applied to model mental-
implements production rules that recursively conjoin                 state attribution (Baker et al., 2009; 2017): Markov Decision
primitives and units to yield desire expressions. We endow           Processes (MDPs). Given a set of states, a set of actions, and
the grammar with several primitives ‚Äì And, Or, and Then ‚Äì            an underlying reward function, MDPs allow us to determine
but the framework is general. These primitives are motivated         the sequence of actions that an agent should take to fulfill her
by common-sense intuitions, but our primary goal is to               goal as efficiently as possible. By using MDPs, we can
develop a framework for compositional desires, not to                compute the expected cost of achieving each goal, and
identify the exact primitives that underlie goal-directed            define an intention‚Äôs utility as the reward gained by fulfilling
behavior.                                                            the desire minus the sum of the costs for achieving each goal
   To connect desires to actions, we rely on an intermediate         in the intention. Given each intention‚Äôs utility, we assume that
representation of intentions (see Jara-Ettinger et al., under        agents probabilistically select an intention:
review). Given a composite desire, our model derives the
space of intentions as the set of all ordered sequences of sub-                                              ùëà(ùêº)                   (1)
goals that satisfy it. For instance, if an agent desires to get                                 ùëù(ùêº) ‚àù exp‚Å°(      )
                                                                                                               ùúè
either coffee and milk, or just tea, and then a scone afterwards
                                                                 1285

    Figure 3: Detailed results from the experiment. Each plot represents one trial from the experiment. The x-axis shows the
    model‚Äôs top three hypotheses and the y-axis shows the z-scored prediction with participant judgments. Blue lines and dots
    show model predictions and red lines and dots show participant judgments. Vertical bars show 95% confidence intervals. In
    each plot, the schematic represents the paths the agent took in the event (see Figure 2 for examples of the actual stimuli).
where ùúè is a parameter that captures expectations about the        given some observed actions, we use Bayesian inference to
agent‚Äôs rationality. When ùúè is low, the agent invariably           invert the generative model. Given an observable set of
selects the intention with the highest utility; as ùúè increases,    actions A, the posterior belief for each underlying desire D is
the agent is more likely to choose a suboptimal intention.         given by:
  Finally, once the agent has selected an intention, we define
the action plan as the ordered sequence of goals along with                             ùëù(ùê∑|ùê¥) ‚àù ùëô(ùê¥|ùê∑)ùëù(ùê∑)                   (2)
the motor programs that complete each goal (computed
through MDPs).                                                     where the prior p(D) is set to favor simpler explanations using
                                                                   a simple penalization for the length of the expression (as in
Inference in the generative model                                  Goodman et al., 2008).
We have specified a generative model for compositional                To compute the likelihood, l(A|D), we integrate over the
desires, intentions, and action plans. To recover a desire         space of all possible intentions the agent could have:
                                                               1286

                ùëô(ùê¥|ùê∑) =        ‚àë       ùëù(ùê¥|ùêº)ùëù(ùêº|ùê∑)          (3)
                           ùêº‚ààùêºùëõùë°ùëíùëõùë°ùëñùëúùëõùë†
Both the probability of the intention given the desire (p(I|D)),
and the probability of the action, given the intention (p(A|I))
are computed through the assumption that agents act to
maximize their utilities‚Äîthe difference between the
subjective reward for fulfilling their desires minus the cost
for fulfilling it. This expectation implies that agents are more
likely to act efficiently given their intention, but that they are
also more likely to select the intention that can fulfill the            Figure 4. Comparison between our model (simplicity &
desires with the overall lowest cost. We enumerate a set of              efficiency) and the alternative model (prior only). Each dot
desires using breadth-first-search over the grammar, and then            represents a judgment of a hypothesis for a given trial. The
approximate the posterior over that space using Bayesian                 x-axis shows the model‚Äôs prediction and the y-axis shows
inference.                                                               participant judgments.
Simplicity prior alternative model                                     locations, omitting paths between 3 locations to prevent the
                                                                       stimuli set from growing too large. In contrast to the single
To better understand our model, we developed a simple
                                                                       event set, we keep the equivalent paths, as they become
alternative that uses a deterministic likelihood function,
                                                                       necessary to construct the most primitive desires occurring
where the probability of a desire generating an action
                                                                       over two events e.g. (A or B). This creates a base set of 9
(p(A|D)) is 1 if the action satisfies the desire and 0 otherwise.
                                                                       paths. To generate the trials with two events, we first split the
This model continues to have much of the power of the full
                                                                       9 paths into two classes, one for paths that go to only one
model: it has access to rich representations of desires and the
                                                                       location (3 paths) and another for paths that go to two
prior over hypotheses creates a preference for simpler
                                                                       locations (6 paths). For each class we compute the cartesian
explanations. Unlike the main model, this model is
                                                                       product of itself, and after removing duplicate pairs of stimuli
insensitive to the intermediate representations of intentions,
                                                                       in each class, (e.g. A,B = B,A), this provided a set of 27 two
as it does not account for how the agent chooses the intention
                                                                       event trials. From that set, events that violated the principle
that will fulfill their desires.
                                                                       of rational action were removed (10 trials). Additionally, if a
                                                                       trial with repeated events was the reflection or rotation of
                          Experiment                                   another trial with two events, it was removed (5 trials); e.g.
                                                                       between (A,A) and (C,C), we kept (A,A). Last, trials with two
Design                                                                 events were removed if only one possible hypothesis could
To evaluate our model, we designed a simple task where                 explain the trials (2 trials), these trials trials impact our ability
participants watched an agent‚Äôs behavior across one or two             to get graded responses on alternative plausible hypotheses
days and were asked to determine their belief that the agent           (an ideal trial would have more than one plausible
had certain desires (see Figure 2).                                    explanation, to determine if the model captures the same
                                                                       graded measure humans have for alternatives). For example,
Methods                                                                if the agent only goes to the farthest location on event 1 and
Participants 33 participants, mean age (SD) = 32.13 years              2, it's clear the only compatible hypothesis is that the agent
(9.38 years), range = 20-61 years from the US (as determined           wants to go that location. As an exception, we included one
by their IP address) were recruited using Amazon‚Äôs                     of these cases in the final set, just to show that the model was
Mechanical Turk Framework.                                             capable of inferring the only plausible hypothesis. After
                                                                       filtering the original 27 two event stimuli, 11 remained. These
Stimuli                                                                11 plus the 8 one event trials result in the 19 stimuli used in
Figure 2 shows an example of the stimuli. Stimuli consisted            the experiment.
of 19 two-dimensional images of an agent traveling to one or
more of three potential static locations. Eight of these trials        Procedure
consisted of a single event and the remaining 11 consisted of          Participants first read a tutorial that explained the logic of the
two events. The one event trials were built by designing all           task. Participants then completed a short survey that ensured
possible efficient paths agents could take to reach between 1          they had read the instructions, and the test phase followed
and 3 of the locations and removing equivalent paths (i.e.             immediately after.
identical under a rotation or reflection of the map).                     During the test phase, participants completed 19 trials. In
   Trials with two events were built by first creating a set           each trial participants saw the stimuli on the left side, and
including possible efficient paths between 1 and 2 of the              they were asked to rate their belief that the agent had each of
                                                                       three different desires. Each desire was rated on a scale from
                                                                       0-10 for each, with 0 indicating ‚ÄúDefinitely not‚Äù; 5 ‚ÄúMaybe‚Äù;
                                                                   1287

and 10 ‚ÄúDefinitely.‚Äù The three desires were obtained by               right location), and then travels diagonally to the bottom right
selecting the three hypotheses with the highest posterior             location. Our full model gives a high probability to the desire
distribution according to the model. In order to present these        that the agent wanted to visit those two locations in that
hypotheses to participants, we translated the description from        specific order (A then C), an average probability to the desire
the model into descriptions in English. To ensure their               that she could have wanted to visit the locations in any order
accuracy, two coders blind to the original hypotheses back-           (A and C), and a low probability to the desire that the agent
translated the descriptions into the model‚Äôs original                 wanted to visit either A or B first, then C ((A or B) then C).
representations. The two coders showed 100% agreement and             Although all hypotheses explain the actions, our model is
recovered the correct model hypothesis in all trials.                 sensitive to the probability that each desire would generate
                                                                      the observed actions relative to competing ways to fulfill the
                                                                      same desire (driving the difference between the first and
                                                                      second hypotheses) and to the baseline complexity of the
                                                                      desires (driving the difference between the second and third
                                                                      hypotheses). That is, our model recognizes that there are two
                                                                      equally good intentions that fulfill the desire ‚ÄúA and C‚Äù (A
                                                                      and then C, or C and then A), but only one that fulfills the
                                                                      ordered desire ‚ÄúA then C‚Äù (A and then C). This makes our
                                                                      model favor the ordered explanation, as participants do (see
                                                                      Figure 5). This is not captured in the prior only model, as it
                                                                      is only sensitive to expression complexity. These results
                                                                      show how people are both sensitive to the likelihood that a
                                                                      desire would generate the observed actions, and to the
                                                                      complexity of the ascribed desire. Figure 6 shows how this
                                                                      failure becomes even stronger in the case where participants
                                                                      watch the agent behave identically across two events.
                                                                                               Discussion
                                                                      Here we presented a formal model of action understanding
                                                                      that represents desires as composite entities sampled from a
                                                                      probabilistic context free grammar. Desires get transformed
                                                                      into intentions and then into action plans by the assumption
                                                                      that agents act to maximize their utilities. By performing
  Figure 5. Detailed results one of the trials. The top left plot     Bayesian inference over this generative model, we showed
  shows the schematic of the stimuli we used. The top right           how we can capture desires that have rich logical and
  plot shows participant judgments (z-scored); the bottom             temporal structure, as well as enabling us to represent desires
  two plots show the predictions of the full model and the            that can be fulfilled in more than one way. We tested our
  alternative model (z-scored). This example illustrates how,         model by comparing its inferences with those made by human
  by removing the probabilistic nature of the likelihood              participants, finding that it closely mirrors their judgments,
  function, the model loses sensitivity to variability in             and that an alternative model is less successful.
  participant judgments.                                                 Our model shows that combinations of primitives and
                                                                      objects using a probabilistic context free grammar supports
Results                                                               rich representations of desires in Theory of Mind. The
                                                                      primitives, composing over objects, generate structured
Figure 3 shows the results from the experiment.
                                                                      desires that capture temporal and logical structure.
Qualitatively, our model fit participant judgments well. Our
                                                                         Our goal was to develop a more nuanced representation of
model predictions showed a correlation of r=0.92 with
participant judgments (95% CI: 0.86-0.95). See Figure 4. By           desires, and the framework we propose works for any
contrast, the alternative model (prior only) showed a weaker          arbitrary set of primitives and objects. To test our model, we
correlation (r=0.80; 95% CI: 0.69 -0.88). A bootstrap over the        focused on three specific primitives: And, Or, and Then. Our
                                                                      results do not imply that these are the only primitives people
correlation difference showed that the full model performed
                                                                      use when they reason about others‚Äô desires, or even that they
reliably better than the alternative model (correlation
                                                                      are central in action-understanding. Other primitives such as
difference = 0.11; 95% CI: 0.009-0.18).
   Figure 5 shows the detailed results of a single trial that         If, Any, and Not, are likely also at play when we reason about
illustrates how the alternative model with a deterministic            other people‚Äôs behavior. More research is needed to
likelihood function fails to capture participant judgments. In        characterize the primitives we use in action-understanding,
                                                                      and their developmental origins.
this trial the agent begins by going to the top left location
                                                                         To characterize desire complexity, we used a simple prior
(which is one of the closest ones, together with the bottom
                                                                  1288

that penalized the length of the expression (based on               light on how we learn about the world by watching more
Goodman et al., 2008). Although this is a useful                    competent agents (see also Jara-Ettinger, Baker &
approximation, different primitives may have different priors       Tenenbaum, 2012).
which capture both their conceptual complexity and the
extent to which they are useful in explaining behavior. Future                          Acknowledgments
work may attempt to uncover primitive-specific priors and           This work was supported by the Simons Center for the Social
the forces that shape these priors.                                 Brain. This material is based upon work supported by the
                                                                    Center for Brains, Minds, and Machines (CBMM), funded by
                                                                    NSF-STC award CCF-1231216.
                                                                                             References
                                                                    Baker, C. L., Saxe, R., & Tenenbaum, J. B. (2009). Action
                                                                       understanding as inverse planning. Cognition, 113(3).
                                                                    Baker, C. L., Jara-Ettinger J., Saxe, R., & Tenenbaum, J. B.
                                                                       (2017). Rational quantitative attribution of beliefs,
                                                                       desires, and percepts in human mentalizing. Nature
                                                                       Human Behavior
                                                                    Dennett, D. C. (1989). The intentional stance. MIT press.
                                                                    Gergely, G., & Csibra, G. (2003). Teleological reasoning in
                                                                       infancy: The naƒ±ve theory of rational action. Trends in
                                                                       cognitive sciences, 7(7), 287-292.
                                                                    Goodman, N. D., Tenenbaum, J. B., Feldman, J., & Griffiths,
                                                                       T. L. (2008). A Rational Analysis of Rule‚ÄêBased Concept
                                                                       Learning. Cognitive Science, 32(1), 108-154.
                                                                    Goodman, N. D., Tenenbaum, J. B., & Gerstenberg, T.
                                                                       (2014). Concepts in a probabilistic language of thought.
                                                                       Center for Brains, Minds and Machines (CBMM).
 Figure 6. Results from the trial where participants watch two      Gopnik, A., Meltzoff, A. N., & Bryant, P. (1997). Words,
 repeated events. While the prior only model continues to              thoughts, and theories (Vol. 1). Cambridge, MA: Mit Press.
 make the same predictions, both participants and our model         Jara-Ettinger, J., Schulz, L. E., & Tenenbaum, J. B. (under
 have a stronger belief that the order mattered, in comparison         review). The na√Øve utility calculus as a rational,
 to the trial with a single event (Figure 5)                           quantitative foundation of action understanding.
                                                                    Jara-Ettinger, J., Baker, C. L., & Tenenbaum, J. B. (2012).
   In our current work, we focused specifically on desires and         Learning What is Where from Social Observations.
we assumed that the agents had full knowledge about the                In CogSci.
environment. In more realistic cases, agents can be uncertain,      Jara-Ettinger, J., Gweon, H., Schulz, L. E., & Tenenbaum, J.
ignorant, or wrong about the world, and people‚Äôs reasoning             B. (2016). The na√Øve utility calculus: computational
about others is sensitive to this fact (Baker et al., 2017;            principles underlying commonsense psychology. Trends in
Kov√°cs, T√©gl√°s, & Endress, 2010). Our grammatical                      cognitive sciences, 20(8), 589-604.
approach to desires may also support more structured                Kov√°cs, √Å. M., T√©gl√°s, E., & Endress, A. D. (2010). The
representations about beliefs. Intuitively, people‚Äôs beliefs are       social sense: Susceptibility to others‚Äô beliefs in human
often structured logically (e.g. my laptop is in my backpack           infants and adults. Science, 330(6012), 1830-1834.
or at home; she thinks he is hungry and tired). In future work      Lucas, C. G., Griffiths, T. L., Xu, F., Fawcett, C., Gopnik, A.,
we will investigate the power and limitations of applying this         Kushnir, T., ... & Hu, J. (2014). The child as
approach to the representations of beliefs, and to the                 econometrician: A rational model of preference
interaction of beliefs and desires.                                    understanding in children. PloS one, 9(3), e92160.
   Although in our work we focused on these representations         Piantadosi, S. T., Tenenbaum, J. B., & Goodman, N. D.
as applying to desires, these desires often inherit their              (2012). Bootstrapping in a language of thought: A formal
structure from how the world works. If Bob wants to shoot a            model of numerical concept learning. Cognition, 123(2).
water gun, he needs to pour water into the tank first, then         Puterman, M. L. (2014). Markov decision processes: discrete
pump air into valve, and then press the trigger, in that order.        stochastic dynamic programming. John Wiley & Sons.
The fact that Bob‚Äôs desire takes this structure is a reflection     Tenenbaum, J. B., Kemp, C., Griffiths, T. L., & Goodman,
of how water guns work. This opens the possibility that,               N. D. (2011). How to grow a mind: Statistics, structure, and
through the ability to reason about other people‚Äôs desires, we         abstraction. Science, 331(6022), 1279-1285.
may simultaneously learn procedural knowledge about how
to make changes to the world. As such, our model may shed
                                                                1289

