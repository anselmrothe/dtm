                Quantifying the impact of active choice in word learning
            Shohei Hidaka (shhidaka@jaist.ac.jp) and Takuma Torii (tak.torii@jaist.ac.jp)
                                      Japan Advanced Institute of Science and Technology
                                               1-1 Asahidai, Nomi, Ishikawa, Japan
                                 George Kachergis (george.kachergis@gmail.com)
                           Dept. of Artificial Intelligence / Donders Institute, Radboud University
                                                    Nijmegen, the Netherlands
                            Abstract                               word production starts when the child is 12 months old
   Past theoretical studies on word learning have oﬀered           on average, and by 18 months children can produce 50
   simple sampling models as a means of explaining real            words and comprehend 100-150 (Hulit & Howard, 2002).
   word learning, with a particular goal of addressing the         By 18 years of age, it is estimated we know over 60,000
   speed of word learning: people learn tens of thousands
   of words within their first 18 years. The present study         words (Bloom, 2000). Under the assumption that each
   revisits past theoretical claims by considering a more re-      child has 8 hours of word learning opportunity everyday,
   alistic word frequency distribution in which a large num-       these estimates mean the child learns a new word every
   ber of words are sampled with extremely small probabil-
   ities (e.g., according to Zipf’s law). Our new mathemati-       learning hour for 18 years of the life.
   cal analysis of a recently-proposed simple learning model          Given these empirical estimates of word learning, the-
   suggests that the model is unable to account for word           oretical studies have attempted to account for the quan-
   learning in feasible time when the distribution of word
   frequency is Zipfian (i.e., power-law distributed). To          titative characteristics of word learning. The first ques-
   ameliorate the diﬃculty of learning real-world word fre-        tion is: What combination of learning mechanisms and
   quency distributions, we consider a type of active, self-       structure in the language environment allows children to
   directed learning in which the learner can influence the
   construction of contexts from which they learn words.           learn at this rate? This question poses a good neces-
   We show that active learners who choose optimal learn-          sary condition for any account of child word learning,
   ing situations can learn words hundreds of times faster         as it needs to address this quantitative aspect of word
   than passive learners faced with randomly-sampled situ-
   ations. Thus, in agreement with past empirical studies,         learning.
   we find theoretical support for the idea that statistical          As a first-order approximation, child learning may be
   structure in real-world situations–potentially structured
   for learning by both a self-directed learner, and by a          modeled as an independent sampling process in which
   beneficent teacher–is a potential remedy for the patho-         each word is learned independently. To estimate the
   logical case of learning words with Zipf-distributed fre-       fastest possible learning rate, (Blythe, Smith, & Smith,
   quency.
                                                                   2010, 2016) proposed an idealized learning model to ad-
   Keywords: cognitive models of language acquisition;
   cross-situational word learning; statistical learning           dress acquiring a full lexicon in the long term: 60,000
                                                                   words over 18 years. In their model, each word is
                   Child word learning                             learned with its first sample – known as fast mapping
One of the most prominent diﬀerences between human                 in the developmental literature. Under the simplifying
and nonhuman cognition is our language ability. Much               assumption that each word is independently learned via
research has been dedicated to understanding the human             fast-mapping, and its word frequency is distributed uni-
capability for language, with a great deal of discussion           formly, their mathematical analysis of the model showed
focused on the process of language acquisition. A central          that a cross-situational learner is suﬃciently fast to learn
debate in this conversation considers whether acquisi-             all 60,000 words after experiencing a reasonably small
tion is based on innate and language-specific mechanisms           number of spoken words.
(Chomsky, 1965; Gleitman, 1990), or bootstrapped from
domain-general mechanisms (Smith, 2000; Kachergis,                                Theoretical approach
2012). From the former perspective, humans become                  Blythe et al.’s theoretical estimate has been treated as
competent language users–mastering a complex system                a theoretical implication that shows learning via inde-
of syntax to produce endless semantics–very rapidly, and           pendent fast-mapping of words is eﬃcient enough to be
with relatively little training.                                   a model of child word learning. In this study, we rein-
   Word learning has been treated as an indicator of               spect this theoretical implication by introducing a more
language development, and has been compared with a                 realistic word frequency distribution. Our mathematical
number of other indicators of cognitive abilities, such as         analysis implies that the learning rate of the independent
memory (Vlach & Johnson, 2013; Vlach & Sandhofer,                  fast mapping is quite sensitive to the word frequency
2012). Although there are multiple empirical estimates             distribution. More importantly, even fast mapping–the
of the number of words that children acquire, many stud-           most eﬃcient learning, requiring only a single sample,
ies agree that child’s word learning is quite fast. Early          can be too slow to learn 60,000 words in 18 years, if
                                                               519

word frequency follows Zipf’s law (Zipf, 1949) or a fat-         dren as young as three years old can quickly generalize
tailed distribution which is often found in natural cor-         a novel name to objects when they hear the novel name
pora. Thus, our analysis implies that the independent            given to its fast instance. Due to this one-shot nature
fast-mapping model cannot be an account for child word           of their word learning, it is called fast mapping. Cap-
learning, if there are many words sampled less frequently.       turing this empirical finding, the fast-mapping learner in
This mathematical implication leads to an empirical test         the model is supposed to learn a new pair of word and
of whether the word distribution in the child-directed           object only with the first experience of it. The fast-
speech is uniform or non-uniform such as a Zipf dis-             mapping learner is equivalent to the cross-situational
tribution. Thus, in the second study, we analyzed the            learner, if there is one correct pair of object and word in
CHILDES corpora for word distribution in child-directed          each episode (M = 1).
speech.                                                             As fast-mapping learning is the most eﬃcient scheme
   Given this result of the mathematical analysis, we ex-        (at least for independent word learning), it gives a good
plore an extension of the word learning mechanism by             baseline estimate of the number of samples to learn all
additionally assuming that the word learning is more ac-         the words in a given list. Blythe et al. (2010) model
tive than that is supposed to be traditionally. Typically,       a fast-mapping learner acquiring words independently
as analyzed in the past studies above (Blythe et al., 2010,      drawn from a uniform distribution of W words given in
2016), the learning is supposed passive – the learner has        each episode. As every episode has one word with prob-
no choice but observing samples words and objects from           ability 1/W , this is equivalent to the so-called Coupon
a given probability distribution. This is certainly over-        Collector’s Problem (Blom, Holst, & Sandell, 1994). In
simplified, as actual child word-learners choose when,           this problem, the expected time T to finish sampling all
where and from whom they would like to learn words.              the words is
Thus, our second analysis estimates the impact of a form                                            ∑W
                                                                                           E[T ] =      E[ti ].
of active choice of situations in word learning. Our anal-
                                                                                                    i=1
ysis shows that active learning is likely to have a suf-
ficiently beneficial impact to make word learning fast           where ti is the time to sample a ith new word given (i−1)
enough to happen on a realistic timescale.                       words being learned. Thus,
                                                                                               ∑ W
     Independent fast-mapping learning                                          E[T ] = W           1/i ≈ W log W.        (1)
Uniformly distributed word frequency                                                            i=1
Blythe, Smith, & Smith (2010) proposed a mathematical            Setting the number of words W = 60, 000, which is an
model of word learning, which has a closed-form expres-          empirical estimate of the number of words 18 years old
sion under a certain simplification. In their recent study,      knows on average, T = 660, 126. This estimate is compa-
Blythe, Smith, & Smith (2016) analyzed essentially the           rable with the “reasonable” number of samples justified
same model, although slightly modified for analytic con-         by Blythe et al. (2010) which individual children can be
venience. Here we briefly introduce the most recent form         exposed to for their 18 years of lives.
(2016) of their model.                                           Non-uniformly distributed word frequency
   Blythe et al. originally consider cross-situational word      Here we extend this analysis on the fast-mapping
learning. Suppose there are W words and O objects in             learner to the case with word frequency distributed non-
the hypothetical world. Further the numbers of words             uniformly. Our extended analysis will reveal that the
and objects are equal, W = O, in their cross-situational         estimate based on Equation (1) by Blythe et al. is quite
learning scheme, and every object has its name and no            “optimistic”, as an estimate with non-uniform word dis-
objects have two names. Namely, there are W correct              tribution is larger than that in general.
pairs of words and objects. Without loss of generality,             Here let us derive the number of episodes T that,
denote the W pairs by 1, 2, . . . , W , and suppose k th ob-     for 0 ≤ ϵ ≤ 1, the (1 − ϵ) of children learned all
ject is paired with the k th word.                               the W words listed. Suppose a set of W words in
   Given these pairs being unknown, a word learner is to         which each word 1, . . . , W is drawn from the distribu-
infer correct pairs by going through episodes. In each           tion p = (p1 , . . . , pW ). The proportion of children who
episode, the learner is exposed to M ≤ W words and               finished learning all the words is (1 − ϵ) for 0 < ϵ < 1
M objects, without any explicit information on which             requires the number of episodes T , which is the root of
word is paired with which object. With one episode with
M ≥ 2 objects and words, the learner cannot tell which                            ∏W
                                                                                        (               )
of M words should be associated to which of M objects.                                    1 − (1 − pi )T = 1 − ϵ.         (2)
                                                                                  i=1
   The most simple model among a series of extended
ones is called fast-mapping learning model. In the liter-        The left hand side of (2) is the probability that every
ature of language development, it is well-known that chil-       word is present at least once in the T episodes.
                                                             520

   Write                                                            Sensitivity to non-uniformity of word
                                 (                )                 frequency distribution
                             log 1 − (1 − ϵ)  1/W
                 fW,ϵ (x) :=                        .               To analyze the sensitivity to the non-uniformity, here
                                   log (1 − x)
                                                                    we analyze the Zipf distribution with diﬀerent ex-
For the uniform distribution, pi = 1/W for every i =                ponent parameters.          Denote the Zipf distribution
1, . . . , W , the root of (2) is given by                          with the exponent parameter a ≥ 0 by p =
                                                                    (1−a /HW,a , 2−a /HW,a , . . . , W −a /HW,a ) where HW,a is
                                                                                                                    ∑W
                          T = fW,ϵ (1/W ).                  (3)     the generalized harmonic number HW,a = i=1 i−a . It
                                                                    is reduced to the uniform distribution by a = 0. The
This T is the number of episodes with which the propor-             larger the exponent a is, the minimal probability min p
tion of children finished learning all the words is (1 − ϵ).        is smaller. Thus, here we analyze the upper bound T+
Setting ϵ = 1/2 in (3), we obtain the median of T ,                 as a function of the exponent parameter a.
fW,1/2 (1/W ), that is comparable with the mean of T                   Write T+ = fN,ϵ (min p), which gives a reasonable es-
in (1).                                                             timate of the upper bound of the root T of (2). As a
   Unlike (3) for the uniform distribution, the root T of           function of the exponent a, we have
Equation (2) in general is not closed-form. Thus, let us                                        (                       )
                                                                                                   ∂HN,a
consider the upper and lower bound for the root instead                      ∂ log T+     pmin       ∂a /HN,a + log W
of the rigorous form of it. For the general word distri-                               =
                                                                                 ∂a         (1 − pmin ) log (1 − pmin )
bution p = (p1 , . . . , pW ), the intermediate value theo-
rem states that there exists a unique constant c holding            and further we have
min p ≤ c ≤ max p, with which the root of (2) is ex-
pressed as                                                                                 ∂ 2 log T+
                                                                                                        ≥ 0.
                            T = fW,ϵ (c).                                                      ∂a2
Equivalently, we have inequality                                    This implies the T+ is a super-exponential monotone
                                                                    function of the exponent a. It is also numerically con-
                 fW,ϵ (max p) ≤ T ≤ fW,ϵ (min p).                   firmed in Figure 1, in which the numbers of episodes
                                                                    are shown as functions of the exponent for W =
As we are interested in the worst possible estimate of              10000, 60000. In this plot, a = 0 shows an estimate for
T , this inequality states that the upper bound T+ :=               the uniform distribution, and a = 1 shows that of the
fW,ϵ (min p) of T is characterized with the probability to          standard Zipfian distribution. It is striking that even
sample the least frequent word min p.                               the fastest learning such as fast mapping can be quite
   This extended mathematical analysis implies that the             slow (exponentially as a function of a) for with distribu-
uniform distribution q = (1/W, . . . , 1/W ) of words gives         tions with some item with a very small probability.
the minimal possible upper bound T+ among any fre-
quency distribution of W words, as any distribution                 Empirical dataset
min p of W words holds min p ≤ min q. Therefore, the                Given theoretical implication in the previous study, let
expectation of T in the form of (1) with the uniform                us analyze an empirical word distribution, which chil-
distributed words is the most optimistic, which may un-             dren typically are exposed to. It is diﬃcult to exactly
derestimate the number of episodes required for learning            count “episodes” or “pairs of word and object” in a
with a realistic word distribution.                                 real dataset, due to its ambiguity of definition and it
   For example, let us consider an alternative case                 is also up to children’s subjective perspective. Here,
that the W word follows the Zipf distribution p =                   as a proxy of them, we counted the word frequency
(1−1 /HW , 2−1 /HW , . . . , W −1 /HW ), where HW is the            based on child-directed speech in the CHILDES corpus
                                  ∑W −1
harmonic number HW =                 i=1 i  . In this case, the     (MacWhinney & Snow, 1990). Figure 2 shows a repre-
minimal probability is min p ≈ 1.44 × 10−6 , and the up-            sentative word distribution of 51,446 words aggregated
per bound T+ is 1.08 × 107 for ϵ = 0.01. This estimate              over 4,163 transcripts of all the corpora in CHILDES re-
means that learning of Zipf-distributed words requires              trieved in December 2007. The minimal word probabil-
16.4 times as many samples as learning of uniformly-                ity was 1.089 × 10−7 , which gives the upper bound T+ =
distributed words. That means that 206 independent                  f51446,0.01 (1.089 × 10−7 ) = 1.420 × 108 or the median
episodes exposed to a word learner every hour (or three             estimate T+ = f51446,0.5 (1.089 × 10−7 ) = 1.030 × 108 .
episodes every minute), assuming 8 hours of learning ev-            These estimates of required samples, an order of mag-
eryday of 18 years of life. This estimate cannot possibly           nitude larger than the optimistic theoretical estimate,
be considered “reasonable” with respect to ordinary life            suggest that it is diﬃcult to learn these empirical words
of children in any culture.                                         with this Zipfian-like frequency distribution.
                                                                521

                                  109                                                                                  102
                                            N = 10000
                                            N = 60000
                                                                                                                       101
                                  108
                                                                                                                       100           log P = -1.7548 log( rank ) + 2.6105
                                                                                                                        -1
                                                                                                                      10
     Required number of samples
                                  107                                                                                 10-2
                                                                                                                      10-3
                                  106
                                                                                                                      10-4
                                                                                                                      10-5
                                  105                                                                                 10-6
                                                                                                                      10-7
                                  104
                                        0               0.5                                       1   1.5
                                                                                                                      10-8
                                                              Exponent of the Zipf distribution                          100        101       102        103       104      105
Figure 1: For ϵ = 0.01, 0.5, 0.99 (broken and solid lines),                                                       Figure 2: Word frequency in a corpus aggregated from
N = 10000, 60000 and the exponent a = 0, 0.25, . . . , 1.5,                                                       the CHILDES transcripts.
the required number of samples
                         ∑N       M for a generalized Zipf
distribution pk = k −a / k=1 k −a is numerically calcu-
lated by the root of Equation (2).                                                                                ysis in the previous section, the minimal probability of
                                                                                                                  objects decides the number of samples required to com-
                                                                                                                  plete the word learning, the best choice for the active
                                            Active choice of situations                                           learner is given by the choice probability
Formulation                                                                                                                           q̂ = arg maxq min(P q).
The implication of the mathematical analysis above,
which suggested that even fast-mapping may not be eﬃ-                                                             This minimal probability, min(P q̂), gives the theoret-
cient enough for non-uniformly distributed words, raises                                                          ical upper bound for the minimal number of samples
a controversy between past theoretical analyses and em-                                                           fW,ϵ (min(P q̂)), as P is not known before empirical learn-
pirical findings of quantitative aspects of word learning.                                                        ing, and the active leaner also needs to estimate P from
   Here, we explore a possibility to reconcile the discrep-                                                       the sample. For a given matrix P , the optimal q̂ can be
ancy between theory and empirical findings, by consider-                                                          computed by the iterated linear programming algorithm
ing a further relaxation of past theoretical assumptions                                                          (See also Appendix for the detail).
about children’s word learning. In the conventional the-                                                             As a baseline for the passive learner, we consider the
oretical framework, the learner is assumed to be pas-                                                             average min(P q) with the uniform distribution over the
sive, having no choice but to observe and learn from a                                                            vector q, whose lower bound is given by the Jensen’s
given context: a randomly-sampled set of of objects, of                                                           inequality
which a (random) subset are labeled with words. This                                                                     ∫
assumption of a passive learner simplifies the theory,                                                                          min(P q)(N − 1)!dq ≥ min(P 1N /N ),
                                                                                                                             q∈SN
but surely underestimates real learners, who have some
choice about which contexts they experience. Here, we                                                             where the integral is taken over the N − 1 dimensional
consider a type of active learner who is able to choose                                                           unit simplex q ∈ SN . For a suﬃciently small x ≪ 1
from which situation/context he or she learns words.                                                              and y ≪ 1, fW,ϵ (x)/fW,ϵ (y) ≈ y/x. Thus, the rate
   Suppose that there are N word-object pairs and M sit-                                                          R = min(P q̂)/ min(P 1N /N ) gives a good estimate for
uations, and that the conditional probability to observe                                                          the rate of eﬃciency R, by which the active learning
the ith word-object pair is pij given the j th situation.                                                         with the optimal probability q̂ R times faster than the
Thus, the active learner has a choice of the situation out                                                        passive learning with a fixed probability q.
of the given M situations from which he or she learns
the word-object pairs. Suppose that the active learner                                                            Empirical evaluation
chooses the j th situation by the probability qj . Let us                                                         To evaluate the potential impact of the active leaning,
denote the N × M matrix of the conditional probability                                                            we study the SUN database (Xiao, Hays, Ehinger, Oliva,
by P = {pij }ij and the N × 1 vector of the choice prob-                                                          & Torralba, 2010) as an empirical object distributions in
ability by q = (q1 , q2 , . . . , qM )T . With this notation, the                                                 an collection of real-life scenes. The SUN database (re-
marginal probability of word-object pairs is given by the                                                         trieved on September 25th in 2016) has N = 3, 458 ob-
vector P q ∈ RN . According to our mathematical anal-                                                             jects and M = 1, 111 scenes in it. This data is supposed
                                                                                                            522

to give the N × M matrix P in which each column is the                                                 choice probability q1 = 1N /N . For k th batch of 1000
conditional probability of the objects given each scene.                                               steps, the online learner samples the objects according
If the scene choice probability is the uniform distribu-                                               to the probability P qk , and constructs the sample proba-
tion q = 1N /N , the min(P q) was 8.30 × 10−9 . Mean-                                                  bility matrix P̂k according to the sample frequency. Af-
while, with the optimal q̂, the min(P q̂) was 1.95 × 10−6 ,                                            ter the k th sampling step, the online learner estimates
which implies the active learning was approximately                                                    qk := arg maxq min(P̂k q). In each run of this procedure,
min(P 1N /N )/ min(P q̂) = 235.3 times faster than the                                                 we repeat up to 100×1000 samples, and obtain one sam-
baseline passive learning. The marginal probability dis-                                               ple for the number of required samples to finish learning
tributions of objects for the baseline and optimal q are                                               all the 1000 objects. With 100 runs, we obtain the Monte
shown in Figure 3. The diﬀerence between the two                                                       Carlo estimate of the online learner shown in Figure 4.
marginal distributions is visible at their tails – the tail                                            Figure 4 shows the sample probability distribution of the
for the uniform q decreases like an exponential function,                                              number of required samples in the Monte Carlo simula-
but that for the optimal q̂ decreases as a power function                                              tion (circles: histogram, line: smoothed estimate), and
(linear in the double log plot). This empirical evaluation                                             its comparable median estimate for the optimal learner
suggests that the active learning of interest can boost the                                            (green vertical line) and the passive learner with the uni-
fast mapping a few orders more eﬃciently.                                                              form q (red vertical line). This simulation result shows
                                                                                                       that the online learner is as fast as the optimal learner,
                                                                                                       and is likely to be faster than the passive learner.
                                     100
                                                                    Uniform q (baseline)
 The maginal probability of object
                                                                    Optimal q
                                                                                                                                                                0.25
                                                                                                                                                                                                                  Passive learning
                                     10-2
                                                                                                          Probability (Samples in the Monte Carlo Simulation)
                                                                                                                                                                                                                  Optimal active learning
                                                                                                                                                                                                                  Online active learning
                                                                                                                                                                0.2
                                     10-4
                                                                                                                                                                0.15
                                     10-6                                                                                                                       0.1
                                     10-8                                                                                                                       0.05
                                        100   101          102            103              104
                                                    Rank of the objects
                                                                                                                                                                  0
                                                                                                                                                                      0   1      2             3           4              5          6
                                                                                                                                                                              Number of required samples to finish learning              104
Figure 3: The marginal probability of objects for the
optimal q̂ (line) and its baseline (dots).
                                                                                                       Figure 4: The probability distribution of the number
                                                                                                       of required samples to finish learning for passive (red),
Online active learning                                                                                 optimal (green), and online active learner (blue).
The quantification of the eﬃciency of active learning is
based on the optimal q̂ with the knowledge of P . This
gives an optimistic estimate for the active learner, as                                                                                                                             Discussion
the matrix P is not fully known in reality. Here we                                                    This study has provided mathematical analyses of quan-
performed a Monte Carlo simulation to quantify the eﬃ-                                                 titative aspects of word learning that provide key con-
ciency of an online active leaner who gradually updates                                                straints which any theoretical account for children’s word
knowledge in the matrix P and estimates q on the basis                                                 learning should satisfy. We reinspected the past theo-
of the sample estimate of P . If this online active leaner                                             retical claim by Blythe et al. (2010) that learning via
is comparable with the optimal active learner with q̂, we                                              independent fast mapping was eﬃcient enough to ac-
can treat the performance analysis on the optimal active                                               count for the average number of words known by 18-
leaner above (a few orders more eﬃcient) as holding for                                                year-olds. Our new analysis extends their analysis to
the online active leaner. For this purpose, we generated                                               fast mapping with non-uniform word frequency distri-
a N × M matrix P with N = 1000, M = 100, which                                                         butions, and shows that even learning via fast mapping
has the elements in each column are Zipfian probabili-                                                 is not eﬃcient enough to learn words whose distribu-
ties Pπ(i) ∝ i−a with the random coeﬃcients a ∈ [1, 1.5],                                              tion has rarely sampled words–including the Zipf (i.e.
where π : {1, . . . , N } 7→ {1, . . . , N } is a random per-                                          power-law) distribution, which describes empirical word
mutation. The online active learner has the uniform                                                    frequency distributions from natural language.
                                                                                                 523

   Given that this new analysis implies learning would         Hulit, L., & Howard, M. R. (2002). Born to talk.
be too slow under realistic distributions, we consider           Toronto: Allyn and Bacon.
a more eﬃcient learning scheme, in which the learner           Kachergis, G. (2012). Learning nouns with domain-
can choose preferred situations from which words are             general associative learning mechanisms. In N. Miyake,
learned. This type of active control over situations or          D. Peebles, & R. P. Cooper (Eds.), Proceedings of the
contexts seems natural with respect to general obser-            34th annual conference of the cognitive science society
vations of children’s behavior, and has been shown to            (p. 533-538). Austin, TX: Cognitive Science Society.
benefit adult word learners (Kachergis, Yu, & Shiﬀrin,         Kachergis, G., Yu, C., & Shiﬀrin, R. M. (2013). Ac-
2013), but has not been subjected to theoretical analysis        tively learning object names across ambiguous situa-
as far as we know. We quantify and evaluate the eﬀect of         tions. Topics in Cognitive Science.
this type of self-directed learning in word learning. As       MacWhinney, B., & Snow, C. (1990). The child language
the least probable word in the distribution determines           data exchange system: An update. Journal of Child
learning eﬃciency, we analyzed the active choice for the         Language, 17 (02), 457–472.
situations maximizing this key parameter. Analyzing an         Smith, L. B. (2000). How to learn words: An associa-
empirical dataset of the words given situations, we esti-        tive crane. In R. Golinkoﬀ & K. Hirsh-Pasek (Eds.),
mate that active learning is over two hundred times more         Breaking the word learning barrier (pp. 51–80). Ox-
eﬃcient in learning time than passive learning. This             ford: Oxford University Press.
result suggests that active choice in word learning can        Vlach, H. A., & Johnson, S. P. (2013). Memory con-
resolve the issue that naturalistic non-uniform word dis-        straints on infants’ cross-situational statistical learn-
tributions greatly slows passive fast mapping.                   ing. Cognition, 127 , 375–382.
   Our analyses in this paper utilized one of the simplest     Vlach, H. A., & Sandhofer, C. M. (2012). Fast mapping
learning schemes, fast mapping, in order to highlight            across time: Memory processes support children’s re-
the eﬀects of varied word frequency distributions, and of        tention of learned words. Frontiers in Developmental
active learning. However, we expect the analytic tech-           Psychology, 3 (46), 1–8.
niques we employed would also allow analysis of other          Xiao, J., Hays, J., Ehinger, K., Oliva, A., & Torralba, A.
learning algorithms, including many proposed variants            (2010). Sun database: Large-scale scene recognition
of cross-situational learning. In future work, we will re-       from abbey to zoo. In Ieee conference on computer
port similar analyses for learning schemes with perhaps          vision and pattern recognition.
greater cognitive plausibility. On this path towards ever      Zipf, G. (1949). Human behavior and the principle of
more realistic assumptions about the language environ-           least eﬀort. Cambridge, MA: Addison-Wesley.
ment and learners’ ability to shape it, we expect to make
progress toward a general theoretical framework span-           Appendix: Iterated linear programming
ning many proposed word learning schemes.                      For a N × M matrix P , write its ith row by Pi . Let
                                                               I = {1, 2, . . . , N } be the set of all indices. At the initial
                  Acknowledgment                               step, define
This study is supported by the JSPS KAKENHI Grant-
in-Aid for Young Scientists JP 16H05860.                                         K0 := ∅, C0 := SM , q0 := e1 ,
                                                               where e1 := (1, 0, . . . , 0)T ∈ RN . Then for 0 < n ≤ N ,
                      References
                                                               define
Blom, G., Holst, L., & Sandell, D. (1994). Problems and
   snapshots from the world of probability. In (p. 85-87).         kn    :=       arg min Pk qn−1 , Kn := Kn−1 ∪ {kn },
   New York, NY: Springer-Verlag New York.                                       k∈I\Kn−1
                                                                                 {                                }
Bloom, P. (2000). How children learn the meaning of                                          ∧
   words. Cambridge, MA: MIT Press.                                Cn    :=        q ∈ C0 |      (Pkn − Pk )q ≤ 0 ,
                                                                                            k∈Kn
Blythe, R. A., Smith, A. D. M., & Smith, K. (2016).
   Word learning under infinite uncertainty. Cognition,            qn    := arg max Pkn q,
                                                                                   q∈Cn
   151 , 18–27.
Blythe, R. A., Smith, K., & Smith, A. D. M. (2010,             until n = m such that mink∈Km Pk qm ≤ mink∈I Pk qm .
   January). Learning Times for Large Lexicons Through         The algorithm stops the iterative procedure by out-
   Cross-Situational Learning. Cognitive Science, 34 (4),      putting q := qm .
   620–642.
Chomsky, N. (1965). Aspects of the theory of syntax.
   MIT Press.
Gleitman, L. (1990). The structural sources of word
   meaning. Language Acquisition, 1 , 3–55.
                                                           524

