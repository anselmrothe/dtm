Quantifying the impact of active choice in word learning
Shohei Hidaka (shhidaka@jaist.ac.jp) and Takuma Torii (tak.torii@jaist.ac.jp)
Japan Advanced Institute of Science and Technology
1-1 Asahidai, Nomi, Ishikawa, Japan

George Kachergis (george.kachergis@gmail.com)
Dept. of Artificial Intelligence / Donders Institute, Radboud University
Nijmegen, the Netherlands
Abstract

word production starts when the child is 12 months old
on average, and by 18 months children can produce 50
words and comprehend 100-150 (Hulit & Howard, 2002).
By 18 years of age, it is estimated we know over 60,000
words (Bloom, 2000). Under the assumption that each
child has 8 hours of word learning opportunity everyday,
these estimates mean the child learns a new word every
learning hour for 18 years of the life.
Given these empirical estimates of word learning, theoretical studies have attempted to account for the quantitative characteristics of word learning. The first question is: What combination of learning mechanisms and
structure in the language environment allows children to
learn at this rate? This question poses a good necessary condition for any account of child word learning,
as it needs to address this quantitative aspect of word
learning.
As a first-order approximation, child learning may be
modeled as an independent sampling process in which
each word is learned independently. To estimate the
fastest possible learning rate, (Blythe, Smith, & Smith,
2010, 2016) proposed an idealized learning model to address acquiring a full lexicon in the long term: 60,000
words over 18 years. In their model, each word is
learned with its first sample – known as fast mapping
in the developmental literature. Under the simplifying
assumption that each word is independently learned via
fast-mapping, and its word frequency is distributed uniformly, their mathematical analysis of the model showed
that a cross-situational learner is suﬃciently fast to learn
all 60,000 words after experiencing a reasonably small
number of spoken words.

Past theoretical studies on word learning have oﬀered
simple sampling models as a means of explaining real
word learning, with a particular goal of addressing the
speed of word learning: people learn tens of thousands
of words within their first 18 years. The present study
revisits past theoretical claims by considering a more realistic word frequency distribution in which a large number of words are sampled with extremely small probabilities (e.g., according to Zipf’s law). Our new mathematical analysis of a recently-proposed simple learning model
suggests that the model is unable to account for word
learning in feasible time when the distribution of word
frequency is Zipfian (i.e., power-law distributed). To
ameliorate the diﬃculty of learning real-world word frequency distributions, we consider a type of active, selfdirected learning in which the learner can influence the
construction of contexts from which they learn words.
We show that active learners who choose optimal learning situations can learn words hundreds of times faster
than passive learners faced with randomly-sampled situations. Thus, in agreement with past empirical studies,
we find theoretical support for the idea that statistical
structure in real-world situations–potentially structured
for learning by both a self-directed learner, and by a
beneficent teacher–is a potential remedy for the pathological case of learning words with Zipf-distributed frequency.
Keywords: cognitive models of language acquisition;
cross-situational word learning; statistical learning

Child word learning
One of the most prominent diﬀerences between human
and nonhuman cognition is our language ability. Much
research has been dedicated to understanding the human
capability for language, with a great deal of discussion
focused on the process of language acquisition. A central
debate in this conversation considers whether acquisition is based on innate and language-specific mechanisms
(Chomsky, 1965; Gleitman, 1990), or bootstrapped from
domain-general mechanisms (Smith, 2000; Kachergis,
2012). From the former perspective, humans become
competent language users–mastering a complex system
of syntax to produce endless semantics–very rapidly, and
with relatively little training.
Word learning has been treated as an indicator of
language development, and has been compared with a
number of other indicators of cognitive abilities, such as
memory (Vlach & Johnson, 2013; Vlach & Sandhofer,
2012). Although there are multiple empirical estimates
of the number of words that children acquire, many studies agree that child’s word learning is quite fast. Early

Theoretical approach
Blythe et al.’s theoretical estimate has been treated as
a theoretical implication that shows learning via independent fast-mapping of words is eﬃcient enough to be
a model of child word learning. In this study, we reinspect this theoretical implication by introducing a more
realistic word frequency distribution. Our mathematical
analysis implies that the learning rate of the independent
fast mapping is quite sensitive to the word frequency
distribution. More importantly, even fast mapping–the
most eﬃcient learning, requiring only a single sample,
can be too slow to learn 60,000 words in 18 years, if

519

word frequency follows Zipf’s law (Zipf, 1949) or a fattailed distribution which is often found in natural corpora. Thus, our analysis implies that the independent
fast-mapping model cannot be an account for child word
learning, if there are many words sampled less frequently.
This mathematical implication leads to an empirical test
of whether the word distribution in the child-directed
speech is uniform or non-uniform such as a Zipf distribution. Thus, in the second study, we analyzed the
CHILDES corpora for word distribution in child-directed
speech.
Given this result of the mathematical analysis, we explore an extension of the word learning mechanism by
additionally assuming that the word learning is more active than that is supposed to be traditionally. Typically,
as analyzed in the past studies above (Blythe et al., 2010,
2016), the learning is supposed passive – the learner has
no choice but observing samples words and objects from
a given probability distribution. This is certainly oversimplified, as actual child word-learners choose when,
where and from whom they would like to learn words.
Thus, our second analysis estimates the impact of a form
of active choice of situations in word learning. Our analysis shows that active learning is likely to have a sufficiently beneficial impact to make word learning fast
enough to happen on a realistic timescale.

dren as young as three years old can quickly generalize
a novel name to objects when they hear the novel name
given to its fast instance. Due to this one-shot nature
of their word learning, it is called fast mapping. Capturing this empirical finding, the fast-mapping learner in
the model is supposed to learn a new pair of word and
object only with the first experience of it. The fastmapping learner is equivalent to the cross-situational
learner, if there is one correct pair of object and word in
each episode (M = 1).
As fast-mapping learning is the most eﬃcient scheme
(at least for independent word learning), it gives a good
baseline estimate of the number of samples to learn all
the words in a given list. Blythe et al. (2010) model
a fast-mapping learner acquiring words independently
drawn from a uniform distribution of W words given in
each episode. As every episode has one word with probability 1/W , this is equivalent to the so-called Coupon
Collector’s Problem (Blom, Holst, & Sandell, 1994). In
this problem, the expected time T to finish sampling all
the words is
W
∑
E[T ] =
E[ti ].
i=1

where ti is the time to sample a ith new word given (i−1)
words being learned. Thus,

Independent fast-mapping learning

E[T ] = W

W
∑

1/i ≈ W log W.

(1)

i=1

Uniformly distributed word frequency

Setting the number of words W = 60, 000, which is an
empirical estimate of the number of words 18 years old
knows on average, T = 660, 126. This estimate is comparable with the “reasonable” number of samples justified
by Blythe et al. (2010) which individual children can be
exposed to for their 18 years of lives.

Blythe, Smith, & Smith (2010) proposed a mathematical
model of word learning, which has a closed-form expression under a certain simplification. In their recent study,
Blythe, Smith, & Smith (2016) analyzed essentially the
same model, although slightly modified for analytic convenience. Here we briefly introduce the most recent form
(2016) of their model.
Blythe et al. originally consider cross-situational word
learning. Suppose there are W words and O objects in
the hypothetical world. Further the numbers of words
and objects are equal, W = O, in their cross-situational
learning scheme, and every object has its name and no
objects have two names. Namely, there are W correct
pairs of words and objects. Without loss of generality,
denote the W pairs by 1, 2, . . . , W , and suppose k th object is paired with the k th word.
Given these pairs being unknown, a word learner is to
infer correct pairs by going through episodes. In each
episode, the learner is exposed to M ≤ W words and
M objects, without any explicit information on which
word is paired with which object. With one episode with
M ≥ 2 objects and words, the learner cannot tell which
of M words should be associated to which of M objects.
The most simple model among a series of extended
ones is called fast-mapping learning model. In the literature of language development, it is well-known that chil-

Non-uniformly distributed word frequency
Here we extend this analysis on the fast-mapping
learner to the case with word frequency distributed nonuniformly. Our extended analysis will reveal that the
estimate based on Equation (1) by Blythe et al. is quite
“optimistic”, as an estimate with non-uniform word distribution is larger than that in general.
Here let us derive the number of episodes T that,
for 0 ≤ ϵ ≤ 1, the (1 − ϵ) of children learned all
the W words listed. Suppose a set of W words in
which each word 1, . . . , W is drawn from the distribution p = (p1 , . . . , pW ). The proportion of children who
finished learning all the words is (1 − ϵ) for 0 < ϵ < 1
requires the number of episodes T , which is the root of
W
∏
(

)
1 − (1 − pi )T = 1 − ϵ.

(2)

i=1

The left hand side of (2) is the probability that every
word is present at least once in the T episodes.

520

Sensitivity to non-uniformity of word
frequency distribution

Write
(

fW,ϵ (x) :=

)
1/W

log 1 − (1 − ϵ)
log (1 − x)

.

To analyze the sensitivity to the non-uniformity, here
we analyze the Zipf distribution with diﬀerent exponent parameters.
Denote the Zipf distribution
with the exponent parameter a ≥ 0 by p =
(1−a /HW,a , 2−a /HW,a , . . . , W −a /HW,a ) where HW,a is
∑W
the generalized harmonic number HW,a = i=1 i−a . It
is reduced to the uniform distribution by a = 0. The
larger the exponent a is, the minimal probability min p
is smaller. Thus, here we analyze the upper bound T+
as a function of the exponent parameter a.
Write T+ = fN,ϵ (min p), which gives a reasonable estimate of the upper bound of the root T of (2). As a
function of the exponent a, we have
(
)
∂HN,a
pmin
∂a /HN,a + log W
∂ log T+
=
∂a
(1 − pmin ) log (1 − pmin )

For the uniform distribution, pi = 1/W for every i =
1, . . . , W , the root of (2) is given by
T = fW,ϵ (1/W ).

(3)

This T is the number of episodes with which the proportion of children finished learning all the words is (1 − ϵ).
Setting ϵ = 1/2 in (3), we obtain the median of T ,
fW,1/2 (1/W ), that is comparable with the mean of T
in (1).
Unlike (3) for the uniform distribution, the root T of
Equation (2) in general is not closed-form. Thus, let us
consider the upper and lower bound for the root instead
of the rigorous form of it. For the general word distribution p = (p1 , . . . , pW ), the intermediate value theorem states that there exists a unique constant c holding
min p ≤ c ≤ max p, with which the root of (2) is expressed as
T = fW,ϵ (c).

and further we have
∂ 2 log T+
≥ 0.
∂a2
This implies the T+ is a super-exponential monotone
function of the exponent a. It is also numerically confirmed in Figure 1, in which the numbers of episodes
are shown as functions of the exponent for W =
10000, 60000. In this plot, a = 0 shows an estimate for
the uniform distribution, and a = 1 shows that of the
standard Zipfian distribution. It is striking that even
the fastest learning such as fast mapping can be quite
slow (exponentially as a function of a) for with distributions with some item with a very small probability.

Equivalently, we have inequality
fW,ϵ (max p) ≤ T ≤ fW,ϵ (min p).
As we are interested in the worst possible estimate of
T , this inequality states that the upper bound T+ :=
fW,ϵ (min p) of T is characterized with the probability to
sample the least frequent word min p.
This extended mathematical analysis implies that the
uniform distribution q = (1/W, . . . , 1/W ) of words gives
the minimal possible upper bound T+ among any frequency distribution of W words, as any distribution
min p of W words holds min p ≤ min q. Therefore, the
expectation of T in the form of (1) with the uniform
distributed words is the most optimistic, which may underestimate the number of episodes required for learning
with a realistic word distribution.
For example, let us consider an alternative case
that the W word follows the Zipf distribution p =
(1−1 /HW , 2−1 /HW , . . . , W −1 /HW ), where HW is the
∑W −1
harmonic number HW =
. In this case, the
i=1 i
minimal probability is min p ≈ 1.44 × 10−6 , and the upper bound T+ is 1.08 × 107 for ϵ = 0.01. This estimate
means that learning of Zipf-distributed words requires
16.4 times as many samples as learning of uniformlydistributed words. That means that 206 independent
episodes exposed to a word learner every hour (or three
episodes every minute), assuming 8 hours of learning everyday of 18 years of life. This estimate cannot possibly
be considered “reasonable” with respect to ordinary life
of children in any culture.

Empirical dataset
Given theoretical implication in the previous study, let
us analyze an empirical word distribution, which children typically are exposed to. It is diﬃcult to exactly
count “episodes” or “pairs of word and object” in a
real dataset, due to its ambiguity of definition and it
is also up to children’s subjective perspective. Here,
as a proxy of them, we counted the word frequency
based on child-directed speech in the CHILDES corpus
(MacWhinney & Snow, 1990). Figure 2 shows a representative word distribution of 51,446 words aggregated
over 4,163 transcripts of all the corpora in CHILDES retrieved in December 2007. The minimal word probability was 1.089 × 10−7 , which gives the upper bound T+ =
f51446,0.01 (1.089 × 10−7 ) = 1.420 × 108 or the median
estimate T+ = f51446,0.5 (1.089 × 10−7 ) = 1.030 × 108 .
These estimates of required samples, an order of magnitude larger than the optimistic theoretical estimate,
suggest that it is diﬃcult to learn these empirical words
with this Zipfian-like frequency distribution.

521

102

109
N = 10000
N = 60000

101

log P = -1.7548 log( rank ) + 2.6105

100

8

10

Required number of samples

-1

10

10-2

107

10-3
10-4

6

10

10-5
10-6

105

10-7
104

0

0.5

1

10-8
100

1.5

Exponent of the Zipf distribution

Figure 1: For ϵ = 0.01, 0.5, 0.99 (broken and solid lines),
N = 10000, 60000 and the exponent a = 0, 0.25, . . . , 1.5,
the required number of samples
M for a generalized Zipf
∑N
distribution pk = k −a / k=1 k −a is numerically calculated by the root of Equation (2).

101

102

103

104

105

Figure 2: Word frequency in a corpus aggregated from
the CHILDES transcripts.

ysis in the previous section, the minimal probability of
objects decides the number of samples required to complete the word learning, the best choice for the active
learner is given by the choice probability

Active choice of situations
Formulation

q̂ = arg maxq min(P q).

The implication of the mathematical analysis above,
which suggested that even fast-mapping may not be eﬃcient enough for non-uniformly distributed words, raises
a controversy between past theoretical analyses and empirical findings of quantitative aspects of word learning.
Here, we explore a possibility to reconcile the discrepancy between theory and empirical findings, by considering a further relaxation of past theoretical assumptions
about children’s word learning. In the conventional theoretical framework, the learner is assumed to be passive, having no choice but to observe and learn from a
given context: a randomly-sampled set of of objects, of
which a (random) subset are labeled with words. This
assumption of a passive learner simplifies the theory,
but surely underestimates real learners, who have some
choice about which contexts they experience. Here, we
consider a type of active learner who is able to choose
from which situation/context he or she learns words.
Suppose that there are N word-object pairs and M situations, and that the conditional probability to observe
the ith word-object pair is pij given the j th situation.
Thus, the active learner has a choice of the situation out
of the given M situations from which he or she learns
the word-object pairs. Suppose that the active learner
chooses the j th situation by the probability qj . Let us
denote the N × M matrix of the conditional probability
by P = {pij }ij and the N × 1 vector of the choice probability by q = (q1 , q2 , . . . , qM )T . With this notation, the
marginal probability of word-object pairs is given by the
vector P q ∈ RN . According to our mathematical anal-

This minimal probability, min(P q̂), gives the theoretical upper bound for the minimal number of samples
fW,ϵ (min(P q̂)), as P is not known before empirical learning, and the active leaner also needs to estimate P from
the sample. For a given matrix P , the optimal q̂ can be
computed by the iterated linear programming algorithm
(See also Appendix for the detail).
As a baseline for the passive learner, we consider the
average min(P q) with the uniform distribution over the
vector q, whose lower bound is given by the Jensen’s
inequality
∫
min(P q)(N − 1)!dq ≥ min(P 1N /N ),
q∈SN

where the integral is taken over the N − 1 dimensional
unit simplex q ∈ SN . For a suﬃciently small x ≪ 1
and y ≪ 1, fW,ϵ (x)/fW,ϵ (y) ≈ y/x. Thus, the rate
R = min(P q̂)/ min(P 1N /N ) gives a good estimate for
the rate of eﬃciency R, by which the active learning
with the optimal probability q̂ R times faster than the
passive learning with a fixed probability q.

Empirical evaluation
To evaluate the potential impact of the active leaning,
we study the SUN database (Xiao, Hays, Ehinger, Oliva,
& Torralba, 2010) as an empirical object distributions in
an collection of real-life scenes. The SUN database (retrieved on September 25th in 2016) has N = 3, 458 objects and M = 1, 111 scenes in it. This data is supposed

522

to give the N × M matrix P in which each column is the
conditional probability of the objects given each scene.
If the scene choice probability is the uniform distribution q = 1N /N , the min(P q) was 8.30 × 10−9 . Meanwhile, with the optimal q̂, the min(P q̂) was 1.95 × 10−6 ,
which implies the active learning was approximately
min(P 1N /N )/ min(P q̂) = 235.3 times faster than the
baseline passive learning. The marginal probability distributions of objects for the baseline and optimal q are
shown in Figure 3. The diﬀerence between the two
marginal distributions is visible at their tails – the tail
for the uniform q decreases like an exponential function,
but that for the optimal q̂ decreases as a power function
(linear in the double log plot). This empirical evaluation
suggests that the active learning of interest can boost the
fast mapping a few orders more eﬃciently.

choice probability q1 = 1N /N . For k th batch of 1000
steps, the online learner samples the objects according
to the probability P qk , and constructs the sample probability matrix P̂k according to the sample frequency. After the k th sampling step, the online learner estimates
qk := arg maxq min(P̂k q). In each run of this procedure,
we repeat up to 100×1000 samples, and obtain one sample for the number of required samples to finish learning
all the 1000 objects. With 100 runs, we obtain the Monte
Carlo estimate of the online learner shown in Figure 4.
Figure 4 shows the sample probability distribution of the
number of required samples in the Monte Carlo simulation (circles: histogram, line: smoothed estimate), and
its comparable median estimate for the optimal learner
(green vertical line) and the passive learner with the uniform q (red vertical line). This simulation result shows
that the online learner is as fast as the optimal learner,
and is likely to be faster than the passive learner.

Uniform q (baseline)
Optimal q
0.25

10-2

Probability (Samples in the Monte Carlo Simulation)

The maginal probability of object

100

10-4

10-6

10-8
100

101

102

103

104

Passive learning
Optimal active learning
Online active learning
0.2

0.15

0.1

0.05

Rank of the objects
0
0

Figure 3: The marginal probability of objects for the
optimal q̂ (line) and its baseline (dots).

1

2
3
4
5
Number of required samples to finish learning

6
104

Figure 4: The probability distribution of the number
of required samples to finish learning for passive (red),
optimal (green), and online active learner (blue).

Online active learning
The quantification of the eﬃciency of active learning is
based on the optimal q̂ with the knowledge of P . This
gives an optimistic estimate for the active learner, as
the matrix P is not fully known in reality. Here we
performed a Monte Carlo simulation to quantify the eﬃciency of an online active leaner who gradually updates
knowledge in the matrix P and estimates q on the basis
of the sample estimate of P . If this online active leaner
is comparable with the optimal active learner with q̂, we
can treat the performance analysis on the optimal active
leaner above (a few orders more eﬃcient) as holding for
the online active leaner. For this purpose, we generated
a N × M matrix P with N = 1000, M = 100, which
has the elements in each column are Zipfian probabilities Pπ(i) ∝ i−a with the random coeﬃcients a ∈ [1, 1.5],
where π : {1, . . . , N } 7→ {1, . . . , N } is a random permutation. The online active learner has the uniform

Discussion
This study has provided mathematical analyses of quantitative aspects of word learning that provide key constraints which any theoretical account for children’s word
learning should satisfy. We reinspected the past theoretical claim by Blythe et al. (2010) that learning via
independent fast mapping was eﬃcient enough to account for the average number of words known by 18year-olds. Our new analysis extends their analysis to
fast mapping with non-uniform word frequency distributions, and shows that even learning via fast mapping
is not eﬃcient enough to learn words whose distribution has rarely sampled words–including the Zipf (i.e.
power-law) distribution, which describes empirical word
frequency distributions from natural language.

523

Given that this new analysis implies learning would
be too slow under realistic distributions, we consider
a more eﬃcient learning scheme, in which the learner
can choose preferred situations from which words are
learned. This type of active control over situations or
contexts seems natural with respect to general observations of children’s behavior, and has been shown to
benefit adult word learners (Kachergis, Yu, & Shiﬀrin,
2013), but has not been subjected to theoretical analysis
as far as we know. We quantify and evaluate the eﬀect of
this type of self-directed learning in word learning. As
the least probable word in the distribution determines
learning eﬃciency, we analyzed the active choice for the
situations maximizing this key parameter. Analyzing an
empirical dataset of the words given situations, we estimate that active learning is over two hundred times more
eﬃcient in learning time than passive learning. This
result suggests that active choice in word learning can
resolve the issue that naturalistic non-uniform word distributions greatly slows passive fast mapping.
Our analyses in this paper utilized one of the simplest
learning schemes, fast mapping, in order to highlight
the eﬀects of varied word frequency distributions, and of
active learning. However, we expect the analytic techniques we employed would also allow analysis of other
learning algorithms, including many proposed variants
of cross-situational learning. In future work, we will report similar analyses for learning schemes with perhaps
greater cognitive plausibility. On this path towards ever
more realistic assumptions about the language environment and learners’ ability to shape it, we expect to make
progress toward a general theoretical framework spanning many proposed word learning schemes.

Hulit, L., & Howard, M. R. (2002). Born to talk.
Toronto: Allyn and Bacon.
Kachergis, G. (2012). Learning nouns with domaingeneral associative learning mechanisms. In N. Miyake,
D. Peebles, & R. P. Cooper (Eds.), Proceedings of the
34th annual conference of the cognitive science society
(p. 533-538). Austin, TX: Cognitive Science Society.
Kachergis, G., Yu, C., & Shiﬀrin, R. M. (2013). Actively learning object names across ambiguous situations. Topics in Cognitive Science.
MacWhinney, B., & Snow, C. (1990). The child language
data exchange system: An update. Journal of Child
Language, 17 (02), 457–472.
Smith, L. B. (2000). How to learn words: An associative crane. In R. Golinkoﬀ & K. Hirsh-Pasek (Eds.),
Breaking the word learning barrier (pp. 51–80). Oxford: Oxford University Press.
Vlach, H. A., & Johnson, S. P. (2013). Memory constraints on infants’ cross-situational statistical learning. Cognition, 127 , 375–382.
Vlach, H. A., & Sandhofer, C. M. (2012). Fast mapping
across time: Memory processes support children’s retention of learned words. Frontiers in Developmental
Psychology, 3 (46), 1–8.
Xiao, J., Hays, J., Ehinger, K., Oliva, A., & Torralba, A.
(2010). Sun database: Large-scale scene recognition
from abbey to zoo. In Ieee conference on computer
vision and pattern recognition.
Zipf, G. (1949). Human behavior and the principle of
least eﬀort. Cambridge, MA: Addison-Wesley.

Appendix: Iterated linear programming

Acknowledgment

For a N × M matrix P , write its ith row by Pi . Let
I = {1, 2, . . . , N } be the set of all indices. At the initial
step, define

This study is supported by the JSPS KAKENHI Grantin-Aid for Young Scientists JP 16H05860.

K0 := ∅, C0 := SM , q0 := e1 ,

References

where e1 := (1, 0, . . . , 0)T ∈ RN . Then for 0 < n ≤ N ,
define

Blom, G., Holst, L., & Sandell, D. (1994). Problems and
snapshots from the world of probability. In (p. 85-87).
New York, NY: Springer-Verlag New York.
Bloom, P. (2000). How children learn the meaning of
words. Cambridge, MA: MIT Press.
Blythe, R. A., Smith, A. D. M., & Smith, K. (2016).
Word learning under infinite uncertainty. Cognition,
151 , 18–27.
Blythe, R. A., Smith, K., & Smith, A. D. M. (2010,
January). Learning Times for Large Lexicons Through
Cross-Situational Learning. Cognitive Science, 34 (4),
620–642.
Chomsky, N. (1965). Aspects of the theory of syntax.
MIT Press.
Gleitman, L. (1990). The structural sources of word
meaning. Language Acquisition, 1 , 3–55.

kn

:=

arg min Pk qn−1 , Kn := Kn−1 ∪ {kn },
k∈I\Kn−1

{
Cn

:=

q ∈ C0 |

∧

}
(Pkn − Pk )q ≤ 0 ,

k∈Kn

qn

:= arg max Pkn q,
q∈Cn

until n = m such that mink∈Km Pk qm ≤ mink∈I Pk qm .
The algorithm stops the iterative procedure by outputting q := qm .

524

