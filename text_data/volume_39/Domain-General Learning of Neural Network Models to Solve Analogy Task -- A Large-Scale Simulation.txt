     Domain-General Learning of Neural Network Models to Solve Analogy Tasks
                                                  – A Large-Scale Simulation
                                                 Arianna Yuan (xfyuan@stanford.edu)
                                Department of Psychology, Stanford University, Stanford, CA 94305 USA
                               Abstract                                   due to enhanced ability to suppress associative responses and
                                                                          to maintain contexts.
   Several computational models have been proposed to
   explain the mental processes underlying analogical reasoning.             The evidence for a domain-specific component comes
   However, previous models either lack a learning component              from the finding that children’s ability to perform analogy
   or use limited, artificial data for simulations. To address these      task depends on their familiarity with the test material.
   issues, we build a domain-general neural network model that
   learns to solve analogy tasks in different modalities, e.g., texts     Goswami and Brown (1990a, 1990b) found that when
   and images. Importantly, it uses word representations and              familiar concepts were used in analogy tasks, performances
   image representations computed from large-scale naturalistic           were much better. These findings underscore the contribution
   corpus. The model reproduces several key findings in the
   analogical reasoning literature, including relational shift and        of domain-specific knowledge in analogy-making, yet change
   familiarity effect, and demonstrates domain-general learning           in this aspect is unlikely to boost task performance in other
   capacity. Our model also makes interesting predictions on              domain or modality.
   cross-modality transfer of analogical reasoning that could be
   empirically tested. Our model makes the first step towards a              Several computational models have been proposed to
   computational framework that is able to learn analogy tasks            account for how people solve analogy tasks. A useful
   using naturalistic data and transfer to other modalities.
                                                                          classification scheme is to group them into three types
   Keywords: analogical reasoning; learning; cross-modality               of models (French, 2002; Gentner & Forbus, 2010):
   transfer; neural network models.
                                                                          symbolic models (Kuehne, Forbus, Gentner, & Quinn,
                                                                          2000; Falkenhainer, Forbus, & Gentner, 1989), connectionist
                           Introduction                                   models (Holyoak & Thagard, 1989; Hummel & Holyoak,
Analogy is arguably one of the most important mechanisms                  1997; Kollias & McClelland, 2013) and hybrid models
through which people acquire new knowledge (Gentner,                      (Mitchell, 1993; Kokinov & Petrov, 2000). Most of symbolic
Holyoak, & Kokinov, 2001). Verbal analogy task such                       models represent analogy questions using predicates and
as “PIG:BOAR::DOG:? a. WOLF b. CAT” and visual                            logical forms. When asked to solve an analogy task such
analogy task such as Raven’s Progressive Matrices has                     as A:B::C:?, they use symbolic manipulations and search
been widely used in standardized test to assess students’                 algorithms to find the correct answer. One of the most
intellectual ability (Buck et al., 1998). For a long time,                influential symbolic approach to analogy-mapping is the
there has been a heated debate over whether analogy-mapping               Structure Mapping Engine (SME) (Falkenhainer et al., 1989;
is a domain-specific or domain-general process (Forbus,                   Gentner, 1983). It represents the base and source using
Gentner, Markman, & Ferguson, 1998). Nowadays, it has                     predicate-calculus and compares the two representations to
been increasingly clear that there are both a domain-general              see if there is any structural similarities between them. Once
component and a domain-specific component in learning                     optimal matching structures are identified, the system then
analogical reasoning.                                                     transfer structure in the source to the target. Later version of
   One phenomenon related to the domain-general                           SME have relaxed matching criteria to allow similar, but not
component is the relational shift in child development,                   identical predicate to match (Brian, 1990), but whether two
i.e., early in development children tend to choose the item               predicates are similar need to be explicitly computed.
that is more associated to the third item in the analogy                     Contrary to these symbolic approaches, connectionist
question A:B::C:[D1|D2] (Sternberg & Nigro, 1980), and                    models often use distributed representation for the items in
only older kids are able to use relational matching rather                an analogy task and encode their semantic and structural
than associations to perform the task. Some researchers have              similarity in a more implicit and continuous way, e.g., Kollias
argued that the lack of inhibitory control, which requires a              and McClelland (2013). The connectionist models learn to
full-blown pre-frontal cortex is partially responsible for the            make a correct response by adjusting connection weights so
associative response (Richland & Burchinal, 2013). The                    that the spreading activation within the neural networks could
ability to inhibit associative responding and maintain the                reveal the distributed representation of the target, thus leading
relational constraints imposed by A:B is domain-general, i.e.,            to the correct answer.
it is a universal prerequisite for analogy-making no matter                  Previous computational models of analogy-making have
which sensory modality or semantic domain the analogy task                significantly deepened our understanding of the mental
is built on. Hence, if one person is trained to perform analogy           processes underling analogical reasoning. Many of such
task in a particularly domain or modality, it is likely that the          models claim that they provide a domain-general explanation
training will help them do better in other domain or modality,            of how people perform analogy tasks. For example, the
                                                                      3621

Structure Mapping Engine, which was originally introduced            & Christiansen, 2015), which proposes that language
to solve analogy task in discrete semantic space, was later          acquisition relies partially on a domain-general mechanism,
used to answer analogy questions in continuous visual                which is learning and processing sensory stimuli unfolding
domain (Lovett, Forbus, & Usher, 2007). Most of the                  across time and space (Saffran, Aslin, & Newport, 1996).
connectionist models are theoretically domain-general as             Early since 1990s, researchers have found that if you train
well, since we can easily feed the distributed representation        a recurrent neural network to predict the next word in
of stimuli from different domain/modality to the input               a sentence, the word representation it learns reveals the
placeholders of those models. It is important for these              syntactic and semantic role of the word (Elman, 1990).
models to be domain-general, since humans are able to                Inspired by these previous studies, we use distributed
make within-modality generalization, such as recognizing             representations of words that reflect the statistics of word
examples that they have never encountered before (Lake,              co-occurrence in everyday life, which are more naturalistic.
Salakhutdinov, & Tenenbaum, 2015), or make cross-modality               As for the representations of visual stimuli, we process the
transfer (Hupp & Sloutsky, 2011). Specifically, it has been          images (geometric figures) using a deep convolutional neural
shown that relational knowledge is critical to the development       network that has been trained to perform object recognition
of analogical reasoning (Goswami, 1991). If one person has           task (Krizhevsky, Sutskever, & Hinton, 2012), and use the
received sufficient training to solve verbal analogy tasks, they     activation of the 7th hidden layer as the representations of
would gain some experience in relational knowledge. When             the visual stimuli. Previous studies have shown that deep
later asked to solve a visual analogy-making task, they do not       convolutional networks share a lot of similarities with human
need to learn it from scratch (Figure 1).                            visual system (Yamins, Hong, Cadieu, & DiCarlo, 2013).
                                                                     After we obtain the representations (embeddings) of the
                                                                     words and the images, we build a simple, light-weighted
                                                                     neural network to learn the analogy tasks.
                                                                                                Experiment
                                                                     Data. We first describe the representation we use for the
Figure 1: Stimuli used in the current study. Left: Verbal            word. The distributed representation of words are computed
analogy task. Right: Visual analogy task.                            using the continuous Skip-Gram model (Mikolov, Sutskever,
                                                                     Chen, Corrado, & Dean, 2013). It takes the current word to
   Despite the generalization ability that previous                  predict the surrounding window of context words. Hence,
computational models may have claimed, they are often                the estimated word embeddings capture the semantic and
missing some critical components. Symbolic approaches,               syntactic role of the words (Mikolov, Yih, & Zweig, 2013).
for example, rarely address how people learn to make an              We download the pre-trained word vectors from Google
analogy. Since the knowledge representations and the search          Word2Vec1 , which have 300 dimensions. For computational
algorithms in those models are preprogrammed, it is not clear        simplicity and efficiency, we reduce the dimensionality to 30
how experience of analogy-making in one domain could                 using principle component analysis (PCA) so that each word
facilitate analogy-making in another unfamiliar domain.              has a 30-d vector representation.
   As for the connectionist models, despite their theoretically         We use the same verbal analogy dataset from Mikolov, Yih,
domain-general nature, none of the previous studies has              and Zweig (2013)2 , which contains 19529 examples in total
directly tested cross-modality transfer. Particularly, they          with 907 unique words. We divide the dataset into three
only demonstrate within-modality generalization, i.e., the           sets, a training set (percentage: 80%, 15634 examples)3 , a
models are trained on some examples and tested on a                  validation set (10%, 1955 examples) and a test set (10%,
different set of examples in the same modality. Also,                1955 examples). We use the accuracy on validation data to
the distributed representation of stimuli are either manually        tune the hyper-parameters of the model and report accuracies
defined according to the semantic features of the items, or          on the test data. We run two types of tasks. The first
randomly assigned to some localist codes, which are not very         one is A:B::C:[D1|D2|...|Dn], in which the model is given
naturalistic.                                                        some choices and has to select the correct one (Task 1). We
                                                                     simulate Task 1 with different number of choices ranging
   Finally, all the previous modeling works have used small
                                                                     from 2 to 5. The second type of task takes the form of
datasets, containing hundreds of examples at most. We are
                                                                     A:B::C:?, i.e., the model is required to find the correct D from
wondering if we could build a model that scales up to handle
                                                                     all words in its vocabulary (Task 2).
a very large and naturalistic dataset. Particularly, we want
to understand if we use a dataset that reflects the statistical         To simulate associative responses children usually give
distribution of stimuli in real life, can the model still learn          1 https://code.google.com/archive/p/word2vec/
analogical reasoning and even make cross-modality transfer?              2 http://download.tensorflow.org/data/questions-words.txt
This idea is motivated by the statistical learning account               3 Training with fewer data (e.g., 50%) does not lead to
of language acquisition (Frost, Armstrong, Siegelman,                qualitatively different results.
                                                                 3622

                                                                                                             exi
when first learning analogy tasks, we construct three                  the softmax function φ(xx)i =             x  to normalize the input
                                                                                                          ∑nj=1 e j
datasets from the original analogy dataset. All of them                x to the final layer O, which amounts to W0 H. The ith value
are binary-choice questions, but the incorrect alternatives            of the output, φ(xx)i , indicates the probability of the ith choice
have different levels of associations with the third item C            being correct.
in the question. In the High Association Dataset, each
questions has an incorrect response alternative (foil) that
is strongly associated with the third word in the analogy
question, whereas examples in the Low Association Dataset
contain alternatives that is weakly associated with the third
item. Finally, in the Random Association Dataset, the
incorrect response alternatives are randomly selected from
the vocabulary so that it does not necessarily has a strong or
weak association with the third item C. We determine word
associations by calculating the cosine distance between the
word vectors of the two words. The smaller the distance, the
stronger the association.
   As for the visual stimuli, we use the Shape dataset from            Figure 2: Model Architecture with an exemplar question
Reed, Zhang, Zhang, and Lee (2015). It is a dataset of 2-D             “Boy:Girl::Brother:[Sister|Mom]”
colored shapes, with 8 colors, 4 shapes, 4 scales, 5 row and
column positions, and 24 rotation angles. We only use one
                                                                          Training. The model is trained by back-propagation
value for the rotation variable to avoid potential confusion
                                                                       using the TensorFlow framework (Abadi et al., 2015). We
(e.g., a square rotated 180◦ would be the same figure as the
                                                                       only update the weights W1 ,W2 and b. We train the neural
original figure, but it has a different label in the dataset), and
                                                                       networks with a batch size of 50 for each task. We find that
vary the other 5 variables to create a dataset. An example
                                                                       the model cannot learn well in the Task 2 setting, where it
question is showed in Figure 1, right. We generate 19080
                                                                       needs to pick up the correct D from all the possible words.
examples in total and randomly split them into a training set
                                                                       Therefore, in the following section we only report our results
(80%), a validation set (10%) and a test set (10%). Next, we
                                                                       for Task 1. We run 2 simulations. In the first simulation,
use the AlexNet, a deep neural network trained to recognize
                                                                       we examine whether we could reproduce the relational shift
objects (Krizhevsky et al., 2012), to process these images. We
                                                                       phenomenon. To this end, we train the model on a verbal
use the pre-trained connection weights from Caffe (Jia et al.,
                                                                       analogy dataset with random association. As the training
2014) to process each image in our dataset and use the hidden
                                                                       proceeds, we test it periodically on verbal High Association
activation of the 7th layer as its embedding. We also reduce
                                                                       test set and verbal Low Association test set (within-modality),
the dimensionality of the image embeddings to 30 using PCA.
                                                                       as well as the visual High Association test set and visual
   Model. The model architecture is fairly simple (Figure 2).
                                                                       Low Association test set (cross-modality). In the second
There are three layers, the input layer, the hidden layer and
                                                                       simulation, we look at the influences of number of choices on
the output layer. The input layer contains three pools that
                                                                       within-modality generalization and cross-modality transfer.
encode the first three items (A, B and C in the analogy
                                                                       Particularly, we first train the model to perform verbal
question). Each pool has 30 nodes, which corresponds to
                                                                       analogy tasks with different numbers of choices, then test it
the dimensionality of the word/visual embeddings. The
                                                                       on visual analogy tasks. We also conduct another version
connection weights from the input pool encoding A to the
                                                                       of the experiment in which the visual analogy tasks are
hidden layer H and the ones from the pool encoding C to H
                                                                       learned first. We run each of the simulations 10 times with
are the same, denoted by W1 . The connection weights from
                                                                       different random initialization of parameters, and in each run
the pool encoding B to the hidden layer H are denoted by W2 .
                                                                       the model is trained for 41 epochs.
The connection weights from the hidden layer H to the output
layer O is the embedding matrix of either the choices in the
                                                                                                     Results
current example (Task 1) or the whole vocabulary (Task 2).
Mathematically, the model can be described by the following            Simulation 1. We first test if our model reproduces
equations:                                                             the relational shift observed during child development.
                  H = W1 vA +W2 vB +W1 vC + b                          Figure 3 shows the accuracy curves of four test sets: verbal
                                                                (1)    High Association Dataset, verbal Low Association Dataset,
                  O = φ(W0 H)
                                                                       visual High Association Dataset and visual Low Association
where vA , vB , vC ∈ R30 are the word/image embeddings for             Dataset.
the stimuli, W1 ,W2 ∈ R30×30 are the connection weights from              First of all, we notice that our model clearly demonstrates
the input pools to the hidden layer, b ∈ R30 is the bias in            the tendency of associative responding in the early stage of
the hidden layer, and W0 = [VD1 ;VD2 ; ...;VDn ]T ∈ R30×30 is a        learning, since the accuracy on the High Association Dataset
matrix composed of embeddings of all the choices. We use               grows slowly (solid curves), compared with the accuracies
                                                                   3623

on the Low Association Dataset. As the training proceeds,                    same-modality test: β = −0.004, t(114) = −2.12, p = 0.036,
the model gradually learns to inhibit associative responses for              different-modality test: β = −0.1, t(114) = −54.63, p <
High Association Dataset (black solid curve).                                .001). However, we also find an interaction between test
   Second, the test accuracies on verbal sets are consistently               conditions and choice numbers. Particularly, the influence of
higher than those on the visual test sets, which is not                      choice numbers on different-modality test is much larger than
surprising given that the model is only trained on verbal                    the one on same-modality test, β = 0.096, t(114) = 37.13,
stimuli.     However, we still find a decent amount of                       p < .001. In the “learning visual analogy first” experiment
cross-modality transfer. For one thing, the tendency to                      (Figure 4, right), we find a similar effect of choice numbers
give associative responses earlier in the training is carried                on the different-modality test condition, as well as a similar
over to the visual modality, even though no visual stimuli                   interaction between test conditions and choice numbers, β =
has been used to train the network. In addition, as the                      0.071, t(114) = 14.07, p < .001.
accuracy on verbal Datasets gradually increases, the model                      Although both modalities demonstrate near perfect
also becomes better at answering questions in visual Low                     performance of within-modality generalization after
Association Dataset (gray dash-dot curve). However, this                     sufficient training (∼40 epochs), the performance of
improvement is not reliably transferred to the visual High                   “learning verbal analogy first” is consistently better than
Association Dataset, as the accuracy of this dataset remains                 the one of “learning visual analogy first” throughout the
near chance-level after prolonged training (gray solid curve).               training. For instance, half way through the training, the
   Third, we find that when the accuracy on verbal Low                       same-modality test accuracy of “learning verbal analogy
Association test stops to grow after roughly 3 epochs,                       first” is higher than the one of “learning visual analogy first”
the accuracy on the corresponding visual dataset continues                   (mean difference is 4.77 %, t(234) = 3.279, p = 0.001). This
to improve until after 11 epochs, which then slowly                          implies that the semantic space of word embeddings may
decreases (gray dash-dot curve). This can be explained                       have a stronger structural regularity, which makes it easier to
by the domain-general and the domain-specific component                      discover relations between words than images.
of analogical reasoning.        The model first learns the
domain-general component of analogical reasoning from the                                           learning verbal                        learning visual
                                                                                    1.00 ●               ●             ●          ●   ●         ●
                                                                                                                                                ●     ●      ●
training in the verbal domain, but later the training becomes                                                                                                ●
detrimental to cross-modality transfer since it continuously                        0.75
shapes the model to be specific to the verbal domain, thus
                                                                             Accuracy
reducing the accuracy in the corresponding visual domain.                           0.50   ●                                          ●
                                                                                                         ●                                      ●
                1.0                                                                 0.25                               ●
                                                                                                                                  ●
                                                                                                                                                      ●
                                                                                                                                                             ●
                                                                                                 train
                0.9                                                                            ● test: same modality
                                                                                                 test: different modality
                                                                                    0.00
         Accuracy
                0.8                                              verbal                    2             3             4          5 2           3     4      5
                                                                 visual                                                     Number of Choices
                0.7                                              high
                                                                 low
                0.6
                                                                             Figure 4: The influence of number of choices on accuracy.
                0.5
                                                                             Solid dots indicate chance-levels.
                      0   5   10   15   20   25   30   35   40
                                    Epoch
                                                                             Visualization of connection weights
Figure 3: The effect of association on accuracy and                          To get a deeper understanding of what the model has learned,
cross-modality transfer. The dotted line indicates the                       we visualize the weight matrix W1 and W2 . We find that W1 is
chance-level.                                                                very much like a identity matrix (Figure 5, left), whereas W2
                                                                             does not have a easily describable pattern (Figure 5, right).
   Simulation 2. To understand the effect of number of                          We compare our model with the vector offset method,
choices on accuracy for different data type (train/test) and                 which was used by Mikolov, Yih, and Zweig (2013) to solve
modality (verbal/visual), we run two linear models. In                       analogy tasks. Given the problem A:B::C:?, they found the
the first linear model, we look at two sets of data, which                   word D such that its embedding vector had the greatest cosine
are obtained from the experiment “learning verbal analogy                    similarity to xB − xA + xC . Their method amounts to assigning
first” and the one “learning visual analogy first”. In the                   an identity matrix I to W1 , −I to W2 , and a zero vector to
“learning verbal analogy first” experiment (Figure 4, left),                 the bias b in our model. The weights of our neural network
we find that both training and same-modality test accuracy                   show that our model is not doing exactly the same thing as
are almost perfect, whereas the cross-modality accuracy is                   the vector offset method does, since B does not approximate
not. The linear model shows that for all of these three                      the negative identity matrix. Hence, its weights are tuned to
conditions, the accuracy decreases as the number of choices                  solve the current analogy task, and the same weights are also
increases (train: β = −0.005, t(114) = −2.57, p = 0.011,                     capable of solving analogy task in another modality.
                                                                          3624

                                                                      that if you match the training task with the test task, neural
                                                                      networks are able to learn from few examples. In their paper,
                                                                      they trained a network to map a query example to one of the
                                                                      four candidates example so that both of them belong to the
                                                                      same category. The model learned the task very well. Our
                                                                      results lend further support to their approach. We find that
                  (a)                          (b)                    our model only learns efficiently under the Task 1 setting,
                                                                      where it chooses among a few choices rather than the whole
Figure 5: (a): Connections from A to the hidden layer.
                                                                      vocabulary. Our work extends Vinyals and colleagues’ results
(b): Connections from B to the hidden layer. Lighter areas
                                                                      by showing that our network model can make an inference
represent larger weights.
                                                                      not only on unseen stimuli, but also on unseen stimuli from a
                                                                      completely different modality.
                           Discussion                                    There are some limitations of the current work. First,
                                                                      the model is a simplification of the actual mental processes
In this article, we build the first neural network model that can     underlying analogy-making. A lot of previous computational
learn to solve analogy tasks and make cross-modality transfer.        model have given very insightful explanations of those mental
It uses word representations and image representations                processes (Gentner, 1983; Morrison et al., 2004; Kollias &
estimated from large-scale naturalistic corpora.             The      McClelland, 2013; Gergel’ & Farkaš, 2015), and our goal
model demonstrates both the domain-general and the                    is not to argue against those models or to provide a better
domain-specific component of analogical reasoning.                    model. Instead, our goal is to demonstrate the possibility
   Specifically, we see that the accuracy on the same-modality        that domain-general neural network models can learn from
test set is consistently higher than the accuracy on the              large-scale, realistic datasets to solve analogy tasks. Second,
cross-modality test set. This is aligned with the empirical           we have not directly compared our model performance with
finding that domain-specific knowledge boosts performance             human performance. It would be interesting to see how
in analogical reasoning (Goswami & Brown, 1990b). On                  human would respond to the analogy questions in the current
the other hand, the model demonstrates the domain-general             study and whether our model predictions align with human
property of analogy-making by showing cross-modality                  data in the future.
transfer. This is relevant to a broader topic in cognitive
science, the zero-shot learning. Zero-shot learning refers                                  Acknowledgments
to the ability to solve a task despite not having received            A.Y. is grateful to Jay McClelland for helpful discussions
any training examples of that task. As human beings,                  about this work, and the whole Parallel Distributed
we do zero-shot learning all the time. Only recently did              Processing Lab at Stanford for their useful comments.
researchers begin to simulate zero-shot learning using neural
network models. For instance, in Socher, Ganjoo, Manning,                                        References
and Ng (2013), they showed that learning the distributions            Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
of words in texts as a semantic space helps the model                    Citro, C., . . . Zheng, X. (2015). TensorFlow: Large-scale
understand the visual appearances of objects, and enables                machine learning on heterogeneous systems. (Software
the model to recognize objects even if no training data is               available from tensorflow.org)
available for that category. Our model contribute to the              Brian, F. (1990). Analogical interpretation in context. In
zero-shot learning literature by showing that zero-learning              Proceedings of the 12th Annual Meeting of the Cognitive
is possible for analogy-making task as well. It also makes               Science Society.
some interesting predictions that can be empirically tested.          Buck, G., VanEssen, T., Tatsuoka, K., Kostin, I., Lutz, D., &
The success of our model suggests the possibility that there             Phelps, M. (1998). Development, selection and validation
might be some similar structural regularities in the word                of a set of cognitive and linguistic attributes for the SAT
embeddings extracted from naturalistic corpus and in the                 I Verbal: Analogy section. ETS Research Report Series,
image embeddings extracted from object recognition models.               1998(1).
This similarity explains why our model makes cross-modal              Elman, J. L. (1990). Finding structure in time. Cognitive
transfer.                                                                Science, 14(2), 179–211.
   Our results are also relevant to another line of research,         Falkenhainer, B., Forbus, K. D., & Gentner, D. (1989).
the one-shot learning. One-shot learning refers to the                   The structure-mapping engine: Algorithm and examples.
problem of learning from one or very few examples. Classic               Artificial Intelligence, 41(1), 1–63.
deep learning neural networks could not perform one-shot              Forbus, K. D., Gentner, D., Markman, A. B., & Ferguson,
learning, which is a common criticism of neural networks                 R. W. (1998). Analogy just looks like high level perception:
being plausible cognitive models of human learning (Lake,                Why a domain-general approach to analogical mapping is
Ullman, Tenenbaum, & Gershman, 2016). However, recently                  right. Journal of Experimental & Theoretical Artificial
Vinyals, Blundell, Lillicrap, Wierstra, et al. (2016) showed             Intelligence, 10(2), 231–257.
                                                                  3625

French, R. M. (2002). The computational modeling of                Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015).
   analogy-making. Trends in Cognitive Sciences, 6(5),               Human-level concept learning through probabilistic
   200-205.                                                          program induction. Science, 350(6266), 1332–1338.
Frost, R., Armstrong, B. C., Siegelman, N., & Christiansen,        Lake, B. M., Ullman, T. D., Tenenbaum, J. B., & Gershman,
   M. H. (2015). Domain generality versus modality                   S. J. (2016). Building machines that learn and think like
   specificity: the paradox of statistical learning. Trends in       people. arXiv preprint arXiv:1604.00289.
   Cognitive Sciences, 19(3), 117-125.                             Lovett, A., Forbus, K., & Usher, J. (2007). Analogy
Gentner, D. (1983). Structure-mapping: A theoretical                 with qualitative spatial representations can simulate solving
   framework for analogy. Cognitive Science, 7(2), 155–170.          Raven’s Progressive Matrices. In Proceedings of the 29th
Gentner, D., & Forbus, K. D. (2010). Computational models            Annual Meeting of the Cognitive Science Society.
   of analogy. Wiley Interdisciplinary Reviews: Cognitive          Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &
   Science, 2(3), 266–276.                                           Dean, J. (2013). Distributed representations of words and
Gentner, D., Holyoak, K. J., & Kokinov, B. N. (2001). The            phrases and their compositionality. In Advances in Neural
   analogical mind: Perspectives from cognitive science. MIT         Information Processing Systems (pp. 3111–3119).
   press.                                                          Mikolov, T., Yih, S. W.-t., & Zweig, G. (2013). Linguistic
Gergel’, P., & Farkaš, I. (2015). Connectionist modeling            regularities in continuous space word representations. In
   of part–whole analogy learning. In Proceedings of the             Proceedings of the 2013 Conference of the North American
   EuroAsianPacific Joint Conference on Cognitive Science.           Chapter of the Association for Computational Linguistics:
Goswami, U. (1991). Analogical reasoning: What develops?             Human Language Technologies (NAACL-HLT-2013).
   a review of research and theory. Child Development, 62(1),        Association for Computational Linguistics.
   1–22.                                                           Mitchell, M. (1993). Analogy-making as perception: A
Goswami, U., & Brown, A. L. (1990a). Higher-order                    computer model. MIT Press.
   structure and relational reasoning: Contrasting analogical      Morrison, R. G., Krawczyk, D. C., Holyoak, K. J., Hummel,
   and thematic relations. Cognition, 36(3), 207–226.                J. E., Chow, T. W., Miller, B. L., & Knowlton, B. J. (2004).
                                                                     A neurocomputational model of analogical reasoning
Goswami, U., & Brown, A. L. (1990b). Melting chocolate
                                                                     and its breakdown in frontotemporal lobar degeneration.
   and melting snowmen: Analogical reasoning and causal
                                                                     Journal of Cognitive Neuroscience, 16(2), 260–271.
   relations. Cognition, 35(1), 69–95.
                                                                   Reed, S. E., Zhang, Y., Zhang, Y., & Lee, H. (2015). Deep
Holyoak, K. J., & Thagard, P. (1989). Analogical mapping by
                                                                     visual analogy-making. In Advances in Neural Information
   constraint satisfaction. Cognitive Science, 13(3), 295–355.
                                                                     Processing Systems (pp. 1252–1260).
Hummel, J. E., & Holyoak, K. J. (1997). Distributed
                                                                   Richland, L. E., & Burchinal, M. R. (2013). Early executive
   representations of structure: A theory of analogical access
                                                                     function predicts reasoning development. Psychological
   and mapping. Psychological Review, 104(3), 427–466.
                                                                     Science, 24(1), 87–92.
Hupp, J. M., & Sloutsky, V. M. (2011). Learning to                 Saffran, J. R., Aslin, R. N., & Newport, E. L. (1996).
   learn: From within-modality to cross-modality transfer            Statistical learning by 8-month-old infants.         Science,
   during infancy. Journal of Experimental Child Psychology,         274(5294), 1926–1928.
   110(3), 408-421.                                                Socher, R., Ganjoo, M., Manning, C. D., & Ng, A.
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J.,          (2013). Zero-shot learning through cross-modal transfer.
   Girshick, R., . . . Darrell, T. (2014). Caffe: Convolutional      In Advances in Neural Information Processing Systems (pp.
   architecture for fast feature embedding. arXiv preprint           935–943).
   arXiv:1408.5093.                                                Sternberg, R. J., & Nigro, G. (1980). Developmental patterns
Kokinov, B., & Petrov, A. (2000). Dynamic extension of               in the solution of verbal analogies. Child Development,
   episode representation in analogy-making in AMBR. In              27–38.
   Proceedings of the 22nd Annual Meeting of the Cognitive         Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.
   Science Society.                                                  (2016). Matching networks for one shot learning. In
Kollias, P., & McClelland, J. (2013). Context, cortex, and           Advances in Neural Information Processing Systems (pp.
   associations: a connectionist developmental approach to           3630–3638).
   verbal analogies. Frontiers in Psychology, 4, 857.              Yamins, D. L., Hong, H., Cadieu, C., & DiCarlo,
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012).               J. J. (2013). Hierarchical modular optimization of
   ImageNet classification with deep convolutional neural            convolutional networks achieves representations similar to
   networks. In Advances in Neural Information Processing            macaque IT and human ventral stream. In Advances in
   Systems (pp. 1097–1105).                                          Neural Information Processing Systems (pp. 3093–3101).
Kuehne, S., Forbus, K., Gentner, D., & Quinn, B. (2000).
   SEQL: Category learning as progressive abstraction using
   structure mapping. In Proceedings of the 22nd Annual
   Meeting of the Cognitive Science Society.
                                                               3626

