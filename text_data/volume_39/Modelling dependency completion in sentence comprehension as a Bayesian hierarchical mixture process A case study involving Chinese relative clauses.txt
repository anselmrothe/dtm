                   Modelling dependency completion in sentence comprehension
                                   as a Bayesian hierarchical mixture process:
                                A case study involving Chinese relative clauses
                                           Shravan Vasishth (vasishth@uni-potsdam.de)
                                      Department of Linguistics, University of Potsdam, Germany.
                                             Nicolas Chopin (nicolas.chopin@ensae.fr)
                        École Nationale de la Statistique et de l’administration économique, Malakoff, France.
                                           Robin Ryder (ryder@ceremade.dauphine.fr)
              Centre de Recherche en Mathématiques de la Décision, CNRS, UMR 7534, Université Paris-Dauphine,
                                                  PSL Research University, Paris, France.
                                    Bruno Nicenboim (bruno.nicenboim@uni-potsdam.de)
                                      Department of Linguistics, University of Potsdam, Germany.
                              Abstract                                  determines comprehension difficulty as measured by read-
   We present a case-study demonstrating the usefulness of              ing times or question-response accuracy. For example, the
   Bayesian hierarchical mixture modelling for investigating cog-       Dependency Locality Theory (DLT) by Gibson (2000) and
   nitive processes. In sentence comprehension, it is widely as-        the cue-based retrieval account of Lewis and Vasishth (2005)
   sumed that the distance between linguistic co-dependents af-
   fects the latency of dependency resolution: the longer the           both assume that the longer the distance between two co-
   distance, the longer the retrieval time (the distance-based ac-      dependents such as a subject and a verb, the greater the re-
   count). An alternative theory, direct-access, assumes that re-       trieval difficulty at the moment of dependency completion.
   trieval times are a mixture of two distributions: one distribu-
   tion represents successful retrievals (these are independent of      As shown in (1), the distance between co-dependents can in-
   dependency distance) and the other represents an initial failure     crease if a phrase intervenes.
   to retrieve the correct dependent, followed by a reanalysis that        As another example, consider the self-paced reading study
   leads to successful retrieval. We implement both models as
   Bayesian hierarchical models and show that the direct-access         in Gibson and Wu (2013) in Chinese subject and object rel-
   model explains Chinese relative clause reading time data better      ative clauses. The dependent variable here was the reading
   than the distance account.                                           time at the head noun (official). As shown in (2), the dis-
   Keywords: Bayesian Hierarchical Finite Mixture Models;               tance between the head noun and the gap it is coindexed with
   Psycholinguistics; Sentence Comprehension; Chinese Relative
   Clauses; Direct-Access Model; K-fold Cross-Validation                is larger in subject relatives compared to object relatives.1
                                                                        Thus, the distance account predicts an object relative advan-
                           Introduction                                 tage. For simplicity, we operationalize distance here as the
                                                                        number of words intervening between the gap inside the rel-
Bayesian cognitive modelling (Lee & Wagenmakers, 2014),
                                                                        ative clause and the head noun. In the DLT, distance is op-
using probabilistic programming languages like JAGS
                                                                        erationalized as the number of (new) discourse referents in-
(Plummer, 2012), is an important tool in cognitive science.
                                                                        tervening between two co-dependents; and in the cue-based
We present a case study from sentence processing research
                                                                        retrieval model, distance is operationalized in terms of decay
showing how hierarchical mixture models can be profitably
                                                                        in working memory (i.e., time passing by).
used to develop probabilistic models of cognitive processes.
Although the case study concerns a specialized topic in psy-
                                                                           (2)    a. Subject relative
cholinguistics, the approach developed here will be of general
interest to the cognitive science community.                                          [GAPi yaoqing fuhao de] guanyuani
   In sentence comprehension research, dependency comple-                             GAP invite      tycoon DE official
tion is assumed by many theories to be a key event. For ex-                           xinhuaibugui
ample, consider a sentence such as (1):
                                                                                      have bad intentions
   (1)     a. The man (on the bench) was sleeping
                                                                                      ‘The official who invited the tycoon has bad in-
In order to understand who was doing what, the noun The                               tentions.
man must be recognized to be the subject of the verb phrase
was sleeping; this dependency is represented here as a di-                        b. Object relative
rected arrow. One well-known proposal (Just & Carpenter,                    1 The dependency could be equally well be between the relative
1992), which we will call the distance account, is that depen-          clause verb and the head noun; nothing hinges on assuming a gap-
dency distance between linguistically related elements partly           head noun dependency.
                                                                    1278

              [fuhao yaoqing GAPi de] guanyuani                            nally presented in Gibson and Wu (2013) as a repeated mea-
                                                                           sures ANOVA.
              tycoon invite         GAP DE official
                                                                              To summarize, the conclusion from the above result would
              xinhuaibugui
                                                                           be that in Chinese, subject relatives are harder to process than
               have bad intentions                                         object relatives because the gap inside the relative clause is
                                                                           more distant from the head noun in subject vs. object rela-
              ‘The official who the tycoon invited has bad in-
                                                                           tives. This makes it more difficult to complete the gap-head
              tentions.
                                                                           noun dependency in subject relatives. This distance-based ex-
   In the Gibson and Wu study, reading times were recorded                 planation of processing difficulty is plausible given the con-
using self-paced reading in the two conditions, with 37 sub-               siderable independent evidence from languages such as En-
jects and 15 items, presented in a standard Latin square de-               glish, German, Hindi, Persian and Russian that dependency
sign. The experiment originally had 16 items, but one item                 distance can affect reading time (see review in Safavi, Hu-
was removed in the published analysis due to a mistake in the              sain, and Vasishth (2016)).
item. We coded subject relatives as −1/2, and object relatives
as +1/2; this implies that an overall object relative advantage
would show a negative coefficient. In other words, an object
relative advantage corresponds to a negative sign on the esti-
mate.
                                                                                                      8
                                                                              reading time (log ms)
   The distance account’s predictions can be evaluated by fit-
ting the hierarchical linear model shown in (1). Assume that
(i) i indexes participants, i = 1, . . . , I and j indexes items,
 j = 1, . . . , J; (ii) yi j is the reading time in milliseconds for the
i-th participant reading the j-th item; and (iii) the predictor X                                     7
is sum-coded (±1/2), as explained above. Then, the data yi j
(reading times in milliseconds) are defined to be generated by
the following model:
                                                                                                      6
                     yi j = β0 + β1 Xi j + ui + w j + εi j          (1)
where ui ∼ Normal(0, σ2u ), w j ∼ Normal(0, σ2w ) and εi j ∼
Normal(0, σ2e ); all three sources of variance are assumed to
be independent. The terms ui and w j are called varying in-                                           5
tercepts for participants and items respectively; they repre-                                             obj−ext            subj−ext
sent by-subject and by-item adjustments to the fixed-effect                                                         Condition
intercept β0 . Their variances, σ2u and σ2w represent between-
participant (respectively item) variance.
   This model is effectively a statement about the generative              Figure 1: Boxplots showing the distribution of reading times
process that produced the data. If the distance account is cor-            by condition of the Gibson and Wu (2013) data.
rect, we would expect to find evidence that the slope β1 is
negative; specifically, reading times for object relatives are                However, the distributions of the reading times for the
expected to be shorter than those for subject relatives. As                two conditions show an interesting asymmetry that cannot be
shown in Table 1, this prediction appears, at first sight, to be           straightforwardly explained by the distance account. At the
borne out. Subject relatives are estimated to be read 120 ms               head noun, the reading times in subject relatives are much
slower than object relatives, apparently consistent with the               more spread out than in object relatives. This is shown in
predictions of the distance account.                                       Figure 1, where reading times are shown on the log scale.
                                                                           Although this spread was ignored in the original analysis, a
                       Estimate      Std. Error      t value               standard response to heterogeneous variances (heteroscedas-
              βˆ 0       548.43           51.56      10.64*                ticity) is to delete “outliers” based on some criterion; a com-
                                                                           mon criterion is to delete all data lying beyond ±2.5SD in
              βˆ 1      -120.39           48.01      -2.51*
                                                                           each condition.2 This procedure assumes that the data points
Table 1: A linear mixed model using raw reading times in                   identified as extreme are irrelevant to the question being in-
milliseconds as dependent variable, corresponding to the re-               vestigated. An alternative approach is to not delete data but to
ported results in Gibson and Wu 2013. Statistical significance             downweight the extreme values by applying a variance stabi-
is shown by an asterisk.                                                   lizing transform (Box & Cox, 1964). Taking a log-transform
                                                                              2 In the published paper, Gibson and Wu (2013) did not delete
  The object relative advantage shown in Table 1 was origi-                any data, leading to the results shown in Table 1.
                                                                       1279

of the reading time data, or a reciprocal transform, can re-
duce the heterogeneity in variance; see Vasishth, Chen, Li,
and Guo (2013) for analyses of the Gibson and Wu data us-              Subject relatives
ing a transformation.                                                                 yi j ∼psr · LogNormal(β + δ + ui + w j , σ2e0 )
                                                                                           +(1 − psr ) · LogNormal(β + ui + w j , σ2e )
   One might think that if subject and object relatives are gen-                                                                        (2)
                                                                        Object relatives
erated by LogNormal distributions with different means, then
modelling the data as being generated by LogNormals would                             yi j ∼por · LogNormal(β + δ + ui + w j , σ2e0 )
adequately explain the data. Table 2 shows that if we assume                               +(1 − por ) · LogNormal(β + ui + w j , σ2e )
such a model, there is no longer a statistically significant ob-
ject relative advantage: the absolute t-value for the estimate of     Here, the terms ui and w j have the same interpretation as in
the β1 parameter is smaller than the critical value of 2 (Bates,      equation 1.
Maechler, Bolker, & Walker, 2015). Thus, assuming that the
data are generated by LogNormal distributions with different          Model comparison
means for the subject and object relatives leads to the conclu-       Bayesian model comparison can be carried out using differ-
sion that there isn’t much evidence for the distance account.         ent methods. Here, we use Bayesian k-fold cross-validation
                                                                      as discussed in Vehtari, Gelman, and Gabry (2016). This
                                                                      method evaluates the predictive performance of alternative
                                                                      models, and models with different numbers of parameters can
                   Estimate    Std. Error     t value                 be compared (Vehtari, Ojanen, et al., 2012; Gelman, Hwang,
              βˆ 0      6.06         0.07    92.64*                   & Vehtari, 2014).
              βˆ 1     -0.07         0.04       -1.61                   The k-fold cross-validation algorithm is as follows:
Table 2: A linear mixed model using log reading times in mil-        1. Split data pseudo-randomly into K held-out sets y(k) , where
liseconds as dependent variable in the Gibson and Wu, 2013,             k = 1, . . . , K that are a fraction of the original data, and K
data.                                                                   training sets, y(−k) . Here, we use K = 10, and the length of
                                                                        the held-out data-vector y(k) is approximately 1/K-th the
                                                                        size of the full data-set. We ensure that each participant’s
                                                                        data appears in the training set and contains an approxi-
                                                                        mately balanced number of data points for each condition.
   Consider next the possibility that the heteroscedasticity in
subject and object relatives in the Gibson and Wu data reflects      2. Sample from the model using each of the K training sets,
a systematic difference in the underlying generative processes          and obtain posterior distributions ppost(-k) (θ) = p(θ |
of reading times in the two relative clause types. We investi-
                                                                        y(−k) ), where θ is the vector of model parameters.
gate this question by modelling the extreme values as being
generated from a mixture distribution.                               3. Each posterior distribution p(θ | y(−k) ) is used to compute
                                                                        predictive accuracy for each held-out data-point yi :
   Using the probabilistic programming language Stan (Stan
Development Team, 2016), we show that a hierarchical mix-                                                Z
ture model provides a better fit to the data (in terms of predic-           log p(yi | y(−k) ) = log        p(yi | θ)p(θ | y(−k) ) dθ   (3)
tive accuracy) than several simpler hierarchical models. As
Nicenboim and Vasishth (2017) pointed out, the underlying
generative process implied by a mixture model is consistent          4. Given that the posterior distribution p(θ | y(−k) ) is summa-
with the direct-access model of McElree, Foraker, and Dyer              rized by s = 1, . . . , S simulations, i.e., θk,s , log predictive
(2003). We therefore suggest that, at least for the Chinese             density for each data point yi in subset k is computed as
relative clause data considered here, the direct-access model
may be a better way to characterize the dependency resolution                                                               !
                                                                                                           1 S          k,s
process than the distance account.                                                           pd i = log
                                                                                           eld               ∑ p(yi | θ )               (4)
                                                                                                           S s=1
   We can implement the direct-access model as a hierarchi-
                                                                     5. Given that all the held-out data in the K subsets are yi ,
cal mixture model with retrieval time assumed to be generated
from one of two distributions, where the proportion of trials           where i = 1, . . . , n, we obtain the eld   pd for all the held-out
in which a retrieval failure occurs (the mixing proportion) is          data points by summing up the eld        pd i :
psr in subject relatives, and por in object relatives. The ex-
                                                                                                              n
pectation here is the extreme values that are seen in subject
                                                                                                   eldpd = ∑ eld  pd i                  (5)
relatives are due to psr being larger than por .                                                            i=1
                                                                  1280

   The difference between the eld          pd’s of two competing mod-     The answer to both questions was “yes”. This raises our con-
els is a measure of relative predictive performance. We can               fidence that the models can identify the underlying parame-
also compute the standard deviation of the sampling distribu-             ters with real data. The fake-data simulation also showed that
tion (the standard error) of the difference in eld        pd using the    when the true underlying generative process was consistent
formula discussed in Vehtari et al. (2016). Letting ELPD     \ be         with the distance account but not the direct access model, the
the vector el pd 1 , . . . , el pd n , we can write:
             d                d                                           hierarchical linear model and the mixture model had com-
                                                                          parable predictive performance. In other words, the mixture
                                                                          model furnished a superior fit only when the true underlying
                                             q
            se(eldpd m0 − eld     pd m1 ) = nVar(ELPD)\            (6)    generative process for the data was in fact a mixture process.
   When we compare the model (1) with (2), if (2) has a                   Further details are omitted here due to lack of space.
higher eldpd, then it has a better predictive performance com-            The original Gibson and Wu study The estimates from
pared to (1).                                                             the hierarchical linear model (equation 1) and the mixture
   The quantity eld    pd is a Bayesian alternative to the Akaike         model (equation 2) are shown in Tables 3 and 4. Note that in
Information Criterion (Akaike, 1974). Note that the relative              Bayesian modelling we are not interested in “statistical sig-
complexity of the models to be compared is not relevant: the              nificance” here; rather, the goal is inference and comparing
sole criterion here is out-of-sample predictive performance.              predictive performance of two competing models.
As we discuss below (Results section), increasing complexity
will not automatically lead to better predictive performance.                                            mean    lower     upper
See Vehtari et al. (2012); Gelman, Hwang, and Vehtari (2014)                                  βˆ 1        6.06     5.91     6.20
for further details.3                                                                         βˆ 2       -0.07    -0.16     0.02
                                                                                              σ̂e         0.52     0.49     0.55
The data
                                                                                              σ̂u         0.25     0.18     0.34
The evaluation of these models was carried out using two sep-                                σ̂w          0.20     0.12     0.33
arate data-sets. The first was the original study from Gibson
and Wu (2013) that was discussed in the introduction. The                 Table 3: Posterior parameter estimates from the hierarchical
second study was a replication of the Gibson and Wu study                 linear model (equation 1) corresponding to the distance ac-
that was published in Vasishth et al. (2013). This second                 count. The data are from Gibson and Wu, 2013. Shown are
study served the purpose of validating whether independent                the mean and 95% credible intervals for each parameter.
evidence can be found for the mixture model selected using
the original Gibson and Wu data.
                                                                                                            mean     lower    upper
Results
                                                                                                    βˆ 0     5.85      5.76    5.95
In the models presented below, the dependent variable is read-                                        δˆ     0.93      0.73    1.14
ing time in milliseconds. Priors are defined for the model pa-                           p̂sr − p̂or         0.04     -0.04    0.13
rameters as follows. All standard deviations are constrained                                       p̂sr      0.25      0.17    0.34
to be greater than 0 and have priors Cauchy(0, 2.5) (Gelman,                                       p̂or      0.21      0.14    0.29
Carlin, et al., 2014); probabilities have priors Beta(1, 1); and                                   σ̂e0      0.64      0.54    0.74
all coefficients (β parameters) have priors Cauchy(0, 2.5).                                         σ̂e      0.22      0.20    0.25
                                                                                                    σ̂u      0.24      0.18    0.31
Fake-data simulation for validating model Before evalu-                                            σ̂w       0.09      0.05    0.16
ating relative model fit, we first simulated data from a mixture
distribution with known parameter values, and then sampled                Table 4: Posterior parameter estimates from the hierarchi-
from the models representing the distance account and the                 cal mixture model (equation 2) corresponding to the direct-
direct-access model. The goal of fake-data simulation was to              access model. The data are from Gibson and Wu, 2013.
validate the models and the model comparison method: with                 Shown are the mean and 95% credible intervals for each pa-
reference to the simulated data, we asked (a) whether the 95%             rameter.
credible intervals of the posterior distributions of the param-
eters in the mixture model contain the true parameter values                 Table 4 shows that the mean difference between the prob-
used to generate the data; and (b) whether k-fold cross valida-           ability psr and por is 4%; the posterior probability of this
tion can identify the mixture model as the correct one when               difference being greater than zero is 82%. K-fold cross-
the underlying generative process matches the mixture model.              validation shows that eld          pd for the hierarchical model is
                                                                          −3761 (SE: 38) and for the mixture model is −3614 (35).
    3 We also used a simpler method than k-fold cross-validation          The difference between the two eld             pds is 148 (18). The
to compare the models; this method is described in Vehtari et al.
(2016). The results are the same regardless of the model compari-         larger eldpd in the hierarchical mixture model suggests that
son method used.                                                          it has better predictive performance than the hierarchical lin-
                                                                      1281

ear model. In other words, the direct-access model has better                                     mean    lower    upper
predictive performance than the distance model.                                           βˆ 0     5.86     5.78    5.95
The replication of the Gibson and Wu study This data-                                       δˆ     0.75     0.56    0.97
set, originally reported by Vasishth et al. (2013), had 40 par-                   p̂sr − p̂or      0.07    -0.01    0.15
ticipants and the same 15 items as in Gibson and Wu’s data.                              p̂sr      0.23     0.15    0.33
Figure 2 shows the distribution of the data by condition; there                          p̂or      0.16     0.09    0.25
seems to a similar skew as in the original study, although the                           σ̂e0      0.69     0.59    0.81
spread is not as dramatic as in the original study.                                       σ̂e      0.21     0.18    0.23
                                                                                          σ̂u      0.22     0.17    0.29
                                                                                         σ̂w       0.07     0.04    0.12
                     10000                                         Table 6: Posterior parameter estimates from the hierarchical
                                                                   linear model (equation 2) corresponding to the direct-access
 reading time (ms)
                                                                   model. The data are from the replication of Gibson and Wu,
                     7500                                          2013 reported in Vasishth et al., 2013. Shown are the mean
                                                                   and 95% credible intervals for each parameter.
                     5000
                                                                   Discussion
                     2500                                          The model comparison and parameter estimates presented
                                                                   above suggest that, at least as far as the Chinese relative
                                                                   clause data are concerned, a better way to characterize the de-
                         0                                         pendency completion process is in terms of the direct-access
                                     obj−ext       subj−ext        model and not the distance account implied by Gibson and
                                           Condition               Wu (2013) and Lewis and Vasishth (2005). Specifically, there
                                                                   is suggestive evidence in the Gibson and Wu (2013) data that
                                                                   a higher proportion of retrieval failures occurred in subject
Figure 2: Boxplots showing the distribution of reading times       relatives compared to object relatives. In other words, in-
by condition of the replication of the Gibson and Wu data.         creased dependency distance may have the effect that it in-
                                                                   creases the proportion of retrieval failures (followed by re-
                                                                   analysis).4
   Tables 5 and 6 show the estimates of the posterior distri-
                                                                      There is one potential objection to the conclusion above.
butions from the two models. Table 4 shows that the mean
                                                                   It would be important to obtain independent evidence as
difference between the probability psr and por is 7%; the pos-
                                                                   to which dependency was eventually created in each trial.
terior probability of this difference being greater than zero is
                                                                   This could be achieved by asking participants multiple-choice
96%.
                                                                   questions to find out which dependency they built in each
   The eldpd for the hierarchical model is −3959 (53), and for     trial. Although such data is not available for the present study,
the hierarchical mixture model, −3801 (38). The difference         in other work (on number interference) (Nicenboim, Engel-
in eld
     pd is 158 (29). Thus, in the replication data as well, the    mann, Suckow, & Vasishth, 2016) did collect this informa-
predictive performance of the mixture model is better than the     tion. There, too, we found that the direct-access model best
hierarchical linear model.                                         explains the data (Nicenboim & Vasishth, 2017). In future
                                                                   work on Chinese relatives, it would be helpful to carry out a
                                     mean    lower    upper        similar study to determine which dependency was completed
                                                                   in each trial. In the present work, the modelling at least shows
                              βˆ 0    6.00     5.88    6.12
                                                                   how the extreme values in subject relatives can be accounted
                              βˆ 1   -0.09    -0.16   -0.01
                                                                   for by assuming a two-mixture process.
                              σ̂e     0.44     0.41    0.47
                             σ̂u      0.25     0.19    0.33                                      Conclusion
                             σ̂w      0.16     0.10    0.26
                                                                   The mixture models suggest that, in the specific case of Chi-
Table 5: Posterior parameter estimates from the hierarchical       nese relative clauses, increased processing difficulty in sub-
linear model (equation 1) corresponding to the distance ac-        ject relatives is not due to dependency distance leading to
count. The data are from the replication of Gibson and Wu,         longer reading times, as suggested by Gibson and Wu (2013).
2013 reported in Vasishth et al., 2013. Shown are the mean            4A  reviewer suggests that the direct-access model may simply
and 95% credible intervals for each parameter.                     be an elaboration of the distance model. This is by definition not the
                                                                   case: direct access (i.e., distance-independent access) is incompati-
                                                                   ble with the distance account.
                                                               1282

Rather, a more plausible explanation for these data is in terms       Lewis, R. L., & Vasishth, S. (2005). An activation-based
of the direct-access model of McElree et al. (2003). Under              model of sentence processing as skilled memory retrieval.
this view, retrieval times are not affected by the distance be-         Cognitive Science, 29, 1–45.
tween co-dependents, but a higher proportion of retrieval fail-       McElree, B., Foraker, S., & Dyer, L. (2003). Memory struc-
ures occur in subject relatives compared to object relatives.           tures that subserve sentence comprehension. Journal of
This leads to a mixture distribution in both subject and ob-            Memory and Language, 48, 67–91.
ject relatives, but the proportion of the failure distribution is     Nairne, J. S. (1990). A feature model of immediate memory.
higher in subject relatives.                                            Memory & Cognition, 18(3), 251–269.
   In conclusion, this paper serves as a case study demon-            Nicenboim, B., Engelmann, F., Suckow, K., & Vasishth, S.
strating the flexibility of Bayesian cognitive modelling using          (2016). Number interference in German: Evidence for cue-
finite mixture models. This kind of modelling approach can              based retrieval. Retrieved from https://osf.io/mmr7s/
be used flexibly in many different research problems in cog-            (submitted to Cognitive Science)
nitive science. One example is the above-mentioned work by            Nicenboim, B., & Vasishth, S. (2017). Models of retrieval
Nicenboim and Vasishth (2017). Another example, also from               in sentence comprehension: A computational evaluation
sentence comprehension, is the evidence for feature overwrit-           using Bayesian hierarchical modeling. Retrieved from
ing (Nairne, 1990) in parsing (Vasishth, Jäger, & Nicenboim,           https://arxiv.org/abs/1612.04174 (Under revision
2017).                                                                  following review, Journal of Memory and Language)
                                                                      Plummer, M. (2012). JAGS version 3.3.0 manual. Interna-
                     Acknowledgments                                    tional Agency for Research on Cancer. Lyon, France.
We are very grateful to Ted Gibson for generously provid-             Safavi, M. S., Husain, S., & Vasishth, S. (2016). Depen-
ing the raw data and the experimental items from Gibson and             dency resolution difficulty increases with distance in Per-
Wu (2013). Thanks also go to Lena Jäger for many insight-              sian separable complex predicates: Implications for expec-
ful comments. Helpful observations by Aki Vehtari are also              tation and memory-based accounts. Frontiers in Psychol-
gratefully acknowledged. For partial support of this research,          ogy, 7. doi: 10.3389/fpsyg.2016.00403
we thank the Volkswagen Foundation through grant 89 953.              Stan Development Team.            (2016).    Stan modeling
                                                                        language users guide and reference manual, version
                          References                                    2.12 [Computer software manual].           Retrieved from
Akaike, H. (1974). A new look at the statistical model iden-            http://mc-stan.org/
   tification. IEEE transactions on automatic control, 19(6),         Vasishth, S., Chen, Z., Li, Q., & Guo, G. (2013, 10). Pro-
   716–723.                                                             cessing Chinese relative clauses: Evidence for the subject-
Bates, D., Maechler, M., Bolker, B., & Walker, S. (2015).               relative advantage. PLoS ONE, 8(10), 1–14.
   Fitting linear mixed-effects models using lme4. Journal of         Vasishth, S., Jäger, L. A., & Nicenboim, B. (2017).
   Statistical Software, 67, 1–48. doi: 10.18637/jss.v067.i01           Feature overwriting as a finite mixture process: Ev-
Box, G. E., & Cox, D. R. (1964). An analysis of transfor-               idence from comprehension data. In Proceedings of
   mations. Journal of the Royal Statistical Society. Series B          MathPsych/ICCM.         Warwick, UK.       Retrieved from
   (Methodological), 211–252.                                           https://arxiv.org/abs/1703.04081
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Ve-           Vehtari, A., Gelman, A., & Gabry, J. (2016). Practi-
   htari, A., & Rubin, D. B. (2014). Bayesian data analysis             cal Bayesian model evaluation using leave-one-out cross-
   (Third ed.). Chapman and Hall/CRC.                                   validation and WAIC. Statistics and Computing.
Gelman, A., Hwang, J., & Vehtari, A. (2014). Understanding            Vehtari, A., Ojanen, J., et al. (2012). A survey of bayesian
   predictive information criteria for bayesian models. Statis-         predictive methods for model assessment, selection and
   tics and Computing, 24(6), 997–1016.                                 comparison. Statistics Surveys, 6, 142–228.
Gibson, E. (2000). Dependency locality theory: A distance-
   based theory of linguistic complexity. In A. Marantz,
   Y. Miyashita, & W. O’Neil (Eds.), Image, language, brain:
   Papers from the first mind articulation project symposium.
   Cambridge, MA: MIT Press.
Gibson, E., & Wu, H.-H. I. (2013). Processing Chinese rela-
   tive clauses in context. Language and Cognitive Processes,
   28(1-2), 125–155.
Just, M. A., & Carpenter, P. A. (1992). A capacity theory of
   comprehension: Individual differences in working mem-
   ory. Psychological Review, 99(1), 122–149.
Lee, M. D., & Wagenmakers, E.-J. (2014). Bayesian cogni-
   tive modeling: A practical course. Cambridge University
   Press.
                                                                  1283

