     Assessing the Linguistic Productivity of Unsupervised Deep Neural Networks
                                        Lawrence Phillips (Lawrence.Phillips@pnnl.gov)
                                                   Pacific Northwest National Laboratory
                                             Nathan Hodas (Nathan.Hodas@pnnl.gov)
                                                   Pacific Northwest National Laboratory
                              Abstract                                    2014). The success of a domain general learner does not nec-
                                                                          essarily imply that human learners acquire the phenomenon
   Increasingly, cognitive scientists have demonstrated interest in       in a similar fashion, but it does open the possibility that we
   applying tools from deep learning. One use for deep learning is
   in language acquisition where it is useful to know if a linguistic     need not posit innate, domain-specific knowledge.
   phenomenon can be learned through domain-general means.                   The crux of these learning problems typically lies in mak-
   To assess whether unsupervised deep learning is appropriate,           ing a particular generalization which goes beyond the input
   we first pose a smaller question: Can unsupervised neural net-
   works apply linguistic rules productively, using them in novel         data. One major type of generalization that DL models would
   situations? We draw from the literature on determiner/noun             need to capture is known as linguistic productivity. A gram-
   productivity by training an unsupervised, autoencoder network          matical rule is considered productive when it can be applied
   measuring its ability to combine nouns with determiners. Our
   simple autoencoder creates combinations it has not previously          in novel situations. For example, as a speaker of English you
   encountered and produces a degree of overlap matching adults.          may never have encountered the phrase a gavagai before, but
   While this preliminary work does not provide conclusive evi-           you now know that gavagai must be a noun and can therefore
   dence for productivity, it warrants further investigation with
   more complex models. Further, this work helps lay the foun-            combine with other determiners to produce a phrase such as
   dations for future collaboration between the deep learning and         the gavagai. Before DL might be applied to larger questions
   cognitive science communities.                                         within language acquisition, the issue of productivity must
   Keywords: Deep Learning; Language Acquisition; Linguistic              first be addressed. If DL models are not capable of produc-
   Productivity; Unsupervised Learning; Determiners
                                                                          tivity, then they cannot possibly serve to model the cognitive
                                                                          process of language acquisition. On the other hand, if DL
                          Introduction                                    models demonstrate basic linguistic productivity, we must ex-
Computational modeling has long played a significant role                 plore what aspects of the models allow for this productivity.
within cognitive science, allowing researchers to explore
the implications of cognitive theories and to discover what
                                                                          The Special Case of Determiners
properties are necessary to account for particular phenom-                For decades, debate has raged regarding the status of produc-
ena (J. L. McClelland, 2009). Over time, a variety of mod-                tive rules among children acquiring their native language. On
eling traditions have seen their usage rise and fall. While the           the one hand, some have argued that children seem hardwired
1980s saw the rise in popularity of connectionism (Thomas &               to apply rules productively and demonstrate this in their ear-
McClelland, 2008), more recently symbolic Bayesian mod-                   liest speech (Valian, Solt, & Stewart, 2009; C. Yang, 2011).
els have risen to prominence (Chater & Oaksford, 2008; Lee,               On the other, researchers have argued that productivity ap-
2011). While the goals of cognitive modelers have largely                 pears to be learned, with children’s early speech either lack-
remained the same, increases in computational power and ar-               ing productivity entirely or increasing with age (Pine & Mar-
chitectures have played a role in these shifts (J. L. McClel-             tindale, 1996; Pine, Freudenthal, Krajewski, & Gobet, 2013;
land, 2009). Following this pattern, recent advances in the               Meylan, Frank, Roy, & Levy, 2017). Of particular interest
area of deep learning (DL) have led to a rise in interest from            to this debate has been the special case of English determin-
the cognitive science community as demonstrated by a num-                 ers. In question is whether or not English-learning children
ber of recent workshops dedicated to DL (Saxe, 2014; J. Mc-               have acquired the specific linguistic rule which allows them
Clelland, Hansen, & Saxe, 2016; J. McClelland, Frank, &                   to create a noun phrase (NP) from a determiner (DET) and
Mirman, 2016).                                                            noun (N) or if they have simply memorized the combinations
   As with any modeling technique, DL can be thought of                   that they have previously encountered. This linguistic rule,
as a tool which is best suited to answering particular types              NP → DET N, is productive in two senses. First, it can be
of questions. One such question is that of learnability,                  applied to novel nouns, e.g. a gavagai. Second, consider the
whether an output behavior could ever be learned from the                 determiners a and the. If a singular noun can combine with
types of input given to a learner. These types of ques-                   one of these determiners, it may also combine with the other,
tions play an integral role in the field of language acquisi-             e.g. the wug.
tion where researchers have argued over whether particular                   This type of rule seems to be acquired quite early in acqui-
aspects of language could ever be learned by a child with-                sition, making it appropriate to questions of early productiv-
out the use of innate, language-specific mechanisms (Smith,               ity, and provides an easy benchmark for a DL model. Yet
1999; C. D. Yang, 2004; Chater & Christiansen, 2010; Pearl,               answering such a simple question first requires addressing
                                                                      937

how one might measure productivity. Most attempts to mea-            most prominently images, we describe their use here primar-
sure productivity have relied on what is known as an overlap         ily for text. The first half, the encoder, takes in sentences
score, intuitively what percentage of nouns occur with both a        and transforms them into a condensed representation. This
and the (C. Yang, 2011). This simple measure has been the            condensed representation is small enough that the neural net-
source of some controversy. C. Yang (2011) argues that early         work cannot simply memorize each sentence and instead is
attempts failed to take into account the way in which word           forced to encode only the aspects of the sentence it believes
frequencies affect the chance for a word to “overlap”. Be-           to be most important. The second half, the decoder, learns to
cause word frequency follows a Zipfian distribution, with a          take this condensed representation and transform it back into
long tail of many infrequent words, many nouns are unlikely          the original sentence. Backpropogation is used to train model
to ever appear with both determiners. He proposes a method           weights to reduce the loss between the original input and the
to calculate an expected level of overlap which takes into ac-       reconstructed output. Although backpropagation is more typ-
count these facts. Alternatively, Meylan et al. (2017) propose       ically applied to supervised learning problems, the process is
a Bayesian measure of productivity which they claim takes            in fact unsupervised because the model is only given input
into account the fact that certain nouns tend to prefer one de-      examples and is given no external feedback.
terminer over another. For instance, while one is more likely           AEs have been shown to successfully capture text repre-
to hear a bath than the phrase the bath, the opposite is true of     sentations in areas such as paragraph generation (Li, Luong,
the noun bathroom which shows a preference for the deter-            & Jurafsky, 2015), part-of-speech induction (Vishnubhotla,
miner the (Meylan et al., 2017).                                     Fernandez, & Ramabhadran, 2010), bilingual word represen-
   The literature is quite mixed regarding whether or not chil-      tations (Chandar et al., 2014), and sentiment analysis (Socher,
dren show early productivity. Differences in pre-processing          Pennington, Huang, Ng, & Manning, 2011), but have not
have lead researchers to draw opposite conclusions from sim-         been applied to modeling language acquisition. While any
ilar data, making interpretation quite difficult (C. Yang, 2011;     number of DL architectures could be used to model language
Pine et al., 2013). Indeed, most corpora involving individual        acquisition, the differences between ANNs and actual neu-
children are small enough that Meylan et al. (2017) argue it is      rons in the brain make any algorithmic claims difficult. In-
impossible to make a statistically significant claim as to child     stead, DL models might be used to address computational-
productivity. For analyzing whether or not text generated by         level questions, for instance regarding whether or not a piece
a DL model is productive or not, we thankfully do not need           of knowledge is learnable from the data encountered by chil-
to fully address the problem of inferring child productivity.        dren. Before this can be done, however, it remains to be seen
Ideally, the model would demonstrate a similar level of over-        whether DL models are even capable of creating productive
lap to the data it was exposed to. We make use of the overlap        representations. If they cannot, then they do not represent
statistic from Yang because it is more easily comparable to          useful models of language acquisition. This work attempts to
other works and has been better studied than the more recent         address this not by creating a model of how children acquire
Bayesian metric of Meylan et al. (2017).                             language, but by using methods from the psychological liter-
                                                                     ature on productivity to assess the capability of DL to learn
Deep Learning for Language Acquisition                               productive rules.
Deep learning, or deep neural networks, are an extension of
traditional artificial neural networks (ANN) used in connec-                                    Methods
tionist architectures. A “shallow” ANN is one that posits a
single hidden layer of neurons between the input and out-
                                                                     Corpora
put layers. Deep networks incorporate multiple hidden lay-           To train our neural network, we make use of child-directed
ers allowing these networks in practice to learn more com-           speech taken from multiple American-English corpora in
plex functions. The model parameters can be trained through          the CHILDES database (MacWhinney, 2000). In particu-
the use of the backpropogation algorithm. The addition of            lar, we make use of the CDS utterances in the Bloom 1970,
multiple hidden layers opens up quite a number of possible           Brent, Brown, Kuczaj, Providence, Sachs, and Suppes cor-
architectures, not all of which are necessarily applicable to        pora (Bloom, 1970; Brent & Siskind, 2001; Brown, 1973;
problems in cognitive science or language acquisition more           Kuczaj, 1977; Demuth & McCullough, 2009; Sachs, 1983;
specifically.                                                        Suppes, 1974). The combined corpora contain almost 1 mil-
   While the most common neural networks are discrimina-             lion utterances and span a wide age range, including speech
tive, i.e. categorizing data into specific classes, a variety of     directed to children as young as 6 months and as old as 5
techniques have been proposed to allow for truly generative          years. Relevant information about the used corpora can be
neural networks. These generative networks are able to take          found in Table 1.
in input data and generate complex outputs such as images or            Because we are interested in seeing what the AE can learn
text which makes them ideal for modeling human behavior.             from data similar to that encountered by children, we train
We focus on one generative architecture in particular known          the model only on child-directed utterances. These can be
as a deep autoencoder (AE) (Hinton & Salakhutdinov, 2006).           produced by any adult in the dataset, including parents and
   While AEs have been used for a variety of input data types,       researchers. Although a comparison with child-produced text
                                                                 938

  yT     yT-1          y1         Dense Softmax Layer               an inefficient representation because it assumes all words are
                                                                    equally similar, e.g. that dog is equally similar to dogs as
                                   GRU Layer (20 dim)               it is to truck. To deal with this, the model passes the one-
                                                                    hot vector to an embedding layer. Neural word embeddings,
                             Latent Representation (20 dim)         as popularized by the word2vec algorithm (Mikolov, Chen,
                                                                    Corrado, & Dean, 2013), are a way to represent words in
                                   GRU Layer (20 dim)               a low-dimensional space without requiring outside supervi-
                                                                    sion. Words are placed within the space such that words that
                               Embedding Layer (30 dim)             are predictive of neighboring words are placed closer to one
  X1     X2            XT
                                                                    another. Because our training data is relatively small, we keep
                                                                    the embedding dimensionality low, at only 30. Standard em-
                                                                    beddings trained on much larger NLP corpora tend to use 100
  Figure 1: Visual representation of the autoencoder model.
                                                                    or 200 dimensions.
                                                                       Once each word has been transformed into a 30-
holds great interest, it is not clear whether child-produced        dimensional embedding vector, the sequence of words is
speech is rich enough to support robust language learning on        passed into a gated-recurrent unit (GRU) layer (Cho et al.,
its own. It therefore provides a poor basis upon which to train     2014). The GRU is a type of recurrent (RNN) layer which
the AE.                                                             we choose because it can be more easily trained. RNN lay-
   Text from the various corpora is processed as a single docu-     ers read in their inputs sequentially and make use of hidden
ment. Child-directed utterances are cleaned from the raw files      “memory” units that pass information about previous inputs
using the CHILDESCorpusReader function of the Python                to later inputs, making them ideal for sequence tasks such as
Natural Language Toolkit (NLTK). Utterances from all non-           language. As such, the model creates a representation of the
children speakers are included and not limited just to the pri-     sentence which it passes from word to word. The final repre-
mary caregiver. Each utterance is split into words according        sentation is the output of the encoder, a latent representation
to the available CHILDES transcription and then made low-           of the full sentence.
ercase. The model represents only the most frequent 3000               This 20-dimensional latent vector serves as the input to the
words, while the remainder are represented as a single out-         decoder unit. The first layer of the decoder is a GRU layer of
of-vocabulary (OOV) token. This step is taken both to re-           the same shape as in the encoder. For each timestep, we feed
duce computational complexity but also to mimic the fact that       into the GRU the latent vector, similar to the model proposed
young children are unlikely to store detailed representations       in Cho et al. (2014). Rather than producing a single output,
of all vocabulary items encountered. Because the neural net-        as in the encoder, the decoder’s GRU layer outputs a vector at
works require each input to be of the same length, sentences        each timestep. Each of these vectors is fed into a shared dense
are padded to a maximum length of 10 words. Sentences that          softmax layer which produces a probability distribution over
are longer than this are truncated, while short sentences are       vocabulary items. The model then outputs the most likely
prepended with a special PAD token.                                 word for each timestep.
                                                                       The model loss is calculated based on the model’s ability to
         Corpora          Age Range     N. Utterances               reconstruct the original sentence through categorical crossen-
         Bloom 1970         1;9 - 3;2           62,756              tropy. Model weights are trained using the Adam optimzer
         Brent              0;6 - 1;0          142,639              over 10 epochs. During each epoch the model sees the entire
         Brown              1;6 - 5;1          176,856              training corpus, updating its weights after seeing a batch of 64
         Kuczaj             2;4 - 4;1           57,719              utterances. While this process does not reflect that used by a
         Providence         1;0 - 3;0          394,800              child learner, it is a necessary component of training the neu-
         Sachs              1;1 - 5;1           28,200              ral network on such a small amount of data. If the network
         Suppes            1;11 - 3;3           67,614              had access to the full set of speech that a child encounters
         Overall            0;6 - 5;1          930,584              such a measure likely would not be necessary. Future work
                                                                    might also investigate whether optimizing the dimensionality
Table 1: Descriptive statistics of CHILDES corpora. Ages            of the network might lead to better text generation with higher
are given in (year;month) format and indicate the age of the        levels of productivity.
child during corpus collection.
                                                                    Baseline Models
                                                                    Because the AE is learning to reproduce its input data, one
Neural Network Architecture                                         might wonder whether similar results might be achieved by a
Our autoencoder model was implemented using Keras and               simpler, distributional model. To assess this, we also mea-
Tensorflow. The words in each sentence are input to the             sure the performance of an n-gram language model. We
model as a one-hot vector, a vector of 0s with a single 1 whose     train bigram and trigram language models using the modi-
placement indicates the presence of a particular word. This is      fied Kneser-Ney smoothing (Heafield, Pouzyrevsky, Clark,
                                                                939

& Koehn, 2013) implemented in the KenLM model toolkit                 and Zipfian parameter a represents an improvement upon the
to estimate the distributional statistics of the training corpus.     original measure.
Sentences are generated from the n-gram language model by
picking a seed word and then sampling a new word from the                                         Results
set of possible n-grams. The smoothing process allows for
                                                                      We analyze our overlap measures for the adult-generated (i.e.
the model to generate previously unseen n-grams. Sampling
                                                                      child-directed) as well as the autoencoder and n-gram model-
of new words continues for each utterance until the end-of-
                                                                      generated text and present these results in Figure 2. We ana-
sentence token is generated or a maximum of 10 tokens is
                                                                      lyze overlap scores across 10 training epochs with three lev-
reached (the same maximum size as for the AE).
                                                                      els of dropout, 10%, 20%, and 30%. Dropout is typically
   Since the AE is able to generate sentences from a latent
                                                                      included in neural models to encourage the model to better
representation, it would be inappropriate to generate n-gram
                                                                      generalize. We hypothesized that a certain level of dropout
sentences from random seed words. Instead, for every sen-
                                                                      would encourage the model to generate novel combinations
tence in the test set we begin the n-gram model with the first
                                                                      of words that might lead to higher overlap scores. We find
word of the utterance. While this allows the model to always
                                                                      that with only two training epochs the AEs have already be-
generate its first token correctly, this does not directly impact
                                                                      gun to near their maximum overlap performance. The 30%
our measure of productivity as it relies on combinations of
                                                                      dropout AE achieves the highest level of performance, match-
tokens.
                                                                      ing the empirical overlap score of the original corpus. The
Productivity Measures                                                 10% and 20% dropout models perform somewhat worse sug-
We measure the productivity of our autoencoders through the           gesting that high levels of dropout may be necessary for good
overlap score described in C. Yang (2011). Words both in              text generation.
the child-directed corpus and the autoencoder-generated out-             In Table 2, we present the results for the final epoch of
put are tagged using the default part-of-speech tagger from           the AE models as well as for the adult-generated and n-
NLTK. The empirical overlap scores are simply calculated              gram generated text. We note that the expected overlap mea-
as a percentage of unique nouns that appear immediately af-           sure consistently overestimates the productivity of all learn-
ter both the determiners a and the. The expected overlap              ers, including the adult-generated text. It is unclear why
score is calculated based off of three numbers from the cor-          this should be the case, but could be a result of capping the
pus under consideration, the number of unique nouns N, the            model vocabularies, resulting in lower N values. In particu-
number of unique determiners D, and the total number of               lar, the autoencoders tend to produce a relatively limited set
noun/determiner pairs S. The expected overlap is defined as           of nouns. Looking at empirical overlap measures, the worst-
in Equation 1:                                                        performing models are the bigram and trigram models with
                                                                      overlap scores below 30%. The AEs fair much better all pro-
                               1 N                                    ducing overlap scores over 50%. The 30% dropout AE is
                O(N, D, S) =      ∑ O(r, N, D, S)             (1)     actually able to match the overlap score of the original adult-
                               N r=1
                                                                      generated corpus (59.4% vs. 59.3%).
   where O(r, N, D, S) is the expected overlap of the noun at            Looking at the number of unique nouns following a de-
frequency rank r:                                                     terminer (N) and the total number of determiner-noun pairs
                                                                      (S), it becomes clear there are large differences between the
                                             D                        n-gram and AE models. The n-gram models tend to pro-
 O(r, N, D, S) = 1 + (D − 1)(1 − pr )S − ∑ [(di pr + 1 − pr )S ]      duce very few determiner-noun pairs (low S) but are likely to
                                            i=1                       choose from any of the nouns in the corpus, leading to high
                                                              (2)     N. This fact accounts for the low overlap scores that they
   di represents the probability of encountering determiner i,        achieve. In contrast, the AEs follow a pattern which mir-
for which we use the relative frequencies of a and the cal-           rors the adult corpus with few unique nouns but a large num-
culated from the training corpus (39.3% and 60.7%, respec-            ber of noun-determiner pairs. In all cases, however, the AEs
tively). The probability pr represents the probability assigned       produce both fewer unique nouns and fewer noun-determiner
to a particular word rank. The Zipfian distribution takes             pairs than the original corpus.
a shape parameter, a which C. Yang (2011) set equal to 1
                                                                         One possible problem for calculating the expected over-
and which we optimize over the training corpus using least
                                                                      laps comes from the difficulty of part-of-speech tagging text
squares estimation and set at 1.06:
                                                                      generated by the neural network. Whereas adult-generated
                                  1/ a                                speech follows set patterns that machine taggers are built to
                                     r
                         pr =                                 (3)     recognize, the neural network does not necessarily generate
                               ∑n=1 ( n1a )
                                 N
                                                                      well-formed language. Examples of AE-generated text can
   It should be noted that Zipfian distributions are not perfect      be found in Table 3. In some cases, the tagger treats items
models of word frequencies (Piantadosi, 2014), but assigning          that occur after a determiner as a noun regardless of its typ-
empirically-motivated values to the determiner probabilities          ical usage. For example, in the generated sentence let put
                                                                  940

                                                                    dimensional latent space (Hinton & Salakhutdinov, 2006). If
                                                                    the brain likewise attempts to find efficient representations of
                                                                    the stimuli it encounters then it may prove fruitful to investi-
                                                                    gate how these representations compare to one another.
                                                                          Adult                          Autoencoder
                                                                          falling down                   down down
                                                                          you’re playing with            you’re playing with
                                                                          your bus                       the head
                                                                          why did OOV say what’s         what what you say say
                                                                          wrong with these apples        say with the dada
Figure 2: Empirical overlap scores. Adult-generated speech          Table 3: Example adult and AE-generated language. The AE-
is marked by the solid black line while autoencoder-generated       generated text is from the final epoch of the AE with 20%
speech is marked by the dashed colored lines. Results are           dropout. In bold is a DET+N combination that does not ap-
presented for three levels of dropout, 10%, 20%, and 30%.           pear in the AEs input.
The x-axis represents the training epoch of the model.
                     N        S    Exp. Over.   Emp. Over.                                    Conclusion
  Adult         1,390    34,138        77.5%          59.3%         While there is great interest regarding the inclusion of deep
  AE 10%          861    29,497        88.4%          53.3%         learning methods into cognitive modeling, a number of ma-
  AE 20%          870    28,817        87.6%          53.4%         jor hurdles remain. For the area of language acquisition,
  AE 30%          816    31,181        90.8%          59.4%         deep learning is poised to help answer questions regarding
  Bigram        1,780     5,177        17.6%          28.6%         the learnability of complex linguistic phenomena without ac-
  Trigram       2,506     4,595        11.2%          22.1%         cess to innate, linguistic knowledge. Yet it remains unclear
                                                                    whether unsupervised versions of deep learning models are
Table 2: Expected and empirical overlap scores for adult-           capable of capturing even simple linguistic phenomena. In
and autoencoder-generated language with varying levels of           this preliminary study, we find that a simple autoencoder with
dropout. Expected overlap scores were calculated as in Yang         sufficient levels of dropout is able to mirror the productivity
(2011). Empirical overlap was calculated as the percent of          of its training data, although it is unclear whether this proves
unique nouns that appeared immediately following both a and         productivity in and of itself.
the.                                                                   Future work will need to investigate whether more com-
                                                                    plex models might be able to generate text with higher pro-
                                                                    ductivity as well as further investigating how particular model
put the over over here, the phrase the over is tagged as a
                                                                    choices impact performance. It would also be worthwhile to
DET+N pair. These type of errors are further evidenced by
                                                                    compare AEs against simpler models such as a basic LSTM
the fact that the trigram language model produces a larger set
                                                                    language model. While additional work needs to be done to
of words tagged as nouns than the original adult-generated
                                                                    motivate the use of deep learning models as representations of
corpus (2,506 vs. 1,390).
                                                                    how children might learn, this preliminary work shows how
    Another explanation for the difference between expected
                                                                    one might combine techniques from deep learning and devel-
and empirical overlaps may come from deviation from a true
                                                                    opmental psychology.
Zipfian distribution of word frequencies. If word frequencies
are Zipfian, we should expect a perfect correlation between                              Acknowledgments
log ranks and log counts. C. Yang (2011) report a correla-
tion of 0.97, while our larger corpus deviates from this with       The authors thank the reviewers for their thoughtful com-
r2 = 0.86. Although we attempt to take this into account by         ments and Lisa Pearl for initial discussion regarding produc-
fitting the Zipfian distribution’s shape parameter, this diver-     tivity.
gence clearly indicates that further work is needed.
    The success of the AE model in generating productive                                      References
text serves as a confirmation that unsupervised neural models       Bloom, L. (1970). Language development: Form and func-
might be used in future work to investigate other cognitive            tion in emerging grammars. MIT Press.
phenomena. This work does not directly address the ques-            Brent, M., & Siskind, J. (2001). The role of exposure to iso-
tion of how infants might learn to produce productive speech,          lated words in early vocabulary development. Cognition,
it does represent one possible approach. AEs can, for in-              81, 31–44.
stance, be thought of as information compression algorithms         Brown, R. (1973). A first language: The early stages. Har-
which learn to represent high-dimensional data into a low-             vard University Press.
                                                                941

Chandar, S., Lauly, S., Larochelle, H., Khapra, M., Ravin-         Piantadosi, S. (2014). Zipf’s word frequency law in natural
  dran, B., Raykar, V. C., & Saha, A. (2014). An autoen-             language: A critical review and future directions. Psycho-
  coder approach to learning bilingual word representations.         nomic Bulletin & Review, 21(5), 1112-1130.
  In Advances in Neural Information Processing Systems             Pine, J. M., Freudenthal, D., Krajewski, G., & Gobet, F.
  (pp. 1853–1861).                                                   (2013). Do young children have adult-like syntactic cat-
Chater, N., & Christiansen, M. H. (2010). Language acqui-            egories? Zipf’s law and the case of the determiner. Cogni-
  sition meets language evolution. Cognitive Science, 34(7),         tion, 127(3), 345–360.
  1131–1157.                                                       Pine, J. M., & Martindale, H. (1996). Syntactic categories in
Chater, N., & Oaksford, M. (2008). The probabilistic mind:           the speech of young children: The case of the determiner.
  Prospects for bayesian cognitive science. Oxford Univer-           Journal of Child Language, 23(02), 369–395.
  sity Press.                                                      Sachs, J. (1983). Talking about the there and then: The emer-
Cho, K., Van Merriënboer, B., Gulcehre, C., Bah-                    gence of displaced reference in parent-child discourse. In
  danau, D., Bougares, F., Schwenk, H., & Bengio, Y.                 K. Nelson (Ed.), Children’s language (Vol. 4). Lawrence
  (2014). Learning phrase representations using rnn encoder-         Erlbaum Associates.
  decoder for statistical machine translation. arXiv preprint      Saxe, A. (Ed.). (2014). Workshop on deep learning and the
  arXiv:1406.1078.                                                   brain.
Demuth, K., & McCullough, E. (2009). The prosodic                  Smith, L. B. (1999). Do infants possess innate knowledge
  (re)organization of children’s early english articles. Jour-       structures? The con side. Developmental Science, 2(2),
  nal of Child Language, 36, 173–200.                                133–144.
Heafield, K., Pouzyrevsky, I., Clark, J. H., & Koehn, P.           Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., & Man-
  (2013). Scalable modified kneser-ney language model es-            ning, C. D. (2011). Semi-supervised recursive autoen-
  timation. In Proceedings of the Association of Computa-            coders for predicting sentiment distributions. In Proceed-
  tional Linguistics conference (pp. 690–696).                       ings of the conference on empirical methods in natural lan-
Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing               guage processing (pp. 151–161).
  the dimensionality of data with neural networks. Science,        Suppes, P. (1974). The semantics of children’s language.
  313(5786), 504–507.                                                American Psychologist, 29, 103–114.
Kuczaj, S. (1977). The acquisition of regular and irregular        Thomas, M. S., & McClelland, J. L. (2008). Connectionist
  past tense forms. Journal of Verbal Learning and Verbal            models of cognition. Cambridge handbook of computa-
  Behavior, 16, 589–600.                                             tional cognitive modelling, 23–58.
                                                                   Valian, V., Solt, S., & Stewart, J. (2009). Abstract categories
Lee, M. D. (2011). How cognitive modeling can benefit
                                                                     or limited-scope formulae? The case of children’s deter-
  from hierarchical bayesian models. Journal of Mathemati-
                                                                     miners. Journal of Child Language, 36(04), 743–778.
  cal Psychology, 55(1), 1–7.
                                                                   Vishnubhotla, S., Fernandez, R., & Ramabhadran, B. (2010).
Li, J., Luong, M.-T., & Jurafsky, D. (2015). A hierarchical
                                                                     An autoencoder neural-network based low-dimensionality
  neural autoencoder for paragraphs and documents. arXiv
                                                                     approach to excitation modeling for HMM-based text-to-
  preprint arXiv:1506.01057.
                                                                     speech. In IEEE International Conference on Acoustics
MacWhinney, B. (2000). The CHILDES project: The
                                                                     Speech and Signal Processing (pp. 4614–4617).
  database (Vol. 2). Psychology Press.
                                                                   Yang, C. (2011). A statistical test for grammar. In Pro-
McClelland, J., Frank, S., & Mirman, D. (Eds.). (2016). Con-         ceedings of the 2nd workshop on Cognitive Modeling and
  temporary neural network models: Machine learning, arti-           Computational Linguistics (pp. 30–38).
  ficial intelligence, and cognition.                              Yang, C. D. (2004). Universal grammar, statistics or both?
McClelland, J., Hansen, S., & Saxe, A. (Eds.). (2016). Tuto-         Trends in Cognitive Sciences, 8(10), 451–456.
  rial workshop on contemporary deep neural network mod-
  els.
McClelland, J. L. (2009). The place of modeling in cognitive
  science. Topics in Cognitive Science, 1(1), 11–38.
Meylan, S. C., Frank, M. C., Roy, B. C., & Levy, R.
  (2017). The emergence of an abstract grammatical cat-
  egory in children’s early speech. Psychological Science,
  0956797616677753.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Ef-
  ficient estimation of word representations in vector space.
  arXiv preprint arXiv:1301.3781.
Pearl, L. (2014). Evaluating learning-strategy components:
  Being fair (commentary on Ambridge, Pine, and Lieven).
  Language, 90(3), e107–e114.
                                                               942

