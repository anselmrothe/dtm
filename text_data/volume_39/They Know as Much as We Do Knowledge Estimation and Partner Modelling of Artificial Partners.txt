     They Know as Much as We Do: Knowledge Estimation and Partner Modelling of
                                                        Artificial Partners
                                         Benjamin R. Cowan (benjamin.cowan@ucd.ie )
                              School of Information & Communication Studies, University College Dublin
                                                             Belfield, Dublin 4
                                            Holly Branigan (holly.branigan@ed.ac.uk)
                                           Department of Psychology, University of Edinburgh
                                                   7 George Square, Edinburgh, EH8 9JZ
                                             Habiba Begum (HXB368@student.bham.ac.uk)
                                                   HCI Centre, University of Birmingham
                                                        Edgbaston Campus, B15 2TT
                                            Lucy McKenna (lucy.mckenna@adaptcentre.ie)
                                             ADAPT Centre, Trinity College Dublin, Dublin 2
                                                       Eva Szekely (szekely@kth.se)
                                              KTH Royal Institute of Technology, Stockholm
                                Abstract
                                                                        Such perspective taking is critical to successful
   Conversation partners’ assumptions about each other’s                communication and is not solely the preserve of HHD.
   knowledge (their partner models) on a subject are important          People consistently perceive the flexibility and ability of
   in spoken interaction. However, little is known about what           automated artificial (computer) partners as far lower than
   influences our partner models in spoken interactions with
   artificial partners. In our experiment we asked people to name
                                                                        those of a human dialogue partner, leading us to categorise
   15 British landmarks, and estimate their identifiability to a        them as ‘at risk’ listeners in dialogue (Oviatt, MacEachern,
   person as well as an automated conversational agent of either        & Levow, 1998). Moreover, our initial expectations about
   British or American origin. Our results show that people’s           artificial partner’s abilities affect our language choices in
   assumptions about what an artificial partner knows are related       Human-Computer Dialogue (HCD) (Branigan et al., 2011;
   to their estimates of what other people are likely to know -         Edlund, Gustafson, Heldner, & Hjalmarsson, 2008). Yet we
   but they generally estimate artificial partners to have more         know little of how people come to have these expectations:
   knowledge in the task than human partners. These findings
   shed light on the way in which people build partner models of        What factors impact people’s preconceptions and
   artificial partners. Importantly, they suggest that people use       expectations about what an artificial partner is likely to
   assumptions about what other humans know as a heuristic              know, before they have even begun to interact with it? In
   when assessing an artificial partner’s knowledge.                    other words, what determines people’s initial partner
   Keywords: knowledge estimation, human-computer interac-              models for artificial partners?
   tion, partner modelling, theory of mind, human-computer
   dialogue                                                             Understanding what governs and impacts our partner
                                                                        models when interacting with artificial partners, especially
                            Introduction                                in speech-based interactions, has important theoretical and
Psycholinguistic research on human-human dialogue (HHD)                 applied implications (e.g., in developing robust and
has shown that our language choices are affected by the                 effective speech-based interfaces). In this paper, we
assumptions we make about our partners as communicative                 investigate whether our initial assumptions about what an
                                                                        artificial speech-based interaction partner knows are related
and social beings (i.e. our partner models) (Branigan,
Pickering, Pearson, McLean, & Brown, 2011): People tend                 to the sense we have of the social distribution of knowledge.
to estimate their conversational partner’s knowledge and                We also look at how our initial beliefs about partner
communicative abilities, and formulate their utterances                 knowledge are influenced by (1) partner type (humans vs.
accordingly. This complex set of judgements is simplified               artificial) as well as (2) the partner’s signalled nationality.
by using a range of heuristics such as accent and social cues
                                                                        Perspective-Taking in Dialogue
(Clark, 1996; Nickerson, 1999) as well as our beliefs about
the social distribution of knowledge, i.e., assumptions about           Imagine that a stranger asks for directions to a local
what information is likely to be known to whom (e.g.,                   landmark. How do we ensure that the information we
students, residents of Dublin, opticians, birdwatchers)                 include and the language we use to communicate the
(Fussell & Krauss, 1992a).                                              message is appropriate for them? Research suggests that we
                                                                    1836

use verbal and non-verbal cues to assess our conversational         other work (Bell & Gustafson, 1999; Kennedy, Wilkes,
partner’s characteristics, e.g. where they are from, their          Elder, & Murray, 1988). Compared to HHD, users tend to
language proficiency, their age, profession etc., and use           use simpler grammatical structures, use more words in their
these cues to construct a partner model to guide our                descriptions, use fewer pronominal anaphors (e.g. her/him;
language choices (Nickerson, 1999).                                 he/she), and use simpler lexical choices (Amalberti et al.,
                                                                    1993; Kennedy et al., 1988). Such research assumes that
This initial global partner model (Brennan, Galati, &               people’s perceptions and beliefs about their partner’s
Kuhlen, 2010), which is consulted at the stage of initial           abilities affect their language choices in these contexts. Yet
interaction, is formed through relatively superficial cues          it is not clear what factors determine these beliefs in the first
(e.g., stereotypes and pre-conceived expectations and               place, and thus what may be driving people’s global partner
assumptions that are in place prior to the dialogue) and            model during their initial interaction with an artificial
assumptions about the social distribution of knowledge              partner. Our work aims to shed light on this question.
(Fussell & Krauss, 1992a). These initial inferences act to
give a speaker an initial model of common ground between            Research on robotic agents has shown that the perceived
interlocutors, i.e., a representation of mutual knowledge,          nationality of the agent, and the content that it is being
assumptions and beliefs shared between the interlocutors in         asked to process, both influence participants’ judgements
a conversation, crucial to successful and effective                 about its abilities (Lee, Lau, Kiesler, & Chiu, 2005).
communication (Bromme, Rambow, & Nückles, 2001;                     Participants used these cues in a similar way to that which
Clark, 1996). Although our partner models may be                    they are used in HHD: When they were asked to judge the
subsequently updated by local experiences within the                likelihood that a robot ‘from New York’ or ‘from Hong
dialogue interaction (e.g. feedback about comprehension,            Kong’ would know and recognize a set of New York and
via verbal and non-verbal cues) (Brennan et al., 2010), the         Hong Kong landmarks, they judged that the robot would be
global model acts as a guide for our initial interaction,           more likely to identify landmarks associated with its
especially before feedback has been gathered from within            perceived nationality (Lee et al., 2005). In this context,
the dialogue (Fussell & Krauss, 1992a).                             accent can play an important role. It acts as a strong signal
                                                                    of identity and a speaker’s linguistic background (Ikeno &
Understanding how we develop and form these initial                 Hansen, 2007), and allows listeners to identify
models is important, as research shows that they guide our          characteristics such as age, gender and geographic
language choices. We tend to adjust our language based on           affiliation, as well as stimulating specific stereotypes (Ryan,
our assumptions about our addressees’ knowledge. For                Giles, & Sebastian, 1982).
instance, when people are asked to describe items for their
friends, they adapt their descriptions to their friend’s            Research Aims and Hypotheses
knowledge – and these adjustments lead to better                    There is currently little understanding of what factors affect
communication, i.e., higher accuracy in identification              people’s assumptions about partner knowledge and abilities
(Fussell & Krauss, 1989). Crucially, studies also show that         in HCD contexts. The limited existing research on people’s
we are very accurate at assessing others’ knowledge and that        perceptions of artificial dialogue partners tends to focus on
these assessments guide how we construct our initial                affective factors such as interface likeability rather than on
message in communication (Fussell & Krauss, 1991,                   assumptions about a computer’s knowledge and abilities.
1992a).                                                             Other work in tangential fields such as HRI cannot be
                                                                    assumed to hold more widely as the embodiment of robots
Similar effects of partner models on language choice are            tend to facilitate the mapping of human abilities to a robot
thought to drive our dialogue interactions with artificial          partner (Kiesler, 2005).
dialogue partners. People tend to see artificial partners as
poorer interlocutors and alter their language choices and           We present a study using a similar method to previous work
speech behaviours as a result (Branigan et al., 2011; Oviatt,       investigating how people initially estimate human partners’
Bernard, & Levow, 1998). For example, people are more               knowledge (Fussell & Krauss, 1992a), in order to
likely to converge (or align) with their partner’s choice of        investigate how people estimate artificial partners’
referring expression when they believe their partner to be a        knowledge. People are asked to name landmarks and judge
computer rather than a human. In addition, they adjust their        the identifiability of those landmarks’ names to others. We
behaviour more in this way when they are led to believe that        hypothesise that people will use the same heuristics to
the artificial partner is a ‘basic’ interlocutor with restricted    estimate partner knowledge for artificial partners as they use
capability than a partner with more advanced capability             for human partners. That is, people will rate both human and
(Branigan et al., 2011). Similarly, people’s linguistic             artificial partners as more likely to know the name of those
choices in a telephone conversation concerning air-fares and        landmarks that are generally more accurately identified by
timetables change depending on whether they believe their           other people (H1). This would be evidence that people have
partner to be a human or a computer (Amalberti, Carbonell,          a sense of the spread of knowledge about a topic in the
& Falzon, 1993). Similar findings have been reported in             population (i.e., the social distribution of knowledge) with
                                                                1837

this being related to their assessment of a partners’ likely      researchers were developing a British-based (British
knowledge, including artificial partners. We also expect a        nationality condition) or a US-based (US nationality
strong positive correlation between judgements of humans’         condition) automated agent. They then listened to a sample
and artificial partners’ knowledge (H2), giving support to        audio clip taken from the system. Participants listened to a
the idea that our judgements of artificial agents are related     sample audio introduction from the agent (e.g. “Hello, my
to our judgements of humans in this context. Based on the         name is Laura. How can I help you?”), simulating the type
intimated difference in partner models between humans and         of content that would guide people’s initial partner models
artificial partners in the literature we also hypothesise that    in these types of interactions. To further emphasise the
there will be a statistically significant difference between      nationality, the introductory message from the service was
people’s judgements of how likely a person versus an              played in either a British or a US accent. This procedure
artificial agent is to know the name of the stimuli (H3). We      was used to make sure that participants who lacked previous
also hypothesise that people will make different judgements       experience with agents had a frame of reference for their
about partner knowledge based on the relation between the         ratings.
system’s signalled nationality (UK or US) and the type of
content being judged in the experiment (i.e., UK landmarks)       Measures
(H4).                                                             Participant’s ability to name landmarks To identify the
                                                                  spread of knowledge within the sample, all participants
                         METHOD                                   were initially asked to name the 15 landmarks used in the
                                                                  study. A 300x250 pixel image of each landmark was
Participants                                                      displayed along with a textbox. Participants were asked to
32 (16 F, 16 M) Native British English speakers with a            name the item. They were informed that if they did not
mean age of 32.0 years (S.D.=12.1) from a UK university           know the name of the item they could leave this box blank.
community took part in the study. The majority (N=26) of          The lead author then marked the names given by the
participants had previously spoken to an automated system.        participants as either accurate or inaccurate.
Those who had used such systems were asked to rate how
frequently they used them on a 7 point Likert scale (Very         Others’ knowledge of the landmark names Based on
Infrequently-Very Frequently). The mean rating suggests           scales used in previous research on perception of others’
that their level of experience with these types of interfaces     knowledge in HHD (Fussell & Krauss, 1992b) and human-
was low (M= 2.73, SD= 1.43).                                      robot interaction (HRI) (Lee et al., 2005), participants were
                                                                  asked to judge how identifiable they felt the name of each
Items                                                             landmark would be to others. This was measured using a 7-
                                                                  point Likert scale from Not Identifiable (1) to Very
Fifteen UK landmarks were used as the stimuli in the study,       Identifiable (7).
selected based on the frequency of accurate naming in a pre-
study. This was to ensure that there was variation in the         Procedure
frequency of accurate naming across the items in the
experiment.                                                       Participants were recruited via email from a British
                                                                  university community. Upon responding to the email
Conditions                                                        participants were sent a link to the online survey.
                                                                  Participants completed the demographic section of the
Partner Type All participants were asked to judge both an         survey. They were then asked to name the 15 landmarks,
artificial partner’s (i.e. automated agent) and a human           and subsequently asked to judge how identifiable the name
partner’s (within participants) likely knowledge of the           of the landmarks would be to a human (either a British or
landmark names. The order in which participants were              US person), and then how identifiable the name of the
asked to judge the artificial and the human partner was           landmark would be to a computer (either British or US
randomised. Participants judged all 15 landmarks in each          accented automated agent). Again, the order of these was
condition. The display of the 15 landmarks in each partner        randomised. They were then debriefed as to the purpose of
condition was randomised to reduce potential order effects.       the experiment.
Nationality Participants were asked to judge how likely
                                                                                           RESULTS
either an American (N=14) or British (N=18) partner
(between participants- randomly assigned) would be to
know the landmarks. When in the human partner condition,          Social Distribution of Knowledge
participants were asked to rate how identifiable the              Following previous work on knowledge estimation in HHD
landmarks’ names would be to either a British or American         (Bromme et al., 2001; Fussell & Krauss, 1992b) we ran
person (participants were told that ‘identifiable’ referred to    analysis on the item level data to test H1 and 2. Using the
the likelihood of knowing the landmark name). When in the         item level data means we can see whether landmarks that
artificial partner condition, participants were told that the     were more accurately named across the sample were rated
                                                              1838

as more likely to be known to both human and artificial          knowledge is more or less likely to be known) and that this
partners. This would give us a sense of how people’s             has a strong relationship to their judgements of how likely
assumptions of knowledge for each item relate to actual          the name is to be known to a person and an artificial partner.
levels of knowledge in the group of participants for each
item. This type of fine grained insight would not be possible
using the participant level data as we would only have a
measure of accuracy for each participant, giving us no sense
of the spread of knowledge of each item in the sample as a
whole.
                                                                  Figure 3: Relationship between human and artificial partner
                                                                                      identifiability ratings
                                                                 Moreover, people’s assessment of how identifiable a
                                                                 landmark’s name is to an artificial partner seems related to
                                                                 how identifiable they believe it is to a human partner. This
   Figure 1: Relationship between percentage accurate item
                                                                 supports the idea that people’s initial model of an artificial
        naming and human partner identifiability rating
                                                                 partner’s knowledge is related to their initial model of other
There was a strong positive correlation between the              people’s knowledge, with both closely reflecting people’s
percentage of accurate responses for an item and                 actual rates of accuracy in naming each item.
participants’ mean judgements of other people’s [r (13)=
.85, p<.001] (Figure 1) as well as an artificial partner’s       The Effect of Partner Type & Nationality
knowledge of its name [r (13)= .86, p<.001] (Figure 2).          To test H3 and H4, we analysed the data at the participant
There was also a strong positive correlation between             level using a 2x2 Mixed ANOVA looking at the effects of
judgments of other people’s knowledge of the names and an        partner type (Human vs. Artificial -within participants) and
artificial partner’s knowledge [r (13)= .78, p<.001] (Figure     nationality (US vs. British- between participants) on
3).                                                              people’s knowledge estimation. We saw a statistically
                                                                 significant main effect of partner type on people’s
                                                                 knowledge estimations [F (1, 30)= 6.43, p=.016, η2G=
                                                                 0.058]. People rated item names in general to be more
                                                                 identifiable to an artificial partner (M=4.60, S.D.=1.06) than
                                                                 to a human partner (M=4.19, S.D.=0.74), supporting our
                                                                 hypothesis but contradicting the direction intimated by
                                                                 previous HCD work. There was no statistically significant
                                                                 main effect of nationality [F (1, 30)= 0.31, p=.58,
                                                                 η2G=0.007] or interaction effect between partner type and
                                                                 nationality [F(1, 30)=2.94, p=.097, η2G=0.028]. Therefore a
                                                                 partner’s nationality did not affect people’s knowledge
                                                                 judgements of human or artificial partners in relation to the
                                                                 landmarks; H4 was therefore not supported.
                                                                 DISCUSSION
 Figure 2: Relationship between percentage accurate naming       We found that people have a strong sense of the social
           and artificial partner identifiability rating         distribution of knowledge and this relates to people’s
These correlations support our hypotheses (H1 and 2). They       judgements about others’ knowledge, irrespective of the
suggest that people have relatively accurate awareness of        other being an artificial agent or a human. The number of
the actual distribution of knowledge (with respect to which      times each item was named correctly correlated strongly and
                                                             1839

positively with people’s estimations of both artificial and       models that recognition is poor and inflexible. Hence rather
human partners’ knowledge of landmark names. We also              than artificial partners being seen as ‘at risk’ dialogue
found that people in general judged the names of the              actors, people’s partner models are likely more nuanced and
landmarks in the experiment to be more identifiable to a          multi-dimensional, presumably encompassing assumptions
computer than a person. Surprisingly, partner nationality did     about both underlying knowledge and processing abilities.
not have statistically significant effects on knowledge
estimation.                                                       To be clear, this study focused on how people establish
                                                                  estimates of knowledge in their initial global partner
Our research highlights that people are relatively accurate at    models, in the absence of dialogue interaction with the
estimating what other people are likely to know based on a        system. Our findings are particularly relevant to how people
sense of the general distribution of that knowledge, similar      form a priori partner knowledge assumptions in a dialogue
to previous research (Fussell & Krauss, 1992b; Lau, Chiu,         context. Yet when in dialogue, our perspective taking is
& Hong, 2001). But importantly, these effects also apply to       likely to be informed by both the global models we create of
our estimates of artificial partners’ knowledge. The actual       our partner (e.g. assumptions of their knowledge and
percentages of correct responses for each item correlated         abilities formed by stereotypes and expectations before
highly and positively with the knowledge estimates for both       interaction) and local experiences within the dialogue (e.g.
artificial and human partners. We therefore seem to use our       feedback of comprehension via verbal and non verbal cues)
estimates of what other people will know to inform our            (Brennan et al., 2010). Indeed these factors are likely to
judgements of what an artificial partner will likely know.        interact in dialogue interactions. Work on HHD interaction
That is, people seem to use their perceptions of the social       has shown that behaviours within a dialogue that do not
distribution of knowledge among humans to anchor their            match our expected partner models impact our speech
perceptions of an artificial partner’s knowledge.                 (Kuhlen & Brennan, 2010). Research suggests that these
                                                                  models should be considered as being dynamic and
We also see that people judged an artificial partner as being     adaptable over time (Fussell & Krauss, 1991; Nickerson,
more likely to know the name of the landmark in the study         1999). Investigating the dynamism of partner models across
than a human partner. It is important to note that our finding    the course of an interaction is a critical issue for future
may reflect users’ assumptions about one specific                 research in HCD as it has been in HHD.
dimension of an artificial partner’s abilities (i.e., their
knowledge of proper names) rather than their                      In addition, although partner models are assumed to be
communicative capabilities or knowledge as a whole.               important in influencing people’s language choices and
Participants were asked to judge how identifiable the name        linguistic processing in HCD (Edlund et al., 2008), more
of a landmark (e.g., Stonehenge) would be. Proper names           research is needed to fully explore the role that they play.
pick out unique entities in the world. As such, they do not       This question has received considerable attention in research
require any complex inferencing, knowledge of ontologies,         on HHD, with particular reference to the extent to which our
conceptual relations between categories. They can (usually)       partner models impact processing: Is their influence
be captured by a simple association between the name and a        immediate and pervasive, or delayed and restricted? (see
unique object, the kind of data that are prototypically           Brennan et al., (2010) for summary of the main theoretical
perceived as easy for computer systems to store, index, and       positions). Within HCD research, partner models have been
retrieve. This may explain why a computer was judged more         invoked to explain the differences in language use between
likely than a human to know the name of the landmarks that        HHD and HCD (Branigan et al., 2011; Edlund et al., 2008),
we used. Other types of knowledge that involve more               but recent research has shown that this may not be true in all
complex conceptual relationships, or operations over              contexts (Cowan & Branigan, 2015; Cowan, Branigan,
elements might not show the same pattern. Note however            Bugis, Obregon, & Beale, 2015). Clearly, partner models
that people did not attribute complete omniscience to the         affect language choice and processing in both HHD and
artificial partner; their judgements about its knowledge were     HCD – but it is not yet clear whether they do so in the same
strongly related to the social distribution of knowledge.         ways and to the same extent. An interesting possibility for
                                                                  future research is that partner models may play a more
There is also likely to be a distinction between what we          pervasive and far-reaching role in HCD than in HHD.
perceive artificial partners to know and what we believe
they can do with this knowledge in dialogue, or even              Implications & Conclusions
whether these names will be recognised effectively in the            Our research set out to investigate the factors that affect
first place. For instance people may assume that artificial       people’s expectations about what an artificial partner is
partners know the proper names of landmarks but may not           likely to know, before they have begun to interact with it.
be sufficiently confident that these names will be recognised     Our findings suggest that we come to interactions with an
during speech recognition. Although vast improvements on          existing presumption of what an artificial partner is likely to
error rates have been made in speech technology research,         know that is based on assumptions of how knowledge is
there may still be a perception within people’s partner           socially distributed. Moreover we found that under some
                                                              1840

circumstances they may have the preconception that an                     design on message comprehension. European
artificial partner knows more than a human partner. These                 Journal of Social Psychology, 19, 509–525.
results      suggest    that   models     of    human-human      Fussell, S. R., & Krauss, R. M. (1991). Accuracy and bias in
communication are applicable in important ways to                         estimates of others’ knowledge. European Journal
communication with artificial agents. They also have                      of Social Psychology, 21, 445–454.
important applied implications for HCD, by casting light on      Fussell, S. R., & Krauss, R. M. (1992a). Coordination of
factors that can lead users towards or away from an                       knowledge in communication: Effects of speakers’
appropriate mental model of a partner’s abilities and                     assumptions about what others know. Journal of
intentions, with implications for successful communication                Personality and Social Psychology, 62(3), 378–
(Kiesler, 2005). When designing artificial systems,                       391.
developers should be aware that people bring with them           Fussell, S. R., & Krauss, R. M. (1992b). Coordination of
assumptions about the social distribution of knowledge,                   knowledge in communication: Effects of speakers’
which could significantly affect their interaction.                       assumptions about what others know. Journal of
                                                                          Personality and Social Psychology, 62(3), 378–
                          References                                      391.
Amalberti, R., Carbonell, N., & Falzon, P. (1993). User          Ikeno, A., & Hansen, J. H. L. (2007). The Effect of Listener
           representation of computer systems in human-                   Accent Background on Accent Perception and
           computer speech interaction. International Journal             Comprehension. EURASIP J. Audio Speech Music
           of Man-Machine Studies, 38, 547–566.                           Process., 2007(3), 4:1–4:8.
Bell, L., & Gustafson, J. (1999). Interaction with an            Kennedy, A., Wilkes, A., Elder, L., & Murray, W. S.
           animated agent in a spoken dialogue system. In                 (1988). Dialogue with machines. Cognition, 30(1),
           Proceedings of the Sixth European Conference on                37–72.
           Speech Communication and Technology (pp.              Kiesler, S. (2005). Fostering common ground in human-
           1143–1146). Budapest, Hungary: ISCA.                           robot interaction. In IEEE International Workshop
Branigan, H. P., Pickering, M. J., Pearson, J. M., McLean, J.             on Robot and Human Interactive Communication,
           F., & Brown, A. (2011). The role of beliefs in                 2005. ROMAN 2005 (pp. 729–734).
           lexical alignment: Evidence from dialogs with         Kuhlen, A. K., & Brennan, S. E. (2010). Anticipating
           humans and computers. Cognition, 121(1), 41–57.                Distracted      Addressees:     How       Speakers’
Brennan, S., Galati, A., & Kuhlen, A. (2010). Two Minds,                  Expectations and Addressees’ Feedback Influence
           One Dialog: Coordinating Speaking and                          Storytelling. Discourse Processes, 47(7), 567–587.
           Understanding. In B. Ross (Ed.), The Psychology       Lau, I. Y.-M., Chiu, C., & Hong, Y. (2001). I Know What
           of Learning and Motivation: Advances in Research               You Know: Assumptions About Others’
           and Theory (Vol. 53, pp. 301–344). Elsevier.                   Knowledge and Their Effects on Message
Bromme, R., Rambow, R., & Nückles, M. (2001). Expertise                   Construction. Social Cognition, 19(6), 587–600.
           and estimating what other people know: The            Lee, S., Lau, I., Kiesler, S., & Chiu, C.-Y. (2005). Human
           influence of professional experience and type of               Mental Models of Humanoid Robots. Robotics and
           knowledge. Journal of Experimental Psychology:                 Automation, 2005. ICRA 2005. Proceedings of the
           Applied, 7(4), 317–330.                                        2005 IEEE International Conference on, 2767–
Clark, H. H. (1996). Using Language. Cambridge                            2772.
           University Press.                                              https://doi.org/10.1109/ROBOT.2005.1570532
Cowan, B. R., & Branigan, H. P. (2015). Does voice               Nickerson, R. S. (1999). How we know—and sometimes
           anthropomorphism affect lexical alignment in                   misjudge—what others know: Imputing one’s own
           speech-based human- computer dialogue? In                      knowledge to others. Psychological Bulletin,
           Proceedings of Interspeech 2015. Dresden,                      125(6), 737–759.
           Germany: ISCA.                                        Oviatt, S., Bernard, J., & Levow, G. A. (1998). Linguistic
Cowan, B. R., Branigan, H. P., Bugis, E., Obregon, M., &                  adaptations during spoken and multimodal error
           Beale, R. (2015). Voice anthropomorphism,                      resolution. Language and Speech, 41 ( Pt 3-4),
           interlocutor modelling and alignment effects on                419–442.
           syntactic alignment in human-computer dialogue.       Oviatt, S., MacEachern, M., & Levow, G.-A. (1998).
           International Journal of Human-Computer Studies,               Predicting hyperarticulate speech during human-
           83, 27–42.                                                     computer error resolution. Speech Communication,
Edlund, J., Gustafson, J., Heldner, M., & Hjalmarsson, A.                 24(2), 87–110.
           (2008). Towards human-like spoken dialogue            Ryan, E. B., Giles, H., & Sebastian, R. J. (1982). An
           systems. Speech Communication, 50(8–9), 630–                   integrative perspective for the study of attitudes
           645.                                                           towards language variation. In Attitudes Towards
Fussell, S. R., & Krauss, R. M. (1989). Understanding                     Language: Social and Applied Contexts. London:
           friends and strangers: The effects of audience                 Arnold.
                                                             1841

