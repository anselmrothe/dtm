                               Actions that Modify Schedules of Reinforcement
                                                   Mac Strelioff (mstrelio@uci.edu)
                                               Mimi Liljeholm (m.liljeholm@uci.edu)
                                   Department of Cognitive Sciences, University of California Irvine
                                                         Irvine, CA 92697-5100 USA
                            Abstract                                 latency of a single action at a time, ignoring situations in
   Many everyday activities involve the use of one action to
                                                                     which multiple actions are performed in concert and
   modify the effects of another: When driving, shifting             potentially interact. In our paradigm, an intermediate
   gears modifies the influence of pressing the gas pedal on         rate of responding on one action maximizes the reward
   acceleration; when cooking, the rate of adding a                  contingent on performing a different, concurrently
   particular ingredient modifies the influence of stirring on       available, action.
   viscosity. Here, we investigate a general ability to learn
   how to use actions to control schedules of reinforcement.
   In Experiment 1, participants quickly discovered the
                                                                                           Experiment 1
   optimal rate of responding on an action that controlled
   the rate of reward contingent on performing a different           Methods
   action. In Experiment 2, when the modifying action was            Participants Thirty undergraduates at the University of
   itself rewarded, participants failed to discover the optimal      California, Irvine (22 females; mean age=20±2.17)
   rate. Implications for formal theories of instrumental
                                                                     participated in the study for course credit. All
   behavior are discussed.
                                                                     participants gave informed consent and the study was
   Keywords: Schedules of reinforcement; reward learning;            approved by the Institutional Review Board of the
   instrumental contingencies.                                       University of California, Irvine.
                        Introduction                                 Task & Procedure The task is illustrated in Figure 1.
Since the early 20          th
                                century, researchers have            We used a free operant paradigm in which participants
investigated the influence of various reward schedules               were allowed to respond at will on either or both of two
on the rate and selection of instrumental responses. For             concurrently available actions, graphically represented
example, ratio schedules, in which reward delivery                   on the computer screen, by pressing the corresponding
depends on the number of responses since the last                    keys on the computer keyboard. Whenever a response
reward, produce higher rates of responding than do                   was made a selection square appeared around the chosen
interval schedules, in which reward delivery depends on              action for 300ms. If the response was rewarded, an
the time elapsed since the last reward (Fester & Skinner,            image of a quarter appeared center screen for 500ms and
1957). When two or more action alternatives are                      a count of the cumulative monetary earning,
available, that which yields the greatest, most                      continuously displayed above the quarter image
immediate, or most certain reward is, all other things               location, would increment by +$0.25. The task was
being equal, generally that most frequently selected                 comprised of ten 2-minute blocks separated by brief rest
(e.g., Rachlin et al., 1991). However, in the real world,            periods. All monetary earnings were fictitious.
many responses serve only to modulate the effects of                    In the “Modify” group (n=15), the rate of responding
other actions: The rate and pattern of pressing strings on           on a “modifying” action influenced the probability that
a guitar does not itself yield music, but profoundly                 the concurrently available “modified” action would
impacts the sounds produced by strumming. Here, we                   produce a reward. When the modifying action was
assess a domain-general capacity for learning about                  performed at an “optimal” rate of 1.25 to 2.75 presses
actions that control schedules of reinforcement on other             per second, the probability of reward given a response
actions.                                                             on the modified action was 0.9. When response rates on
   Formally, the relationship between a particular action            the modifying action were outside of the 1.25 to 2.75
and its outcome has been modeled as a complex                        range, the probability of reward given the modified
associative structure (Dickinson & Balleine, 1993), as               action was 0. The modifying action did not itself
the difference between probabilities of reward given the             produce any reward. Response rates on the modifying
presence versus absence of the action (Hammond,                      action were tracked using a differential equation that
1980), as the probability and subjective utility of the              increased by an impulse of 1 at the time of a response
outcome given the action (Savage, 1954), or as a cached              and decayed each impulse at a linear rate of 0.2 per
value assigned to the action based on its reinforcement              second, so that each impulse from a response decayed to
history (Watkins, 1989). What these diverse approaches               zero after 5 seconds.
have in common is that they address the identity and/or
                                                                     3261

   Specifically, for an impulse (ai), which was 1 if an    bin-by-group interaction, F(1,28)=5.78, p<0.05.
action were taken during the current iteration of the      Planned comparisons revealed that while the two groups
program and 0 otherwise, a decay rate of 0.2, a counter    did not differ with respect to optimal responding on the
for the number of responses that occurred within the last  modifying action in the first bin, t(28)=0.13, p=0.89, by
5 seconds (N5) and the difference in time between the      the last bin, participants in the Modify group were
current iteration of the program and the previous
                                                           significantly closer to the optimal response rate than
iteration (dt), the response rate variable (R) was updated
on each iteration i by:                                    were participants in the Yoked group, t(28)=3.61,
                                                           p<0.01. As can be seen in Figure 2, while the mean
                 Ri ← Ri−1 + ai − 0.2N 5dt                 deviation from the optimal rate significantly decreased
                                                           from the first to the last bin in the Modify group,
This method adjusts more quickly to changes in
response rate than the commonly used approach of           t(14)=2.69, p<0.05, they remained unchanged across
dividing the number of responses in a time window by       bins in the Yoked group, t(14)=0.05, p=0.63. The
the length of the window (e.g., Soto et al., 2006). The    apparent absence of a change in optimal responding by
probability of reward on the modified action was set to    Yoked participants reflects a tendency to either increase
0.9 whenever the response rate variable, R, was in the     or decrease responding on the modifying action across
optimal, 1.25 to 2.75, range and 0.0 otherwise.            blocks, resulting in no net change for the group; in
   Note that the optimal rate of responding on the         contrast participants in the Modify group coherently
modifying action was intermediate; this was done to rule   converged towards the optimal rate.
out the contribution of systematic biases of either very
high or very low responding. On the other hand, an
intermediate rate might represent an average towards
which most responders converge in free operant tasks.
To address this possibility, a second, “Yoked”, group
was included (n=15), in which the rate of responding on
the modifying action had no influence, while the
probability of reward on the modified action was yoked
to that of a participant in the Modify group. We
predicted that, by the end of the session, participants in
the Modify group would respond on the modifying
action at a rate falling within the optimal range, while
those in yoked group would not.
                                                                Figure 2: Mean deviation of response rates on
                                                                the modifying action from the optimal range in
                                                                the first and last 5 seconds of task performance,
                                                                for subjects in the Modify (black) and Yoked
                                                                (blue) groups, and for the single group of
                                                                Experiment 2, in which the modifying action
                                                                was rewarded (red). Error bars=SEM.
                                                           We also assessed performance in terms of the proportion
                                                           of bins with optimal response rates, early and late in the
        Figure 1: Task Illustration, see text for details. task. Bins were scored as optimal if the windowed (5
                                                           seconds) response rate was in the optimal range of 1.25
Results                                                    to 2.75 responses per second. For each subject, we
We divided the number of responses on the modifying        assessed the number of optimal 5-second bins in the first
                                                           and last 30 seconds of the task. (We used 30 seconds,
action in each 5-second bin of task performance with 5
                                                           rather than the full 2-minute blocks, to ensure that the
(i.e., responses per second), and computed the distance    index of early learning did not include already
of this response rate from the bounds of the optimal       asymptotic performance.) The results using this metric
range, for the first and last 5 seconds of the task. We    were consistent with those described above: The groups
then used a mixed analysis of variance (ANOVA) with        did not differ in the first 30-second block, t(28)=1.12,
“group” as the between-subject factor and “bin” as the     p=0.24, but by the last 30-second block, the mean
within-subject factor to assess a change in optimal        proportion of optimal bins was significantly greater for
responding between the first and last bins. There was      the Modify group than for the Yoked group, t(28)=6.93,
no main effect of bin, F(1,28)=3.19, p=0.09, but a main    p<0.01. Indeed, while the proportion of optimal bins
effect of group, F(1,28)=6.53, p<0.05, and, critically, a  increased significantly from the first to the last block in
                                                           3262

the Modify group, t(14)=3.60, p<0.01, it decreased,       with the ability to control the schedule of reinforcement
albeit with marginal significance, t(14) = 2.09, p=0.06,  on the modified action.
in the Yoked group. The mean proportion of optimal
bins in each 30-second block throughout the task is       Methods
shown in Figure 3.                                        Participants Fifteen undergraduates at the University of
                                                          California, Irvine (10 females; mean age=19.7±1.1)
                                                          participated in the study for course credit. All
                                                          participants gave informed consent and the study was
                                                          approved by the Institutional Review Board of the
                                                          University of California, Irvine.
                                                          Task & Procedure Participants performed a task that
                                                          was identical to that of the Modify group in Experiment
                                                          1, with one exception: In addition to modulating the
                                                          schedule of reinforcement on the modified action, the
                                                          modifying action was itself rewarded by $0.25, with a
                                                          probability of 0.2.       Note that, since this reward
                                                          probability is much lower than the conditional, 0.9,
                                                          probability of reward on the modified action,
                                                          maintaining an optimal, intermediate, response rate on
     Figure 3: Mean proportion of bins with an
                                                          the modifying action dramatically increases the average
     optimal response rate on the modifying action in
                                                          reward rate.
     each 30-second block of the task for the Modify
     (black) and Yoked (blue) groups of Experiment        Results
     1, and for the Reward group of Experiment 2
     (red). Shading=SEM.                                  We computed the same measures of optimal responding
                                                          as those used in Experiment 1. Comparing the first and
   With respect to the modified action, response rates    last 5 seconds of performance, there was no change in
were, overall, higher than those on the modifying action, the deviation of response rates on the modifying action
for both the Modify, t(14)=3.10, p<0.05, and Yoked,       from the bounds of the optimal rate, t(14)=0.48, p=0.64
t(14)=6.76, p<0.05, groups. This likely reflects the fact (see Figure 2). Likewise, the proportion of optimal bins
that, while the probability of reward given the modified  did not differ between the first and last 30-second
action was either a function of (Modify) or independent   blocks of the task, t(14)=0.00, p=1.00. In the absence of
of (Yoked) responding on the modifying action, the        random assignment, we refrain from making any
actual delivery of reward was contingent only on          statistical comparisons between the results of this
performing the modified action.                           experiment and those obtained in Experiment 1.
                                                          Nonetheless, it is worth noting that, as illustrated in
                    Experiment 2                          Figures 2 and 3, when the modifying action was itself
A well-studied phenomenon closely related to our query    rewarded, the rate of responding on the modifying
is that of “melioration” – a tendency to select an action action was apparently closer to that in the Yoked group
alternative that produces a greater immediate pay-off,    than in the Modify group. Finally, although, overall,
but that, when selected repeatedly, lowers the overall    response rates were again higher on the modified than
rate of reward (Herrnstein, 1991). Such tendencies are    the modifying action, unlike for the groups in
commonly attributed to impulsivity (Herrnstein, 1991;     Experiment 1, this difference was only marginally
Otto, Markman, & Love, 2012), but have also been          significant, t(14)=2.09, p=0.06, presumably reflecting
described as rational choices under uncertainty           the fact that, in Experiment 2, reward delivery was
(Gureckis & Love, 2009a, 2009b; Sims et al., 2013).       potentially contingent on performing either action.
Other related paradigms, such as delay discounting
(Ainslie, 1975; Johnson & Bickel, 2002) and differential                   General Discussion
reinforcement of low response rates (Wilson & Keller,
1953; Carter & MacGrady, 1966), have convincingly         In two experiments, we assessed the discovery and
demonstrated the interfering influence of salient reward  performance of an action that controlled the schedule of
on rational decision-making (Ainslie, 1975; Van den       reinforcement on another, concurrently available,
Broek, Bradshaw, & Szabadi, 1987).                        action. In Experiment 1, participants quickly discovered
   In Experiment 2, we assess whether the lure of an      and implemented an optimal, intermediate, response rate
immediate reward results in a failure to suppress         on a modifying action that, while not producing any
responding on the modifying action, thus interfering      rewards itself, modulated the reward contingent on a
                                                          distinct, concurrently available, action. Response rates
                                                          3263

in a yoked control group confirmed that convergence to            the dependencies between actions, states and rewards
the optimal rate was due to the influence of the                  (e.g., Acuna & Schrater, 2010). Although initially
modifying action on the reward schedule of the                    ignorant of the nature of these dependencies, a Bayesian
modified action. In Experiment 2, consistent with a               reinforcement learner generates beliefs over a set of
large literature on the failure to suppress inappropriate         possible dependency structures and updates those
responding in the face of immediate reward (Ainslie,              beliefs, after each observation, using Bayesian
1975; Carter & MacGrady, 1966; Van den Broek et al.,              inference. For example, a learner in our task might
1987; Wilson & Keller, 1953), reinforcement of the                postulate two possible worlds: one in which the latency
modifying action apparently prevented discovery of the            to respond on an action can modulate the probability of
optimal response rate. The focus in the existing                  reward given that same, or some other, action, and one
literature on the disruptive effects of immediate reward          in which response latencies have no influence on
has largely overshadowed the question raised here of              schedules of reinforcement. The former possibility must
whether, and how, agents learn about actions that                 of course be further partitioned into several putative
modify schedules of reinforcement. Our results suggest            structures, each with a particular set of links (e.g., an
that, in the absence of interfering or competing reward           action modifying its own probability of reward vs. that
contingencies, increasing levels of instrumental control          of a different action) and associated parameters. The
can be achieved by incorporating information about                learner then updates the belief distribution over
dependencies between actions.                                     structures based on sequences of actions, latencies and
   In a model-free reinforcement learning account of              rewards.
free operant responding, Niv et al. (2007) proposed that,            Critically, the approach sketched in the previous
for each decision, the agent selects both the latency and         paragraph, to address model-based inferences regarding
the identity of the to-be-executed action, based on the           action dependencies, can also be used to explain some
relative degree to which that action increases the                of the most basic aspects of instrumental behavior, such
average reward rate. Although it is possible that                 as the distinction between interval and ratio schedules –
participants in the Modify group of Experiment 1                  Recall that, whereas on interval schedules a response is
similarly learned about the modifying action based on             rewarded based on the amount of time elapsed since the
its reinforcement history, several aspects of our task            last reward, on ratio schedules a response is rewarded
depart from the specification of Niv et al. (2007). Most          based on the number of responses since the last reward.
notably, participants in our task would have to include a          These qualitatively different schedules produce distinct
representation of the modified action in their state space        response profiles (Fester & Skinner, 1957), suggesting
when updating the value of the modifying action – that            some, implicit or explicit, discrimination by the agent.
is, assess the value of a particular latency of the                Notably, the interval schedule can be conceptualized as
modifying action given that the modified action is                a case in which the rate of performing an action
simultaneously1 or proximally performed – since the               modifies the schedule of reinforcement, rather than just
modifying action is never itself rewarded. Likewise, the          the rate of reward: Specifically, on a given interval
value of the modified action has to be specified                  schedule, any response rate greater than “one per the
conditional on the performance of the modifying action,           required interval” will decrease the probability of
since the probability of reward on the former is zero             reward conditional on that action. Other well-
whenever responding on the latter falls outside the               established schedules, such as differential reinforcement
optimal range. It is of course possible to specify a              of high or low responding (Van den Broek, et al., 1987)
model-free learner that has enough conditionals built             can also be characterized as actions modifying
into its state-representation to identify the combination         schedules of reinforcement, as can the “seeking”
of responding on modifying and modified actions that              component of seeking-taking schedules (Balleine,
maximizes reward2.                                                Garner, Gonzalez, & Dickinson, 1995). Thus, the
   An alternative, model-based, approach is for the agent         framework proposed here potentially applies to a wide
to create a graphical probabilistic model representing            range of instrumental phenomena.
                                                                     At the neural level, model-free and model-based RL
1
  Note that even the possibility of simultaneously performing     approaches have been mapped to dissociable neural
multiple responses falls outside the scope of Niv et al.’s (2007) substrates, with the ventral striatum, posterior putamen
model, according to which all action-latency pairs are serially   and premotor cortex being implicated in model-free
implemented (i.e., no alternative actions may be executed         responding (Glascher et al., 2010; Lee et al., 2014;
while the time indicated by the chosen latency passes).           Tricomi et al., 2009; de Wit et al., 2012), and the
2                                                                 caudate, ventromedial prefrontal cortex and inferior
   Indeed, Niv et al.’s (2007) model hard-codes into the
definition of each state several variables that are needed to     parietal lobule in model-based computations (de Wit et
discover an optimal policy in the environments addressed by       al., 2012; Liljeholm et al., 2011, 2013, 2015; Lee et al.,
the model (e.g., the time elapsed since the last response when    2014). It should be noted, however, that, with some
modeling interval schedules and the number of presses since       exceptions (e.g., Liljeholm et al., 2013), the work
the last reward when modeling ratio schedules).
                                                                  3264

identifying such dissociations has focused on relatively     differences in the balance between habitual and goal-
simple model-based processes, such as the encoding of        directed action control. Journal of
individual action-outcome contingencies or sensitivity       Neuroscience, 32(35), 12066-12075.
to changes in an outcomes utility. In contrast, the        Dickinson, A., & Balleine, B. (1993). Actions and
model-based learner postulated here engages in complex       responses: The dual psychology of behaviour. In N.
reasoning regarding how actions may be used to control       Eilan, R. A. McCarthy, & B. Brewer (Eds.), Spatial
action-outcome relationships. Such processes may             representation: Problems in philosophy and
warrant the involvement of brain regions known to            psychology (pp. 277-293). Malden: Blackwell
support relational and inductive reasoning, including the    Publishing
rostolateral and dorsolateral prefrontal cortex (e.g.,     Fester, C. B., & Skinner, B. F. (1957). Schedules of
Krawczyk et al., 2011).                                      reinforcement. New York, NY: Appleton-Century-
   Finally, an important point regarding action              Croft.
dependencies such as those addressed here is how they      Gläscher, J., Daw, N., Dayan, P., & O'Doherty, J. P.
relate to the actual representations of actions. In our      (2010). States versus rewards: dissociable neural
task, the instructions and materials clearly defined and     prediction error signals underlying model-based and
distinguished between action alternatives (see Figure 1),    model-free reinforcement learning. Neuron, 66(4),
so that there could be little doubt about how many, and      585-595.
exactly what, actions were available. It is interesting to Gureckis, T. M., & Love, B. C. (2009a). Learning in
consider how inferences and performance might have           noise: Dynamic decision-making in a variable
differed had the grouping of elements into discrete          environment. Journal of Mathematical Psychology,
action alternatives been more ambiguous.              One    53(3), 180–193.
possibility is that increasing ambiguity would afford a    Gureckis, T. M., & Love, B. C. (2009b). Short term
more rapid acquisition of relevant dependencies              gains, long term pains: How cues about state aid
(Pezzulo, Rigoli, & Friston, 2015) and, further, that        learning in dynamic environments. Cognition, 113(3),
those inferred dependencies might serve to configure         293–313.
action elements into more clearly delineated action        Hammond, L. J. (1980). The effect of contingency upon
representations based on reinforcement learning              the appetitive conditioning of free‐operant
principles (e.g. Reynolds, & O’Reilly, 2009).                behavior. Journal of the experimental analysis of
   In conclusion, we have demonstrated a domain-             behavior, 34(3), 297-304.
general ability to learn about, and take advantage of, an  Herrnstein, R. J. (1991). Experiments on stable
action that modifies the schedule of reinforcement on a      suboptimality in individual behavior. The American
different action. We have also sketched a model that, by     Economic Review, 81(2), 360–364.
making inferences about dependencies between               Hogarth, L., Dickinson, A., Wright, A., Kouvaraki, M.
response latencies and conditional reward probabilities,     & Duka, T. (2007). The role of drug expectancy in the
might account for behavior across a wide range of            control of human drug seeking. Journal of
instrumental schedules. Future work will focus on            Experimental Psychology-Animal Behavior
extensions of our experimental paradigm, further             Processes, 33(4), 484-496.
development of formal accounts, and investigations of      Johnson, M. W., & Bickel, W. K. (2002).
mediating neural substrates.                                 Within‐subject comparison of real and hypothetical
                                                             money rewards in delay discounting. Journal of the
                       References                            experimental analysis of behavior, 77(2), 129-146.
Acuna, D., & Schrater, P. (2010). Structure learning in    Krawczyk, Daniel C., M. Michelle McClelland, and
   human       sequential      decision-making.      PLoS    Colin M. Donovan. "A hierarchy for relational
   Computational Biology, 6, 1–8.                            reasoning in the prefrontal cortex." Cortex 47.5
Ainslie, G. (1975). Specious reward: a behavioral theory     (2011): 588-597.
  of impulsiveness and impulse control. Psychological      Lee, S. W., Shimojo, S., & O’Doherty, J. P. (2014).
  bulletin, 82(4), 463.                                      Neural computations underlying arbitration between
Balleine, B. W., Garner, C., Gonzalez, F., & Dickinson,      model-based and model-free learning.Neuron, 81(3),
  A. (1995). Motivational control of heterogeneous           687-699.
  instrumental chains. Journal of Experimental             Liljeholm, M., Tricomi, E., O'Doherty, J. P., & Balleine,
  Psychology: Animal Behavior Processes, 21(3), 203.         B. W. (2011). Neural correlates of instrumental
Carter, D. E. & MacGrady, G. J. (1966). Acquisition of       contingency learning: differential effects of action–
  a temporal discrimination by human subjects.               reward conjunction and disjunction. Journal of
  Psychonomic Science, 5, 309-310.                           Neuroscience, 31(7), 2474-2480.
de Wit, S., Watson, P., Harsay, H. A., Cohen, M. X.,       Liljeholm, M., Wang, S., Zhang, J., & O'Doherty, J. P.
   van de Vijver, I., & Ridderinkhof, K. R. (2012).          (2013). Neural correlates of the divergence of
   Corticostriatal connectivity underlies individual
                                                           3265

  instrumental probability distributions. Journal of
  Neuroscience, 33(30), 12519-12527.
Liljeholm, M., Dunne, S., & O'doherty, J. P. (2015).
  Differentiating neural systems mediating the
  acquisition vs. expression of goal‐directed and
  habitual behavioral control. European Journal of
  Neuroscience, 41(10), 1358-1371.
Niv, Y., Daw, N. D., Daphna, J., & Dayan, P. (2007).
  Tonic dopamine: opportunity costs and the control of
  response vigor. Psychopharmacology, 191(3), 507–
  520.
Otto, A. R., Markman, A. B., & Love, B. C. (2012).
  Taking more, now: The optimality of impulsive
  choice hinges on environment structure. Social
  Psychological and Personal-ity Science, 3, 131–138.
Pezzulo, G., Rigoli, F., & Friston, K. (2015). Active
  Inference, homeostatic regulation and adaptive
  behavioural control. Progress in Neurobiology, 134,
  17-35.
Rachlin, H., Raineri, A., & Cross, D. (1991). Subjective
  probability and delay. Journal of the experimental
  analysis of behavior, 55(2), 233-244.
Reynolds, J. R., & O’Reilly, R. C. (2009). Developing
  PFC representations using reinforcement
  learning. Cognition, 113(3), 281-292.
Savage, Leonard J. (1954). The Foundations of
  Statistics. New York, Wiley.
Sims, C. R., Neth, H., Jacobs, R. A., & Gray, W. D.
  (2013). Melioration as rational choice: Sequential
  decision making in uncertain environments.
  Psychological Review, 120(1), 139–154.
Soto, P. L., McDowell, J. J., & Dallery, J. (2006).
  Feedback functions, optimization, and the relation of
  response rate to reinforcement rate. Journal of the
  Experimental Analysis of Behavior, 85, 57-71.
Tricomi, E., Balleine, B. W., & O’Doherty, J. P. (2009).
  A specific role for posterior dorsolateral striatum in
  human habit learning. European Journal of
  Neuroscience, 29(11), 2225-2232.
Van den Broek, M. D., Bradshaw, C. M., & Szabadi, E.
  (1987). Behaviour of ‘impulsive’and ‘non-
  impulsive’humans in a temporal differentiation
  schedule of reinforcement. Personality and Individual
  Differences, 8(2), 233-239.
Watkins, C. J. C. H. (1989). Learning from delayed
  rewards (Doctoral dissertation, University of
  Cambridge).
Wilson, M. P. & Keller, F. S. (1953). On the selective
  reinforcement of spaced responding. Journal of
  Comparative and Physiological Psychology, 46, 190-
  193.
                                                         3266

