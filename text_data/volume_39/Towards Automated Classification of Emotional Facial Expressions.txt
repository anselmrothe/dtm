               Towards Automated Classification of Emotional Facial Expressions
                 Lewis J. Baker (lewis.j.baker@rutgers.edu)1 , Vanessa LoBue (vlobue@rutgers.edu)2 ,
     Elizabeth Bonawitz (elizabeth.bonawitz@rutgers.edu)2 , & Patrick Shafto (patrick.shafto@gmail.com)1
                           1 Department   of Mathematics and Computer Science, 2 Department of Psychology
                                 Rutgers University – Newark, 101 Warren St., Newark, NJ, 07102 USA
                               Abstract                                   psychological studies. In doing so, we highlight the need for
                                                                          real-world data to solve real-world problems, as models based
   Emotional state influences nearly every aspect of human cog-
   nition. However, coding emotional state is a costly process            on well-curated training images that are common in the field
   that relies on proprietary software or the subjective judgments        often fail to accurately categorize messy, uncontrolled im-
   of trained raters, highlighting the need for a reliable, automatic     ages. We further show how a single, large dataset that lever-
   method of recognizing and labeling emotional expression. We
   demonstrate that machine learning methods can approach near-           ages controlled and uncontrolled images can improve gener-
   human levels for categorization of facial expression in natural-       alization to real-world stimuli.
   istic experiments. Our results show relative success of models            Recognition of facial expressions is a useful, non-invasive
   on highly controlled stimuli and relative failure on less con-
   trolled images, emphasizing the need for real-world data for           method of reasoning about another’s thoughts. The seem-
   application to real-world experiments. We then test the poten-         ing universality of emotional expressions further underscores
   tial of combining multiple freely available datasets to broadly        their importance (Ekman & Friesen, 1971). However, to un-
   categorize faces that vary across age, race, gender and photo-
   graphic quality.                                                       derstand how emotion influences cognition, researchers must
   Keywords: Classification, machine learning, computer vision,           be able to categorize facial expressions in continuous time –
   support vector machines, emotion and cognition, facial recog-          and no existing measure can do this without great expense of
   nition                                                                 time or money. Participant surveys lack temporal resolution
                                                                          and fall prey to metacognitive errors. Physiological methods
                           Introduction                                   require expensive or invasive apparatus such as galvanic skin
Emotions are widely assumed to play a causal role in nearly               response monitors or cortisol measurements Picard, Vyzas,
every aspect of cognition (e.g., Pessoa, 2008), and yet many              and Healey, 2001. Even the gold-standard method of the Fa-
studies in cognitive science (and developmental science in                cial Action Coding System (FACS; Ekman and Rosenberg,
particular) neglect to measure emotion because current mea-               1997) requires hours of effort by trained technicians or pro-
sures are either expensive, tedious, or inaccurate. Conse-                hibitively expensive proprietary software. As the demand for
quently, many standard practices in the field have turned to              ecological experimentation increases, so too does the volume
indirect measures of affect. One prevalent example is the as-             of video data for researchers (or more often their students)
sociation of infant looking time with vastly different emotions           to scrutinize and label, frame by frame. The relatively con-
depending on the researchers’ theoretical stance, including               strained problems of identifying, labeling and categorizing
preference (e.g., to positive emotional expressions; LaBar-               facial features over thousands of datapoints is a prime oppor-
bera, Izard, Vietze, and Parisi, 1976), interest (e.g., to ani-           tunity for a machine learning solution.
mate stimuli; Csibra, 2008), or surprise (e.g., to violations of             Advances in data science and machine learning offer an
belief; Baillargeon, Scott, and He, 2010). Another example                affordable and accurate measure of participant emotional
is the notable lack of emotional state measures in studies on             state using only filmed recordings. Computer scientists have
attention, learning and memory, even though the field has ac-             demonstrated the uncanny accuracy of basic algorithms to
knowledged the impact of emotion on these functions for over              classify highly controlled emotional images (e.g., Cohn, Zlo-
50 years (Easterbrook, 1959). Rather than inferring the role              chower, Lien, and Kanade, 1999; Littlewort, Bartlett, Fasel,
of emotions, future studies could measure it efficiently using            Susskind, and Movellan, 2006), and recent efforts to catego-
facial recognition algorithms. The advent of elegant machine-             rize emotion “in the wild” (Yao, Shao, Ma, & Chen, 2015)
learning algorithms offers a free, reliable, non-invasive and             have the problem to unsupervised learning for less controlled
easily implemented method that may be able to measure af-                 images. Here we are interested in applying machine-learning
fective state in real-world settings at levels that meet or ex-           to the varied contexts typical of cognitive science experi-
ceed trained human raters.                                                ments. Laboratory settings offer more control than streaming
   Here, we demonstrate automatic classification of emo-                  surveillance footage, but less control than posed photography.
tional faces using three different datasets. We concentrate on            We approximate this by comparing human performance to
young populations, as developmental science is particularly               an algorithm trained on three datasets with unique attributes,
interested and constrained by hand-coding, but also demon-                each of which could reasonably be applied to experimen-
strate that methods are easily extended to adult populations.             tal settings. We demonstrate the need for large amounts of
We also concentrate on relatively simple machine learning                 highly varied data to consistently and accurately categorize
algorithms that may be flexibly implemented for a variety of              human facial expressions. Furthermore, we present a model
                                                                      1574

trained on images that vary by age, ethnicity, gender and pho-        the labels most often generated by the raters. This variability
tographic conditions that nonetheless approaches human rater          makes the CAFE useful to compare to computer models, as
performance. It is worth noting that our goal is not to model         we can test the model’s success on “difficult” or “easy” faces
human performance or develop new machine-learning meth-               compared to human performance.
ods; rather, we wish to explore the kinds of data required to            The CK+ dataset. One method of producing cleaner data
approximate human-level emotion coding for cognitive ex-              for machine learning is to extract images with tightly con-
periments.                                                            trolled visual features. Although our focus is on develop-
   We begin by introducing several datasets with unique at-           mental populations, the CAFE set is the only publicly avail-
tributes of in interest to different applications. We define          able database of children’s faces. We therefore included a
the methods required to use open-source libraries to create           dataset comprised only of extensively vetted faces: the Cohn-
a simple machine-learning classifier. Applying this model             Kanade AU-Coded Expression Database, Version 2 (Lucey
to the datasets reveals that highly controlled training stimuli       et al., 2010). The Cohn-Kande dataset (CK+) consists of
are more easily categorized, and that noisier, real-world stim-       over 11,000 image sequences of 120 adult models as they
uli are unsurprisingly more difficult. We discuss the trade-          changed from neutral resting faces to peak emotional ex-
off between accuracy and generality by amalgamating three             pression from 7 categories (the same as the CAFE expres-
datasets into a comprehensive model that is more robust to            sions, with the addition of contempt). It is currently unknown
noisy input.                                                          whether training images from an adult dataset would improve
                                                                      performance on child facial categorization. Given the abun-
                     General Methods                                  dance of adult datasets, any improvement on child facial clas-
Databases                                                             sification would expand the available training data for future
                                                                      models.
Machine learning requires a large number of samples for re-
liable classification. However, the type of input can greatly            Machine vision researchers often use the CK+ dataset as
affect the generalizability of the model. For instance, a model       a benchmark for performance of an algorithm (e.g., Little-
trained on only children’s faces might not perform well with          wort et al., 2006). For our purposes, training the algorithm
adult faces. Likewise, a model trained on highly controlled           on the CK+ dataset allows us to test the best case scenario
images might not perform well on naturalistic stimuli. We             of facial classification, as it contains only highly controlled
drew from three sources for training, each with a particular          images with little cross-category variability. This comes at
strength that could improve performance in a given setting.           a cost to ecological validity, as all faces are of adults aged
A “face” was defined as a front-facing image containing two           18 to 30, and less than 18% were minorities. Additionally,
eyes and no obstructions to facial features.                          whereas items in the CAFE set were validated using subjec-
   The CAFE dataset. Most face databases for psychology               tive judgments from adult raters, the peak faces from the CK+
and machine learning focus on adults. However, a recent ef-           database were validated using the Facial Action Coding Sys-
fort by LoBue and Thrasher (2015) documented the facial ex-           tem (FACS). Briefly, FACS categorizes faces into emotional
pressions of young children for applications in developmental         categories using reliable expressions of specific facial motor
psychology. Although stimulus sets exist for older children           groups, or action units (Ekman & Rosenberg, 1997). Lucey
(aged 8-17, Egger et al., 2011) and adults (Cohn et al., 1999),       et al. (2010) validated the emotional labels given to each peak
the Child Affective Facial Expression (CAFE) set is the only          face using a linear support vector classifier trained on action
collection featuring young children. The set contains pho-            units. Selecting these initial and peak faces generated 308
tographs of 154 racially and ethnically diverse 2- to 8-year-         emotional expressions from the 6 emotional categories with a
old children posing for six emotional facial expressions (an-         corresponding neutral face for each. A single neutral face was
gry, disgusted, fearful, happy, sad, and surprised) as well as a      randomly selected for each participant to prevent over-fitting
resting neutral expression. Facial expressions were further la-       of neutral faces, leaving 120 neutral faces and a total of 428
beled for “open” or “closed” mouths for angry, fearful, happy,        faces.
sad and neutral faces. Disgust expressions were uniquely                 Google image search by category. The CAFE and CK+
coded as with or without a protruding tongue. The CAFE                sets feature images taken under ideal lighting and camera
set features multiple emotional faces for each child, though          positions, with labels that have been rigorously validated.
not every child demonstrated every subcategory of emotion.            However, real-world use of a facial expression classifier
Altogether, the set contains 1192 images. Children’s facial           would necessarily include less-than-ideal photographic cir-
features offer a great deal of variability, and the ethnic diver-     cumstances. To approximate the noisiness of real-world stim-
sity of the participant sample approximates the demographics          uli, we extracted images from a Google image search with the
of the United States.                                                 search term “X child face”, where X was an emotional cate-
   The CAFE set was validated by a group of 100 independent           gory of interest. Images were selected by research assistants,
adult raters, who viewed each image and labeled it with one           with the criteria that each image featured an individual human
of the seven emotions. Importantly, the images are labeled            child’s face (approximately aged 3-10) without obstruction on
by the expression the child was asked to give, and not by             the face area.
                                                                  1575

   Research assistants terminated the collection of images if
the total number of collected images exceeded 100 exemplars
or if the search returned more than 20 images in a row without
a viable exemplar. This produced only 2 neutral exemplars,
so an additional search was conducted using “calm” and “se-
rious” as additional terms for neutral. This produced a total
of 609 faces from all seven categories
Face Extraction
Images from all datasets contained extraneous information,
including body parts (e.g., hair and shoulders) or photo-
graphic artifacts (e.g., serial numbers in the CAFE and CK+
datasets, miscellaneous objects in the Google dataset). All
images were passed through a facial recognition algorithm 1
and reduced to a 300 x 300 pixel rectangle centered on the
identified face.
                                                                     Figure 1: Validation of the CAFE and Google datasets by “online”
   Facial recognition was conducted using Haar Feature-              human raters via Mechanical Turk vs in-person, “live” raters.
based Cascade Classifiers. Generally, the cascade classifier
breaks an image into clusters of pixels and excludes clusters
that do not resemble facial features from later analysis. The           We first confirmed that Mechanical Turk raters performed
process is then repeated until only clusters that resemble fa-       comparably to live human raters (Figure 1). Overall accu-
cial features remain. For methodological details and valida-         racy of ratings for CAFE set images between live and online
tion, see Viola and Jones (2001). The end result is a compu-         raters was significantly positively correlated (r = .783,t412 =
tationally efficient method for identifying facial regions.          25.53, p < .0001). Furthermore, accuracy of online and live
   A trained cascade classifier was obtained from the                raters were further correlated for all emotional categories
OpenCV website (Itseez, 2016). All faces from all datasets           (lowest rsurprise = .410,t55 = 3.340, p = .001; highest rsad =
were passed through the classifier and cropped. A member             .820,t57 = 10.807, p < .0001), with the exception of happy
of the research team then examined each extracted face and           expressions (r = .22,t57 = 1.742, p = .08), which likely had
discarded false positives on non-face objects. This method           a reduced correlation due to ceiling effects. These results sup-
produced 1187 faces (5 removed) from the CAFE set, 427               port the use of Mechanical Turk raters to validate the Google
faces (1 removed) from the CK+ dataset, and 477 faces (132           dataset.
removed) from the Google dataset.                                       We then confirmed that the human categorization perfor-
                                                                     mance for the novel Google images were comparable to the
Human Validation                                                     CAFE images. A two-way ANOVA modeling mean online
Image category labels for the CAFE and CK+ datasets were             rater categorization performance by dataset and emotional la-
validated using adult human raters. To ensure that all im-           bel found significant differences in categorization accuracy
ages were of equal quality when training the classifier, we          by dataset (F1,877 = 8.753, p = .003, η2 = .086) and emotion
validated the Google dataset using 87 adult human raters re-         (F6,877 = 89.504, p < .0001, η2 = .881). There was also a
cruited via Amazon Mechanical Turk. This was necessary               significant interaction (F6,877 = 2.366, p = .028, η2 = .023),
to compare classifier and human performance for images that          indicating no significant difference between online and live
more closely resemble the real world.                                raters for angry, happy, neutral, sad and surprised expres-
                                                                     sions (highest t877 [sad] = .9694, p = .167), and significantly
   Raters (median age: 31; 61 females, 41 college graduates,
                                                                     poorer performance by online raters for disgusted and fear-
28 parents) labeled a representative subset of the Google faces
                                                                     ful expressions (lowest t877 [disgust] = 2.134, p < .017, d =
(between 47 and 49 images, evenly distributed across cate-
                                                                     .144). These results suggest that the Google dataset is compa-
gories) into one of seven emotional categories. Raters also
                                                                     rable to the CAFE set for five of seven emotions and follows
labeled a subset of the CAFE dataset (42 images, 6 from each
                                                                     the same trends for sadness and disgust, making the Google
category). The CAFE faces were evenly distributed by diffi-
                                                                     set ideal for testing an algorithm on ecological images.
culty according to the CAFE set’s previous validation metrics.
This was done to compare the performance of in-person (live)         The Machine Learning Algorithm
and online raters.
                                                                     Whole research communities are dedicated to the application
    1 The facial recognition algorithm was adapted from the Open     of machine learning to emotional recognition, using both su-
Source Computer Vision Library (OpenCV v2.4.13; Bradski, 2000)       pervised and unsupervised algorithms for still image, video,
and programmed in Python 2.7. OpenCV is an open source library
that provides a common infrastructure to machine vision applica-     audio or multimedia data (e.g., the EmotiW challenge at the
tions in academia and industry.                                      annual ACM ICMI conference). Although there have been re-
                                                                 1576

cent successes modeling dynamic features (Littlewort et al.,
2006), We opted to analyze still images to simplify imple-
mentation for the target demographic of psychologists, and
used a supervised learning approach due to the relatively few
training images available. We therefore selected a Support
Vector Machine (SVM) algorithm, as SVMs are ideal for use
by non-computer scientists for their simple implementation
and ease of interpretation. SVMs have a long and success-
ful history in image recognition (Tong & Chang, 2001), par-
ticularly with facial recognition (Osuna, Freund, & Girosit,
1997). An SVM is a type of supervised learning in which
the algorithm identifies the optimal boundary between labeled
data points. The boundary is defined by the support vec-
tors, the subset of the data that define the boundary between        Figure 2: Classification of test images improved as a function of
classes. This boundary can then be used to infer, based on ob-       training data. The top line denotes average human accuracy across
                                                                     live and online human validation; the lower line denotes chance.
served features, which category a novel image (or novel im-
ages) best fit. We recommend Cristianini and Shawe-Taylor
(2000) for an in-depth overview.                                     may be more likely to categorize the test image by the stable
   We trained an SVM for each dataset, as well as on a com-          facial similarities of Child A than to the desired similarities
prehensive dataset containing training images from all three         in emotional expression of Child B. One solution might be
datasets. We used an open-source SVM classifier available            to randomly select only one face per child participant in the
through the scikit-learn database (Pedregosa et al., 2011).          CAFE and CK+ sets. This is not ideal, as it would greatly
SVMs require the user to choose a similarity function, called        reduce the training set. Instead, we trained the SVM on all
a kernel, that governs the complexity of the possible bound-         faces for a proportion of participants and tested on all faces
aries between classes. There are many standard options for           for the withheld subset of participants. This eliminated the
kernels including linear, polynomial, and radial basis func-         possibility that a test image might be paired with a training
tion (RBF; aka Gaussian). Each computes similarity some-             image of the same child, while also maximizing the richness
what differently and they consequently differ in the kinds of        of the dataset.
classification boundaries they admit; as one might expect, a            Another issue unique to emotional classification is the
linear kernel gives a linear boundary and polynomial and RBF         breadth of expression. For instance, the CAFE set makes a
kernels allow non-linear boundaries. While these non-linear          distinction between faces with open and closed mouths, and
methods offer increased expressiveness, they also increase the       both the CAFE and Google sets contain exemplars that were
risk of overfitting.                                                 difficult to label by human raters. We opted to include all in-
   Additionally, there are two parameters that must be set and       stances under the basic emotional category, regardless of sub-
affect outcomes: the regularization parameter C and kernel           ordinate labels or validation score, so as to maximize training
coefficient γ. C is a regularization parameter which, when           data with the greatest possible variation between features.
set to higher values, allows more complex solutions. The
kernel coefficients γ affect the influence of specific specific                                   Results
supports. When γ is small, a support has broad influence             The algorithm was trained on incrementally increasing sizes
on classification decisions, whereas when γ is large the in-         of training data from all three datasets individually and a com-
fluence of each support is localized to the area near the            prehensive dataset trained from all sources. Each sample size
supporting data point. A grid search for kernels, {linear,           by dataset was repeated 40 times with a new random selection
polynomial, radial basis function (RBF)}, penalty param-             of training and test data to approximate error.
eters, C = (.001, .01, .1, 1, 10, 100), and kernel coefficients,        The primary goal of these analyses was to demonstrate
γ = (.0001, .001, .01, .1, 1, 10, 100, 1000), yielded the optimal    machine-learning categorization on different training data
combination of a polynomial kernel with a C = 1 and a                versus human raters. Figure 2 illustrates overall perfor-
γ = .0001, as assessed via cross-validation.                         mance by sample size for each of the datasets. An ANCOVA
   Although SVM classifiers are often used for facial recogni-       modeling dataset by training sample size revealed a signif-
tion, training a classifier for emotional features offers unique     icant effect of dataset (F3,1512 = 1412.39, p < .0001, η2 =
problems. The classifier might divide faces by other similar-        .540) and training sample size (F1,1512 = 1103.76, p <
ities; emotional expressions are but a subset of the consider-       .0001, η2 = .422), with a significant interaction (F3,1512 =
able variability between faces. For example, say a classifier        96.91, p = .0001, η2 = .037). Paired comparisons revealed
is trained on two stimuli: Child A with an angry expression          that performance on within-dataset models increased faster
and Child B with a happy expression. When presented with a           than the comprehensive dataset as a function of training
test image of Child A making a happy expression, the SVM             size (CAFE: t1512 = 12.246, p < .0001, d = .629; Google:
                                                                 1577

Figure 3: Classification by source of testing data. Accuracy on un-     Figure 4: The comprehensive model from all three datasets paral-
curated Google images improved with the comprehensive model.            leled human performance.
t1512 = 3.705, p < .0001, d = .191; CK+: t1512 = 11.886, p <            els for each individual dataset (the within-set models). This
.0001, d = .611). Altogether, performance on all datasets im-           comparison demonstrates how the addition of training im-
proved as a function of training data, but performance on               ages outside the dataset improves performance. An ANOVA
within-dataset models increased faster than the comprehen-              comparing accuracy by model type (within-dataset or com-
sive model.                                                             prehensive) and source of test images revealed a no effect of
   The bold dotted line on Figure 2 denotes average human               model (F1,234 = 0.001, p = .973, η2 < .001) but a significant
categorization performance for the CAFE and Google sets,                effect of test image source (F2,234 = 1175.71, p < .0001, η2 =
although it should be noted that no item-wise validation met-           .961) as well as a significant interaction (F3,234 = 47.13, p <
rics were available for the CK+ set. T-tests revealed that              .0001, η2 = .039). Accuracy for the comprehensive model
maximum training sizes on the CK+ dataset exceeded hu-                  was significantly greater than the within-set model for Google
man performance (t40 = 6.00, p < .0001, d = 1.90). Clas-                (t234 = 6.03, p < .0001, d = .792), not significantly differ-
sifier performance was significantly below human perfor-                ent for CK+ test images (t234 = 1.09, p = .140, d = .142)
mance for the CAFE (t40 = −3.62, p = .0006, d = 1.145),                 and significantly less for CAFE test images (t234 = 6.50, p <
Google (t40 = −38.65, p < .0001, d = 9.92), and compre-                 .0001, d = .849). Comparing Figure 2 and Figure 3, the over-
hensive datasets (t40 = −39.51, p < .0001, d = 10.68). In-              all performance deficit of the comprehensive model relative
terestingly, human performance was not significantly corre-             to the CK+ and CAFE sets in Figure 2 are due to the high
lated with classifier performance at maximum training sam-              proportion of training images in the comprehensive model
ple size for any dataset (CK+: r = −.031, p = .837; CAFE:               that come from the CAFE set (57.9%). Importantly, these
r = .260, p = .105; Google: r = −.030, p = .869; Compre-                results show that a comprehensive dataset from multiple cu-
hensive: r = .240, p = .135), suggesting that the basis on              rated sources improves classification of more realistic and un-
which categorization decisions were made by the algorithm               controlled Google set.
differed from human judgments.                                             Finally, it is worthwhile to see how a comprehensive
   It is crucial for future applications that a classifier not only     dataset compares to human raters. Figure 4 compares hu-
categorizes within a training dataset, but can also general-            man and comprehensive model performance by emotional
ize beyond that set. A common method of gauging gener-                  category. An ANOVA modeling emotion by rating type
alizability is to train models for each dataset and test on the         (human vs the comprehensive algorithm) revealed than hu-
other datasets. However, all of the present datasets, partic-           man raters were significantly more accurate than the clas-
ularly the CK+ dataset, have unequal numbers of exemplars               sifier (F1,546 = 1450.24, p < .0001, η2 = .567). There were
for each emotional category. As classifier performance is di-           significant differences by emotion (F6,546 = 1021.81, p <
rectly related to the amount of training data, we would have            .0001, η2 = .400) as well as a significant interaction (F6,546 =
to hold training data constant to the minimum possible value            84.51, p < .0001, η2 = .048).
across all emotional categories and datasets, which in this                Overall, these results suggests that the comprehensive
case would be only 25 exemplars per category (the number of             dataset follows similar trends as human raters. Sampling
exemplars for “fear” in the CK+ dataset), for a training set of         from multiple datasets improves performance on the highly
only 175 images. Instead, we tested how well a single com-              uncontrolled Google stimuli and the highly controlled CK+
prehensive model performs against maximally trained mod-                stimuli; although performance drops for the CAFE stimuli, it
                                                                    1578

likely stems from the high proportion of images in the com-                                      References
prehensive dataset that are sampled from the CAFE. These             Baillargeon, R., Scott, R. M., & He, Z. (2010). False-belief under-
results suggest that models comprised of more training data                 standing in infants. Trends in cognitive sciences, 14(3), 110–
                                                                            118.
may approach human performance on varied images.                     Bradski, G. (2000). The open source computer vision library. Dr.
                                                                            Dobb’s Journal of Software Tools.
                         Discussion                                  Cohn, J. F., Zlochower, A. J., Lien, J., & Kanade, T. (1999). Auto-
                                                                            mated face analysis by feature point tracking has high con-
Psychological theories emphasize the causal role of emotions                current validity with manual facs coding. Psychophysiology,
across a variety of phenomena including learning, memory,                   36(1), 35–43.
and attention. However, emotion is rarely measured in such           Cristianini, N. & Shawe-Taylor, J. (2000). An introduction to sup-
                                                                            port vector machines and other kernel-based learning meth-
studies due to the cost, inefficiency and tediousness of mod-               ods. Cambridge University Press.
ern methods. Widely available and accessible methods for             Csibra, G. (2008). Goal attribution to inanimate agents by 6.5-
coding emotion would greatly reduce barriers to advancing                   month-old infants. Cognition, 107(2), 705–717.
                                                                     Easterbrook, J. A. (1959). The effect of emotion on cue utiliza-
theory by allowing dense measurement of emotion in contin-                  tion and the organization of behavior. Psychological Review,
uous time. Using a standard machine learning method, we                     66(3), 183.
explored the types of training data one would need to ap-            Egger, H. L., Pine, D. S., Nelson, E., Leibenluft, E., Ernst, M., Tow-
                                                                            bin, K. E., & Angold, A. (2011). The NIMH child emotional
proach human-level coding of the big 7 emotional categories.                faces picture set (NIMH-ChEFS): a new set of children’s
These included curated image sets developed by psychologi-                  facial emotion stimuli. International Journal of Methods in
cal researchers and uncontrolled images drawn from Google                   Psychiatric Research, 20(3), 145–156.
                                                                     Ekman, P. & Friesen, W. V. (1971). Constants across cultures in the
with crowdsourced labels. We find that comprehensive mod-                   face and emotion. Journal of Personality and Social Psychol-
els generated from multiple datasets improve classification of              ogy, 17(2), 124.
uncurated images. Overall model performance follows the              Ekman, P. & Rosenberg, E. L. (1997). What the face reveals: Basic
                                                                            and applied studies of spontaneous expression using the Fa-
same trends as human performance, and the inclusion of addi-                cial Action Coding System (FACS). Oxford University Press.
tional datasets promises to further approach human accuracy.         Itseez. (2016). Open Source Computer Vision Library. https : / /
   Cognitive science, and developmental science in particu-                 github.com/itseez/opencv.
                                                                     LaBarbera, J., Izard, C., Vietze, P., & Parisi, S. (1976). Four-and six-
lar, are greatly limited by the methods of the day. A typical               month-old infants’ visual responses to joy, anger, and neutral
developmental experiment takes place with one child and one                 expressions. Child Development, 47(2), 535–538.
experimenter for fifteen minutes. Such tight controls have led       Littlewort, G., Bartlett, M. S., Fasel, I., Susskind, J., & Movellan,
                                                                            J. (2006). Dynamics of facial expression extracted automati-
to important insights at the cost of ecological validity. The               cally from video. Image and Vision Computing, 24(6), 615–
past 20 years have seen incredible improvements to compu-                   625.
tational theory and processing power that permit a more flex-        LoBue, V. & Thrasher, C. (2015). The child affective facial ex-
                                                                            pression (CAFE) set: Validity and reliability from untrained
ible study of human behavior. With machine-learning meth-                   adults. Frontiers in Psychology, 5, 1532.
ods, scientists are no longer bound to brief interventions or        Lucey, P., Cohn, J. F., Kanade, T., Saragih, J., Ambadar, Z., &
constrained to discrete conditions. Rather, we can now con-                 Matthews, I. (2010). The extended Cohn-Kanade dataset
                                                                            (CK+): A complete dataset for action unit and emotion-
tinuously monitor affect and behavior as a response to the real             specified expression. In IEEE conference on computer vision
world. Instead of inferring surprise from an infant’s looking               and pattern recognition (pp. 94–101).
times, these models provide a method to measure a reliable           Osuna, E., Freund, R., & Girosit, F. (1997). Training support vec-
                                                                            tor machines: An application to face detection. In IEEE com-
indicator of emotion. Instead of assuming a role of affect in               puter society conference on computer vision and pattern
student outcomes, we can incorporate emotional expression                   recognition (pp. 130–136).
with an intervention in real time.                                   Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B.,
                                                                            Grisel, O., . . . Duchesnay, E. (2011). Scikit-learn: Machine
   This paper represents an effort toward integrating compu-                learning in Python. Journal of Machine Learning Research,
tational methods with cognitive science with the goal of ac-                12, 2825–2830.
tively measuring all features that support cognition. For now,       Pessoa, L. (2008). On the relationship between emotion and cogni-
                                                                            tion. Nature Reviews Neuroscience, 9(2), 148–158.
we have demonstrated the feasibility of using publicly avail-        Picard, R. W., Vyzas, E., & Healey, J. (2001). Toward machine emo-
able software and data to code images in minutes rather than                tional intelligence: analysis of affective physiological state.
days. We have not yet reached human-level performance, but                  IEEE transactions on pattern analysis and machine intelli-
                                                                            gence, 23(10), 1175–1191.
we have shown that the curated datasets that have traditionally      Tong, S. & Chang, E. (2001). Support vector machine active learning
been collected improve performance over training on natural-                for image retrieval. In The 9th ACM international conference
istic uncontrolled images. This marks the first step towards                on multimedia (pp. 107–118).
                                                                     Viola, P. & Jones, M. (2001). Rapid object detection using a boosted
building theories that explain how emotion interacts with cog-              cascade of simple features. In Proceedings of the IEEE con-
nition in real-world learning scenarios.                                    ference on computer vision and pattern recognition (Vol. 1).
                                                                     Yao, A., Shao, J., Ma, N., & Chen, Y. (2015). Capturing au-aware fa-
                     Acknowledgments                                        cial features and their latent relations for emotion recognition
                                                                            in the wild. In Proceedings of the 2015 acm on international
This research was supported in part by NSF grant CISE-1623486 to            conference on multimodal interaction (pp. 451–458). ACM.
L.B., V.L., and P.S.
                                                                 1579

