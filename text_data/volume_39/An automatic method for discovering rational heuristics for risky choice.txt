          An automatic method for discovering rational heuristics for risky choice
                                             Falk Lieder1 (falk.lieder@berkeley.edu)
                                             Paul M. Krueger1 (pmk@berkeley.edu)
                                       Thomas L. Griffiths (tom griffiths@berkeley.edu)
                       Department of Psychology,University of California Berkeley, Berkeley, CA 94720 USA
                                                      1
                                                        These authors contributed equally.
                               Abstract                                   ical estimation (Lieder, Griffiths, & Goodman, 2012), avail-
                                                                          ability biases (Lieder, Hsu, & Griffiths, 2014), and strategy
   What is the optimal way to make a decision given that your
   time is limited and your cognitive resources are bounded? To           selection (Lieder, Plunkett, et al., 2014). However, this ap-
   answer this question, we formalized the bounded optimal de-            proach has not been applied to the domain in which heuristics
   cision process as the solution to a meta-level Markov deci-            have perhaps been studied in greatest detail: multi-alternative
   sion process whose actions are costly computations. We ap-
   proximated the optimal solution and evaluated its predictions          risky choice. Work on risky choice suggests that people adap-
   against human choice behavior in the Mouselab paradigm,                tively switch between multiple different strategies depend-
   which is widely used to study decision strategies. Our compu-          ing on how much time is available and whether one of the
   tational method rediscovered well-known heuristic strategies
   and the conditions under which they are used, as well as novel         outcomes is much more likely than the others (Payne et al.,
   heuristics. A Mouselab experiment confirmed our model’s                1988). Yet, it remains unclear how people’s decision pro-
   main predictions. These findings are a proof-of-concept that           cesses compare to resource-rational behavior.
   optimal cognitive strategies can be automatically derived as the
   rational use of finite time and bounded cognitive resources.              To answer these questions, we model the decision process
   Keywords: Decision-Making; Heuristics; Bounded Rational-
                                                                          as a sequence of costly computations and formalize the opti-
   ity; Strategy Selection; Rational Metareasoning                        mal decision process as the solution to a meta-level Markov
                                                                          decision process. We combine this theory with an algorithm
                            Introduction                                  for approximating the optimal solution to create a computa-
                                                                          tional method that can automatically derive optimal cogni-
Some situations require us to decide quickly whereas others
                                                                          tive strategies. These rational heuristics can be interpreted
call for careful consideration of all available options and po-
                                                                          as a fair normative standard for human decision making that
tential consequences. People seem to master this challenge
                                                                          takes into account that people’s time is costly and that their
by choosing adaptively from a toolbox of diverse decision
                                                                          cognitive resources are bounded. We are optimistic that this
strategies (Payne, Bettman, & Johnson, 1988; Gigerenzer &
                                                                          novel approach will lead to new insights about how decision-
Selten, 2002). This toolbox is assumed to include fast-and-
                                                                          makers cope with limited time and bounded computational
frugal heuristics (Gigerenzer & Goldstein, 1996) as well as
                                                                          resources, and advance the debate about human rationality.
slower and more effortful strategies. Fast-and-frugal heuris-
                                                                             We illustrate our approach in multi-alternative risky choice
tics include Take-The-Best (TTB), which chooses the alterna-
                                                                          and test its predictions using the Mouselab paradigm that
tive that is favored by the most predictive attribute and ignores
                                                                          is widely used to study decision strategies (Johnson, Payne,
all other attributes, satisficing (SAT) (Simon, 1956), which
                                                                          Bettman, & Schkade, 1989). Two known heuristics, TTB
chooses the first alternative whose expected value exceeds
                                                                          and random choice, emerged from our theory as resource-
some threshold, and random choice; slower strategies include
                                                                          rational strategies for low-stakes decisions with high and low
the Weighted-Additive Strategy (WADD), which computes
                                                                          dispersion of their outcome probabilities, respectively. In ad-
all gambles’ expected values based on all possible payoffs.
                                                                          dition, our computational method discovered a novel heuristic
Except for WADD, all of these strategies are heuristics: they
                                                                          that combines TTB with satisficing. Our experiment demon-
solve some problems very efficiently but err on others.
                                                                          strated that people do indeed use the newly discovered heuris-
   The systematic errors that result from people’s use of
                                                                          tic and confirmed our rational model’s predictions of when
heuristics are inconsistent with classic notions of rationality
                                                                          people use which strategy: people used simple heuristics
such as logic, probability theory, and expected utility theory
                                                                          more frequently when the stakes were low, employed fast-
(Tversky & Kahneman, 1974). Making good decisions is re-
                                                                          and-frugal heuristics less frequently when all outcomes were
markably constrained: decisions have to be made in a finite
                                                                          almost equally likely (low dispersion), and invested more
amount of time, people’s cognitive resources are limited, and
                                                                          time and effort when the stakes were high. This is the first
maximizing expected utility entails intractable computational
                                                                          demonstration that rational meta-reasoning can be used to au-
problems. This makes expected utility theory an unrealisti-
                                                                          tomatically discover decision strategies used by people.
cally high bar for human rationality. According to a more
realistic normative standard, people should decide in a way
                                                                                                   Background
that makes the best possible use of their limited cognitive re-
sources (Griffiths, Lieder, & Goodman, 2015). Previous re-                We will formulate our theory using the mathematical frame-
search has applied this resource-rational approach to numer-              works of Markov decision processes, bounded optimality, and
                                                                      742

rational metareasoning, introduced in this section.                      takes action. The computations C do not yield any external
                                                                         reward. Their only effect is to update the agent’s beliefs.
Markov Decision Processes                                                Hence, the meta-level reward for performing a computation
Each sequential decision problem can be modeled as a                     c ∈ C is rmeta (bt , c) = −cost(c). By contrast, terminating de-
Markov Decision Process (MDP)                                            liberation and taking action (⊥) does not update the agent’s
                                                                         belief. Instead, its value lies in the anticipated reward for tak-
                      M = (S , A , T, γ, r, P0 ) ,              (1)      ing action, that is
where S is the set of states, A is the set of actions, T (s, a, s0 )                                                        (µ)
                                                                                               rmeta (bt , ⊥) = arg maxa bt (a),             (5)
is the probability that the agent will transition from state s to
state s0 if it takes action a, 0 ≤ γ ≤ 1 is the discount factor                        (µ)
                                                                         where bt (a) is the expected reward of taking action a ac-
on future rewards, r(s, a, s0 ) is the reward generated by this          cording to the belief bt .
transition, and P0 is the probability distribution of the initial
state S0 (Sutton & Barto, 1998). A policy π : S 7→ A speci-              Adaptive strategy selection in risky choice
fies which action to take in each of the states. The expected            Consistent with rational metareasoning, people flexibly adapt
sum of discounted rewards that a policy π will generate in the           their decision processes to the structure of the problem they
MDP M starting from a state s is known as its value function             face. Concretely, Payne et al. (1988) found that people use
                         "
                           ∞
                                                        #                fast-and-frugal heuristics, like TTB, more frequently when
                                                                         they are under time pressure and when one outcome is much
             VMπ (s) = E   ∑ γt · r (St , π(St ), St+1 )  .     (2)
                                                                         more likely than the others. In this research, participants were
                          t=0
                                                                         given the choice between gambles g1 , · · · , gn . Each gamble
The optimal policy π?M maximizes the expected sum of dis-                was defined by the payoffs it assigns to each of four possible
counted rewards, that is                                                 outcome whose probabilities are known (P(O)). Participants
                            "
                              ∞
                                                            #            could inspect a payoff matrix Vo,g with one row for each out-
          π?M = arg max E     ∑ γt · r (St , π(St ), St+1 )   . (3)      come o and one column for each gamble g. Critically, each
                      π
                             t=0                                         payoff is only revealed when the participant clicks on the cor-
                                                                         responding cell of the payoff matrix using a mouse; this task
Bounded optimality and rational metareasoning                            is hence referred to as the Mouselab paradigm (see Figure 1).
People and robots have to make decisions in a limited amount                The adaptiveness of people’s strategy choices in the
of time and with bounded cognitive resources. Given that                 Mouselab paradigm suggests that their decision processes are
these resources are scarce, which strategy should a decision-            efficient and effective. But it is difficult to test whether they
maker employ to use its resources most effectively? The                  are optimal, because it is unclear what it means to decide op-
theory of bounded optimality and rational metareasoning                  timally when one’s time is valuable and one’s cognitive re-
(Russell & Wefald, 1991; Russell & Subramanian, 1995) was                sources are limited. To clarify this, the following section de-
developed to answer this question for rational agents with               velops a normative theory of resource-bounded decision mak-
limited performance hardware. It frames this problem as se-              ing in the Mouselab paradigm.
lecting computations so as to maximize the sum of the re-
wards of resulting decisions minus the costs of the computa-                          Boundedly-optimal decision-making
tions involved.                                                          To model the meta-decision problem posed by the Mouse-
   Concretely, the problem of choosing computations opti-                lab task, we characterize the decision-maker’s belief state
mally can be formulated as a meta-level MDP (Hay, Russell,               bt by probability distributions on the expected values
Tolpin, & Shimony, 2012). A meta-level MDP                               e1 = E[vO,g1 ], · · · en = E[vO,gn ] of the n available gambles
                                                                         g1 , · · · , gn . Furthermore, we assume that for each element vo,g
                    Mmeta = (B , C , Tmeta , rmeta )            (4)
                                                                         of the payoff matrix V there is one computation co,g that in-
is a Markov decision process whose actions C are cognitive               spects the payoff vo,g and updates the agent’s belief about the
operations, its states B represent the agent’s probabilistic be-         expected value of the inspected gamble according to Bayesian
liefs, and the transition function Tmeta models how cognitive            inference. Since the entries of the payoff matrix are drawn
operations change the agent’s beliefs. In addition to a set              from the normal distribution N (v̄, σ2v ), the resulting posterior
of computations C that update the agent’s belief, the cog-               distributions are also Gaussian. Hence, the decision-maker’s
nitive operations also include the meta-level action ⊥ that              belief state bt can be represented by bt = (bt,1 , · · · , bt,n ) with
terminates deliberation and translates the current belief into                                                
                                                                                                                (µ) (σ2 )
                                                                                                                          
action. The meta-level state bt encodes the agent’s proba-                                            bt,g = bt,g , bt,g ,                   (6)
bilistic beliefs about the domain it is reasoning about.The
                                                                                       (µ)       (σ2 )
meta-level reward function rmeta captures the cost of think-             where bt,g and bt,g are the mean and the variance of the
ing (Shugan, 1980) and the reward r the agent expects to re-             probability distribution on the expected value of gamble g of
ceive from the environment when it stops deliberating and                the belief state bt .
                                                                     743

                                              (1)   (1)           (t) (t)
    Given the set Ot of the indices (ko , kg ), · · · , (ko , kg ) of              unknown belief state resulting from performing computation
                                                                                                                                                       (µ)
the t observations made so far, the means and variances char-                      c in belief state bt and ER(bt ) = E[regret(arg maxg bt,g )|bt ]).
acterizing the decision-maker’s beliefs are given by                                  The weights w are learned by Bayesian linear regression of
              (µ)
                                                                                   the bootstrap estimate Q̂(b, c) of the meta-level value function
            bt,g =        ∑    p(o) · vo,g +      ∑     p(o) · v̄         (7)      onto the features f. The bootstrap estimator is
                       (o,g)∈O                      /O
                                               (o,g)∈
              (σ2 )                                                                          Q̂(bt , ct ) = rmeta (bt , ct ) + ŵt0 · f(bt+1 , ct+1 ),     (10)
            bt,g =          ∑   p(o)2 · σ2v .                             (8)
                             /O
                        (o,g)∈                                                     where ŵt is the posterior mean on the weights w given the ob-
                                                                                   servations from the first t trials, and f(bt+1 , ct+1 ) is the feature
The meta-level transition function T (bt , co,g , bt+1 ) encodes
                                                                                   vector characterizing the subsequent belief state bt+1 and the
the probability distribution on what the updated means and
                                                                                   computation ct+1 that will be selected in it.
variances will be given the observation of a payoff value Vo,g
                                                                                      Given the learned posterior distribution on the feature
sampled from N (v̄, σ2v ). The meta-level reward for perform-
                                                                                   weights w, the next computation c is selected by contextual
ing the computation co,g ∈ C encodes that acquiring and pro-
                                                                                   Thompson sampling (Agrawal & Goyal, 2013). Specifically,
cessing an additional piece of information is costly. We as-
                                                                                   to make the t th meta-decision, a weight vector w̃ is sampled
sume that the cost of all such computations is an unknown
                                                                                   from the posterior distribution of the weights given the series
constant λ. The meta-level reward for terminating delibera-
                                                           (µ)                     of meta-level states, selected computations, and the resulting
tion and taking action is rmeta (bt , ⊥) = maxg bt (g).                            value estimates experienced so far, that is
Approximating the optimal meta-level policy:
                                                                                   w̃ ∼ P(w|(b1 , c1 , Q̂(b1 , c1 )), · · · , (bk−1 , ck−1 , Q̂(bk−1 , ck−1 ))).
Bayesian value function approximation
Unfortunately, computing the optimal policy for the meta-                          The sampled weight vector w̃ is then used to predict the
level MDP defined above is intractable. However, it can be                         Q-values of each available computation c ∈ C according to
approximated using methods from reinforcement learning.                            Equation 9. Finally, the computation with the highest pre-
We initially used the semi-gradient SARSA algorithm (Sutton                        dicted Q-value is selected.
& Barto, 1998) with limited success. We therefore developed
                                                                                   Application to Mouselab experiment
a new algorithm that replaces the gradient descent component
of that algorithm by Bayesian linear regression.                                   As a proof of concept, we applied our approach to the Mouse-
    Our algorithm learns a linear approximation to the meta-                       lab experiment described below. The experiment comprises
level Q-function                                                                   50% high-stakes problems and 50% low-stakes problems.
                                                                                   Since participants are informed about the stakes, we learned
                    Qmeta (b, c) ≈ ∑ wk · fk (b, c),                      (9)      two separate policies for high-stakes and low-stakes prob-
                                       k                                           lems, respectively. Half of each of those problems had nearly
whose features f include a constant, features of the belief                        uniform outcome probabilities (“low dispersion”) and for the
state bt , and features of the computation ct . The features                       other half one outcome was much more likely than all others
of the belief state were the expected value of the maximum                         combined (“high dispersion”). The parameters of the simu-
                                                                                   lated environment were exactly equal to those of the experi-
of the gambles’ expected values (E [max              pg Eg |bt ]) and the          ment described below. Our model assumed that people play
decision-maker’s uncertainty about it ( Var[maxg Eg |bt ]).
                                                (µ)                                each game as if they receive the payoff of the selected gam-
The largest posterior mean (maxg bt,g ) and its associated                         ble. We estimated the cost per click to be about λ = 3 cents.
               q
                      (σ2 )                              (µ)                       This value was selected to roughly match the average number
uncertainty ( µt,g? where g? = arg maxg bt,g ), the second
largest posterior mean and the decision-maker’s uncertainty                        of acquisitions observed in the experiment.
about it, and the expected regret E [regret(g)|bt ] that the                          To approximate the optimal meta-decision policy for this
decision-maker would experience if they chose based on                             task, we ran our feature-based value function approximation
their current belief (where regret(g) = maxg Eg − maxg bt,g
                                                                           (µ)     method for 4000 low-stakes training trials and 4000 high-
                  (µ)    (σ)                                                       stakes training trials, respectively.
for Ei ∼ N (bt,i , bt,i ) for all gambles i). The features of
the computation co,g were its myopic value of computation                          Model predictions
(VOC(bt , co,g ); see Russell & Wefald, 1991), the current un-                     The meta-level MDP described above formalizes the costs
certainty about the expected value of the inspected gamble                         and benefits of acquiring and processing additional pieces of
   (σ)
(bt,g ), the probability of the inspected outcome, the differ-                     information: acquiring additional information can improve
ence between the largest posterior mean and the posterior                          the decision that will be taken later on but also incurs an
mean of the inspected outcome, a binary variable indicat-                          immediate cost. Hence, the optimal solution approximated
ing whether the computation acquired new information, and                          by our computational method executes a cognitive operation
the expected reduction in the expected regret ER(b) minus its                      or sequence of operations if and only if the resulting im-
cost (i.e. E [ER(Bt+1 )|bt , c] − ER(bt ) − λ, where Bt+1 is the                   provement in decision quality is larger than cost of those
                                                                               744

                                                                         curate than more effortful strategies. Our model used these
                                                                         heuristics for 100% of the low-stakes problems. But for high-
                                                                         stakes problems, the model never used any of these or other
                                                                         frugal strategies. Instead, the model typically inspected the
                                                                         vast majority of all cells (24.8/28 for low-dispersion prob-
                                                                         lems and 23.7/28 for high-dispersion problems). The few
                                                                         cells that it did not inspect were mostly the payoffs of less-
                                                                         likely outcomes of the best gamble when its inspected payoffs
                                                                         for the most likely outcome(s) were high enough to guarantee
                                                                         that it would be optimal.
Figure 1: The Mouselab paradigm, showing an example se-                     Third, our model predicts that when the stakes are high
quence of clicks generated by the SAT-TTB strategy, which                people should invest more time and effort (F(1, 396) =
was discovered through approximate rational metareasoning.               9886.8, p < 0.0001) to reap a higher fraction of the highest
                                                                         possible expected payoff (F(1, 339) = 135.24, p < 0.0001).
                                                                         This, too, is consistent with the rational speed-accuracy trade-
operations. Intuitively, this means that the decision process            off inherent in our theory. When the stakes were low the
prescribed by our model achieves the optimal tradeoff be-                model inspected only 4.3 payoffs on average and reaped only
tween decision quality versus decision time and mental ef-               87% of the possible reward; but when the stakes were high the
fort. This tradeoff depends on the stakes of the decision such           model inspected 24.3 of the 28 possible payoffs and reaped
that higher stakes usually warrant more deliberation. Like-              99% of the best expected payoff on average. In 97% of these
wise, since processing probable outcomes is more likely to               trials, the model achieved this near-maximal performance
improve the quality of the resulting decision than process-              while being more efficient and more frugal than the WADD
ing improbable outcomes, we expect our model to prioritize               strategy which it employed for only 3% of these problems.
probable outcomes over less probable outcomes—especially
in high-dispersion trials.
                                                                                 Experimental test of novel predictions
   Our computational method automatically discovered                     To test the predictions of our model, we conducted a new
strategies that people are known to use in the Mouselab                  Mouselab experiment that manipulated the stakes and disper-
paradigm as well as a novel strategy that has not been re-               sion of outcome probabilities within subjects in an identical
ported yet. Our method rediscovered TTB, WADD, and the                   manner to the model simulations.
random choice strategy. In addition, it discovered a new hy-             Methods
brid strategy that combines TTB with satisficing (SAT-TTB).
Like TTB, SAT-TTB inspects only the payoffs for the most                 Participants We recruited 200 participants on Amazon
probable outcome. But unlike TTB and like SAT, SAT-TTB                   Mechanical Turk. The experiment took about 30min. Partic-
terminates as soon as it finds a gamble whose payoff for the             ipants received a base pay of $1.50, and one of their twenty
most probable outcome is high enough. On average, this                   winnings was selected at random and awarded as a bonus to
value was about $0.15 when the payoffs ranged from $0.01 to              motivate them to take each trial seriously (avg. bonus $3.53).
$0.25 (i.e., low-stakes trials). Figure 1 illustrates this strategy.
   Furthermore, our model makes intuitive predictions about              Procedure Participants performed a variation of the
the contingency of people’s choice processes on stakes and               Mouselab task (Payne et al., 1988). Participants played a se-
outcome probabilities. First, our model predicts that people             ries of 20 games divided into two blocks. Figure 1 shows
should use fast-and-frugal heuristics more frequently in high-           a screenshot of one game. Every game began with a 4 × 7
dispersion trials. This is intuitively rational because high dis-        grid of occluded payoffs: there were seven gambles to choose
persion means that one outcome is much more likely than                  from (columns) and four possible outcomes (rows). The oc-
all others and fast-and-frugal heuristics ignore all outcomes            cluded value in each cell specified how much the gamble indi-
except for the most probable one(s). Concretely, our model               cated by its column would pay if the outcome indicated by its
generated TTB as the strategy of choice for 100% of the high-            row occurred. The outcome probabilities were described by
dispersion problems with low-stakes, but for low-dispersion              the number of balls of a given color in a bin of 100 balls, from
problems with low-stakes the model considered the random                 which the outcome would be drawn. For each trial, partici-
choice strategy to be optimal in the majority (56%) of cases;            pants were free to inspect any number of cells before select-
it used the SAT-TTB hybrid strategy for 24% of such trials,              ing a gamble, with no time limit. The value of each inspected
and it indicated the TTB strategy only for the remaining 20%.            cell remained visible onscreen for the duration of the trial.
   Second, our model predicts that people should use simple              Upon selecting a gamble, the resulting reward was displayed.
heuristics, like TTB, SAT-TTB, and random choice, primar-
ily when the stakes are low. This, too, is intuitively rational          Experimental design The experiment used a 2 × 2 within
because fast and frugal heuristics tend to be faster but less ac-        subjects design. Each block of ten trials was either low-stakes
                                                                     745

or high-stakes, with block order randomly counterbalanced                                                        Frequency
                                                                               Strategy      Total    HS-HD HS-LD LS-HD                  LS-LD
across participants. In games with low-stakes, the possible                    TTB           1012     392         64        449          107
outcomes ranged from $0.01 to $0.25, while in high-stakes                      SAT-TTB       412      68          54        140          150
games, outcomes ranged from $0.01 to $9.99. The payoffs                        Random        320      41          42        111          126
                                                                               TTB2          251      34          94        25           98
were drawn from a truncated normal distribution with mean                      WADD          178      33          84        19           42
rmax +rmin
     2      and standard deviation 0.3 · (rmax − rmin ). Within                SAT           89       14          22        23           30
each block, there were five low-dispersion trials and five high-             HS-HD = High-stakes, high-dispersion HS-LD = High-stakes, low-dispersion
                                                                             LS-HD = Low-stakes, high-dispersion LS-LD = Low-stakes, low-dispersion
dispersion, ordered randomly. In low-dispersion trials, the
probability of each of the four outcomes ranged from 0.1 to            Table 1: Frequency of strategy types for each type of trial.
0.4, whereas in high-dispersion trials, the probability of the
most likely outcome ranged from 0.85 to 0.97.
                                                                     tailed). Finally, consistent with our model’s third prediction,
                                                                     the frequency of the most effortful and most accurate strategy,
Strategy identification We identified six different decision
                                                                     WADD, increased with the stakes (χ2 (1) = 19.3, p < 0.0001).
strategies, in humans and in simulations, using the follow-
ing definitions: TTB was defined as inspecting all cells in the         Together, the strategies reported in Table 1 account for only
row corresponding to the most probable outcome and nothing           about half (48.6%) of all trials.To test our model’s predictions
else. SAT occurs when one gamble’s payoffs are inspected             on all of the trials, we quantified people’s decision style by
for all four outcomes, potentially followed by the inspection        four metrics introduced by Payne et al. (1988): the number of
of all outcomes of another gamble, and so on, but leaving            inspected cells (acquisitions), the proportion of those inspec-
at least one gamble unexamined. The hybrid strategy, SAT-            tions that pertained to the most probable outcome (prioritiza-
TTB, was defined as inspecting the payoffs of 1 to 6 gambles         tion), the degree to which subsequent acquisitions inspected
for the most probable outcome and not inspecting payoffs for         the payoffs of different gambles for the same outcome ver-
any other outcome. TTB2 was defined as inspecting all four-          sus the payoffs of the same gamble for different outcomes
                                                                                                                 nsame outcome −nsame gamble
teen cells of the two most probable outcomes, and nothing            (outcome-based processing: n                                 +n                  ), and
                                                                                                                  same outcome        same gamble
else. WADD was defined as inspecting all 28 cells column by          the average ratio of the expected value of the chosen gamble
column.Random decisions mean zero samples were taken.                over the expected value of the optimal choice (relative per-
                                                                     formance). To further test our model’s predictions, we ran a
Results                                                              2-way mixed-effects ANOVA for each of these four metrics.
Our process tracing data confirmed that people do indeed                As shown in Figure 2, the effects of the stakes and out-
use the SAT-TTB strategy discovered by our model. Table              come probabilities on the four metrics confirmed the model’s
1 shows the frequency of various decision strategies, for each       predictions. Our model’s first prediction that high dispersion
of the four different types of trials. Out of 4000 trials across     promotes the use of fast-and-frugal heuristics was confirmed
all participants, TTB was the most common strategy overall,          by a decrease in the number of acquisitions (F(1, 3798) =
accounting for 25.3% of all trials. SAT-TTB was the sec-             78.24, p < 0.0001) in conjunction with an increases in
ond most common strategy among those we examined: par-               outcome-based processing (F(1, 3432) = 68.31, p < 0.0001)
ticipants employed this strategy on 10.7% of all trials. In          and prioritization ((F(1, 3478) = 280.1, p < 0.0001)). The
8.0% of trials participants chose randomly without making            increase in prioritization was especially striking: while only
any observations—mostly during low-stakes games. Interest-           40.4% of participants’ clicks inspected the most probable
ingly, we also observed a second novel strategy that we call         outcome when dispersion was low, they focused 70.6% of
Take-The-Best-Two (TTB2). This strategy inspects all gam-            their acquisitions on the most probable outcome when dis-
bles’ payoffs for the two most probable outcomes, and was            persion was high. Our model’s second prediction that the
used in 6.3% of trials. The WADD strategy occurred in 4.5%           higher stakes should decrease people’s reliance on fast-and-
of trials. Finally, the SAT strategy was used in 3.1% of games.      frugal heuristics was confirmed by a significant increases
   Consistent with our model’s first prediction, people used         in the number of acquisitions (F(1, 3798) = 281.47, p <
TTB more frequently when the dispersion was high (χ2 (1) =           0.0001) which was accompanied by a decrease in prioritiza-
897.9, p < 0.0001). Consistent with our model’s second pre-          tion (F(1, 3478) = 62.42, p < 0.0001) and an increase in rel-
diction, participants used simple heuristics more frequently         ative performance (F(1, 3798) = 47.62, p < 0.0001). Consis-
when the stakes were low: the frequency of the random                tent with the model’s third prediction, the average outcome-
choice—the simplest heuristic—increased significantly from           based processing metric was lower for high stakes but this ef-
4.2% on high-stakes problems to 19.9% on low-stakes prob-            fect was not statistically significant (F(1, 3432) = 2.45, p =
lems (χ2 (1) = 88.2, p < 0.0001), and so did the frequency of        0.06, one-tailed). Our model’s third prediction that high-
the second simplest heuristic, SAT-TTB (χ2 (1) = 86.3, p <           stakes increases time, effort, and performance, was con-
0.0001), and the third simplest heuristic, TTB (χ2 (1) =             firmed by a significant increases in the number of acquisitions
20.0, p < 0.0001). The frequency of SAT also increased               (F(1, 3798) = 281.47, p < 0.0001) and relative performance
from high- to low-stakes games (χ2 (1) = 3.4, p < 0.05, one-         (F(1, 3798) = 47.62, p < 0.0001) with high stakes.
                                                                 746

                                                                      the decision strategies the model employed on the vast major-
                                                                      ity of high-stakes problems where it did not use WADD.
                                                                         Our proof-of-concept study suggests that formulating the
                                                                      problem of making optimal use of finite time and limited
                                                                      cognitive resources as a meta-level MDP is a promising ap-
                                                                      proach to discovering cognitive strategies. This approach can
                                                                      be leveraged to develop more realistic normative standards
                                                                      of human rationality. This might enable future work to sys-
                                                                      tematically evaluate the extent to which people are resource-
                                                                      rational. In the long term, our approach could be used to im-
                                                                      prove human reasoning and decision-making by discovering
                                                                      rational heuristics and teaching them to people.
                                                                      Acknowledgments. This work was supported by grant number
                                                                      ONR MURI N00014-13-1-0341 and a grant from the Templeton
Figure 2: People’s decision style by stakes and dispersion of         World Charity Foundation.
the outcome probabilities.
                                                                                                   References
                                                                      Agrawal, S., & Goyal, N. (2013). Thompson sampling for contex-
   Despite these qualitative agreements, there were quantita-                 tual bandits with linear payoffs. In Proceedings of the 30th in-
                                                                              ternational conference on machine learning (pp. 127–135).
tive differences. Most notably, our model predicted a more            Gigerenzer, G., & Goldstein, D. G. (1996). Reasoning the fast and
pronounced effect of the stakes on the number of acquisitions                 frugal way: models of bounded rationality. Psychological
than we observed in people (+19.6 vs. +3.4); the smaller                      review, 103(4), 650.
                                                                      Gigerenzer, G., & Selten, R. (2002). Bounded rationality: The
effect in people might reflect their concave utility function.                adaptive toolbox. MIT Press.
                                                                      Griffiths, T. L., Lieder, F., & Goodman, N. D. (2015). Rational use
                          Discussion                                          of cognitive resources: Levels of analysis between the com-
                                                                              putational and the algorithmic. Topics in Cognitive Science,
In summary, our resource-rational theory of multi-alternative                 7, 217–229.
                                                                      Hay, N., Russell, S., Tolpin, D., & Shimony, S. (2012). Select-
risky choice predicted some of the main strategies people use                 ing computations: Theory and applications. In N. de Freitas
in the Mouselab paradigm and the conditions under which                       & K. Murphy (Eds.), Uncertainty in artificial intelligence:
they are selected. In addition to automatically discovering                   Proceedings of the twenty-eighth conference. P.O. Box 866
                                                                              Corvallis, Oregon 97339 USA: AUAI Press.
known strategies and contingencies, our computational ap-             Johnson, E. J., Payne, J. W., Bettman, J. R., & Schkade, D. A.
proach also discovered a novel, previously unknown heuris-                    (1989). Monitoring information processing and decisions:
tic that integrates TTB with satisficing (SAT-TTB), and our                   The mouselab system (Tech. Rep.). DTIC Document.
                                                                      Lieder, F., Griffiths, T. L., & Goodman, N. D. (2012). Burn-in,
experiment confirmed that people do indeed use SAT-TTB on                     bias, and the rationality of anchoring. Advances in Neural
a non-negligible fraction of problems—especially when the                     Information Processing Systems, 2699–2707.
stakes are low.                                                       Lieder, F., Hsu, M., & Griffiths, T. L. (2014). The high availability
                                                                              of extreme events serves resource-rational decision-making.
   Tajima, Drugowitsch, and Pouget (2016) solved meta-level                   In Proceedings of the 36th annual conference of the cognitive
MDPs to derive boundedly optimal drift-diffusion models.                      science society.
The strategy discovery method presented here generalizes this         Lieder, F., Plunkett, D., Hamrick, J. B., Russell, S. J., Hay, N., &
                                                                              Griffiths, T. (2014). Algorithm selection by rational metarea-
approach to more complex decision mechanisms that can pro-                    soning as a model of human strategy selection. In Z. Ghahra-
cess and generate evidence in many different ways.                            mani, M. Welling, C. Cortes, N. Lawrence, & K. Weinberger
   One limitation of the current work is that we do not know                  (Eds.), Advances in neural information processing systems 27
                                                                              (pp. 2870–2878). Curran Associates, Inc.
how closely our algorithm approximated the optimal pol-               Payne, J. W., Bettman, J. R., & Johnson, E. J. (1988). Adaptive strat-
icy, and it is possible that a more accurate approximation                    egy selection in decision making. Journal of Experimental
would yield somewhat different predictions. Future work                       Psychology: Learning, Memory, and Cognition, 14(3), 534.
                                                                      Russell, S. J., & Subramanian, D. (1995). Provably bounded-
will systematically evaluate the accuracy of our approxima-                   optimal agents. Journal of Artificial Intelligence Research,
tion method on smaller problems for which the optimal meta-                   2, 575–609.
level policy can be computed exactly. Another limitation of           Russell, S. J., & Wefald, E. (1991). Principles of metareasoning.
                                                                              Artificial Intelligence, 49(1-3), 361–395.
the present work is that the cost of computation had to be fit to     Shugan, S. M. (1980). The cost of thinking. Journal of consumer
the participants’ responses. Future work will control the cost                Research, 7(2), 99–111.
per click and measure it independently. This will enable a di-        Simon, H. A. (1956). Rational choice and the structure of the envi-
                                                                              ronment. Psychological review, 63(2), 129.
rect comparison of the time and effort people invest against          Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An
the optimal amount of deliberation. However, a thorough an-                   introduction. Cambridge, MA, USA: MIT press.
swer to this question will require a more detailed model of           Tajima, S., Drugowitsch, J., & Pouget, A. (2016). Optimal policy for
                                                                              value-based decision-making. Nature communications, 7.
people’s cognitive architecture including a model of working          Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty:
memory. Another direction for future work is to characterize                  Heuristics and biases. Science, 185(4157), 1124–1131.
                                                                  747

