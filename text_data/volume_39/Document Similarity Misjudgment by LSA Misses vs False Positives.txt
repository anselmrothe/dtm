              Document Similarity Misjudgment by LSA: Misses vs. False Positives
                                           Kyung Hun Jung (kjung2@kennesaw.edu)
                             Department of Psychology, Kennesaw State University (Kennesaw Campus)
                                       402 Bartow Ave Kennesaw (Rm 4030), GA 30144 USA
                                                Eric Ruthruff (ruthruff@unm.edu)
                                         Department of Psychology, Logan Hall MSC03-2020
                                  1 University of New Mexico, Albuquerque, NM 87131-0001 USA
                                               Timothy Goldsmith (gold@unm.edu)
                                         Department of Psychology, Logan Hall MSC03-2020
                                  1 University of New Mexico, Albuquerque, NM 87131-0001 USA
                              Abstract                                   To better understand the weaknesses of LSA, and thereby
                                                                      improve models of text document similarity, this study
   Modeling text document similarity is an important yet
   challenging task. Even the most advanced computational             investigated the pattern of discrepancy between LSA and
   linguistic models often misjudge document similarity relative      humans with respect to their document similarity ratings.
   to humans. Regarding the pattern of misjudgment between            Specifically, we examined the frequency and degree of
   models and humans, Lee and colleagues (2005) suggested that        underestimation (misses) and overestimation (false positives)
   the models’ primary failure is occasional underestimation of       made by LSA relative to humans under favorable parameter
   strong similarity between documents. According to this             settings of LSA.
   suggestion, there should be more extreme misses (i.e., models
   failing to pick up on strong document similarity) than extreme     Misses vs. False Positives
   false positives (i.e., models falsely detecting document
   similarity that does not exist). We tested this claim by           Regarding the nature of the misjudgment by models, Lee et
   comparing document similarity ratings generated by humans          al. (2005) suggested that extreme misses would be a stronger
   and latent semantic analysis (LSA). Notably, we implemented        cause than extreme false positives. They made this suggestion
   LSA with 441 unique parameter settings, determined optimal         based on an observation that the common features model (Lee
   parameters that yielded high correlations with human ratings,      & Navarro, 2002) occasionally misses the high similarity
   and finally identified misses and false positives under the        between documents that is readily apparent to humans. In a
   optimal parameter settings. The results showed that, as Lee et
                                                                      scatterplot of the model ratings against human ratings for
   al. predicted, large errors were predominantly misses rather
   than false positives. Potential causes of the misses and false
                                                                      each document pair, the authors found a cluster of points
   positives are discussed.                                           (document pairs) with low model ratings but high human
                                                                      ratings. That is, the model missed some of the strong
   Keywords: text document relatedness; semantic similarity;          document similarities that humans detected.
   latent semantic analysis (LSA)
                                                                         Lee et al.’s (2005) analysis above was based on the
                                                                      common features model, but it might apply to LSA as well.
                          Introduction                                The common features model judges document similarity
Modeling how humans judge the semantic similarity of text             primarily based on the proportion of common features
documents is an interesting topic in cognitive science with           (words) shared by two documents. Notably, LSA’s
numerous practical implications. In an effort to better model         underlying model, the vector space model, determines
human document similarity judgments, Lee, Pincombe, and               document similarity in a similar manner. Therefore, Lee et
Welsh (2005) compared several models of document                      al.’s findings suggesting more extreme misses over extreme
similarity, including latent semantic analysis (LSA). They            false positives may also apply to LSA. This interesting
found that LSA’s cosine similarity scores yielded higher              hypothesis, if validated, would provide a valuable clue to
agreement (r = .60) with aggregate human ratings than other           improving models of text document similarity. However, it
models, such as Tversky’s (1977) ratio model (r = .50).               has not yet been rigorously tested.
Considering that the inter-rater correlation among human                 Testing the hypothesis seems straightforward at first
raters is also about .60, LSA seems to judge about as well as         glance: compare human and LSA’s document similarity
a single human rater. However, the moderate correlation               ratings for a set of documents pairs. Then, document pairs
between humans and LSA also suggests that LSA does not                with especially low LSA ratings compared to human ratings
fully capture human similarity judgments 1.                           should be considered as misses and the reverse as false
  1 For the document pairs used in Lee et al. (2005), the highest
reported correlation between a model and humans was .77 (Yeh,
Ramage, Manning, Agirre, & Soroa, 2009).
                                                                  2338

positives. However, LSA’s document representation depends               singular values. The corresponding columns and rows of U
on the parameters used such as the quantity and quality of the          and VT, respectively, are discarded, too. The original USVT,
background documents (Bullinaria & Levy, 2006), the                     after discarding some values, then can be denoted as
dimensionality (Dumais, 1991; Landauer & Dumais, 1997),                 U’S’(VT)’, where U’ is a w * k matrix whose columns are the
and the local-global weighting schemes (Lintean, Moldovan,              first k columns of U, S’ is a k * k diagonal matrix whose
Rus, & Mcnamara, 2010; Nakov, Popova, & Mateev, 2001).                  diagonal elements are the k largest singular values of S, and
Therefore, LSA’s misjudgments relative to human judgments               (VT)’ is a k * d matrix whose rows are the first k rows of VT.
could vary depending on the parameters.                                 By multiplying these three reduced sub-matrices, one can
  In this study, we attempted to investigate the nature of              obtain the least squares approximation of the original matrix.
misjudgment by LSA under its optimal parameter settings.                Finally, documents can be represented as vectors on a k
Therefore, we first identified LSA’s optimal parameter                  dimensional singular-value-space, which has k orthogonal
settings by employing as many as 441 unique parameter                   axes. These dimensions are constructed so that the first axis
combinations. Then, under the selected optimal parameter                explains the largest amount of variance of A, and the second
settings, we identified misjudgments by LSA as misses or                axis explains the second largest amount of variance of A, and
false positives. Finally, we measured the degree of                     so on.
misjudgment of the two types using normalized scores.                      Furnas et al. (1988) was the first to apply the reduced SVD
  The remainder of this paper has the following structure: (1)          to the vector space model. This method was later called latent
introduction to LSA, (2) an experiment identifying optimal              semantic analysis by Deerwester, Dumais, Furnas, Landauer,
parameter settings, (3) identification of misjudgments as               and Harshman (1990), who also demonstrated that LSA
misses and false positives under the optimal parameter                  retrieves information better than traditional word-matching
settings, and (4) discussion of the underlying causes of the            methods. Deerwester et al. argued that SVD uncovers latent
misses and false positives.                                             semantic relations across documents that are buried in the
                                                                        corpus by removing noise (small singular values) in the
                               LSA                                      original word-by-document matrix.
LSA is based on a vector space model in which documents
are first transformed into a word-by-document matrix. Rows                  Identification of Misses and False Positives
of the matrix correspond to the unique words across                          under LSA’s Optimal Parameter Settings
documents, whereas columns correspond to individual
documents. Cell values are the frequencies of words within              Stimuli and Procedure
each document. The cell values can be weighted in two                   Target Text Documents We used the 1,225 document pairs
respects: to what degree a word is important in representing            from Pincombe (2004), which Lee et al. (2005) also adopted.
a document’s topic (local weighting), and to what degree a              These document pairs were generated by pairing 50 target
word is important in distinguishing one document from                   news articles selected from Australian Broadcasting
another according to their topics (global weighting). Using             Corporation’s news mail service. Each news article had a
the weighted cell values, each document can be represented              single paragraph containing 51 to 126 words (average: 82
as a vector in a multidimensional space, where the                      words). They covered a variety of topics, such as terrorism
dimensions correspond the unique words. Finally, the                    and hunger in Africa. For each of the 1,225 document pairs,
sematic similarity between two document vectors is typically            Pincombe collected about 10 human ratings by asking 83
measured using the cosine similarity score.                             university students to each rate the relatedness of a subset of
  The core process that distinguishes LSA from the vector               the document pairs. Participants used a five-point scale, with
space model is singular value decomposition (SVD)                       one indicating “highly unrelated” and five indicating “highly
implemented on the word-by-document matrix. SVD is a                    related”.
matrix factorization method that decomposes an original
matrix (A) into three sub-matrices, USVT, where U is a                  Background Documents Lee et al. (2005) used 314 news
unitary w * r matrix (word-by-dimension matrix), S is an r *            articles from the same Australian news corpus as background
r diagonal matrix with non-negative real numbers on its                 documents2. In this study, to explore the optimal parameter
diagonal (singular value matrix), and VT is a unitary r * d             settings of LSA, we employed 4,172 additional news articles
matrix (dimension-by-document matrix). By multiplying                   from the same news corpus (total of 4,486). These new
these three sub-matrices, the original matrix can be retrieved,         background documents contained a single paragraph
and this type of SVD is called full SVD.                                (average: 152 words). They also covered a variety of topics
  In a modified version of the full SVD, called reduced SVD,            as the 50 target news articles did. In addition to the
small singular values located in the lower right corner of S            background document size used in Lee et al. (314) and the
are intentionally discarded, while preserving the first k largest       maximum size available in this study (4,486), we examined
  2 Background documents are included in the original corpus            documents are represented. It is generally regarded that LSA's
subject to LSA, along with the target documents. They are employed      performance improves as the number of background documents
only for constructing the multidimensional space in which the target    increases (Bullinaria & Levy, 2006).
                                                                    2339

five intermediate background document sizes by randomly              normalized human rating by at least 1.0, then the LSA’s
selecting the following numbers of documents from the new            cosine score was considered a miss. But if a document pair’s
set of 4,172 articles: 314, 750, 1,000, 2,000, and 3,000 (see        normalized cosine score was greater than the normalized
Table 1).                                                            human rating by 1.0, then the LSA cosine score was
                                                                     considered a false positive.
       Table 1. Seven background document conditions.
                                                                     Results and Discussion
   Size                           Source                             Optimal Parameters of LSA To determine which parameter
   314      The same 314 news articles as in Lee et al. (2005)       settings are optimal for LSA’s document similarity
   314          Randomly selected from the 4,172 articles            representation, we examined the correlation between LSA
   750          Randomly selected from the 4,172 articles            cosine scores and human ratings. The correlation was
  1,000         Randomly selected from the 4,172 articles            affected more systematically and strongly by the interaction
                                                                     of background document size and dimensionality than the
  2,000         Randomly selected from the 4,172 articles
                                                                     local-global weighting schemes. Therefore, for the sake of
  3,000         Randomly selected from the 4,172 articles            simplicity, we merged the correlations across the nine
            Combination of the 314 news articles from Lee et         weighting schemes at a given background document size and
  4,486
                      al. and the new 4,172 articles                 dimensionality. As shown in Table 2, the correlation
                                                                     increased markedly as we added more background
                                                                     documents, consistent with previous research (Bullinaria &
Dimensionality Regarding the dimensionality of the reduced           Levy, 2006). But this effect was more prominent at relatively
SVD, the maximum possible dimension for a given                      high dimensions than at low dimensions.
background document size corresponds to the total number
of documents subjected to SVD (50 + the number of                         Table 2. Correlations between human ratings and LSA
background documents). For example, in the 314-                        cosine scores as a factor of the background document size
background document condition, the maximum dimension is                and dimensionality. Correlations were merged across the
364 (= 50 + 314). In most of the background document                  nine local-global weighting schemes at a given background
conditions employed in this study, higher dimensions than                  document size and dimensionality. Relatively high
364 were possible. However, following some researchers’                             correlations (r ≥ .67) are shaded.
arguments for the importance of maintaining 300 dimensions
(Landauer & Dumais, 1997), we selected the following seven                                            Dimension
dimensions for the reduced SVD (i.e., LSA): 50, 100, 150,
                                                                        Background   50    100    150   200     250   300    364    Average
200, 250, 300, and 364.
                                                                            314     0.53   0.55  0.55   0.56    0.56  0.57  0.59     0.56
Other LSA Parameters Stemming, normalization, and                        New 314    0.62   0.59  0.58   0.56    0.57  0.58  0.61     0.59
removal of stopwords and alphanumeric words are known to
                                                                            750     0.65   0.66  0.63   0.61    0.61  0.60  0.59     0.62
improve LSA’s document representation (Pincombe, 2004;
Stone, Dennis, & Kwantes, 2011). Therefore, they were                      1000     0.62   0.66  0.66   0.64    0.62  0.61  0.60     0.63
applied to all LSA runs. Three local weighting schemes (tf,                2000     0.51   0.61  0.64   0.68    0.67  0.68  0.67     0.64
log, and alt-log) and three global weighting schemes (idf,                 3000     0.53   0.61  0.64   0.64    0.66  0.67  0.69     0.63
entropy, and p-inverse) were selected based on their
significant effects observed in a pilot study (not reported                4486     0.58   0.65  0.67   0.69    0.68  0.68  0.66     0.66
here).                                                                    Average   0.58   0.62  0.62   0.63    0.62  0.63  0.63     0.62
   LSA cosine scores were computed for every possible (441)
combination of the above parameters: 7 background                       To identify optimal parameter settings of LSA, we first
document sizes * 7 dimensions * 3 local weighting schemes            selected the 10 combinations of background document size
* 3 global weighting schemes.                                        and dimension that yielded correlations of at least .67,
                                                                     averaged across all weighting schemes (see the shaded cells
Identifying Misses and False Positives To classify LSA               in Table 2). Then, for each of these 10 combinations, we
ratings as misses and false positives relative to human ratings,     chose the local-global weighting scheme that yielded the
we first normalized the human ratings and LSA’s cosine               highest correlation with human ratings. Table 3 shows the
scores using z-score 3 . The degree of misjudgment was               specific parameter settings of these 10 selected combinations
measured as the absolute difference between the two                  as optimal parameter settings. The table also shows the
normalized scores for a given document pair. If a document           correlation, number of misses and false positives, and the
pair’s normalized cosine score was smaller than the                  average absolute z-score errors.
   3 We considered the approach of transforming scores into a 0-1    the z-score normalization yields more reliable results with respect to
scale, as in Lee et al. (2005). However, this approach is overly     the frequency of misses and false positives.
sensitive to the minimum and maximum values. On the other hand,
                                                                 2340

Table 3. Ten optimal parameter settings of LSA selected for the identification of misses and false positives. The parameters,
correlation with human ratings, number of misses and false positives, and the average of the absolute z-score errors are shown.
                                                                                                                           Average
                                                                                                        Average
  Background                                                                           Number                            misjudgment
                                     Local      Global                     Number                     misjudgment
   document         Dimension                              Correlation                  of false                       (absolute z-score
                                  Weighting   Weighting                   of misses                 (absolute z-score
      size                                                                             positives                        error) for false
                                                                                                    error) for misses
                                                                                                                           positives
      2000              200            tf      p-inverse       0.70          112           86              1.57              1.43
      2000              250            tf      p-inverse       0.68          121           67              1.60              1.56
      2000              300            tf         idf          0.68          117           72              1.61              1.56
      2000              364         alt-log    p-inverse       0.68          118           78              1.60              1.54
      3000              300            tf      p-inverse       0.68          115           86              1.63              1.47
      3000              364         alt-log     entropy        0.69          104           76              1.61              1.42
      4486              150         alt-log    p-inverse       0.68          119           95              1.57              1.42
      4486              200            tf      p-inverse       0.70          120           83              1.62              1.48
      4486              250            tf      p-inverse       0.68          124           78              1.64              1.58
      4486              300            tf         idf          0.68          119           69              1.61              1.49
                                               Average         0.69          117           79              1.57              1.43
Nature of Misjudgments by LSA To determine the nature                  Effect of the Parameters on the Frequency of Misses and
of LSA’s misjudgments under optimal parameters, we used                False Positives Although the distribution of the two types of
the z-score errors obtained from the 10 parameter settings             errors by LSA at optimal parameters was the primary focus
shown in Table 3. As shown at the bottom of the table, misses          of this study, we also examined the ratio of misses to false
(MMiss = 117) were much more common than false positives               positives across all the 441 parameter settings. The results
(MFalse Positive = 79), χ2 (1, N = 1,959) = 73.324, p < .001, just     showed that the ratios were systematically affected by the
as suggested by Lee et al. (2005). Also, as the error                  interaction between the background document size and
magnitude increases, the ratio of misses to false positives also       dimensionality. That is, the ratio of misses to false positives
increases, which is consistent across the 10 optimal parameter         increased as the dimensionality increased. However, the
settings. Figure 1 shows the frequency of the two types of             degree of increase is getting less prominent as the background
errors (misses vs. false positives) as a function of the absolute      document size increases. In other words, although there were
z-score error.                                                         more misses than false positives in general, the disproportion
                                                                       of misses over false positives is more prominent at high
                                                                       dimensions with small number of background documents.
                                                                       Effect of the Number of Background Documents on
                                                                       Correlation between Humans and LSA One of the most
                                                                       striking findings above was the strong effect of background
                                                                       document size on LSA’s document similarity representation.
                                                                       As shown in Table 2, employing more background
                                                                       documents (combined with an appropriate dimensionality)
                                                                       tends to significantly improve LSA’s document similarity
                                                                       judgments. To illustrate the significant effect of background
                                                                       documents, we plotted the correlation between LSA and
                                                                       human ratings for three background document sizes (0, 314,
                                                                       and 4,486) and the nine weighting schemes as a function of
                                                                       dimensionality (Figure 2). The graph illustrates (a) the strong
                                                                       effect of the number of background documents, (b) important
                                                                       effect of dimensionality when the background document size
                                                                       is small (i.e., the left side of the graph), and (c) the relative
                                                                       unimportance of weighting schemes.
Figure 1. The frequency of the two types of misjudgment by
       LSA as a function of the absolute z-score error.
                                                                  2341

                                                                    identified the frequency and degree of large misses and large
                                                                    false positives under optimal parameters of LSA. The results
                                                                    confirmed that LSA makes more misses than false positives,
                                                                    especially among the most severe errors.
                                                                      The results also suggest that if one attempts to further
                                                                    improve models of text document similarity by reducing its
                                                                    errors relative to humans, the misses rather than the false
                                                                    positive would be the primary focus of the revision. More
                                                                    specifically, one should look for ways to help models pick up
                                                                    on some of the strong semantic similarities that they currently
                                                                    miss.
                                                                     Potential Causes of Misses and False Positives
                                                                    An obvious follow-up question of this study is what causes
                                                                    LSA’s greatest misses and false positives. Considering that
  Figure 2. The correlation of ratings between humans and
                                                                    LSA’s basis, the vector space model, judges document
 LSA for three background document conditions (0, 314 and
                                                                    similarity based on the overall word similarity between two
     4,486) and nine weighting schemes as a function of
                                                                    documents, a potential cause of error is that LSA misses or
                        dimensionality.
                                                                    falsely overestimates the semantic similarity of some word
   One may suspect that including even more background              pairs from two documents. In fact, there are various cases
documents would further increase the correlation. However,          where LSA cannot help but miss some of the word
to make a positive impact on LSA's performance, the                 similarities, which in turn would cause one type of error,
background documents should not only be numerous but also           miss. For example, although “United States”, “US”, “U.S.”,
relevant to the content of the target documents (Foltz, Britt,      and “U.S.A” refer to the same country, they may not be
& Perfetti, 1995). For example, Stone, Dennis, and Kwantes          recognized as the same entity in the word-by-document
(2011) tested the effect of various kinds of background             matrix for various reasons: because they are not a single word
documents on LSA's document similarity judgments, using             (United States), too short to be included (US or U.S. after the
the same 50 target news articles examined in this study. They       special character removal), or happen to match an excluded
tested 55,021 Canada Toronto Star newspaper articles                stop word (US and the pronoun us). However, humans would
(miscellaneous gossip paragraphs, from the year 2005) and           correctly recognize them and utilize these words for
10,000 articles selected by the researchers from online             document similarity judgment.
encyclopedia,      Wikipedia      (http://www.wikipedia.org/).        Also, some words (especially proper nouns including
However, the highest correlation between humans and LSA             human names) may occur in the target documents but not in
obtained was about .10 with the gossip news articles as             the background documents, preventing LSA from utilizing
background and .40 with the Wikipedia background                    those words in judging document similarity. However, those
documents. These correlations were significantly lower than         words could be critical for humans to judge the document
the highest correlation of .60 obtained in Lee et al. (2005) and    similarity. Then, LSA may judge document pairs including
.70 in the current study, despite utilizing only 314 and 4,486      those words to be less related than humans would do (i.e.,
background documents, respectively. Therefore, not only the         leading to a miss).
size but also the relevance of background documents to the            The above-mentioned potential cause of misses (i.e., LSA
target documents seem to be critical for LSA’s document             misses document similarity because it misses word similarity
similarity judgments.                                               in document pairs) could be further supported if LSA’s
   If target documents came from a certain population (e.g.,        document similarity scores do correspond to the overall word
specific news corpus), we recommend using documents from            similarity between two documents. To confirm this, we
the same population as background documents. In our case,           calculated the correlation between the 1,225 document pairs’
employing background documents of 4,172 news articles that          LSA cosine scores and the average LSA cosine scores of
came from the same population as the target articles increased      every possible word pair from each of the document pairs.
the correlation between humans and LSA from .60 to .70.             We found a correlation of .73 from this analysis, indicating
                                                                    that LSA’s document similarity is heavily relying on the
                                                                    overall word similarity in document pairs.
Conclusion                                                            Similar to the potential cause of misses by LSA addressed
Lee et al. (2005) suggested that the primary weakness of            in the above, a potential cause of false positives by LSA is
computational models of document similarity is failing to           that LSA mistakenly perceives semantic similarity between
pick up on some of the strong document similarities that            words that are in fact unrelated. Table 4 shows 10 word pairs
humans easily detect. To test this hypothesis, we compared          that were judged to be highly related by humans and LSA,
the document similarity ratings made by humans and LSA              respectively in one of the document pairs used in this study.
based on a range of parameter combinations. Then we                 Although LSA does generally make reasonable judgments on
                                                                2342

 word relatedness, some word pairs judged to be highly related      Foltz, P. W., Britt, M. A., & Perfetti, C. A. (1995). Measuring
 by LSA do not seem to have a meaningful relationship. For            text influence on learning history. Proceedings of the Fifth
 example, design and document were the most strongly related          Annual Winter Text Conference, Jackson, WY.
 word pair to LSA, despite being seemingly unrelated. Thus,         Furnas, G. W., Deerwester, S. C., Dumais, S. T., Landauer,
 LSA will occasionally overestimate the relatedness of the            T. K., Harshman, R. A., Streeter, L. A., & Lochbaum, K.
 document pairs that include this word pair.                          E. (1988). Information retrieval using a singular value
                                                                      decomposition model of latent semantic structure.
     Table 4. The 10 most related words pairs to humans and           Proceedings of the Eleventh Annual International ACM 81
                 LSA from a pair of news articles.                    SIGIR Conference on Research and Development in
                                                                      Information Retrieval. 465-480. Grenoble, France.
Highly related                   Highly related                     Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007).
                        Ratings                          Ratings
word pairs by                     word pairs by
                     (1-5 scale)                        (z-score)     Topics in semantic representation. Psychological Review,
    human                             LSA
                                                                      114, 211-244.
  dollar-money            5.00     design-document         6.87     Jung, K. (2013). Mismatches between humans and latent
  job-money               5.00     increase-rise           5.61       semantic analysis in document similarity judgments
  angrily-attack          4.90     paid-worker             3.97       (Doctoral dissertation).
  plan-target             4.83     effect-target           3.38     Landauer, T. K., & Dumais, S. T. (1997). A solution to
  increase-profit         4.80     group-work              3.37       Plato’s problem: The latent semantic analysis theory of
  money-profit            4.80     effect-increase         3.08       acquisition, induction, and representation of knowledge.
  cost-lawsuit            4.78     disclosure-profit       2.89       Psychological Review, 104, 211-240.
  job-meet                4.75     disclosure-financial    2.83     Lee, M. D., & Navarro, D. J. (2002). Extending the ALCOVE
  agreement-plan          4.73     commonwealth-deal        2.41      model of category learning to featural stimulus domains.
  job-paid                4.73     australia-target        2.32       Psychonomic Bulletin & Review, 9, 43-58.
                                                                    Lee, M. D., Pincombe, B. M., & Welsh, M. B. (2005). An
    An alternative hypothesis regarding the misses and false          empirical evaluation of models of text document similarity.
 positives of LSA is that, when judging document similarity,          Proceedings of Twenty Seventh Annual Conference of the
 humans do not rely on the overall word similarity as much as         Cognitive Science Society (pp. 1254-1259). Mahwah, NJ:
 LSA does. As Griffiths, Steyvers, and Tenenbaum (2007)               Erlbaum.
 suggested, humans may catch the gist of each document and          Lintean, M., Moldovan, C., Rus, V., & McNamara D. S.
 compare the semantic representations of the gist rather than         (2010). The role of local and global weighting in assessing
 relying on the overall similarity of words in the documents.         the semantic similarity of texts using Latent Semantic
 Then, two documents with a large overlap of words but with           Analysis. Proceedings of the Twenty Third International
 different topics would be regarded unrelated by humans               Florida Artificial Intelligence Research Society
 although they could be highly related to LSA (resulting in           Conference. Daytona Beach, FL.
 false positives). To assess to what degree human document          Nakov, P., Popova, A., & Mateev, P. (2001). Weight
 similarity judgments rely on the overall word similarity, one        functions impact on LSA performance. Proceedings of the
 could examine the correlation between human document                 Recent Advances in Natural Language Processing (pp.
 similarity ratings and the average of the human similarity           187-193). Tzigov Chark, Bulgaria.
 ratings for all the possible word pairs in a given document        Pincombe, B. M. (2004). Comparison of human and LSA
 pair. If humans do not rely on the overall word similarity as        judgments of pairwise document similarities for a news
 much as LSA does, then the correlation would not be as high          corpus (Tech. Rep. DSTO-RR-0278). Adelaide, Australia:
 as the corresponding correlation of LSA.                             Australian Defense Science and Technology Organization,
                                                                      Intelligence, Surveillance and Reconnaissance Division.
                           References                               Stone, B., Dennis, S., & Kwantes, P. J. (2011). Comparing
                                                                      methods for single paragraph similarity analysis, Topics in
 Bullinaria, J. A., & Levy, J. P. (2006). Extracting semantic         Cognitive Science, 3, 92-122.
    representations from word co-occurrence statistics: A           Tversky, A. (1977). Features of similarity. Psychological
    computational study. Behavior Research Methods, 39,               Review, 84, 327-352.
    510-526.                                                        Yeh, E., Ramage, D., Manning, C. D., Agirre, E., & Soroa,
 Deerwester, S. C., Dumais, S. T., Furnas, G. W., Landauer,           A. (2009). WikiWalk: Random walks on Wikipedia for
    T. K., & Harshman, R. A. (1990). Indexing by latent               semantic relatedness. Proceedings of the 2009 Workshop
    semantic analysis. Journal of the American Society for            on Graph-based Methods for Natural Language
    Information Science, 41, 391-407.                                 Processing (pp. 41-49). Stroudsburg, PA, USA:
 Dumais, S. T. (1991). Improving the retrieval of information         Association for Computational Linguistics.
    from external sources. Behavior Research Methods,
    Instruments, and Computers, 23, 229-236.
                                                                2343

