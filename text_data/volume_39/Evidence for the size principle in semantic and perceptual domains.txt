              Evidence for the size principle in semantic and perceptual domains
                                          Joshua C. Peterson (peterson.c.joshua@gmail.com)
                                           Thomas L. Griffiths (tom griffiths@berkeley.edu)
                                        Department of Psychology, University of California, Berkeley
                                Abstract                                    The prior p(h) represents the learner’s knowledge about the
   Shepard’s Universal Law of Generalization offered a com-                 consequential region before observing x. The likelihood
   pelling case for the first physics-like law in cognitive science         p(x | h) depends on our assumptions about how the process
   that should hold for all intelligent agents in the universe. Shep-       that generated x relates to the set h. The key innovation over
   ard’s account is based on a rational Bayesian model of general-
   ization, providing an answer to the question of why such a law           Shepard’s model is the use of the likelihood function
   should emerge. Extending this account to explain how humans
   use multiple examples to make better generalizations requires
                                                                                                            1
                                                                                                            |h|     x∈h
   an additional assumption, called the size principle: hypotheses                           p(x | h) =                                      (4)
   that pick out fewer objects should make a larger contribution                                             0   otherwise
   to generalization. The degree to which this principle warrants
   similarly law-like status is far from conclusive. Typically, eval-       where |h| is the number of objects in the set picked out by h.
   uating this principle has not been straightforward, requiring
   additional assumptions. We present a new method for evaluat-             The motivation for this choice of likelihood function is the
   ing the size principle that is more direct, and apply this method        size principle, which uses the assumption of random sam-
   to a diverse array of datasets. Our results provide support for          pling to justify the idea that smaller hypotheses should be
   the broad applicability of the size principle.
                                                                            given greater weight (see Tenenbaum and Griffiths, 2001, for
   Keywords: size principle; generalization; similarity; percep-
   tion                                                                     a demonstration of this when x represents multiple examples).
                                                                                The value of the size principle lies in the fact that it allows
                           Introduction                                     for the benefit of multiple examples of a concept to influence
In the seminal work of Shepard (1987), the notion of stimu-                 generalization. Assuming samples are drawn independently,
lus similarity was made concrete through its interpretation as              the likelihood of a hypothesis for n samples is simply the like-
stimulus generalization. It was shown that, across species (in-             lihood of that hypothesis for a single sample to the power of
cluding humans), generalization probabilities follow an ex-                 n. From this, it can be shown that generalization tightens
ponential law with respect to an internal psychological space.              as the number of examples increases, consistent with human
Specifically, the probability that y is in some set C that con-             judgments (see Tenenbaum & Griffiths, 2001).
tains x (what Shepard terms a “consequential subset”) is an                     The size principle thus plays an important role in under-
exponentially decreasing function of distance (d) in psycho-                standing generalization, placing equal importance on deter-
logical space:                                                              mining whether it actually describes human similarity judg-
                             sxy = e−d(x,y) .                       (1)     ments in a wide range of settings. If the size principle is dis-
                                                                            confirmed, an alternative augmentation of Shepard’s model
Shepard termed this phenomenon the Universal Law of Gen-
                                                                            is needed to explain generalization from multiple instances.
eralization, in that it should apply to any intelligent agent,
                                                                            If it can be confirmed to hold broadly, it is a good candidate
anywhere in the universe. This result has been used in nu-
                                                                            for a second law of generalization, or an amendment of the
merous cognitive models that invoke similarity (e.g., Nosof-
                                                                            original law. In this paper we build on previous work evalu-
sky, 1986; Kruschke, 1992).
                                                                            ating the evidence for the size principle (Navarro & Perfors,
   In spite of this, one could argue that generalization from
                                                                            2010), providing a novel and more direct methodology and
a single stimulus to another does not adequately describe the
                                                                            a broader empirical evaluation that includes rich perceptual
full scope of human behavior. Indeed, in a concept learning
                                                                            feature spaces.
task, people are asked to generalize from multiple examples
of a concept. To capture this, Tenenbaum and Griffiths (2001)                            Evaluating the Size Principle
extended Shepard’s original Bayesian derivation of the law to
rationally integrate information about multiple instances. The              In this section we describe previous work evaluating the size
resulting model defines the probability of generalization (that             principle and provide the details of our approach.
y is in C) as a sum of the probabilities of all hypotheses h                Previous work
about the true set C that include both x and y,
                                                                            Navarro and Perfors (2010) made three important contribu-
                    p(y ∈ C | x) =      ∑    p(h | x).              (2)     tions towards understanding the scope of the size principle.
                                       h:y∈h                                First, they made explicit a link between the Bayesian model
The posterior probability of each hypothesis is given by                    of generalization and a classic model of similarity judgment.
Bayes’ rule,                                                                The similarities between a set of objects can be summarized
                                    p(x | h)p(h)                            in a similarity matrix S, where the entry in the ith row and
                      p(h | x) =                   .                (3)      jth column gives the similarity si j between objects i and j.
                                          p(x)
                                                                        919

                                                                                                  Size Principle Prediction
                                                                                                  Best Fitting Line
                             Figure 1: Feature Size/Weight Relationships in Semantic Dataset Group 1.
The additive clustering model (Shepard & Arabie, 1979) de-                Using the link between hypotheses and features, Navarro
composes such a similarity matrix into the matrix product of           and Perfors (2010) made their second contribution: an alter-
a feature-by-object matrix F, its transpose, and a diagonal            native derivation showing that the relationship predicted by
weight matrix W,                                                       the size principle can hold even in the absence of random
                            S = FWFT .                        (5)      sampling. They argued that learners encode the similarity
                                                                       structure of the world by learning a set of features F that effi-
The feature matrix F is binary and can represent any of a              ciently approximate that structure. Under this view, a “coher-
broad set of structures including partitions, hierarchies, and         ent” feature is said to be one for which all objects that possess
overlapping clusters, and can either be inferred by a number           that feature exhibit high similarity. If a learner seeks a set of
of different models or generated directly by participants. The         features that are high in coherence, the size principle emerges
individual entries of S are defined as                                 even in the absence of sampling since the variability in the
                                                                       distribution of similarities between objects sharing a feature
                               Nf
                                                                       is a function of |hk |.
                       si j =  ∑ wk fik f jk .                (6)
                                                                          The third contribution that Navarro and Perfors (2010)
                              k=1
                                                                       made was to evaluate this prediction using data from the
Navarro and Perfors (2010) pointed out that each feature               Leuven Natural Concept Database (LNCD; De Deyne et al.,
could be taken as a single hypothesis h, as it likewise picks          2008). This database consists of human-generated feature
out a set of objects with a common property. Having made               matrices for a large number of objects, as well as pair-wise
this link, the degree of generalization between objects i and          similarity ratings for those objects. Navarro and Perfors
 j predicted by the Bayesian model can be put in the same              (2010) observed that, under some simplifying assumptions,
format as Equation 6: a weighted sum of common features                the size principle predicts that the similarity between objects
(a similar point was made by Tenenbaum & Griffiths, 2001).             that share a feature will be inversely related to the size of that
The equivalence can be seen if we let wk represent the pos-            feature. They showed that this prediction was borne out in 11
terior p(h | x) and fk be the kth hypothesis (hk ), since fik f jk     different domains analyzed in the LNCD.
selects only the features that contain both objects. If the prior
                                                                       Directly testing the size principle
probabilities of the different hypotheses are similar, the like-
lihood (and the size principle) will dominate and                      The method adopted by Navarro and Perfors (2010) depends
                                                                       on the derived relationship between the similarity of objects
                                   1                                   that share a feature and the number of those objects. However,
                             wk ∝       .                     (7)      the link that they established between Bayesian clustering and
                                  |hk |
                                                                   920

                           Animals11                Animals5                Birds                     Clothing1             Clothing2              Fish
Normalized Log Weight
                              Fruit1                  Fruit2                Fruit3                    SIMLEX                  Tools            Vegetables1
Normalized Log Weight
                          Vegetables2               Vehicles1             Vehicles2                  Weapons1              Weapons2
Normalized Log Weight
                        Normalized Log Size     Normalized Log Size   Normalized Log Size         Normalized Log Size   Normalized Log Size
                                                Figure 2: Feature Size/Weight Relationships in Semantic Dataset Group 2.
the additive clustering model (Equations 5-7) can also be used                                   call these features “semantic” because they are linguistic de-
to directly test the size principle. Since both models take the                                  scriptions of general concepts that exclude perceptual-level
same mathematical form, we can directly test the size princi-                                    information associated with actual instances of that concept
ple by estimating the weights wk for a set of features F and                                     or brought to mind when the instance is perceived.
verifying that Equation 7 holds. If we take the logarithm of
both sides of this equation, we obtain the linear relationship                                   Semantic Dataset Group 1
                                                                                                 Following Navarro and Perfors (2010), the first evaluation
                                         log wk = − log |hk | + c                    (8)         dataset is comprised of similarity and feature matrices from
                                                                                                 the Leuven Natural Concept Database (De Deyne et al.,
which can be evaluated by correlating wk with the number of                                      2008). It includes data for 15 categories (Kitchen Uten-
objects that possess feature k. Given a feature matrix F and                                     sils, Clothing, Vegetables, Professions, Fish, Sports, Birds,
similarity matrix S, the weights wk can be obtained through                                      Fruit, Reptiles, Insects, Tools, Vehicles, Musical Instruments,
linear regression (Peterson, Abbott, & Griffiths, 2016). In ad-                                  Mammals, and Weapons), each containing ∼20 − 30 exem-
ditive clustering the weights are often constrained to be non-                                   plars. Binary feature matrices for each category contain
negative. To obtain such weights, we employ a non-negative                                       ∼200 − 300 unique features each. The feature descriptions
least squares algorithm (Lawson & Hanson, 1995). We can                                          are much broader than merely visually apparent features (e.g.,
thus directly test the size principle in any domain where a fea-                                 “has wings”, “eats fruit”, “is attracted by shiny objects”).
ture matrix and a corresponding similarity matrix are avail-
able. In the remainder of the paper we consider two different                                    Semantic Dataset Group 2
sources of such matrices: semantic feature norms and percep-                                     The second dataset group consisted of 17 similarity ma-
tual neural networks.                                                                            trices from a variety of sources throughout the literature.
                                                                                                 The experimental contexts and methodologies differed con-
                                Semantic Hypothesis Spaces                                       siderably compared to the those in group 1. All but one
We first evaluate evidence for the size principle using two                                      of these datasets (SIMLEX) were taken from the simi-
groups of datasets in which people judge the similarity of                                       larity data repository on the website of Dr. Michael Lee
words. Both datasets contain human similarity ratings of                                         (http://faculty.sites.uci.edu/mdlee/similarity-data/). SIMLEX
noun pairs corresponding to concrete objects (e.g., “zebra”                                      was taken from a larger word similarity dataset (Hill, Re-
and “lion”) and lists of binary feature labels associated with                                   ichart, & Korhonen, 2016). The majority of the datasets
each object that can be filtered by frequency of mention. We                                     (Birds, Clothing1, Clothing2, Fish, Fruit1, Fruit2, Furniture1,
                                                                                           921

Furniture2, Tools, Vegetables1, Vegetables2, Weapons1, and            general case, rendering explicit feature descriptions difficult.
Weapons2) are from Romney, Brewer, and Batchelder (1993).             Here, we offer a method to overcome this challenge by lever-
For dataset pairs such as (Vegetables1, Vegetables2), the first       aging representations learned from deep neural networks.
contains more prototypical items than the second. Since none
of these datasets contain corresponding object-feature data,          Perceptual Features
we matched objects from each set to the feature norms re-             Recent work (Peterson et al., 2016) has provided evidence
ported in McRae, Cree, Seidenberg, and McNorgan (2005).               that deep image feature spaces can be used to approximate
                                                                      human similarity judgments for complex natural stimuli. For
Analysis & Results                                                    our analysis, we extracted image features from an augmented
For each dataset, we computed the element-wise multiplica-            version of Alexnet with a binarized final hidden layer (Wu,
tion of each pair of rows in F and used non-negative least            Lin, & Tang, 2015). This allows for a comparison both to non-
squares to regress this matrix onto the corresponding empir-
ical similarity values. We then computed the log of all non-
zero weights, as well as the log of the feature sizes (column
                                                                      Table 1: Correlations between feature size and feature weight
sums of the F matrix for which there was a corresponding
                                                                      (Semantic Datasets Group 1)
non-zero weight). The resulting log weights and log feature
sizes are z-score normalized and plotted in Figures 1 and 2 for         Set               Pearson     Spearman         FR       R2MP
each category in each subgroup. Red lines indicate perfect -1
                                                                        K. Utensils           -0.64         -0.67    94/328     0.84
slopes as predicted by the size principle, whereas black lines
                                                                        Clothing              -0.42         -0.47    84/258     0.71
are best fitting lines to the actual data. The corresponding cor-
                                                                        Vegetables            -0.41         -0.43    91/291     0.68
relation coefficients are reported in Tables 1 and 2 along with
                                                                        Professions           -0.48         -0.51    73/370     0.76
a number of other statistics to be discussed.
                                                                        Fish                  -0.48         -0.49    43/156     0.80
   Average Pearson and Spearman correlations were -0.43
                                                                        Sports                -0.58         -0.65    85/382     0.81
and -0.47, respectively for group 1, and -0.63 and -0.61 for
                                                                        Birds                 -0.36         -0.37    72/225     0.75
group2. For nearly all individual datasets in all groups, coef-
                                                                        Fruit                 -0.23         -0.37    78/233     0.74
ficients are consistently negative, with the exception of Ani-
                                                                        Reptiles              -0.23         -0.20    45/179     0.94
mals11 in group 2, which along with Animals5 are the only
                                                                        Insects               -0.49         -0.52    52/214     0.73
datasets with no published method. Correlations were gen-
                                                                        Tools                 -0.61         -0.61    62/285     0.74
erally stronger for group 2. All correlations in group 1 were
                                                                        Vehicles              -0.52         -0.57    97/322     0.93
significant at the α = 0.05 level except for the Reptiles and
                                                                        M. Instruments        -0.50         -0.56    72/218     0.90
Mammals datasets. In contrast, virtually no correlations were
                                                                        Mammals               -0.19         -0.22    84/288     0.85
significant in group 2 given the small number of features
                                                                        Weapons               -0.36         -0.38    49/181     0.88
with non-zero coefficients, however one-sample t-tests con-
firmed that the mean slopes were significantly less than 0
for both Pearson (t(16) = −7.65, p < 0.0001) and Spearman
(t(16) = −7.65, p < 0.0001) correlations.                             Table 2: Correlations between feature size and feature weight
   The FR (feature ratio) column indicates how many coeffi-           (Semantic Datasets Group 2)
cients were positive out of the total possible. Although there            Set            Pearson    Spearman        FR      R 2MP
were many more features overall in group 1, the average per-              Animals11          0.01              0   7/37     0.31
centage of features with non-zero weights was comparable                  Animals5          -0.15          -0.08    8/37    0.35
(28% and 23% respectively).                                               Birds             -0.94          -0.95    4/24    0.10
   Finally, model performance in predicting similarity (R2 ) is           Clothing1         -0.68          -0.78   6/28     0.10
reported in the R2MP column, and indicates the degree to which            Clothing2         -0.42          -0.53   12/35    0.11
the feature sets are sufficient to accurately predict human sim-          Fish              -1.00          -1.00    2/17    0.18
ilarity judgments. (R2 ) values for group 1 are markedly higher           Fruit1            -0.24          -0.29   12/38    0.19
than group 2 (which have many fewer features) and match re-               Fruit2            -0.53          -0.43    4/42    0.14
liability ceilings reported in the original experiments.                  Fruit3            -0.52          -0.64   11/42    0.25
                                                                          SIMLEX            -0.99          -1.00   3/151    0.24
              Perceptual Hypothesis Spaces                                Tools             -0.95          -0.87    5/13    0.18
While evidence for the size principle seems apparent from                 Vegetables1       -0.20          -0.08    9/31    0.31
studies of semantic hypothesis spaces, there has been no work             Vegetables2       -0.46          -0.57    9/31    0.26
attempting to verify the operation of the principle for con-              Vehicles1         -0.97          -0.71    5/24    0.06
crete objects, especially with complex, real-world instances              Vehicles2         -0.87          -0.82    5/23    0.03
of these objects such as natural images. The featural repre-              Weapons1          -0.85          -0.87    8/32    0.16
sentations of such instances are complex and include innu-                Weapons2          -0.96          -0.74    4/30    0.03
merable details not contained in semantic descriptions of the
                                                                  922

                Figure 3: Feature Size/Weight Relationships for Convolutional Neural Network Representations.
   Animals        Fruit     Furniture    Vegetables  Automobiles
                                                                     Table 3: Correlations between feature size and feature weight
                                                                     (Perceptual Dataset)
                                                                       Set               Pearson     Spearman          FR        R 2MP
                                                                       Animals              -0.32          -0.34    122/4096     0.56
                                                                       Fruits               -0.43          -0.44    302/4096     0.41
                                                                       Furniture            -0.48          -0.51    170/4096     0.38
                                                                       Vegetables           -0.41          -0.34    295/4096     0.45
                                                                       Automobiles          -0.46          -0.49    125/4096     0.31
                                                                     full feature set is meant to characterize 1000 mostly qualita-
Figure 4: Examples of stimuli from each of the 5 natural im-         tively distinct categories from which they were learned (Deng
age categories                                                       et al., 2009), whereas features from the semantic datasets
                                                                     were relevant only to the objects in each group, this discrep-
                                                                     ancy is to be expected.
perceptual binary feature sets (i.e., features from the previous        In all five datasets, correlation coefficients are moderate,
section) and non-binary perceptual feature sets (i.e., previous      negative, and significant at the α = 0.001 level. Average Pear-
work on similarity prediction).                                      son and Spearman correlation was 0.42 in both cases. Vari-
                                                                     ance explained in similarity matrices was comparable to pre-
Stimuli & Data Collection                                            vious work on predicting similarity from deep features, but
We obtained pairwise image similarity ratings for 5 sets of          was generally reduced given the constraint of binary features
120 images (animals, fruits, furniture, vegetables, vehicles)        and non-negative weights.
using Amazon Mechanical Turk, following Peterson et al.
(2016). Examples of images in each dataset are given in Fig-                                   Discussion
ure 4. The image sets represent basic level categories, with         We have attempted to provide a direct evaluation of the size
20-40 subordinate categories in each.                                principle in both semantic and perceptual hypothesis spaces.
   Subjects rated at least 4 unique pairs of images and we re-       In some cases, the correlations we report using our method
quired that at least 10 unique subjects rate each possible pair.     are weaker overall than those reported in past work (Navarro
Each experiment yielded a 120 × 120 similarity matrix.               & Perfors, 2010), but are consistently negative nonetheless.
                                                                     If anything, this discrepancy serves as a caution to trusting a
Analysis & Results                                                   single method for evaluating the size principle.
As before, we computed the pairwise multiplication of each              Across all datasets, variance explained in similarity judg-
pair of rows in F (120 images × 4096 neural features) and re-        ments ranged from .03 to .94, however these fluctuations
gressed this matrix onto the corresponding empirical similar-        don’t appear to vary systematically with the magnitude of the
ity values. The resulting weights and feature sizes are plotted      size principle effect, This may indicate that the size principle
in Figure 3 for each category, and the corresponding correla-        should emerge with respect to both “good” and “bad” feature
tions are reported in Table 3.                                       sets, so long as they are related to the objects and vary in their
   Like the previous semantic datasets, only a small portion         inclusiveness.
of the total features obtained non-zero weights, although the           Furthermore, it appears that the size principle can be shown
average percentage was much smaller (∼ 4%). Given that the           to operate in more ecologically valid stimulus comparisons
                                                                 923

such as visual image pairs. In cases such as these, the specific      regardless of the assumptions that are employed to derive it.
visual details of the image are relevant, and our feature sets        Thus, the size principle is a good candidate for a second uni-
derived from convolutional neural networks included only              versal law of generalization, and can be motivated both by
these features. There may be hundreds of small visual details         rational theories based on strong sampling and feature learn-
                                                                                        1
that are only present in novel instantiations of familiar objects     ing. Further, a |h| law can provide a solid basis for generaliz-
that we encounter on a daily basis and that actually represent        ing from multiple instances, a behavior that we should expect
the more abstract concepts used in semantic datasets. These           to find in any intelligent agent, anywhere in the universe.
results may also have implications for the method of estimat-
ing human psychological representations recently proposed                                       References
by Peterson et al. (2016). In this work, it was shown that hu-        De Deyne, S., Verheyen, S., Ameel, E., Vanpaemel, W., Dry,
man similarity judgments for natural images can be estimated                 M. J., Voorspoels, W., & Storms, G. (2008). Exemplar
by a linear transformation of deep network features, and the                 by feature applicability matrices and other dutch nor-
current results imply that this transformation is perhaps partly             mative data for semantic concepts. Behavior research
accounted for by the size principle. This finding may lead to                methods, 40(4), 1030–1048.
better methods for approximating complex human represen-              Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei,
tations based on psychological theories.                                     L. (2009). Imagenet: A large-scale hierarchical image
                                                                             database. In Computer vision and pattern recognition,
   It is apparent from the FR columns of each table that few
                                                                             2009. cvpr 2009. ieee conference on (pp. 248–255).
of the total features were used in the actual models. This may
                                                                      Hill, F., Reichart, R., & Korhonen, A. (2016). Simlex-999:
be due in part to useless features, or features associated with
                                                                             Evaluating semantic models with (genuine) similarity
too many or too few objects. It may also be due to multi-
                                                                             estimation. Computational Linguistics.
collinearity in our feature matrices (some columns are linear
                                                                      Kruschke, J. K. (1992). Alcove: An exemplar-based con-
combinations of others). These are unique consequences of
                                                                             nectionist model of category learning. Psychological
using a regression model. For this reason, our method may be
                                                                             review, 99(1), 22.
less susceptible to over-representing certain features that are
                                                                      Lawson, C. L., & Hanson, R. J. (1995). Solving least squares
redundant. On the other hand, the size principle is meant to
                                                                             problems. SIAM.
address the problem of redundant hypotheses directly, and it
                                                                      McRae, K., Cree, G. S., Seidenberg, M. S., & McNorgan, C.
may be an undesirable property of our model that these hy-
                                                                             (2005). Semantic feature production norms for a large
potheses are eliminated through other means, which is per-
                                                                             set of living and nonliving things. Behavior research
haps the cost of direct estimation of the weights in the addi-
                                                                             methods, 37(4), 547–559.
tive clustering framework. In any case, this variability in the
                                                                      Navarro, D. J., & Perfors, A. F. (2010). Similarity, feature
amount of non-redundant features does not appear to co-vary
                                                                             discovery, and the size principle. Acta Psychologica,
with the size principle in any systematic way.
                                                                             133(3), 256–268.
   The only notable discrepancy between our results and the           Nosofsky, R. M. (1986). Attention, similarity, and the
predictions of the size principle is the variation in the magni-             identification–categorization relationship. Journal of
tude of the negative slopes obtained, which does not appear                  experimental psychology: General, 115(1), 39.
to depend on model performance, number of features, or even           Peterson, J., Abbott, J., & Griffiths, T. (2016). Adapting deep
aspects of the dataset groups or individual datasets. Semantic               network features to capture psychological representa-
dataset group 2 had more large slopes (e.g., SIMLEX) than                    tions. In Proceedings of the 38th annual conference of
group 1, but also had many small slopes. Similar datasets                    the cognitive science society. Austin, TX.
from group 1 (e.g., Fruit and Vegetables) had fairly dissim-          Romney, A. K., Brewer, D. D., & Batchelder, W. H. (1993).
ilar slopes, and nearly identical datasets from group 2 (e.g.,               Predicting clustering from semantic structure. Psycho-
Fruit1 and Fruit2) had widely varying slopes. Prototypicality                logical Science, 4(1), 28–34.
doesn’t seem to matter either, since Fruit1 and Vegetables1           Shepard, R. N. (1987). Toward a universal law of general-
have smaller slopes than Fruit2 and Vegetables2, but Cloth-                  ization for psychological science. Science, 237(4820),
ing1 and Vehicles1 have larger slopes than Clothing 2 and                    1317–1323.
Vehicles2. Furthermore, we can find examples of both natural          Shepard, R. N., & Arabie, P. (1979). Additive clustering:
and artificial stimuli with comparable slopes. For these rea-                Representation of similarities as combinations of dis-
sons, it is unclear what the source of these deviations could                crete overlapping properties. Psychological Review,
be. It is possible that certain experimental contexts encour-                86(2), 87.
age a focus on certain featural comparisons that can be rep-          Tenenbaum, J. B., & Griffiths, T. L. (2001). Generaliza-
resented by a weighting of our feature sets, and so still allow              tion, similarity, and bayesian inference. Behavioral and
for good model fit. Alternatively, it may be an artifact of the              brain sciences, 24(04), 629–640.
weight estimation algorithm, in which case it will be useful          Wu, Z., Lin, D., & Tang, X. (2015). Adjustable bounded
to compare alternative methods.                                              rectifiers: Towards deep binary representations. arXiv
   Our results provide broad evidence for the size principle                 preprint arXiv:1511.06201.
                                                                  924

