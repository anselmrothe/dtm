 Optimization of American English, Spanish, and Mandarin Chinese over time for
                                                     efficient communication
                                                 John K Pate (johnpate@buffalo.edu)
                                                 Department of Linguistics, 609 Baldy Hall
                                                           Buffalo, NY 14260 USA
                              Abstract                                   actual word lengths to optimal word lengths in context and in
                                                                         isolation. The first ‘backward-looking’ study computes opti-
   Frequent words tend to be short, and many researchers have
   proposed that this relationship reflects a tendency towards ef-       mal word lengths using modern-day language data, and finds
   ficient communication. Recent work has sought to formalize            that the mismatch between optimal and actual word lengths
   this observation in the context of information theory, which es-      is smaller for older words. Moreover, the mismatch drops
   tablishes a limit on communicative efficiency called the chan-
   nel capacity. In this paper, I first show that the compositional      more rapidly, as a function of word age, when the optimal
   structure of natural language prevents natural language com-          word length is computed relative to a context-sensitive tri-
   munication from getting close to the channel capacity, but that       gram model than when it is relative to a unigram model.
   a different limit, which incorporates probability in context,
   may be achievable. Next, I present two corpus studies in three           The second ‘forward-looking’ study divides the 350-year
   typologically-diverse languages that provide evidence that lan-       period into 25-year partitions, uses language data from each
   guages change over time towards the achievable limit. These           partition to compute optimal word lengths for each partition,
   results suggest that natural language optimizes for efficiency
   over time, and does so in a way that is appopriate for compo-         and trains a regression model to predict whether the word-
   sitional codes.                                                       form appears in the next 25-year partition as a function of the
                                                                         mismatch between the word’s actual length and its optimal
Keywords: Communicative efficiency; Uniform Information                  length. This study finds that words with larger mismatches
Density; Smooth Signal Hypothesis; Noisy channel                         are less likely to ‘survive’ to the next partition, and moreover
                                                                         finds a stronger effect of mismatch relative to the context-
                          Introduction
                                                                         sensitive trigram model than relative to the unigram model.
Natural language researchers have long been interested in                Together, these two studies provide evidence that natural lan-
the prospect that natural language is organized for efficient            guage lexicons change over time in a way that reflects com-
communication: frequent words tend to be shorter than rare               municative efficiency pressures on a compositional code.
words, allowing talkers to produce shorter word-forms on av-                I start by presenting previous work on information-
erage. More recent work (Plotkin & Nowak, 2000; Genzel &                 theoretic approaches to language production, along with the
Charniak, 2002; Aylett & Turk, 2004; Levy & Jaeger, 2007;                minimal technical background necessary for this paper. I then
Jaeger, 2010; Piantadosi, Tily, & Gibson, 2011; Seyfarth,                show why natural language does not approach information-
2014) has sought to formalize this work in the context of in-            theoretic bounds, and use this result to suggest a new bound
formation theory by proposing that natural language commu-               for compositional codes that may be achievable by consid-
nicates information close to a limit called the channel capac-           ering probability in context. Finally, I present two corpus
ity, although it has left open the question of how closely the           studies that find evidence that three typologically-diverse lan-
limit is approached.                                                     guages have changed to approach this new bound.
   In this paper, I first show that it is not possible for a com-
positional code like natural language to transmit information                                     Background
close to the channel capacity. Specifically, average signal
lengths must exceed the entropy of the distribution over mes-            Linguists have proposed that language is adapted for commu-
sages by at least the Kullback-Leibler divergence of the true            nication in a general sense for decades. Zipf (1949) proposed
probability distribution over messages from a fully-factorized           the ‘Principle of Least Effort’ to explain the observation that
probability distribution in which every component of the mes-            frequent words tend to be short: frequent words tend to be
sage is statistically independent. Natural language commu-               short so that talkers usually only have to say short words.
nication must then underperform the channel capacity by at               Lindblom (1990) proposed Hyper- & Hypo-articulation the-
least this Kulback-Leibler divergence.                                   ory to explain the observation that vowels in careful speech
   However, in light of recent work (Piantadosi et al., 2011)            tend to be less centralized in formant space: talkers provide
that showed a stronger relationship between a word’s length              more distinct vowels when they believe errors are more likely.
and its average probability in context than its unigram proba-              Plotkin and Nowak (2000) proposed an explicit model of
bility, I investigate the possibility that language changes over         word formation over the course of language change in an
time so that the optimal length of a word, as computed from              information-theoretic framework, and showed, analytically
its average probability in context, better matches its actual            and via simulation, that it approached information-theoretic
length. I present two corpus studies over 350 years in Amer-             bounds as the vocabulary size increases. However, their
ican English, Spanish, and Mandarin Chinese that compare                 model considered words in isolation, but natural language ut-
                                                                     901

terances consist of sequences of words. This paper will show                b to be the size of the signal alphabet |S |, then the Shannon
that codes with the compositional structure that characterizes              information of m is the optimal signal length in signal char-
natural language cannot approach these information-theoretic                acters for message m. Adjusting signal lengths to match the
bounds, and focus on optimizing sentence lengths.                           Shannon entropy, called source coding eliminates redundancy
   Subsequent work has mainly consisted of corpus studies                   in the code, and achieves property 1: short codes.
that show that synchronic samples of natural language ex-                       Listeners may encounter noise in real-world situations, due
hibit the correlations on would expect, under an information-               to slips of the tongue on the part of the talker, distraction or
theoretic account, between different measures of word length                cognitive overload on the part of the listener, dialect differ-
or distinctiveness and word probability, both overall and in                ences, environmental noise, or other sources of noise. Noise
context (Aylett & Turk, 2004; Frank & Jaeger, 2008; Bell,                   can be countered by adding redundancy to the signal. For
Brenier, Gregory, Girand, & Jurafsky, 2009). Piantadosi et                  example, a word may differ from all other words by several
al. (2011) revisited Zipfian distributions, and compared both               phonemes, allowing the listener to recover the intended word
log word probability and average log word probability in con-               even if some phonemes are mis-perceived or masked by en-
text, operationalized as a trigram model, with word lengths,                vironmental noise. While the resulting code is more robust,
operationalized as the length of the word’s spelling in letters.            it is also longer, and we might worry that signals will have to
They found that, of the two probability measures, average log               become arbitrarily long to drive the error rate toward zero.
probability in context exhibited a stronger correlation with                    The Noisy Channel theorem shows that an arbitrarily low
word lengths. At first glance, this result would seem to con-               error rate can be achieved with signals that are not arbitrar-
tradict an information-theoretic approach: the optimal length               ily long, as long as they do not exceed the channel capac-
of a word in isolation simply is its negative log probability,              ity (Shannon, 1948). The channel capacity depends on both
with an appropriate choice of base for the logarithm. How-                  H(m m) and the uncertainty about the intended signal, given
ever, I soon show that average probability in context is a more             the received signal. Adding redundancy, such as pronounc-
appropriate measure for optimizing sentence lengths.                        ing words more slowly, that anticipates likely noise is called
   In this paper, I return to Plotkin and Nowak’s (2000) pro-               channel coding, and makes signals robust but still short.
posal that language adapts towards these bounds over time,                      For our purposes, the crucial observation is that it is not
using a corpus-based methodology and an emphasis on the                     possible to get arbitrarily close to the channel capacity if it is
relationship between words and their sentential contexts.                   not possible to obtain a source code that is arbitrarily close
                                                                            to the entropy of the distribution over messages. The next
Information theory                                                          section shows that, for compositional codes like natural lan-
In the information-theoretic framing of language, a talker has              guage, optimal source coding is not possible.2
a message m that is a sequence of message characters mi from
some alphabet of message characters M . For example, M                                Compositional codes and optimality
may be the set of lexical entries or, as in CCG, a set of (syn-             This section shows that optimal source coding is not possi-
tactic category, semantic category) pairs. The message cannot               ble for a compositional code like natural language. If a code
be transmitted directly, so the talker encodes it into a signal s           is optimal and compositional, then it follows that the com-
that is a sequence of signal characters si from some alphabet               ponents of every message are statistically independent. How-
of signal characters S .1 For example, S may be the inventory               ever, this is not true for natural language, since, e.g., transitive
of syllables or phonemes of the language.                                   verbs tend to appear with at least two noun phrases.
   An efficient code has two properties. First, it is short: the                By ‘compositional,’ I mean only that natural language mes-
number of signal characters per message character, on aver-                 sages consist of components that are realized the same way
age, is low. Second, it is robust: the probability that the lis-            across different messages, and that the length of the signal
tener fails to identify the correct message is low. The length              for a message is the total length of the signal for each com-
of the shortest possible code depends only on the probability               ponent of that message. Setting lm to be the length of the
distribution over messages P(m      m), and is given by the entropy
                                                                            signal for message m and lmi to be the length of the signal for
of that distribution:                                                       component mi , compositionality provides:
                                                     
                                                 1
                  H(m m) = ∑ P(m) logb                              (1)                                          |m|
                            m∈m m              P(m)                                                       lm = ∑ lmi                         (2)
                                                                                                                i=1
where m = M + is the set of all messages (i.e. all sequences of
message characters). The log term is called the Shannon in-                 For example, if a message is a sequence of lexical entries, and
formation of m, and the entropy is just the expected Shannon                a signal is a sequence of phones, then Equation 2 says that
information under the probability distribution P(m). If we set              the length of a sentence in phones is the sum of the lengths
    1 All the results follow straightforwardly for structured messages          2 I here consider only discrete signals and messages. The contin-
as long as there is a deterministic linearization, such as reverse Pol-     uous case requires either a limit on the power of the signal or for
ish notation for tree-structured messages.                                  source and channel coding to be considered simultaneously.
                                                                        902

of the phonological forms of the lexical entries in that sen-                Optimizing towards the new bound
tence.3 Equation 2 is not trivial. Arithmetic codes, for exam-               While the bound in Equation 7 shows that natural language
ple, encode each message as a number between 0 and 1 that                    does not approach the channel capacity, natural languages
is determined by the conditional probability of each message                 may still adapt over time for communicative efficiency to-
character given the previous message characters; an individ-                 wards the less efficient bound. In fact, the findings of
ual message character is not directly expressed in any part of               Piantadosi et al. (2011) suggest that languages adapt to mini-
the signal, and lmi is not even defined.                                     mize KL(Pm ||Qm ). Piantadosi et al. examined how a word’s
   Now assume that the length of the sentence lm for each                    length (in orthographic letters) relates to its unigram prob-
message m is optimal. Because the optimal signal length for a                ability and its probability in context (operationalized as a
message m is − logb (P(m)), the probability distribution over                smoothed trigram model). Across all eleven languages they
messages Pm can be recovered from lm by exponentiating:                      examined, word lengths had a stronger relationship with av-
                              Pm (m) = b−lm                          (3)     erage probability in context than unigram probability.
                                                                                I propose the following interpretation of their result. Mes-
Since only sentence lengths are assumed to be optimal, com-                  sage characters are lexical entries, signal characters are ortho-
ponent signal lengths lmi may not be negative log probabil-                  graphic letters, and probability in context is Pm . While Qm is
ities. They do, however, assume an implicit distribution for                 determined by word lengths, Pm is determined by a stochas-
which they are least sub-optimal (MacKay, 2003, Ch. 5):                      tic grammar and lexicon together with typical real-world sit-
                                                                             uations. Their results suggest that, as a speech community
                               b−lmi
                Qm (mi ) =             ; z = ∑ b−lmi                 (4)     gains experience with the use of lexical entries in real-world
                                 z           m ∈M
                                              i                              situations, the grammar, including the lexicon, adapts so that
   Equations 2, 3, and 4 imply that each message component                   Pm is better approximated by Qm . This adaptation could be
is statistically independent:                                                achieved by adjusting the grammar, narrowing or broadening
                                                                             word meanings, or deleting lexical entries whose length often
                                   |m|          |m|
          Pm (m) = b−lm = b∑i=1 −lmi = b∑i=1 logb (zQm (mi ))        (5)     differs substantially from their optimal in-context length.
                           |m|
                                                                                The next two sections present corpus studies that look at
                                        def                                  adaptation of this sort over centuries in three languages.
                  = z|m| ∏ Qm (mi ) = Qm (m)                         (6)
                          i=1
                                                                                                     Corpus studies
There does not seem to be any notion of message in natu-
                                                                             I now present two corpus studies that find evidence of opti-
ral language that allows for statistically independent message
                                                                             mization relative to probability in context over time for En-
components. For example, messages may be high-level event
                                                                             glish, Spanish, and Mandarin Chinese. The first ‘backwards-
representations, but such messages that include transfer tend
                                                                             looking’ study relates a word’s mismatch with its optimal
to include at least three entities (a giver, a receiver, and a
                                                                             length to its age. If the lexicon evolves over time for effi-
thing being transferred), and such messages that include edi-
                                                                             cient communication, the lengths of oldest words should most
ble entities tend to include entities that can eat. Alternatively,
                                                                             closely match their optimal lengths. Moreover, to the extent
messages may be syntactic analyses, but such messages with
                                                                             that efficiency pressures respect sentence length, there should
a determiner tend to have at least one noun, and such mes-
                                                                             be a stronger relationship between a word’s age and its mis-
sages with a complementizer tend to have at least two main
                                                                             match with optimal lengths under a trigram model than be-
verbs. While other framings are possible, they do not appear
                                                                             tween a word’s age and its mismatch under a unigram model.
to satisfy the independence assumption above. Thus, natural
                                                                                The ‘forwards-looking’ study uses a sequence of language
language is not information-theoretically optimal.
                                                                             models, estimated in 25-year partitions, to predict whether
   More specifically, the average signal length of the best
                                                                             a word appears in the next partition based on how well its
compositional source code must exceed the entropy of the
                                                                             length matches its optimal length under each language model.
true distribution over messages Pm by at least the Kullback-
                                                                             If language change reflects efficiency pressures, words with
Leibler divergence of the fully factorized distribution Qm
                                                                             many extra characters should be less likely to remain in use;
from the true distribution:
                                                                             and if efficiency pressures respect sentence lengths, the effect
                       H(Pm ) + KL(Pm ||Qm )                         (7)     of mismatches under trigram models should be stronger.
Intuitively, language uses at least an extra KL(Pm ||Qm ) sig-               Corpus study 1 – Looking backwards
nal characters per message character because it incorrectly                  In this study, I used a large dataset containing texts from about
assumes the message characters are statistically independent.                1990 to about 2010 for each of the three languages to compute
    3 Composition operations that involve copying, such as Suffixauf-        synchronic unigram and trigram language models for each
nahme in Old Georgian (Michaelis & Kracht, 1996), present an in-             language. The language models are used to compute optimal
teresting wrinkle. If they can be handled by introducing an integer          lengths for each word in and out of context by subtracting the
coefficient for each lmi , the ultimate independence result of this sec-
tion still holds. In any case, they make the signal longer, so they          optimal lengths from the actual lengths to quantify extra char-
should not present a more efficient bound than Equation 7.                   acters. I used Google books, a dataset of scanned books, to
                                                                         903

                 Table 1: Study 1 dataset sizes                      Chinese language models, I used ‘story’ documents from the
                                                                     3rd edition of the Spanish Gigaword dataset of newswire text
              Language Model              Regression                 from 1993 to 2010 (Ângelo Mendonça, Jaquette, Graff, &
  Dataset     Tokens                Unigrams Trigrams                DiPersio, 2011) and the Tagged Chinese Gigaword version
  English     71,531,906            81,742      24,965,851           2.0 dataset of newswire text from 1991 to 2004 (Ren Huang,
  Spanish     279,744,284           545,708     74,144,973           2009), respectively. While written Chinese does not separate
  Chinese     26,800,660            3,182       13,642,166           words with whitespace, this dataset is segmented into words.
                                                                        For each language, I discarded punctuation and words that
                                                                     contained a symbol that was not part of the usual character set
                                                                     for that language, estimated unsmoothed trigram and unigram
                                                                     probabilities. The datasets for regression were obtained by
                                                                     discarding words that did not appear in Google Books after
                                                                     1650, producing datasets with sizes as reported in Table 1.
                                                                        These particular languages were chosen because they ap-
                                                                     peared in Google Books, allowing us to obtain an estimate of
                                                                     word age, and because they use words in very different ways.
                                                                     Spanish has relatively rich derivational and inflectional mor-
                                                                     phology, with agreement for person and number for verbs and
                                                                     number and gender for adjectives. While English also has rel-
                                                                     atively rich derivational morphology, it has little inflectional
                                                                     morphology with few agreement constraints. Mandarin Chi-
                                                                     nese occupies a morphological extreme, with no inflectional
                                                                     morphology or agreement.
                                                                     Method For each word token in CoCA and Gigaword
                                                                     datasets, I computed the optimal length of the word under its
                                                                     unigram probability and probability in context, operational-
                                                                     ized as its trigram probability. The numbers of ‘extra’ letters
                                                                     relative to each model euni and etri are then the actual length
                                                                     minus the optimal length:
                                                                                     euni (w) = l(w) − (− logb (P(w)))
Figure 1: Heatmaps of actual length minus optimal word                  etri (wi |wi−2 , wi−1 ) = l(wi ) − (− logb (P(wi |wi−2 , wi−1 )))
length for trigram (left) and unigram (right) models, as a func-
tion of the word’s earliest appearance in Google books. The          where b is the size of the signal alphabet. English and Spanish
blue line is a GAM fit.                                              both have a mostly alphabetic orthography, with roughly one
                                                                     letter per sound, so I simply set b to the number of distinct let-
                                                                     ters in these datasets. For English b = 27 (a-z plus hyphen),
estimate when each word of the synchronic time slice first ap-       and for Spanish, b = 33 (with some additional accented let-
peared, and perform a regression of the extra character mea-         ters). Chinese orthography has one character per syllable, and
sure against year of first appearance, probability model type,       so similarly provides a good indication of word length, but
and their interaction, to identify how word length inefficiency      the alphabet size is more complicated. The strict phonotac-
varies as a function of word recency and probability model           tics of spoken Chinese lead to a syllabic inventory of about
type. A positive coefficient for year of appearance will indi-       1, 500 syllables, but our Chinese dataset contained 6, 780 dis-
cate that more recent words are longer than they should be,          tinct characters (many characters are homophonous). I set
and a negative coefficient for the interaction will indicate a       b = 1, 518, the number of distinct syllables in the CEDict pro-
stronger relationship between year of appearance and ineffi-         nouncing dictionary, to reflect the size of the ‘syllable alpha-
ciency relative to a trigram model than between year of ap-          bet’ for spoken Chinese (CC-CEDICT, 2016).4
pearance and inefficiency in isolation.                                 I performed linear regressions of extra letters against the
Data I approximated a word’s year of first appearance as             word’s year of first appearance, probability model type, and
the first year that it appeared in the Google Books unigram          an interaction between the word’s first appearance and proba-
records in each language (Michel et al., 2010).                      bility model type. To make the regressions easier to interpret,
   To estimate the American English language models, I used          I subtracted 1650 from the year of first appearance, so that the
the spoken portion of the Corpus of Contemporary Amer-               oldest words had a year of first appearance of zero.
ican English (CoCA) (Davies, 2008), which contains news                  4 I obtained similar results when using b = 6, 780, the number of
broadcasts from 1990 to 2012. To estimate the Spanish and            distinct characters.
                                                                 904

Table 2: Coefficients of one linear regression each for American English, Mandarin, and Spanish, of extra letters (English,
Spanish) or extra characters (Mandarin) against first appearance, a main effect of probability model type (with unigram coded
as 1), and an interaction between first appearance and probability model. All coefficients are significant (p < 0.01).
                                                                                      Am. English      Spanish       Mandarin
            Intercept                                                                 3.289            4.408         1.067
            Year of first appearance (since 1650)                                     0.006481         0.007399      0.00214
            Which language model                                                      -0.491           -2.506        -1.288
            Years of first appearance (since 1650) × Which language model             -0.001465        -0.001438     -0.000349
Results Figure 1 presents hexagram-binned heatmaps with                     Moreover, the extra characters relative to the trigram model
a Generalized Additive Model fit for each language of extra              decreased faster than the extra characters relative to the uni-
letters against year of first appearance, separated by language          gram model. This is a remarkable finding, since it is much
model. All cases show a broad trend where older words have               harder to optimize for the trigram model – there are many
fewer extra letters. The trend is roughly linear except for the          trigram contexts but only one unigram ‘context,’ and, un-
latest decades; information-theoretic pressures may be differ-           der this operationalization of ‘word,’ a word has only one
ent for recently-coined words.                                           length. However, as previously discussed, there are good
   Table 2 presents coefficients from the three regressions of           reasons to optimize towards a context-sensitive probability
extra characters against a word’s year of first appearance, the          model. Communicative efficiency ultimately depends on sen-
probability model used, and their interaction. Each intercept            tence lengths, not word lengths directly, so considering con-
expresses the number of predicted extra letters or characters            text can make sentences shorter even if it does not minimize
under the trigram model for words that first appeared in 1650.           the typical length of individual words.
Adding the ‘Which Language Model’ coefficient to the inter-
cept obtains the predicted extra letters or characters under the
                                                                         Corpus study 2 – Looking forwards
unigram model for words that first appeared in 1650.                     This corpus study looks for evidence that a word is less likely
   The ‘Year of first appearance’ coefficient expresses how              to remain in use if it has more extra characters. For each
many extra characters we expect a word to have for each year             language, I divided the 350 years of Google Books data de-
that it is younger than the oldest words. For all three lan-             scribed above into 14 partitions of 25 years each, and esti-
guages, this coefficient is positive, indicating that younger            mated a unigram and a trigram language model for each of the
words tend to be longer, than older words, relative to their             first 13 partitions to compute extra characters for each word
ideal length under the trigram model. Dividing this coeffi-              and trigram in each partition under each probability model.
cient into 1 obtains how old we expect a word to be before               To guard against OCR errors in Google Books, I computed
an additional letter or character has been ‘optimized’ away.             extra characters only for words that also appeared in the lan-
American English optimizes one letter every 154 years, Span-             guage model datasets from Study 1. I then performed a logis-
ish optimizes one letter every 135 years, and Mandarin Chi-              tic regression that predicted whether each word that appeared
nese optimizes one character every 467 years.5                           in partition n also appeared in partition n + 1 using the extra
   Finally, the interaction between year of first appearance             characters measure, probability model type, and an interac-
and model type expresses the effect of a word’s year of ap-              tion between extra characters and the probability model type.
pearance under the unigram model minus the effect of a                   Results Table 3 presents strikingly consistent regression re-
word’s year of appearance under the trigram model. The                   sults across the three languages. The large intercepts indicate
coefficients are negative but smaller in absolute magnitude              that most words carry over from one partition to the next. As
than ‘Year of first appearance,’ which indicates that first ap-          the unigram model is again coded as 1, the negative main ef-
pearance still has a lengthening effect relative to the unigram          fect of extra characters indicates that words with more extra
model, but a weaker one. American English optimizes an ex-               letters relative to a given partition’s trigram model are less
tra letter relative to the unigram model only every 199 years,           likely to persist in the next 25-year partition. Moreover, the
Spanish optimizes an extra letter only every 168 years, and              positive coefficient of the interaction indicates that the effect
Mandarin optimizes an extra character only every 558 years.              of extra letters relative to the unigram model is weaker: the
   These results show that words that first appeared in                  coefficients suggest the effect of unigram mismatch is about
books recently tend to be further from their information-                half the effect of trigram mismatch in English, two-thirds in
theoretically optimal lengths than words that first appeared             Spanish, and about one-third in Mandarin.
in books several decades ago, and so provide evidence of op-
timization of the lexicon towards efficiency bounds.                                              Conclusion
    5 As                                                                 This paper has answered an important question about nat-
         the CEDict pronouncing dictionary has an average length
of about 3.1 non-tone pinyin letters, or 2.8 phonemes, per character     ural language communication, whether talkers approach
type, the optimization rate of Mandarin is similar to the others.        information-theoretic limits on efficiency, in the negative. Be-
                                                                     905

Table 3: Coefficients of a logistic regression each for American English, Mandarin, and Spanish, of appearance in the next 25-
year partition against extra letters or characters, a main effect of probability model type (unigram coded as 1), and an interaction
between extra characters and probability model. All coefficients are significant (p < 0.01).
                                                                         Am. English       Spanish   Mandarin
                         Intercept                                       11.772            7.477     10.415
                         Extra letters or characters                     -0.316            -0.320    -2.360
                         Which language model                            -1.015            -0.852    -2.415
                         Extra characters × Which language model         0.165             0.101     1.713
cause language is compositional and natural language mes-                English: 520 million words, 1990 – present. (Available
sages are highly interdependent, natural language cannot ap-             online at http://corpus.byu.edu/coca/)
proach information-theoretic limits on efficiency. I have used         Frank, A., & Jaeger, T. F. (2008). Speaking rationally: Uni-
this result to propose a new bound that appreciates probabil-            form information density as an optimal strategy for lan-
ity in context, and interpreted a previous result as evidence            guage production. In Proceedings of CogSci (pp. 933–
that languages optimize for this more appropriate bound.                 938).
   I then performed two corpus studies that examined how               Genzel, D., & Charniak, E. (2002). Variation of entropy and
the mismatch between a word’s actual length and its optimal              parse trees as a function of sentence number. In Proceed-
length relates to its preservation over the course of language           ings of the Association for Computational Linguistics.
change. The first ‘backwards-looking’ study found, using op-           Jaeger, T. F. (2010). Redundancy and reduction: Speakers
timal lengths computed using fairly homogenous modern-day                manage syntactic information density. Cognitive Psychol-
corpus data, that present-day words more closely match their             ogy, 61, 23–62.
optimal lengths if the word has been in use for a long time.           Levy, R., & Jaeger, T. F. (2007). Speakers optimize informa-
Moreover, this first study found that the mismatch according             tion density through syntactic reduction. In Proceedings of
to probability in context decreased more rapidly as words age.           NIPS.
The second ‘forwards-looking’ study found that if a word’s             Lindblom, B. (1990). Explaining phonetic variation: A
length more closely matches its optimal length under a lan-              sketch of the H & H theory. In W. Hardcastle & A. Mar-
guage model computed in one 25-year partition, it is more                chal (Eds.), Speech production and speech modelling (pp.
likely to be retained in the next 25-year partition. Moreover,           403–439). Kluwer Academic Publishers.
extra letters relative to probability in context was a stronger        MacKay, D. J. C. (2003). Information theory, inference,
predictor than extra letters relative to a unigram model. To-            and learning algorithms. Cambridge University Press.
gether, these results indicate that natural language lexicons            (http://www.inference.phy.cam.ac.uk/mackay/itila/)
develop over time towards an information-theoretic efficiency
bound that is appropriate for compositional codes.                     Michaelis, J., & Kracht, M. (1996). Semilinearity as a syn-
                                                                         tactic invariant. In Proceedings of LACL.
                    Acknowledgements                                   Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray,
I thank Rui Chaves for feedback on a very early draft, and               M. K., Brockman, W., . . . Aiden, E. L. (2010). Quanti-
anonymous reviewers for helpful comments.                                tative analysis of culture using millions of digitized books.
                                                                         Science, 331.
                          References                                   Piantadosi, S., Tily, H., & Gibson, E. (2011). Word lengths
Ângelo Mendonça, Jaquette, D., Graff, D., & DiPersio, D.               are optimized for efficient communication. Proceedings of
   (2011). Spanish Gigaword Third Edition LDC2011T12                     the National Academy of Sciences, 108(9), 3526.
   [Computer software manual]. Web download. Philadel-                 Plotkin, J. B., & Nowak, M. A. (2000). Language evolution
   phia, PA.                                                             and information theory. Journal of Theoretical Biology,
Aylett, M., & Turk, A. (2004). The smooth signal redundancy              205, 147–159.
   hypothesis: A functional explanation for relationships be-          Ren Huang, C. (2009). Tagged Chinese Gigaword Version 2.0
   tween redundancy, prosodic prominence, and duration in                LDC2009T14 [Computer software manual]. Web down-
   spontaneous speech. Language and Speech, 47(1), 31–56.                load. Philadelphia, PA.
Bell, A., Brenier, J. M., Gregory, M., Girand, C., & Jurafsky,         Seyfarth, S. (2014). Word informativity influences acous-
   D. (2009). Predictability effects on durations of content             tic duration: Effects of contextual predictability on lexical
   and function words in conversational English. Journal of              representation. Cognition, 133(1), 140–155.
   Memory and Language, 60, 92–111.                                    Shannon, C. E. (1948). A mathematical theory of communi-
CC-CEDICT. (2016). http://www.mdbg.net/chindict/.                        cation. The Bell System Technical Journal, 27, 379–423.
   (Accessed: 2016 - 30 - 05)                                          Zipf, G. K. (1949). Human behavior and the principle of
Davies, M. (2008). The Corpus of Contemporary American                   least effort. Addison-Wesley.
                                                                  906

