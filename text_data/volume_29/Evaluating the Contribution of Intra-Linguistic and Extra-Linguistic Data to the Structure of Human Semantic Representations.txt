UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Evaluating the Contribution of Intra-Linguistic and Extra-Linguistic Data to the Structure of
Human Semantic Representations
Permalink
https://escholarship.org/uc/item/9ss6k1pt
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Andrews, Mark
Vigliocco, Gabriella
Vinson, David
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                    Powered by the California Digital Library
                                                                       University of California

   Evaluating the Contribution of Intra-Linguistic and Extra-Linguistic
            Data to the Structure of Human Semantic Representations
                                       Mark Andrews (m.andrews@ucl.ac.uk)
                                    Gabriella Vigliocco (g.vigliocco@ucl.ac.uk)
                                         David Vinson (d.vinson@ucl.ac.uk)
                                   Department of Psychology, University College London,
                                                          26 Bedford Way
                                                       London, WC1H 0AP
                                                         United Kingdom
                           Abstract                                  any one of these data types had been considered inde-
                                                                     pendently and to the exclusion of the other. To address
   We describe Bayesian models that learn semantic rep-              this concern, we considered the combined effects of both
   resentations from either extra-linguistic data or intra-          sources of data and introduced a probabilistic model that
   linguistic data, or from both in combination. We evalu-           learns semantic representations on the basis of both at-
   ate the validity of these models using three human-based
   measures of semantic similarity. The results provide              tributional and distributional data simultaneously. We
   strong evidence for the hypothesis that human semantic            then compared this model with probabilistic models that
   representations are the product of the statistical combi-         learn semantic representations from each data source in-
   nation of extra- and intra-linguistic sources of data.            dependently.
                                                                        In our above mentioned work, we did not provide an
                                                                     analysis of how well the semantic representations learned
                        Introduction                                 by our model predict human data. The primary aim of
For the purposes of this paper, we use the term seman-               this paper is to address this issue. For this purpose,
tic representation to refer to a language user’s mental or           we have also found it necessary to elaborate and extend
cognitive representation of the meaning of words. We in-             upon the models that we previously used. As such, in
formally define this as the knowledge that allows the lan-           what follows, we provide Bayesian models of semantic
guage user to infer, amongst other things, which words               representations that are learned from either attributional
are similar or identical in meaning, what are the semantic           data or distributional data, or from both in combina-
or ontological categories to which a word belongs, what              tion. We then evaluate the validity of these models us-
(if anything) are the referents of a word. Our general               ing three human-based measures of semantic similarity:
aim is to consider how both extra-linguistic and intra-              word-association norms, semantic-priming results from
linguistic data can be used to acquire this knowledge.               a lexical decision task, and interference patterns from a
Extra-linguistic, or attributional data, is data that is de-         picture-word interference task.
rived from our perception and interaction with the phys-
ical world, and in particular, from the perceived physi-                              Model Description
cal attributes or properties associated with the referents           We provide Bayesian models that learn semantic repre-
of words1 . In contrast, intra-linguistic, or distributional         sentations from examples of attributional data, or from
data, is derived from the statistical characteristics within         distributional data, or from both combined. The proba-
a language itself, or how a given word is distributed                bilistic models we employ for each of the various data
across different spoken or written texts2                            types are described graphically in Figure 1. The at-
   In previous literature, it has been repeatedly demon-             tributional model (leftmost) describes any given word
strated that semantic representations can be learned                 wf as a probability distribution over a set of binary
from either attributional data alone, e.g. McRae, Sa,                attributes, such that {ym[f ] : 1 ≤ m ≤ M [f ] } is a set
and Seidenberg (1997); Vigliocco, Vinson, Lewis, and                 of bit vectors, each being an instance of the referent
Garrett (2004); McClelland and Rogers (2003), or dis-                of the word wf . These probability distributions are
tributional data alone, e.g. Lund and Burgess (1996);                compositions of a basic repertoire of latent distributions
Landauer and Dumais (1997); Griffiths and Steyvers                   ψ = {ψ1 . . . ψ KAtt } that intuitively correspond to clus-
(2002). However, in previous work of our own (Andrews,
                                                                     ters of interrelated attributes each describing basic char-
Vigliocco, & Vinson, 2005), we described how, for the
                                                                     acteristics of the attributional data. The distributional
most part throughout this literature, the contribution of
                                                                     model (second left) describes texts as multinomial dis-
    1
      For example, the word apple refers to objects in the world     tribution over words, such that {wn[t] : 1 ≤ n ≤ N [t] } is
whose perceived attributes or properties include being red or        a sample of words from text t. These distributions are
green, round, shiny, smooth, crunchy, juicy, sweet, tasty, etc       compositions of latent distributions φ = {φ1 . . . φKDist }
    2
      We use the term text here in a very general sense to refer     that intuitively correspond to discourse-topics in a cor-
to any coherent and self-contained piece of written or spo-
ken language. This could include, for example, a newspaper           pus of text. The combined model (second right) de-
article, a spoken conversation, a letter or email message, an        scribes texts as probability distributions over words, and
essay, a speech, etc.                                                words as distributions over attributes. These distribu-
                                                                 767

                                                                                             yn[t]                                            yn t
                                                                                                                                                 [ ℄
               ym[f ]                            wn[t]
                                                                                   ψ
   ψ                                        φ                                                                                                 xyn  [t ℄
                                                                          β                  wn[t]
                                                                                                                                          yt
                                                                                   φ                                                    y     wn  [t ℄
               xm[f ]                    wf      xn[t]
                                                                          γ
                                                                                             xn[t]                                        
                                            γ                                                                       PSfrag repla ements
   β                  1 ≤ m[f ] ≤ M [f ]
                                                       1 ≤ n[t] ≤ N [t]
                                                                                                                                               xwn  [t ℄
                                                                                                   1 ≤ n[t] ≤ N [t]
                                                                                                                                          wt
                                                                                                                                        w                1  n [t℄ Nt
                                                                                                                                                                     [ ℄
                πf                                πt                                          πt
                                                                                                             1≤t≤T
                                                                                                                                               t
                              1≤f ≤F                         1≤t≤T
                                                                                                                                                             1tT
                α                                 α                                           α
Figure 1: Bayesian Networks for the (from left) attributional model, distributional model, combined model, inde-
pendent model. Each node corresponds to a variable, parameter or hyper-parameter, with observed variables being
shaded. Note that in the rightmost diagram, the variable ξt is an indicator variable denoting the identity of the text
at time t. All other variables are described in the main text.
tions are compositions of coupled latent distributions                      Semantic Representations
ψ, φ = {ψ1 , φ1 . . . ψ KComb , φKComb }, each intuitively cor-             The latent distributions in each model intuitively cor-
responding to an attribute cluster coupled to a discourse-                  respond to that model’s semantic knowledge. We can
topic. Finally, our so-called independent model (right-                     provide examples of this knowledge by drawing samples
most) acts as a experimental control model to our com-                      from the mean of the posterior distribution over the la-
bined model. Like the combined model, the indepen-                          tent distributions. Examples for the cases of the attri-
dent model describes texts as probability distributions                     butional, distributional and combined models are shown
over words, and words as distributions over attributes.                     in Table 1(a). From these examples, it can be seen
However, these distributions are de-coupled and inde-                       that the latent-distributions are clusters of inter-related
pendent.                                                                    attributes (in the case of the attributional model), or
   Each of the four models can be seen as a hierarchi-                      words (in the distributional model), or both (in the com-
cal mixture model. Each observed data point is sampled                      bined model)4 . Importantly, in the case of the combined
from a single latent distribution that is indicated by an                   model, attribute clusters align with discourse-topics that
unobserved indicator variable (this is denoted by the in-                   are consistent with the same general meaning.
dexed x variables in each diagram in Figure 1). These                          Within each model, each word can be expressed as a
indicator variables are themselves sampled from multi-                      distribution over that model’s latent distributions. From
nomial distributions (denoted by the indexed π variables                    this we can measure the correspondence between any
in the diagrams). We treat the latent distributions, as                     pair of words in each model. In general, in a model
well as the multinomial distributions over the indicator                    whose unobserved indicator variable is denoted by x the
variables, as the parameters of the model. The objective                    correspondence P between words wi and wj is given by
of learning is to infer the posterior distribution of these                 P(wj |wi ) = {x} P(wj |x)P(x|wi ). In Table 1(b), using
parameters. This was accomplished using the Markov                          this formula, and averaging over samples from the pos-
Chain Monte Carlo method of Gibbs sampling. For this,                       terior over the parameters, we provide examples of the
conjugate prior distributions in the form of Dirichlet dis-                 near-neighbors of a set of example words according to
tributions for ψ and π, and Beta distributions for ψ were                   each of our four models.
used. Each of these distributions had controlling hyper-
parameters (the α, β and γ variables in the diagrams).                                             Model Evaluation
Model selection by marginal likelihood optimization was
                                                                            We evaluate each model by comparing its set of inter-
used to find optimal values for these hyper-parameters,
                                                                            word similarities with human-based measures of seman-
as well as for the total number of latent distributions in
                                                                            tic similarity. There is, of course, no flawless means by
each model.
                                                                            which to measure human semantic representations or the
   The data used for model training were as follows: Fol-                   inter-word similarities implied by them. In light of this,
lowing common practice, we obtained the attributional                       we have used a collection of methods that will hope-
data for 456 words by way of speaker-generated attribute                    fully lead to converging evidence. These are the Nel-
norms collected in Vigliocco et al. (2004). The distri-                     son word-association norms5 , and semantic-priming re-
butional data was 2245 texts3 taken from the British
                                                                                4
National Corpus (BNC).                                                            We do not display examples from the independent model
                                                                            as these will, by design, be identical to the independent prod-
                                                                            uct of the attributional and distributional models’ latent dis-
   3                                                                        tributions.
     Each text was approximately 200-250 words in length.
                                                                                5
In total there were 7818 unique word types.                                       http://w3.usf.edu/FreeAssociation/
                                                                        768

                                       Table 1: Semantic Knowledge and Inter-word Similarities in the Models
                    (a) Examples of latent distributions learned by the attributional model (upper left), distributional model (lower
                    left) and combined model (right). The latent distributions in the combined model are coupled distributions
                    over both attributes and words.
                             mouth       transport      foot       food                        read                      fast          make        explode
                             tongue        vehicle       leg       oven                       write                       leg        construct      danger
                                                                             Attributes
                              taste         wheel       ball       heat                       story                     move            tool        destroy
                              food            fly      force      cook                        pencil                  exercise        building      action
                               eat          drive       arm         eat                   communicate                    walk          build          war
                             throat      passenger     pain     prepare                      question                destination       wood            kill
                           taste-bud         seat     game     consume                         pen                       foot          house           fire
                              sense        motor        win      mouth                     knowledge                    speed            fix       demolish
                            hospital        party     market       rate                        book                       run          build         killed
                              death       election     price        cut                        film                     team           house           fire
                               died       political   prices    interest                       draw                      race          repair       attack
                                                                             Words
                           operation      national     sales      rates                        page                      next            fix          war
                           treatment     opposition   stock      figures                       star                    winner         building     security
                            injuries      elections     sold   economic                      written                   grand        equipment       women
                            medical         held        sale      trade                       series                     field     construction       shot
                             cancer         seats       buy    industry                     television                running           steel        bomb
(b) Examples of the near neighbors of a set of five words (boldface) according to the attributional, distributional, combined and
independent models. The five words were chosen so as to highlight the differences between the four models.
                     ankle     exchange         punch       knife       threat                              ankle      exchange      punch         knife        threat
                     elbow          pay         punch        knife       threat                              knee          buy           hit        knife       threat
                      ankle         buy          chin         axe         warn                               ankle         pay        punch          kill       attack
   Attributional                                                                             Combined
                      knee      exchange         slap         saw      threaten                             elbow           sell       down        blood       threaten
                        toe          sell          hit     scissors      argue                                toe       exchange        slap      assault       attacks
                        cut        trade        pound      hatchet        bark                                cut         sales       knock          axe         killed
                       ache      acquire         bark       chisel       growl                                 leg       selling       chin         dead        armed
                    shoulder     donate        shoulder     razor           hit                             injury        trade       injury      dagger        murder
                     thumb          loan        knock        chop      challenge                              ran        money        pound        arrest      military
                        leg      borrow          face      dagger          fear                            broken        billion     hitting      charges         kill
                      twist       accept         neck        drill         kill                            walked        markets        fell      murder        killings
                      walk      donation        break       sword       murder                             injured         loan       blood        illegal     violence
                     injury      market          fight      court        threat                             injury       market        fight        court      northern
                         fit       stock        world       blood      terrorist                               fit        stock       world          saw        threat
   Distributional
                     squad         price         title       case       violence                            squad         price        title         case      violence
                                                                                             Independent
                      coach       prices        punch        knife     northern                               side        prices      punch         knife     community
                      knee         sales     heavyweight   murder     community                               cup         sales        chin         man           fear
                     fitness    financial       boxing     alleged     province                              ankle      exchange        slap        then       violence
                      ankle     customer        knock       prison      loyalist                             knee          sold       knock        heard         warn
                       side     exchange       manager       trial        army                              elbow          sale          hit         axe       threaten
                       cup      business        battle     appeal        forces                              draw           sell     manager        door         argue
                     season      markets          lost      judge          fear                             break       business     boxing         razor        clash
sults from lexical decision tasks and interference patterns                               igram probability of a word from the vocabulary in the
from picture-word interference task, both obtained from                                   corpus is simply the relative frequency of occurrence of
Vigliocco et al. (2004). These three methods were cho-                                    that word. We can use this probability distribution as
sen so as to provide complementary measures of human                                      a null model of the extent to which word vi predicts vj
semantic representations. In particular, it is arguable                                   as it specifies that ∀i, P(vj |vi ) , P(vj ). In effect, this
that word-association norms are primarily a measure of                                    means that the highest probability words predicted by
syntagmatic relationships. While, by contrast, behav-                                     any words will be simply the highest frequency words.
ioral measures like semantic priming and picture-word                                        Note that the attributional model contains only a sub-
interference data are arguably based more upon paradig-                                   set of the words (i.e. concrete words for which attributes
matic rather than syntagmatic relationships. Syntag-                                      exist) that also occur in the distributional, combined, in-
matic relations are said to hold between two words that                                   dependent and unigram models. Accordingly, we divided
commonly co-occur within a sentence, often when both                                      our analysis in such a way that we compare all the mod-
are of different parts of speech. Examples are easy to                                    els with one another using the subset of words that they
come by: sit-chair, drink-wine rain-wet. On the other                                     all share, and we compare the larger vocabulary models
hand, paradigmatic relations hold between words that                                      with one another using all available words.
have similar roles with respect to the other words, or syn-
tactic structures, within sentences. Examples of paradig-                                 Bayes Factors Based Hypothesis Testing
matically related pairs would include eat-drink, sit-stand,                               In keeping with the Bayesian nature of the models, we
wet-dry. By using multiple human based measures of                                        explore the use of Bayesian hypothesis tests rather than
semantic relationships that are either more influenced                                    the more commonly used classical, or sampling-theory,
by syntagmatic over paradigmatic relationships, or vice                                   approaches. The relative merits of these two approaches
versa, we can hopefully provide a more general or unbi-                                   is subject to (sometimes intemperate) debate, but it is
ased picture of human semantic representation against                                     beyond the scope of this paper to either review or con-
which to compare our models.                                                              tribute to this debate. Suffice it to say that the Bayesian
  For the purposes of comparison we also include in our                                   approaches are ideally suited to the analysis we wish to
analysis what we refer to as a unigram model. The un-                                     pursue.
                                                                              769

                  log λ         Evidence for M1                  shows the results for the subset of words that all mod-
               log λ < 0            Negative                     els share (see above note). Note that the differences in
             0 ≤ log λ < 1             Weak                      the log of these probabilities is equivalent to the log of
            1 ≤ log λ < 2.5          Positive                    the ratio of the probabilities. Hence to evaluate log λ for
            2.5 ≤ log λ < 5           Strong                     any pair of models, simply subtract the log probability
               log λ ≥ 5           Very strong                   of one from the other. Upon inspection of the graphs, it
                                                                 is evident that there is very strong evidence (according
 Table 2: Interpretation of λ in the Bayes Factor Test.          to the Jeffreys definition of the term) for the superior-
                                                                 ity of the combined model’s predictiveness of the word
   The analysis we will pursue is often referred to as the       association norms. In particular, the order of perfor-
Bayes factor test. Given any test data-set Dtest and any         mance of the models (proceeding from best to worst) is
two alternative models M0 and M1 (parameterized by               the combined model, independent model, distributional
θ0 and θ1 , respectively) the Bayes factor for M1 relative       model, unigram model (when all words are used) and
to M0 is given by                                                the combined model, attributional model, independent
                         R                                       model, distributional model and unigram model (when
         P(Dtest |M1 )     dθ1 P(Dtest |θ1 )P(θ1 |M1 )           the subset of words is used). In both cases, there is very
   λ=                   =R                             . (1)
         P(Dtest |M0 )     dθ0 P(Dtest |θ0 )P(θ0 |M0 )           strong evidence for these orderings6 .
                                                                 Lexical Decision Based Priming Semantic prim-
The term λ is a measure of evidence for the superiority          ing using a lexical decision task is one of the most com-
of M1 over M0 . Jeffreys (1961) provides a scale of in-          monly used behavioral measures of the semantic rela-
terpretation for λ as shown in Table 2. Clearly, this test       tionship between pairs of words. In the study carried
is easily applied to our model comparisons, whereby we           out by Vigliocco et al. (2004), priming data for a set of
integrate over the posterior probabilities of the param-         prime-target word-pairs, all of which occur in our data-
eters for each model, evaluating the probability of data         sets, were collected. The speed of response to the target
set for each parameter value. In our case, however, we           word, given the presence of the prime, is compared to
must replace the integral with a sum over samples from           the speed of response of the target word in the presence
the posteriors.                                                  of an obviously unrelated baseline word (matched to the
Word Association Norms The Nelson word associ-                   prime word on salient characteristics such as length, fre-
ation norm data-set is a collection of the close word-           quency, etc.). This allows each prime-target pair to be
associates of 5019 English words. These have been col-           represented in terms of the relative speed up of response
lected from human participants under controlled circum-          to the target in the presence of the prime.
stances, and each word associate is assigned a probability          In order to assess how well each model predicts this
indicating the relative frequency of its being paired with       data, we used a separate Bayesian linear regression
the target word. Of the 5019 words, a subset of 2824             model for each model. In each case, we regressed rel-
also occurred in our text-corpus vocabulary.                     ative speed-up (msec) for target-word vi given prime-
   The word association norms data-set can be re-                word vj on the log of P(vi |vj ) derived from each model.
described as a (sparse) V × V matrix W, where V is the           The quantity P(vi |vj ) was obtained by averaging over the
number of unique words in our text-corpus (i.e. 7818),           posterior of the parameters in each model. The outcome
and Wij is the probability that word wj is associated            of the Bayesian regression is a posterior distribution over
with target word wi . If either wi or wj do not occur in         the parameters of the linear-Gaussian regression model.
the association norms set, then Wij is set to 0. From W          From this, we can calculate the marginal likelihood of
we can define W′ as the V × V matrix with Wij′ = 1 if            the priming-data for the case of each model, and com-
Wij > 0 and zero otherwise.                                      pare these in a Bayes factor test as in Equation 1. We
   The likelihood of the Nelson norms for any one of our         have plotted the log of these marginal likelihoods in the
models, with specific parameter values denoted by θ, is          upper right sub-figure of Figure 2. Note that, as be-
given by                                                         fore, the differences between any pair of log probabilities
                            Y                                    will be equal to the log of the ratio of these probabili-
               P(W′ |θ) =
                                               ′
                                 P(vi |vj , θ)Wij ,      (2)     ties. As can be seen in this figure, there is very strong
                           {i,j}
                                                                 evidence (using the Jeffreys’ definition) in favor of the
                                                                 superiority of the combined model as a model of P. The
By sampling from each model’s posterior distributions            exact ordering of the models’ performance is (from best
over its parameters, and averaging over these samples,           to worse): combined model, distributional model, inde-
we can thus calculate how how probable the norms’                pendent model, attributional model and unigram model.
word-pairs are according to each model. In other words,          This ordering is also strongly supported by the results
given the set of word-associate pairs in the Nelson norms,       of the Bayes factor test.
how likely are these data according to each of four mod-
els’ predictions of word relationships.                              6
                                                                       For the purposes of comparison, we performed an analy-
   The log of these predictive likelihoods for each model        sis of these data using non-parametric statistics and sampling
are shown in the left column of Figure 2. The upper left         based null hypothesis tests and the relative ordering of the
shows the results for all available words. The lower left        models’ performances was identical.
                                                             770

                           -32500
                                                                                                                                                              -330
          Log Likelihood                                                                                                                     Log Likelihood
                           -35000
                                                                                                                                                              -335
                           -37500
                                                                                                                                                              -340
                           -40000
                                         D                             C                                   t            U                                            At               D                   C                      t     U
                                          is t                             om                            en                  am                                                 l     is t                om                   en           am
                                                       l                        d                      nd                 gr                                                  na                    l             d          nd          gr
                                                   iona                       ne                     pe                 ni                                                 tio                  iona            ne         pe          ni
                                                ut                          bi                     de                                                                 tribu                  ut               bi         de
                                             rib                                                 In                                                                                       rib                          In
                           -1750
                                                                                                                                                              -190
                           -2000
          Log Likelihood                                                                                                                     Log Likelihood
                           -2250                                                                                                                              -195
                           -2500
                                                                                                                                                              -200
                                    At                     D
                                                           is
                                                                                    C                  In                   U                                        At               D                   C            In              U
                                    tri                        tri                  om                  de                  ni
                                                                                                                                gr                                    tri             is
                                                                                                                                                                                          tri             om            de             ni
                                                                                                                                                                                                                                           gr
                                     bu                         bu                      bi                  pe                                                         bu                  bu                 bi            pe
                                          tio                                            ne                    nd                 am                                        tio                                ne            nd            am
                                           na                        tio                     d                                                                               na                 tio                d
                                                l                     na   l                                     en t                                                             l              na   l                          en
                                                                                                                                                                                                                                  t
Figure 2: Log Likelihoods for three human-based data-sets for each of the models under investigation. The left-most
column provides results for the word-association norm data. The right-most column provides the results for priming
and picture-word interference data. See text for details.
   For the purpose of comparison, it is also useful to con-                                                                                    a baseline distractor) can be used as a measure of the
sider the results from a standard, or non-Bayesian, linear                                                                                     semantic similarity between vi and vj .
regression test. Commonly used measures from this type                                                                                            As in the case of the priming data, we used separate
of analysis include a measure of the strength of the lin-                                                                                      Bayesian linear regression models, regressing naming la-
ear relationship between the variables R, the amount of                                                                                        tency against the log of P(vi |vj ) in each model (aver-
variance in the dependent variable accounted for by the                                                                                        aging over parameters). From this, we can calculate the
independent variable R2 , and the p-value significance of                                                                                      marginal likelihood of the picture-word interference data
these statistics p. These are as follows:                                                                                                      according to each model. The marginal likelihoods can
                                                                                                                                               be compared in a Bayes factor test, as before. We have
             Model                                    R                          R2                p-value
                                                                                                                                               plotted the log of these marginal likelihoods in the lower
         Attributional                               .31                         .09                 .006                                      right sub-figure of Figure 2. The relative pattern of re-
         Distributional                              .29                        .086                 .008                                      sults is almost identical to that seen in the priming data
          Combined                                   .39                         .16                .0002                                      case. The combined model shows the strongest predic-
         Independent                                 .22                         .05                  .04                                      tive power with the ordering from strongest to weakest
           Unigram                                  .078                        .006                  .49                                      model is combined model, distributional model, indepen-
                                                                                                                                               dent model, attributional and unigram models. These
Picture Word Interference In a picture-word inter-                                                                                             results are strongly supported by the Bayes factor test.
ference task, naming latencies of drawings of objects (or                                                                                      As with the case of the priming data, for the purposes
actions/events) are recorded. When these pictures are                                                                                          of comparison, we can mention standard measures from
presented simultaneously with a word, and if that word                                                                                         non-Bayesian regression analysis, i.e. R, R2 and p:
is semantically related to the picture, naming latencies
increase. This increase resembles the Stoop phenomenon                                                                                                                   Model                                  R                R2        p-value
whereby the semantically related word interferes with                                                                                                                Attributional                             −.24              .06         .09
the activation of the picture’s name. In Vigliocco et al.                                                                                                            Distributional                            −.35              .12         .01
(2004), picture-word interference data was collected for
                                                                                                                                                                      Combined                                 −.38              .14        .009
a set of word pairs (all of which occur in all our models).
                                                                                                                                                                     Independent                               −.26              .06         .08
If the picture depicts word vi and the distractor word is
vj , the slow-up for naming the picture as vi (relative to                                                                                                             Unigram                                 −.19              .03         .19
                                                                                                                                       771

                       Discussion                                would allow the correspondences between the two data-
The general aim in this paper has been to consider how           types to be apparent, and to be exploited. For example,
semantic representations are acquired. To answer this            if the child learned that the word cat refers to creatures
we have identified two major types of data from which            with claws and whiskers and tails, etc. and that it also
semantic information can be attained. We have referred           co-occurs with terms like dog, pet, owner, etc., it may
to these as attributional and distributional data types.         also infer that creatures with claws and whiskers and
These represent data types that are, respectively, extra-        tails, etc., are conceptually related to the words dog, pet,
linguistic and intra-linguistic in their origin. Of particu-     owner, etc. From this, we can see that while using either
lar concern to us has been the question of how these two         extra-linguistic or intra-linguistic data can allow seman-
distinct data types can be combined to learn coherent            tic representations to be learned by discovering the cor-
semantic representations. We have provided a model of            relations within that specific data-type, using the combi-
the semantic representations that are learned from attri-        nation of both allows the discovery of correlations both
butional and distributional data taken in combination,           within and between these data-types.
and compared this to the representations learned from
either source taken independently. Our specific aim has
                                                                                        References
then been to evaluate these models against human-based           Andrews, M., Vigliocco, G., & Vinson, D. (2005). The
measures of semantic representations.                               role of attributional and distributional information in
   Although the relative performance of each model to               semantic representation. In B. Bara, L. Barsalou, &
predict the human data is not identical across the three            M. Bucciarelli (Eds.), Proceedings of the Twenty Sev-
different data-sets there are obvious and compelling gen-           enth Annual Conference of the Cognitive Science So-
eral trends. For example, and unsurprisingly, all four of           ciety.
the attributional, distributional, combined and indepen-         Griffiths, T., & Steyvers, M. (2002). A probabilistic
dent models outperform the null model on all data-sets.
                                                                    approach to semantic representation. In Proceedings
While superior performance against a null-model is not
surprising, it does serve as a worthwhile sanity check, ef-         of the 24th annual conference of the cognitive science
fectively corroborating the impression given by Table 1             society.
that each of these models is providing (at the very least)       Jeffreys, H. (1961). Theory of probability. Oxford, UK:
a modest description of the meaning of words.                       Clarendon Press.
   If the unigram model represents a lower-bound on the          Landauer, T., & Dumais, S. (1997). A solutions to
models’ predictive performances, then it appears as if the          Plato’s problem: The Latent Semantic Analyis theory
combined model represents an upper-bound. The com-                  of acquistion, induction and representation of knowl-
bined model outperforms all other models consistently               edge. Psychological Review, 104, 211-240.
across all three sets of human-based measures. This              Lund, K., & Burgess, C.              (1996).     Producing
corroborates the impression given by Table 1(b) that                high-dimensional semantic spaces from lexical co-
the combined model provides a more comprehensive and                occurrence. Behavior Research Methods, Instrumen-
valid account of the meanings of words than do either
                                                                    tation, and Computers, 28, 203-208.
the attributional, distributional or independent models.
As such, we can take this as direct evidence in favor of         McClelland, J., & Rogers, T. (2003). The parallel dis-
our primary hypothesis that human semantic represen-                tributed processing approach to semantic cognition.
tations are the product of the statistical combination,             Nature Reviews Neuroscience, 4 (4), 310-322.
and not simply the sum or average, of attributional and          McRae, K., Sa, V. de, & Seidenberg, M. (1997). On the
distributional data-types.                                          nature and scope of featural representation of word
                                                                    meaning. Journal of Experimental Psychology: Gen-
Conclusion                                                          eral, 126, 99-130.
The results imply a certain picture of how word-                 Vigliocco, G., Vinson, D. P., Lewis, W., & Garrett, M. F.
meanings are learned. This can be described by reference            (2004). Representing the meanings of object and ac-
to following scenario: A child learning his or her native           tion words: The featural and unitary semantic space
language will regularly experience words referring to, for          hypothesis. Cognitive Psychology, 48, 422-488.
example, everyday objects in the context of one or more
of their referents. On the other hand, the words that the
child is learning are not necessarily heard in isolation,
but rather will regularly occur in the context of mean-
ingful sentences. From this, the data from which the
child can learn word-meanings occur in two forms simul-
taneously: There is the set of attributes associated with
a given word, and the set of textual contexts in which
that word occurs. While it has been repeatedly shown in
previous literature that either one of these sources can
provide information from which word-meanings can be
learned, learning from both data-types in combination
                                                             772

