UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Representational Shifts During Category Learning
Permalink
https://escholarship.org/uc/item/2x06p5bd
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Vanpaemel, Wolf
Navarro, Daniel J.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                             Representational Shifts During Category Learning
                                       Wolf Vanpaemel (wolf.vanpaemel@psy.kuleuven.be)
                                  Department of Psychology, K.U. Leuven, Leuven, B-3000, Belgium
                                       Daniel J. Navarro (daniel.navarro@adelaide.edu.au)
                              School of Psychology, University of Adelaide, Adelaide SA 5005, Australia
                              Abstract                                   plar models were evaluated, leaving the vast majority of po-
                                                                         tential category representations unexplored. Since these data
   Prototype and exemplar models form two extremes in a class
   of mixture model accounts of human category learning. This            provide a natural testing ground for exploring the flexibility
   class of models allows flexible representations that can inter-       of category representations, the current paper undertakes pre-
   polate from simple prototypes to highly differentiated exem-          cisely this analysis. The plan of the paper is as follows. We
   plar accounts. We apply one such framework to data that af-
   ford an insight into the nature of representational changes dur-      first introduce the formal modeling framework, and then dis-
   ing category learning. While generally supporting the notion          cuss the data provided by Smith and Minda (1998). We then
   of a prototype-to-exemplar shift during learning, the detailed        analyze two key data sets from this paper, looking first to ex-
   analysis suggests that the nature of the changes is considerably
   more complex than previous work suggests.                             tract an explicit model for the individual differences in per-
                                                                         formance (Webb & Lee, 2004) before analyzing the data set
   Keywords: cognitive models, category learning, abstraction,
   representational change.                                              using the Varying Abstraction Model (VAM) introduced by
                                                                         Vanpaemel et al. (2005) which provides a much richer set of
                          Introduction                                   potential category representations.
Classification tasks present people with stimuli and their                          Treating Categories as Mixtures
accompanying category labels, and require label prediction               In most theories of human concepts (e.g., Nosofsky, 1986;
for novel stimuli. Starting with seminal work in the 1970s               Love et al., 2004), people are assumed to have some inter-
(Rosch, 1978), the psychology of categorization has been                 nal representation of a category C that provides a probability
assumed to be best thought of in terms of a kind of “fam-                distribution p(· | C) over possible objects in the world. When
ily resemblance”. For example, prototype theories (Reed,                 translated into formal models, it is typical to assume that this
1972) represent a category using a single prototypical stim-             distribution is a mixture of several component densities, gen-
ulus, which need not necessarily correspond to a real object.            erally on the implicit assumption that each “element” of the
Similarity to a category is defined as similarity to the proto-          category representation constitutes a psychologically distinct
type. In contrast, exemplar theories (Medin & Schaffer, 1978;            component of the category.
Nosofsky, 1986) represent a category as the set of all of its
previously observed members (its exemplars), and the cate-               The General Approach
gory similarity as the aggregated similarity to the exemplars.           We begin by introducing the general approach, in which the
More recently, it has been argued (Love, Medin, & Gureckis,              psychological representation of a category is modeled as a
2004; Anderson, 1991; Rosseel, 2002; Vanpaemel, Storms,                  mixture of simpler components. If the internal representation
& Ons, 2005) that exemplar representations and prototype                 contains q components, the probability assigned by category
representations constitute the two extremes of a spectrum of             C to the ith stimulus xi is given:
models. Although different authors have adopted slightly dif-                                             q
ferent formalisms to advance their viewpoint, they share the
                                                                                                         X
                                                                                           p(xi | C) =       wj p(xi | j),             (1)
common view that human conceptual structure is sufficiently                                              j=1
flexible to adopt simple, highly abstracted “prototype-like”
representations at times, but can also accommodate highly                where p(xi | j) is the density assigned by component j to the
differentiated “exemplar-like” representations at others.                ith stimulus xi , and wj weights the contribution made by each
   An elegant experimental test of this idea was conducted by            component. The general mixture formulation in Equation 1
Smith and Minda (1998), involving a series of categorization             is often translated into a specific statistical model by applying
experiments. In each experiment, prototype and exemplar ac-              the exponential law for generalization developed by Shepard
counts were contrasted at different stages of the learning pro-          (1987). In view of this law, it is natural to treat each of the
cess, to see which model provided a superior account. Al-                component distributions as an exponential density,
though the overall pattern of performance across experiments                                  p(xi | j) ∝ e−λd(xi ,µj ) ,              (2)
is complex, the general finding was that exemplar models
tend to be favored late in learning, with the possibility that           where µj denotes the internal representation of the jth com-
prototype models are favored early in learning (but see, e.g.,           ponent to the category, λ is a scaling parameter that governs
Nosofsky & Zaki, 2002 for a contrasting view). However,                  the specificity of the generalization away from that represen-
one drawback to the study is that only prototype and exem-               tation, and d(·, ·) describes a psychological distance function.
                                                                    1599

                              Prototype Model                                    Intermediate Model                                  Exemplar Model
                        0.1                                                0.1                                                0.1                                Table 1: Stimulus representations for the non-linearly separable cat-
 Probability Density                                Probability Density                                Probability Density
                                                                                                                                                                 egories used by Smith and Minda (1998), experiments 2 (panel a)
                                                                                                                                                                 and 3 (panel b). In both panels each column corresponds to a feature
                       0.05                                               0.05                                               0.05                                (i.e., letter), and each row to a stimulus.
                                                                                                                                                                  (a)   A    0   0   0   0    0   0
                         0                                                  0                                                  0                                             1   0   0   0    0   0
                         −1           0         1                           −1           0         1                           −1           0         1                      0   1   0   0    0   0
                              Stimulus Location                                  Stimulus Location                                  Stimulus Location                        0   0   1   0    0   0     (b)   A   0   0    0   1
                                                                                                                                                                             0   0   0   0    1   0               0   1    0   0
Figure 1: Category densities for the a one-dimensional category                                                                                                              0   0   0   0    0   1               1   0    1   1
                                                                                                                                                                             1   1   1   1    0   1               0   0    0   0
consisting of items located at x = (−0.2, −0.1, 0, 0.3, 0.4) and                                                                                                        B    1   1   1   1    1   1           B   1   0    0   0
λ = 10. The density on the left is produced by the prototype model,                                                                                                          0   1   1   1    1   1               1   0    1   0
and the one on the right by the exemplar model. The middle density                                                                                                           1   0   1   1    1   1               1   1    1   1
                                                                                                                                                                             1   1   0   1    1   1               0   1    1   1
belongs to a model that groups the three stimuli on the left and the                                                                                                         1   1   1   0    1   1
two stimuli on the right.                                                                                                                                                    1   1   1   1    1   0
                                                                                                                                                                             0   0   0   1    0   0
When applying such models, it is typical to assume that λ has
the same value for all components. While there are many                                                                                                          ci ∈ 1, 2, . . . , q indexes the representational cluster to which
possibilities for a psychological distance function, a common                                                                                                    the ith stimulus belongs. As such q can be interpreted as the
choice is to use one of the attention-weighted Minkowski r                                                                                                       level of abstraction of the category representation. In the con-
metrics,                                                                                                                                                         strained framework, the representation of the jth cluster is
                                                                                                                                    ! r1                         taken to be the average of the representations   of its constituent
                                                                             m                                                                                                                      P
                                                                             X                                                  r                                stimuli. Thus, µjk = (1/nj ) i|ci =j xik where nj denotes
                                   d(xi , µj ) =                                      ak |xik − µjk |                                      ,          (3)
                                                                                                                                                                 the number of stimuli that belong to cluster j. Applying the
                                                                             k=1
                                                                                                                                                                 same logic, the mixture weights are constrained to reflect the
where ak represents the proportion of attention applied to the                                                                                                   proportion wj = nj /n of category members that fall in the
kth stimulus dimension. To provide a concrete illustration of                                                                                                    cluster.
the approach, Figure 1 shows three different mixture repre-                                                                                                         In this framework, the partitions cA and cB of categories
sentations of the same category, corresponding to prototypes,                                                                                                    A and B define a particular model for these categories, with
exemplars and an intermediate case. In order to describe hu-                                                                                                     an overall level of abstraction qA + qB . The model’s free
man behavior in a two-alternative forced-choice task between                                                                                                     parameters are the attention weights ak and the specificity
categories A and B, it is typical to apply a standard choice                                                                                                     λ (the metric r is taken to be a property of the stimulus
rule,                                                                                                                                                            space itself, and is held fixed). This formulation ensures that
                                    p(xi | A)                                                                                                                    all models have the same number of free parameters. The
          P (xi ∈ A | xi ) =                       .       (4)
                             p(xi | A) + p(xi | B)                                                                                                               standard prototype and exemplar models are special cases
                                                                                                                                                                 of the VAM: a category represented using a single-cluster
A Simplified Framework                                                                                                                                           partition c = (1, 1, . . . , 1) has a prototype representation,
The general mixture model formulation is broad enough to                                                                                                         and a category represented using a cluster for every stimulus
cover a range of approaches. However, the simplest proposal                                                                                                      c = (1, 2, . . . , n) has an exemplar representation. In between
is perhaps the one introduced by Vanpaemel et al. (2005).                                                                                                        these two extremes, however, lie a wealth of infrequently-
Unlike some approaches (e.g., Love et al., 2004; Anderson,                                                                                                       explored representational possibilities.
1991) it makes no particular assumptions about how human
learning takes place, and it is much more constrained than                                                                                                                  Looking for Representational Shifts
the mixture model formulation adopted by Rosseel (2002) in                                                                                                       It has been argued (Smith & Minda, 1998) that much of the
terms of how the weights wj and probabilities p(xi | j) are                                                                                                      categorization literature is overly-reliant on experiments that
assigned. While the simplicity of this arrangement does not                                                                                                      provide participants with a great deal of training before at-
necessarily make it a superior cognitive model, it provides a                                                                                                    tempting to measure the structure of their conceptual repre-
very clean framework in which to ask questions about repre-                                                                                                      sentation. With that in mind, Smith and Minda (1998) con-
sentational structure without introducing any additional psy-                                                                                                    ducted a series of experiments aimed to show that the cate-
chological principles that could confound the analysis.                                                                                                          gory representation changes as learning progresses. In this
   In Vanpaemel et al.’s (2005) Varying Abstraction Model                                                                                                        paper, we focus on their experiments 2 and 3, involving non-
(VAM) the mixture components µj and wj that might other-                                                                                                         linearly separable categories. For both experiments, the stim-
wise be treated as free parameters are fully determined by a                                                                                                     uli took the form of pronounceable nonsense words (e.g., ga-
specific partition of category members into a set of clusters.                                                                                                   fuzi, daki), on the assumption that each letter corresponds to
Each cluster implies a specific psychological representation                                                                                                     a feature. In experiment 2, both categories consisted of seven
µj , that can be viewed as a kind of sub-prototype. Thus, a                                                                                                      exemplars possessing six features each, and were designed
category of n exemplars represented in terms of q clusters                                                                                                       to be well-differentiated category structures even despite the
can be described using the vector c = (c1 , . . . , cn ), where                                                                                                  fact that both categories contain obvious exception items: the
                                                                                                                                                          1600

Table 2: The series of representations used to build models for ex-                                         1
periment 2. Each column corresponds to a partition, either of the
                                                                                      Proportion Correct
category A exemplars or the category B exemplars.                                                          0.8
                cA    1   1    1   1    1   1   1                                                          0.6
                      1   1    1   1    1   1   2
                      1   1    1   1    1   2   3
                      1   1    1   1    2   3   4                                                          0.4
                      1   1    1   2    3   4   5
                      1   1    2   3    4   5   6                                                          0.2
                      1   2    3   4    5   6   7
                cB    1   1    1   1    1   1   1
                      1   1    1   1    1   1   2                                                           0
                      1   1    1   1    1   2   3                                                                2    4      6     8   10
                      1   1    1   1    2   3   4                                                                    Trial Segment
                      1   1    1   2    3   4   5
                      1   1    2   3    4   5   6                       Figure 2: Empirical learning curves for all 16 participants in the
                      1   2    3   4    5   6   7
                                                                        experiment 2. The data segregate naturally into two groups.
                                                                                                            1
logical structure of the categories is shown in Table 1(a). In
                                                                                      Proportion Correct
contrast, experiment 3 used the smaller, less differentiated                                               0.8
category structures shown in Table 1(b). Both experiments
                                                                                                           0.6
involved 16 participants who were presented with 560 trials,
divided into 10 segments of 56 trials each. On each trial, one                                             0.4
of the stimuli was presented and the participant was asked to
                                                                                                           0.2
classify it as a member of category A or category B. Feed-
back was provided after each trial. Smith and Minda (1998)                                                  0
                                                                                                                 2    4      6     8   10
analyzed the data by fitting exemplar and prototype model to
                                                                                                                     Trial Segment
each segment, in order to find evidence for representational
transitions during learning. They concluded that a shift had            Figure 3: Empirical learning curves for all 16 participants in the
                                                                        experiment 3. The data segregate naturally into three groups.
occurred during experiment 2, but not during experiment 3.
   Although the idea of a representational change is in agree-
ment with the spirit of the VAM, Smith and Minda (1998)                 a more detailed picture of the nature of the representational
only considered prototype and exemplar models, which                    changes.
makes it difficult to trace out these changes in any detail.
To address this, in the remainder of the paper, we reanalyze            Part 1: Individual Differences
the data from these experiments using the VAM, exploring                Recent work (Webb & Lee, 2004) has emphasized the fact
the full class of potential category representations. For the           that category learning tasks show strong individual differ-
smaller category, we fit all 15 × 15 = 225 category models,             ences, and highlighted the fact that averaging across peo-
but for the larger category the 877 × 877 = 769129 models               ple may lead to substantial distortions. In Smith and Minda
are too many to work with, particularly since model fitting             (1998), this problem was solved by fitting models to each
is required for 10 different trial segments. Accordingly, in            participant independently. However, in addition to inflating
this case we used only a smaller set of 7 × 7 = 49 mod-                 the risks of overfitting, this approach is unwieldy and time-
els, with one model at each level of abstraction (i.e., number          consuming. A faster and more robust approach emphasizes
of clusters) for each category. These 49 models were found              both the similarities and differences between people, and
by applying a simple average-link clustering procedure to the           seeks to find groups of participants with similar patterns of
stimulus representations in Table 1(a), and are shown in Ta-            performance.
ble 2. Each model was fit to the observed classification ac-               To apply this idea, we took the learning curves for each
curacies using maximum likelihood estimation, which means               participant, and partitioned them into meaningful groups. To
that the values of the free parameters maximizing the likeli-           do so, we applied the Minimum Description Length (MDL)
hood of observing the data were determined. The free param-             clustering technique pioneered by Kontkanen, Myllymäki,
eters were the scaling parameter λ and five or three attention          Buntine, Rissanen, and Tirri (2005) and extended to learn-
weights ak in experiment 2 or 3 respectively.                           ing curves by Navarro and Lee (2005). This method, which
                                                                        is based on information theoretic ideas, assigns two observa-
           A Varying Abstraction Analysis                               tions to the same group only if this allows a better compres-
Our analysis involves three stages. Firstly, we analyze the             sion of the overall data set. Although the technical details are
individual differences in the data, in order to make sure that          complicated (see Grünwald, 1998, for details on MDL), what
we can draw sensible conclusions about what particular par-             matters for the current purposes is that the approach allows
ticipants were doing. Secondly, we reproduce Smith and                  us to find a statistically-optimal method of grouping people’s
Minda’s (1998) prototype-to-exemplar result within the con-             data. Applying this method, we were able to extract three
text of the VAM. Finally, we use this framework to develop              strikingly different types of performance for the experiment
                                                                 1601

                   49               prototype model                                                       14
                                                                                   Level of Abstraction
                   40                                                                                     12
                                                                                                          10
                   30
            Rank
                                                                                                          8
                   20                                                                                     6
                                    exemplar model
                                                                                                          4
                   10
                                                                                                          2
                    0                                                                                     0
                         2    4      6     8    10                                                             2    4      6     8   10
                             Trial Segment                                                                         Trial Segment
Figure 4: The rank of the prototype and exemplar models at each        Figure 6: The level of abstraction of the best model at different
segment for experiment 2.                                              stages of learning during experiment 2.
                   225                                                                                    8
                                     prototype model
                                                                                   Level of Abstraction
                   180
                                                                                                          6
                   135
            Rank                                                                                          4
                   90
                   45                                                                                     2
                                     exemplar model
                    0
                         2    4      6     8    10                                                        0
                             Trial Segment                                                                     2    4      6     8   10
                                                                                                                   Trial Segment
Figure 5: The rank of the prototype and exemplar models at each
segment for experiment 3.                                              Figure 7: The level of abstraction of the best model at different
                                                                       stages of learning during experiment 3.
3 data (Figure 3) and two probably-distinct groups for the ex-
periment 2 data (Figure 2). Due to space constraints, we re-           experiment 2, there is an early advantage for the prototype
strict the analyses in this paper to the largest groups, indicated     model, and a late advantage for the exemplar model, with the
by the solid lines in both figures.                                    changeover point located between segments four and five. In
                                                                       experiment 3, only during the first segment does the proto-
Part 2: Comparing Prototypes to Exemplars                              type model outrank the exemplar model, and the extent of the
Our approach to analyzing the prototype-to-exemplar shift              exemplar advantage grows throughout the experiment.
differs from Smith and Minda’s in several ways. Firstly, we               The dramatic shift in the relative fortunes of the prototype
fit a much broader range of models to the data (225 for ex-            and exemplar models illustrated in Figure 4 and 5 suggests
periment 3, and 49 for experiment 2), and used a maximum               that some kind of representational shift has taken place dur-
likelihood estimation rather than the least squares approach           ing the learning process, particularly with respect to the larger
adopted in the original paper. Secondly, we fit data that were         category structure used in experiment 2. This was essentially
aggregated in an optimal fashion, as discussed in the previous         the conclusion in Smith and Minda (1998), but our applica-
section. Finally, unlike Smith and Minda (1998), we did not            tion of the VAM allows us to gain further insight in the nature
include a “guessing parameter”.                                        of the representational shift, a topic which we turn to in the
   Despite the very substantial differences in representational        next section.
possibilities considered, the choice of loss function, individ-
ual differences, and guessing behavior, the basic pattern of           Part 3: A Richer View of Representational Change
exemplar and prototype performance remains intact. This is             The analyses reported by Smith and Minda (1998) and shown
most naturally shown by looking at how well the two mod-               in our Figures 4 and 5 imply that the representational shift in-
els fared at different stages of learning, when compared to            volves a jump from a prototype representation to an exemplar
all (225 or 49) models under consideration, as illustrated in          representation. However, this is somewhat misleading, in the
Figures 4 and 5. In one key respect, this pattern is far more          sense that the shift appears to be considerably more complex.
compelling in the current analysis than in the original: in            To illustrate this, we classified every model in terms of its
Smith and Minda (1998), the exemplar and prototype models              overall level of abstraction (i.e., qA +qB ). Figures 6 and 7 dis-
are evaluated without consideration of the other representa-           play, at all 10 segments, the level of abstraction of the model
tional possibilities. Happily, when a broad spectrum of rep-           that best accounts for the behaviorial data. For both experi-
resentational possibilities are included to provide an appro-          ments, the level of abstraction of the best model changes sys-
priate context, the substantive finding remains unchanged. In          tematically across trials. Experiment 2 in particular shows
                                                                1602

                             segment 1                    segment 2                      segment 3                       segment 4                   segment 5
                  −265                       −255                         −260                            −260                           −245
            Fit   −270                       −260                         −265                            −265                           −255
                  −275                       −265                         −270                            −270                           −265
                         2       4   6   8            2       4   6   8          2        4      6    8          2        4    6     8          2      4    6       8
                             segment 6                    segment 7                      segment 8                       segment 9                   segment 10
                  −150                       −100                         −100                            −50                            −50
            Fit   −200                       −175                         −175                            −150                           −150
                  −250                       −250                         −250                            −250                           −250
                         2       4   6   8            2       4   6   8          2        4      6    8          2        4    6     8          2      4    6       8
                                                                              Level of Abstraction
Figure 8: Scatterplot of the fit of all the 225 models (y-axes) versus the level of abstraction qA +qB (x-axes) at each segment for experiment 3.
Over the first three segments, the best models tend to be more prototype-like (though the prototype model itself performs poorly). As learning
progresses, the profile of good models shifts, and from segments 5-10 it is clear that the best models all have a differentiated, exemplar-like
structure.
                             segment 1                    segment 2                  segment 3                       segment 4                      segment 5
                  −480                       −395                      −300                           −330                           −260
            Fit   −482                       −400                      −320                           −332                           −270
                  −484                     −405                         −340                           −334                          −280
                     7                        7                            7                              7                             7
                         5                  7 5                          7 5                            7 5                           7 5                           7
                             3           5                3           5              3               5               3             5            3               5
                                 1 1 3                        1 1 3                      1 1 3                           1 1 3                       1 1 3
                             segment 6                    segment 7                  segment 8                       segment 9                  segment 10
                  −200                       −200                      −150                           −150                           −100
                  −250                       −220                      −200                           −200                           −200
            Fit
                                             −240
                  −300                                                  −250                           −250                          −300
                     7                            7                        7                              7                             7
                         5                    7       5                  7 5                            7 5                           7 5                           7
                             3           5                3           5              3               5               3             5            3               5
                                 1 1 3                        1 1 3                      1 1 3                           1 1 3                       1 1 3
Figure 9: Fit of all the 49 models at each level of abstraction for both categories at each segment in experiment 2. In each subplot, the
dependent variable is the model fit, measured in terms of the log-likelihood. The independent variables are the levels of abstraction for each
category, qA and qB . As is clear from inspection of the plot, there is a fairly sharp change in the profile of models at around segment 4-5.
a steady progression from prototypes to exemplars. In the                                     get a more detailed picture of the pattern of changing model
first two trial segments, the very best model is the prototype                                fits during the course experiment 3, instead of looking at the
model, and in the last segment the best model is very nearly                                  best fitting models only, we can look at all 225 possible rep-
an exemplar model (having an level of abstraction of 12, out                                  resentational models. This is illustrated in Figure 8 which
of a maximum possible of 14). However, the transition here                                    shows 10 scatterplots, one for each trial segment. Each plot
is steady, very nearly linear. In other words, while the ex-                                  displays the level of abstraction and the data fit for each of all
emplar model improves so dramatically against the prototype                                   the 225 possible representational models. In the first four seg-
model that the shift looks discrete (as in Figure 4), the inclu-                              ments, highly abstract representations are able to account for
sion of a broader class of models suggests that the change is                                 the data relatively well (though notably the prototype model
somewhat more gradual across the best fitting models (as in                                   itself fits poorly), while very detailed exemplar-like represen-
Figure 6). Moreover, examination of Figure 7 suggests that a                                  tations perform poorly. In contrast, from segment five on-
small shift takes place in experiment 3, which was not evident                                wards the profile reverses: in order to provide a good account
in the original analysis.                                                                     of human performance, the category representation requires
   The analysis presented above suggests a representational                                   at least four clusters across the two categories. Although not
shift in which conceptual structures smoothly move from pro-                                  shown, a similar change occurs for the 49 models analyzed in
totypes to exemplars via a range of intermediate models. To                                   experiment 2, also at segment five.
                                                                                 1603

   Another detailed picture of the shift is shown in Figure 9,                              Acknowledgments
this time looking at all 49 models used in experiment 2. In           WV was supported by the Research Council of the University of
this figure, the overall level of abstraction qA + qB is split        Leuven (IDO/02/004), and DJN was supported by an Australian
                                                                      Research Fellowship (ARC grant DP-0773794). We would like to
in its two constituent parts. Each plot displays the number           thank John Paul Minda for providing us with the data, and the re-
of clusters required to represent category A, the number re-          viewers for detailed and helpful comments.
quired to represent category B, and the data fit for each of the
49 models considered. The prototype model sits at the (1, 1)                                     References
level of abstraction, while the exemplar model is at (7, 7).          Anderson, J. (1991). The adaptive nature of human categorization.
                                                                             Psychological Review, 98, 409–429.
Figure 9 shows the maximized log-likelihood for each model
                                                                      Grünwald, P. (1998). The minimum description length principle and
at each level of abstraction per category, at each stage of ex-              reasoning under uncertainty. Unpublished doctoral disserta-
periment 2. It shows that, like in experiment 2, an abrupt shift             tion, University of Amsterdam.
takes place after segment four. Until that segment, models            Kontkanen, P., Myllymäki, P., Buntine, W., Rissanen, J., & Tirri,
                                                                             H. (2005). An MDL framework for data clustering. In Ad-
with a highly abstract representation are clearly the best, but              vances in minimum description length: Theory and applica-
from segment four onwards the models with a more detailed                    tions. Cambridge, MA: MIT Press.
representation are dominant.                                          Love, B. C., Medin, D. L., & Gureckis, T. M. (2004). SUSTAIN: A
                                                                             network model of category learning. Psychological Review,
                          Discussion                                         111, 309–332.
                                                                      Medin, D. L., & Schaffer, M. M. (1978). Context theory of classif-
Neither prototypes nor exemplars appear to provide a suffi-                  fication learning. Psychological Review, 85, 207–238.
cient account of human category learning, at least for large          Navarro, D. J., & Lee, M. D. (2005). An application of minimum
categories (Smith & Minda, 1998), since neither model ac-                    description length clustering to partitioning learning curves.
                                                                             In B. Petrov & B. Csaki (Eds.), 2005 ieee international sym-
counts for changes in representational structure, and there are              posium on information theory (p. 587-591). Piscataway, NJ:
very clear signs that these changes occur in empirical data.                 IEEE.
On top of this, when we consider the fact that prototypes and         Nosofsky, R. M. (1986). Attention, similarity, and the identification-
exemplars are two extremes in the class of mixture represen-                 categorization relationship. Journal of Experimental Psy-
                                                                             chology: General, 115, 39–57.
tations (Rosseel, 2002), a prototype-to-exemplar shift should         Nosofsky, R. M., & Zaki, S. R. (2002). Exemplar and prototype
be expected to pass through the kind of intermediate models                  models revisited: Response strategies, selective attention,
that are encompassed by VAM introduced by Vanpaemel et                       and stimulus generalization. Journal of Experimental Psy-
                                                                             chology: Learning, Memory, and Cognition, 28, 924–940.
al. (2005). To demonstrate that this does in fact occur, we re-
                                                                      Reed, S. K. (1972). Pattern recognition and categorization. Cogni-
analyzed the data from two experiments in Smith and Minda                    tive Psychology, 3, 392–407.
(1998), replicating their results (Figures 4 and 5). Our analy-       Rosch, E. (1978). Principles of categorization. In E. Rosch &
sis indicated that such changes are somewhat more complex                    B. B. Lloyd (Eds.), Cognition and categorization (pp. 27–
than previously suggested. Firstly, unlike Smith and Minda                   48). Hillsdale, NJ: Lawrence Erlbaum.
                                                                      Rosseel, Y. (2002). Mixture models of categorization. Journal of
(1998), we are able to find evidence of a small shift in experi-             Mathematical Psychology, 46, 178–210.
ment 3, as well as the large changes in experiment 2. Also, al-       Shepard, R. N. (1987). Towards a universal law of generalization
though the overall level of abstraction of the best-fitting mod-             for psychological science. Science, 237, 1317-1323.
els moves smoothly from abstract, prototype-like models to-           Smith, J. D., & Minda, J. P. (1998). Prototypes in the mist: The
wards differentiated, exemplar-like models (Figures 6 and 7),                early epochs of category learning. Journal of Experimental
                                                                             Psychology: Learning, Memory, and Cognition, 24, 1411–
when we look at the performance of all possible models (Fig-                 1436.
ures 8 and 9) there appear to be some much sharper transi-            Vanpaemel, W., Storms, G., & Ons, B. (2005). A varying abstraction
tions, as the performance of previously good models can de-                  model for categorization. In B. Bara, L. Barsalou, & M. Buc-
                                                                             ciarelli (Eds.), Proceedings of the 27th annual conference of
teriorate rapidly. In light of these findings, it appears that the           the Cognitive Science Society (pp. 2277–2282). Mahwah, NJ:
current trend toward developing and applying mixture mod-                    Lawrence Erlbaum.
els for categorization can provide useful insights, by allowing       Webb, M. R., & Lee, M. D. (2004). Modeling individual differences
us to trace changes in representation in more detail than pre-               in category learning. In K. Forbus, D. Gentner, & T. Regier
                                                                             (Eds.), Proceedings of the 26th annual conference of the cog-
viously possible.                                                            nitive science society (p. 1440-1445). Mahwah, NJ: Erlbaum.
                                                                 1604

