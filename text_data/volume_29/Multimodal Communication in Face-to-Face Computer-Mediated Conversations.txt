UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Multimodal Communication in Face-to-Face Computer-Mediated Conversations
Permalink
https://escholarship.org/uc/item/52p6d12m
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Louwerse, Max M.
Benesh, Nick
Hoque, Mohammed E.
et al.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

   Multimodal Communication in Face-to-Face Computer-Mediated Conversations
                                         Max M. Louwerse (mlouwerse@ memphis.edu)a
                                               Nick Benesh (nbenesh@memphis.edu)a
                                         Mohammed E. Hoque (mhoque@memphis.edu)b
                                           Patrick Jeuniaux (pjeuniau@memphis.edu)a
                                             Gwyneth Lewis (glewis1@ memphis.edu)a
                                                     Jie Wu (jwu4@ memphis.edu)c
                                           Megan Zirnstein (mzirnstn@ memphis.edu)a
                                       Department of Psychology / Institute for Intelligent Systemsa
                       Department of Electrical and Computer Engineering / Institute for Intelligent Systemsb
                                  Department of Computer Science / Institute for Intelligent Systemsc
                                                          Memphis, TN 38152 USA
                              Abstract                                   information available concerning the conditions under
                                                                         which students use facial movements or eye gaze, tapping
  Multimodal communication involves multiple communicative               into students’ cognitive states is difficult (cf. Graesser, et
  channels including speech, facial movement, and gesture.               al., in press).
  Relatively few studies on how various communicative
                                                                           From a computational perspective, an understanding of the
  modalities are aligned in natural, face-to-face communication
  exist. As part of a larger project, the current study investigates
                                                                         interplay between modalities can help in the development of
  how discourse structure, speech features, eye gaze, and facial         animated conversational agents (Louwerse, Graesser, Lu &
  movements interrelate during a map coordination task. The              Mitchell, 2005). These agents maximize the availability of
  study thereby sheds light on multimodal communication in               both linguistic (semantics, syntax) and paralinguistic
  humans and gives guidelines for the development of                     (pragmatic, sociological) features (Cassell & Thórisson,
  embodied conversational agents.                                        1999; Massaro & Cohen, 1994; Picard, 1997). But without
   Keywords: multimodal communication; discourse structure;              experimental data on multimodal communication, the
   dialogue; embodied conversational agents                              guidelines for implementing human-like multimodal
                                                                         behavior in agents are missing (cf. Cassell, et al., 1994).
                          Introduction                                     The current paper presents some initial results of an
                                                                         extensive data collection study of 256 conversations from
Multimodal communication is comprised of various
                                                                         64 participants, all native speakers of English, and totaling
modalities such as speech, facial movement, and gesture.
                                                                         35 hours. Conversations were monitored for dialogue acts,
Avoiding gestures while talking on the phone is difficult,
                                                                         speech, facial movements, gesture, eye gaze, and route
not looking at someone in a face-to-face setting is
                                                                         drawing accuracy. The data from this study will shed light
challenging, , and listening to someone in a face-to-face
                                                                         on human multimodal communication and will provide
setting without acknowledging information with the
                                                                         guidelines for the development of natural, embodied
occasional mhmm’s and head nods, again, is a challenge.
                                                                         conversational agents.
Despite the deceptively simple appearance of these
communicative tools, little is understood regarding their                                  Map Task Dialogues
interaction and alignment. Modalities such as speech,
                                                                         Though there is considerable amount of research on
discourse, facial movements, eye gaze, and gesture seem to
                                                                         multimodal communication (Argyle & Cook, 1976;
be intrinsically related. At the same time, little is known
                                                                         Doherty-Sneddon, et al. 1997; Ekman, 1979; Goldin-
about how they interrelate and how they are aligned.
                                                                         Meadow, 2003; Louwerse & Bangerter, 2005; McNeill,
    Knowing the nature of the interaction of modalities and
                                                                         1992), this research can be characterized by the fact that 1)
their alignment can shed light on various areas of cognitive
                                                                         pairs of modalities are considered, so that it remains unclear
science. From a psychological perspective, an understanding
                                                                         whether multiple modalities are mutually substitutable; 2)
of the interplay of modalities can help us understand
                                                                         language situations are highly diverse, making it difficult to
language and communication (Clark, 1996). Limited
                                                                         interpret why certain multimodal behavior occurs; and 3)
experimental research is available that can help determine
                                                                         dialogue is unpredictable, making it hard to model when
whether modalities can be substituted or whether they are
                                                                         modalities behave in certain ways.
complementary (cf. Doherty-Sneddon, et al., 1997).
                                                                           In the current research project on multimodal
  From an educational perspective, an understanding of
                                                                         communication in humans and agents (Louwerse, et al.,
modalities can help answer questions regarding student
                                                                         2004), we are investigating the interaction between dialogue
motivation, interest, and confusion, as well as how
                                                                         act, speech, eye gaze, facial movements, gesture, and map
instructors and tutors can monitor and respond to these
                                                                         drawing. The project aims to determine how these
cognitive states (Kort, Reilly & Picard, 2001). But with little
                                                                     1235

modalities are aligned, whether, and if so when, these
modalities are observed, and whether the correct use of
these channels actually aids comprehension.
    Due to the inherent complexity of multimodal
communication, controlling for genre, topic, and goals
during unscripted dialogue is crucial. With these concerns in
mind, we used the Map Task scenario (Anderson, et al.,
1991), a restricted-domain, route-communication task. In
the Map Task scenario it is possible for experimenters to
determine exactly what each participant knows at any given
time. In this scenario, the Instruction Giver (IG) coaches the
Instruction Follower (IF) through a route on the map. By
way of instruction, participants are told that they and their
interlocutors have maps of the same location, but drawn by
different explorers, and so are potentially different in detail.
                                                                      Figure 1. Examples maps for the IG (left) and the IF (right)
They are not told where or how the maps differ.
  The present paper reports a preliminary analysis of the
                                                                     remote eye tracker. Speech was recorded using a Marantz
experimental data gathered for this project.
                                                                     PMD670 recorder, whereby IG and IF were recorded on two
                           Method                                    separate (left and right) channels using two AKG C420
                                                                     headset microphones. Two high-resolution webcams were
For the current paper, we randomly sampled 16 of 256                 used for the interface. The IF drawings of the routes on the
available conversations, totaling 72 minutes of dialogue             screen were recorded both spatially and temporally.
with different participants and different maps for each
conversation. Data from each conversation consisted of               Procedure
recordings of participants’ facial movements, gestures,
speech, eye gaze patterns (for IG), and map drawings (for            Participants were seated in front of each other but were
IF). Each of the participants performed the role of IG (4            separated by a divider to ensure that they could not see each
conversations in a row) and the role of IF (4 conversations          other. They communicated through microphones and
in a row). In each conversation, different maps were used            headphones, and could see the upper torso of their dialogue
that varied in terms of homogeneity of objects. An example           partner through the webcam and the map on a computer
of maps for the IG and IF is given in Figure 1.                      monitor in front of them. This computer-mediated session,
                                                                     using webcams, was necessary for eye tracking calibration,
Participants                                                         as well as to reduce torso movement.
                                                                       The IG was presented with a colored map with a route
Of the 64 participants in the total data set, 32 were included       (similar to the one presented in Figure 1). The IG’s task was
in this analysis yielding a representative sample. Of these          to communicate the route to the IF as accurately as possible.
participants, 21 were female. Sixteen participants were              To ensure on-task conversations, participants were promised
Caucasian, 14 African-American, and 2 Asian-American, all            extra payment if the route drawn by the IF matched the
being native speakers of English.                                    route on the IG’s map.
                                                                       Equipment was calibrated before the start of each
Materials                                                            conversation. The five camcorders were positioned and
Sixteen different maps were used, each varying according to          focused in order to best capture the facial movements and
the presentation of landmarks, route shape, and method of            upper torso movements of each participant. The eye gaze of
distortion in the IF map. For instance, IF’s maps were               the IG was calibrated using nine calibration points on the
distorted with blurred out portions of the map. Four possible        computer monitor. To avoid interruption of the dialogue,
route shapes were used. As for landmark presentation, maps           calibration only occurred once per map. When calibration
either had mixed or single themes. For example, a mixed              was lost during the study, recording of eye tracking data
landmark map would include a majority of landmarks from              was discontinued and this part of the data was eliminated
a theme (e.g., birds) and a few randomly selected landmarks          from the analysis. Each conversation started with a flash of
from other themes (e.g., aliens), as in Figure 1.                    light and the sounding of a brief tone, in order to ensure
                                                                     alignment of the different channels in later analysis.
Apparatus
Participants’ communication was recorded using five                                   Results and Discussion
Panasonic camcorders, two capturing the faces of each
dialogue participant (PV-GS31), two capturing the upper              Dialogue acts
torsos of each participant (PV-GS150), and one capturing
                                                                     The 12 dialogue acts (DA) that are typically used for Map
both participants from a bird’s eye view (PV-GS150). Eye
                                                                     Task coding were used (Carletta et al., 1997; Louwerse &
gaze was recorded for the IG only using an SMI iView RED
                                                                 1236

Crossley, 2006). A description and an example of these DAs         (FACS) are problematic for Map Task scenarios because
is presented in Table 1. Two trained coders manually coded         emotions like disgust, anger, and sadness do not occur
the utterances of half of the conversations as one of the          frequently during these interactions. Implementing a subset
twelve DAs. Inter-rater reliability between the coders in          of the action units themselves is, however, beneficial. A
terms of Cohen’s Kappa was at .67. Coders resolved the             total of 20 facial movements were coded for. Their labels,
conflicts, primarily relating to the acknowledgment DA and         descriptions, and frequencies are presented in Table 2.
coded the remaining transcripts for DAs. An overview of               Three coders rated four conversations for facial
the frequency of the DAs is presented in Table 1.                  movement. Cohen’s Kappa was .77 for the head, .73 for eye
                                                                   brows, .79 for the eyes, and .85 for the mouth, yielding an
Speech                                                             overall inter-rater reliability for facial movement coding of
Speech features related to pitch, pause, and speaking rate         .78. Coders independently rated the remaining
were calculated for each dialogue act, using Praat speech          conversations. Facial movements were first coded then
processing software (Boersma & Weenink, 2006).                     aligned with speech in order to analyze their interaction over
  Pause was analyzed, using the upper intensity limit and          time.
minimum duration of silence. In measurement of intensity,
minimum pitch specifies the minimum periodicity                    Eye gaze
frequency in any signal. In our case, 75 Hz for minimum            Eye fixation for the IG only was recorded in order to
pitch yielded a sharp contour for the intensity. Audio             implement findings in the embodied conversational agent
segments with intensity values less than 58 dB and with a          fulfilling the role of IG in the project. Two main areas of
duration of silences longer than 0.2 seconds were classified       interest were identified: the IF’s webcam image and the IG’s
as pauses. Features such as number of pauses, maximum              map. In addition, eye fixation on the rest of the screen,
duration of pauses, average duration of pauses, and total          fixation off of the screen, and lost fixation times were
duration of pauses per dialogue act were computed.                 recorded. 74.1% of the conversation had fixations recorded
  Pitch information from each dialogue act speech segment          on the screen. Of these fixations, 23.5% were not aimed at
was computed using the autocorrelation method processed            the map or the IF, 9.7% was aimed directly at the IF, and
through Praat speech processing software. The lower and            66.8%, on the map.
upper thresholds for pitch were set to 75Hz and 400Hz,
respectively. In other words, only pitch components ranging        Map drawings
from 75Hz to 400Hz were considered for our analysis.               The route drawn by the IF is useful to determine the extent
Speaking rate was computed as the ratio of voiced frames           to which the IF deviated from the route on the IG’s map.
(1/voiced frames).                                                 This gives us an idea as to whether communication between
  Of course, the maximum threshold for intensity is                the IG and IF was executed successfully. To obtain this
dependent on individuals’ vocal tone, gender, and other            information, we computed the minimal difference between
issues. Also, speaking rate is different for each individual.      the given route on the IG’s map and the drawn route on the
Our future efforts will therefore include using an adaptive        IF’s map. The average difference between the ideal route on
approach to automatically predict the maximum threshold            the IG’s map and the drawn IF’s map was 11.53 pixels (SD
for intensity and minimum duration for silence, per                 = 20.47), which translates to .58 cm, with a min. difference
participant.                                                        of 0 and a max. difference of 134 (approximately 6.8 cm).
Facial movements
Standard emotion coding schemes like Ekman, Friesen,
Wallace, and Hager’s (2002) Facial Action Coding Scheme
     Table 1. The 12 DAs used in the Map Task, a description, an example, and frequencies for the selected conversations
Dialogue Act Description                                                Example                                       IG     IF
INSTRUCT      Commands partner to carry out action                      Go down between the blue and the red car. 698 12
EXPLAIN       States information not directly elicited by partner       Ok I went the wrong way.                      234 76
CHECK         Requests partner to confirm information                   So, between the black and the grey one?       10     56
ALIGN         Checks attention, readiness, agreement of partner         Ok, do you see those two blue cars?           38     1
QUERY-YN Yes/no question that is not CHECK or ALIGN                     Do you see that?                              218 50
QUERY-W       Any query not covered by the other categories             If I'm at the red car what do I do there?     22     23
ACKNOWL       Verbal response minimally showing understanding           Uh huh.                                       176 470
REPLY-Y       Reply to any yes/no query with yes-response               Yeah, start at the top.                       59     135
REPLY-N       Reply to any yes/no query with no-response                No,go like above the puddle.                  8      6
REPLY-W       Reply to any type of query other than ‘yes or ‘no’        It goes below.                                17     20
CLARIFY       Reply to question over and above what was asked           So you'll be between the blue and red car.    21     41
READY         Preparing conversation for new dialogue game              Alright. We're going to move to the left.     22     6
UNCODBL                                                                                                               20     4
                                                               1237

             Table 2. Facial movements used in the coding, a description and their average frequencies by IG and IF
                   Note: AU = Corresponding Action Unit, IG = Information Giver, IF = Information Follower
       Label                        Description                                                         AU          IG        IF
        Forward movement            Slow forward head movement towards the monitor                      AU57        46.38 34.06
        Backward movement           Slow backward head movement away from the monitor                   AU58        44.00 28.13
        Left tilt                   Head tilting resulting in head cocked to left                       AU55        21.19 14.88
Head
        Right tilt                  Head tilting resulting in head cocked to the right                  AU56        17.06 14.25
        Nodding fast                Quick downward and upward movement of head                           n/a        16.13 13.06
        Nodding slow                Slow downward and upward movement of head                            n/a        6.19      7.06
        Turning (left)              Neck movement results in face orienting towards the left             AU51       22.06 19.06
        Shaking right               Neck movement results in face orienting towards the right            AU52       22.25 19.44
        Brows (up)                  Inner or outer portion of the eyebrows are pulled upwards.          AU1/2       10.38 6.31
Eye / eyebrows
        Brow (down)                 Lowering of eyebrows as in frowning                                 AU4         0         0
        Asymmetrical                Only one eyebrow (left or right) is raised.                         n/a         1.81      1.13
        Rapid blinking              Eyes close and open very quickly with no pause in succession AU45               0.69      1.13
        Squinting                   Narrowed eye aperture.                                              AU44        4.56      2.56
        Widening                    Widens the eye aperture                                             AU5         1.00      0.44
        Rolling                     Upward rolling of the eyes.                                         M68         0         0
        Smile                       Pulls the corners of the lips back and upward                       AU12        2.00      2.56
        Lip tightener               Tightens the lips, making lips appear more narrow                   AU23        2.00      4.75
Mouth   Lip pucker                  Pushes the lips of the mouth forward and pulls medially             AU18        0.00      0.44
        Mouth open                  Mouth is in O shape                                                 AU25/26 0.88          2.63
        Biting lip                  Teeth biting the lip (teeth may or may not be viewable)             AD32        1.13      3
                                                                     guidelines for agent development. Therefore, what is
Correlations between modalities                                      desirable is one modality that can be used as a basis onto
While the correlational data are at best exploratory in              which  other modalities can be mapped.
nature, they can provide the first steps in helping to resolve
the complex issue of how to associate these communicative            Dialogue structure and its relations to modalities
modalities with one another. The data for all modalities             One of the aims of the current project is, as described, the
were time aligned on 250 millisecond intervals. Though eye           development of an animated conversational agent that can
gaze, map drawings, and speech features allowed for                  interact with a human dialogue partner and behave similarly
smaller time frames (< .016 msecs), facial movement coding           to the human dialogue partner in terms of using modalities
did not. We therefore aggregated the data to a unit of               like dialogue structure, speech features, eye gaze, and facial
analysis of 1 second. To avoid a Type I error, only those            movements.
correlations were considered that were significant at p                The correlations presented above may give insights into
<.001.                                                               the interrelations of the various modalities. They are of
  Correlations were found between route difference, facial           limited use in agent development, however, because we lack
movements, and speech features. Larger route differences             a basis onto which the modalities can be mapped. In order to
correlated with a smile in the IF (r = .13) and a head tilt to       solve this problem, we used dialogue structure as the basis
the left (r = .46). These facial movements may signify               of comparison, because it provides precise cues regarding
surprise or confusion (Ekman, 1979). For instance,                   the message’s meaning and the speaker’s meaning.
asymmetrical eye brows in the IF is associated with                    Using dialogue structure has an additional advantage, in
squinting of the eyes in the IG (r = .84) as well as a shaking       that algorithms have been put in place to classify utterances
of the head of the IG (r = .68). But correlations were also          into dialogue acts (Louwerse & Crossley, 2006). This means
found within a speaker. For instance, the IF’s eye brows             that once the agent has identified a dialogue act, it can elicit
going up correlated with the squinting of the eyes (r = .62)         the modalities that are most affected by this dialogue act.
as well as widening of the eyes (r = .79) in lip biting (r =         The agent can then identify the dialogue act in its own
.64) and smiling (r = .77) in the IF.                                utterance and elicit the relevant modalities, but also identify
  Dialogue partners seem to copy each other’s modalities.            the dialogue act in the IF utterance and respond with the
When the IG nodded slowly, the IF nodded slowly (r = .87),           appropriate modalities.
and smiles from the IG were accompanied with a smile from              Multiple regression analysis allows us to infer how
the IF (r = .67).                                                    accurately predictions can be made of a particular modality
  Correlational data like these show that different                  if the dialogue act is known. We conducted a multiple
communicative channels are correlated with other channels,           regression analysis with the 12 dialogue acts as dummy-
both within a speaker and across dialogue partners. They             coded independent variables entered in a Stepwise fashion
thereby confirm the complexity of the interrelations                 with the modalities as dependent variables. Scores for the
between modalities and make it hard to interpret the results.        dependent variables were averaged by the IG and IF
In addition, these correlations make it difficult to provide         dialogue acts, resulting in 279 cases per modality. We will
                                                                1238

focus only on those findings wherein the overall dialogue             communication. The reasons for this are simple. Collecting
structure explained a significant amount of the variance (R2          data, whereby a range of modalities are recorded properly,
> .15, p < .01) and where the standardized regression                 all participants elicit in natural dialogue while being
coefficients (β) for individual dialogue acts explained a             recorded, and all data can be temporally aligned, is difficult.
significant (p < .001) amount of the variance.                        Moreover, when such naturalistic data is collected,
  The results, summarized in Table 3, show which                      researchers are confronted with a wealth of possible
modality’s variance can be significantly explained per                variables.
dialogue act per participant role (IG/IF). For instance, when           The present study, part of a larger project, provides insight
the IG uses a REPLY-N dialogue act, typically a no answer to          into how eye gaze, facial movements, speech features, map
a yes/no question, it is likely that the IG’s eyebrows will be        drawings, and dialogue structures correlate with each other
up, that the IG is expressing a smile, and that the speaking          and which dialogue acts best predict the expression of a
rate will be high. While the REPLY-N is expressed, the IF             particular modality. We believe that by incorporating the
tends to perform a backward head movement. Similarly,                 findings of human-to-human, multimodal communication
when the IF asks a QUERY-W, typically a wh-question, there            into an artificial agent, the agent will be able to interact with
tends to be an increase in rising and falling pitch in speech,        humans more naturally and fluently. Based on the sample of
upward eyebrow movements, and rapid blinking in the IF,               data discussed in this paper, we have provided preliminary
while the IG’s eyes are widening.                                     guidelines for the development of embodied, conversational
  In terms of the development of human-like, embodied                 agents and shown that multiple communicative channels are
conversational agents, results like these can provide                 really interdependent communicative channels.
guidelines concerning when to use what modalities.
Therefore, if the system has identified a DA in the IF, it then                             Acknowledgments
knows which modalities to activate.                                   This research was supported by grant NSF-IIS-0416128.
                                                                      Any      opinions,        findings,     and     conclusions      or
                                                                      recommendations expressed in this material are those of the
                          Conclusion                                  authors and do not necessarily reflect the views of the
Even though multimodal communication includes perhaps                 funding institution. We would like to thank Ellen Bard and
the most fundamental forms of communication, there are                Markus Guhe for the creation of the maps and their help in
relatively few studies on how various communicative                   setting up the study.
modalities are aligned              in natural, face-to-face
       Table 3. Regression relationships between dialogue act and modalities. For all relations included β > .31, p < .001
              Note: All relations are positive: presence of a dialogue act increases chance of presence of modality
 IG               IG                                                                                     IF
 ACKNOWL          speaking rate
 ALIGN            speaking rate
 CHECK            squinting eyes
 EXPLAIN          freq. falling and rising pitch, average duration pauses, freq. pauses
 INSTRUCT         freq. falling and rising pitch, average duration pauses, freq. pauses
 QUERY W          freq. falling and rising pitch
 QUERY YN         freq. falling and rising pitch, average duration pauses, speaking rate
 READY            head backward movement, speaking rate
 REPLY N          eyebrows up, smile, speaking rate                                                      head backward movement
 REPLY Y          speaking rate                                                                          nodding slow
 UNCODABLE laughing
 IF             IF                                                                                       IG
 ACKNOWL.       freq. falling and rising pitch, freq. pauses, speaking rate
 ALIGN          nodding fast, rapid blinking, speaking rate                                              turning right
 CHECK          freq. falling and rising pitch, freq. pauses
 CLARIFY        freq. falling and rising pitch
 EXPLAIN        freq. falling and rising pitch, freq. pauses, speaking rate, dur. Pauses                 nodding slow
 INSTRUCT       freq. rising pitch, eyebrows asymmetrical
 QUERY_W        freq. falling and rising pitch, eyebrows up, rapid blinking                              eyes widening
 QUERY_YN       freq. falling and rising pitch, freq. pauses
 READY          speaking rate, left tilt                                                                 lip tightener, head tilt right,
 REPLY_N        mouth pucker, mouth smile, turning right                                                 mouth open
 REPLY_W        freq. falling and rising pitch, freq. pauses, route distance, left tilt, dur. pauses
 REPLY_Y        biting lip, lip tightener, speaking rate                                                 squinting eyes
                                                                 1239

                                                                   Ekman, P., Friesen, Wallace V., & Hager, J.C. (2002).
                        References                                   Facial Action Coding System (FACS). CD-ROM.
Anderson, A., Bader, M., Bard, E., Boyle, E., Doherty, G.          Goldin-Meadow, S. (2003). Hearing gesture: How our
  M., Garrod, S., Isard, S., Kowtko, J., McAllister, J.,             hands help us think. Cambridge, MA: Harvard University
  Miller, J., Sotillo, C., Thompson, H. S., & Weinert, R.            Press.
  (1991). The HCRC Map Task Corpus. Language and                  Graesser, A. C., D'Mello, S. K., Craig, S. D., Witherspoon,
  Speech, 34, 351-366.                                               A., Sullins J., McDaniel B., & Gholson, B. (in press). The
Argyle, M., & Cook, M. (1976). Gaze and mutual gaze.                 relationship between affective states and dialog patterns
  Cambridge: Cambridge University Press.                             during interactions with AutoTutor. Journal of Interactive
Boersma, P., & Weenink, D. (2006). Praat: Doing                      Learning Research.
  phonetics by computer (Version 4.4.06) [Computer                Kort, B., Reilly, R., & Picard, R. W. (2001). An affective
  program]. Retrieved January 30, 2006, from                         model of interplay between emotions and learning:
  http://www.praat.org/                                              Reengineering educational pedagogy-building a learning
Carletta, J., Isard, A., Isard, S., Kowtko, J., Doherty-             companion. In Proceedings of the International
                                                                     Conference on Advanced Learning Technologies (ICALT
  Sneddon, G., & Anderson, A. (1997). The reliability of a
                                                                     2001), Madison Wisconsin, August 2001.
  dialogue structure coding scheme. Computational
                                                                  Louwerse, M. M., & Bangerter, A. (2005). Focusing
  Linguistics, 23, 13-31.
                                                                     attention with deictic gestures and linguistic expressions.
Cassell, J., Pelachaud, C., Badler, N., Steedman, M.,                Proceedings of the 27th Annual Meeting of the Cognitive
  Achorn, B., Becket, T., Douville, B., Prevost, S., & Stone,        Science Society.
  M. (1994) Animated Conversation: Rule-Based                     Louwerse, M. M., Bard, E.G., Steedman, M., Hu, X., &
  Generation of Facial Expression, Gesture and Spoken                Graesser, A. C. (2004). Tracking multimodal
  Intonation for Multiple Conversational Agents.                     communication in humans and agents. Technical report,
  Proceedings of SIGGRAPH '94, 413-420.                              Institute for Intelligent Systems, University of Memphis,
Cassell, J., & Thórisson, K. R. (1999). The power of a nod           Memphis, TN.
  and a glance: Envelope vs. emotional feedback in                Louwerse, M. M., & Crossley, S. (2006). Dialog act
  animated conversational agents. Applied Artificial                 classification using n-gram algorithms. In Proceedings of
  Intelligence, 13, 519-538.                                         the Florida Artificial Intelligence Research Society
Clark, H. H. (1996). Using language. Cambridge:                      International Conference (FLAIRS). Menlo Park, CA:
  Cambridge University Press.                                        AAAI Press.
Doherty-Sneddon, G., Anderson, A. H., O'Malley, C.,               Louwerse, M. M., Graesser, A. C., Lu, S., & Mitchell, H. H.
  Langton, S., Garrod, S., & Bruce, V. (1997). Face-to-face          (2005). Social cues in animated conversational agents.
  and video-mediated communication: A comparison of                  Applied Cognitive Psychology, 19, 1-12.
  dialogue structure and task performance. Journal of             Massaro, D. W., & Cohen, M. M. (1994). Visual,
  Experimental Psychology: Applied, 3, 105-125.                      orthographic, phonological, and lexical influences in
Ekman, P. (1979). About brows: Emotional and                         reading. Journal of Experimental Psychology: Human
  conversational signals. In M. von Cranach, K. Froppa,              Perception and Performance, 20, 1107- 1128.
  W. Lepenies, & D. Ploog (Eds.), Human ethology: Claims          McNeill, D. (1992). Hand and mind: What gestures reveal
  and limits of a new discipline: Contributions to the               about thought. Chicago: University of Chicago Press.
  colloquium (pp. 169-248). Cambridge: Cambridge                  Picard, R. (1997). Affective computing. Cambridge, MA:
                                                                  MIT Press.
  University Press.
                                                              1240

