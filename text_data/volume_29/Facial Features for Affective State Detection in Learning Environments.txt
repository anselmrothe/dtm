UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Facial Features for Affective State Detection in Learning Environments
Permalink
https://escholarship.org/uc/item/9w00945d
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
McDaniel, Bethany
D'Mello, Sidney
King, Brandon
et al.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

            Facial Features for Affective State Detection in Learning Environments
                                          Bethany McDaniel (btmcdanl@memphis.edu)
                                           Department of Psychology, University of Memphis
                                             Sidney D’Mello (sdmello@memphis.edu)
                                        Department of Computer Science, University of Memphis
                                              Brandon King (bgking@memphis.edu)
                                           Department of Psychology, University of Memphis
                                          Patrick Chipman (pchipman@memphis.edu)
                                           Department of Psychology, University of Memphis
                                              Kristy Tapp (kmsnyder@memphis.edu)
                                           Department of Psychology, University of Memphis
                                             Art Graesser (a-graesser@memphis.edu)
                                           Department of Psychology, University of Memphis
                              Abstract                                   System. This system specified how specific facial
   This study investigated facial features to detect the affective
                                                                         behaviors, based on the muscles that produce them, could
   states (or emotions) that accompany deep-level learning of            identify “basic emotions”. Each movement in the face is
   conceptual material. Videos of the participants’ faces were           referred to as an action unit (or AU). There are
   captured while they interacted with AutoTutor on computer             approximately 58 action units. These facial patterns were
   literacy topics. After the tutoring session, the affective states     used to identify the emotions of happiness, sadness,
   (boredom, confusion, delight, flow, frustration, and surprise)        surprise, disgust, anger, and fear (Ekman & Friesen, 1978;
   of the student were identified by the learner, a peer, and two        Elfenbein & Ambady, 2002). Doubts have been raised,
   trained judges. Participants’ facial expressions were coded by
   two independent judges using Ekman’s Facial Action Coding             however, that these six emotions are frequent and
   System. Correlational analyses indicated that specific facial         functionally significant in the learning process (D’Mello et
   features could segregate confusion, delight, and frustration          al., 2006; Kapoor, Mota, & Picard, 2001). Some have
   form the baseline state of neutral, but boredom was                   challenged the adequacy of basing a theory of emotions on
   indistinguishable from neutral. We discuss the prospects of           these “basic” emotions (Rozin & Cohen, 2003). Moreover,
   automatically detecting these emotions on the basis of facial         Ekman’s coding system was tested primarily on static
   features that are highly diagnostic.
                                                                         pictures rather than on changing expressions over time..
   Keywords: Facial features; action units, affective states;               There is some evidence for a different set of emotions that
   emotions; learning; AutoTutor; classifying affect                     influence learning and cognition, specifically boredom
                                                                         (Csikszentmihalyi, 1990; Miserandino, 1996), confusion
                          Introduction                                   (Graesser & Olde, 2003; Kort, Reilly, & Picard, 2001), flow
It is widely acknowledged that cognition, motivation, and                (i.e. engagement, Csikszentmihalyi, 1990), and frustration
emotions are three fundamental components of learning                    (Kort, Reilly, & Picard, 2001; Patrick et al., 1993).
(Snow, Corno, & Jackson, 1996). Emotion has been viewed                  Curiosity and eureka (i.e. the “a-ha” experience) are also
as source of motivational energy (Harter, 1981;                          believed to accompany learning. A study was recently
Miserandino, 1996; Stipek, 1998), but it can also be viewed              conducted to investigate the occurrence of these emotions,
as a more complex independent factor that plays an                       as well as Ekman’s basic emotions. The study used an
explanatory role in both learning and motivation (Ford,                  emote-aloud procedure (D’Mello et al., 2006), a variant of
1992; Meyer & Turner, 2002). The link between emotions                   the think-aloud procedure (Ericsson & Simon, 1993), as an
and learning has received more attention during the last                 online measure of the learners’ affective states during
decade in the fields of psychology, education, and computer              learning. College students were asked to express the
science (Craig, Graesser, Sullins, & Gholson, 2004;                      affective states they were feeling while working on a task, in
Graesser, Jackson, & McDaniel, 2007; Kort, Reilly, &                     this case being tutored in computer literacy with AutoTutor.
Picard, 2001; Picard 1997; Meyer & Turner, 2002).                        Using the emote-aloud method allowed for the on-line
   Ekman and Friesen (1978) highlighted the expressive                   identification of emotions while working on the learning
aspects of emotions with their Facial Action Coding                      task. A sample of 215 emote-aloud observations were
                                                                     467

produced by 7 participants. The emotions of interest were            participants completed a pretest with multiple-choice
listed and defined for the student before they started the           questions, then interacted with the AutoTutor system for 32
learning task. The emotions came from both groups: (1)               minutes on one of three randomly assigned topics in
learning-specific emotions, i.e., boredom, confusion,                computer literacy (Hardware, Internet, Operating Systems),
curious, eureka, and frustration and (2) basic emotions, e.g.,       and then completed a posttest. During the tutoring session,
anger and disgust. The results indicated that the 5 learning         the system recorded a video of the participant’s face, their
specific emotions accounted for 91% of the total verbalized          posture pressure patterns, and a video of their computer
reports while the remaining 9% of the emote-aloud                    screen.
utterances were for the basic emotions. In addition, curiosity          The second phase involved affect judgments by the
rarely occurred (3%) and eureka was often confused with              learner, a peer, and two trained judges. A list of the affective
delight when giving a correct answer or surprise when                states and definitions was provided for all four judges. The
getting an unexpected feedback from the tutor. These                 states were boredom, confusion, flow, frustration, delight,
affective states were therefore replaced with delight and            neutral and surprise. The selection of emotions was based
surprise.                                                            on previous studies of AutoTutor (Craig et al., 2004;
   The present study focused on boredom, confusion,                  D’Mello et al., 2006; Graesser et al., 2006) that collected
delight, flow, frustration, and surprise as the affective states     observational data and emote-aloud protocols while college
that are relevant to learning. We acknowledge that this set of       students learned with AutoTutor.
affective states is not exhaustive for all learning                     In the affect judging session, participants viewed video
environments, but they were the most prominent while                 streams of both the computer screen and face of the learner,
college students learn with AutoTutor (Graesser et al.,              during the AutoTutor session. The judges were instructed to
2006). Moreover, some of these affective states have been            make judgments on what affective states were present in
correlated with learning gains. Boredom is negatively                each 20-second interval (called mandatory judgments); at
correlated with learning, whereas confusion and flow are             these points the video automatically paused for their affect
positively correlated with learning (Craig et al., 2004).            judgments. They were also instructed to indicate any
   This study identified the action units that accompany the         affective states that were present in between the 20-second
experience of these selected emotions. Once the                      stops (voluntary judgments). Four sets of emotion
representative AUs for the learning-specific affective states        judgments were made for each participant’s AutoTutor
are determined, computer analyses can be used to                     session. First, in the self judgments, participants viewed
automatically identify those AUs and make inferences on              their own AutoTutor session immediately after their
the emotions of the learner. If the affective states of a            learning session. Second, in the peer judgments, participants
learner can be detected with a reasonable degree of                  came back one week later to view and judge another
accuracy, then intelligent tutoring systems, peer learning           participant’s session on the same topic in computer literacy.
companions, and educational software in general can revise           Finally, two trained judges independently judged all of the
their pedagogical strategies by incorporating both the               sessions. The judges were trained on Ekman’s Facial Action
affective states of the learner and their cognitive states.          Coding System (Ekman & Friesen, 1978) and on
                                                                     characteristics of dialogue.
                           Methods
Participants                                                         Data Treatment
                                                                     Analysis of Agreement among Judges. Interjudge
The participants were 28 undergraduate students who
                                                                     reliability was computed using Cohen’s kappa for all
participated for extra course credit in a psychology course.
                                                                     possible pairs of judges: self, peer, trained judge 1, and
Materials                                                            trained judge 2. There were 6 possible pairs altogether. The
                                                                     kappas for the mandatory judgments were: self-peer (.06),
AutoTutor. AutoTutor is a fully automated computer tutor             self-judge 1 (.11), self-judge 2 (.13), peer-judge 1 (.11),
that simulates human tutors and holds conversations with             peer-judge 2 (.15), and judge 1-judge 2 (.31). These kappa
students in natural language (Graesser, Chipman, Haynes, &           scores revealed that the trained judges had the highest
Olney, 2005; Graesser et al., 1999). AutoTutor helps                 agreement, the self and peer pair had lowest agreement, and
students learn Newtonian physics, computer literacy, and             the other pairs of judges were in between. It should be
critical thinking by presenting challenging problems (or             noted, however, that the kappa scores increased
questions) from a curriculum script and engaging in a                substantially [self-peer (.12), self-judge 1 (.31), self-judge 2
mixed-initiative dialog while the learner constructs an              (.24), peer-judge 1 (.36), peer-judge 2 (.37), and judge 1-
answer. In this study, students interacted with AutoTutor on         judge 2 (.71)] when we focused on voluntary judgments.
topics concerning computer literacy.                                 Additional details on this analysis are reported in Graesser
                                                                     at al. (2006).
Procedure
The study was divided into two phases. The first phase was           Data Selection. The above kappa scores indicate that an
a standard pretest–intervention–posttest design. The                 emotion labeling task is more difficult if judges are asked to
                                                                 468

make emotion judgments at regularly polled timestamps,            frustration, and neutral. Surprise was not included because
rather than being able to stop a video display to make            this emotion was extremely rare. Flow was also excluded
spontaneous judgments. The states at regular timestamps are       because it rarely appeared in the voluntary emotion samples.
much less salient so there is minimal information to base
their judgments, compared with those points when affective        Scoring Procedure. The expression of emotions tends to be
states are detected by the judge. Therefore, we decided to        short in duration, lasting only about 3 seconds (Ekman,
focus on data points where the affect judgments were              2003). Two judges independently coded the facial videos 3-
voluntarily provided by the judges. This approach is              4 seconds before the emotional episode, using the Facial
beneficial for two reasons. First, the increased kappa scores     Action Coding System (Ekman & Friesen, 1978).
made us more confident in the validity of the emotion             Specifically, 212 video clips 3-4 seconds in duration were
measurement task. Secondly, it is reasonable to assume that       prepared for action unit coding. The clips were not
the facial expressions of learners are more animated at these     associated with any discernable emotion annotation, so the
voluntary points compared to regularly polled timestamps          judges were unaware of the learner’s emotion while viewing
when their face is frequently neutral. This hypothesis was        the clips. The two AU coders were not the trained judges
confirmed by analyzing the proportions of emotion                 used for the emotion judgment procedure. Each coder
categories at the mandatory points. When averaged over the        watched the clips and recorded the AUs present along with
4 judges, the most common affective state was neutral             the time of each observation. It is important to note that we
(36.9%), followed by confusion (21.2%), flow (18.8%), and         decided to focus on a subset (N = 33) of the action units that
boredom (16.7%). The remaining states of delight,                 were most relevant to the emotions under exploration.
frustration and surprise constituted 6.5% of the
observations. The more salient voluntary points had a rather                                 Results
different distribution. The most prominent affective state        We computed the proportion of AUs observed by each rater
was confusion (37.7%), followed by delight (19.2%), and           and discarded AUs that occurred relatively infrequently. We
frustration (19.1%). The remaining affective states               adopted an ad-hoc selection threshold of 3%, such that AUs
comprised 24.0% of the observations (boredom, surprise,           that appeared in less than 3% of the samples were discarded.
flow, and neutral, in descending order). Therefore, the           When averaged across both judges, we preserved 12 AUs
affective states that presumably accompany heightened             that collectively represented 80% of the observations. The
arousal (confusion, delight, and frustration) are more            21 less frequent AUs comprised the remaining 20% of the
prominent at the voluntary points.                                observations and were subsequently discarded.
   One consequence of exclusively relying on voluntary
judgments for facial expression coding is that the frequency
                                                                                Facial Action Unit              Prop. Kappa
of these points is substantially less than the mandatory
observations. There were 2688 data points for the                                    AU1 Inner Brow Raiser      .056    .642
mandatory observations, but only 1133 points of voluntary                            AU2 Outer Brow Raiser      .033    .534
observations. A subset of the voluntary points was identified                        AU4 Brow Lowerer           .057    .779
by only a single judge (self, peer, or one of the trained           Upper Face
                                                                                     AU7 Lid Tightener          .079    .590
judges). This sample size reduction problem was mitigated
                                                                                     AU43 Eye Closure           .047    .605
by an exhaustive voluntary affect judgment session in which
the trained judges repeated the judgment procedure with the                          AU45 Blink                 .172    .681
added requirement that they had to provide affect ratings on        Lower Face       AU12 Lip Corner Puller     .100    .707
all 1133 voluntary emotion observations. We found that the          Lip Parting & AU25 Lips Part                .089    .912
two trained judges agreed 64% of the time (N = 720)                 Jaw Opening AU26 Jaw Drop                   .056    .851
yielding a kappa score of 0.49. This kappa score is higher
                                                                                     AU55 Head Tilt Left        .035    .770
than that achieved for the mandatory observations (0.31) but        Head Positions
substantially lower when compared to the purely voluntary                            AU56 Head Tilt Right       .035    .665
observations (0.71). However, it is on par with reliability         Eye Positions AU64 Eyes Down                .037    .833
scores reported by other researchers who have assessed              Other            -      -                   .206      -
identification of emotions by humans (e.g. Ang et al., 2002;
Forbes-Riley & Litman, 2004).                                               Table 1. Proportion of Action Units Observed
   Since facial action unit coding is a time consuming, labor     Table 1 presents the proportion of each of the AUs averaged
intensive process, we sampled 212 emotion episodes out of         across the two human coders. Kappa scores between the two
the 720 data points obtained from the exhaustive voluntary        coders for each of the AUs are also presented. We note that
affect judgment procedure. These points were sampled to           the majority of the activity of the facial features during
approximate a uniform distribution of the different               emotional experiences occurred on the upper face, with the
emotions, i.e., an approximately equal number of                  mouth area a close second. The kappa scores also indicate
observations was obtained from each participant and for           that the level of agreement achieved by the AU judges in
each of the affective states of boredom, confusion, delight,
                                                              469

coding the target action units ranged from fair to excellent            perhaps indicative of a half smile as evident in Figure 1d.
(M = .721, SD = .117).                                                  This may be an attempt by the learner to disguise an
                                                                        emotion associated with negative connotations in society.
Relationship between Action Units and Emotions
Correlations were computed to determine the extent to                                A. Boredom                B. Confusion
which each of the AUs were diagnostic of the affective
states of boredom, confusion, delight, frustration, and
neutral. Two sets of correlations were computed in order to
determine whether significant patterns emerged across both
independent coders. These correlations are presented in
Table 2. The analyses revealed that there was a good degree
of concordance among the two judges. Barring a few
anomalies that may be attributed to individual differences of                         C. Delight               D. Frustration
the coders, the directions of the relationships are consistent
between the two AU coders. In the subsequent discussion
we only focus on the significant correlations presented in
Table 2, when both coders achieved a consensus.
   We found that confusion was manifested by a lowered
brow (AU 4), the tightening of the eye lids (AU 7), and a
notable lack of a lip corner puller (AU 12). Figure 1b
presents an example of confusion in which AUs 4 and 7 are                            Figure 1. Examples of Affective States
prominent (see furrowed brow). This pattern replicates
D’Mello et al.’s (2006) results when learners verbally                     It appears that boredom is not easily distinguishable from
expressed their affective states while interacting with                 neutral on the basis of the facial features. Indeed, boredom
AutoTutor. However, one exception is that, in the D’Mello               resembles an expressionless face (see Figure 1a). This result
et al. (2006) study, the presence of a lip corner puller was            replicates an earlier finding by Craig et al. (2004), where no
associated with a state of confusion.                                   action unit was found to be associated with boredom.
   The present study revealed that a number of action units
that span the entire face can distinguish delight from neutral.         Discriminating between Affective States
In particular, the presence of AU 7 (lid tightener), AU 12
(lip corner puller), AU 25 (lips part), and AU 26 (jaw drop)            A discriminant analysis was performed with the affective
coupled with an absence of AU 45 (blink) segregate this                 state of the learner as the dependent variable and the various
emotion from neutral. These patterns are generally                      facial features as predictors. In this analysis only the AUs
consistent with a smile, as illustrated in Figure 1(c) where            that significantly correlated with the affective states were
AUs 7 and 12 are activated.                                             included as predictors (AU 1, AU 4, AU 7, AU 12, AU 25,
   Frustration is a state that is typically associated with             AU 26, and AU 45). Although four discriminant functions
significant physiological arousal, yet the facial features we           were calculated, the results indicated that only the first two
tracked were not very good at distinguishing this emotion               functions were statistically significant (χ2(28) = 200.8, p
from neutral. The only significant correlation with                     <.001 for function 1 and χ2(18) = 79.64, p < .001 for
frustration was obtained for AU 12 (lip corner puller) –
                                                                              Affective State
                                     Boredom               Confusion               Delight           Frustration           Neutral
    Facial Action Unit
                                      (N=26)                (N=59)                 (N=43)              (N=47)               (N=37)
                                 Judge1 Judge2         Judge1 Judge2          Judge1 Judge2        Judge1 Judge2       Judge1 Judge2
AU1 Inner Brow Raiser                                              .196
AU4 Brow Lowerer                  -.186                  .458      .417                  -.191                          -.240   -.160
AU7 Lid Tightener                 -.247 -.223            .157      .275         .240      .180                          -.288   -.270
AU12 Lip Corner Puller            -.260 -.300           -.208 -.150            .522      .456       .167     .161       -.265   -.224
AU25 Lips Part                                                                 .342      .337                           -.156   -.197
AU26 Jaw Drop                                                                  .363      .282                           -.164   -.145
AU43 Eye Closure                                                                                    .195                -.178
AU45 Blink                                   .313                              -.169 -.180
   Note. All listed correlations statistically significant at the p < .05 level.
                      Table 2 Statistically Significant Correlations between Action Units and Affective States
                                                                    470

function 2). These functions were able to account for 98%         of this emotion may have to use additional indicators,
of the variance, with 63.5% of the variance explained by the      acoustic-prosodic features of speech and posture patterns.
first function and the remaining 34.5% of the variance            We have also had some success in separating boredom from
attributed to function 2. Correlations between the predictor      neutral on the basis of dialogue features in the log file of
variables and the discriminant functions indicated that AU        interactions with AutoTutor (D’Mello & Graesser, 2006).
12 (lip corner puller) was the best predictor for function 1         In the earlier study that utilized emote-aloud protocols
while AU 1 (inner brow raiser), AU4 (brow lowerer), and           (Craig et al., 2004), it was reported that frustration was
AU 7 (lid tightener) were the best predictors for function 2.     associated with a raised inner and outer brow (AUs 1 and 2)
This implies that function 1 utilizes information from the        and a dimpler (AU 14). However, these patterns were not
lower face (i.e. the mouth) to discriminate between the           replicated in the current study. This suggests that there
affective states. The second function relies on information       might be occasional differences between our current offline
from the upper face (i.e. the brow and eyelids) to                methodology and our previous emote-aloud methodology,
disambiguate the affective states.                                which was an on-line measure. A smaller sample of
   The discriminant function was also able to successfully        participants (N = 7) were run in the emote-aloud study
predict the affective states with an accuracy of 49.1%, a         whereas 28 participants were run in the current study.
significant improvement over the base rate of 0.21 (kappa =          The broader goal of this research is to transform
.35). This is a reasonable level of accuracy in inferring         AutoTutor into an affect-sensitive intelligent tutoring
complex mental states. We also computed kappa scores for          system (D’Mello et al., 2005). This endeavor involves the
individually detecting each of the affective states. The          development of automatic affect detection systems and a
results indicated that the discriminant function was most         reengineering of AutoTutor’s pedagogical strategies to
successful in detecting delight (kappa = .65) followed by         incorporate the learner’s emotions. This research directly
confusion (kappa = .41). This was expected since both these       contributes to that effort by identifying the facial features
states are typically accompanied by animated facial               that accompany certain emotions. There are existing
expressions. The reliability of detecting the more subtle         systems that can automatically code AUs with reasonable
affective states of boredom and neutral was lower than            accuracy (Cohn & Kanade, in press). We are currently
delight and confusion (kappa = 0.12 for both states). Our         investigating the possibility of integrating these
results also indicated that the discriminant analysis was         computational systems to aid in inferring the affective states
unable to distinguish frustration from the other emotions. In     of the learner. We are also considering the use of additional
fact, the kappa score for this emotion reflected an accuracy      sensors that track posture patterns, speech contours, and
equal to random guessing (kappa = -0.07).                         dialogue information. We hope that the combination of
                                                                  these methods will yield reliable estimates of the affective
                         Discussion                               states of the learner.
This study examined the facial features that accompany the
affective states that routinely occur during learning complex                         Acknowledgments
topics with AutoTutor. We have discovered several                 We thank our research colleagues at the University of
important patterns in the manner in which learners convey         Memphis and MIT, as well as Steelcase Inc. for providing
these emotions though their faces. The highly animated            us with the Tekscan Body Pressure Measurement System at
affective states of confusion and delight are easily              no cost. This research was supported by the National
detectable from facial expressions. It is tempting to             Science Foundation (REC 0106965, ITR 0325428, REESE
speculate, from an evolutionary perspective, that learners        0633918). Any opinions, findings and conclusions or
use their face as a social cue to indicate that they are          recommendations expressed in this paper are those of the
confused, which potentially recruits resources to alleviate       authors and do not necessarily reflect the views of NSF.
their perplexity. Delight is also readily expressed on the
face, perhaps because it is a positive emotion. However, it                                References
appears that learners do not readily display frustration,         Ang, J., Dhillon, R., Krupski, A., Shriberg, E., & Stolcke,
perhaps due to the negative connotations associated with             A. (2002). Prosody-based automatic detection of
this emotion. This finding is consistent with Ekman’s theory         annoyance and frustration in human-computer dialog. In
of social display rules, in which social pressures may result        J. H. L. Hansenet al (Eds.), Proceedings of the
in the disguising of negative emotions such as frustration.          International Conference on Spoken Language Processing
   The associations between the various facial features and          (ICSLP’02) (pp. 2037-2039). Denver.
the affective states of confusion and boredom generally           Cohn, J. F. & Kanade, T. (In press). Use of automated facial
replicate earlier findings from the emote-aloud study. For           image analysis for measurement of emotion expression. In
example, the raising of the inner brow coupled with the              J. A. Coan & J. B. Allen (Eds.), The handbook of emotion
tightening of the lids appears to be the prototypical                elicitation and assessment. Oxford University Press Series
expression of confusion. However, for boredom, in neither            in Affective Science. New York: Oxford.
study could any particular subset of action units be              Craig S., D’Mello S., Gholson B., Witherspoon A., Sullins
associated with this emotion. This suggests that the tracking        J., and Graesser A, (2004). Emotions during learning: The
                                                              471

  first steps toward an affect sensitive intelligent tutoring     Graesser, A.C., & Olde, B.A. (2003). How does one know
  system. E-learn (in press). Association for the                   whether a person understands a device? The quality of the
  Advancement of Computing in Education, Norfolk, VA.               questions the person asks when the device breaks down.
Csikszentmihalyi, M. (1990). Flow: The Psychology of                Journal of Educational Psychology, 95, 524-536
  Optimal Experience. Harper-Row: NY.                             Graesser, A. C., Wiemer-Hastings, K., Wiemer-Hastings, P.,
D'Mello, S. K., Craig, S. D., Gholson, B.; Franklin, S.,            Kreuz, R., & the TRG (1999). AutoTutor: A simulation of
  Picard, R., & Graesser, A. C. (2005). Integrating affect          a human tutor. Journal of Cognitive Systems Research, 1,
  sensors in an intelligent tutoring system. In Affective           35-51.
  Interactions: The Computer in the Affective Loop                Graesser, A.C., Witherspoon, A., McDaniel, B., D’Mello,
  Workshop at 2005 International conference on Intelligent          S., Chipman, P., Gholson, B. (2006). Detection of
  User Interfaces, 7-13. New York: AMC Press.                       emotions during learning with AutoTutor. In R. Son (Ed.),
D’Mello, S. K., Craig, S. D., Sullins, J., & Graesser, A. C.        Proceedings of the 28th Annual Meetings of the Cognitive
  (2006). Predicting affective states through an emote-aloud        Science Society. (pp. 285-290). Mahwah, NJ: Erlbaum.
  procedure from AutoTutor’s mixed-initiative dialogue.           Harter, S. (1981). A model of mastery motivation in
  International Journal of Artificial Intelligence in               children: Individual differences and developmental
  Education, 16, 3-28.                                              change. In W. A. Collins (Ed.). The Minnesota
D’Mello, S., & Graesser, A.C. (2006). Affect detection from         symposium on child psychology. Aspects of the
  human-computer dialogue with an intelligent tutoring              development of competence, 14, (pp. 215-255). Hillsdale,
  system. In J. Gratch, M. Young, R. Aylett, D. Ballin, and         NJ: Lawrence Erlbaum Associates.
  P. Oliver (Eds.), Lecture Notes in Computer Science:            Kapoor, A., Mota, S. and Picard, R. (2001). Towards a
  Intelligent Virtual Agents: 6th International Conference          learning companion that recognizes affect. AAAI Fall
  (pp. 54-67). Berlin, Heidelberg, Germany: Springer.               Symposium 2001, North Falmouth MA, November 2001.
Elfenbein, H. A. & Ambady, N. (2002) On the universality          Kort, B., Reilly, R., & Picard, R. (2001). An affective model
  and cultural specificity of emotion recognition: a                of interplay between emotions and learning:
  metaanalysis, Psychological Bulletin, 128, 203–235                Reengineering educational pedagogy—building a learning
Ekman, P. 2003. Emotions Revealed. New York: Henry                  companion. In T. Okamoto, R. Hartley, Kinshuk, & J. P.
  Holt and Company.                                                 Klus (Eds.), Proceedings IEEE International Conference
Ekman, P. & Friesen, W.V. (1969). Nonverbal leakage and             on Advanced Learning Technology: Issues, Achievements
  clues to deception. Psychiatry, 32, 88-105.                       and Challenges (pp. 43-48). IEEE Computer Society.
Ekman, P, & W. V. Friesen. (1978). The facial action              Meyer, D. K., & Turner, J. C. (2002). Discovering Emotion
  coding system: A technique for the measurement of facial          in Classroom Motivation Research. Educational
  movement. Palo Alto: Consulting Psychologists Press               Psychologist, 37, 107-114. Miserandino, M. (1996)
Ekman, P., & Friesen, W. (1972). Constants across culture           Children who do well in school: Individual differences in
  in the face and emotion. Journal of Personality and Social        perceived competence and autonomy in above-average
  Psychology, 17, 124–129. Ericsson, K. A. & Simon, H. A.           children, Journal of Educational Psychology, 88, 203–
  (1993). Protocol analysis: Verbal reports as data. Revised        214.
  edition. Cambridge, MA: The MIT Press.                          Patrick B., Skinner, E. & Connell, J. (1993). What motivates
Forbes-Riley, K. & D. Litman. (2004). Predicting emotion            children’s behavior and emotion? Joint effects of
  in spoken dialogue from multiple knowledge sources. In            perceived control and autonomy in the academic domain,
  Proceedings of the Human Language Technology                      Journal of Personality and Social Psychology, 65, 781–
  Conference of the North American Chapter of the                   791.
  Association for Computational Linguistics, 201-208.             Picard, R. W. (1997). Affective computing. Cambridge,
  Boston, MA.                                                       MA: MIT Press. Rozin, P. & Cohen, A. B. (2003). High
Graesser, A. C., P. Chipman, B. C. Haynes, & A. Olney.              frequency of facial expressions corresponding to
  (2005). AutoTutor: An intelligent tutoring system with            confusion, concentration, and worry in an analysis of
  mixed-initiative dialogue. IEEE Transactions in                   maturally occurring facial expressions of Americans,
  Education, 48, 612-618.                                           Emotion, 3, 68–75.
Graesser, A.C., Jackson, G.T., & McDaniel, B. (2007).             Snow, R. E., Corno, L., & Jackson, D. (1996). Individual
  AutoTutor holds conversations with learners that are              differences in affective and conative functions. In D. C.
  responsive to their cognitive and emotional states.               Berliner & R. C. Calfee (Eds.) Handbook of educational
  Educational Technology, 47, 19-22.                                psychology (pp. 243-310). New York: Macmillan.
Graesser, A.C., McDaniel, B., Chipman, P., Witherspoon,           Stipek, D. (1998) Motivation to learn: from theory to
  A., D’Mello, S., & Gholson, B. (2006). Detection of               practice (3rd ed.) Boston, MA: Allyn and Bacon.
  emotions during learning with AutoTutor. In R. Son (Ed.),
  Proceedings of the 28th Annual Meetings of the Cognitive
  Science Society (pp. 285-290). Mahwah, NJ: Erlbaum.
                                                              472

