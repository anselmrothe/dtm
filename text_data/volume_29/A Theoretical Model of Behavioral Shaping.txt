   Although this paper analyzes shaping with respect to                                 Target concept T
its benefits on search problems, the reader should recog-
                                                                  0                                                        1
nize that shaping is often intimately related to reinforce-
ment learning. The objective in reinforcement learning
                                                                 Figure 1: The target concept T is a subset of the concept
is to find a policy (i.e., a mapping from states to ac-
                                                                 S = [0, 1]. The search agent needs to search S for a point
tions) that maximizes the reward obtained. In general,
                                                                 in T .
searching the policy space for an optimal policy (or even
a good policy) is computationally intractable. However,
it is possible that an agent can learn to perform a task         reward functions are binary, whereas those of Ng et al.
faster if a teacher is available that adaptively modifies        allow real-valued rewards.
the reward function in the manner suggested by behav-               In the next section, we describe a formal model of
ioral shaping. For example, consider a sequential deci-          behavioral shaping. This model can be used to prove
sion problem in which an agent attempts to reach the             the benefits of shaping in several cases involving convex
reward state by choosing one of two actions at each mo-          reward regions lying in multi-dimensional spaces. For
ment in time. The agent chooses sequences of actions of          pedagogical reasons, the following section focuses on a
length dlog2 ne, and then is told whether the resulting          simple one-dimensional case where we prove that shap-
state is the reward state. This situation can be charac-         ing can be helpful in finding intervals on a straight line.
terized by a binary tree in which nodes correspond to            We show a lower bound on regret when shaping is used,
states and edges correspond to actions. The tree has n           and a learning algorithm which hits that bound. Lastly,
leaf nodes, one of which corresponds to the reward state,        we show that the convexity of reward regions is an im-
and the tree’s depth is dlog2 ne. Consequently, a policy         portant requirement of our framework.
is a sequence of dlog2 ne actions. The optimal policy is
the sequence leading to the leaf corresponding to the re-                            Formal Model
ward state. This scenario is analogous to the scenario           Let (X, B) be a measurable space with a measure µ (see
described above; instead of searching for a reward ele-          Doob, 1994, for an introduction to mathematical mea-
ment in an array, the agent now searches for a sequence          sure theory). Let C ⊆ B. The set C is called a concept
of actions leading to a reward leaf of a tree. Using the ac-     class and its members are called concepts. Examples
tual target reward function, the agent accumulates O(n)          of concept classes that we study include intervals in R,
regret in the worst case where regret is defined as the to-      axis-parallel rectangles in Rd , balls in Rd , and convex
tal number of action sequences that the agent tries before       bodies in Rd . We will assume that there is a representa-
discovering the optimal sequence leading to the reward           tion scheme for the concept class C (Kearns & Vazirani,
state. What if a teacher is available that modifies the          1994).
reward function in the manner suggested by behavioral
shaping? Assuming that the teacher marks 1/2 of the              Definition 1 (Search problem). Let C be a concept class.
leaves with reward at iteration 1 and successively halves        A search problem is a pair of concepts (S, T ) such that
the reward area at each iteration (as described above),          T ⊆ S, and S, T ∈ C.
then the agent can learn the optimal policy with only               A search agent is given a representation of S and has to
O(log2 n) regret. Again, this is an exponential improve-         find a point in T using the membership oracle of T . This
ment in performance relative to the case where there is          scenario is illustrated in Figure 1 where the target con-
no teacher available to guide the agent’s search.                cept T is a subset of the concept S = [0, 1]. The number
                                                                 of oracle queries until a point is found in T is defined as
   Our work is related to the “reward shaping” frame-            the regret. Throughout this paper, without loss of gener-
work of Ng, Harada, & Russell (1999). These authors              ality, we assume that µ(S) = 1, and µ(T ) = 1/R, where
sought to speed-up reinforcement learning by transform-          R ≥ 1. Note that for any concept class there is a nat-
ing the original reward function to a new reward function        ural randomized algorithm to solve the search problem:
that provides additional training information to guide an        query independent uniform random points from S until
agent’s search. They gave mathematical conditions un-            you find a point in T . The expected regret of the algo-
der which a transformation is policy invariant meaning           rithm is R. For sufficiently complicated concept classes
that an optimal policy for the original reward function is       (e.g., finite unions of intervals), the use of randomness
also an optimal policy for the new reward function. Our          might be inevitable because a deterministic algorithm
approach is different from theirs in at least two ways.          with bounded regret need not exist.
First, we use a temporal sequence of reward functions               In a shaped search problem the agent’s search task will
as compared to their fixed reward transform. Conse-              be aided by a shaping sequence which is a sequence of
quently, we believe that our approach is more consistent         nested sets between S and T . The sets in the shaping
with the practice of behavioral shaping as reported in           sequence will be gradually shrinking concepts from the
the psychological literature (Skinner, 1938). Second, our        underlying concept class C. The rate of shrinking will
                                                             876

                                Concept S                                  tion of T (through the membership oracles O1 , . . . , Ok−1 ),
                                                                           an agent with access to a shaping sequence can poten-
 0                                                                  1
                                                                           tially achieve a smaller regret than would otherwise be
                  Concept S1                                               the case. For certain classes of search problems, for
                                                                           example, it might be possible to approximately deter-
 0                                                                  1      mine Si using oracle Oi . If Si is approximately located,
              Concept S2
                                                                           then Si+1 could be approximately located (via queries
                                                                           to Oi+1 ) by searching only inside the approximation of
 0                                                                  1      Si rather than inside S. As Si has a smaller measure
                                                                           than S, this search may require a relatively small num-
              Concept S3 = T
                                                                           ber of queries. This process can be used iteratively from
                                                                    1
                                                                           i = 1 to i = k − 1 to find a point inside Sk = T . In the
 0
                                                                           next section we show that it is indeed possible to reduce
Figure 2: This figure illustrates the use of a sequence of                 an agent’s regret using this method when S and T are
nested concepts. Intuitively, the agent initially searches                 intervals on a straight line.
concept S for a point that lies in concept S1 . Next,                         The regret accumulated by the agent during the search
the agent searches in the neighborhood of the discovered                   also depends on the value of γ. Very small (close to 0)
point in S1 for a point that lies in S2 , and then searches                γ values are detrimental because the agent will need to
in the neighborhood of the discovered point in S2 for a                    search a relatively large region for a point lying in a
point that lies in S3 which is the target concept T .                      much smaller region [e.g., µ(S1 ) will be a small fraction
                                                                           of µ(S)], meaning that the agent will need to search for
                                                                           a “needle in a haystack”. Large values of γ (close to 1)
be a parameter denoted γ. This scenario is illustrated                     may seem detrimental (because the teacher will take a
in Figure 2 which shows a sequence of nested concepts.                     long time to converge to T ), but are actually not as we
Intuitively, the agent initially searches concept S for a                  discuss below.
point that lies in concept S1 . Next, the agent searches in                   Note that our approach is related to a technique from
the neighborhood of the discovered point in S1 for a point                 the mathematical optimization literature known as quasi-
that lies in S2 , and then searches in the neighborhood                    convex optimization (Boyd & Vandenberghe, 2004). A
of the discovered point in S2 for a point that lies in S3                  weaker version of the shaped search problem in which the
which is the target concept T .                                            concepts are convex and all the oracles are available to
Definition 2 (Shaped search problem). Let C be a con-                      the agent simultaneously can be viewed as an instance of
cept class. A shaped search problem is defined by the                      a quasi-convex optimization problem. However, in our
tuple (S, T, γ, S1 , . . . , Sk ) such that S, T, S1 , . . . , Sk ∈ C,     approach, the oracles are not available simultaneously
 1                                                                         but, rather, are available as a temporal sequence. This
R ≤ γ < 1, T = Sk ⊆ Sk−1 ⊆ . . . S1 ⊆ S, µ(S1 ) =
γµ(S), µ(Si+1 ) = γµ(Si ) for all i = 1, . . . , k − 2. k                  is because behavioral shaping almost always proceeds in
is such that µ(Sk−1 ) ≥ 1/R ≥ γµ(Sk−1 ); i.e., k =                         a temporal fashion (typically, rewards are initially pro-
dlog γ1 Re. The sequence (S1 , . . . , Sk−1 ) is called a γ shap-          vided to coarse approximations to the target behavior,
ing sequence.                                                              and then only finer approximations are rewarded at later
                                                                           stages of training).
   An agent in a shaped search problem setting is given
a representation of S and has access to the membership                              Finding a Point in an Interval
oracles O1 , . . . , Ok of S1 , . . . , Sk , respectively. However,
if the agent makes a query to Oi , it can no longer make                   To keep our discussion relatively simple, this section fo-
a query to any Oj such that j < i. In other words, the                     cuses on the case where concepts are intervals on the real
oracles Oi are presented to the agent in k iterations, with                line. Before analyzing the effectiveness of using a shap-
the agent making (zero or more) queries only to oracle                     ing sequence in this circumstance, we consider learning
Oi at iteration i. The agent has to find a point in T                      in the absence of such a sequence.
by making queries to the oracles. In this context, the                        Consider a search problem (S, T ) such that S = [0, 1] ⊂
regret is defined as the total number of queries made to                   R is a closed interval of length 1, and T ⊂ S is an inter-
all oracles until a point in T is found.                                   val of size 1/R. The search agent has access to O, which
   Note that an agent solving the shaped search problem                    is the membership oracle of T . Consider the following
cannot have a regret larger than what its regret would                     simple deterministic algorithm to find a point inside T .
be on the original search problem; the agent can always                    The algorithm starts with the leftmost point in S, and
choose to ignore the shaping sequence and only use the                     query points 0, 1/R, 2/R, 3/R, . . . , 1 (as a matter of ter-
last membership oracle Ok for Sk = T . Because a shap-                     minology, we say that the algorithm “makes jumps of
ing sequence provides extra information about the loca-                    size 1/R in S”). As T is a contiguous, closed interval of
                                                                       877

size 1/R, the algorithm is guaranteed to hit one point            Algorithm 2 fixes this problem and achieves a regret of
inside T . As T has size 1/R, there will be at most O(R)          O(log2 R) for γ > 1/2.
queries. Therefore the regret will be at most O(R).                  We start with Algorithm 1. At the first iteration, the
   The previous algorithm assumed that R is known.                algorithm’s goal is to find a point inside S1 . The algo-
Next, we show that even without this information, it              rithm does not know where S1 lies, but it knows that
is possible to solve the problem with O(R) regret. To             S1 is of size γ, so it makes jumps of size γ in S and is
do this, the agent first sets µ̂(T ), its estimate of the         guaranteed to hit a point inside S1 . At the first itera-
length of T , to 1 and makes queries at points 0 and 1.           tion, the algorithm makes at most d γ1 e queries to oracle
If it hits a point in T , it stops; otherwise it halves the       O1 . At the ith iteration (where i > 1), the algorithm’s
value of µ̂(T ) to 1/2, and queries points 0, 1/2, and 1.         goal is to find a point in Si . The algorithm has a point
The algorithm keeps halving µ̂(T ), and keeps making              p which lies in Si−1 (from the previous iteration). The
jumps of size µ̂(T ) in S, until it hits a point inside T .       interval Si of size γ i can contain points lying on either
In general, if the agent estimates µ̂(T ) = 21k , it queries      side of p but, because Si ⊂ Si−1 , its left (right) edge can
less than 2k+1 points. Importantly, there is a nonneg-            lie at most γ i−1 to the left (right) of p. The algorithm
ative integer k0 such that 2k01+1 ≤ 1/R ≤ 2k10 . When             makes d γ1 e jumps of size γ i to the left and right of p,
the agent guesses µ̂(T ) = 2k01+1 , it is guaranteed to hit       and hits a point in Si . Therefore, at each iteration, at
a point in T . The total number of queries to O will              most 2d γ1 e queries are made. As there are dlog γ1 Re iter-
be 2 + · · · + 2k0 +2 = 2k0 +3 − 2 which is less than 8R.         ations, the total regret is 2d γ1 edlog γ1 Re = O( γ1 log γ1 R).
Therefore, there is an algorithm to solve the search prob-
                                                                  For a fixed value of γ, this is an exponential improvement
lem with regret O(R) even if the agent does not know
                                                                  over the case when a shaping sequence is not available.
the value of R and, thus, does not know the size of T .
                                                                  However, when γ is close to 1, this regret exceeds O(R)
Throughout the rest of the paper, we assume that the
                                                                  (as discussed below, Algorithm 2 solves the problem for
agent knows the value of R and knows that the size of T
                                                                  large γ by skipping oracles). This gives us the following
is 1/R.
                                                                  theorem:
   We now show that no deterministic algorithm can find
a point in T with less than bR − 1c regret. Suppose a             Theorem 1. If the concept class C is the set of closed
deterministic algorithm has a regret r < bR − 1c. With-           intervals then there is an algorithm that can solve the
out loss of generality, assume that all the r queries to          shaped search problem (S, T, γ, S1 , . . . , Sk ) for any γ shap-
the oracle are distinct. These r distinct points will in-         ing sequence S1 , . . . , Sk with O( γ1 log γ1 R) regret where
duce r + 1 intervals in S. The average size of the interval       |S| = 1 and |T | = 1/R.
           1
will be r+1  . Therefore, at least one interval will be of
                1
size at least r+1  . As r < bR − 1c, at least one interval        Algorithm 1 An algorithm to solve the shaped search
will be of size at least 1/R which means that there is at         problem (S, T, γ, S1 , . . . , Sk ) such that T, S, S1 , . . . , Sk ⊂
least one way in which T can be placed such that the              R using the sequence of oracles O1 , . . . , Ok .
agent will not be able to query any point inside T if it
makes less than bR − 1c queries. Using a similar argu-            Require: 1/R ≤ γ < 1, S = [0, 1], membership oracles
ment, it can be shown that any randomized algorithm                    O1 , . . . , Ok of the sets S1 , . . . , Sk .
will accumulate Ω(R) regret. This gives us the following            1. p ⇐ 0
proposition:                                                        2. for i = 1 to k do
Proposition 1. If the concept class C is the set of closed          3.   if i=1 then
intervals then there is an algorithm that can solve the             4.       query O1 with points 0, γ, 2γ . . .
search problem (S, T ) with O(R) regret where |S| = 1               5.       p ← first point at which O1 outputs 1.
and |T | = 1/R. Further, any (randomized) algorithm                 6.   else
that solves the search problem will have Ω(R) regret.               7.       Query oracle Oi with points p − γ i−1 , . . . , p −
   We now turn our attention to the case where a shaping                     2γ i , p − γ i , p, p + γ i , p + 2γ i , . . . , p + γ i−1
sequence is available. In summary, we show that when                8.       p ← first point at which Oi outputs 1.
any γ shaping sequence S1 , . . . , Sk−1 is available such          9.   end if
that the Si are closed intervals, it is possible to find a        10. end for
point in T with a regret which is logarithmic in R. Sup-          11. return p
pose a shaping sequence S1 , . . . , Sk−1 of nested intervals
is available. We present two algorithms which make use               An interesting aspect of our analysis is that it reveals
of this sequence to find a point in T . Algorithm 1 solves        a number of issues regarding the shrinking rate param-
the problem with O( γ1 log γ1 R) regret for R1 ≤ γ < 1.           eter γ. First, the above algorithm assumes that γ is
Although this regret is logarithmic in R for a fixed γ,           known. Importantly, it can be shown that even if γ is
it goes beyond the O(R) regret when γ approaches 1.               unknown, it is still possible to solve the shaped search
                                                              878

problem with O( γ1 log γ1 R) regret. The method to do so               lem (S, T, 1/2, Ss , S2s , . . . , Sk ) using Algorithm 1, which
is very similar to the one described above which solved                solves the search problem with O(log2 R) regret.
the search problem (S, T ) when R was unknown (and,                        When R1 ≤ γ < 12 , the bound on regret depends on γ.
thus, will not be described here).                                     Note that Algorithm 2 calls Algorithm 1 when γ < 12 ,
   Second, there exists an optimal value for γ that min-               meaning that the total regret is O( γ1 log γ1 R). We show
imizes γ1 log γ1 R. This value is γ = 1/e. This result is              that this is also a lower bound when γ < 12 . In Algo-
                               1                                       rithm 1, suppose that at each iteration i, the location of
obtained by differentiating log γ1 R with respect to γ,
                               γ
setting the derivative to zero, and solving for γ.                     the interval Si is fully revealed if the algorithm makes
   Lastly, there is a trade-off involving the value of γ. If           more than b γ1 − 1c queries. We show that even with this
γ = R1 , then k = 1 and the shaping sequence is non-                   extra information, the total regret to find a point in the
existent (i.e., S1 = T ) meaning that the regret becomes               interval T is still O( γ1 log γ1 R).
O(R). If γ is close to one, then k is large and the shaping                At iteration i, the interval Si of size γ i must lie within
sequence is very long. In this case, the regret increases              the interval Si−1 of size γ i−1 . For each i, we are sup-
rapidly to infinity as γ approaches 1. Consequently,                   posing that the algorithm knows Si−1 exactly, as it was
Algorithm 1 is not optimal because, when γ is large,                   revealed at the end of the previous iteration. Without
the agent could simply ignore the shaping sequence and                 loss of generality, we assume the agent queries only inside
make queries only to the final oracle Ok which will lead               Si−1 at iteration i. If the algorithm queries t points, it
to O(R) regret. Fortunately, it is possible to do better               induces t + 1 intervals on Si−1 . The average interval size
than this. When γ is large, the agent can choose to make                                                   i−1
                                                                       of these intervals will be γt+1 . This means that there
queries only to oracles Os , O2s , . . . , Ok where s is the first     will be at least one interval which will be i−1      at least the
integer such that γ s < 1/2 (for simplicity, we assume                 average size. If t < b γ1 − 1c < γ1 − 1, then γt+1 > γ i . In
that k is a multiple of s). In other words, the strategy of            other words, there is at least one interval which will be
a new algorithm, denoted Algorithm 2, is to use sets of                of size at least γ i . As Si might lie anywhere in Si−1 , the
the shaping sequence that are successively shrinking by                alleged algorithm is not guaranteed to be able to find Si
a factor of about 1/2, and ignore other elements of the                if Si lies in this interval of size at least γ i . Consequently,
shaping sequence. In this way, γ is effectively reduced                any correct search algorithm will have to make at least
to 1/2 for the agent, and the regret reduces to O(log2 R).             b γ1 − 1c = O( γ1 ) queries at each iteration and, thus, the
We next prove a lower bound on regret, and show that
                                                                       total regret is bounded from below by Ω( γ1 log γ1 R). A
Algorithm 2 achieves this lower bound. That is, we show
that no algorithm can do better than Algorithm 2 (up                   similar argument holds for randomized algorithms.
to a constant factor).
Theorem 2. If the concept class C is the set of closed
intervals then there is a deterministic algorithm that can             Algorithm 2 An optimal algorithm to solve the
solve the shaped search problem (S, T, γ, S1 , . . . , Sk ) with       shaped search problem (S, T, γ, S1 , . . . , Sk ) such that
O( γ1 log γ1 R) regret for γ < 1/2 and with O(log2 R) re-              T, S, S1 , . . . , Sk ⊂ R using the sequence of oracles
                                                                       O 1 , . . . , Ok .
gret for γ ≥ 1/2 for any shaping sequence S1 , . . . , Sk
where |S| = 1 and |T | = 1/R. Further, there is no al-
gorithm, deterministic or otherwise, that can solve the                Require: 1/R ≤ γ < 1, S = [0, 1], membership oracles
shaped search problem (with Si being closed intervals)                       O1 , . . . , Ok of the sets S1 , . . . , Sk .
with smaller regret (up to a constant factor).                           1. if γ ≤ 21 , call Algorithm 1 and return p, otherwise
                                                                             continue.
Proof. First we show an information-theoretic lower bound                2. s = d log1 1 e
                                                                                          2 γ
on regret which is independent of γ. For a given R, we                   3. call Algorithm 1 with γ = 1/2 and membership ora-
can place R intervals of size 1/R next to each other inside                  cles Os , O2s , . . . , Ok and return p.
an interval of size 1. To encode the identity of a partic-
ular interval, we need log2 R bits. As the oracles are
providing one bit of information at each query, any al-
gorithm that successfully solves the problem must make                                     Convexity is Important
Ω(log2 R) total queries to the oracles.                                In Theorem 2 we showed that if the concept class is the
   We now show that Algorithm 2 hits this bound when                   set of closed intervals, it is possible to solve the shaped
γ > 12 , meaning that the bound is tight for γ > 12 .                  search problem with smaller regret than required to solve
When γ > 12 , the algorithm sets s = d log1 1 e (with-                 the original search problem. In this section, we show
                                                    2 γ
out loss of generality, assume that γ s = 1/2) and calls               that if the members of the concept class are a union of
Algorithm 1 with the oracles Os , O2s , . . . , Ok . There-            two closed intervals, then there are shaped search prob-
fore the algorithm is solving the shaped search prob-                  lems such that, for a fixed γ, it is not possible to find a
                                                                   879

point in T with less than O(R) regret for certain shaping           cept class is the set of unions of two intervals, there are
sequences.                                                          shaping sequences which do not reduce regret.
   Consider the following example with γ = 1/2. Sup-                  The results presented here form the foundation for ad-
pose each set Si in the shaping sequence consists of two            ditional results reported in a longer article (Chhabra,
segments. The first segment is the target interval T , and          Jacobs, and Stefankovic, 2007). In this article, we study
it remains fixed throughout all sets in the shaping se-             the cases where concepts are rectangles, ellipsoids, or
quence. The second segment shrinks at a rate such that              general convex bodies in high dimensions. In multi-
successive sets in the sequence maintain a size ratio of            dimensions, new methods are required to create efficient
1/2. In the last iteration, this second segment vanishes.           search algorithms.
   More formally, let S = [0, 1] and γ = 1/2. The
shaping sequence S1 , . . . , Sk−1 is the sequence of sets                            Acknowledgments
Si = T ∪ Qi , i = 1, . . . , k − 1, where Qi = [0, 1/2i − 1/R],     This work was supported by AFOSR research grant FA9550-
T = [l, l + 1/R], and 1/2 ≤ l ≤ 1 − 1/R. In this case,              06-1-0492.
even if the teacher fully reveals the location of the seg-
ment Qi to the agent at each iteration, the agent still                                    References
needs to locate the set T , which can lie anywhere in a             Anthony, M. & Biggs, N. (1992). Computational Learn-
region of size 1/2. Hence, to find a point in T , O(R)                ing Theory. Cambridge, UK: Cambridge University
queries are needed.                                                   Press.
   We have just shown that there are shaping sequences              Boyd, S. & Vandenberghe, L. (2004). Convex Optimiza-
which do not exponentially reduce an agent’s regret if the            tion. Cambridge, UK: Cambridge University Press.
concept class is the set of unions of two closed intervals.
                                                                    Chhabra, M., Jacobs, R. A., & Stefankovic, D. (2007).
Although this example assumes a fixed value of γ, similar
                                                                      Behavioral shaping for geometric concepts. Journal of
results can be shown for arbitrary values of γ when the
                                                                      Machine Learning Research, in press.
concept class consists of more complicated non-convex
concepts. For example, if the concept class is the set of           Doob, J. L. (1994). Measure Theory. New York: Springer-
unions of three closed intervals, an exponential reduction            Verlag.
in regret is not possible for any value of γ.                       Dorigo, M. & Colombetti, M. (1994). Robot shaping:
   We conjecture that convexity is an important require-              Developing autonomous agents through learning. Ar-
ment for shaped search. That is, we conjecture that                   tificial Intelligence Archive, 71, 321-370.
when an agent seeks to discover a concept through a                 Kearns, M. J. & Vazirani U. V. (1994). An Introduction
shaped search procedure, it will only be able to achieve              to Computational Learning Theory . Cambridge, MA:
a small regret when the concept is convex. In fact, all               MIT Press.
our additional results regarding shaped search in multi-
dimensions are for concept classes consisting of convex             Ng, A., Harada, D., & Russell, S.(1999). Policy invari-
bodies (Chhabra, Jacobs, & Stefankovic, 2007).                        ance under reward transformations: Theory and appli-
                                                                      cation to reward shaping. In I. Bratko (Ed.), Proceed-
                          Summary                                     ings of the 16th International Conference on Machine
                                                                      Learning. San Francisco: Morgan Kaufmann.
Shaping is a commonly used procedure for teaching com-
plicated tasks to people, animals, and robots. Both                 Randlov J. (2000). Shaping in reinforcement learning
behavioral experiments and computer simulations have                  by changing the physics of the problem. In P. Langley
demonstrated that learners trained via shaping achieve                (Ed.), Proceedings of the 17th International Confer-
significant improvements in learning speeds.                          ence on Machine Learning. San Francisco, CA: Mor-
                                                                      gan Kaufmann.
   In this paper, we mathematically formalized a model
of shaping, and studied it in the context of search prob-           Skinner, B. F. (1938). The Behavior of Organisms. New
lems. To keep our discussion simple, we focused on the                York: Appleton-Century-Crofts.
one–dimensional case where concepts are intervals on                Sutton, R. S. & Barto, A. G. (1998). Reinforcement
the real line. When a shaping sequence is available, the              Learning: An Introduction. Cambridge, MA: MIT
search problem can be solved with exponentially less re-              Press.
gret than would otherwise be possible. We also showed
that there do not exist algorithms which can solve the
search problem using a smaller number of queries. Our
analysis revealed a number of interesting issues regarding
the shrinking rate parameter γ. Lastly, we conjectured
that it is important that concepts are convex for shaping
sequences to be useful. We showed that when the con-
                                                                880

