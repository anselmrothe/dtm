UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Rules and Exemplars in Categorization: A Computational Exploration
Permalink
https://escholarship.org/uc/item/4b6754n7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Brumby, Duncan P.
Hahn, Ulrike
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               Rules and Exemplars in Categorization: A Computational Exploration
                                              Duncan P. Brumby (Brumby@cs.drexel.edu)
                                              Department of Computer Science, Drexel University
                                                         Philadelphia, PA 19104 USA
                                                   Ulrike Hahn (HahnU@cardiff.ac.uk)
                                                    School of Psychology, Cardiff University
                                                             Cardiff, CF10 3AT UK
                                Abstract                                   is therefore adaptive in this task in a way it need not be in
   Studies have found that human categorization judgments are
                                                                           general.
   affected by exemplar similarity, even when a simple, perfectly              In response to the critique of the rule description used in
   predictive rule is provided and paying attention to instance            Allen and Brooks’ (1991) experiments, Hahn, Prat-Sala and
   similarity is harmful to performance. These data provide an             Pothos (2002) sought to test whether exemplar similarity
   interesting challenge for recent hybrid rule-plus-exemplar models       effects would arise in a rule-based task in which category
   of category learning. We report the results of a modeling effort        membership was entirely uncorrelated with similarity. Hahn et
   with a pre-existing hybrid model developed in the ACT-R                 al. found effects of exemplar similarity on error patterns and
   cognitive architecture. A search of the model’s parameter space         reaction times, even under conditions where attending to
   revealed that increasing use of an exemplar route improved the fit      similarity interfered with performance on the rule application
   of the model to the data, because it resulted in faster
   categorization judgments for high-similarity items compared to
                                                                           task. At the same time they also found very low error rates,
   low-similarity items. However, use of the exemplar route carried        which suggests that the rule was in fact used.
   no adaptive value for the model because it necessarily lead to             Hahn et al.’s findings are of interest because hybrid rule-
   more categorization errors than simply basing judgments on the          plus-exemplar models of categorization would generally
   categorization rule alone. The fact that people’s categorization        predict that (1) categorization errors should be associated with
   judgments juxtapose rule application with instance-similarity           exemplar similarity effects and (2) any reduction in error rates
   while maintaining very low error rates presents a non-trivial           should be associated with diminished similarity effects. That
   problem for current hybrid models of category learning.                 people’s categorization judgments juxtapose rule application
   Keywords: categorization; rules;           exemplars;  similarity;      with instance-similarity, while maintaining very low error rates
   computational modeling; ACT-R                                           seems at odds with the basic predictions that can be derived
                                                                           from these models of categorization.
                           Introduction                                       In this paper, we first describe the data from two experiments
Over the last two decades, categorization research has seen a              (Hahn et al., 2002; Hahn et al., submitted) that investigate the
steady rise in interest in hybrid accounts; particularly rule-plus-        effect of similarity on the application of a simple, perfectively
exemplar accounts that assume human categorization                         predictive rule. These data suggest that combining rule
judgments are formed through some mix of exemplar- and                     application with instance-similarity occurs even under
rule-based processes (e.g., Erickson & Kruschke, 1998;                     conditions where paying attention to instance similarity is
Palmeri, 1997; for a different hybrid approach, see Ashby et al,           harmful to performance. A reimplementation of Anderson and
1998). This interest, for which there is good theoretical reason           Betz’s (2001) ACT-R model follows, along with a
(e.g., Hahn & Chater, 1998), has fueled experimental tests as              computational exploration of the parameter space of the model,
well as a range of computational models of varying scope and               in order to find the best-fitting model for the first data set.
specificity.                                                               Based on these best-fitting parameter values, a comparison
   Allen and Brooks (1991) provided a seminal experimental                 between the performance predictions of the model and the
demonstration of the joint effects of rules and exemplars in               second data set is presented.
categorization. Participants in the study were given a simple
rule to classify both old and novel items. Even though the rule                                    Empirical Data
was perfectively predictive there was evidence for systematic              Hahn et al. (2002), in an experiment which we will refer to as
effects of exemplar similarity on categorization.                          Experiment 1, constructed a set of items governed by a simple,
   Allen and Brooks’ results are somewhat less surprising when             perfectly predictive rule that specified three necessary and
one takes into account the specific nature of the rule used in the         sufficient features for category membership (e.g., “is an A if it
study. Specifically, the rule described an m-of-n concept (“an             has an upside-down triangle at the sides, a cross in the centre,
object is a digger if it has at least 3 of the following 5                 and a curly line at the top”). Participants were told this rule at
features…”). This type of rule description is functionally                 the beginning of the experiment, and were then given a series
equivalent to a prototype-plus-similarity threshold account.               of positive exemplars as illustration. At test, participants were
Consequently, for Allen and Brooks’ materials similarity is                given 96 novel items, distributed over four blocks. Participants
correlated with the rule’s applicability. Attending to similarity          did not receive feedback regarding the accuracy of their
                                                                           categorization judgments. Half of the test items complied with
                                                                       131

the rule, and half violated it. At the same time, half of the items     responses for low-similarity compliant items (1290 ms vs. 1405
were high in similarity to one of the initial training exemplars        ms, respectively), and RT also decreased over consecutive
(as determined by the amount of overlap in the non-rule                 blocks of trials (1776 ms, 1314 ms, 1205 ms, 1095 ms,
features of the objects) and half were low in similarity to the         respectively). There was no interaction between similarity and
training exemplars. Manipulations of similarity were                    block.
orthogonal to category membership, such that exactly half the              In summary, then, these data provide robust evidence of
rule-compliant items were high in similarity to the training            exemplar similarity effects, even under conditions where
items and half were not, and likewise for the non-compliant             attending to similarity interferes with performance on the rule
items. In other words, while using the rule would lead to               application task. At the same time, the consistently low error
perfect performance, basing categorization on exemplar                  rates demonstrate that the rule was used. We next describe a
similarity would lead to chance performance.                            hybrid model of categorization and derive predictions for these
   The crucial question addressed by Experiment 1 was whether           data.
effects of exemplar similarity would arise even under
circumstances where attending to similarity had no adaptive                         Hybrid Model of Categorization
value. Analyzing data from 42 participants, significant effects         Current hybrid models differ substantially in the way rules and
of exemplar similarity were found on error rates and reaction           exemplars are related. The first set of models assumes two
time (RT). From a total of (96x42) 4032 responses, only 7.56%           independent routes; a route is chosen on a trial-by-trial basis
were errors (where the participant pressed the YES key when             depending on a number of factors such as simplicity or
the NO key was expected or the other way round). In addition,           reliability (e.g., Anderson & Betz, 2001; Ashby et al, 1998).
there were significantly more errors on low-similarity                  The second set of models assumes a parallel competition or
compliant items than on high-similarity compliant items (61 vs.         race between the two components (e.g., Palmeri, 1997), with
46, respectively). However, there was no significant difference         the fastest route governing the response. The third set again
between response errors to low- and high-similarity non-                assumes that both routes operate in parallel and that their
compliant items. Analysis of reaction time data only considered         respective outputs are blended into an overall response (e.g.,
responses to compliant items across each block in the                   Erickson & Kruschke, 1998).
experiment, excluding all trials where an incorrect response               At present the model that is most explicitly defined in all
was made. Responses for high-similarity compliant items were            respects is that of Anderson and Betz (2001); this
found to be significantly faster than responses for low-                computational model fully implements the Exemplar-Based
similarity compliant items (1005 ms vs. 1070 ms, respectively).         Random Walk (EBRW) model of Nosofsky and Palmeri
Average RT also speed-up over consecutive blocks of trials              (1997) and the rule-plus-exception (RULEX) model of
(1221 ms, 1048 ms, 955 ms, 920 ms, respectively).                       Nosofsky, Palmeri, and McKinley (1994) in the ACT-R
Importantly, there was not a significant interaction between            cognitive architecture (Anderson et al., 2004). ACT-R is a
similarity and block, suggesting that the effect of exemplar-           cognitive architecture that consists of multiple modules that are
similarity did not diminish with practice.                              integrated through a central production system to simulate
   Participants did not receive any feedback on the accuracy of         cognition. As a first step, we reimplemented Anderson and
their categorization judgments in Experiment 1. Feedback was            Betz’s original model to run in the most recent version of the
introduced in a follow-up study, which we shall refer to as             ACT-R software (ACT-R 6, Anderson et al., 2004). A general
Experiment 2 (for full details, see Hahn, Prat-Sala, Pothos, &          strength of this modeling approach is that the model of
Brumby, submitted). If participants in this study responded             category learning is embedded within a broader theory of
using the correct key, then the message “CORRECT!”                      human memory and perceptual/motor processing. As a
appeared on the screen. If they responded using the incorrect           consequence, predictions of both categorization judgments and
key, then the message “WRONG!” appeared on the screen and               reaction times are simulated.
a short beep alerted the participant to the mistake. In all other          For the training phase, the category rule and the training
respect the two experiments were identical.                             items were each stored in exemplar (or declarative) memory.
   As expected the inclusion of feedback in Experiment 2                On the subsequent test trials, the model could choose on a trial-
reduced the total number of categorization errors: From a total         by-trial basis whether to use the rule- or the exemplar-route.
of (96x40) 3840, 4.87% were errors. As before, there were               The choice between routes is determined by a route’s utility.
significantly more errors on low-similarity compliant items             Anderson and Betz (2001) define this as a simple trade-off
than on high-similarity compliant items (36 vs. 21,                     function between the probability P that the route would be
respectively). There was no significant difference between              expected to lead to a correct judgment and the expected time C
response errors to low- and high-similarity non-compliant               required to reach that judgment Specifically, the utility U of a
items (60 vs. 70, respectively). Moreover, the decrease in              route is defined as,
overall error rates in the second experiment carried an                                              U i = PiG − C i + ε              (1)
associated time cost: Average RTs for Experiment 2 were
                                                                        where G is a constant that reflects the value of the objective
elevated in comparison to Experiment 1 (1347 ms vs. 1036 ms,
                                                                        (which can be thought of as a maximum time investment to
respectively). Regardless of this increase, the overall pattern for
                                                                        complete the goal). Utility estimates are also stochastic, with
RT data was robust across experiments: Responses for high-
                                                                        the addition of transient noise ε . On each trial, the route with
similarity compliant items were significantly faster than
                                                                        the greatest utility is selected.
                                                                    132

   A route’s utility estimate is updated following usage. The           Exemplar Route
probability P that a route would be expected to lead to a correct       The exemplar-based route implemented Nosofsky and
judgment is defined as follows,                                         Palmeri’s (1997) Exemplar-Based Random Walk (EBRW)
                            successes                                   model. The exemplar route determines category membership
                  P=
                       successes+ failures                      (2)     by recalling declarative memory representations of rule-
where successes is a count representing the frequency of                compliant items leant at training. The latency and probability of
positively rewarded responses attributed to the route and               retrieving an item is determined by its activation (Eq. 3), which
failures is a count of negatively rewarded responses. Initially,        is a summation of the chunks base-level activation, a partial
both rule- and exemplar-route were equally likely to be chosen.         matching score, and a transient noise. (As before, spreading
It is worth noting that because participants in Experiment 1 did        activation did not play a functional role in determining chunk
not receive explicit feedback regarding the accuracy of                 activation.) We unpack each in turn.
categorization judgments, the probability P of a route increased           First, the partial matching score provided a definition for the
at a constant rate after each successive trial that it was selected     degree of similarity between the current test item and training
(i.e., P’ = (successes + 1) / (successes + failures + 1)). In           exemplars in memory. The matching score is a sum computed
contrast, participants in Experiment 2 received feedback for the        over all six dimensions of the object. The match scale P reflects
accuracy of responses; therefore, the probability P of a route          the amount of weight given to a dimension; by default this is a
decreased if an incorrect response was made (i.e., P’ =                 constant across all dimensions. The match similarities Mli
successes / (successes + failures + 1)). This was the only              determine the similarity between the feature in the retrieval
difference between models.                                              specification and the corresponding dimension of exemplars in
   In the model, as in ACT-R generally, declarative knowledge           memory. Matches received a value of 1.0 and mismatches
is represented as chunks. The activation A of chunk i is                received a value of -1.0, so that partial matching scores varied
defined as,                                                             between 6.0 and -6.0. The net result of this is that a training
                  Ai = Bi + ∑ ∑W kj S ji + ∑ PM li + ε                  exemplar becomes more likely to be retrieved, as the similarity
                                                                        between it and the current test item increases.
                             k   j           l                  (3)
                                                                           Second, each time an exemplar is retrieved from memory, it
which represents a summation over the base-level activation of          receives a temporary boost in base-level activation. Over the
the chunk, spreading activation, a partial matching score, and          course of the experiment, exemplars that are frequently
transient noise, respectively. Both the rule route and exemplar
                                                                        retrieved have their declarative memory representation further
route relied on this activation-based account of declarative            strengthened. These increases in base-level activation due to
memory. We next provide a detailed description of each route;           frequency and recency of use mean that an exemplar is more
specifically, how the definition of chunk activation was used to
                                                                        likely to be retrieved in the future, and in less time.
judge whether or not a test item was compliant with the                    Finally, one also needs to define how similarities between a
categorization rule. by a route.
                                                                        test item and training exemplars are translated into a category
Rule Route                                                              decision. Anderson and Betz’s (2001) model, like the EBRW,
                                                                        made use of a random walk procedure. The random walk
The rule-based route implemented Nosofsky, Palmeri, and                 procedure makes repeated attempts to retrieve exemplars from
McKinley’s (1994) rule-plus-exception (RULEX) model. The                memory. The successful retrieval of an exemplar provides
rule route determines category membership through the                   positive evidence that the test item is compliant with the rule. A
retrieval of a declarative memory representation of the                 retrieval failure provides negative evidence to the contrary.
categorization rule, which is then systematically compared to           Whether or not a training exemplar is retrieved is determined
the test item. The latency and probability of the rule’s retrieval      by its activation exceeding a retrieval threshold. A random
is determined by its activation in memory (Eq. 3). The                  walk threshold determines when ‘enough’ evidence is
spreading activation and partial matching components of the             accumulated to make a decision. We explored the effect of
equation did not play a functional role in the rule route; that is,     varying the random walk threshold. The random walk
though implemented, these processes do not contribute to the            threshold affected neither the actual decision nor the relative
routes performance. On a small number of trials rule retrieval          differences in reaction time between high- and low-similarity
will fail, and a random (guessing) response is made. When               items. This is because the definition of chunk activation (Eq. 3)
retrieved, the rule is held in working memory and each of the           is itself already sensitive to similarity. Consequently, the
rule feature values are iteratively compared to the feature             reported model fits are based on a single step threshold, where
values of the current test item. This differs from Anderson and         category decisions are made on the basis of evidence from a
Betz’s (2001) original model, where all features were                   single retrieval: If a training exemplar is retrieved in the context
exhaustively compared. This change was forced by evidence               of a test item, then the model makes a positive response,
that rule-complexity (i.e., the number of rule-features) affects        indicating that the test item is compliant with the categorization
reaction time (Hahn et al., 2002); thus, only rule relevant             rule. Whereas, if none of the training exemplars are retrieved
features were evaluated.                                                (i.e., because the activation values of the training exemplars
                                                                        in declarative memory are less than the retrieval threshold),
                                                                        then a negative response is made, indicating that the test
                                                                        item is not compliant with the categorization rule.
                                                                    133

    Table 1: Comparison between main effects in the human data and those predicted by the model over different parameter values
 for Experiment 1. ‘X’ represents points in the parameter space where effects were consistent between model and data. Highlighted
                                    cells indicate best fitting model parameters (see test for details).
   A) Main effects for error rates.                                   B) Main effects for reaction time.
                                                                        of categorization errors made. The model was rerun over the
                     Model Evaluation                                   experimental procedure, with each model run representing a
The model initially stored the categorization rule in declarative       single participant in the experiment. This meant that the error
memory, and was then presented with various training                    distributions could be fit to the data.
exemplars, which too, were added to memory. Model                          One important question was whether the model could fit
performance was then evaluated on subsequent test items.                both error data and RT data. Both error rates and RT
Following categorization, test items were not added to                  differences are directly related to the core theoretical
memory.                                                                 assumptions of the model, in that, given the nature of the test
   ACT-R makes theoretical commitments about the amount of              items, (systematic) errors only arise through the use of the
time it takes to encoding a stimuli item. It is assumed that            exemplar route, as do the RT differences between high- and
visual encoding entails a number of basic processes, which              low-similarity exemplars. Correcting an excessive number of
were represented as production rules. An initial observation is         total errors means that the exemplar route has to have been
that while ACT-R provided timing estimates for these encoding           used on proportionally fewer trials; however, reducing the
processes, it was apparent that these estimates were massively          relative usage of the exemplar route necessarily reduces any
greater than the RTs found in the empirical data. Specifically,         effect of similarity on RT data. It is clear that these two aspects
in ACT-R the visual encoding of each feature of an item takes           of the data might not be trivial to satisfy. In addition, the
185 ms (representing a 50 ms cognitive cycle to initiate                exemplar route’s retrieval threshold influences the distribution
perception and 135 ms for a shift of visual attention). Given           of errors over high- and low-similarity compliant and non-
that the exemplar route requires that all six features of an item       compliant items. If the retrieval threshold is very low, then a
are encoded, the model predicts that the encoding of an item            match will always be found and the exemplar route will be
should take 1,100 ms. In contrast, the empirical data show that         biased toward “yes” responses; if the retrieval threshold is very
participants RT (which not only included visual encoding, but           high, the exemplar route will be biased toward “no” responses.
also decision and response processes) was approximately 1,000           Only somewhere in between will systematic differences
ms on average. The only way to account for the human RT data            between high- and low-similarity items emerge.
therefore, is to assume that stimuli features are encoded more
rapidly than predicted by ACT-R’s theory of visual attention.           Model Results for Experiment 1
Consequently, we assume a constant time for the visual                  We conducted a systematic exploration of the models
encoding all six of the stimuli features of 555 ms.                     performance across different proportions of rule route usage
   To compare model and experimental results, we simulated a            and varying retrieval thresholds. We factorially combined 5
population of ‘model participants’. This approach was                   possible retrieval threshold values (-0.5, 0, 0.5, 1, & 1.5) with
necessary because the model’s behavior is stochastic. In                increments of .1 in the probability of rule use within the range
particular, the error data could not be fit in any other way            from 0 to 1. For each combination we ran 42 ‘model
because the relevant quantity of interest was the total frequency       participants’. At each of these points in the parameter space,
                                                                   134

model predictions for error and RT data were statistically
evaluated and compared to the human data. Recall that the
main empirical findings from the human data were: (1)
Significantly fewer errors for high-similarity compliant items
than low-similarity compliant items, (2) no effect of similarity
on errors for non-compliant items, (3) significantly faster RTs
for high-similarity compliant items than low-similarity items,
and (4) a significant speed-up in RT over successive trial
blocks. Table 1 summarizes comparisons between these
modeling results and the human data, where “X” entries signal
a match between the corresponding statistical tests.
   Several things are apparent from Table 1. First, RT patterns
are easier to capture than the error patterns, as can be seen in
the greater number of “X” cells in the RT panels of the table
(panel A vs. B). Second, the trade-off between capturing error                   Figure 1. Data and model fits of reaction time across
and RT patterns goes beyond that intuitively described above.                similarity manipulations and trial block for Experiment 1.
Scanning the table, one sees that the number of parameter
combinations that successfully capture the error patterns
increases as one moves right in the table, whereas the opposite
is true for the RT patterns. In other words, the pattern of errors
and RTs place conflicting demands on the retrieval threshold
parameter. This suggests that collecting both error and RT data
provide a far more stringent test of the model, than either kind
of data alone. Finally, and most importantly, there are a number
of cells where all four behavioral criteria are satisfied (i.e., cells
where there is a “X” entry in all four panels). In other words,
there are multiple parameter combinations that reproduce the
key qualitative aspects of the participant data. In order to
discriminate among these different possibilities, we consider
the overall number of error responses made by the model at
various points in the parameter space. This additional factor
heavily constrains the model. We found that an exemplar                       Figure 2. Data and model predictions of reaction time across
retrieval threshold of 0.5 and probability of rule use of 0.9 gave         similarity manipulations and trial block for Experiment 2.
model performance that not only satisfied the main qualitative
                                                                           Model Results for Experiment 2
aspects of the data, but also matched the data quantitatively as
well. This is because a high probability of rule use guarantees            Given the best-fitting parameters for the first data set, we
that the overall error rate remains low, but allows the exemplar           ran the model on a supervised version of the experimental
route to be selected often enough to give an overall effect of             materials where feedback was given about the accuracy of
exemplar-similarity on RT.                                                 each categorization judgment. For each trial a route was
   We provide detailed analysis of the model’s performance                 selected, and if it resulted in a correct judgment, then the
using the best fitting parameter values. The model made fewer              probability that the route was selected on future trials was
errors for high-similarity compliant items than low-similarity             increased. Whereas, if a route resulted in an incorrect
items, t (41) = 3.27, p < 0.005, at a rate comparable to the               judgment, then the probability that it was selected on a
human data (i.e., 54 vs. 97 for the model compared to 41 vs. 61            future trial was reduced (see Eq. 2).
for data). For the non-compliant items the model did not predict              Figure 2 shows the RT predictions for the model
a difference in errors for high- and low-similarity items,                 compared to the human data. It is clear that in contrast to the
t (41) = 1.08, p = 0.29; again these error rates were comparable           human data, the model does not predict the presence of
to the human data (127 vs. 107 for model compared to 92 vs.                instance-similarity effects on RT, F (1, 39) = 3.70, p = 0.06.
107 for data). Figure 1 shows the RT fits for the model                    However, the model does predict a significant similarity x
compared to data. Although the absolute magnitude of speed-                block interaction, F (3, 117) = 3.80, p < 0.05. That is, the
up in RT over block is under-predicted, the model                          model started out predicting a reliable effect of similarity on
demonstrates        a       reliable    effect       of       block,       RT, but only for the first block of trials — for all subsequent
F (3, 123) = 57.61, p < 0.001. Model responses for high-                   blocks there were no predicted differences in RT across
similarity compliant items were significantly faster than                  similarity manipulations.
responses        to     low-similarity       complaint         items,         The model demonstrated a reliable speed-up effect over
F (1, 41) = 42.38, p < 0.001. Moreover, the interaction was                successive blocks of trials, F (3, 123) = 43.97, p < 0.001;
non-significant, F (3, 123) = 2.05, p = 0.11.                              however, as before the absolute magnitude of this speed-up
                                                                           was under predicted. This is particularly pertinent in the
                                                                       135

current data set because the human data shows elevated RTs         tentatively propose that participants in Hahn et al’s
compared to Experiment 1. The model did not predict this           experiment may have encoded rule-irrelevant features
increase in RT, which presumably reflects changes to the           parafoveally at the same time that rule-relevant features
participant’s speed/accuracy trade-off in the context of           were encoded; thus, incurring no additional time cost. Eye-
explicit feedback.                                                 tracking data would be useful to evaluate this proposal.
   Finally, the model’s predictions for the frequency of error        Providing feedback about the accuracy of categorization
rates across different conditions for Experiment 2 was also        judgments (Experiment 2) revealed a critical weakness of
inconsistent with the empirical data. The model predicted more     Anderson and Betz’s (2001) model. The model predicted that
errors than were observed in the empirical data (283 vs. 187).     similarity effects should diminish over time as feedback
Furthermore, the model predicted an effect of similarity for       demonstrates that paying attention to exemplar-similarity is
non-compliant items (92 vs. 70, for high- and low-similarity       harmful to performance. The reasons why the model predicts
conditions), t (39) = 2.04, p < 0.05, but no such effect for       that the effect of instance-similarity diminishes over successive
compliant items (62 vs. 59, for high- and low-similarity           trials is quite clear: Exemplar-similarity effects are brought
conditions), t (39) = 0.26, p = 0.80. These predictions are        about through the use of the exemplar route. However, because
inconsistent with the human data, which found effects of           the exemplar route leads to frequent categorization errors, its
similarity on error rates for compliant but not for non-           utility is strategically lowered, which results in it being chosen
compliant items.                                                   less frequently. In contrast, the rule route, which does not
                                                                   convey any effect of instance-similarity, has its utility
                    General Discussion                             strategically increased following use because its use rarely
The juxtaposition of low error rates and similarity effects        leads to an incorrect judgment. This strategic account of
reported in Hahn et al.’s (2002) study sets an interesting         choosing between routes was not supported by the empirical
benchmark for hybrid theories of categorization (whether           data. In fact, the empirical data suggests that the coupling of
that be accounts assuming independent routes, a parallel           rule application with instance similarity might be mandatory in
competition between routes, or a blending of routes)               the formation of human categorization judgments. We
because sensitivity to exemplar-similarity in this task should     speculate that these data would also be problematic for other
necessarily result in categorization errors. In this paper we      hybrid models of categorization in the literature.
focused on Anderson and Betz’s (2001) hybrid model of
categorization. This model was chosen for evaluation                                      Acknowledgments
because it is currently one of the most explicitly defined         This work was supported by European Commission grant
computational models in the categorization literature, and it      51652 (NEST).
is fully implemented within a general framework of the
human cognitive architecture. The model was constrained                                       References
by theoretical commitments about the cognitive architecture        Allen, S. & Brooks, L. (1991). Specializing the operation of an explicit
and used largely default parameter values. A search of the            rule. Journal of Experimental Psychology: General, 120, 3-19.
model’s parameter space revealed that the fit between the          Anderson, J.R., & Betz, J. (2001). A hybrid model of categorization.
performance of the model and the empirical data was                   Psychonomic Bulletin & Review, 8, 629-647.
improved by increasing the use of an exemplar-based route,         Anderson, J.R., Bothell, D., Byrne, M.D., Douglass, S., Lebiere, C.,
                                                                      and Qin, Y. (2004). An integrated theory of mind. Psychological
relative to a rule-based route. This is because use of the
                                                                      Review, 111, 1036-1060.
exemplar route resulted in faster categorization judgments         Ashby, F.G., Alfonso-Reese, L.A., Turken, U., & Waldron, E.M.
for high-similarity items compared to low-similarity items;           (1998). A neuropsychological theory of multiple systems in
however, its use carried no adaptive value because it                 category learning. Psychological Review, 105, 442-481.
necessarily lead to more errors than simply basing                 Erickson, M.A., & Kruschke, J.K. (1998). Rules and exemplars in
judgments on the categorization rule alone. That Anderson             category learning. Journal of Experimental Psychology: General,
and Betz’s model gave qualitative as well as quantitative fits        127, 107-140.
with the empirical data was somewhat surprising and                Hahn, U., Prat-Sala, M., Pothos, E. & Brumby, D. (submitted). How
demonstrates that the model is robust enough to capture data          Exemplar Similarity Influences Rule Application.
                                                                   Hahn, U and Chater, N. (1998). Similarity and Rules: Distinct?
that seems intuitively outside of its range of behavior.
                                                                      Exhaustive? Empirically Distinguishable? Cognition, 65, 197-203
   However, there were at least two important limitations of       Hahn, U., Prat-Sala, M., & Pothos, E. (2002). How similarity affects
Anderson and Betz’s (2001) model. First, while ACT-R                  the ease of rule application. In Proceedings of the 24th Annual
provided timing estimates for the visual encoding of stimuli          Meeting of the Cognitive Science Society. Mahwah, NJ: Erlbaum.
features, it was apparent that these estimates were much           Nosofsky, R.M., & Palmeri, T.J. (1997). An exemplar-based random
greater than those observed in the human data. Throughout,            walk model of speeded classification. Psychological Review, 101,
we assumed a constant time for the visual encoding of 555             53-79.
ms. We speculate that rule-relevant features are directly          Nosofsky, R.M., Palmeri, T.J., & McKinley, S. (1994). Rule-plus-
encoded based on evidence that reaction time is strongly              exception model of classification learning. Psychological Review,
                                                                      104, 266-300.
associated with rule-complexity (see, Hahn et al., 2002).
                                                                   Palmeri, T.J. (1997). Exemplar similarity and the development of
However, exemplar similarity can only be determined when              automaticity. Journal of Experimental Psychology: Learning,
all of the stimuli features are encoded; therefore, we                Memory, and Cognition, 23, 324-354
                                                               136

