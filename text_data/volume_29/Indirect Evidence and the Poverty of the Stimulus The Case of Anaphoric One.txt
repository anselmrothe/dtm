UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Indirect Evidence and the Poverty of the Stimulus: The Case of Anaphoric One
Permalink
https://escholarship.org/uc/item/6jh9r7d1
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Foraker, Stephani
Reiger, Terry
Khetarpal, Naveen
et al.
Publication Date
2007-01-01
Peer reviewed
  eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                              Indirect Evidence and the Poverty of the Stimulus:
                                                 The Case of Anaphoric One
        Stephani Foraker, Terry Regier, Naveen Khetarpal ({sforaker, regier, khetarpal}@uchicago.edu)
             Department of Psychology, The University of Chicago, 5848 S. University Ave., Chicago, IL 60637 USA
                                Amy Perfors, Joshua B. Tenenbaum ({perfors, jbt}@mit.edu)
  Department of Brain and Cognitive Sciences, MIT, Building 46-4015, 77 Massachusetts Ave., Cambridge, MA 02139 USA
                               Abstract                                     learner is illustrated in (1), below. The antecedent of the
   It is widely held that children’s linguistic input
                                                                            anaphor one is ambiguous in that one could refer to 3
   underdetermines the correct grammar, and that language                   different levels of noun phrase structure, shown in Figure 1:
   learning must therefore be guided by innate linguistic                   (a) the upper N', referring to a yellow bottle, (b) the lower
   constraints. In contrast, a recent counterproposal holds that            N', referring to a bottle of some unspecified color, or (c) N0,
   apparently impoverished input may contain indirect sources               also referring to a bottle of some unspecified color.
   of evidence that allow the child to learn without such
   constraints. Here, we support this latter view by showing that                1.    Here’s a yellow bottle. Do you see another one?
   a Bayesian model can learn a standard “poverty-of-stimulus”
   example, anaphoric one, from realistic input without a                              [NP a [N' yellow [N' [N0 bottle] ] ] ]
   constraint traditionally assumed to be necessary, by relying on
   indirect evidence. Our demonstration does however assume                                           NP
   other linguistic knowledge; thus we reduce the problem of
   learning anaphoric one to that of learning this other
   knowledge. We discuss whether this other knowledge may
                                                                                             det               N'
   itself be acquired without linguistic constraints.
   Keywords: language acquisition; poverty of the stimulus;
   indirect evidence; Bayesian learning; syntax; anaphora.
                                                                                                   adj                    N'
                           Introduction
Language-learning children are somehow able to make                                                                       N0
grammatical generalizations that are apparently unsupported
by the overt evidence in their input. Just how they do this                                  a    yellow                bottle
remains an open question. One influential proposal is that
children succeed in the face of impoverished input because
                                                                                 Figure 1: Structure of the noun phrase a yellow bottle.
they bring innate linguistic constraints to the task. This
argument from poverty of the stimulus is a long-standing
                                                                               What is the right answer, and how can a child acquire that
basis for claims of innate linguistic knowledge (Chomsky,
                                                                            knowledge? It is generally accepted that one can take any
1965). An alternative solution is that the learner instead
                                                                            N' constituent as its antecedent (Radford, 1988). For
relies on indirect evidence (e.g. Landauer & Dumais, 1997;
                                                                            instance, in example (1), one is usually taken to refer to a
Reali & Christiansen, 2005), rather than requiring innate
                                                                            yellow bottle; thus the preferred antecedent here is the upper
linguistic constraints. We pursue this idea here, and focus in
                                                                            N' (but see below for more general treatment).              Lidz,
particular on what can be learned by noting that certain
                                                                            Waxman, and Freedman (2003) showed that 18-month-old
forms systematically fail to appear in the input. In exploring
                                                                            infants, given sentences like (1), looked longer at a yellow
this idea, we assume a rational learner that is sensitive to the
                                                                            bottle than at a bottle of a different color, which they
statistical distribution of linguistic forms in the input.
                                                                            interpreted to mean that the infants knew that one here took
                                                                            [upper N'] as its antecedent. Lidz et al. (2003) also searched
Noun phrase structure and anaphoric one
                                                                            through a child language corpus for input that, in a single
An established example of the argument from poverty of the                  occurrence, could unambiguously rule out incorrect
stimulus concerns the anaphoric use of the word one (Baker,                 hypotheses. They found that such input was effectively
1978; Hornstein & Lightfoot, 1981). The learning challenge                  absent from the corpus. They argued that this poverty of the
is to determine the antecedent of one within a hierarchically
structured noun phrase.1 Concretely, the problem facing the
                                                                            Bayesian model that acquired this general knowledge from child-
                                                                            directed speech, without any prior bias favoring hierarchical
1
  A separate interesting question is how the child comes to know in         structure. We pursue the same general principles here in learning
the first place that language is hierarchically, not just sequentially,     which part of this hierarchical structure is the antecedent of
structured. Perfors, Tenenbaum, and Regier (2006) presented a               anaphoric one.
                                                                        275

stimulus implicated innate syntactic knowledge: children            be a bottle of any color. This is a problem because the lower
know something about language that they couldn’t have               N' situation is consistent with the correct hypothesis [any
learned from the input, so at least part of the knowledge           N'] while the N0 situation is not.
must be innate. In particular, they argued that learning is            Since referential evidence will not suffice to learn the
innately constrained to exclude the [N0] hypothesis from            correct [any N'] hypothesis, what sort of evidence might?
consideration.                                                      We know that one cannot be anaphoric to N0 because, as
   However, Regier and Gahl (2004) showed that a simple             shown in (3), it is ungrammatical for one to be anaphoric
Bayesian model could learn the [upper N'] solution for such         with a complement-taking noun (side) without its
sentences, given only input of the form shown in (1),               complement (of the road, a prepositional phrase in argument
without this constraint. Thus, after learning, their model          slot, Radford, 1998; Lidz & Waxman, 2004).
qualitatively matched the behavior of the children Lidz et al.
had tested. Their demonstration relied on a simple domain-               3.   *I’ll walk by the side of the road and you can walk
general principle: hypotheses gradually lose support if the                   by the one of the river.
evidence they predict consistently fails to appear. Here, if                  [NP the [N' [N0 side] [PP of the road] ] ]
either [N0] or [lower N'] were the correct choice, we would                   [NP the [N' [N0 one] [PP of the river] ] ]
expect to see utterances like (1) sometimes spoken in
contexts in which one referred to a non-yellow bottle. But          Such unacceptable complement structures contrast with
since the correct antecedent for this utterance is the upper        modifiers. In (4), which has a noun with a post-nominal
N', such evidence will not appear, and that absence of              modifier, it is grammatical for one to be anaphoric with the
evidence can drive learning, in a gradual rather than one-          noun (ball) without its modifier (with stripes). In syntactic
shot fashion. Regier and Gahl (2004) argued on this basis           structure this is reflected by the modifier attaching to the
that it is not necessary to posit an innate exclusion of the        lowest N' rather than N0.
[N0] hypothesis, contra Lidz et al. More broadly, they
argued that investigations of the poverty of stimulus should             4.   I want the ball with stripes and you can have the
attend closely to what is absent from the input, as well as                   one with dots.
what is present (Chomsky, 1981:9).                                            [NP the [N' [N0 ball]] [PP with stripes] ]
   In response, Lidz and Waxman (2004) argued that Regier                     [NP the [N' [N0 one]] [PP with dots] ]
and Gahl’s model is inadequate in two important ways.
First, the input it received was not realistic: it was given        Thus, if the language-learning child had grasped the
only determiner-adjective-noun NPs as input, whereas the            distinction between complements and modifiers, that
vast majority of uses of anaphoric one have an antecedent           distinction could serve as a basis for learning about
NP that does not contain an adjective. Second, and more             anaphoric one.
fundamentally, they argue that the model learned the wrong             This idea inverts a standard linguistic test (e.g. Radford,
thing. The model learned to support the [upper N']                  1988: 175), in which the acceptability or unacceptability of
hypothesis only, whereas as stated above, more generally            substituting one in an NP is used to determine whether a
anaphoric one can substitute for any N' constituent. For            given post-nominal phrase within the NP is a complement
instance, as they noted, and as shown here in (2), one can          or a modifier, as in examples (3) and (4). There is an
also substitute for the lower N':                                   apparent circularity in this: we can use the acceptability of
                                                                    substituting anaphoric one to determine whether a phrase is
     2.   Here’s a yellow bottle. Do you see a blue one?            a complement or modifier – but we need the distinction
          [NP a [N' yellow [N' [N0 bottle] ] ] ]                    between complements and modifiers to learn the correct use
                                                                    of anaphoric one in the first place. This circularity is only
   This critique represents a slight shifting of the goalposts,     apparent, however, since complements may be distinguished
since the Lidz et al. (2003) experiment itself, to which            from modifiers on semantic and conceptual grounds, as we
Regier and Gahl responded, suggested that children interpret        discuss below. We assume that the child is able to use
one as anaphoric to the upper N' in the context of (1) – and        semantic/conceptual information to begin distinguishing
the experiment did not speak to the more general [any N']           between complements and modifiers.
hypothesis. Still, since adults do know that the correct               In this paper, we address both of the criticisms that Lidz
answer is [any N'], it is reasonable to require that an             and Waxman (2004) directed at the Regier and Gahl (2004)
adequate model explain how that knowledge is learned.               model. We show that a different Bayesian model can learn
   This requirement is especially troublesome for Regier and        the correct [any N'] hypothesis given realistic input, without
Gahl’s (2004) model, since that model based its                     innately excluding the [N0] hypothesis. In doing so, we
discrimination among hypotheses on referential grounds, by          provide further support for the central claim of Regier and
noting the color of the real-world bottle when sentences like       Gahl (2004): that by relying on indirect negative evidence, a
(1) were uttered. And referentially, nothing distinguishes a        child can learn the knowledge governing anaphoric one,
situation in which the antecedent of one is the lower N' from       without the allegedly necessary innate linguistic constraints.
a situation in which it is N0, since the referenced object can
                                                                276

                                Model                               between complements and modifiers can be captured in
                                                                    semantic or conceptual terms, and thus could in principle be
We assume a rational learner that assesses support for
                                                                    learned without innate specifically syntactic constraints. A
hypotheses on the basis of evidence using Bayes’ rule:
                                                                    complement is necessarily conceptually evoked by its head.
                                                                    For instance, member necessarily evokes the organization of
                    p ( H | e ) ∝ p (e | H ) p ( H )
                                                                    which one is a member; so in member of congress, the
                                                                    phrase of congress is a complement. In contrast, a modifier
  Here H is a hypothesis in a hypothesis space, and e is the        is not necessarily evoked by its head. The word man does
observed evidence. The likelihood p(e|H) is the probability         not necessarily evoke conceptually where the man is from,
of observing evidence e given that hypothesis H is true, and        whether he has long hair, etc.; so in man from Rio, the
the prior probability p(H) is the a priori probability of that      phrase from Rio is a modifier, not a complement (Baker,
hypothesis being true. To flesh this general framework out          1989; Bowen, 2005; Keizer, 2004; Taylor, 1996). While
into a model, we need to specify the sort of evidence that          there are more subtle intermediate cases, this is the
will be encountered, any general assumptions, the                   conceptual core of the distinction.
hypothesis space, the prior, and the likelihood.                       We return in the discussion to the question of just how
                                                                    much of our argument hangs on these assumptions.
Evidence
The model observes a series of noun phrases, drawn from             Hypothesis space
child-directed speech. Each noun phrase is represented              We assumed a hypothesis space containing two hypotheses
without hierarchical structure, as a sequence of part-of-           which addressed the question “Which of the constituents of
speech tags (e.g. the big ball would be coded as “determiner        the NP does anaphoric one take as its antecedent?” The two
adjective noun”), supplemented with a code for a modifier           hypotheses are [any N'], and [N0]. Thus, we chose the
or complement, if any (e.g. side of the road would be coded         simplest possible hypothesis space that includes both the
as “noun complement”). This source of evidence was                  correct answer [any N'] and the hypothesis that Lidz et al.
chosen because (1) children receive a steady supply of such         argue must be innately excluded if learning is to succeed
input, and (2) following our discussion above, it is linguistic     [N0]. If a rational learner can learn the correct answer given
data of this sort, rather than the real-world objects to which      this hypothesis space and realistic input, that outcome will
anaphoric one may refer, that can discriminate among the            indicate that the posited innate exclusion of [N0] is
relevant hypotheses.                                                unnecessary.
                                                                       Each hypothesis takes the form of a grammar that
Assumptions                                                         generates a string of part-of-speech tags corresponding to a
We made three assumptions about the knowledge available             noun phrase. The two grammars are identical except for one
to the language-learning child. First, following Lidz et al.        rule. Each grammar contains the following productions,
(2003), we assumed that the child is able to recognize              with options separated by “|”:
anaphoric uses of one. Second, we assumed that the child
knows a fundamental fact about pronouns generally,                            NP → Pro | Nbar | Det Nbar | Poss Nbar
including one: that a pronoun effectively substitutes for its                 Poss → NP ApostropheS | PossPronoun
antecedent, and must therefore be of the same syntactic type                  Nbar → Poss Nbar | Adj Nbar | Nbar Mod
as the antecedent. Thus, if the pronoun occupies an N'                        Nbar → Nzero | Nzero Comp
position within its noun phrase, the antecedent must                          Det → determiner
similarly occupy an N' position in its noun phrase, for
                                                                              Adj → adjective
otherwise the pronoun would not be able to substitute for
                                                                              PossPronoun → possessive-pronoun
the antecedent. By the same token, if the pronoun occupies
an N0 position, the antecedent should, too. Critically, given                 ApostropheS → apostrophe-s
this assumption, the problem of determining whether the                       Mod → modifier
antecedent of one is N' or N0 reduces to the problem of                       Comp → complement
determining whether one itself, within its own NP, takes the                  Nzero → noun
role of N' or N0. We felt justified in making this assumption                 Pro → pronoun
since the knowledge we assumed concerns pronouns
generally, and could be learned by observing the behavior of        In addition to these productions, the [any N'] hypothesis
pronouns other than one.                                            contains the production:
  Finally, we assumed that the child is able to recognize and
distinguish between complements and modifiers when they                                 Nbar → anaphoric-one,
appear in the child’s linguistic input. To our knowledge,
there are no studies that have tested whether young children        while the N0 hypothesis instead contains the production
are indeed sensitive to this distinction, but we felt justified
in making this assumption since the core distinction                                    Nzero → anaphoric-one.
                                                                277

Thus, the two grammars embody, in their last production,           those noun phrases we selected a random 5%, 10%, and
the link between one and either N' or N0. The grammars             15% cumulative sample for further coding by one of the
were designed to be able to parse noun phrases in a child-         authors (SF). These percentages yielded samples large
language corpus. Each production in each grammar has a             enough for our purposes, yet small enough to code by hand.
production probability associated with it, and these                  We coded each noun phrase as a sequence of part-of-
probabilities may be adjusted to fit the observed corpus.          speech tags, without hierarchical structure, e.g. “determiner
                                                                   adjective noun”, “determiner noun”, “pronoun”, etc., of the
Prior                                                              sort generated by the above grammars. We used these
The two grammars are equally complex: they differ only in          corpus data in two ways. First, we designed both grammars
one production, which is itself equally complex in the two         to accommodate these data. Second, we provided the data as
cases. Since grammar complexity gave no grounds for                input to the model. While both grammars were designed to
assigning either hypothesis greater prior probability, we          be consistent with the data, we were interested in finding
assigned the two hypotheses equal prior probability p(H):          which grammar fit the data most closely.
p(N0) = p(any N') = 0.5. Thus, all discrimination between             We also coded whether a complement or modifier (or
hypotheses was done by the likelihood.                             neither) was present. Thus, for example, the noun phrase a
                                                                   piece of cheese would be coded “determiner noun
Likelihood                                                         complement”, while crackers with cheese would be coded
                                                                   “noun modifier”. We limited ourselves to post-head
Given a hypothesis H in the form of a grammar, and
                                                                   complements and modifiers in the form of prepositional
evidence e in the form of a corpus of noun phrases, we used
                                                                   phrases or clauses (Bowen, 2005; Keizer, 2004; Radford,
the inside-outside algorithm2 to obtain a maximum
                                                                   1988). To identify a complement or a modifier we used the
likelihood fit of the grammar to the corpus. This algorithm
                                                                   conceptual intuition described earlier, identified by several
iteratively reestimates production probabilities in the
                                                                   sources. The head noun that takes a complement
grammar so as to maximize the probability p(e|H) that the
                                                                   presupposes some other entity which must be expressed
corpus e would be generated by the grammar H. Given the
                                                                   (Huddleston & Pullum, 2002:221; Taylor, 1996:39, see also
prior and likelihood, we then obtained the probability of
                                                                   Fillmore’s “inalienably possessed nouns”, 1965) or inferable
each grammar given the corpus, p(H|e), using Bayes’ rule.
                                                                   from context (Bowen, 2005:18, Keizer, 2004). To classify
   Both grammars were designed to be consistent with all
                                                                   post-head strings that followed anaphoric one, such as the
noun phrases in our corpus. What differs between the
                                                                   one in the picture, we identified the head noun of the
grammars is the expected observations given that a
                                                                   antecedent NP from the transcript, and applied the same test
hypothesis is true. To see why this is the case, consider the
                                                                   as for nouns. Note that the head noun is the same regardless
interaction of two rules from the N0 grammar: [Nbar →
                                                                   of whether N' or N0 is the correct hypothesis, so this coding
Nzero Comp], and [Nzero → anaphoric-one]. Together,                does not depend on knowing the correct hypothesis.
these two rules produce strings of the form “one +                    We did not use substitution of anaphoric one as a test for
complement”, as in (3) above. Thus the N0 hypothesis               classifying the post-head forms, to avoid the circularity
predicts that such strings will be encountered in the input.       alluded to above. We restricted ourselves to the conceptual
But since such strings are ungrammatical, that expectation         distinction that could lead a child to the complement-
will not be fulfilled. In contrast, the N' hypothesis does not     modifier distinction without requiring prior syntactic
give rise to this false expectation, since it lacks the second     knowledge of anaphoric one. However, we did find post hoc
rule. This difference between the two grammars is captured         that the anaphoric one test for count nouns4 yielded results
in their likelihoods. If no instances of “one + complement”        consistent with the criteria we adopted.
appear in the input, the N0 grammar will progressively lose
support, and the learner will select the N' grammar as the              Table 1: Frequency counts of post-head structures in the
correct hypothesis.                                                       input for 5%, 10%, and 15% cumulative samples
                              Data                                           Noun phrase forms                  5%     10%   15%
We selected as our input data source the Nina corpus                 noun                                     1253    2478   3752
(Suppes, 1974) in the CHILDES database (MacWhinney,                  pronoun                                  1605    3213   4784
2000), which Lidz et al. (2003) had consulted. The corpus            anaphoric one                               13       32   49
was collected while the child was 23-39 months old, and              noun + complement                           29       47   65
consists of just over 34,300 child-directed mother                   noun + modifier                             66     113   161
utterances, containing approximately 60,000 noun phrases,            noun + complement + modifier                 0        1    1
which we identified preliminarily using a parser.3 From              anaphoric one + modifier                     3        3    4
2
  We used the code made publicly available by Mark Johnson at
http://www.cog.brown.edu/~mj/Software.htm
3
   We used the Stanford Parser, version 1.5.1, available at
                                                                      4
http://nlp.stanford.edu/downloads/lex-parser.shtml.                     Anaphoric one does not substitute for mass nouns.
                                                               278

   Table 1 shows a summary of the types of noun phrase                                      Why does this happen? The N0 hypothesis falsely predicts
forms available in the input, focusing on the post-head                                  that the input will include strings containing “one +
structure (none, complement, modifier). Following a head                                 complement”, while the N' hypothesis does not. Thus, the
noun we found complements (piece of a puzzle, your side of                               likelihood of the actual observed data is higher for the N'
the road) and modifiers (food for the llamas, the ocean                                  hypothesis. As we have seen, the false N0 prediction arises
beach with big waves), while following anaphoric one we                                  from the interaction of two rules in the N0 grammar: [Nbar
found only modifiers (the one with the girl, the other ones                              → Nzero Comp], and [Nzero → anaphoric-one]. The first
you like), consistent with an adult-state grammar. The                                   production is shared with N', while the second is not. The
complements were all prepositional phrases, while the                                    probabilities of both productions must be substantial, since
modifiers consisted of prepositional phrases or subordinate                              the corpus contains complements, which require the first
clauses.                                                                                 rule, and instances of one, which require the second.
   To explore what data are critical to learning anaphoric                               However, if the corpus lacked either one (e.g. the no-ones
one, we also created two variants of the 15% sample: the                                 corpus), or complements (e.g. the no-complements corpus),
first variant (the “no-ones” corpus) was stripped of all NPs                             the corresponding rule would receive 0 probability in a
containing anaphoric one, while the other variant (the “no-                              maximum likelihood fit to the data. In such cases the N0
complements” corpus) was stripped of all NPs containing a                                hypothesis would not make the false “one + complement”
complement. We refer to the 15% sample as “the full                                      prediction, and there would be nothing to distinguish N0
corpus”. The no-ones, no-complements, and full corpora                                   from N'. These expectations were confirmed, as shown in
contained 8763, 8750, and 8816 NPs respectively. Thus,                                   Figure 2.
the manipulations removing instances of one or
                                                                                                                         0.8
complements each eliminated only a very small proportion                                                                                                                N'
(0.6% and 0.8%, respectively) of the “full corpus”.                                                                      0.7
                                                                                                                                                                        N0
                                                                                             p ( hypothesis | corpus )
                                                                                                                         0.6
                                                Methods                                                                  0.5
We first calculated the probability of each hypothesis ([N0],                                                            0.4
[any N']), given the 5, 10, or 15% samples, corresponding to
                                                                                                                         0.3
4.2, 8.4, and 12.6 hours of mother input, respectively. We
then re-calculated the same probabilities on the no-ones and                                                             0.2
no-complements corpora.                                                                                                  0.1
                                                                                                                          0
                                                 Results                                                                       Full corpus      No-ones corpus   No-complements
                                                                                                                                                                     corpus
Figure 1 shows that the probability of the correct [any N']
hypothesis is equal to that of the incorrect [N0] hypothesis                               Figure 2: Probability of hypotheses given the full corpus,
prior to observing any data (0 hours of input). However, as                                 and same corpus stripped of one, or of complements.
more data are seen, the probability of the correct hypothesis
given those data grows steadily higher, at the expense of the                              Thus, the successful learning we see on the full corpus is
incorrect hypothesis. This indicates, contra the poverty-of-                             dependent on the interaction of anaphoric one and
stimulus argument, that a rational learner can discover that                             complements. When both appear in the corpus, even in very
the antecedent of one is N', even though N0 is not innately                              modest quantities as was true here, the N0 hypothesis falsely
excluded from consideration during learning.                                             predicts the unattested “one + complement” pattern, and is
                                                                                         penalized for its absence. This interaction supports learning
                                1                                                        without the innate exclusion of N0.
                               0.9     N'
                               0.8     N0
                                                                                                                                             Discussion
    p ( hypothesis | input )
                               0.7
                                                                                           We have shown that a rational learner can learn the
                               0.6
                                                                                         behavior of anaphoric one without a linguistic constraint
                               0.5
                                                                                         that has been held to be necessary, and necessarily innate.
                               0.4
                                                                                         Our demonstration relies on learning from the absence of
                               0.3
                               0.2
                                                                                         predicted input patterns, a form of indirect evidence that is
                               0.1
                                                                                         broadly consistent with other recent work emphasizing the
                                0
                                                                                         power of indirect evidence in countering standard poverty-
                                        0          4.2            8.4      12.6          of-stimulus arguments (Landauer & Dumais, 1997; Reali &
                                                     Hours of input                      Christiansen, 2005).
                                                                                           We anticipate a number of objections to our
                               Figure 1: Probability of hypotheses given varying         demonstration. First, our hypothesis space is very restricted,
                                          amounts of mother input.                       containing just two hypotheses. One might concede our
                                                                                   279

point that N0 need not be excluded from consideration,                                        References
contra the standard argument, but then counter that we
                                                                     Baker, C. L. (1978). Introduction to generative
ourselves use a very constrained space. Thus, perhaps the
                                                                       transformational syntax. Englewood Cliffs, NJ: Prentice
fundamental “constrained space” idea is correct, even if the
                                                                       Hall.
excluded-N0 proposal was not. We consider it self-evident
                                                                     Baker, C. L. (1989). English Syntax. Cambridge, MA: MIT
that the space must be constrained; the critical question is
                                                                       Press.
whether the constraints are specifically linguistic. Will our
                                                                     Bowen, R. (2005). Noun complementation in English: A
demonstration scale up in a hypothesis space that is
                                                                       corpus-based study of structural types and patterns.
constrained only by non-linguistic general cognitive
                                                                       Gothenburg Studies in English 91. Göteborg, Sweden:
considerations? We consider that an open and interesting
                                                                       Acta Universitatis Gothoburgensis.
question.
                                                                     Chomsky, N. (1965). Aspects of the Theory of Syntax.
   A related possible objection is that we have assumed a
                                                                       Cambridge, MA: MIT Press.
good deal of linguistic knowledge: for instance, knowledge
                                                                     Chomsky, N. (1981). Lectures on government and binding:
that pronouns substitute for their antecedents, that language
                                                                       The Pisa lectures. Berlin: Mouton de Gruyter.
is hierarchically structured, and knowledge of the
                                                                     Fillmore, C. J. (1965). Indirect object constructions in
complement-modifier distinction. This is true. Our primary
                                                                       English and the ordering of transformations. The Hague:
contribution has been to reduce the problem of learning
                                                                       Mouton.
anaphoric one to the problem of learning this other
                                                                     Hornstein, N., & Lightfoot, D. (1981). Introduction. In N.
knowledge. The critical subsequent question is whether this
                                                                       Hornstein, & D. Lightfoot (Eds.), Explanation in
knowledge that we have assumed can itself be learned
                                                                       linguistics. London: Longman.
without innate linguistic constraints (e.g. Perfors et al., 2006
                                                                     Huddleston, R., & Pullum, G. K. (2002). Cambridge
show that the hierarchical structure of language may be
                                                                       Grammar of the English Language. Cambridge:
learned without prior bias). If this knowledge we have
                                                                       Cambridge University Press.
assumed can be so learned, a standard poverty-of-stimulus
                                                                     Keizer, E. (2004). Postnominal PP complements and
example will have been shown to be learnable without
                                                                       modifiers: a cognitive distinction. English Language and
specifically linguistic constraints. If not, the example of
                                                                       Linguistics,8, 323-350.
anaphoric one will retain its status as an argument for innate
                                                                     Landauer, T. K. & Dumais, S. T. (1997). A solution to
linguistic knowledge – but we will have shown that the
                                                                       Plato's problem: the Latent Semantic Analysis theory of
critical linguistic constraints lie elsewhere than traditionally
                                                                       acquisition, induction and representation of knowledge.
imagined.
                                                                       Psychological Review, 104(2), 211-240.
   Perhaps the broadest potential objection is that it may
                                                                     Lidz, J. & Waxman, S. (2004). Reaffirming the poverty of
seem wrong-headed – or paradoxical – to argue against the
                                                                       the stimulus argument: a reply to the replies. Cognition,
nativist poverty-of-stimulus claim while using structured
                                                                       93, 157-165.
linguistic representations of exactly the sort commonly
                                                                     Lidz, J., Waxman, S., & Freedman, J. (2003). What infants
proposed by nativists. We see no problem here. We consider
                                                                       know about syntax but couldn’t have learned: Evidence
ourselves to be working “from the inside out.” We start with
                                                                       for syntactic structure at 18-months. Cognition, 89, B65-
linguistic representations that a nativist should recognize,
                                                                       B73.
and show that domain-general principles support learning of
                                                                     MacWhinney, B. (2000). The CHILDES Project: Tools for
the nominally correct grammar, contra specific
                                                                       Analyzing Talk (3rd ed.). Mahwah, NJ: Erlbaum.
unlearnability claims in the literature. This allows us to
                                                                     Perfors, A., Tenenbaum, J., & Regier, T. (2006). Poverty of
engage the poverty-of-stimulus argument in its own
                                                                       the stimulus? A rational approach. Proceedings of the
representational terms, while working “outwards” to
                                                                       28th Annual Conference of the Cognitive Science Society
domain-generality. In contrast, connectionist studies that
                                                                       (pp. 663-668). Mahwah, NJ: Erlbaum.
also question the poverty of the stimulus (e.g. Reali &
                                                                     Radford, A. (1988). Transformational Grammar: A First
Christiansen, 2005) work “from the outside in.” They start
                                                                       Course. New York: Cambridge University Press.
with domain-general representations, and learn linguistic
                                                                     Reali, F. & Christiansen, M. (2005). Uncovering the
behavior similar to that of a grammar. The two approaches
                                                                       richness of the stimulus: Structure dependence and
complement each other: the starting-point for connectionist
                                                                       indirect statistical evidence. Cognitive Science 29, 1007-
studies is undeniably domain-general, while in our case that
                                                                       1028.
which is learned is undeniably a grammar.
                                                                     Regier, T., & Gahl, S. (2004). Learning the unlearnable: The
                                                                       role of missing evidence. Cognition, 93, 147-155.
                     Acknowledgments                                 Suppes, P. (1974). The semantics of children’s language.
We thank Susanne Gahl and four anonymous reviewers for                 American Psychologist, 29, 103-114.
helpful comments. This research was partially supported by           Taylor, J. R. (1996). Possessives in English: An exploration
NIH fellowship HD049247 to SF.                                         in cognitive grammar. Oxford, UK: Oxford University
                                                                       Press.
                                                                 280

