UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Assessing Student Self-Explanations in an Intelligent Tutoring System
Permalink
https://escholarship.org/uc/item/6745g0c3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Rus, Vasile
McCarthy, Philip M.
Graesser, Arthur C.
et al.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                     University of California

               Assessing Student Self-Explanations in an Intelligent Tutoring System
                                                   Vasile Rus (vrus@memphis.edu)
                                                       Department of Computer Science
                                                       Institute for Intelligent Systems
                                                         The University of Memphis
                                                             Memphis. TN 38152
                        Philip M. McCarthy                                    Arthur C. Graesser (a-graesser@memphis.edu)
                (pmmccrth@memphis.edu)                                                       Department of Psychology
                      Department of Psychology                                             Institute for Intelligent Systems
                   Institute for Intelligent Systems                                         The University of Memphis
                     The University of Memphis                                                   Memphis. TN 38152
                         Memphis. TN 38152
                                                                                             Danielle S. McNamara
       Mihai C. Lintean (M.Lintean@memphis.edu)                                 (d.mcnamara@mail.psyc.memphis.edu)
                  Department of Computer Science                                             Department of Psychology
                   Institute for Intelligent Systems                                       Institute for Intelligent Systems
                     The University of Memphis                                               The University of Memphis
                         Memphis. TN 38152                                                       Memphis. TN 38152
                                                                         generated from a lexico-syntactic computational tool called
                                Abstract                                 The Entailer (Rus et al., 2005; Rus, McCarthy, & Graesser,
                                                                         2006).
     Research indicates that guided feedback facilitates                    Our corpus of natural language input generated from an
     learning, whether in the classroom or with Intelligent              ITS is student contributions from users of iSTART
     Tutoring Systems (ITS). Improving the accuracy of the
                                                                         (Interactive Strategy Training for Active Reading and
     evaluation of user input is therefore necessary for
     providing optimal feedback. This study investigated an              Thinking; McNamara, Levinstein, & Boonthum 2004), a
     automated assessment of students’ input that involved a             web based tutoring system that provides students with self-
     lexico-syntactic (entailment) approach to textual analysis          explanation and reading strategy training. The iSTART
     along with a variety of other textual assessment                    student statements were sampled from the final phase of
     measures. The corpus consisted of 357 student responses             iSTART training. During this stage, a pedagogical agent
     taken from a recent experiment with iSTART, an ITS                  reads sentences from a textbook aloud and asks the student
     that provides students with self-explanation and reading            to type a self-explanation of each sentence. The focus of this
     strategy training. The results of our study indicated that          study is to distinguish two very similar student self-
     the entailment approach provided the highest single
                                                                         explanation categories: Topic identification sentences and
     measure of accuracy for assessing input when compared
     to the other measures in the study. A set of indices                Paraphrases. This distinction is challenging because the
     working in conjunction with the entailment approach                 lexicon used for both topic identification and paraphrase
     provided the best overall assessments.                              tends to largely overlap with the iSTART target sentences.
                                                                         Thus, for the iSTART agent to provide the most appropriate
  Keywords: entailment; intelligent tutoring systems; iSTART;
                                                                         feedback to the student, accurate algorithms are required to
  paraphrase; latent semantic analysis.
                                                                         successfully interpret the student’s input and make this
                                                                         distinction. This study tests various measures for evaluating
                            Introduction                                 student input and formulates an algorithm from a
A major challenge for Intelligent Tutoring Systems (ITSs)                combination of successful indices. The algorithm accurately
that incorporate natural language interaction is to accurately           assesses the student input, distinguishing topic sentence type
evaluate users’ contributions and to produce accurate                    self explanations from paraphrase-type self explanation.
feedback. Available research in the learning sciences                    Thus, once implemented, iSTART agents will be able to
indicates that guided feedback and explanation is more                   provide more informative feedback to students.
effective than simply providing an indication of rightness or
wrongness of student input (Aleven & Koedinger, 2002;                    Interactive Strategy Training for Active Reading
Anderson et al., 1989; Kluger & DeLisi, 1996; McKendree,                 and Thinking (iSTART)
1990; Sims-Knight & Upchurch, 2001). And the benefits of                 iSTART provides young adolescent to college-aged students
feedback specifically in ITS are equally evident (Azevedo &              with tutored self-explanation and reading strategy training
Bernard, 1995). This study addresses the challenge of                    via pedagogical agents (McNamara et al., 2004). iSTART is
evaluating users’ textual input in ITS environments. More                designed to improve students ability to self-explain by
specifically, we assess entailment evaluations that are                  teaching them to use reading strategies such as
                                                                    623

comprehension monitoring, bridging, and paraphrasing.              In the iSTART context, the Text (T) corresponds to the
Following introduction and practice phases of the iSTART           textbook sentence and the Hypothesis (H) to the Self-
training, the final practice phase has students use reading        Explanation. Our approach to measuring entailment begins
strategies by typing self-explanations of sentences from           with mapping the T (textbook sentence) and H (student self
science texts. For example, the following sentence, called         explanation) into a graph representation (Rus et al., 2005).
Text (T), is from a science textbook and the student input,        Words are mapped onto vertices (V) and syntactic relations
called self-explanation (SE), is reproduced from a recent          among words are mapped onto edges (E) in the graph.
iSTART experiment. The SE samples in this study are all                 The mapping process has three phases: preprocessing,
reproduced as typed by the student.                                dependency graph generation, and final graph generation.
                                                                   In the preprocessing phase, we (a) strip the punctuation
     T: The largest and most visible organelle in a                from words (tokenization), (b) map morphological
          eukaryotic cell is the nucleus.                          variations of words to their base or root form
     SE: the nucleusis the center of the cell it contains the      (lemmatization), (c) assign part-of-speech labels to each
          ribsome and more.                                        word (tagging), and (d) identify the inter-relationship of
                                                                   major phrases within the texts (parsing). The second phase
Computational Approaches to Text Assessment                        is the actual mapping from text to the graph representation.
Providing appropriate feedback to students concerning self-        This mapping is based on information from parse trees
explanations requires an accurate evaluation of both the           generated during the parsing process. A parse tree groups
meaning and quality of the self-explanation. In order to           words in a sentence into phrases and organizes phrases into
assess the best measures available, we assessed seven              hierarchical tree structures from which we can detect
approaches to self explanation evaluation. The algorithms          syntactic dependencies among concepts. We use Charniak’s
differ in terms of whether they are word-based, incorporate        (Charniak 2000) parser to obtain such parse trees and head
syntactic information, or use a combination of both word           detection rules (Magerman, 1994) to obtain the head of each
and syntactic information.                                         phrase. A dependency tree is generated by linking the head
                                                                   of each phrase to its modifiers. In the third phase, the
(1) Latent Semantic Analysis (LSA) The ability of LSA              dependency tree is transformed into a dependency graph by
(Landauer et al., 2007) to evaluate similarities between texts     generating remote dependencies such as the dependency
is based on particular statistical analyses of word-by-text        between speak and person in the sentence I saw the person I
and word-by-word co-occurrence matrices. Essentially, LSA          spoke to.
bases semantics on the premise that a word’s meaning is               Once graph representations have been obtained, a graph
related to the kind of words with which it tends to co-occur.      matching operation is initialized. This operation evaluates
Thus, chair is closer in meaning to table, sit, and easy than      the degree of similarity between graphs. Several variations
chair is to horse, because chair-table, chair-sit, and easy-       of graph matching exist, but the subsumption model best fits
chair co-occurs in texts more often than does chair-horse.         our task. Graph subsumption consists of finding a mapping
LSA has an excellent record of success in text comparison          from the vertices (V) in SE to the vertices in T such that
analyses (Landauer et al., 2007), but three major problems         edges (E) among the same two vertices in SE hold among
with LSA reduce its ability to accurately assess short text of     mapped vertices in T. The subsumption algorithm for
the sort commonly encountered in ITS dialogue. First, LSA          textual entailment has three major steps: (1) find an
does not encode word order (syntax). Second, LSA ignores           isomorphism between the set of vertices of the Hypothesis
negation. And third, longer sentence pairs tend to be judged       graph (VH) and the Text graph (VT); (2) check whether the
by the LSA as more similar because longer texts increase           labeled edges in H, EH, have correspondences in ET; and (3)
the likelihood of word similarity between word pairs               compute the subsumption score. Step 1 uses a word-
(McCarthy et al., 2007).                                           matching method and a thesaurus (Miller, 1995) to find all
                                                                   possible synonyms for words in T (Rus et al., 2005). Words
(2) The Entailment Index The Entailment Index, generated           in H have different priorities: head words are most
from The Entailer (Rus et al., 2005; Rus et al., 2006), is         important followed by modifiers. Step 2 takes each relation
applied in this study based on previous success in assessing       in H and checks its presence in T. In Step 2, we also use
similarity between short dialogue exchanges in natural             relation equivalences among appositions, possessives and
language environments (McCarthy et al., 2007). The                 linking verbs. Lastly, a normalized score for vertex and edge
Entailment Index is relatively impervious to the three major       mapping is computed. The score for the entire entailment is
challenges of LSA. Because The Entailment Index is a               the weighted sum of each individual vertex and edge
relatively new metric, we describe its calculation in some         matching score. The evaluation is structured so as to
detail (see Figure 1 for The Entailer’s process flow).             generate a value that ranges from 0 to 1, with 1 meaning
   Measuring entailment requires assessing whether the             TRUE entailment and 0 meaning FALSE entailment.
meaning of one text, referred to as the Hypothesis, or simply      However, one final stage of the evaluation is then
H, can be logically inferred from another text, referred to as     implemented to account for negation: If only one of the text
the Text, or simply T (Dagan, Glickman, & Magnini, 2005).          fragments (i.e., H or T) is negated, the entailment decision is
                                                               624

reversed; however, if an even number of negations occur          (entailment approach). This approach is asymmetrical; the
(e.g., both T and H are negated) the decision is retained        values are different if we switch T and SE.
(double-negation). For example, the Text Yahoo bought
Overture does not entail the Hypothesis Yahoo did not buy        (4) Word-Overlap Approach-2 This second word overlap
Overture because even though the Text subsumes the               approach differs from the first in that a cosine value is
Hypothesis, the presence of negation reverses that decision.     derived from vectors formed from word co-occurrences.
(For an extensive review of the components of the Entailer       This approach is symmetrical, indicating the degree of
and the evaluation formula, see Rus et al., 2005).               similarity between two sentences.
                                                                 (5) Lemma-Overlap Approach The lemma overlap
         TEXT                         HYPOTHESIS
                                                                 approach is calculated in the same way as the second word
                                                                 overlap approach, except that lemma co-occurrence rather
                                                                 than simple word co-occurrence is evaluated. For example,
    MAPPING                                                      the lemma index evaluates table/tables and run/ran as the
                                                                 same, whereas the word indices view such pairs as different.
                        Preprocessing:                           Like the second overlap approach, this index is symmetrical,
            (tokenization, lemmatization, parsing)               providing a similarity measure between the two sentences.
                                                                  (6) Synonymy This metric simply adds synonymy to the
                Dependency Graph Generation                      first word overlap method. The synonymy and word-based
                                                                 approaches are equivalent to The Entailer’s lexico-syntactic
                                                                 approach for cases when only the lexical component is used
                    Final Graph Generation                       and the syntactic component is ignored. This is an
                                                                 entailment approach.
                                                                  (7) Syntactic The syntactic approach is equivalent to The
     MATCHING                                                    Entailer’s lexico-syntactic approach when only the syntactic
                                                                 component is used and the lexical component is ignored.
                                                                 The evaluation of the latter allows us to understand the
                       Vertex Matching                           degree to which syntax alone can contribute to entailment.
                                                                 For instance, this component will check whether a direct
                                                                 object relation presented in Hypothesis is also present in the
                                                                 Text.
                    Dependency Matching
                                                                 The Corpus
                      Negation Handling                          For our corpus, we selected a set of 357 iSTART derived
                                                                 Text/SE pairs taken from a recent iSTART experiment. The
                                                                 experiment included 90 high-school students drawn from
                                                                 four 9th grade Biology classes (all taught by the same
                                                                 teacher). The T/SE pairs were assigned by two experts in
                            Scoring                              discourse processing to one of two groups: Topic
                                                                 identification (TopicID, n = 96) and Paraphrase (n = 261).
                                                                 The major difference between the two main categories was
                                                                 that the TopicID responses tended to include what the
                 Decision (TRUE or FALSE)                        sentence was about. Thus, sentences often began with
                                                                 frozen expressions such as “The sentence talks about …”.
                                                                 Paraphrase responses, on the other hand, were restatements
          Figure 1: Processing flow in our approach.             of the Text, incorporating different words and syntax while
                                                                 lacking any kind of frozen expressions. The Paraphrase
 (3) Word-Overlap Approach-1 The first word based                group in this study was further subdivided into three sub-
approach in this study incorporates a simple lexical overlap     category paraphrase types: Paraphrase Inaccurate (P-
method: tokenize, lemmatize (using wnstemm algorithm in          Inaccurate, n = 210); Paraphrase accurate but Close (P-
WordNet library), and compute the degree of lexical overlap      Close; n = 16); and Paraphrase accurate and Distant (P-
between the Text and SE. We normalize results by dividing        Distant, n = 35). P-Inaccurate sentences were defined as a
the lexical overlap by the total number of words in the SE.      failed paraphrase. For example, a participant may have used
The normalization factor makes the difference in this            similar words to the target sentence but created a sentence
approach. The values indicate how much the SE is                 with a different meaning.
subsumed by the T and not the other way around
                                                             625

                       Table 1: An iSTART text together with participant examples of all four response types.
Text                            Sometimes a dark spot can be seen inside the nucleus.
TopicID                         yes i know that can be a dartkn spot on .think aboyt what thje sentence
Paraphrase-Inaccurate           in dark spots you can see inside the nucleus and the cell
Paraphrase-Close                if you ever notice that a dark spot can be seen inside the nucleus sometime
Paraphrase-Distant              the nucleus have a dark spot that sometimes be seen.its located in the inside of the nucleus.
P-Close sentences were defined as highly similar to the             against the test set. The effect of category for each of the
original sentence in terms of sentence structure and/or             predictor variables (see Table 2) indicates that The
content words. P-Distant sentences were defined as highly           Entailment Index was the best predictor of topic
similar to the original sentence in terms of semantics but          identification type self-explanations, F(1.228) = 25.051, p <
different in terms of structure and/or content words. (For an       .001. The weakest predictor was LSA, F(1.228) = 2.975, p =
extensive review of the classifications, see Best, Ozuru, &         .086. The value of the discriminant analysis generated
McNamara, 2004; for examples of TopicID and Paraphrase              function was significant (X2 = 31.18, df = 4, p < .001).
categories, see Table 1 above)
                                                                         Table 2: Effect of category for each predictor variable
Predictions
We predicted that The Entailer Index would result in a more                                     Means
accurate distinction of the two self-explanation user input                            TopicID       Paraphrase              F
types, mainly because of the syntactic and negation handling
                                                                       Entailer       0.60 (0.27)    0.44 (0.20)        25.05**
components. However, because the P-Close sub-category
demonstrates very similar lexicon and syntax to the TopicID            Synonymy 0.00 (0.02)          0.03 (0.05)         *8.64*
category (as the name suggests), we predicted weaker                   Word           0.52 (0.25)    0.44 (0.25)         *4.10*
results for this distinction. Similarly, the P-Inaccurate              LSA            0.67 (0.35)    0.59 (0.33)         *2.98*
subcategory provides self-explanations that typically contain          Note: df = 1,228, SD in parentheses, ** p< .001; * p< .01
lexical items least like the Text and, consequently, least like
TopicID sentences. Thus, we predicted the strongest                   The Fisher’s Function Coefficients (see Table 3)
distinction for this sub-category.                                  demonstrates the direction of the indices used in this
                                                                    analysis. The Entailment Index coefficient values are higher
                              Results                               for the TopicID function than for the Paraphrase function.
To distinguish the two SE groups, we conducted a                    This suggests that self explanations that are subsumed by
discriminant analysis, using the TopicID/Paraphrase                 the Text are more likely to be viewed as identifying the
categories as the dependent variable. To assess which of the        topic rather than a paraphrase of the Text. In contrast, the
available independent variables (i.e., the Entailment Index,        synonymy index is higher for the Paraphrase category. The
LSA, and the five alternative approaches outlined above)            synonymy values suggest that synonymous terms are more
best predicted group membership, the 357 item data set was          common to paraphrased responses than to those which
randomly divided into a training set (67%) and a test set           identify the topic when other variables have been taken into
(33%). We conducted an analysis of variance (ANOVA) on              consideration. The word overlap index is also higher for the
the training set data to eliminate any of the seven indices         Paraphrase category. This result suggests that paraphrased
that failed to discriminate between the two groups at p >           self explanations share more lexical units with their
.100. The ANOVA resulted in the lemma index being                   corresponding Text than do TopicID sentences when other
dropped from the analysis; LSA was retained although its            variables in the analysis have been taken into consideration.
discrimination value was significant only in a 1-tailed test.       The LSA coefficients are very similar for both categories.
The ANOVA showed that six variables distinguished the               This weak distinction suggests that LSA is not a strong
two sentence-type groups; however, because a discriminant           discriminator of the categories once other predictors have
analysis is sensitive to collinearity, we followed similar          been taken into consideration.
previous studies (e.g., McCarthy et al. 2006) and rejected            The accuracy of the discriminant function can best be
any variables with a correlation at r>= .70, retaining the          judged by assessing its generated predictions of category
variables with the larger univariate F-value. This process          membership against the test set data. The distinction
reduced the indices in the analysis to four: The Entailment         between the two groups was significant (X2 = 17.27, df = 1,
Index, Synonymy Index, Word Overlap (2), and LSA.                   p < .001); however, the accuracy of the predictions was
  A discriminant analysis was conducted on the training set         higher for the Paraphrase category: TopicID category (recall
and the accuracy of the generated predictions was assessed          = .692; precision = .409); Paraphrase category (recall =
                                                                626

.743; precision = .904). Although the significant results are     Paraphrase (TopicID: M = .504, SD = .344; Paraphrase: M =
encouraging, the low precision score for the TopicID              .599, SD = .322).
category required further analysis.                                 Thus, our final question was why this reversal might have
                                                                  occurred. We hypothesized that this reversal might be
                                                                  explained by the text length confound affecting the LSA
                                                                  index. (As described in the introduction, this confound
        Table 3: Fisher’s unstandardized coefficients for         posits that longer texts tend to generate higher LSA values).
       topic identification (TopicID) and paraphrase              Thus, the higher LSA means may largely have been caused
                                                                  by longer sentences. And indeed, assessing the lengths of
                            TopicID          Paraphrase           the SEs in the Paraphrase Close sub-category, we can report
Entailer                                                          that the incorrectly classified SEs were typically longer than
                             9.478               6.009
                                                                  the correctly classified SEs (TopicID: M = 17.143, SD =
Synonymy                     5.741              16.519            8.375; Paraphrase: M = 23.778, SD = 7.412). Although the
Word Overlap                 1.255               1.657            difference in length was not significant: (F(1,14) = 2.820, P
LSA                          2.850               2.951            = .115), the effect size (η = .168) indicates that the
Constant                     -4.816             -3.403            difference in sentence length is substantial enough to have
                                                                  affected the analysis. Thus, we conclude that the main cause
                                                                  for the misclassification leading to the low precision results
Post Hoc Analysis                                                 may be attributable to the LSA text length confound.
Precision values are calculated as hits/hits + false alarms.
Thus, the low precision value for the TopicID category was                                    Discussion
caused by a great many false alarms. More simply put,             In this study, we assessed the capacity of a variety of
many Paraphrase type SEs were classified as TopicID. The          computational indices to distinguish two highly similar text
question however, was which of the three Paraphrase sub-          types: Topic identification sentences and Paraphrases. The
categories was most responsible for this problem. To answer       purpose of the study was to offer approaches to improving
this question, we assessed the generated predictions against      assessment algorithms of user inputs for Intelligent Tutoring
each of the three Paraphrase sub-categories and found that        Systems such as iSTART. When incorporated into such
the sub-category of Paraphrase Close produced the lowest          systems, the algorithms can be used to provide more
prediction accuracy (Close = 56.35%; Distant = 63.86%;            accurate and more appropriate feedback to users. Accurate
Inaccurate = 74.29%). These results were in line with our         and appropriate feedback facilitates learning and is therefore
predictions, indicating that the more inaccurate a paraphrase     critical to ITS operating within natural language dialogue.
is judged to be, the better it can be distinguished from the        The results of our study suggested that The Entailment
topic identification category.                                    Index in conjunction with the synonymy index, word-
   Our next question was which of the indices in the analysis     overlap, and LSA, significantly distinguished the two self
(if any) was contributing to the inaccuracy of evaluating the     explanation categories under analysis. However, despite this
categories. An individual analysis of the contribution of The     significant result, approximately 44% of one sub-category’s
Entailer Index and the LSA index revealed that the Entailer       sentence pairs (Paraphrases Close) were misclassified. At
index tended to generate higher values for the TopicID            least part of the cause of this misclassification appears to be
group (M = .611, SD = .266) than for the Paraphrase               the text length confound that affects the LSA index.
category as a whole (M = .421, SD = 187), the means of the          The contribution of The Entailment Index to the
Paraphrase Close sub-category value resembling the                discriminant algorithm was larger than any other index.
Paraphrase category as a whole (M = .482, SD = .234). This        Such a result is encouraging for the Entailer. Thus, future
direction towards higher values for TopicID was also found        development for The Entailer will focus on improving
for LSA values: TopicID group (M = .679, SD = .334);              accuracy still further. One approach to this improvement is
Paraphrase group as a whole (M = .591, SD = .334). And            through changing the way syntactic information is gathered
once more, the means of the Paraphrase Close sub-category         for students’ self-explanations. For our experiments in this
value resembled the Paraphrase category as a whole (M =           paper, the syntactic information was gathered from syntactic
.557, SD = .324). However, this trend of higher values for        parsers trained on English sentences written by professional
the TopicID category was reversed for LSA evaluations of          journalists. However, student self explanations are much
misclassified items in the Paraphrase Close sub-category          less grammatically correct than journalists’ articles,
(i.e., the sub-category that was least accurate in the            typically containing textual chunks that are syntactically
analysis). Specifically, considering only the misclassified       uncommon. When a parser trained on edited sentences is
SEs, the Entailer Index values followed the general trend of      applied to less correct sentences the retrieved syntactic
higher values for TopicID (TopicID: M = .702, SD = .124;          information is not entirely reliable. This observation leads
Paraphrase = M = .311, SD = 127). However, the LSA                us to believe that we can employ a partial parser that detects
values showed the opposite trend, with higher values for          major phrases (Noun Phrases, Verb Phrases) in a sentence
                                                                  without grouping these phrases into a full S (sentence)
                                                              627

structure. The parser will be able to give us syntactic              reading skill. In Y. B. Kafai, W. A. Sandoval, N. Enyedy,
structures for the correct chunks and we can augment this            A. S. Nixon, & F. Herrera (Eds.), proceedings of the Sixth
information with an approach using a set of heuristics to be         International Conference of the Learning Sciences:
applied to the most frequent less common structures used by          Embracing diversity in the learning sciences (pp. 89-96).
students. For instance, in the SE given in the introduction          Mahwah, NJ: Erlbaum.
the nucleus the center of the cell it contains the ribsome and     Charniak, E. (2000). A maximum-entropy-inspired parser.
more there is a noun phrase, the nucleus, followed by                In Proceedings of North American Chapter of Association
another noun phrase, the center of the cell, followed by a           for Computational Linguistics (NAACL-2000).
pronoun, it. Such a sequence is grammatically unlikely and         Dagan, I., Glickman, O., & Magnini, B. (2005). The
would confuse a full parser. A partial parser, however,              PASCAL Recognising Textual Entailment Challenge. In
would be able to detect the noun phrases, the nucleus and            Proceedings of the Recognizing Textual Entailment
the center of the cell, along with the pronoun, it, but it           Challenge Workshop.
would not try to group them together into a sentence               Kluger, A. N., & DeLisi, A. (1996). The effects of feedback
structure. Thus, we plan to enhance the partial parser with          interventions on performance: A historical review, a
heuristics that would create the missing syntactic relations.        metaanalysis, and a preliminary feedback intervention
For instance, two consecutive articulated noun phrases               theory. Psychological Bulletin, 119, 254-284.
would be separated by a linking verb. Similarly, a noun            Graesser, A.C., Chipman, P., Haynes, B.C., and Olney, A.
phrase followed by a pronoun followed by a verb would                (2005). AutoTutor: An intelligent tutoring system with
indicate that the pronoun starts a new sentence. We predict          mixed-initiative dialogue. IEEE Transactions in
that these two heuristics would substantially increase the           Education 48, 612-618.
likelihood of solving the incorrect structures of typical          Landauer, T., McNamara, D. S., Dennis, S., and Kintsch, W.
student input.                                                       Eds. (2006). The Handbook of LSA. Mahwah, NJ:
   The application of findings in cognitive science to               Erlbaum.
Intelligent Tutoring Systems is largely dependent upon the         Magerman, D. (1994). Natural Language Parsing as
accuracy of the algorithms that assess the systems’ input.           Statistical Pattern Recognition. Ph.D. Dissertation,
This study demonstrates a new and successful approach to             Stanford University.
assessing such input. Future research will focus on                McCarthy, P.M, Lewis, G.A., Dufty, D.F., & McNamara,
developing this accuracy still further so as to offer systems        D.S. (2006). Analyzing writing styles with Coh-Metrix. In
such as iSTART the algorithms necessary to provide                   Proceedings of the Florida Artificial Intelligence
                                                                     Research Society International Conference (FLAIRS),
learners with the most appropriate feedback. Accurate
                                                                     Melbourne, Florida.
feedback is a critical aspect of learning and the development
                                                                   McCarthy, P.M., Rus, V., Crossley, S.A., Bigham, S.C.,
of the Entailment Index may play a critical role in
                                                                     Graesser, A.C., & McNamara, D.S. (In Press). Assessing
enhancing that accuracy.
                                                                     the Entailer with a Corpus of Natural Language From an
                                                                     Intelligent Tutoring System. Submitted to FLAIRS, 2007.
                     Acknowledgements                              McKendree, J. (1990). Effective feedback content for
This research was supported by the Institute for Education           tutoring complex skills. Human-Computer Interaction, 5,
Sciences (IES; R305G020018, R305G040046) and partially               381-413.
by the National Science Foundation (NSF; REC-0241144).             McNamara, D.; Levinstein, I. B.; & Boonthum, C. (2004).
Any       opinions,    findings,     and     conclusions    or       iSTART: Interactive strategy trainer for active reading
recommendations expressed in this material are those of the          and thinking. Behavioral Research Methods, Instruments,
authors and do not necessarily reflect the views of the IES          and Computers 36:222–233.
or NSF.                                                            Miller, G. (1995). WordNet: a lexical database for English.
                                                                     Communications of the ACM 38.39–41.
                           References                              Rus, V., Graesser, A.C., McCarthy, P. &Lin, K. (2005). A
Aleven, V., & Koedinger, K. R. (2002). An Effective Meta-            lexico-syntactic approach to textual entailment. In
   cognitive Strategy: Learning by Doing and Explaining              Proceedings of International Conference on Tools with
   with a Computer-Based Cognitive Tutor. Cognitive                  Artificial Intelligence.
   Science, 26, 147-179.                                           Rus, V., McCarthy, P.M., & Graesser, A.C. (2006).
Anderson, J. R., Corbett, A. T., Koedinger, K. R., &                 Analysis of a Textual Entailer. In Proceedings of the
   Pelletier, R. (1989). Cognitive tutors: Lessons learned.          International Conference on Intelligent Text Processing
   The Journal of the Learning Sciences, 4, 167-207.                 and Computational Linguistics (CICLing-06), February,
Azevedo, R., & Bernard, R. M. (1995). A meta-analysis of             Mexico City, Mexico. Lecture Notes in Computer
   the effects of feedback in computer-based instruction.            Science, Vol. 3878, Springer.
   Journal of Educational Computing Research, 13. 111-             Sims-Knight, J.E., Upchurch, R.L. (2001). What's Wrong
   127.                                                              with Giving Students Feedback? Proceedings of the 2001
Best, R., Ozuru, Y., & McNamara, D. S. (2004). Self-                 ASEE Annual Conference, Albuquerque, NM.
   explaining science texts: Strategies, knowledge, and
                                                               628

