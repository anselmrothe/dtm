UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Agents and Affordances: Listeners Look for What They Don't Hear
Permalink
https://escholarship.org/uc/item/7c67r0p6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Fausey, Caitlin M.
Matlock, Teenie
Richardson, Daniel C.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                Agents and Affordances: Listeners Look for What They Don’t Hear
                                       Caitlin M. Fausey (cmfausey@psych.stanford.edu)
                                             Department of Psychology, Stanford University
                                                         Stanford, CA 94305 USA
                                            Teenie Matlock (tmatlock@ucmerced.edu)
                                      Cognitive Science Program, University of California, Merced
                                                          Merced, CA 95344 USA
                                               Daniel C. Richardson (dcr@ucsc.edu)
                                    Department of Psychology, University of California, Santa Cruz
                                                        Santa Cruz, CA 95064 USA
                               Abstract                                 language comprehension and judgment tasks. For example,
                                                                        when discriminating sensible from nonsensical sentences,
   How do implicit aspects of language guide overt perceptual           participants answered fastest when the location of the
   behavior? In this eyetracking study, we examined whether             response was consistent with the movement described by
   different ways of describing objects and actions would
                                                                        the sensible sentence, as in pressing a button close to the
   influence the visual processing of objects with affordances.
   Specifically, we were interested in the effect of different
                                                                        body after reading “open the drawer” (Glenberg &
   information about the agent of an action. English-speaking           Kaschak, 2002). In a part-judgment task, participants were
   adults viewed objects with interactive regions, such as              faster to verify parts toward the upper half of objects when
   handles, knobs or buttons. Participants viewed each object           they made responses requiring upward movement, and
   after listening to a sentence with or without information about      lower parts with downward movement (Borghi, Glenberg &
   an agent. Participants were faster to fixate the interactive         Kaschak, 2004).
   region of objects after hearing non-agentive language than             Evidence for tight links between semantic and motor
   after hearing agentive language, as if they were searching to        representations of object affordances has been found when
   fill an “agent information gap”. These results may inform
                                                                        movements themselves are the dependent measure. Creem
   theories about how global knowledge and local linguistic
   information mutually determine visual inspection of objects.
                                                                        and Proffitt (2001) found different grasping behavior when
                                                                        participants either did or did not concurrently perform a
   Keywords: Affordances; Language-mediated eye movements               semantic task while grasping. Without an additional task (or
                                                                        with an unrelated spatial task), participants grasped objects
                           Introduction                                 such as combs, spatulas and paintbrushes by their handles.
   Much of our everyday understanding of physical objects               When completing a concurrent semantic task, this normal
is grounded in affordances. This includes tacit knowledge               grasping behavior was disrupted. This effect suggests that
about how objects are canonically oriented, what they are               normal object-directed movement relies on semantic
used for and, critically, how we interact with them. We                 knowledge about object affordances.
know, for instance, that pitchers have handles for pouring,               Even when no overt response is required of experimental
cars have steering wheels for driving and guns have triggers            participants, representations of object affordances may still
for shooting. The current study examines object affordances             be active. One source of evidence in support of this claim is
at the interface of language and visual processing. Do                  the finding that neural circuits that are activated during
different linguistic environments change how people                     grasping are also activated when people simply view
visually inspect objects that afford human action?                      manipulable objects (Chao & Martin, 2000). Additional
Specifically, how might language that differentially codes              evidence that suggests an automatic activation of knowledge
for agency guide attention to interactive regions of these              about object affordances comes from eyetracking studies.
objects?                                                                  Affordances and eyetracking. Eyetracking provides one
   The notion that visual, motor and linguistic                         measure of how people integrate background knowledge,
representations are tightly linked has received empirical               language and visual information in real time. Researchers
support in recent years (e.g., Barsalou, 1999; Glenberg,                have studied the interaction of eye movements and linguistic
1997; Pecher & Zwaan, 2005). For example, Tucker and                    processing in various ways: Many studies have examined
Ellis (1998) found that people were faster to judge whether             the contribution of eye movements to resolving ambiguities
a cup was right side up or upside down when the cup handle              in sentence understanding (e.g., Tanenhaus, Spivey-
was on the same side of the screen as the hand with which               Knowlton, Eberhard & Sedivy, 1995) while others have
they made their response than when the handle was on the                reversed the question and examined the influence of
opposite side of the response hand. Glenberg and colleagues             language itself on visual processing (e.g., Richardson &
have observed similar “action compatibility effects” in                 Matlock, 2007).
                                                                    245

   In general, eye movement data suggest that listeners are              One intriguing finding of this nature was recently reported
sensitive to the semantics of verbs (and co-occurrences with          by Crosby, Monin and Richardson (2006). In a novel
nouns) when integrating linguistic, visual and motor                  eyetracking study of social referencing, participants watched
information. For example, in a study by Kamide, Altmann               a video of four people discussing affirmative action in
and Haywood (2003), participants made more anticipatory               university admissions. Three discussants were white and
eye movements to an image of butter after hearing the verb            one was black. Crosby et al. found that when a white
spread than after hearing the verb slide. Using a task that           discussant strongly opposed affirmative action, listeners
directed participants to move objects, Chambers, Magnuson             looked toward the black discussant. Crosby et al. suggested
and Tanenhaus (2004) found that changing the non-                     that people look toward the potentially-offended in
linguistic context – whether objects were picked up by hand           potentially offensive situations to use their reaction as
or by hook – influenced sentence comprehension. Sentences             information about how to decide if discrimination had
that had been ambiguous in the by-hand context became                 occurred and as cues for how to behave.
unambiguous in the by-hook context because only one                      Even subtly different linguistic input conveys different
object could be picked up using a hook. Participants’ eye             information. In the case of agentive and non-agentive
movements reflected the lessened ambiguity in the hook                minimal sentence pairs, listeners receive information about
context as soon as 150 ms after mention of the potential              the agent only from agentive sentences. Much of the
referent. Thus, knowledge about the affordances of “picking           previous eyetracking literature suggests that eye movements
up” was integrated with visual and linguistic information in          to visual scenes closely follow information in the linguistic
this task.                                                            input. On this account (and/or in situations in which this
   Where people look for information in the visual                    mechanism is most likely to be operative), agentive
environment appears to be sensitive to knowledge of object            language might direct listeners’ attention to regions in the
affordances and local linguistic context: Listeners anticipate        visual world that are associated with agents. That is, after
the location of expected referents and look to that region. In        hearing agentive language such as “He tipped the tea kettle”
the paradigms reviewed above, different lexical items or              listeners may look toward the interactive region of the
different physical objects suggested different affordances            object (i.e., the handle). On the other hand, a different
that then influenced visual inspection patterns. How                  mechanism of information search may operate when people
sensitive is inspection to even more subtle contextual                attempt to elaborate their understanding of events. People
information?                                                          may, in fact, search the visual world specifically for
   One interesting contrast in event descriptions may be              information that was not provided by language. In this case,
illustrated by the following sentences: He tipped the tea             non-agentive language may prompt people to fill an
kettle. versus The tea kettle tipped. In a context containing         information gap by looking toward regions in which they
visual input of a tea kettle and either of these linguistic           expect to find agents. That is, after hearing “The tea kettle
inputs, the surface form of the verb is constant as is the            tipped”, people may look toward the handle in order to learn
potential object affordance. However, in one description              about a potential agent. After briefly reviewing research
listeners receive information about an agent while in the             examining some representational and processing
other description no such information is provided. The                consequences of agentive and non-agentive sentences, we
presence versus absence of linguistic information about an            introduce our study that aimed to examine whether, and
agent may change people’s inspection of the parts of objects          how, these linguistic frames influence how people visually
that are especially associated with agents (e.g., handles,            inspect objects with interactive regions.
steering wheels, triggers). Borghi et al. (2004) suggested               Agentive vs. non-agentive language. What are the
that, indeed, people are sensitive to the parts of objects that       consequences of processing agentive and non-agentive
afford agentive action. How does this knowledge interact              language? Few psychological studies have addressed this
with particular frames of language to produce real-time               question. One exception is Mauner and Koenig (2000), who
visual search behavior?                                               compared the accessibility of agents in passive sentences
   Information search: A new use of eyetracking. In addition          such as The baby’s rattle was shaken repeatedly, and active
to studies of incremental sentence processing, eyetracking is         intransitive sentences such as The baby’s rattle had shaken
a useful methodology to explore visual search in a broader            repeatedly. In English, passive sentences may be extended
sense. People explore the visual world for all sorts of               with agentive information (e.g., by her mother) while
reasons as they attempt to integrate information from                 intransitive sentences may not. In a series of sentence
multiple sources. In everyday experience, many things are             processing experiments, Mauner and Koenig found that
often left unsaid. Sometimes this may occur because two               participants were quicker to detect contradictions between
interlocutors share common ground or a common visual                  clauses that implied agents and passive sentences than these
context and do not need to reiterate shared information,              same agentive clauses and intransitive sentences. These
while other times there is a genuine information gap at a             results suggest that agents are less accessible after English
particular time point during an interaction. In this latter case,     intransitive sentences than after other types of sentences
visual search can sometimes lead to knowledge that fills this         more strongly associated with the explicit linguistic
information gap.                                                      encoding of agents.
                                                                  246

   Non-agentive language not only influences the salience of          hearing “He” may lead people to look toward interactive
agents during language comprehension, but also influences             regions of objects. It is also possible, however, that people
learning and reasoning about agents and objects. For                  will search for information that is not presented in the
example, Fausey and Boroditsky (2007) found that people               linguistic input. In particular, part of people’s knowledge
were sensitive to the distribution of agentive and non-               about the objects in our studies may be that agents typically
agentive language that co-varied with observed events when            cause the events in which the objects participate (e.g., People
learning about novel agents and objects. With increasing              tip tea kettles). Upon hearing a non-agentive sentence such as
non-agentive language, people judged an agent to be less              The tea kettle tipped, participants may search for information
criminal and an object to be more capable of spontaneously            about who tipped the tea kettle. That is, not hearing “He” may
transforming. In a separate series of studies that examined           lead people to look toward interactive regions of objects.
people’s attributions of blame and financial penalties to
                                                                         How does the presence or absence of agent information in
causal agents, Fausey and Boroditsky (in preparation) found
                                                                      linguistic input influence subsequent visual search of objects
that people were more forgiving of agents of accidental
events after reading descriptions that included non-agentive          with action affordances?
language (e.g., The tablecloth ignited) than after reading
descriptions that included agentive language (e.g., She ignited                   Present study: Inspected objects
the tablecloth). Agentive and non-agentive English sentences             When an object captures visual attention, where do viewers
appear to influence a variety of reasoning behaviors. Does the        look first? Local contextual factors, as well as knowledge
reduced salience of agents after non-agentive language                grounded in prior experience, indubitably determine the
influence visual search of objects that afford agent action?          answer to this question. In this eyetracking study, English-
   Present study. Previous research in the embodiment                 speaking adults viewed objects with interactive regions, such
tradition suggests that the actions associated with particular        as handles, knobs or buttons. Participants viewed each object
objects partially constitute those object representations and         after listening to a sentence with or without information about
that this knowledge is active during a variety of tasks. In the       an agent. Participants’ eye movements may inform questions
present study, we examined how local linguistic information           about how global knowledge of object affordances and local
interacts with global affordances knowledge to produce visual         linguistic information mutually determine visual inspection of
search behavior.                                                      objects.
   Linguistics research in this domain has generally focused
on the incremental processing of active voice, transitive             Participants
sentences. This type of language use is consistent with the               Forty-five English-speaking students at the University of
affordance knowledge that people have about objects: Agents           California, Santa Cruz, completed the study in partial
are explicitly mentioned (or are implied addressees in the case       fulfillment of a course requirement. All had normal or
of imperatives) in the linguistic input and objects that afford       corrected-to-normal vision. We were unable to achieve a
human action appear in the visual input. What happens in the          useful track on 16 participants due to equipment vagaries or
case of a mismatch - when people hear non-agentive language           vision correction (hard contacts or certain types of glasses).
in combination with objects with agentive affordances – may
inform broader questions about how people integrate                   Materials
information from a variety of sources.                                    Visual stimuli were 24 color photographs of objects
   We build on previous research that suggests that people are        against a white background, for instance, a tea kettle, toilet or
sensitive to the part of objects that afford action (e.g., Borghi     rifle. Every object had an “interactive region”: a clearly
et al., 2004) by examining people’s eye movements toward              identifiable part that would be the site of any manual
“interactive regions” of objects (e.g., handles, steering wheels      interaction, such as a handle, a button or a trigger. All images
and triggers) following agentive versus non-agentive                  were 500 x 500 pixels, and interactive regions were
language. In each trial of our listen-and-look study, a sentence      determined with respect to each object. On average, the
was presented auditorily, and then a static image of an object        interactive region occupied 22 percent of the full image.
appeared on the screen. Eye movements were recorded for               Images subtended approximately 20° visual angle. See
three seconds following the appearance of the image, and no           Figure 1 for an example visual stimulus.
overt responses were required of participants. Different visual           Linguistic stimuli included 48 English sentences in the
inspection biases following different language may be                 past tense. Two sentences were paired with each object: (1)
revealed by this paradigm, extending both the eye movement            one agentive sentence and (2) one non-agentive sentence.
and affordances literatures by providing evidence for how             Agentive sentences were transitive sentences with the
linguistic and more global knowledge interact during human            pronoun he (e.g., He tipped the tea kettle). Non-agentive
information integration in service of event understanding.            sentences were intransitive sentences with no pronouns
   Two potential outcomes may reveal sensitivity to object            (e.g., The tea kettle tipped).
affordances in visual search. If language simply directs
attention in this paradigm, participants should look toward the
interactive region of objects more quickly following agentive
language than following non-agentive language. That is,
                                                                  247

                                                                          Procedure
                                                                             Participants were instructed to listen to sentences and to
                                                                          look at pictures on a screen. They were asked to pay attention
                                                                          to all stimuli, and told that they would not need to make any
                                                                          responses.
                                                                             Participants completed the experiment in the Eye Think
                                                                          lab’s (D.C.R.) speech and gaze tracking system. Each
                                                                          participant sat in a reclining chair, looking up at an arm-
                                                                          mounted 19” LCD screen approximately 24" away. A
                                                                          Bobax3000 remote eye tracker, consisting of a camera
                                                                          focused on the participant’s eye and a set of LED
                                                                          illuminators, was mounted at the base of the display. Each
                                                                          participant wore a headset, through which s/he listened to
           Figure 1: Visual stimulus used in the study.                   stimuli.
                                                                             Intel iMacs were used to present stimuli and to record data.
   All sentences were judged to be semantically acceptable                The eye trackers passed image data to the iMacs, which
(greater than a rating of 5.5 on a scale in which 1 was                   calculated gaze position for each participant approximately 30
“definitely not English and unacceptable” and 7 was                       times a second and recorded regions of interest that were
“definitely English and acceptable”) by 17 English speakers               being fixated. Data were also streamed to an experimenter’s
who did not participate in the eyetracking study. Sentences               computer, which saved an audio-video record of what
were recorded by the same female native English speaker                   participants saw, heard and said during the experiment,
and presented aloud to participants. Table 1 lists all                    superimposed with their gaze position.
linguistic stimuli.1                                                      Results
                    Table 1: Linguistic stimuli.                             Participants viewed object images for the final three
                                                                          seconds of each trial (3000ms to 6000ms), and eye
     Object       Verb             Object        Verb                     movements were recorded for this time period.
     bell         rang             balloon       lowered                     Participants spent similar total amounts of time looking at
     jug          poured           sword         swung                    the interactive region of objects following non-agentive (M =
     pitcher      emptied          ketchup       squirted                 867 ms) and agentive language (M = 894 ms), F(1, 28) = .20,
     rifle        fired            shirt         unzipped                 p = .66.2 However, participants were approximately 100
     gun          shot             tea kettle    tipped                   milliseconds faster to fixate the interactive region of objects
     faucet       shut off         hose          turned on                after hearing non-agentive language (M = 3435 ms, where the
     piano        played           car           started                  onset of the picture is at 3000ms) than after hearing agentive
     maracas      shook            plane         flew                     language (M = 3543 ms). This pattern was reliable across
     horn         blew             tractor       started up               participants, F(1, 28) = 4.72, p = .038, as well as across items,
     truck        drove            blinds        shut                     F(1, 23) = 6.25, p = .02 (see Figure 2).
     toilet       flushed          perfume       sprayed
     racket       dropped          TV            turned off
Design
   In the course of a passive listen-and-look task,
participants were presented with 24 sentence-object pairs.
Half the sentences were agentive and half were non-
agentive, with agentivity assignment counterbalanced across
participants. During each trial, participants first listened to a
sentence while viewing a blank screen. Then, the object
described in the sentence appeared. It remained onscreen for
three seconds, during which time eye movements were
recorded. Sentence-object pairs were presented in a random
order, intermixed among other linguistic and visual stimuli.
                                                                                                 Non-agentive   Agentive
                                                                                                   language     language
1
  Table 1 includes verbs only. Participants heard full sentences that               Figure 2: Onset of interactive region fixation
included each verb in an agentive frame or a non-agentive frame.
2
  This pattern was observed in each of the six consecutive 500ms
windows comprising the viewing period.
                                                                      248

                         Discussion                                to simulate events than do agentive sentences. In the
                                                                   absence of both a linguistic agent and a visual agent, people
   Participants viewed objects with affordances for human
                                                                   may need to simulate the event as if they were the agent in
action after listening to simple sentences. People’s visual
                                                                   order to understand the event. This simulation might lead to
inspection of these objects was influenced by information in
                                                                   increased speed or amounts of looking to the interactive
the sentence: Participants looked toward the interactive
                                                                   regions of objects as people imagine interacting with the
region of the object more quickly after sentences that did
                                                                   object. Non-agentive sentences in our paradigm may have
not mention an agent (e.g., The tea kettle tipped) than after
                                                                   been most efficiently processed using a simulation
sentences that did mention an agent (e.g., He tipped the tea
                                                                   mechanism, but note that in our follow-up study,
kettle).
                                                                   information per se seemed to guide people’s inspection of
   At first blush, this finding may seem surprising. Most
                                                                   the interactive regions of objects.
previous studies using incremental processing paradigms
                                                                      Agentive and non-agentive sentences differ in several
have shown that listeners look at what is talked about. One
                                                                   ways, all of which may contribute to how people visually
might expect, then, that hearing “he” in agentive sentences
                                                                   inspect objects that afford action. Not only do agentive and
would direct listeners’ eye movements to the regions of
                                                                   non-agentive sentences contain different amounts of
objects associated with agents. However, in our anticipatory
                                                                   information about agents, but they also seem to convey
paradigm, the lack of agentive information in the linguistic
                                                                   different information about motion and end-states. In a
input directed listeners to quickly fixate the interactive
                                                                   paper-and-pencil study, 300 UC Merced students were
regions of objects. One intriguing explanation for our results
                                                                   presented with an agentive or a non-agentive sentence from
may be that people expect agents to do things like tip tea
                                                                   our eyetracking studies and were asked to draw a picture of
kettles, flush toilets and fire guns. When no information
                                                                   what came to mind. As expected, participants drew agents
about agents is provided by language, listeners may attempt
                                                                   more often when depicting agentive sentences than when
to “fill” this information gap by quickly fixating to the
                                                                   depicting non-agentive sentences, χ2(1) = 133.02, p < .001.
interactive region of objects.
   Some evidence in support of this explanation comes from         Additionally, a naïve coder rated the degree of motion (e.g.,
a subsequent eyetracking study in which we examined                motion lines) in each drawing, using a four-point scale from
people’s visual inspection of objects after hearing two            “none” to “high”. Analyses of these ratings revealed that
different kinds of agentive sentences. This study was              participants drew more motion in non-agentive depictions
identical to the present study, with two exceptions: (1) At        (M = 2.13) than in agentive depictions (M = 1.76), t(298) =
the beginning of the experiment, participants viewed a             2.53, p = .012. Finally, though most participants depicted
photograph and listened to short biographical statements           events at the midpoint of a “beginning-middle-end”
about three men (Bill, Dave and Tom). For example,                 timeline, participants were more likely to draw beginning
participants were introduced to Bill, hearing “This is Bill.       states when depicting agentive sentences and more likely to
He’s 23 and likes history”, and (2) During the listen-and-         draw end-states when depicting non-agentive sentences,
look procedure, participants heard 12 sentences starting with      χ2(2) = 21.50, p < .001.
an agent name (four sentences per name), such as “Bill                Related research on mental imagery and language
tipped the tea kettle”, and 12 sentences starting with an          suggests that people naturally direct their attention to
agent pronoun, such as “He tipped the tea kettle”. Thus,           imagined changes in location without any visual stimulus
participants heard agentive sentences in both conditions but       (e.g., Richardson & Spivey, 2000; Spivey & Geng, 2001)
received less information about the agent from the pronoun         and that they imagine movement and scan along paths when
sentences than from the name sentences. With this more             processing sentences that include fictive motion, such as
subtle manipulation of the amount of linguistic information        The road goes through the valley (Matlock, 2004;
about agents, the type of sentence did not influence how           Richardson & Matlock, 2007). Our drawing results suggest
quickly people looked toward the interactive regions of            that people may imagine different scenes after agentive and
objects. It did, however, influence whether or not people          non-agentive sentences, and this different imagery may
looked at these regions at all: People looked at the               impact subsequent visual search.
interactive regions of objects more after hearing the pronoun         A number of future directions will help to unravel the
than after hearing the specific agent name, t(24) = 2.08, p =      many mechanisms that contribute to the integration of
.049. Though both “Bill tipped the tea kettle” and “He             global affordance knowledge and local linguistic
tipped the tea kettle” are plausible sentences that explicitly     information in the visual inspection of objects. Because the
mention an agent, people looked to interactive regions of          current study was designed to specifically examine
objects more often after receiving the less informative “he”,      interactive regions of objects, not all visual stimuli had
as if searching for more information about the agent.              clearly identifiable “end-state” regions. To better understand
   In addition to information search, simulation mechanisms        how people integrate agentive and non-agentive linguistic
may play a role in how people integrate visual and linguistic      frames with visual scenes, we are currently extending our
input. Non-agentive sentences may create stronger pressure         paradigm by presenting people with images that depict
                                                                   agents (e.g., a person), objects (e.g., a tea kettle) and end-
                                                                   states (e.g., a puddle of water). Explicit visual depictions of
                                                               249

all aspects of the event may help to more precisely               Creem, S.H., & Proffitt, D.R. (2001). Grasping objects by
understand how agentive and non-agentive language are               their handles: A necessary interaction between cognition
integrated with knowledge of object affordances during              and action. Journal of Experimental Psychology: Human
visual processing. Future research may also consider                Perception and Performance, 27, 218-228.
additional linguistic manipulations, such as a no-language        Crosby, J.R., Monin, B., & Richardson, D.C. (2006).
baseline, as well as contrasting active vs. passive                 Looked at, but not listened to: Focusing on the reaction of
constructions.                                                      minority group members when deciding if discrimination
   One particularly interesting future direction with respect       has occurred. Paper presented at the 7th Annual Meeting of
to linguistic manipulations would be to examine the                 the Society for Personality and Social Psychology.
integration of language, visual processing and knowledge          Fausey, C.M., & Boroditsky, L. (in preparation). Speaking
about object affordances in speakers of languages other than        of accidental agents.
English. Because there is cross-linguistic variation in the       Fausey, C.M., & Boroditsky, L. (2007). Language changes
distribution of agentive and non-agentive expressions in a          causal attributions about agents and objects. Proceedings
language (see Fausey & Boroditsky, 2006), people in                 of the 29th Annual Meeting of the Cognitive Science
different language communities may vary in their need to            Society. Mahwah, NJ: Erlbaum.
fill agent information gaps. For example, if a community of       Fausey, C.M., & Boroditsky, L. (2006). Linguistic
speakers commonly talk about tea kettles tipping, toilets           contributions to reasoning about causal agents.
flushing and guns firing, without mentioning agents,                Proceedings of the 28th Annual Meeting of the Cognitive
perhaps non-agentive language would not so strongly bias            Science Society. Mahwah, NJ: Erlbaum
looking toward interactive regions of objects. Using this         Glenberg, A.M. (1997). What memory is for. Behavioral
paradigm to examine visual inspection patterns of people in         and Brain Sciences, 20, 1-55.
different linguistic communities may help us to better            Glenberg, A.M., & Kaschak, M.P. (2002). Grounding
understand how language is integrated with other                    language in action. Psychonomic Bulletin & Review, 9,
knowledge to constrain processing of objects and events.            558-565.
                                                                  Kamide, Y., Altmann, G. T. M., & Haywood, S. L. (2003).
                        Conclusion                                  The time-course of prediction in incremental sentence
                                                                    processing: Evidence from anticipatory eye movements.
   When visually inspecting everyday objects, participants
were faster to fixate the interactive region of these objects       Journal of Memory and Language, 49, 133-156.
after hearing non-agentive language than after hearing            Mauner, G., & Koenig, J-P. (2000). Linguistic vs.
agentive language. This preliminary research is suggestive          conceptual sources of implicit agents in sentence
of an influence of linguistic framing on visual information         comprehension. Journal of Memory and Language, 43,
search. Future research will continue to elaborate                  110-134.
mechanisms by which people integrate their rich knowledge         Matlock, T. (2004). Fictive motion as cognitive simulation.
of agents, everyday objects and language as they visually           Memory & Cognition, 32, 1389-1400.
explore their world.                                              Pecher, D., & Zwaan, R.A. (2005). Grounding cognition:
                                                                    The role of perception and action in memory, language,
                   Acknowledgments                                  and thinking. Cambridge: Cambridge University Press.
                                                                  Richardson, D.C., & Spivey, M.J. (2000). Representation,
We thank Michael Spivey and Alexia Toskos for helpful
                                                                    space and Hollywood Squares: Looking at things that
discussions of this work. We thank Jason Hreha for help in
preparing the stimuli, Arezou Ghane for help with data              aren’t there anymore. Cognition, 76, 269-295.
collection, and Vijay Vanchinathan for coding drawings.           Richardson, D.C., & Matlock, T. (2007). The integration of
                                                                    figurative language and static depictions: An eye
                                                                    movement study of fictive motion. Cognition, 102(1),
                        References
                                                                    129-138.
Barsalou, L.W. (1999). Perceptual symbol systems.                 Spivey, M., & Geng, J. (2001). Oculomotor mechanisms
   Behavioral and Brain Sciences, 22, 577-609.                      activated by imagery and memory: Eye movements to
Borghi, A. M., Glenberg, A. M., & Kaschak, M. P. (2004).            absent objects. Psychological Research, 65, 235-241.
    Putting words in perspective. Memory & Cognition, 32,         Tanenhaus, M.K., Spivey-Knowlton, M.J., Eberhard, K.M.,
   863-873.                                                         & Sedivy, J.E. (1995). Integration of visual and
Chambers, C. G., Magnuson, J.S., & Tanenhaus, M. K.                 linguistic information in spoken language comprehension.
   (2004). Actions and affordances in syntactic ambiguity           Science, 268, 1632-1634.
   resolution. Journal of Experimental Psychology:                Tucker, M., & Ellis, R. (1998). On the relations between
   Learning, Memory & Cognition, 30, 687-696.                       seen objects and components of potential actions. Journal
Chao, L. L., & Martin, A. (2000). Representation of                 of Experimental Psychology: Human Perception and
   manipulable man-made objects in the dorsal                       Performance, 24, 830–846.
   stream. Neuroimage, 12, 478–484.
                                                              250

