UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Categorization and Reinforcement Learning: State Identification in Reinforcement Learning
and Network Reinforcement Learning
Permalink
https://escholarship.org/uc/item/5mv347zb
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Veksler, Vladislav D.
Gray, Wayne D.
Schoelles, Micheal J.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                 Categorization and Reinforcement Learning:
               State Identification in Reinforcement Learning and Network Reinforcement Learning
          Vladislav D. Veksler                                Wayne D. Gray                           Michael J. Schoelles
              (vekslv@rpi.edu)                                (grayw@rpi.edu)                           (schoem@rpi.edu)
                                             Cognitive Science Department, 110 8th Street
                                                           Troy, NY 12180 USA
                            Abstract                                     Exploratory and learning mechanisms vary from one
   We present Network Reinforcement Learning (NRL) as
                                                                         version of RL to another, but the basic idea remains.
   more efficient and robust than traditional reinforcement
   learning in complex environments. Combined with
   Configural Memory (Pearce, 1994), NRL can generalize
   from its experiences to novel stimuli, and learn how to deal
   with anomalies as well. We show how configural memory
   with NRL accounts for human and monkey data on a
   classic categorization paradigm. Finally, we argue for why
   the suggested mechanism is better than other reinforcement
   learning and categorization models for cognitive agents and
   AI.
   Keywords: categorization, reinforcement learning,                           Figure 1. Reinforcement Learning. Left: Flow of
   category learning, unsupervised learning, cognitive                      information in RL agents (no learning shown). Right:
   modeling, cognitive architectures, artificial intelligence,             Identified state, S, and possible actions, A1 through An;
   configural.                                                               each arrow represents a competing state-action pair;
                                                                         arrow labels U1 through Un represent the utilities of state-
Introduction                                                                            action pairs S–A1 through S–An.
A red line is not just red, nor is it just a line, nor is it just a
red line. It is all of these things at the same time. Given a               The major problem with reinforcement learning is state
red line, an agent may want to select red-appropriate                    recognition. Instance-based state identification, where
actions, line-appropriate actions, or red-line appropriate               each unique set of input activations is considered a
actions. Identifying the object as a red-line may be                     different state, is largely inefficient due to the exploding
inefficient, and identifying it as red may be misleading.                number of state-action pairs with each new input channel.
   In this paper we identify problems with state                         Instance-based RL could require 2n states for n binary
identification in Reinforcement Learning and suggest a                   inputs, and a·2n state-action pairs, where a is the number
mechanism that addresses these problems, Configural                      of actions.
Memory with Network Reinforcement Learning                                  To decrease the decision space, researchers use various
(CMNRL). We will argue that CMNRL is more efficient                      forms of categorization to preprocess large numbers of
and robust than either instance-based or category-based                  input channels into more manageable numbers of states
reinforcement learning. We will then describe a classic                  (for review see Sutton & Barto, 1998). However, creating
categorization task (Shepard, Hovland, & Jenkins, 1961),                 too many categories still results in too large of a
and suggest the psychological validity of CMNRL by                       decision/learning space. As the number of categories
simulating human and monkey data from this task.                         increases, the category-based agent becomes more similar
                                                                         to an instance-based one. Alternatively, bunching up
Problems with Reinforcement Learning                                     objects into a small number of categories may
It has become popular to use some form of reinforcement                  misrepresent the environment altogether.
learning in computational cognitive agents in order to get                  Consider the following simplified scenario. Imagine a
computational agents to act in a psychologically and                     world where all apples taste great except brown apples
biologically plausible manner (e.g. Fu & Anderson, 2006;                 (brown-apple is the exception to the apple category).
Holroyd & Coles, 2002; Sutton & Barto, 1998).                            Instance-based and category-based RL agents are
Reinforcement learning (RL) is a procedural component                    presented with two red, one green, and two brown apples
of a cognitive agent that can be described as follows: the               each (Table 1). Let us assume that the category-based
agent must identify its current state, S, and then execute               agent in this scenario contains the 'apple' category. At the
some action, A, such that the state-action pair, S–A, has                end of the five stimulus-action-reinforcement cycles
the highest utility of all state-action pairs for state S                presented in Table 1, the category-based agent would
(Figure 1). The utility of the selected state-action pair,               know only one thing – that eating apples has a positive
U(S–A), is updated based on environmental feedback in                    utility (+1), and thus would continue eating brown apples.
the form of a reinforcement signal (e.g. pleasure/pain).                 At the end of the same five cycles, the instance-based
                                                                         agent would know the utility of eating red apples (+2),
                                                                    689

green apples (+1), and brown apples (-2), but would know             all brown things taste bad. The only solution from the
nothing about eating apples in general. Presented with a             model's perspective is to propagate the reinforcement
yellow apple, the instance-based agent will not know                 value to all active states, and hope that the utility of the
what to do with it, and may throw it away. In this world,            apple–eat state is already high enough that the negative
instance-based agents will never know what to do with                reinforcement does not significantly hurt it (unfortunately,
newly encountered apples, whereas category-based ones                if the 'bad apple' phenomena happens early enough,
will never learn to avoid rotten apples. A more efficient            people do get stuck with food aversions).
agent should be able to learn about both categories and
instances.                                                           Configural Memory with Network
                                                                     Reinforcement Learning
         Table 1. Instance-based and Category-based                  Using simultaneous activation of multiple states and
      Reinforcement Learning. Two columns on the right               simultaneous learning of multiple state-action pair utilities
      display the updated utilities after each I-A-R cycle.          (hereafter Network Reinforcement Learning, NRL) has
           (I = input; A = action; R = reinforcement)                been previously suggested by Porta & Celaya (2005).
    I       A     R      Instance-based RL     Category-based RL     They argued that NRL is more efficient than traditional
  red                                                                RL for 'real world' robot learning. The basic idea behind
 apple     Eat   +1     U(red-apple,Eat) = +1  U(apple,Eat) = +1
 green
                                                                     our adaptation of NRL is as follows: unlike traditional
 apple     Eat   +1    U(green-apple,Eat) = +1 U(apple,Eat) = +2     reinforcement learning where a single state S is identified,
  red                                                                in NRL we identify multiple states, {S1,S2,...,Sn} (Figure
 apple     Eat   +1     U(red-apple,Eat) = +2  U(apple,Eat) = +3     2; left). Given n number of identified states and m number
 brown
 apple     Eat    -1  U(brown-apple,Eat) = -1  U(apple,Eat) = +2
                                                                     of possible actions, there are n⋅m competing state-action
 brown                                                               pairs (Figure 2, right; each arrow represents a competing
 apple     Eat    -1  U(brown-apple,Eat) = -2  U(apple,Eat) = +1     state-action pair). The state-action pair with the highest
                                                                     utility is chosen and the winning action, A, is activated.
Network Reinforcement Learning                                       Unlike traditional RL where only the winning state-action
We can imagine an agent with a dynamic state-space,                  pair is updated, the utility values for all s–A state-action
such that the agent could identify categories and category-          pairs are updated with the reinforcement feedback,
exceptions as needed. Such an agent may be able to use               s∈{S1,S2,...,Sn}.
category-based state identification for all apples except
brown apples in the example from Table 1.
   Here is the problem: we cannot disregard the fact that a
brown apple is brown, or that it is an apple. It may be the
case that action x (e.g. eat) results in different
reinforcement values between brown-apple and apple
states (brown-apple is an exception to the apple
category), and action y (e.g. throw) is the same for the
brown-apple state as it is for its parent category (brown-           Figure 2. Network Reinforcement Learning. Left: Flow of
apple follows the apple category rule). Moreover, even if              information in NRL agents (no learning shown). Right:
brown-apple was originally hypothesized by the agent to               Identified states, S1 through Sn, and possible actions, A1
be an exception to its parent categories, we want to allow              through An; each arrow represents a competing state-
the agent to learn through experience whether that                                             action pair.
hypothesis is true or not. If the agent fails to forget useless
exceptions it will continuously shift towards the                       Porta & Celaya (2005) used feature detectors (higher
inefficient instance-based state identification.                     order inputs; e.g. vertical-line and hand-shape neurons are
   Continuing with our example, if we do not disregard               considered feature-detectors in humans) as competing
the parent categories of the identified exception, then we           states. Where our theory differs from Porta & Celaya is in
have a model of reinforcement learning that recognizes a             our use of incremental configural memory instead of
brown apple as three different states – brown, apple, and            arbitrarily preclassified feature detectors. Configural
brown-apple. Let us say that we consider all of the state-           representation is very powerful, and is psychologically
action pairs of all of these states, and choose some action,         validated via habituation and discrimination studies (for
A, and receive some reinforcement, r. Does this mean that            review see Pearce, 1994). Configural memory is more
r belongs to brown–A, apple–A, or brown-apple–A?                     expressive than feature detectors because it may include
   To make this more concrete, if you eat a brown apple,             all possible input configurations (including feature
and it tastes bad, is it because it was an apple, because it         detectors). For example, for three inputs (e.g. white,
was brown, or because it was a brown apple? Maybe it is              square, large) there could be as many as seven configural
time for you to learn that apples just do not taste good.            nodes (e.g. white, square, large, white-square, white-
Maybe apples taste great but brown apples do not. Maybe              large, large-square, large-white-square).
                                                                 690

   Configural representations are sometimes criticized for       Shepard, Hovland, & Jenkins, 1961
the exploding number of configurations (e.g. Heydemann,          The beauty of the Shepard, Hovland, & Jenkins (1961)
1995): for stimuli that vary across 3 binary dimensions          benchmark categorization experiment is in its simplicity.
(e.g. small/large, black/white, triangle/square) a               Subjects are presented with one of eight objects varying
configural model would need 26 configural nodes (e.g.            across three binary dimensions (e.g. small/large,
small, large, black, white, triangle, square, small-black,       black/white, triangle/square), and have to pick one of two
small-white, small-square, ..., small-white-square, small-       responses (e.g. A or B). Feedback is then provided as to
white-triangle). Given a stimulus varying on 6 binary            whether the response was correct.
dimensions, the number of possible configural nodes
would rise to 126, etc. This problem has been addressed
in the IAK model (Heydemann, 1995), which uses
probabilistic sampling to select a small subset of
configurations, thus avoiding the rapid growth.
   Combining configural memory with NRL (CMNRL)
provides for simultaneous multi-level state identification,
such that types and tokens of every level may be used as
reinforcement learning states. In this type of a model there
may be different actions attributable to objects at every          Figure 3. An example of the six types of categorization
level. For example, animal, dog, golden retriever, and my         problems from the Shepard, Hovland, & Jenkins task. For
golden retriever named Sparky may all have some actions             each type, subjects must learn that the items in the left
in common and some actions that set them apart. Upon               column are one category, whereas the items in the right
seeing Sparky, all of these actions would compete. Upon                               column are another.
action-feedback, learning would occur for all of the
activated states, from top-level dog to bottom-level                The idea behind this study is to determine the rate at
Sparky.                                                          which people learn to classify each of the eight objects as
CMNRL versus RL                                                  belonging to one of two categories. Four of the objects
   CMNRL is more efficient and robust than traditional           belonged to category A, and the other four belonged to
Reinforcement Learning. To observe this, consider the            category B. Given this setup, there are only six different
case in Table 1. Instance-based RL would fail to                 types of possible category breakdowns. In the example in
generalize and make any predictions about a new apple            Figure 3 the three binary dimensions are shape (square vs.
object. Category-based RL would fail to learn about the          triangle), color (black vs. white), and size (large vs.
negative utility of eating a brown apple. Due to the fact        small); categories A and B are represented as left and
that CMNRL updates state-action pairs at both object and         right columns; and the six problem types are marked with
category levels simultaneously, it is able to generalize,        roman numerals I – VI.
and learn about exceptions, as well.                                The general results of the study indicated that human
   CMNRL is not a mere combination of a categorization           performance for problem types I through VI follow the
model with reinforcement learning. Integration of RL or          order: I > II > (III, IV, V) > VI. These results have been
NRL with one of the existing categorization models, such         replicated in multiple forms (Love, 2002; Nosofsky,
as RULEX (Nosofsky, Palmeri, & McKinley, 1994) or                Gluck, Palmeri, McKinley, & Glauthier, 1994; Smith,
SUSTAIN (Love, Medin, & Gureckis, 2004), will require            Minda, & Washburn, 2004), each time confirming the
twice as much feedback for learning. Such integration            main effect found by Shepard, Hovland, & Jenkins over
would produce an agent that requires supervised learning         forty-five years ago. It is rather simple to explain and
to identify declarative category structure, and then             model the fact that people performed best on problem I
reinforcement learning to learn procedural utilities. In         and worst on problem VI – while only one dimension is
contrast, CMNRL considers procedural memory part of              necessary to predict the category (A or B) in type I
the categorization process, and thus requires only the           problems, all three dimensions are necessary for correct
reinforcement signal as feedback. In other words,                category identification in type VI. What is less obvious
CMNRL uses the reinforcement signal to learn about               for most models of categorization is that performance on
category structure.                                              problem II is better than that on types III, IV, and V.
   In the following sections we will describe a classic          Supervised and Unsupervised Category Learning in
categorization experiment, and compare CMNRL results             the Shepard, Hovland, & Jenkins categorization task
against human and monkey data from this experimental                In further investigation as to why problem II was
paradigm. We will analyze how CMNRL performs this                learned faster than the problem IV, Love (2002)
task, and contrast it against traditional reinforcement          hypothesized that this was a result of the learning mode –
learning with and without categorization. We will also           namely, supervised classification learning. In supervised
discuss how standard categorization models perform on            classification learning the subject is presented with a
this task, and where CMNRL stands out from these.                stimulus, they give a response, and then corrective
                                                             691

feedback is provided. Given that much of category                performance on problems III, IV, V over problem II.
learning occurs in unsupervised, and sometimes                   None of the leading categorization models have attempted
completely incidental fashion, supervised classification         to explain both modes of category learning.
learning is not sufficient in explaining and predicting all         Among the failed attempts to model human supervised
human categorization behavior.                                   classification data was the configural-cue model, which
   Love omitted problems III and V, and changed his              very closely resembles the CMNRL setup on this task.
experimental procedure so that supervised classification         The major difference between CMNRL and configural-
learning could be directly compared with what he called          cue is that CMNRL uses NRL instead of the least mean
unsupervised-intentional and unsupervised-incidental             squares rule learning used by the configural-cue model.
learning modes. Unfortunately the details of the procedure       The use of NRL allows CMNRL to capture exploration
are beyond the scope of this paper. The results from the         and trial-and-error learning. It also affords the use of
supervised category learning condition of Love (2002)            unsupervised learning, where the environment feeds back
followed in order with the findings from Shepard,                a reinforcement signal instead of suggesting the correct
Hovland, & Jenkins (1961) and Nosofsky, Gluck, et al.            category for each object. This distinction is most
(1994). Subject performance on problem types I through           important because supervised forms of learning are not
VI followed the order: I > II > IV > VI. Unsupervised            always available in natural environments. The necessity
category learning, however, lead to performance                  for supervised learning is a problem with all
differences in this experiment. In particular, during these      categorization models cited above.
less volitional modes of category learning, performance
was better on type IV than type II problems (see Figure 4        Modeling Shepard, Hovland, & Jenkins, 1961
for results). Interestingly enough, further investigation of     with CMNRL
this paradigm by Smith et al. (2004), revealed that rhesus       Traditionally the Shepard, Hovland, & Jenkins paradigm
monkeys performed better on type IV problems than type           was viewed as a study of declarative category learning;
II problems, as well.                                            that is, each of the eight objects had to be classified as
                                                                 category A or B. We assume, however, that since the
                                                                 subjects had to respond, there is a procedural component
                                                                 involved. Thus we use NRL to model the responses, and a
                                                                 configural representation to identify the stimuli.
                                                                    As previously mentioned, for stimuli that vary across 3
                                                                 binary dimensions, as in Figure 3, a configural model
                                                                 would need a maximum of 26 configural nodes. For the
                                                                 simple stimuli used in this experimental paradigm we
                                                                 assume that human subjects have all 26 possible
                                                                 configurations in memory. With only two possible
                                                                 responses (e.g. A and B), there are a total of 52 state-
                                                                 action pairs (Figure 5).
   Figure 4. Subject performance during the test phase of
      Love (2002). Error bars represent standard error.
Modeling the Shepard, Hovland, & Jenkins paradigm
   The Shepard et al. paradigm has become one of the
benchmarks for models of categorization. Among the
models that have tried to model this task are: configural-
cue model (Gluck & Bower, 1988), DALR (Gluck,
Glautheir, & Suton, 1992, as cited by Nosofsky, Gluck, et
al., 1994), the rational model (Anderson, 1991, as cited by
Nosofsky, Gluck, et al., 1994), ALCOVE (Kruschke,
1992), ALCOVE-RL (Phillips & Noelle, 2004), RULEX
(Nosofsky, Palmeri, & McKinley, 1994), SUSTAIN
(Love, Medin, & Gureckis, 2004), and IAK (Heydemann,
1995). The latter five of these eight models produced the
same problem performance ordering as the human                      Figure 5. CMNRL setup for the Shepard, Hovland, &
supervised classification data would suggest: I > II > (III,                      Jenkins (1961) paradigm.
IV, V) > VI. However, the models that failed at
replicating human supervised classification data might              There are three free parameters in the current
have actually provided good fits to unsupervised category        implementation of NRL – utility of exploration (Ue),
learning data from Love (2002) and rhesus monkey data            perceived utility of positive reinforcement (U+), and
from Smith et al. (2004), predicting the advantage in
                                                             692

perceived utility of negative reinforcement (U-). Every              types I, II, IV, and VI were 47%, 50%, 49%, and 50%,
trial the model was presented with three known                       respectively, signifying random behavior).
dimensions and had to answer the fourth. If the
exploration parameter, Ue, was higher than utility values              Table 2. Best parameter fits for each of the six datasets.
of all competing state-action pairs, the model would try an       Fit                             U+        U-         R2     RMSE
action at random; otherwise the model would activate the          Nosofsky, Gluck, et al. (1994)  0.22     -1.85     0.986     0.017
action of the state-action pair with the highest utility
                                                                  Love (2002) Supervised          0.65     -0.60     0.971     0.067
value. If the model answered correctly, positive                  Love (2002) Unsupervised
reinforcement, U+, would propagate to all active state-                 Intentional               6.98     -6.60     0.983     0.024
action pairs. If the model answered incorrectly, negative         Love (2002) Unsupervised
reinforcement, U-, would propagate to all active state-                 Incidental                4.56     -4.30     0.986     0.023
action pairs.                                                     Smith et al. (2004) Human       0.14     -0.43     0.959     0.024
   For example, if the model saw large-white-square in            Smith et al. (2004) Monkey      0.01     -1.03     0.911     0.041
condition IV of Figure 3 there would be seven configural
nodes active (white, square, large, white-square, white-                The better (lower RMSE) parameter values seemed to
large, large-square, and large-white-square), and                    be related to the ratio of positive to negative
fourteen competing state-action pairs. If white-square–B             reinforcement, U+:U-, and the average absolute
had the highest utility of all other state-action pairs, and         reinforcement value, U+- (U+- is equivalent to the ratio
had a higher utility than the parameter Ue, the model                of average absolute reinforcement value to the utility of
would execute action B. In this case, the answer would be            exploration, U+-:Ue, because Ue=1.0). The best RMSE
correct, and the utilities for all seven active state-action         values for Love’s experiments, each involving 80 learning
pairs (white–B, square–B, large–B, white-square–B,                   trials, and 24 test trials where no learning occurred, had
white-large–B, large-square–B, and large-white-square–               the U+:U- ratio of ≈1.1:1. For the 192-trial experiments
B) would be incremented by U+.                                       of Nosofsky et al. and Smith et al. the best U+:U- ratios
   Given that the ratios of these parameters rather than             were ≈1:5. The top U+:U- ratios for the 2000-trial
their absolute values are of the essence, Ue was held                monkey experiment were in the range between ≈1:10 and
constant at 1.0 for all model runs. A genetic algorithm              ≈1:1000. Seemingly, in this sort of an experiment, as the
(using the least mean square error, LMSE, criterion) was             number of trials increases, the average perceived positive
employed to find the best fitting parameters for each of             reinforcement value drops, while the average perceived
the six datasets: Nosofsky, Gluck, et al. (1994), Love               negative reinforcement value grows. Although this makes
(2002) supervised, Love unsupervised intentional, Love               sense intuitively (when Michael Jordan misses a foul shot,
unsupervised incidental, Smith et al. (2004) human, and              he gets down on himself quite a bit more than when a
Smith et al. rhesus monkey. Comparisons to the data from             novice does the same), the NRL mechanism currently in
Nosofsky, Gluck, et al. (1994) study were limited to the             place in the CMNRL architecture does not yet account for
first 192 trials only (same number of trials as Smith et al.,        this phenomenon.
2004).                                                                  Top U+- values seemed to correlate with learning
   This is a simple setup that should demonstrate the                mode. Of the three human supervised classification
flexibility of NRL to learn categories, and do so in a               experiments – Nosofsky et al., Love supervised, and
humanlike (or monkeylike) fashion. The ability of NRL to             Smith et al. human – average U+- of the top parameter
explain the differences between various category-learning            sets was 0.64. Of the unsupervised categorization
modes with mere adjustments of perceived positive and                experiments by Love, average U+- of the top parameter
negative reinforcement gives additional power to the                 sets was 5.61. What this really means is that Ue was
proposed model – including more free parameters in a                 relatively small for supervised classification, and
model could explain much more data, but would be much                relatively large for unsupervised learning modes. This too
less meaningful; see (Su, Myung, & Pitt, 2005).                      makes intuitive sense – the average utility of exploration
Modeling Results                                                     should be higher when we are learning actively, i.e. trial-
   The U+,U- parameter pairs that produced the lowest                and-error, as in the three supervised classification
root mean square errors (RMSE) are shown in Table 2.                 experiments.
RMSE was a stricter criterion for the model than r2.                    Like ALCOVE, ALCOVE-RL, RULEX, SUSTAIN,
Although higher r2 values were found (e.g. best                      and IAK, CMNRL was able to capture the general trend
correlation value for Smith et al. monkey data was 0.986,            in performance across the six problem types in human
RMSE=0.198), maximizing r2 values, as opposed to                     supervised classification learning, going beyond other
minimizing RMSE values, sometimes resulted in                        categorization models like configural-cue, DALR, and the
accidental correlations (e.g. although the best correlation          rational model. With mere adjustments of positive and
value for Love unsupervised incidental data was 1.000,               negative reinforcement values, CMNRL was also able to
RMSE=0.183, the actual error-rate averages on problem                explain category learning in rhesus monkeys, as well as
                                                                     unsupervised category learning in humans.
                                                              693

Summary                                                           References
In this paper we argued that CMNRL is more efficient              Fu, W. T., & Anderson, J. R. (2006). From recurrent choice to
and robust than either instance-based or category-based                      skill learning: A reinforcement-learning model.
reinforcement learning. We also suggest the                                  Journal of Experimental Psychology-General, 135(2),
psychological validity of CMNRL by simulating human                          184-206.
                                                                  Gluck, M. A., & Bower, G. H. (1988). A Configural-Cue
and monkey data from a classic categorization paradigm
                                                                             Network Model of Classification Learning. Bulletin of
(Shepard, Hovland, & Jenkins, 1961).                                         the Psychonomic Society, 26(6), 500-500.
   CMNRL uses configural memory, where multiple types             Heydemann, M. (1995). A connectionist model for classification
and tokens are activated simultaneously upon object                          learning – the IAK model. Paper presented at the
recognition (e.g. my red robin activates {my red robin,                      Seventeenth Annual Conference of the Cognitive
red robins, red, robins, birds, animate objects, ...} nodes).                Science Society.
Network Reinforcement Learning extends traditional RL             Holroyd, C. B., & Coles, M. G. H. (2002). The neural basis. of
to handle simultaneous activation of multiple types and                      human error processing: Reinforcement learning,
tokens. Whereas traditional RL perceives the world as a                      dopamine,     and    the    error-related negativity.
                                                                             Psychological Review, 109(4), 679-709.
single state, NRL uses multiple nodes to represent the
                                                                  Kruschke, J. K. (1992). ALCOVE: an exemplar-based
state of the world. Whereas traditional RL reinforces one                    connectionist model of category learning. Psychol
state-action pair at a time, NRL updates the utilities of all                Rev, 99(1), 22-44.
relevant state-action pairs. In this manner, CMNRL does           Love, B. C. (2002). Comparing supervised and unsupervised
not fail to generalize from its experiences, nor to account                  category learning. Psychon Bull Rev, 9(4), 829-835.
for anomalies.                                                    Love, B. C., Medin, D. L., & Gureckis, T. M. (2004).
   In simulating data from the Shepard, Hovland, &                           SUSTAIN: a network model of category learning.
Jenkins paradigm, CMNRL went beyond other                                    Psychol Rev, 111(2), 309-332.
categorization models. First, CMNRL is an example that            Nosofsky, R. M., Gluck, M. A., Palmeri, T. J., McKinley, S. C.,
                                                                             & Glauthier, P. (1994). Comparing models of rule-
it is not necessary to include a separate model of
                                                                             based classification learning: a replication and
categorization along with a procedural mechanism – the                       extension of Shepard, Hovland, and Jenkins (1961).
procedural mechanism in CMNRL, NRL, is an essential                          Mem Cognit, 22(3), 352-369.
part of the categorization mechanism. Second, CMNRL               Nosofsky, R. M., Palmeri, T. J., & McKinley, S. C. (1994).
does not rely on supervised learning. This is important                      Rule-plus-exception model of classification learning.
because the answers are not always available in the                          Psychol Rev, 101(1), 53-79.
environment, only the reinforcement signals. Last, but not        Pearce, J. M. (1994). Similarity and discrimination: a selective
least, CMNRL captures and explains the data from both                        review and a connectionist model. Psychol Rev,
supervised and unsupervised modes of category learning.                      101(4), 587-607.
                                                                  Phillips, J. L., & Noelle, D. C. (2004). Reinforcement learning
The differences between the two types of experiments are
                                                                             of dimensional attention for categorization. Paper
explained in CMNRL using the relative utility of                             presented at the Twenty-Sixth Annual Conference of
exploration. During the supervised learning mode,                            the Cognitive Science Society.
subjects were more likely to explore (learn by trial and          Porta, J. M., & Celaya, E. (2005). Reinforcement learning for
error). During unsupervised learning modes, subjects                         agents with many sensors and actuators acting in
were more likely to learn passively (no exploration).                        categorizable environments. Journal of Artificial
   CMNRL combines declarative structure and procedural                       Intelligence Research, 23, 79-122.
learning to make for a more complete procedural                   Shepard, R. N., Hovland, C. I., & Jenkins, H. M. (1961).
mechanism and a more complete categorization                                 Learning and memorization of classifications.
                                                                             Psychological Monographs: General and Applied, 75,
mechanism. This issue of integration makes CMNRL a
                                                                             1-42.
better candidate of memory implementation for cognitive           Smith, J. D., Minda, J. P., & Washburn, D. A. (2004). Category
architectures. Without much redesign or modeling efforts,                    learning in rhesus monkeys: a study of the Shepard,
CMNRL may be used to create a cognitive agent that can                       Hovland, and Jenkins (1961) tasks. J Exp Psychol
learn a new environment from scratch, explore, and                           Gen, 133(3), 398-414.
improve performance on arbitrary tasks. Future work on            Su, Y., Myung, I. J., & Pitt, M. A. (2005). Minimum description
CMNRL will involve testing the mechanism in foraging                         length and cognitive modeling. In P. D. Grünwald, I.
environments, simple games, and language learning.                           J. Myung & M. A. Pitt (Eds.), Advances in minimum
                                                                             description length: Theory and applications (pp. 411–
                                                                             433). Cambridge, MA: The MIT Press.
Acknowledgments
                                                                  Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning:
Many thanks to Ron Sun for his advice throughout this                        An Introduction. Cambridge, Massachusetts: The MIT
research. This work was supported, in part, by the                           Press.
Disruptive Technology Office, A-SpaceX contract
N61339-06-C-0139 issued by PEO STRI. The views and
conclusions are those of the authors, not of the US
Government or its agencies.
                                                              694

