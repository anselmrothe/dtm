UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Visual Classification using Bottom-up and Top-down Fixation Selection
Permalink
https://escholarship.org/uc/item/5p3739sh
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Lacroix, Joyca P.W.
Postma, Eric O.
van den Herik, H. Jaap
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

 Modeling Visual Classification using Bottom-up and Top-down Fixation Selection
                                         Joyca P. W. Lacroix (jlacroix@fsw.leidenuniv.nl)
                                        Department of Cognitive Psychology, Leiden University,
                                         Wassenaarseweg 52, 2300 RB, Leiden, The Netherlands
         Eric O. Postma (postma@micc.unimaas.nl) and H. Jaap van den Herik (herik@micc.unimaas.nl)
                                Department of Computer Science, MICC-IKAT, Maastricht University,
                                         Tongersestraat 6, 6211 LN, Maastricht, The Netherlands
                              Abstract                                    to N IM-C LASS A by adding an active top-down mechanism
                                                                          to select fixation locations. We then evaluate to what extent
   This paper describes two initial steps towards the realization
   of a plausible model of natural visual classification. As a first      the use of the top-down mechanism improves classification
   step, we extend the recently developed Natural Input Mem-              by testing N IM-C LASS A on the classification task. Subse-
   ory (N IM) model (Lacroix, Murre, Postma, & van den Herik,             quently, we discuss top-down gaze control models, and the
   2006) to a classification model of natural visual input called
   N IM-C LASS and evaluate the model in a face-classification ex-        scalability and sensitivity to changes in viewpoint of the N IM-
   periment. Our experimental results show that N IM-C LASS is            C LASS models. Finally, we present our conclusion.
   able to recognize and classify faces after a single encounter. In
   addition, N IM-C LASS is insensitive to variations in facial ex-
   pressions, illumination conditions, and occlusions. As a sec-                      Extending N IM to N IM-C LASS
   ond step, we extend N IM-C LASS to N IM-C LASS A by adding             N IM is a model for recognition of natural images (Lacroix
   an active top-down fixation-selection mechanism. We then as-
   sess to what extent N IM-C LASS A improves the performance             et al., 2006). N IM encompasses the following two stages.
   on the face-classification task. The results show that the in-
   corporation of a selection mechanism improves classification          1. A perceptual preprocessing stage during which a natural
   performance, particularly when a limited number of fixations              image is translated into feature vectors.
   are taken during the classification process. Our results lead us
   to the conclusion that N IM-C LASS A may provide a suitable
   basis for a model of natural visual classification.                   2. A memory stage comprising two processes:
   Keywords: Perception, memory, classification, gaze control.             (a) a storage process that stores feature vectors in a straight-
                                                                                forward manner;
                          Introduction                                     (b) a recognition process that compares feature vectors of
Traditional computational models of cognition (e.g., Shiffrin                   a newly presented image with previously stored feature
& Steyvers, 1997) generally operate on an abstract represen-                    vectors.
tation space, because they lack a mechanism to derive rep-
resentations from the physical features of stimuli, i.e., they            Inspired by eye fixations in human vision, the perceptual pre-
are not grounded in the real world. In sharp contrast, natural            processing stage selects image samples (i.e., fixations) ran-
systems ground representations in physical interaction with               domly along the contours in the image. For each fixation,
the world. Acknowledging the importance of the environ-                   visual input is translated into a feature vector that resides in
ment for natural cognition, a recent trend in psychologically             a similarity space. The translation is realized using a biolog-
motivated cognitive models is to focus on grounding repre-                ically informed method that involves a multi-scale wavelet
sentations in terms of their real-world referents (e.g., Pecher           decomposition (see, e.g., Rao & Ballard, 1995) followed by a
& Zwaan, 2005). Following this trend, the recently proposed               principal component analysis. This method from the domain
Natural Input Memory model (N IM; Lacroix et al., 2006) re-               of visual object recognition models the first stages of process-
alizes a memory model that operates directly on real-world                ing of information in the human visual system (i.e., retina,
visual input (i.e., natural digitized images). It builds feature-         LGN, V1/V2, V4/LOC; (Palmeri & Gauthier, 2004)). N IM
vector representations on the basis of local samples (i.e.,               applies the method in a saccadic based manner to build rep-
eye fixations) from natural images and uses these to make                 resentations of fixated image parts that together constitute
recognition-memory decisions (e.g., Lacroix et al., 2006).                the feature-vector representation of an image. The memory
   We aim at extending N IM (Lacroix et al., 2006) to a model             stage stores the feature-vector representation (the storage pro-
of natural visual classification. This paper provides two initial         cess) and makes recognition decisions by matching an incom-
steps towards achieving this objective. The outline of the re-            ing feature-vector representation with previously stored rep-
mainder of this paper is as follows. As a first step, we extend           resentations. For a more detailed description and a schematic
N IM into a classifier of natural images called N IM-C LASS               overview of N IM we refer to Lacroix et al., 2006. While N IM
and assess N IM-C LASS’s performance in a face-classification             is a model for recognition of natural images, here we show
experiment. As a second step, we aim to approach the in-                  that it can readily be adapted into a model for classification
teractive nature of natural vision by extending N IM-C LASS               of natural images which we call N IM-C LASS. The feasibility
                                                                     419

of adapting N IM for classification has been shown recently
by Barrington, Marks, and Cottrell (2007) who presented a
Bayesian version of N IM called N IMBLE and successfully
applied it to face classification. N IM-C LASS uses a slightly
different approach that adopts N IM’s perceptual preprocess-
ing stage (i.e., the perceptual front-end), but introduces a new
memory stage that is expected to be suitable for classifica-              Figure 1: Example of the 13 views of one individual from the
tion. Below, we discuss the two processes of the N IM-C LASS              AR data set.
memory stage: the storage process and the classification pro-
cess.                                                                     the twelve remaining images. In this respect, our evaluation
                                                                          differs from most evaluations in machine learning, where the
The Storage Process
                                                                          training set consists of a much larger fraction of the data set.
The N IM-C LASS storage process retains (i.e., stores) prepro-
cessed samples of natural images (i.e., fixations) that belong            The Data Set
to a certain class. Each natural image is represented by a                For the face-classification task, a data set with different im-
number of low-dimensional feature vectors (one for each fix-              ages of the same individual was needed. We chose to use
ation) in a similarity space. In contrast to the original N IM            the AR data set created by Martinez and Benavente (1998)
that stores unlabeled feature vectors, N IM-C LASS stores class           that contains over 4,000 images corresponding to the faces
labels with each feature vector corresponding to the class as-            of 126 individuals. For each individual, the AR data set in-
sociated with the image (i.e., ‘1’ for class 1, ‘2’ for for class         cludes a sequence of 13 images featuring frontal view faces
2, and so forth).                                                         with different facial expressions, illumination conditions, and
                                                                          occlusions. For the experiment, we selected the sequence of
The Classification Process                                                13 images (i.e., views) of the first 10 male individuals of the
The N IM-C LASS classification process employs a naive                    AR data set as our data set. Fig. 1 shows an example of the
Bayesian method that is based on an incremental estimate                  sequence of 13 views of one individual. The first (standard)
of the class-dependent probabilities (Duda, Hart, & Stork,                view of each individual was selected for the study list, the
2001). During the classification process, each fixation of the            remaining 12 views were assigned to the test list.
test image (i.e., each test feature vector) contributes to an n-
bin histogram, the bins of which represent the ‘beliefs’ in the           The Experimental Procedure
n different classes. For each test feature vector, the bin that           The face-classification experiment entailed a study and a test
corresponds to the label of its nearest neighbouring stored la-           phase. During the study phase, N IM-C LASS was presented
beled feature vector (acquired in the storage process) is in-             with the images from the study list containing the first view
cremented (e.g., if the stored labeled feature vector that is             of each of the n = 10 individuals (i.e., the study faces). For
closest to the test feature vector has label ‘1’, bin 1 is in-            each study face, N IM-C LASS extracted and stored s labeled
cremented). Finally, upon the last fixation, the class with               feature vectors. Then during the test phase, the model was
the largest bin (i.e., belief) determines the classification de-          presented with the 120 images from the test list (i.e., the 12
cision. This heuristic classification process could readily be            test faces of each of the n = 10 individuals). For each of the
extended into a Bayesian approach in which each fixation up-              test faces, the model extracted t test feature vectors to classify
dates class-conditional probabilities according to the Bayes              the face as one of the n = 10 individuals that it had previously
update rule.                                                              encountered. To assess how the N IM-C LASS classification
                                                                          performance varied as a function of the number of storage
            The Classification Experiment                                 fixations s and the number of test fixations t, the experiment
The experiment evaluates N IM-C LASS’s ability to classify                was repeated for values of s and t in the range 10 to 100, i.e.,
natural images of faces. Below, we discuss the classification             s,t ∈ {10, 20, ...100}.
task, the data set, and the experimental procedure.
                                                                                      Classification with N IM-C LASS
The Classification Task                                                   Below, we present the N IM-C LASS results for the face-
The classification task entails the identification of a natural           classification task1 and compare these with human face-
image of a frontal face with variations in facial expressions,            identification results.
illumination conditions (location of the light source), and oc-
clusions (sun glasses and scarf). Humans are generally able               Classification Results
to identify a face after a single encounter only, despite varia-          Table 1 presents the percentages of test faces classified cor-
tions in appearance (e.g., Burton, Jenkins, Hancock, & White,             rectly by N IM-C LASS for a range of values of the number
2005). Inspired by this fact, N IM-C LASS is evaluated on a                   1 These results were partly presented at the workshop Towards
task in which the training set (i.e., the study list) consists of a       Cognitive Humanoid Robots of the IEEE-RAS International Con-
single image for each class and the test set (i.e., the test list) of     ference on Humanoid Robots 2006
                                                                      420

of storage fixations s and the number of test fixations t. Fig.         histograms corresponds to the individual depicted at the top
2(a) presents the same results as a surface plot. The N IM-             of that column. A face is classified correctly when the index
C LASS classification performances range from just above                of the largest bin corresponds to the class of the particular
chance level (16%) for s = t = 10 and reach a good perfor-              face. From Fig. 3 it can be seen that, in most cases, the largest
mance of 89.0% for s = t = 100. Evidently, N IM-C LASS is               bin corresponds to the class of the test face. Where this is not
capable of exhibiting a good performance provided that a suf-           the case, the largest bin is not considerably larger than the
ficient number of fixations is made.                                    other bins. Therefore, the faces classified falsely can be said
   The results show, not surprisingly, that the performance             to be classified with less certainty than the faces classified
increases both with the number of storage fixations and the             correctly.
number of test fixations. Increasing the number of storage
fixations s, improves the performances more than increasing             Comparison with Human Face Identification
the number of test fixations t. For small s values, perfor-             Since this paper addresses the suitability of N IM-C LASS as
mance hardly increases with t. Evidently, taking more test              a cognitive controller of a humanoid robot, we compare the
fixations is only useful when a sufficient number of feature            N IM-C LASS performance with that of human face identifica-
vectors were stored previously. From a statistical perspective          tion in a natural setting.
this makes sense. A proper approximation of the true distribu-             The number of storage and test fixations extracted by N IM-
tion of feature vectors in a similarity space associated with a         C LASS can be interpreted as the amount of viewing time
single face requires a sufficient number of samples (fixations)         of the image during study and test, respectively. Dividing
of that face.                                                           the number of fixations by five provides a rough estimate
                                                                        of the number of seconds the image is inspected, since hu-
Table 1: Percentages of faces classified correctly by N IM-             mans make about five fixations per second (see, e.g., Hen-
C LASS for a range of values for the number of storage fixa-            derson, 2003). As the results show, the N IM-C LASS perfor-
tions s and the number of test fixations t.                             mance relies heavily on the amount of viewing time during
                                                                        the study phase. This accords with results from several psy-
                                 t                                      chological studies indicating that memory for visual informa-
         10    20   30     40   50     60    70    80    90    100
   s                                                                    tion increases with viewing time during study (e.g., Mäntylä
  10    16.0  18.2 20.6   22.1 23.6   23.7  24.4  25.3  25.5   26.2
  20    21.3  26.3 29.5   32.1 35.5   38.3  39.3  41.1  42.7   43.5     & Holm, 2006; Melcher, 2006). Moreover, it is interesting
  30    26.5  32.8 38.1   42.5 46.3   49.0  52.0  53.3  55.5   57.3
  40    30.0  39.5 45.7   51.1 55.1   58.6  60.8  63.1  64.5   66.8     that a considerable percentage of faces (say ≥ 75%) is clas-
  50    34.0  45.2 51.7   57.0 61.8   64.9  68.0  70.0  71.5   73.7
  60    36.7  49.2 57.0   62.7 66.9   70.7  73.7  75.3  77.3   78.5     sified correctly after a short viewing time of about 8 seconds
  70    39.8  52.9 61.8   67.7 71.2   75.3  77.8  79.6  80.9   82.5
  80    42.7  57.0 65.9   70.9 75.4   77.9  80.7  82.9  84.3   85.4     (40 fixations) during the test phase, provided that there was a
  90    45.7  60.1 68.3   73.8 78.3   81.1  83.3  84.8  85.9   87.4
  100   47.6  63.1 71.3   77.0 80.6   83.2  84.7  87.1  87.8   89.0     sufficiently long viewing time of about 20 seconds (100 fix-
                                                                        ations) during the study phase. In additional simulations, we
                                                                        assessed in more detail to what extent N IM-C LASS is able to
Figure 2: The classification performance as a function of the
number of storage fixations s and the number of test fixations
t of N IM-C LASS (left), and N IM-C LASS A (right).
   To provide some insight into the distribution of beliefs in
the different classes for each of the 120 test faces (i.e., 12 test
views for each of the 10 individuals in the data set), Fig. 3
presents an overview of the histograms for each of the 120 test
faces for s = t = 100. Each histogram represents the belief in
class 1 (leftmost bin in each histogram) to 10 (rightmost bin
in each histogram). In other words, the histograms represent
the frequency counts of the labels of the nearest neighbours of
the test feature vectors. Each row of histograms corresponds            Figure 3: Overview of the histograms across the 120 test faces
to the view depicted to the left of that row and each column of         (i.e., 12 views of each of the 10 individuals) for s = t = 100.
                                                                    421

classify the test faces correctly on the basis of a brief viewing      the bottom-up (i.e., contour-based) fixation selection of N IM-
time during the test phase of only 1 second (t = 5). The sim-          C LASS during the storage process. Therefore, the storage
ulation results show that N IM-C LASS is able to reach a con-          process is similar to that in N IM-C LASS, except that N IM-
siderable classification performance on the basis of a view-           C LASS A stores the coordinates of each fixation along with
ing time of 1 second during the test phase, provided that the          the class label. The coordinate labels are used for the top-
viewing time during the study phase, s, is sufficiently long           down fixation selection during the classification process.
(mean percentages of faces classified correctly across all the
views ranged from 36.9% to 74.0% when the number of stor-              The Classification Process
age fixations s were varied in the range 100 to 1000 , i.e.,           For the implementation of the top-down fixation selection in
s ∈ {100, 110, ...1000}, corresponding to about 20 to 200 sec-         N IM-C LASS A, we rely on the notion of Shannon’s (1948)
onds of viewing time). The same holds for human vision, for            entropy. Shannon introduced entropy as a measure of uncer-
which it is known that a brief viewing time will allow for cor-        tainty. In order to decide in the most efficient way to which
rect identification, provided the face is sufficiently familiar to     class a new item belongs, a system should select new in-
the observer (e.g., Burton et al., 2005).                              put that minimizes the entropy, i.e., the uncertainty about the
   Overall, the N IM-C LASS classification results demonstrate         class membership. In N IM-C LASS, uncertainty is represented
that natural images of frontal faces under a variety of poten-         by the histogram in which the heights of the bins represent the
tially disturbing conditions can be classified correctly using a       beliefs in the different classes. Considering the uncertainty,
classification process that compares (a sufficient number of)          the top-down fixation-selection mechanism selects those lo-
stored local image samples (i.e., fixations) acquired during           cations that contain the most relevant information to decide
one encounter (i.e., one stored view) to incoming local sam-           upon the class of the face under consideration (i.e., that min-
ples. N IM-C LASS uses a bottom-up fixation-selection mecha-           imize the entropy or uncertainty about the class). In order
nism that selects fixations on the basis of their visual saliency      to do so, the mechanism relies on short-term episodic knowl-
(contours). While bottom-up processes are important in hu-             edge about attended parts of recently encountered faces which
man vision too, they are integrated with top-down processes            is represented by the labeled feature vectors that were ac-
that direct the gaze to relevant locations on the basis of cogni-      quired during the storage process directly preceding the cur-
tive systems (see, e.g., Henderson, 2003). Below, we explore           rent classification process.
the use of top-down fixation selection and investigate to what             For each fixation, the top-down fixation selection mech-
extent top-down fixation selection aids performance on the             anism first chooses the two most likely classes, A and B, by
classification task.                                                   selecting the two highest bins in the histogram. Subsequently,
                                                                       it selects the fixation location that best discriminates between
      Top-down Fixation Selection: Extending                           the two classes A and B (i.e., contains the most relevant visual
             N IM-C LASS to N IM-C LASS A                              input with respect to A and B). The idea behind the selec-
                                                                       tion is that spatially adjacent fixations within one class give
Several studies showed that human gaze control relies more             rise to similar feature vectors. Hence, the fixation mechanism
on top-down processes than on bottom-up processes when                 searches for a pair of feature vectors a and b coming from
performing an active visual task with meaningful stimuli (see,         classes A and B, respectively, that originate from relatively
e.g., Henderson, Brockmole, Castelhano, & Mack, to appear).            close spatial locations and at the same time are relatively dis-
The top-down processes are driven by several cognitive sys-            tant from each other in the representation space. A detailed
tems, including: (1) short-term episodic memory for previ-             specification of the implementation of this idea can be found
ously attended visual input, (2) stored long-term knowledge            in Lacroix, Postma, Murre, and van den Herik (in prepara-
about visual, spatial, and semantic characteristics of classes         tion).
of items or scenes acquired through experience, and (3) the
goals and plans of the viewer (e.g., Henderson, 2003; Mäntylä                   Classification with NIM-CLASS A
& Holm, 2006). Inspired by fixation selection in human vi-             Below, we present the results for the face-classification task
sion, this section extends N IM-C LASS to N IM-C LASS A by             performed with N IM-C LASS A and compare the classification
introducing a top-down fixation-selection during the classifi-         performances of N IM-C LASS and N IM-C LASS A.
cation process. In order to do so, we rely on the short-term
episodic knowledge about previously attended visual input              Classification Results
(see, e.g., Henderson, 2003; Mäntylä & Holm, 2006) that is           Table 2 presents the percentages of test faces classified cor-
known to operate in human gaze control. Below, we discuss              rectly by N IM-C LASS A for a range of values of the number
the two processes of the memory stage of N IM-C LASS A: (1)            of storage fixations s and the number of test fixations t. In ad-
the storage process and (2) the classification process.                dition, Fig. 2(b) displays the classification performances as
                                                                       a function of s and t for N IM-C LASS A. The N IM-C LASS A
The Storage Process                                                    classification performance ranges from 25.2% for s = t = 10
N IM-C LASS A extends N IM-C LASS with top-down fixation               to a performance of 91.0% for s = t = 100. Overall, the N IM-
selection during the classification process, while featuring           C LASS A performance is improved compared to the origi-
                                                                   422

                                                                       scalability of the N IM-C LASS models and their sensitivity to
Table 2: Percentages of faces classified correctly by N IM-
                                                                       changes in viewpoint.
C LASS A for a range of values for the number of storage fix-
ations s and the number of test fixations t.                           Top-down Gaze-Control Models
                                   t                                   N IM-C LASS A that employs a top-down fixation-selection
        10     20    30     40    50    60    70    80   90   100
    s                                                                  mechanism based on episodic short-term knowledge about
   10  25.2   27.8  28.8   29.1  30.9  29.9  30.1  31.1 30.0  30.6
   20  32.5   40.6  43.8   45.9  48.7  50.6  50.7  51.8 53.6  53.4     previously attended image parts, may be related to probabilis-
   30  36.9   46.3  53.3   56.6  59.3  62.0  64.0  65.4 66.7  67.1
   40  41.0   51.5  58.3   63.9  66.5  68.7  71.2  72.6 73.8  75.4     tic active vision models for classification (for an overview
   50  44.1   56.2  63.7   67.8  71.3  73.5  75.7  78.1 79.3  80.9
   60  46.7   59.6  66.5   72.3  75.2  77.6  79.9  81.2 82.6  84.2     see, de Croon, Sprinkhuizen-Kuyper, & Postma, 2006). Prob-
   70  49.1   62.7  69.8   75.0  78.5  80.8  82.2  84.2 85.8  86.7
   80  51.3   64.8  72.1   77.4  80.0  83.3  85.2  85.8 86.9  88.2     abilistic active models either consider all possible fixation
   90  53.4   67.0  75.3   79.3  82.7  84.7  86.6  87.6 88.7  89.8
  100  54.9   69.5  77.1   81.5  84.4  85.8  87.8  88.9 90.0  91.0     selections at each time step (e.g., Denzler & Brown, 2002),
                                                                       consider all possible fixation selections in advance (e.g., Ar-
                                                                       bel & Ferrie, 2006), or use a fixation selection policy that is
nal N IM-C LASS performance. As for N IM-C LASS, the re-               acquired on the basis of an extensive training (e.g., reinforce-
sults of N IM-C LASS A show that performance increases with            ment learning, see, Paletta, Prantl, & Pinz, 1998) or on the
the number of storage fixations s and the number of test fix-          basis of an evolutionary algorithm (e.g., de Croon, Postma,
ations t and the performance increases more with s than with           & van den Herik, 2006). In contrast, top-down fixation se-
t. As was demonstrated for N IM-C LASS, the results of N IM-           lection in N IM-C LASS A relies solely on the feature vectors
C LASS A show that taking more test fixations t becomes use-           that were stored during one encounter with the class instance
ful when a sufficient amount of feature vectors were stored            (during the storage process).
previously.
                                                                       Scalability
Comparison of Classification Results                                   In our classification task, N IM-C LASS and N IM-C LASS A
The results show that extending N IM-C LASS with top-down              deal with 130 objects (i.e., faces) coming from 10 different
fixation selection to direct the gaze towards relevant loca-           classes. Obviously, this limited number of objects can hardly
tions, improves the performance on the classification task.            be considered to be representative for the enormous number
In N IM-C LASS A, the top-down fixation selection actively             of objects that natural systems encounter in the real world.
constructs a fixation sequence based on: (1) the task to be            Ideally, a plausible classification or recognition model should
solved (i.e., classification), and (2) the stored episodic knowl-      be able to distinguish among large numbers of objects. How-
edge about previous encounters with particular faces (i.e.,            ever, since the different N IM-C LASS models store the com-
the stored labeled feature vectors). By combining top-down             plete encountered visual input, classification time grows lin-
and bottom-up processes for the selection of fixations, N IM-          early with the number of encountered objects (see also Ba-
C LASS A acknowledges the influence of the episodic short              jramovic, Mattern, Butko, & Denzler, 2006). In order to deal
term knowledge and the goals (i.e., classification) that are           with this problem, mechanisms can be incorporated that en-
known to play a role in human gaze control (see, e.g. Hen-             sure an efficient use and maintenance of the representation
derson, 2003). The active strategy employed by N IM-C LASS             space, e.g., neurally inspired representation techniques in-
A ensures that the locations are fixated which are known to            cluding self-organizing maps, radial basis function networks,
discriminate well among the two classes considered to be the           and spiking neural networks.
most likely at that time by the model. Therefore, the model
more often makes the correct classification decision. This             Viewpoint Invariance
is particularly so, when a limited number of fixations are             We have not tested the model’s sensitivity to changes in view-
taken during the classification process. With a large num-             point. For many object recognition techniques, changes in
ber of fixations during the classification process, a sufficient       viewpoint cause major degradations in performance. It has
amount of relevant visual information is gathered for cor-             been suggested that the brain brings about invariance across
rect classification even when fixations are taken randomly             viewpoint through interpolation across the responses of a set
along the contours. With fewer fixations during the classi-            of stored global shape templates corresponding to prototyp-
fication process, the probability that a sufficient amount of          ical object views (e.g., Edelman & Duvdevani-Bar, 1997).
relevant visual information is gathered for correct classifica-        In contrast, the classical recognition-by-components theory
tion decreases. Therefore, performance differences between             attempted to deal with invariance by representing objects
the original N IM-C LASS and the N IM-C LASS A models are              in terms of their invariant parts (Biederman, 1987). How-
most pronounced for small t values.                                    ever, the extraction of invariant parts from natural images
                                                                       has proved to be computationally challenging, if not infea-
                           Discussion                                  sible. N IM-C LASS combines both approaches by extracting
Below, we compare top-down gaze control in N IM-C LASS                 both local and more global shape information (Lacroix et al.,
A with other top-down gaze-control models, and discuss the             2006). Further studies should address to what extent this
                                                                   423

combined approach copes with the weaknesses of the sep-              de Croon, G., Sprinkhuizen-Kuyper, I. G., & Postma, E. O.
arate approaches in dealing with invariance. Also, we may              (2006). Comparing active vision models (Tech. Rep. No.
consider extending N IM-C LASS with existing statistical tech-         06-02). MICC-IKAT, Universiteit Maastricht.
niques that operate on the representation space in order to en-      Denzler, J., & Brown, C. M. (2002). Information theoretic
hance viewpoint invariance (e.g., Prince & Elder, 2006).               sensor data selection for active object recognition and state
                                                                       estimation. IEEE Transactions on Pattern Analysis and
                         Conclusion                                    Machine Intelligence, 24, 145–157.
This paper presented two initial steps towards the realization       Duda, R. O., Hart, P. E., & Stork, D. G. (2001). Pattern
of a plausible model of natural visual classification. As a            classification. New York, NY: Wiley & Sons Inc.
first step, we extended the recently developed N IM model            Edelman, S., & Duvdevani-Bar, S. (1997). Similarity-based
to a model for classification of natural images called N IM-           viewspace interpolation and the categorization of 3d ob-
C LASS. The results obtained by testing N IM-C LASS in a               jects. In Proceedings of the edinburgh workshop on simi-
face-classification experiment, demonstrate that N IM-C LASS           larity and categorization.
is able to recognize and classify faces after a single en-           Henderson, J. M. (2003). Human gaze control during real-
counter despite variations in facial expressions, illumination         world scene perception. Trends in Cognitive Science, 7,
conditions, and occlusions. As a second step, we extended              498–504.
N IM-C LASS to N IM-C LASS A by adding an active top-down            Henderson, J. M., Brockmole, J. R., Castelhano, M. S., &
fixation selection mechanism. The results obtained with                Mack, M. (to appear). Visual saliency does not account for
N IM-C LASS A demonstrate that using a top-down fixation-              eye movements during search in real-world scenes. In Eye
selection mechanism can enhance performance on the face-               movements: A window on mind and brain. Oxford, UK:
classification task by selecting actively the most relevant fix-       Elsevier.
ations. From our results, we may conclude that N IM-C LASS           Lacroix, J. P. W., Murre, J. M. J., Postma, E. O., & van den
A provides a suitable basis for a model of natural visual clas-        Herik, H. J. (2006). Modeling recognition memory using
sification.                                                            the similarity structure of natural input. Cognitive Science,
                                                                       30, 121-145.
                    Acknowledgments                                  Lacroix, J. P. W., Postma, E. O., Murre, J. M. J., & van den
The work described in this paper was partially conducted               Herik, H. J. (in preparation). Active classification with
within the EU Cognitive Systems project PACO-PLUS (FP6-                N IM-C LASS.
2004-IST-4-027657) funded by the European Commission                 Mäntylä, T., & Holm, L. (2006). Gaze control and recollec-
and partially within the Cognition Program project Events              tive experience in face recognition. Visual Cognition, 13,
in Memory and Environment (051.02.2002) funded by the                  365–386.
Netherlands Organization for Scientific Research (NWO).              Martinez, A., & Benavente, R. (1998). The ar face database.
                                                                       CVC Technical Report #24.
                         References                                  Melcher, D. (2006). Accumulation and persistence of mem-
Arbel, T., & Ferrie, F. P. (2006). Entropy-based gaze plan-            ory for natural scenes. Journal of Vision, 6, 8–17.
   ning. Image and Vision Computing, 19, 779–786.                    Paletta, L., Prantl, M., & Pinz, A. (1998). Reinforcement
Bajramovic, F., Mattern, F., Butko, N., & Denzler, J. (2006).          learning for autonomous three-dimensional object recogni-
   A comparison of nearest neighbor search algorithms for              tion. In Proceedings of the 6th symposium on intelligent
   generic object recognition. In Proceedings of the advanced          robotics systems (pp. 63–72). Edinburgh, UK.
                                                                     Palmeri, T. J., & Gauthier, I. (2004). Visual object under-
   concepts for intelligent vision systems (ACIVS 2006) (pp.
                                                                       standing. Nature Reviews Neuroscience, 5, 291–303.
   1186–1197).
                                                                     Pecher, D., & Zwaan, R. A. (2005). Grounding cognition.
Barrington, L., Marks, T. K., & Cottrell, G. W. (2007). N IM -
                                                                       Cambridge, UK: Cambridge University Press.
   BLE : A kernel density model of saccade-based visual mem-
                                                                     Prince, S. J. D., & Elder, J. H. (2006). Tied factor analysis for
   ory. In Proceedings of the 29th annual meeting of the cog-
                                                                       face recognition across large pose changes. In Proceedings
   nitive science society (CogSci 2007).
                                                                       of the british machine vision conference (Vol. 3, pp. 889-
Biederman, I. (1987). Recognition-by-components: A theory
                                                                       898).
   of human image understanding. Psychological Review, 94,
                                                                     Rao, R. P. N., & Ballard, D. H. (1995). An active vision
   29–73.
                                                                       architecture based on iconic representations. Artificial In-
Burton, A. M., Jenkins, R., Hancock, P. J. B., & White, D.
                                                                       telligence, 78, 461–505.
   (2005). Robust representation for face recognition: The
                                                                     Shannon, C. E. (1948). A mathematical theory of commu-
   power of averages. Cognitive Psychology, 51, 256–284.
                                                                       nication. The Bell System Technical Journal, 27, 379–423,
de Croon, G., Postma, E. O., & van den Herik, H. J. (2006).
                                                                       623–656.
   A situated model for sensory-motor coordination in gaze           Shiffrin, R. M., & Steyvers, M. (1997). A model for recog-
   control. Pattern Recognition Letters: Special Issue on Evo-         nition memory: Rem: Retrieving effectively from memory.
   lutionary Computer Vision and Image Understanding, 27,              Psychonomic Bulletin & Review, 4, 145–166.
   287–314. (Guest Editor G. Olague)
                                                                 424

