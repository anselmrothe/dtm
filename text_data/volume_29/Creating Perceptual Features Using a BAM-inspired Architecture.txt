UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Creating Perceptual Features Using a BAM-inspired Architecture
Permalink
https://escholarship.org/uc/item/0508j6hj
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Giguere, Gyslain
Chartier, Sylvain
Proulx, Robert
et al.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               Creating Perceptual Features Using a BAM-inspired Architecture
                                 Gyslain Giguère (giguere.gyslain@courrier.uqam.ca)
                                      UQÀM, Département de psychologie, A/S LEINA,
                                          C.P. 8888, Succ. CV, Montréal, Qc, H3C 3P8
                                     Sylvain Chartier (chartier.sylvain@gmail.com)
                                       Université d’Ottawa, Département de psychologie,
                                            550 Cumberland, Ottawa, Ont, K1N 6N5
                                        Robert Proulx (proulx.robert@uqam.ca)
                                      UQÀM, Département de psychologie, A/S LEINA,
                                          C.P. 8888, Succ. CV, Montréal, Qc, H3C 3P8
                                      Jean-Marc Lina (jean-marc.lina@etsmtl.ca)
                               École de Technologie Supérieure, Département de génie électrique
                                          C.P. 8888, Succ. CV, Montréal, Qc, H3C 3P8
                            Abstract                              statistical solution is to use a principal component analysis
                                                                  (PCA) approach (Abdi, Valentin & Edelman, 1998). PCA
  In this paper, it shown that the Feature-Extracting             neural networks (Diamantaras & Kung, 1996) can deal quite
  Bidirectional Associative Memory (FEBAM) can create its
  own set of perceptual features. Using a bidirectional           effectively with input variability by creating (or
  associative memory (BAM)-inspired architecture, FEBAM           “extracting”) statistical low-level features representing the
  inherits properties such as attractor-like behavior and         data’s intrinsic information to a certain degree.
  successful processing of noisy inputs, while being able to      Unfortunately, these networks, being feedforward by
  achieve principal component analysis (PCA) tasks such as        definition, cannot replicate attractor-like (or “categorical”
  feature extraction. The model is tested by simulating
  prototype development in a noisy environment. Simulations       behavior). Also, the absence of explicitly depicted
  show that the model fares particularly well compared to         connections in the model forces supervised learning to be
  current neural PCA and independent component analysis           done using an external teacher. Moreover, some PCA
  (ICA) algorithms. Therefore, it is argued that the model        models cannot even perform online or local learning (for an
  possesses more cognitive explanative power than any other       extensive review, see Diamantaras & Kung, 1996).
  PCA/ICA algorithm or BAMs taken separately.
                                                                     A class of neural networks known as recurrent
  Keywords: Perceptual feature creation; Neural networks;         autoassociative memory (RAM) models does respond to
  Bidirectional associative memory; PCA; ICA.                     these requirements. One characteristic of RAMs is the use
                                                                  of a feedback loop, which allows for generalization to new
                         Introduction
                                                                  patterns, noise filtering and pattern completion, among other
   Cognitive psychologists have historically taken for            uses (Hopfield, 1982). Feedback enables a given network to
granted that the human cognitive system uses sets of              shift progressively from an initial pattern towards an
properties (or dimensions) to achieve perceptual and              invariable state, namely, an attractor. Since the information
cognitive tasks (Garner, 1974). While most researchers view       is distributed among the units, the network’s attractors are
these properties and dimensions as symbolic (see Murphy,          global. If the model is properly trained, the attractors should
2002), some have argued for perceptual symbol or feature          then correspond to the learned patterns (Hopfield, 1982).
systems, in which properties would be defined in an abstract         Direct generalization of RAM models is the development
iconic fashion and self-created in major part by associative      of bidirectional associative memory (BAM) models (Kosko,
bottom-up processes (Harnad, 1990; Schyns, Goldstone &            1988). BAMs can associate any two data vectors of equal or
Thibaut, 1998). To achieve this “perceptual feature               different lengths (representing for example, a visual input
creation” scheme, humans are sometimes hypothesized to            and a prototype or category). These networks possess the
implicitly use an information reduction strategy, in order to     advantage       of    being     both    autoassociative     and
form cognitive representations that can be processed more         heteroassociative memories (Karhunen, Pajunen & Oja,
efficiently. While the idea of a system creating its own          1998), and therefore encompass both unsupervised and
“atomic” perceptual representations has been argued to be         supervised learning.
crucial to understanding many cognitive processes (Schyns,           A problem with RAM/BAM models is that contrarily to
Goldstone & Thibaut, 1998), few efforts have been made to         what is found in real-world situations, they store
model perceptual feature creation processes in humans.            information using noise-free versions (prototypes) of the
   When faced with the task of modeling this strategy, one        input patterns. In comparison, to overcome the possibly
                                                             1025

infinite number of stimuli (exemplars) stemming, for                 maximizes preservation of relevant dimensions, then the
instance, from multiple perceptual events, humans must               reverse operation:
regroup these unique noisy inputs into a finite number of             xˆ = W T y                                                (2)
stimuli or prototypes. This type of learning in the presence of
                                                                     should produce a result that is close to the original input.
noise should therefore be taken into account by RAM/BAM
                                                                     Thus, the norm of the difference vector x − xˆ should be
models.
                                                                    minimized. This translates by finding a W which
   Unfortunately, few working solutions have been proposed
                                                                    minimizes:
to overcome this situation. The most popular solution consists
                                                                             N                     N
in using an unlearning procedure (e.g. Christos, 1996;
                                                                           ∑                     ∑ x −W
                                                                                           2                      2
                                                                                                            T
Hopfield, Feinstein & Palmer, 1983; Yen & Michel, 1996).              E=         xi − xˆ i    =         i     Wxi               (3)
                                                                            i =1                  i =1
The rationale is that if a given noisy input is composed of a
stimulus (x) and some noise (+n), then by “unlearning” the          where E is the error function to minimize, and N is the
noise (-n) we should only retrieve the stimulus. This                input’s dimension. Several solutions exist for Equation 3
procedure results in good performances, but the noise                (for a review, see Diamantaras & Kung, 1996).
proportion must be known beforehand in order to set the ratio
of learning/unlearning trials accordingly. Moreover, other                                   x1                 x1y1
parameter values must also be determined a priori, and the
                                                                                             x2
procedure implies the inversion of learning parameters, which                                                     y2
renders the model quite inconsistent.
   To sum up, on one hand, PCA models can deal with noise                                    xm
but do not encompass the properties of attractor networks. On                                                     yn
the other hand, BAM models exhibit attractor-like behavior                                             W
but have difficulty dealing with noise without the addition of
free parameters, which must be tuned a priori in order to            Figure 1: General neural PCA architecture.
accomplish the task.
   Recently, Chartier, Giguère, Renaud, Lina & Proulx (in                Generalization to Nonlinear PCA (Karhunen, Pajunen &
press) have shown that the Feature-Extracting Bidirectional         Oja, 1998) is straightforward; in this case, a nonlinear
Associative Memory (FEBAM) is an adequate candidate to              output function is to be used:
unify both classes of networks under the same framework.
They showed that FEBAM can perform competitively in                   y = f ( Wx )                                              (4)
image reconstruction and blind source separation tasks, tasks            The corresponding minimization function is thus given by
that are exclusively associated with PCA/ICA networks.
                                                                             N
                                                                           ∑ x −W
However, the model’s ability to conciliate feature extraction                                             2
                                                                      E=          i
                                                                                           T
                                                                                              f ( Wxi )                         (5)
with attractor behavior has never been tested.                              i =1
   In the present study, it will be shown that FEBAM can
extract features and develop prototypes in a noisy                       Depending on the type of nonlinear output function,
environment while preserving its attractor properties. As           different types of optimization can be achieved. If the output
previously stated, this “noisy learning” task is a challenge for    function uses cubic or higher nonlinearity, then the network
RAM/BAM networks. FEBAM has already been shown to                   directly relates to independent component analysis (ICA;
exhibit bipolar and continuous-valued attractor-like behavior,      Hyvarinen & Oja, 2000).
supervised and unsupervised learning, as well as feature
extraction. Because these various behaviors would usually be         Bidirectional associative memories
achieved by separate models, it is believed that FEBAM               In Kosko’s original BAM, learning is accomplished using a
possesses a clear advantage in explanatory power for                 simple Hebbian rule, according to the following equation:
perceptual and cognitive modeling over its predecessors.              W = YX T                                                  (6)
                                                                         In this expression, X and Y are matrices representing the
                Theoretical background
                                                                     sets of bipolar pairs to be associated, and W is the weight
                                                                     matrix which, in fact, corresponds to a correlation measure
Principal component analysis and neural nets                         between the input and output. The model uses a recurrent
PCA neural networks (Figure 1) are based on a linear output          nonlinear output function in order to allow the network to
function defined by:                                                 filter out the different patterns during recall. The nonlinear
 y = Wx                                                     (1)      output function typically used to recall noisy patterns in
where x is the original input vector, W is the stabilized            BAM networks is expressed by the following equations:
weight matrix, and y is the output. When the weight matrix
                                                                1026

 y (t + 1) = sgn( Wx(t ))                                       (7)        In the context of feature extraction, the W layer is used to
and                                                                     extract features whose dimensionality is determined by the
 x(t + 1) = sgn( W T y (t ))                                    (8)     number of y units; the more units in that layer, the less
                                                                         compression occurs. If the y layer’s dimensionality is
where y(t) and x(t) represent the vectors to be associated at
                                                                        greater or equal to that of the x layer, then no compression
time t, and sgn is the signum function defined by
                                                                         occurs, and the network behaves as autoassociative
            ⎧ 1 if z >0                                                  memory2.
            ⎪                                                   (9)
sgn( z ) = ⎨ 0 if z =0
            ⎪-1 if z <0
            ⎩
    Many enhancements, which increase the BAM’s
modeling capacities and range of applications, have been
proposed over the years; these allow, for instance, for
learning convergence, online learning, progressive recall,
and processing of multi-valued stimuli in addition to bipolar
(binary) stimuli (Chartier & Boukadoum, 2006a; Costantini,
Casali & Perfetti, 2003; Wang, Hwang & Lee, 1996; Zhang
& Chen, 2003).
                                                                         Figure 3: FEBAM network architecture.
                        Model description
Architecture                                                                                 x(0)            W              y(0)
                                                                                             x(1)             V
                                                                                                             W              y(1)
                                                                        Figure 4: Output iterative process used for learning updates.
                                                                         Output function
                                                                           The output function is expressed by the following
                                                                        equations:
                                                                                                     ⎧        1, If Wxi (t ) > 1
      Figure 2: Architecture of the original BAM.                                                    ⎪
                                                                         ∀ i, ..., N , y i (t + 1) = ⎨      −1, If Wxi (t ) < −1            (10)
                                                                                                     ⎪(δ + 1) Wx (t ) − δ ( Wx)3 (t ), Else
FEBAM’s architecture is based on the BAM architecture                                                ⎩           i                i
proposed by Hassoun (1989) and Chartier & Boukadoum                        and
(2006a) (Figure 2). It consists of two Hopfield-like neural
networks interconnected in head-to-toe fashion (Hassoun,                                             ⎧        1, If Vy i (t ) > 1
                                                                                                     ⎪
1989). When connected, these networks allow a recurrent                  ∀ i, ..., M , xi (t + 1) = ⎨       −1, If Vy i (t ) < −1           (11)
flow of information that is processed bidirectionally. As                                            ⎪(δ + 1)Vy (t ) − δ (Vy )3 (t ), Else
                                                                                                     ⎩           i              i
shown in Figure 2, the W layer returns information to the V
layer and vice versa. As in a standard BAM, each layer                  where N and M are the number of units in each layer, i is the
serves as a teacher for the other layer, and the connections            index of the respective vector element, y(t+1) and x(t+1)
are explicitly depicted in the model1. However, to enable a              represent the network outputs at time t + 1, and δ is a
BAM to perform PCA, one set of those explicit connections               general output parameter that should be fixed at a value
must be removed (Figure 3). Thus, in contrast with the                  lower than 0.5 to assure fixed-point behavior (Chartier &
standard BAM architecture, y(0), the initial output is not              Proulx, 2005). Figure 5 illustrates the shape of the output
obtained externally, but is instead acquired by iterating once          function when δ = 0.4.
through the network as illustrated in Figure 4.
    1                                                                      2
      In opposition, the architecture of multi-layer Perceptrons             Contrarily to deep belief networks (Hinton & Salakhutdinov,
strictly illustrates a series of input-output relationships, without    2006), the model possesses no bias units, no backpropagation
ever specifying the origin of the “teacher”’s information. This is      training algorithm, no special parameters for learning and uses no
less desirable in a biopsychological perspective.                       stochastic processes.
                                                                    1027

                                                                         Methodology The learning stimuli were 26 binary-valued
                                                                         7x7 images representing lowercase versions of letters of the
                                                                         alphabet (Figure 10a). For each letter, white pixels were
                                                                         given a value of -1, and black pixels a value of 1. Before a
                                                                         given letter was presented to the network, a random vector
                                                                         was added to it. This vector was normally distributed with a
                                                                         mean of 0 and standard deviation of 0.2. Hence, the network
                                                                         never saw exactly the same letter twice during learning.
Figure 5: Output function for δ = 0.4.                                   Figure 6 illustrates some noisy examples of the letter “a”.
                                                                         Although the noise amount is small, it is sufficient to make
   In contrast to a sigmoid output function, FEBAM’s output              the weight connections grow to infinity in standard
function shows no asymptote at the ±1 values. This output                RAM/BAM networks (Bégin & Proulx, 1996).
function possesses the advantage of exhibiting continuous-
valued (or gray-level) attractor behavior (for a detailed
example see Chartier & Boukadoum, 2006a). Such
properties contrast with networks using a standard nonlinear
output function, which can only exhibit bipolar attractor
behavior (e.g. Kosko, 1988).
                                                                         Figure 6: Different examples of the “a” letter input after a
Learning function                                                        random vector N~(0;0.2) is added.
   Learning is based on time-difference Hebbian association
                                                                              For learning, weights were randomly initialized with
(e.g. Chartier & Proulx, 2005; Kosko, 1990; Sutton, 1988),
                                                                         values ranging from -1 to 1. The learning and output
and is formally expressed by the following equations:
                                                                         parameters were respectively set to η = 0.005 and δ = 0.1.
 W(k + 1) = W(k ) + η (y (0) − y (t ))(x(0) + x(t ))T        (12)        Learning followed this general procedure:
and
                                                                         1- Random selection of an input vector;
 V (k + 1) = V (k ) + η (x(0) − x(t ))(y (0) + y (t ))T      (13)       2- Iteration (one cycle, as illustrated in Figure 4) through
where η represents a learning parameter, y(0) and x(0), the                   the network using the output function (Equations 10 and
initial patterns at t = 0, y(t) and x(t), the state vectors after t           11);
iterations through the network, and k the learning trial. The           3- Weight updates (Equations 12 and 13);
learning rule is thus very simple, and can be shown to                  4- Repetition of 1 until the desired number of learning
constitute a generalization of hebbian/anti-hebbian                           trials has been completed or the squared error between
correlation in its autoassociative memory version (Chartier                   y(0) and y(t) is sufficiently small.
& Proulx, 2005).
   For weight convergence to occur, η must be set                           FEBAM’s performance was compared with that of a
according to the following condition (Chartier & Proulx,                 neural linear PCA model (Diamantaras & Kung, 1996), a
2005):                                                                   neural nonlinear PCA model (Karhunen, Pajunen, & Oja,
                                                                         1998) and an ICA model (Hyvarinen & Oja, 2000). Finally,
                 1
η<                            , δ≠     1
                                         2                   (14)        the network’s generalization ability was tested by
      2(1 − 2δ ) Max[ N , M ]                                            generating new noisy stimuli and verifying if the network’s
Equations 12 and 13 show that the weights can only                       ability to correctly recall the associated prototypes.
converge when “feedback” is identical to the initial inputs
                                                                         Results Figure 7 illustrates the various prototypes
(that is, y(t) = y(0) and x(t) = x(0)). The function therefore
                                                                         developed in function of the number of features extracted. If
correlates directly with network outputs, instead of
                                                                         the network is limited to 10 features (out of a 49-dimension
activations. As a result, the learning rule is dynamically               stimuli), then it cannot correctly abstract the noise-free
linked to the network’s output (unlike most BAMs).                       versions. If there are 26 abstracted features, then the
                                                                         network closely matches the noise-free inputs. Although
                           Simulation                                    theoretically this number should be sufficient to accomplish
                                                                         the task, the network is not able to attain perfection, since
Prototype development in a noisy environment                             different weight combinations may converge to the same
The simulation aimed at evaluating the network’s capacity                solution. Thus, by allowing redundancy in the results, the
to acquire multiple prototype representations by extracting              network needs to extract more features. For example, if the
perceptual features from a set of noisy input patterns.                  network is allowed to develop 40 features, then it is able to
                                                                    1028

correctly abstract the 26 noise-free inputs3. In this last            developed by the network. By combining these features, the
situation, after 2000 trials, the squared error (Figure 8) is         network is able to reconstruct the prototypes correctly.
stabilized to an average of 0.38 (The error cannot be brought         Using a basis of 40 features, FEBAM was compared with
lower because of stimulus variability).                               different models that also extracted 40 features (Figure 10).
   Using       the   weights,     detection     feature   maps        Clearly, FEBAM is the only model that was able to
            5
( DV j = ∑ ( Wi Vij ) and DWij = W T ij Vij ) were created to         reconstruct the noise-free inputs using 40 extracted features.
          i =1                                                        In addition, following learning, FEBAM was also able to
illustrate what each unit in each layer is tuned for (Figure 9).      correctly perform a heteroassociation task (Kosko, 1988),
                                                                      where each lowercase letter was to be associated with its
Number of                                                             uppercase counterpart (Figure 10f), using the BAM
                         Prototypes developed                         architecture illustrated in Figure 2. Figure 11 shows an
  features
                                                                      example of the noisy letters “b” and “c” that are
     10                                                               progressively iterated through the network until
                                                                      convergence at the associated attractor (letter).
                                                                      Consequently, FEBAM still preserves its attractor properties
     26
                                                                      in addition to some PCA properties.
     40                                                                  (a)      Noise-
                                                                                   free
                                                                                  inputs
Figure 7: Prototypes abstracted during learning in function
                                                                         (b)
of the number of features.                                                        APEX
                                                                         (c)
                                                                                  NPCA
                                                                         (d)
                                                                                 fastICA
                                                                         (e)
                                                                                 FEBAM
                                                                         (f)     Hetero-
                                                                               association
Figure 8: Squared error in function of the number of
learning trials.
                                                                      Figure 10: (a) Gaussian noise was added to each of these
                                                                      original images for learning purposes. (b-e) Simulation
                                                                      results using 40 extracted features. For each model,
                                                                      prototypes abstracted during learning are shown. Results
                                                                      for: (b) APEX, a linear PCA neural network; (c) a nonlinear
                                                                      PCA network; (d) fastICA, an ICA algorithm; (e) FEBAM,
                                                                      a nonlinear PCA-BAM network. (f) Results from an
                                                                      heteroassociation simulation using FEBAM.
                   (a)                           (b)
Figure 9: Feature detection performed by each of (a) the 49
units of layer V and (b) the 40 units of layer W.
                                                                      Figure 11: Examples of noisy recall. (a) For each letter, the
   Figure 9a illustrates what the V layer is tuned for.
                                                                      first line represents iterations 1 to 3, and the second line
Seemingly, the fuzzy black dots show that this layer
                                                                      represents iterations 4 to 6. As seen, from iteration to
responds in function of the position of spatially salient traits.
                                                                      iteration, the outputs converge to the original input patterns.
The W layer (Figure 9b) represents the 40 different features
                                                                                               Discussion
   3
     Of course, because this is not a data compression task,
                                                                          In this paper, it was shown that the BAM introduced by
speaking of computational savings becomes irrelevant.
                                                                 1029

Chartier & Boukadoum (2006a) uses a learning function               associative memory storing gray-coded gray-scale
that is closely related to nonlinear PCA networks.                  images. IEEE Transactions on Neural Networks, 14, 703-
Consequently, by simply removing a set of external                  707.
connections from the original BAM architecture, FEBAM is         Diamantaras, K.I., Kung, S.Y. (1996). Principal Component
able to perform feature extraction and reduction of the input       Neural Networks. New York: Wiley.
dimensionality, just like PCA networks do. Because the W         Garner, W.R. (1974). The processing of information and
and V layers are connected, it is possible to observe what          structure. New York: Wiley.
the developed attractors in the V layer look like. Leading to    Harnad, S. (1990). The symbol-grounding problem. Physica
attractor-like behavior, the network can thus be used to            D, 42, 335-346.
perform       prototype    development     from     exemplar     Hassoun, M.H. (1989). Dynamic heteroassociative neural
presentations.                                                      memories. Neural Networks, 2, 275-287.
   Contrarily to the other models presented in this paper,       Hinton, G. E., Salakhutdinov, R. R. (2006). Reducing the
FEBAM, being a special case of BAM, can also be used to             dimensionality of data with neural networks. Science,
simulate other applications such as categorization (Chartier        313(5786), 504-507.
& Proulx, 2005), classification (Chartier & Boukadoum,           Hopfield, J.J. (1982). Neural networks and physical systems
2006a), many-to-one association and multi-step pattern              with emergent collective computational abilities.
recognition (Chartier & Boukadoum, 2006b). Other                    Proceedings of the National Academy of Sciences, 79,
simulations (Chartier, Giguère, Renaud, Lina & Proulx, in           2554-2558.
press) have also shown that the network can achieve feature      Hopfield, J. J., Feinstein, D. I., Palmer, R. G. (1983).
extraction and dimensionality reduction from gray-scale             Unlearning has a stabilizing effect in collective memories.
images, as well as blind source separation of noisy signal          Nature, 304, 158-159.
mixtures. Consequently, in addition to being a superior          Hyvarinen, A., Oja, E. (2000). Independent component
candidate for modeling human perceptual feature creation,           analysis: algorithms and applications, Neural Networks,
the model possesses more cognitive explanative power than           13, 411-430.
any other nonlinear/linear PCA or ICA algorithm.                 Karhunen, J., Pajunen, P., Oja, E. (1998). The nonlinear
                                                                    PCA criterion in blind source separation: Relations with
                         References                                 other approaches. Neurocomputing, 22, 5-20.
Abdi, H., Valentin, D., Edelman, B.G. (1998). Eigenfeatures      Kosko, B. (1988). Bidirectional associative memories. IEEE
   as intermediate-level representations: The case for PCA          Transac. on Systems, Man and Cybernetics, 18,49-60.
   models. Brain and Behavioral Sciences, 21, 17-18.             Kosko, B. (1990). Unsupervised learning in noise. IEEE
Bégin, J., Proulx, R. (1996). Categorization in unsupervised        Transactions on Neural Networks, 1, 44-57.
   neural networks: the Eidos model. IEEE Transactions on         Murphy, G.L. (2002). The big book of concepts. Cambridge,
   Neural Networks, 7(1), 147-154.                                  MA: MIT Press.
Cardoso, J.F. (1998). Blind signal separation: statistical        Schyns, P.G., Goldstone, R.L., Thibaut, J.P. (1998). The
   principles. Proceedings of the IEEE, 86, 2009-2025.              development of features in object concepts. Brain and
Chartier, S., Boukadoum, M. (2006a). A bidirectional                Behavioral Sciences, 21, 1-54.
   heteroassociative memory for binary and grey-level             Sutton, R.S. (1988). Learning to predict by the methods of
   patterns. IEEE Transactions on Neural Networks, 17,              temporal difference. Machine learning, 3, 9-44.
   385-396.                                                       Wang, C.C., Hwang, S.M., Lee, J.P. (1996). Capacity
Chartier, S., Boukadoum, M. (2006b). A sequential dynamic           analysis of the asymptotically stable multi-valued
   heteroassociative memory for multistep pattern                   exponential bidirectional associative memory. IEEE
   recognition and one-to-many association. IEEE                    transactions on systems, Man and Cybernetics - Part B:
   Transactions on Neural Networks, 17, 59-68.                      Cybernetics, 26, 733-743.
Chartier, S., Giguère, G., Renaud, P., Lina, J.M., Proulx, R.     Yen, G. G., Michel, A. N. (1996). Unlearning algorithm in
   (2007, in press). FEBAM : a feature-extracting                   associative memory. IEEE Transactions on Circuits and
   bidirectional associative memory. Proceedings of the 20th        Systems-II: Analog and Digital Signal Procesing, 43,
   International Joint Conference on Neural Networks.               723-729.
Chartier, S., Proulx, R. (2005). NDRAM: nonlinear                 Zhang, D., Chen, S. (2003). A novel multi-valued BAM
   dynamic recurrent associative memory for learning                model with improved error-correcting capability. Journal
   bipolar and nonbipolar correlated patterns. IEEE                 of electronics, 20, 220-223.
   Transactions on Neural Networks, 16, 1393-1400.               Zurada, J.M., Cloete, I., van der Peol, E. (1996).
Christos, G. A. (1996). Investigation of the Crik-Mitchison         Generalized Hopfield networks for associative memories
   reverse-learning dream sleep hypothesis in dynamical             with multi-valued stable states. Neurocomputing, 13, 135-
   settings. Neural Networks, 9, 427-434.                           149.
Costantini, G., Casali, D., Perfetti, R. (2003). Neural
                                                             1030

