UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Ethical System Formalization using Non-Monotonic Logics
Permalink
https://escholarship.org/uc/item/3876p4qw
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Author
Ganascia, Jean-Gabriel
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                     Ethical System Formalization using Non-Monotonic Logics
                                    Jean-Gabriel Ganascia (Jean-Gabriel.Ganascia@lip6.fr)
                                      LIP6, University Paris VI, 104 avenue du Président Kennedy
                                                            75016, Paris, FRANCE
                               Abstract                                   question. Kant’s position was that one should always tell the
                                                                          truth (Kant, 1996), even in such a situation, while Constant
   Ethics is the science of duty, i.e. the science that elucidates        (Constant, 1988) considered that morals are based on many
   the rules of the right behavior. Nevertheless, it seems that
   the way we rule our lives is intuitive and based on common             principles and that, consequently, one should always apply
   sense. For instance, it is common to say that ethical rules are        the one that is the most adapted to the situation. The opposi-
   default rules, which means that they tolerate exceptions. Some         tion between “moral generalism” and “moral particularism”
   authors argue that moral can only be grounded on particular
   cases while others defend the existence of general principles          corresponds to an old opposition between written laws and
   related to ethical rules. Our purpose here is not to justify the       the cases on which the laws are based. The criticism of moral
   first or the second position, but to try to model ethical sys-         values based on general statements or laws or rules is that
   tems using artificial intelligence formalisms. More precisely,
   this is an attempt to show that progress in non-monotonic log-         they may be correct in theory, but not applicable to all prac-
   ics, which simulate common sense reasoning, provides a way             tical cases. The example above illustrates the difficulty of
   to formalize different ethical conceptions. From a technical           applying a rigid and general law to particular cases.
   point of view, the model developed here makes use of the An-
   swer Set Programming (ASP) formalism. It is applied to com-               Formalization of ethical systems using modern artificial in-
   pare different ethical systems with respect to their attitude to-      telligence techniques may be an original way of overcom-
   wards lying and could help to extend classical philosophy and          ing the opposition between “moral particularism” and “moral
   to define general conditions required by any ethical system.
   Keywords: Answer Set Programming (ASP); Common Sense                   generalism”. For instance, in the case of lying, default rules
   Reasoning; Computational Ethics; Machine Ethics; Intelligent           with justified exceptions could be used to satisfy a general
   Agents; Non Monotonic Logics                                           rule or principle that prohibits lying, while simultaneously
                                                                          recommending telling a lie in a given particular situation
                           Introduction                                   where the truth would violate other rules of duty.
Ethics is the science of human duty (Cf. (Webster, 1913)).                   This paper, which is divided in five parts, constitutes an at-
As a science, it has to elucidate the body of rules on which              tempt to model three classical ethical systems using the An-
we have to determine our behavior. In this respect, an ethical            swer Set Programming formalism (ASP) (Baral, 2003). The
system can be viewed as a decision-making procedure based                 first rapidly recalls the ASP semantics and indicates the way
on statements on which almost all of us agree. However,                   ethical systems can be modeled with this formalism. The fol-
in the philosophical tradition, the origin and nature of these            lowing three parts consider the formalization of three clas-
rules have always been considered to be controversial. For in-            sical ethical systems, i.e. the Aristotelian, the Kantian and
stance, some authors think that ethical rules are default rules           Constant’s “Theory of Principles”, using the ASP formalism.
(Väyrynen, 2004), which means that they tolerate exceptions,             Each formalization leads to a program expressed in AnsPro-
while others disagree: some have argued that morals can only              log* and is illustrated with an application based on the ly-
be based on singular cases (Harman, 2005) while others have               ing example referred to above. Mainly dedicated to the Con-
defended the existence of general principles (Kant, 1997);                stant’s “Theory of Principle”, the fourth part goes also on to
some judge an action in terms of its consequences, others in              extend classical Kantian ethics by defining, within this frame-
terms of the law, etc. Many of these debates concern the op-              work, the general conditions that are required by any ethical
position between those who think that principles are many in              system. The last part opens up future research in computa-
numbers and can be contradictory, since they are derived from             tional ethics based on the generalization of the approach de-
experience, while others say that morals have to be based on              scribed here.
general rules, which are valid everywhere and all the time.
   To be more precise, one of the arguments in favor of the
                                                                                                 ASP Formalism
first position, i.e. “moral particularism”, is that ethics has to         The ASP Semantic
refer to each particular situation and cannot be based on gen-            In the past, many Artificial Intelligence researchers tried to
eral principles. Imagine, for instance, that you were living              simulate non-monotonic reasoning, i.e. reasoning based on
in occupied France during the Second World War and that                   general rules and accepting exceptions. Several formalisms
you hid a friend who was wanted by the French militia or the              have been developed, for instance, default logic (Reiter,
Gestapo, in your home. If you were asked where your friend                1980), circumscription (McCarthy, 1980), non-monotonic
was, would you obey the general rule that commands you to                 logics (McDermott & Doyle, 1980), Truth Maintenance Sys-
tell the truth, and denounce the man to the authorities? In the           tems, etc.
18th century, there was a discussion between Immanuel Kant                   However, most of the mechanical solvers based on those
(1724–1804) and Benjamin Constant (1767–1830) about this                  formalisms were very inefficient. Recently, a new general
                                                                     1013

formalism called ASP (Baral, 2003) has been developed to                      founded mathematical theories. There is no doubt that such
simulate non-monotonic reasoning. It has been designed to                     attempts are very fruitful and interesting. However, the goal
unify previous non-monotonic reasoning formalisms. ASP                        here is different, since it is not a question of basing morals
formalism is not only a more recent formalism; it is also more                on simulation, but of understanding the underlying logic on
general than others, since it emulates almost all of them and                 which classical ethical systems rely. In the last few years,
it is fully operational. More precisely, ASP proposes both                    there have been some attempts to formalize ethical systems
a clear formalization with a well-defined semantics and effi-                 using modal logic formalisms (Gensler, 1996) and to opera-
cient operational solvers, which renders automate demonstra-                  tionalize these formalizations on computer (Bringsjord, Ark-
tions possible.                                                               oudas, & Bello, 2006; Powers, 2005). However, these for-
   Within this formalization, it is possible to specify the log-              malizations are mainly based on the use of deontic logics
ical properties of objects with programs Π that are sets of                   (Meyer J.-J. Ch., 1994) that are well adapted to ethical sys-
expressions ρ of the following form:                                          tems focused on laws were permission and prohibitions are
ρ : L0 or L1 or . . . or Lk ← Lk+1 , . . . , Lm , not Lm+1 , . . . , Ln .     well defined, but not to consequentialist ethical systems. Us-
where Li are literals, i.e. atoms or atom negations, and not is               ing non-monotonic logics (cf. (Powers, 2006)) or ASP offers
a logical connective called “negation as failure”.                            a more general approach, since it can describe not only the
   The intuitive meaning of such a rule is that for all Herbrand              consequentialist ethical systems but also deontic ones as it
interpretations that render true all literals in {Lk+1 ,. . . , Lm }          can represent modal and deontic logics.
while not satisfying any literals in {Lm+1 ,. . . , Ln } one can de-             In order to show this, three ethical conceptions have been
rive at least one literal in {L0 ,. . . , Lk }                                formalized: the classical Aristotelian one, the Kantian one
   Let us remark that ASP formalism contains two negations                    based on the categorical imperative and Constant’s theory that
that need to be distinguished: a classical negation noted “¬”                 authorizes a great number of principles tolerating exceptions.
and a negation by failure noted “not”, which means that a                     Each model is illustrated using the dilemma of the lie pre-
literal cannot be proved in the absence of sufficient informa-                sented in the introduction.
tion. The non-monotonic properties are mainly due to this
“negation as failure” connector.                                                                    Aristotelian Rules
   Being given a program Π, an Answer Set (or a stable                        A Decision-Making Procedure
model) is a minimal subset of the Herbrand base of Π, which
satisfies all rules of Π. Each subset describes a possible world              According to the traditional Aristotelian ethics (Aristotle,
that renders true the rules of Π. Let us note that this intuitive             2002), in each situation we have to look at all possible ac-
meaning of the programs may be easily formalized, which                       tions and to choose the best one, i.e. the least unjust. More
provides a formal fixpoint semantics of ASP.                                  precisely, our will — i.e. our goal — can be achieved by
                                                                              choosing the appropriate action among the different actions
Modeling Ethical Systems with ASP                                             we have at our disposal. In modern terms, Aristotelian ethics
Ethical rules are rules of behavior, i.e. rules that help to de-              can be reduced to a general decision-making procedure based
cide what to do and what not to do. Therefore, any ethi-                      on preferences that characterize the just and the unjust. Us-
cal system, i.e. any consistent set of ethical rules, requires                ing ASP formalism, this can be expressed using the following
defining a decision-making procedure; this paper claims that                  rules1 :
these procedures can be described using artificial intelligence
techniques. Since a logical description helps to clarify the                    act(P, G, A) ← action(A), person(P), goal(P, G),
ideas and to highlight differences between different ethical                     solve goal(P, G, A), not un just(A).
systems, the aim is not just try to simulate ethical reasoning                  ← action(P, G, A), action(P, G, AA), A 6= AA.
using classical AI techniques, but to describe these decision-
making procedures in a purely declarative way, using modern                  The just and the unjust are defined with the use of two binary
logic-based AI techniques. Moreover, since ethical reason-                   predicates, worse(A, B), which means that action A is worse
ing is a kind of common sense reasoning, it justifies the use                 than action B, and consequence(A,C), which means that C
of non-monotonic logic. Lastly, ASP techniques have been                      is a consequence of A. Briefly speaking, an action A is just
chosen because they seem appropriate for such a model. The                    if its worst consequences are not worse than those of other
existence of solvers makes is easy to validate our models in                  actions AA. More formally, it can be characterized using the
different situations.                                                        following ASP rules:
   Note that, in the past, there were many attempts to base
ethics on empirical principles, i.e. on observations according,                 just(A) ← worst consequence(A,C),
for instance, to the observed utility, to common uses or to tra-                 worst consequence(AA,CC), worse(CC,C), not un just(A).
ditions. Over the last few years, philosophers have used arti-
ficial intelligence techniques, and more specifically statistical                 1 All these formalizations have been coded in AnsPro-
learning theory (Harman, 2005) or game theory (Braithwaite,                   log* and tested using the smodels solver downloaded from
1955), to model these processes using computers and/or well-                  http://www.tcs.hut.fi/Software/smodels.
                                                                         1014

  un just(A) ← worst consequence(A,C),                                    a rule could be added saying that a lie is better than a mur-
   worst consequence(AA,CC), worse(C,CC), not just(A).                    der: better(tell(P, lie), murder). Our formalization shows
                                                                          that adding such an axiom removes all the answer sets where
The worst consequence is easy to define using the following               act(“I”, question(“I”),tell(“I”,truth)) is true. However, no
two rules once both the worse and the consequence predicates              general principle exists on which such ethical preferences can
have been given:                                                          be based: the preference between the murder and the lie has
  not worst consequence(A,C) ←                                            to be explicitly mentioned, without any justification. The goal
   consequence(A,C), consequence(A,CC),                                   of the Kantian ethical system (Kant, 1998) is to find formal
   worse(CC,C), not worse(C,CC).                                          justifications on which the just and the unjust are founded.
  worst consequence(A,C) ← consequence(A,C),                                                     Kantian Ethics
   not not worst consequence(A,C).                                        Kant wanted to find the formal foundations of ethics without
                                                                          any reference to a particular system of beliefs, e.g. revelation,
The predicate consequence translates physical causality. It is            economy, etc. In so doing, he rejected the notions of just and
a pre-requirement that ethical agents have at their disposal an           un just used in traditional ethics. From a formal point of view,
adequate knowledge of the world. This means that science                  the predicates worse, better and worst consequence also has
and improvement of knowledge contribute to ethics. How-                   to be removed as they are now useless, since they are based
ever, science is not sufficient and a second predicate, worse,            on an implicit theory of value.
is also required. This predicate expresses a system of values                As a consequence, the general principle according to which
that depends on the culture, social environment or personal               we have to act justly, i.e.
commitment of the agent. The aim here is not to justify such
or such system of values, e.g. utilitarian, Epicurean, religious,           act(P, G, A) ← action(A), person(P), goal(P, G),
idealistic, and it is assumed that it has already been specified              solve goal(P, G, A), not un just(A).
through the worse predicate.
                                                                           has to be changed into:
Aristotle and the Lie
                                                                            act(P, G, A) ← action(A), person(P), goal(P, G),
This general formalization can be tested on the lie example.                  solve goal(P, G, A), maxim will(P, G, A).
Let us first suppose that there are three or more persons, “I”,
Peter and Paul, each of whom has several possible actions,                    It means that we are free to adopt any system of maxims
e.g. to tell the truth, to tell a lie, to murder, to eat, to discuss.     we want. The only condition is that it has to obey a formal
Let us now consider a situation similar to the one described              criterion, the so-called “categorical imperative” (Kant, 1997).
above where “I” am in a situation where “I” have to answer
a murderer either by lying or by telling the truth. I know                Kant’s Categorical Imperative
that telling the truth means denouncing a friend, which will              The formal principle on which Kantian ethics are based says
lead to his murder. What should I do? The situation may be                that “I” can conform to any set of rules, which can be gen-
formalized using the following rules:                                     eralized to all the members of a society. According to this
   obliged(P) ← act(P, question(P), A).                                   principle, the values on which I rule my behavior, i.e. the so-
   ← not obliged(“I”).                                                    called “maxim of my will”, may be universalized in an ideal
   consequence(A, A) ← .                                                  society without any contradiction. In the case of the lie, how
   consequence(tell(“I”,truth), murder) ← .                               could we imagine a society where the right to lie would be
   The solution depends on my system of values. Let us now                allowed? According to Kant, such a society would be a night-
suppose that “I” admit that it is bad both to lie and to murder.          mare, since it would not be possible to trust anyone. This may
This can be expressed using the following three rules:                    be expressed using an ASP rule that stipulates that when “I”
   worse(tell(P, lie), A) ← not better(tell(P, lie), A).                   act in such or such way, all persons P could act similarly. In
   worse(murder, A) ← not better(murder, A).                               the case of our example, this can be formalized as follows:
   better(A, A).
   With such a program, half of all the answer sets con-                    maxim o f will(P, question(P),tell(P, S)) ←
tain the decision: act(“I”, question(“I”),tell(“I”,truth))                    maxim o f will(“I”, question(“I”),tell(“I”, S).
which leads to a murder, and half the decision:
act(“I”, question(“I”),tell(“I”, lie)) which prevents a                     ¬maxim will(“I”, question(“I”),tell(“I”,truth)) or
denunciation.                                                                 maxim will(“I”, question(“I”),tell(“I”,truth)) ← .
   This framework does not provide any way to choose be-
tween these two options. If we want to exclude the denun-
ciation, while exceptionally allowing lying, the only pos-                  ¬maxim will(“I”, question(“I”),tell(“I”, lie)) or
sibility is to explicitly add a preference between denuncia-                  maxim will(“I”, question(“I”),tell(“I”, lie)) ← .
tions (when they lead to a murder) and lies. For instance,
                                                                      1015

  ← maxim will(“I”, G,tell(“I”, S)),                                        The first point is that for Kant, a speech act is a public act,
    maxim will(“I”, G,tell(“I”, SS)), S 6= SS.                          whereas for Constant it is a communication act. In practice, it
                                                                        means that the predicate “tell” and the predicate “question”
Rules also have to be added specifying that one may trust at            are respectively ternary and binary predicates that have to ac-
least one person in an ideal society:                                   cept both a transmitter and a receiver as arguments, and not
untrust(P) ← maxim will(P, G,tell(P, lie).                              only a transmitter, as is the case for the Kantian model.
trust(P) ← not untrust(P).                                                  The second point is that default rules have to be used to
ideal world ← not trust(P).                                             formalize Constant’s Theory of Principles, which states that
← not ideal world.                                                      there are many more or less general principles that may con-
                                                                        tradict each other. To formalize this using the ASP formal-
Kant’s Denouncement                                                     ism, it is sufficient to rewrite the act predicate of the Aris-
Coming back to the situation described in the previous sec-             totelian formalization and to replace the not un just literal by
tion, if we replace Aristotelian axioms of choice by the above,         a principle predicate denoting existing principles and then to
this will lead to the conclusion that it is necessary to tell           write the principles with ASP rules as follows:
the truth, even if it leads to denouncing a friend and, con-
sequently, to his murder. If I do not tell the truth, everybody            act(P, G, A) ← action(A), person(P), goal(P, G),
could do the same and I will not be able to trust anyone. To be             solve goal(P, G, A), principle(P, G, A).
more specific, Kant’s categorical imperative does not require
that everybody always tell the truth. It does not indicate pref-           principle(P, question(P, PP),tell(P, PP,truth) ←
erences between different actions but only prevents possible                not not deserve(PP,tell(P, PP,truth).
consequences of an ethical system based on rules that cannot
be universalized. It does not mean that it is impossible to lie:
if someone lies and if I know he is lying, I do not trust him.             principle(P, question(P, PP),tell(P, PP, lie) ←
However, if I accept that there is a right to lie then I am not             not deserve(PP,tell(P, PP,truth).
able to trust anyone, since I have no reason to think that others
are not using this right and therefore that they are not telling
                                                                           not deserve(PP,tell(P, PP,truth) ←
lies. It does not mean that I never lie, but that I cannot accept
                                                                            worst consequence(tell(P, PP,truth),C),
the right to lie as an ethical law. Using our formalization with
                                                                            worse(C,tell(P, PP, lie)).
the help of ASP rules, it is possible to accept that someone is
lying. If I don’t know it, I may be misled; if I know, I will not           Using this formalization, the only generated answer sets
trust him. For instance, a rule could be added specifying that           correspond to the lie, even if it is not explicitly specified that
Peter lies if he knows that the consequence of the truth could           telling a lie is better than denouncing someone. It is also pos-
lead to a murder:                                                        sible to describe the case where someone has no information
                                                                         about the place where the person is hidden, so there is no
  maxim will(peter, question(peter),tell(peter, lie)) ←                  obligation to lie, but just to say everything one knows and no
    consequence(tell(peter,truth), murder).                              more.
The only consequence is that I will not trust him, but I will            Advances in ethics
be able to trust others. In return, if I accept the right to lie as      It is also possible, with computational ethics, to explore new
a rule of my behavior, the consequence is catastrophic, since            ethical perspectives. For instance, Kant’s categorical impera-
in this case I am not able to trust anyone. One advantage of             tive has been defined within a classical logic framework, and
such a principle is its generality. It is not necessary to explic-       its content has been modeled using ASP techniques. We also
itly state ethical preferences among actions, since they derive          have shown different ethical systems such as the Aristotelian
from a formal principle. But its great disadvantage is that              one and Constant’s Theory of Principles. But new questions
many people will not agree with its conclusions, i.e. that you           could be solved within this framework. For instance, it may
have to denounce a friend to whom you are giving hospitality.            also be possible to try extend Kant’s categorical imperative
It is for this reason that we tried to model Constant’s theory.          using non-monotonic logic. More precisely, it might be pos-
                                                                         sible to specify the general conditions under which any sys-
                    Constant’s objection                                 tem of maxims reach at least one solution. In other words,
Constant’s argument is that there are many ethical principles,           one may require that adding to any ethical rule system, a set
which are more or less general. In each situation we have                of general criteria characterizing a harmonious society where
to apply the most specific and the most appropriate one. In              everybody can hope to live and to act freely, e.g. stating that
the case of the lie example, the general principle is that we            men may trust almost anyone without fear of being betrayed,
must always tell the truth. But a more specific principle says           there always exists at least one decision that obeys ethical
that you don’t have to tell the truth to someone who doesn’t             rules in each situation. Therefore, one may define formal con-
deserve it.                                                              ditions e.g. Local stratification (Baral, 2003) under which a
                                                                    1016

system of ethical rules always leads to at least one decision             Even if this paper mainly deals with the computational
satisfying the general criteria that characterize a harmonious         model of ethics, the lie example used to illustrate our mod-
society of men and machines.                                           els is of interest for both computer ethics and computational
   In the case of the lie, this would mean ensuring that, while        ethics: the generalized use of information technologies makes
generalizing the maxim of my will to all members of the so-            all the information about our private lives potentially avail-
ciety, I will always be able to trust someone. Therefore, all          able to everybody. With machine-readable passports and
systems of maxims that can be proved to be consistent – for            electronic ID cards, all international travel is all recorded.
instance that can be proved to be locally stratified – with the        Each time you pay with a credit card, your bank knows what
following requirement are acceptable:                                  you bought and where. Mobile phones and RFID (Radio
   untrust(P) ← maxim will(P, G,tell(P, lie).                          Frequency Identification) tags locate you wherever you are.
   trust(P) ← not untrust(P).                                          Health cards can tell everyone which doctor you visited and
   ideal world ← not trust(P).                                         what treatment you had. Remote sensing data will soon make
   ← not ideal world.                                                  your private garden visible, with a resolution that will make
                                                                       it possible to see what you are doing and with whom. As a
   One of our current projects is to pursue this approach and to
                                                                       consequence, the future information society may become a
revisit Kant’s ethical view in the light of modern logics, espe-
                                                                       society of transparency where everything that is done will be
cially non-monotonic logics. More generally, computational
                                                                       available to all.
ethics may help to define meta-properties that are required by
                                                                          Transparency is good for honest people, who have noth-
any ethical system. In a way, this could help to generalize the
                                                                       ing to hide and there is no reason why such people would
Kantian project, by making it more flexible, more practical
                                                                       hide any of their activities. Knowledge is good for every-
and more open to different cultures.
                                                                       body. On the other hand, some of us think that it is preferable
                                                                       to distinguish the public sphere, which everyone may know
                         Perspectives
                                                                       about and the private one, which is personal. But if so, where
Possible Applications                                                  should the line be drawn between the public and the private?
                                                                       What legitimates that distinction? What defines what we call
This paper is an attempt to model ethical rules using the ASP
                                                                       our privacy? Would it be possible, in a particular situation,
formalism, which allows the simulation of non-monotonic
                                                                       to authorize someone to hide something or to lie? For in-
reasoning. This makes it possible both to formalize ethical
                                                                       stance, when you are in your office working on a project, you
conceptions and to prove the validity of different statements,
                                                                       may want not to be disturbed and will tell everybody that you
in different situations for each of the conceptions. In all cases,
                                                                       have appointments. Is this lie justified? If so, why and when
it helps to clarify ideas and, more generally, it opens up new
                                                                       would it be right? It follows from these questions that the
areas in computational ethics. The applications of compu-
                                                                       legitimacy of lying is an open ethical question that needs to
tational ethics based on ASP formalism are many and var-
                                                                       be re-discussed. We may contribute to the debate with the
ied. The first one is educational; it is easy to teach different
                                                                       help of a clear and relevant formalization thanks to the use of
ethical systems by programming them and by showing how
                                                                       modern artificial intelligence techniques.
they define decision-making procedures. It is also possible to
clearly make explicit each ethical system once it has been pro-        Applications to Computational Ethics
grammed, since it is possible to derive all the practical conse-       Inspired by Asimov’s short story “Runaround” written in
quences, i.e. all the behaviors that it recommends. Moreover,          1942 (Asimov, 1950), computational ethics (Aaby, 2005;
programming ethical systems helps make elicit their implicit           Bringsjord et al., 2006), i.e. ethics for artificial agents, stud-
content; for instance, it is interesting to see that the status of     ies the rules on which robots should base their behavior in or-
speech is different for Kant, for whom it has a potentially uni-       der to be ethically acceptable. For instance web agents have
versal scope, and for Constant who just considers speech as a          to respect privacy; automated hospital agents have to respect
communication act between people.                                      patients and their pain, etc. This article does not directly deal
                                                                       with such questions; however, the way it proposes to model
Computational Model of Ethics versus Computer
                                                                       ethical rules could be useful to design artificial agents, but
Ethics
                                                                       this raises difficult questions. May artificial agents lie? Most
The idea of a computational model of ethics, which is men-             of us would say that they shouldn’t. Having said this, do they
tioned here, has to be distinguished from both computer                have to tell all they know?
ethics and computational ethics, i.e. the ethics of artificial            One of the difficulties we face when writing rules of behav-
agents (Aaby, 2005; Floridi & Sanders, 2004). A compu-                 ior for intelligent agents is that the requirements are many in
tational model of ethics models ethical systems by the use             number and they may be contradictory. For instance, we want
of programs and simulates decision-making procedures using             personal robots to act as faithful dogs that have to defend and
physical information systems, i.e. computers, whereas com-             help their master. Simultaneously, we want and need to pro-
puter ethics deals with the ethical consequences of computer           tect our privacy by restricting access to personal data. But
dissemination.                                                         we also ask the robot to behave ethically, i.e. to tell the truth
                                                                   1017

whenever someone asks them and not to increase information            Braithwaite, R. (1955). Theory of games as a tool for
entropy by divulging incorrect information. These three re-             the moral philosopher. Cambridge: Cambridge University
quirements are somewhat contradictory, since people’s secu-             Press.
rity requires total transparency while personal servants some-        Bringsjord, S., Arkoudas, K., & Bello, P. (2006). Toward
times have to lie to protect their master’s privacy. As a conse-        a General Logicist Methodology for Engineering Ethically
quence, those who claim to be discreet have to obey multiple            Correct Robots (Tech. Rep.). Troy NY 12180 USA: Rens-
and independent principles that may appear to be incompati-             selaer Polytechnic Institute (RPI).
ble.                                                                  Constant, B. (1988). Des réactions politiques. Éditions Flam-
   Last spring, in March 2006, at the AAAI Stanford Spring              marion.
Symposium entitled “What Went Wrong and Why: Lessons                  Floridi, L., & Sanders, J. (2004). On the Morality of Artificial
from AI Research and Applications” there was a session de-              Agents. Minds and Machines, 14.3, 349-379.
voted to intelligent agents. One of the talks presented exper-        Gensler, H. (1996). Formal Ethics. Routledge.
iments with “elves”, which are personal agents that act as ef-        Harman, G. (2005). Moral Particularism and Transduction.
ficient secretaries and help individuals to manage their diary,         Philosophical Issues, 15.
fix appointments, find rooms for meetings, organize travel,           Kant, I. (1996). On a putative right to lie from the love
etc. The talk reported technical success but difficulties with          of mankind, in the metaphysics of morals. In Paperback,
inappropriate agent behavior. For instance, one day, or rather          cambridge texts in the history of philosophy. Cambridge
one night, an elf rang his master at 3am to inform him that his         University Press.
10 o’clock plane was going to be delayed. Another was un-             Kant, I. (1997). Critique of practical reason. In Paperback,
able to understand that his master was in his office for nobody,        cambridge texts in the history of philosophy. Cambridge
since he had to complete an important project... Many of                University Press.
these inappropriate actions make intelligent agents tiresome          Kant, I. (1998). Groundwork of the metaphysics of morals.
and a real nuisance.                                                    In Paperback, cambridge texts in the history of philosophy.
   Our goal is to help in the design of clever and discreet             Cambridge University Press.
agents that act with discernment and good judgment by for-            McCarthy, J. (1980). Circumscription: A form of non-
malizing ethical rules of behavior that use non-monotonic               monotonic reasoning. Artificial Intelligence, 13, 27–39.
logics. But it is difficult to automatically manage inconsistent      McDermott, J., & Doyle, J. (1980). Non-monotonic logic 1.
rules of behavior and to find the one that is the most appro-           Artificial Intelligence, 13, 41–72.
priate for each situation. The notion of “common sense rea-           Meyer J.-J. Ch., W. R., Dignum F.P.M. (1994). The paradoxes
soning” has been developed in artificial intelligence to face a         of deontic logic revisited: a computer science perspective
similar problem. Therefore, our aim is to propose a “common             (Tech. Rep. No. UU-CS-1994-38). Utrecht, Netherlands:
sense ethics” based on “common sense reasoning”, which                  Utrecht University, Department of Computer Science.
could help to design thoughtful intelligent agents. One of the        Powers, T. (2005). Deontological Machine Ethics (Tech.
valuable applications of our logical formalization of ethical           Rep.). Washington, D.C.: American Association of Artifi-
rules would be to design rules of behavior that make robots             cial Intelligence Fall Symposium 2005.
clever.                                                               Powers, T. (2006). Prospect for a Kantian Machine. IEEE
                                                                        Intelligent Systems, 21.4, 46-51.
                           References                                 Reiter, R. (1980). A logic for default reasoning. Artificial
Aaby, A. (2005). Computational Ethics (Tech. Rep.). Walla               Intelligence, 13, 81–132.
   Walla College.                                                     Väyrynen, P. (2004). Particularism and Default Reasoning.
Aristotle. (2002). Nicomachean Ethics. Oxford University                Ethical Theory and Moral Practice, 7, 53-79.
   Press.                                                             Webster. (1913). Webster’s Revised Unabridged Dictionary
Asimov, I. (1950). I, Robot. Gnome Press.                               (N. Porter, Ed.). G. and C. Merriam Co.
Baral, C. (2003). Knowledge Representation, Reasoning and
   Declarative Problem Solving. Cambridge University Press.
                                                                 1018

