UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Assessing the Efficacy of Transitional Probabilities for Learning Syntactic Categories
Permalink
https://escholarship.org/uc/item/8v64d0n7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Conwell, Erin
Balas, Benjamin J.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                     Powered by the California Digital Library
                                                                       University of California

          Assessing the Efficacy of Transitional Probabilities for Learning Syntactic
                                                              Categories
                                           Erin Conwell (Erin_Conwell@brown.edu)
                                                            Brown University
                                     Department of Cognitive and Linguistic Sciences, Box 1978
                                                        Providence, RI 02912 USA
                                               Benjamin J. Balas (bjbalas@mit.edu)
                                                  Massachusetts Institute of Technology
                                    Department of Brain and Cognitive Sciences, 43 Vassar Street
                                                       Cambridge, MA 02139 USA
                              Abstract                                 Artificial language studies demonstrate that learners are
   While research on both infant language abilities and the
                                                                       highly sensitive to the statistics of the language that they
   informativeness of natural language for the formation of            hear and can use that information to find word boundaries
   grammatical categories has advanced considerably, the extent        and learn word classes (Gerken, Wilson & Lewis, 2005;
   to which these two fields inform each other is limited. To          Gómez & Lakusta, 2004; Saffran, Aslin & Newport, 1996).
   address this issue, we ask whether tracking transitional            Models of language learning indicate that the language that
   probabilities, a skill which infants are known to apply to          children hear appears to contain a number of cues that could
   language learning, is useful for learning grammatical               be useful for learning syntactic categories (Mintz, Newport
   categories from natural child-directed speech.             We
   systematically remove subsets of the data to assess the             & Bever, 2002; Monaghan, Chater & Christiansen, 2005;
   relative contributions of several potential sources of              Redington, Chater & Finch, 1998).
   information. Our analysis finds that immediately following a           However, these two lines of work inform one another to a
   high frequency function word provides considerable                  limited degree. That is to say that while our understanding
   information about whether a word is a noun or a verb.               of infants’ linguistic abilities has advanced considerably,
   However, in unsupervised clustering, this information alone         this knowledge is not used to inform models of category
   does not result in highly accurate categorization of nouns and
                                                                       learning. Likewise, although recent models of category
   verbs.
                                                                       learning are very successful at distinguishing between
   Keywords: Language learning; grammatical categories;                grammatical categories, little empirical research is
   unsupervised learning; supervised learning.                         conducted to determine whether infants can make use of the
                                                                       kinds of cues that these models exploit (for exceptions, see
                          Introduction                                 Mintz, 2003; 2006; Shi, Morgan & Allopenna, 1998 and
Central to an understanding of how children learn language             Shi, Werker & Morgan, 1999).
is a theory of how they learn the syntactic categories of                 Improved communication between these two lines of
individual lexical items. Early work on this topic concluded           research is critical. For a model of language acquisition to
that children must have an innate predisposition to learn              be psychologically plausible, it must make use of
categories such as noun and verb as well as a set of innate            information that is actually available to a language learner.
rules linking individual words to these categories via                 Conversely, infants may show evidence of certain linguistic
meaning (e.g., Pinker, 1984). Such theories claim that                 abilities, but if they are to use them for language acquisition,
children cannot possibly learn grammatical categories from             these abilities must be relevant to learning not only the
syntactic distribution because the stimulus is impoverished            carefully controlled language they hear in the laboratory, but
and noisy (Chomsky, 1965). Not only do children not                    also the uncontrolled corpus of speech that they hear in the
receive enough positive evidence to support category                   real world. Our model evaluates whether the kinds of
learning, they also hear words used across category                    statistics that infants track in laboratory studies are useful
boundaries or without any syntactic support at all (e.g.,              for the acquisition of grammatical categories over a natural
isolated words). This characterization of the input to                 speech corpus.
children’s language learning presents a grim outlook for the              While function words are often absent from children’s
possibility that children, like adult linguists, categorize            earliest language productions, studies of infants’ language
words based on their syntactic privileges.                             perception indicate that they are sensitive to the syntactic
   However, empirical research with both adult and child               properties of function words from a very early age (Shady,
language learners, as well as closer examination of the                1996; Soderstrom, White, Conwell & Morgan, in press),
language children hear, suggests that these theories may               which suggests that these words could play a critical role in
underestimate both the abilities of language learners and the          the acquisition of more advanced syntax.                Höhle,
informativeness of a child’s linguistic environment.                   Weissenborn, Kiefer, Schulz and Schmitz (2004)
                                                                   893

demonstrate that German-learning infants can use familiar                                        Method
determiners to categorize novel words as nouns. These
findings dovetail with work by Valian and Coulson (1988)             Corpus Preparation
which shows that increasing the frequency of category                To determine whether transitional probabilities are useful
markers improves adults’ learning of the syntax of a                 for learning grammatical categories from natural language,
miniature artificial language. The frequency of function             we begin with the maternal speech to two children, Lily and
words in speech to children and findings that infants have           William, from the Demuth Providence Corpus (Demuth,
some knowledge of function word syntax both suggest that             Culbertson & Alter, 2006). These children were recorded at
these words might be particularly relevant to the learning of        home for an hour every other week for two years, beginning
grammatical categories.                                              with their first words. Because of some interesting patterns
   One remaining question, however, is how infants might use         in her development, the Lily corpus contains almost twice as
their knowledge of high frequency function words to learn            many recordings as the William corpus. To make the two
categories of content words (e.g., noun vs. verb). Infants are       corpora comparable, we use only the first 20 files from
known to track transitional probabilities (TPs) between              each. During this period, William was 16-26 months old
syllables, which could be useful for segmenting words from           and Lily was 13-21 months old. These data, therefore,
fluent speech (Saffran, et al., 1996). This kind of learning         represent the linguistic input to learners before and during
relies on the ability to track which sound sequences follow          the onset of combinatory speech.
and precede which other sound sequences and how often.                  We extract all maternal utterances from the first 20 files
When adults are exposed to artificial languages, they can            in each corpus and remove the CHAT coding conventions
learn the phrase structure of these languages via transitional       from them to leave only the words in each utterance.
probabilities, especially when these probabilities are less than     Common contractions (e.g., gonna, don’t and I’m) are left in
1 (Thompson & Newport, 2007). These results suggest that             place, while less common contractions (e.g., d’you,
learners are adept at tracking TPs in language and that they         dontcha) are spelled out to ameliorate variation in spelling
can exploit these statistics to learn about the structure of the     among transcribers. We use the first 5 files from each
language.                                                            corpus to create the set of high frequency words, while the
   There are also clear cues to the beginnings and ends of           transitional probabilities are obtained from the next 15 files.
utterances, including pauses and changes in pitch. Not only          The logic is that very young learners may need time to learn
are infants are sensitive to these cues (Nazzi, Kemler Nelson,       which words are highly frequent before they begin using
Jusczyk, & Jusczyk, 2000), but research also shows that the          those words to learn categories. In the Lily corpus, the first
presence of an utterance boundary facilitates infants’               5 files contain 20,110 words in 5,579 maternal utterances
segmentation of words from fluent speech (Fernald,                   and the next 15 files contain 67,526 words in 14,322
McRoberts, & Herrera, 1992). Occurring in utterance initial          maternal utterances. For the William corpus, these numbers
or final position can be described in terms of transitional          are 20,084 words in 4,805 utterances in the first 5 files and
probabilities as well: the probability that a word immediately       47,911 words in 10,821 utterances in the next 15 files.
precedes or follows an utterance boundary. This information
may also be relevant to categorization of nouns and verbs.           Calculating Transitional Probabilities
   We know that these sources of information are available to
infants during the language learning process. The next               To determine which words should be included as highly
question is whether such information is useful in a natural          frequent, we count the number of times each word occurs in
language context. Other research with infants indicates that         the first five files and divide by the total number of words in
the presence of a highly familiar word facilitates the               those files to calculate the percentage of the total number of
segmentation of words from fluent speech (Bortfeld, Morgan,          tokens accounted for by each word.                 Those words
Golinkoff & Rathbun, 2005) and that infants are sensitive to         comprising more than 1% of all tokens are considered to be
the syntactic privileges of high frequency function words            highly frequent. This cut-off is arbitrary, but it is probable
(Höhle, et al., 2004; Soderstrom, et al., in press). If infants      that a word which accounts for more than 1% of all tokens
can rely only on highly frequent words as a cue to                   would be familiar to the learner. In the Lily corpus, these
categorization, this considerably lessens the memory load            words are the, you, a, that, and, that’s, your, in, is, I, yeah
required by other models (Redington, et al., 1998). We now           and little. For the William corpus, these words are you, the,
evaluate whether transitional probabilities with high                that, is, and, a, what, it, this, that’s, on, do, oh, I, right, to,
frequency words and the likelihood of appearing at the               can, what’s and here. Notice that there is considerable
beginnings or ends of utterances are useful cues for                 overlap between the two sets of words and that most of
categorizing nouns and verbs in natural child-directed speech.       them are closed-class function items.
We also assess the relative contributions of each information           For every word in the remaining 15 files, we calculate the
source by systematically removing subsets of the data. In so         forward and backward transitional probabilities between
doing, we are able to identify which aspects of the data make        that word and each high frequency word. Transitional
the greatest contribution to the categorization process.             probability is the number of times a word appears
                                                                 894

immediately adjacent to the high frequency word divided by        varies for each subset, we find that repeating our analysis
the total number of times that word appears in the corpus.        with different choices has a negligible effect on the
For example, the word baseball appears 4 times in the             outcomes. This procedure is carried out anew for each
William corpus and 2 of those times it appears immediately        feature subset we consider.
after the. Therefore, the backward TP between baseball and
the is 0.5.                                                       Clustering With each target word now embedded in a
   Likewise, the probability of every word in each corpus         lower-dimensional space, we use the k-means algorithm to
occurring at the beginning of an utterance is the number of       do unsupervised clustering of the data (MacQueen, 1967).
times the word appears at the beginning of an utterance           The algorithm requires that the user specify the number of
divided by the total number of times the word appears in the      clusters k to search for. The result is a solution for each
corpus. We perform the same calculation with the number           value of k in which each target word is assigned to a unique
of times a word appears at the end of an utterance to             cluster. For each feature subset, we report the characteristics
determine the probability of that word occurring utterance-       of the 2-cluster solution, since ideally this would produce
finally. Single word utterances are excluded from this            one category of nouns and one of verbs.
analysis. We then place all of these probabilities into a
matrix such that each row represents a word and each              k-Nearest-Neighbor Classification Finally, given the same
column represents a feature (i.e., the TP between that word       embedding of the data we use for clustering, we classify
and one of the high frequency words).                             each target word as noun or verb in a supervised manner.
                                                                  We accomplish this using a “leave-one-out” procedure, in
Modeling the Data                                                 which each target word is removed from the data set one at
We carry out two analyses on the transitional probabilities       a time and the remaining labeled items are used to
obtained from the corpora: unsupervised clustering and            determine its category. We use the k-nearest neighbor
supervised classification. In both cases, we ultimately           procedure to assign labels. In this algorithm the user
describe each target word type as a point in a high-              specifies the number of “neighbors” that get to vote for
dimensional space, each axis of which denotes the value of a      category membership of the target according to their own
unique transitional probability. By systematically                label. Given a value for k, the k closest data points to the
eliminating subsets of features (TPs) from consideration, we      target in feature space are identified, and the target is
can perform both procedures in distinct subspaces to              labeled according to the majority label of these neighbors
determine how particular families of transitional                 (Duda, Hart, & Stork, 2001).
probabilities contribute to noun/verb categorization. In             We set k=1 for each feature set and also determine the
particular, we consider (1) the full set of TPs, (2) all TPs,     maximum performance across values of k less than 50.
except first word or last word information, (3) only forward
TPs, and (4) only backward TPs. A description of each                 Table 1: Composition of each subset used for analysis.
subset and the number of components analyzed for each is
                                                                   Subset            Corpus    Nouns      Verbs    Components
provided in Table 1. We proceed by describing the
                                                                   All TPs           Lily        1323      702          5
computational details of our clustering and classification
                                                                                     William      747      501          7
procedures.
                                                                   No First/Last     Lily        1046      535          7
                                                                                     William      605      438          5
Preprocessing Given the matrix of TPs for all target words,
                                                                   Forward           Lily         502      391          4
we first eliminate any words that have zero values for all
                                                                                     William      334      316          8
TPs we consider. We also remove all words that are not
                                                                   Backward          Lily         919      378          4
nouns or verbs. Next, given the high dimensionality of our
                                                                                     William      510      357          4
raw data (26 features for Lily, 40 features for William) we
carry out Principal Components Analysis (PCA) to find a
low-dimensional embedding of the target words in feature
space. PCA is a commonly used technique that finds a rigid                                   Results
rotation of the original coordinate frame such that the              The results of all cluster analyses are summarized in
maximum amount of variance in the data is captured by the         Figure 1 and all k-nearest neighbor analyses in Figure 2.
first new axis, and decreasing amounts are captured by each       For the cluster analyses, accuracy is the percentage of words
subsequent axis. To determine the best number of                  in a particular group that are members of the majority
components for describing each data set with, we perform a        category. For the k-nearest neighbor analyses, accuracy is
graphical analysis on the plots of explained variance vs.         the percentage of trials on which a word is correctly
component number to find an “elbow” in each plot. At this         classified in a leave-one-out procedure.
point, the addition of a new component does not                      When all potential factors are included in the analysis,
substantially increase the amount of variance explained by        two-cluster analysis results in an average of 67.3% correct
the new axes. Table 1 shows the number of components              classification for the Lily corpus and 66.8% correct
used in each analysis. Although the number of components          classification for the William corpus.         The k-nearest
                                                              895

neighbor evaluation indicates that, for one neighbor, the              Forward transitional probability is the likelihood that a
Lily corpus produces 67.8% correct classification and the              target word will immediately precede a high frequency
William corpus has 74% correct classification. Maximum                 word. Backward transitional probability is the likelihood
correct classification is obtained with 31 neighbors in the            that a word immediately follows a high frequency word.
Lily corpus, raising accuracy to 73.9%. In the William
corpus, maximum correct classification of 78.8% is                                           Lily k=1              Lily optimal
obtained with 11 neighbors.                                                                  William k=1           William optimal
  We next ask whether information about how often a word
appears first or last in an utterance is useful for                                    90%
categorization by removing this information from the data                              85%
and re-examining the accuracy of both the cluster analysis
                                                                                       80%
and the k-nearest neighbor analysis. In the Lily corpus, this
                                                                          % accuracy
improves the 2-cluster accuracy to 77.7%. In the William                               75%
corpus, the improvement is less striking, with 2-cluster                               70%
accuracy now 72.4%. For k-nearest neighbor, using only a
                                                                                       65%
single neighbor, the Lily corpus is 78.3% accurate and the
William corpus is 75.1% accurate. Maximum correct                                      60%
classification of 85.8% is achieved with 7 neighbors in the                            55%
Lily corpus and maximum correct classification of 79.1% is                             50%
obtained with 41 neighbors in the William corpus. Since
                                                                                                   1       2   3          4
removing information about sentence position improves
accuracy of categorization in one corpus and does not
change the accuracy for the other corpus, we conclude that             Figure 2: Accuracy for k-nearest-neighbor classification for
this information is not useful for the formation of noun and           each subset of the corpus. (1) contains all information, (2)
verb categories. These data are excluded from further                  includes all TPs, but no information about utterance
analyses.                                                              position, (3) includes only forward TPs and (4) includes
                                                                       only backward TPs.
                        Lily 2-cluster       William 2-cluster            When only forward transitional probabilities are included
                                                                       in the analysis, accuracy for 2 clusters decreases markedly
                  90%                                                  in both the Lily corpus (62.8%) and the William corpus
                  85%                                                  (55%). In a k-nearest neighbor analysis, accuracy when
                  80%                                                  considering one neighbor is 70.3% for the Lily corpus and
    % accuracy.
                  75%
                                                                       61% for the William corpus. The maximum accuracy of k-
                                                                       nearest neighbor for the Lily corpus is 75.4% and occurs
                  70%
                                                                       when 21 neighbors are considered. For the William corpus,
                  65%                                                  the maximum accuracy is 69.4% and occurs when 11
                  60%                                                  neighbors are considered. Again, these accuracies decrease
                  55%                                                  from the analysis in which both forward and backward
                  50%                                                  transitional probabilities are considered. This suggests that
                          1              2      3           4
                                                                       forward transitional probabilities are not a good source of
                                                                       information for forming grammatical categories or,
                                                                       alternatively, that backward TPs are especially good for
Figure 1: Mean accuracy of for k-means clustering for each             categorization.
subset of the corpus. (1) contains all information, (2)                   When only backward transitional probabilities are
includes all TPs, but no first or last word information, (3)           considered, however, clustering accuracy improves relative
contains only forward TPs and (4) contains only backward               to when forward TPs are considered. For the Lily corpus
TPs.                                                                   accuracy rises to 80.1% for two clusters. In the William
                                                                       corpus, accuracy remains unchanged at 72.8% for two
   We now turn to the issue of the relative contributions of           clusters. Turning to the k-nearest neighbor analysis,
preceding and following high frequency words. Because                  accuracy when one neighbor is considered is 82.8% for the
function words often mark the beginnings of phrases, it is             Lily corpus and 78.6% for the William corpus. The
likely that following a function word provides more                    maximum accuracy of 85.6% for the Lily corpus is obtained
information about a category than preceding a function                 when 11 neighbors are considered. For the William corpus,
word (e.g., nouns follow determiners, verbs follow                     the maximum accuracy of 80.7% is obtained when 41
auxiliaries, etc.). To test this, we once again run clustering         neighbors are considered. Because the accuracies of both
and k-nearest neighbor analyses on the data, this time                 the unsupervised and supervised methods improve when
separating forward and backward transitional probabilities.            only backward transitional probabilities are considered, this
                                                                 896

suggests that what information transitional probabilities          backward transitional probabilities between the target word
contain for categorizing nouns and verbs is largely                and a high frequency word are considered, accuracy
contained in backward transitional probabilities. That is to       improves. This suggests that the high frequency word
say, a high frequency word that immediately precedes a             preceding a target word is a good predictor of grammatical
target word contains more information about that word’s            category. Höhle and colleagues (2004) show that German-
category than does a high frequency word that immediately          learning infants do indeed use co-occurrence with
follows the target word.                                           determiners to categorize novel words as nouns. Our results
   For all clustering analyses in all conditions, a similar        complement the findings of their work by showing that
pattern can be observed. While there is often one small,           information which is used by infants to learn categories in
highly accurate cluster, typically composed of nouns, there        the laboratory is also useful over a corpus of natural child-
is always one very large cluster which contains                    directed speech.
approximately equal numbers of each word category. Since              Supervised learning via k-nearest neighbor analysis
all subsets of both corpora contain more nouns than verbs,         suggests that these cues are better for supervised learning
this large category often contains all but a few of the verbs.     than for unsupervised learning. This is problematic for a
Therefore, almost all of the verbs are distributed in a single     theory of natural language acquisition because infants do
location. While this does not allow a learner to discriminate      not learn language in a supervised way. The little feedback
between nouns and verbs, it does allow the learner to              that they receive is often uninformative and there is good
conclude that items outside of this area are probably not          evidence that they do not make use of it (Morgan, Bonamo
verbs. Furthermore, the success of the k-nearest neighbor          & Travis, 1995). The success of these methods does
analyses suggests that there may be some structure within          suggest, however, that infants might be able to use
this large cluster that might be revealed if a much larger         transitional probabilities to assign words to categories once
number of clusters were to be considered.                          rudimentary categories have been formed. Some theories of
                                                                   category acquisition propose that infants might begin with
                   General Discussion                              categories based on semantics (e.g., Pinker, 1989) or
This paper evaluates the effectiveness of transitional             phonological properties (e.g., Gleitman & Wanner, 1982).
probabilities for learning the syntactic categories of words       One major obstacle for both types of theories is explaining
from a corpus of natural, child-directed speech.           By      how a learner would transition from such proto-categories to
removing subsets of data from the analysis, we are able to         an adult-like system based on syntactic distribution.
assess the relative contributions of each data source. While       Perhaps transitional probabilities could be incorporated into
unsupervised clustering does not reach the levels of               these theories in a semi-supervised way to facilitate such a
accuracy that some researchers find using other statistics         transition.
(Mintz, 2003; Monaghan, et al., 2005), it is on par with              This paper finds that frequency and transitional
some models (Redington, et al., 1998) and is motivated by          probabilities, statistics which infants are known to compute
empirical research into children’s linguistic abilities. This      in laboratory studies of language learning, are moderately
work also uses two large corpora of speech and tries to            effective for categorizing nouns and verbs in a corpus of
categorize as many nouns and verbs as possible, not just the       child-directed speech. Systematic manipulation of the data
most frequent ones.                                                indicates that the most powerful indicator of noun or verb
   We examine the extent to which 3 subsets of these TPs           status is the high frequency word that immediately precedes
are useful for categorizing nouns and verbs. The likelihood        the target word. By better characterizing the relationship
that a word appears at the beginning or at the end of an           between the abilities of human learners and the richness of
utterance probably contributes little information relevant to      their linguistic environments, we can create a more accurate
learning the differences between nouns and verbs.                  portrait of the language learning process. A continued
However, it is important to remember that this analysis            exchange of ideas between modelers and experimental
includes disfluent utterances, false starts and interruptions,     researchers will be vital to such a process.
which may introduce noise into the data. Because these are
natural corpora, we know that young language learners are                               Acknowledgments
exposed to such noise. Still, there is evidence that young         BJB is funded by a National Defense Science and
learners are sensitive to disfluency as distinct from fluent       Engineering Graduate fellowship. The authors wish to
speech (Soderstrom & Morgan, in press). If infants and the         thank Katherine Demuth for allowing us to use the Demuth
model were to exclude such utterances from the analysis,           Providence Corpus and Elizabeth McCullough for
the efficacy of utterance initial and final statistics might       facilitating access to the corpus. We are also grateful to
improve.                                                           Naomi Feldman, Melanie Soderstrom and three anonymous
   Furthermore, when only forward transitional probabilities       reviewers for very helpful comments on an earlier version of
between the target word and the high frequency words are           this paper.
considered, accuracy decreases. Conversely, when only
                                                               897

                        References                                   speech to young children. Cognitive Science, 26, 393-
                                                                     424.
Bortfeld, H., Morgan, J. L., Golinkoff, R. M., & Rathbun,
                                                                   Monaghan, P., Chater, N., & Christiansen, M. H. (2005).
  K. (2005). Mommy and me: Familiar names help launch
                                                                     The differential role of phonological and distributional
  babies into speech stream segmentation. Psychological
                                                                     cues in grammatical categorization. Cognition, 96, 143-
  Science, 16, 298-304.
                                                                     182.
Chomsky, N. (1965). Aspects of the theory of syntax.
                                                                   Morgan, J. L., Bonamo, K. M., & Travis, L. L. (1995).
  Boston, MA: MIT Press.
                                                                     Negative evidence on negative evidence. Developmental
Demuth, K., Culbertson, J., & Alter, J. (2006). Word-
                                                                     Psychology, 31, 180-197.
  minimality, epenthesis, and coda licensing in the
                                                                   Nazzi, T., Kemler Nelson, D.G., Jusczyk, P.W., & Jusczyk,
  acquisition of English. Language and Speech, 49, 137-
                                                                     A.M. (2000). Six-month-olds' detection of clauses in
  174.
                                                                     continuous speech: Effects of prosodic well-formedness.
Duda, R. O., Hart, P. E., & Stork, P. G. (2001). Pattern
                                                                     Infancy, 1, 123-147.
  classification. New York: John Wiley and Sons, Inc.
                                                                   Pinker, S. (1984). Language learnability and language
Fernald, A., McRoberts, G. W., & Herrera, C. (1992).
                                                                     development. Cambridge, MA: MIT Press.
  Prosodic features and early word recognition. Paper
                                                                   Pinker, S. (1989).       Learnability and cognition: The
  presented at the 8th International Conference on Infant
                                                                     acquisition of argument structure. Cambridge, MA: MIT
  Studies, Miami, FL.
                                                                     Press.
Gerken, L. A., Wilson, R., & Lewis, W. (2005). 17-month-
                                                                   Redington, M., Chater, N., & Finch, S. (1998).
  olds can use distributional cues to form syntactic
                                                                     Distributional information: A powerful cue for acquiring
  categories. Journal of Child Language, 32, 249-268.
                                                                     syntactic categories. Cognitive Science, 22, 425-469.
Gleitman, L. R. & Wanner, E. (1982). Language
                                                                   Saffran, J. R., Aslin, R. N. & Newport, E. L. (1996).
  acquisition: The state of the state of the art. In E. Wanner
                                                                     Statistical learning by 8-month old infants. Science, 274,
  & L. R. Gleitman (Eds.), Language acquisition: The state
                                                                     1926-1928.
  of the art. Cambridge, UK: Cambridge University Press.
                                                                   Shady, M. (1996). Infants' sensitivity to function
Gómez, R. L. & Lakusta, L. (2004). A first step in form-
                                                                     morphemes. Unpublished dissertation, State University of
  based category abstraction by 12-month-old infants.
                                                                     New York, Buffalo, NY.
  Developmental Science, 7, 567-580.
                                                                   Shi, R., Morgan, J., & Allopenna, P. (1998). Phonological
Höhle, B., Weissenborn, J., Kiefer, D., Schulz, A., &
                                                                     and acoustic bases for earliest grammatical category
  Schmitz, M. (2004). Functional elements in infants'
                                                                     assignment: A cross-linguistic perspective. Journal of
  speech processing: The role of determiners in the
                                                                     Child Language, 25, 169-201.
  syntactic categorization of lexical elements. Infancy, 5,
                                                                   Shi, R., Werker, J. F., & Morgan, J. L. (1999). Newborn
  341-353.
                                                                     infants’ sensitivity to perceptual cues to lexical and
MacQueen, J. B. (1967). Some methods for classification
                                                                     grammatical words. Cognition, 27, B11-B21.
  and analysis of multivariate observations. 5th Berkeley
                                                                   Soderstrom, M. & Morgan, J. L. (in press). Twenty-two-
  Symposium on Mathematical Statistics and Probability,
                                                                     month-olds discriminate fluent from disfluent adult-
  Berkeley, CA.
                                                                     directed speech. Developmental Science.
Mintz, T. H. (2003). Frequent frames as a cue for
                                                                   Soderstrom, M., White, K. S., Conwell, E., & Morgan, J. L.
  grammatical categories in child directed speech.
                                                                     (in press). Receptive grammatical knowledge of familiar
  Cognition, 90, 91-117.
                                                                     content words and inflection in 16-month-olds. Infancy.
Mintz, T. H. (2006). Finding the verbs: distributional cues
                                                                   Thompson, S. P. & Newport, E. L. (2007). Statistical
  to categories available to young learners. In K. Hirsh-
                                                                     learning of syntax: The role of transitional probability.
  Pasek & R. M. Golinkoff (Eds.), Action Meets Word:
                                                                     Language Learning and Development, 3, 1-42.
  How Children Learn Verbs. New York: Oxford
                                                                   Valian, V. & Coulson, S. (1988). Anchor points in
  University Press.
                                                                     language learning: The role of marker frequency. Journal
Mintz, T. H., Newport, E. L., & Bever, T. G. (2002). The
                                                                     of Memory and Language, 27, 71-86.
  distributional structure of grammatical categories in
                                                               898

