UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Discovering Syntactic Hierarchies
Permalink
https://escholarship.org/uc/item/6kp787g3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Savova, Virginia
Roy, Daniel
Schmidt, Lauren
et al.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                          Discovering Syntactic Hierarchies
                          Virginia Savova, Daniel Roy, Lauren Schmidt & Joshua B. Tenenbaum
                                             {savova, droy, lschmidt, jbt}@mit.edu
                                               Department of Brain and Cognitive Sciences
                                                  Massachusetts Institute of Technology
                              Abstract                                    While participation in combinatorial rules is the defining
   The acquisition of syntactic rules is predicated upon the suc-
                                                                       characteristic of a syntactic category, it is often the case that
   cessful discovery of syntactic categories (parts of speech).        members of the same category tend to share semantic prop-
   These do not simply constitute a set, but in fact form a            erties. Verbs tend to refer to events, nouns – to objects or
   nested hierarchy, which allows rules to apply at different lev-     people. Both developmental psychologists and linguists have
   els of generality. Languages provide a variety of cues to
   syntactic categorization - phonological, semantic and distribu-     argued that semantic cues play a significant role in the early
   tional. However, the relative reliability of these cues differs     stages of syntactic development. In particular, Macnamara
   from language to language. This paper presents a computa-           (1972) proposed that children acquire syntactic knowledge on
   tional model capable of acquiring the hierarchy syntactic cat-
   egories from different combinations of cues. Interestingly, the     the basis of already developed knowledge of concepts and se-
   model is domain general and has been successfully applied to        mantic relations. Later, Pinker (1982) suggested that children
   non-linguistic discovery of hierarchical structure. Keywords:       use their understanding of verb meaning to infer the syntactic
   computational modeling; hierarchical clustering; linguistics;
   syntactic categories; language acquisition.                         frames in which they appear and vice versa. This is prob-
                                                                       ably facilitated by the consistency of caregiver speech with
                          Introduction                                 respect to semantic-syntactic mapping (Rondal & Cession,
Knowledge of syntax is knowledge of the combinatorial prop-            1990). Cross-linguistic typologies of case (Grimshaw, 1981)
erties of words. Since it is not only infeasible, but outright         can also be accounted for by postulating an innate mapping
impossible to encounter all licit combinations for any individ-        preference from agents to subjects of active sentences.
ual word, generalization over abstract categories is a crucial            In addition, a category may be marked by overt morpho-
step in language acquisition. However, the task of uncovering          phonological markers. The reliability of this type of cue
the categorial structure of lexical items is highly non-trivial.       varies greatly from language to language. For example, the
While members of the same category share certain seman-                English suffix ’tion’ applies exclusively to the noun class,
tic or morpho-phonological characteristics, there is no guar-          and overwhelmingly to abstract nominals (define-definition,
antee that items with shared characteristics fall in the same          prescribe-prescription etc.). However, English rarely marks
category. For example, all count nouns take the suffix ’s’ (to         the syntactic category of words, as the existence of identical
denote plural), but so do all verbs (to denote 3rd person singu-       noun-verb pairs attests (e.g. to chase – a chase, to jump – a
lar). Similarly, members of the same syntactic category may            jump etc.). In contrast, a morphologically rich language (e.g.
differ widely in both meaning and sound (e.g. ’salt’ and ’fur-         Russian), provides a wide variety of suffix and inflectional
niture’). It follows that a syntactic category is best defined by      cues that distinguish syntactic categories.
abstract combinatorial properties. This leads us to a classical           Ultimately, distributional information is paramount and the
chicken-and-egg problem: while the acquisition of syntactic            contributions of semantics and phonology must be reconciled
rules is predicated upon the successful discovery of syntactic         with it. While previous approaches treat contextual cues as a
categories, the categories are in turn identified on the basis of      type of lexical feature, cooccurence is best described as a bi-
these rules. How could human learners extricate themselves             nary relation. This intuition is captured by virtually all gram-
from this predicament?                                                 mar formalisms, including dependency grammar, varieties of
   Before we present our approach, let us take a closer look           phrase-structure grammar, LFG, X-bar theory and minimal-
at the nature of syntactic categorization. While many re-              ism. In fact, many formalisms postulate more than binary
searchers make the simplifying assumption that the structure           relation among words. Thus, acquiring categories from distri-
of categories is flat (e.g. (Cartwright & M., 1997), (Clark,           butional cues is a special case of identifying categories from
2003)), it is better to conceive of them as organized in a nested      multiple relational cues.
hierarchy. This organization allows combinatorial rules to be             All of this suggests that learning the hierarchical structure
associated with different levels of generality within the hier-        of syntactic categories is a complex process involving the in-
archy. For example, all English verbs require a subject, but           tegration of many cues, which fall into two major classes:
only a subset of verbs require an object (the so-called transi-        relational and feature-based. Feature-based cues involve the
tive verbs, e.g.’hit’). Similarly, while all nouns share some          presence of semantic or morpho-phonological information
combinatorial properties, only common nouns (e.g. ’salt’,              associated with lexical entries. Relational cues involve the
’book’, but not ’John’) can occur with a definite determiner           membership of lexical pairs in certain types of (distributional)
(’the’), and only a subset of these (e.g. ’book’, but not ’salt’)      relations. Since different languages employ feature-based
can occur with an indefinite determiner (’a’).                         cues to different extent, it is important for a computational
                                                                   629

          say
         think
        know
                                                                        another. For example, consider the relations object-of and
       break
      sleep
       walk
       look
       hold
         eat
            go
          get
         kick
                                                                        subject-of; while all verbs require a subject, only transitive
           hit
                                                                        verbs require an object. Therefore, the appropropriate level
       explode   is
                give
   slowly
  quickly          of
          fromto
             for
           nasty
                                                                        in the hierarchy to describe the subject-of relation is the cate-
      yellow nice
           red
     banana
    cheese
    yogurt
       ball
     doll
     car
  bunny
    boy
queen
    girl
picture
     some
           all
         thea                                                           gory of all verbs but the a finer-grained distinction of transi-
                                                                        tive/intransitive is relevant for the relation object-of.
Say−V     Verbs                                  Nouns         Det
                                                                           The idea of an annotated hierarchy is one of the oldest pro-
                                                                        posals in cognitive science, and researchers including Collins
                                                                        and Quillian (1969) and Keil (1979) have argued that seman-
                                                                        tic knowledge is organized into representations of this form.
                                                                        Previous treatments of annotated hierarchies, however, often
                                                                        suffer from two limitations. First, annotated hierarchies are
                                                                        usually hand-engineered, and there are few proposals describ-
                                                                        ing how they might be learned from data. Second, annotated
                                                                        hierarchies typically capture knowledge only about the fea-
              IComp−of                         Adjunct−of               tures of objects: relations between objects are rarely consid-
                                                                        ered. In contrast, our generative probabilistic model simulta-
                                                                        neously handles objects, features, relations, and can be used
                                                                        to recover annotated hierarchies from raw data.
                                                                           The annotated hierarchies model assumes that the objects
                                                                        are located at the leaves of a rooted tree (each node specifies
                                                                        the category of objects in its subtree), and that each feature
                                                                        and relation is generated independently conditioned on the
                                                                        structure of the tree. Intuitively, objects that are nearby in
              Spec−of                           Comp−of                 the tree will tend to have similar features values, and relate
                                                                        to other objects in similar ways. In this setting, objects are
Figure 1: Tree and relation matrix for the X-bar dataset: Spec-of       words and we are trying to discover an annotated hierarchy
relation matrix illustrates the association between determiners and     of these words that summarizes the observed morphological
nouns on one hand (circled in green), and verbs and nouns on the        features and syntactic relations. More precisely, each feature
other. The Comp-of relation matrix associates a subclass of verbs—
the reflective verbs (Say-V) with the verb class as a whole (circled    (or relation) is associated with a partition of all words (or of
gray).                                                                  all pairs of words) and this partition is constrained to respect
                                                                        the tree structure (i.e. each subset in the partition is an en-
                                                                        tire category specified by the hierarchy). Therefore, one can
model to be general enough to profit from these cues if and             think of these partitions as lists of categories (or pairs of cate-
when they are available, while being able to deal with the ab-          gories in the relational case). The model contains a prior over
sence of these cues when unavailable. Thus, a model should              partitions that encourages partitions to use the most general
be able to naturally incorporate multiple sets of both rela-            categories possible without losing too much predictive accu-
tional and feature-based data. Our method for discovering an-           racy.
notated hierarchies (Roy, Kemp, Mansinghka, & Tenenbaum,                   Each category (or pair of categories) in a partition is asso-
2006) was developed specifically for learning situations of             ciated with a real-valued parameter θ between 0 and 1 that
this sort. This is the first application of the model to linguis-       specifies the probability with which the feature (or relation)
tic data.                                                               applies to words (or pairs of words) in that subset. These “pa-
                                                                        rameterized” partitions describe the typical values for each
             Annotated hierarchies model                                feature and relation for different branches of the tree. For ex-
Given a collection of word features (e.g. morphological or              ample, the category of all verbs would likely be included in
semantic), an annotated hierarchy specifies nested categories           the partition describing the “-ing”-suffix feature and the cor-
of words, as well as the appropriate categories with which              responding parameter would be closer to 1 than 0 because
to summarize the observed features. For example, an anno-               many verbs would be observed as gerunds at some point. A
tation for a feature indicating whether a word can take the             parameterized partition associated with a relation describes
“-ing” suffix would specify that the category containing all            how likely it is that any pair of words stand in that relation, as
verbs has this property while the three categories containing           a function of the location of the words in the hierarchy.
all nouns, all adjectives and all adverbs do not. Many syntac-             The probability of the i’th feature, Fi , conditioned on the
tic properties are best described by relations between words            tree T , can be computed by summing the contribution of ev-
and annotation hierarchies summarize the observed relations             ery possible partition π, weighted by its prior probability:
by specifying how certain categories of words relate to one             P(Fi |T ) = ∑π P(Fi |π) P(π|T ). In the same manner, we can
                                                                  630

                                                                                                                                          work
                                      man
                                    group
                                    family
                                  woman
                                      year
                                 day
                              night
                             room
                          money
                            world
                           power
                             week
                   development
                       school    end
                            water
                            place
                    informationdogcar  time
                                  country
                               company state   other
                                           poor
                                          great
                                             old
                                         small
                                           little
                                    different
                                        good full
                                             big
                                          early
                          western
                             whole
                                local
                              public
                           special
                           recent
                           private
                        previous
                         financial
                                 new
                             major
                            single
                       european
                        particular  softly
                             following
                                  widely
                                   closer
                                    large
                                  across
                            forward
                              away
                            home
                      anywhere
                             more
                        together
                         straight far
                               long
                              below
                           much   right
                                 close
                                 there    finally
                               already
                                never
                                 ever
                              actually       now
                                      still
                                     just
                                  usually
                                      only
                                      also
                              probably
                                     even    suddenly
                                                clearly
                                                quickly
                                     start
                                     pass
                                      hold        later
                                                   well
                                   comerun
                                       use
                                    need
                                    show
                            make
                         provide  get
                                see     go
                        become
                          give   say
                              take
                             Nouns                       Adjectives                            Adverbs                           Verbs
     concrete
   intentional
      manner
       moving
      suff ing
        suff s
         them
           first
              or
          from
         must
             he
                        AJ                                 OBJ                               SUBJ
                   Figure 2: Trees induced from BNC data: a) relations, local context features, semantic and morphological features.
compute the probability of the j’th relation, R j , conditioned                structure of tags is assumed, and the pre-defined set of labels
on the tree T : P(R j |T ). Given features F1 , . . . , Fn and rela-           can be viewed as a low-level horizontal cut through the ac-
tions R1 , . . . , Rm , the posterior probability of a tree T is given         tual hierarchy. While the literature on PoS tagging is largely
by Bayes’ rule as                                                              orthogonal to our approach, other attempts of unsupervised
                                                                               clustering on NLP data provide useful comparisons. In par-
P(T |F1 , . . . , Fn , R1 , . . . , Rm ) ∝ P(T ) ∏ P(Fi |T ) ∏ P(R j |T ),     ticular, clustering algorithms have been applied to induce se-
                                               i            j
                                                                               mantic categories in tasks such as word sense disambiguation,
where P(T ) is a prior over tree structures. Roughly speak-                    and identification of word senses. Dekang Lin’s work (Lin,
ing, the best hierarchy will then be the one that provides the                 1998) is particularly interesting in this regard. Using rela-
best categories with which to summarize all the features and                   tional data for multiple dependency relations, he is able to
relations. For lack of space, we refer the reader to a recent                  identify pairs of semantically related words. Although the in-
publication where the model is presented in full detail (Roy                   tention is to obtain words with similar meanings, the resulting
et al., 2006).                                                                 pairs are also close syntactic neighbors. However, there is no
                                                                               notion of hierarchy and association with particular relations.
                             Related work
                                                                                  Unsupervised induction of PoS categories in NLP is rel-
Induction of syntactic categories                                              evant in the context of cross-linguistic applicability, which
Part-of-speech tagging is a highly successful application of                   necessitates combining morphological and distributional in-
statistical Natural Language Processing (NLP) techniques.                      formation. The most important difference with respect to our
However, our work differs from NLP research in fundamen-                       proposal is the resulting category structure. The goal of NLP
tal ways. First, the goal of PoS tagging is to label text with                 approaches is to induce a flat set of categories, rather than
the best tag of a pre-defined set, rather than inducing the cat-               a categorial hierarchy. In addition, the number of clusters
egories themselves. Second, since the goal is to create an                     is set in advance. Clark (2003) presents a series of cross-
accurate engineering application against a particular bench-                   linguistic experiments in unsupervised PoS induction with
mark, learning is always supervised. Third, no hierarchical                    Hidden Markov Models, based on a combination of distri-
                                                                         631

                  time
                                                                                                                  softly
                      man                                         work
                       day                                      across
                 company                                         widely
                     year                                         closer
                     group                                                                                            new                       other
                                      state
                                 family
                                  night
                                week
                                  dog
                                  water
                                 money
                                woman
                                 school                                                  forward far
                                   place
                         information                                                    straight
                                                                                        away
                                country
                                   room
                                     end                                              never
                                                                                          well
                                  power
                            developmentcar                                               more ever                                           full
                                         world
                                           say                                  home later                                              old
                                                                                                                                     different
                                        make
                                      come  use                        finally
                                                                           also                                               western    great
                                      need
                                     showsee                                    much
                                                                                 clearly                                            privatebig
                                         run
                                        take                              probablythere                                          european
                                                                                                                                       major
                         become             get                               righteven                                              good
                                                                                                                                 particular
                               start    pass                                           quickly                                     public
                                                                                                                                      single
                                   providego
                                       hold                                            together
                                                                                              still                                  local
                                                                                                                                         poor
                                         give                                           close
                                                                                      already                                         early
                                                                                                                                       little
                                                                                        actually
                                                                                            only                                 previous
                                                                                                                                   following
                                                                                     below  long                                        small
                                                                                                                                       recent
                                                                                     usually
                                                                                        suddenly                           financial       whole
                                                                                       anywherejust                            large
                                                                                                                            special
                                                                                               now
                                 Noun/Verbs (Argument)                                                 Adjuncts
     concrete
   intentional
      manner
       moving
      suff ing
        suff s
                      Figure 3: Trees induced from BNC data: single dependency relation + semantic and morphological features.
butional and morphological information. He demonstrates                          Previous applications of annotated hierarchies
that the inclusion of morphological information improves the                     model
clustering of rare words in morphologically rich languages.                      In previous work, the model was successfully applied to non-
There is no natural way of modifying his approach to result                      linguistic cognitive tasks (Roy et al., 2006). It was shown
in hierarchical structure.                                                       to discover the conceptual structure of feature data from four
   The role of distributional information in syntactic catego-                   domains: animals, food, vehicles and tools. In addition to
rization has received a reasonable amount of attention in cog-                   identifying the four domains, the model came up with rel-
nitive science. In some cases a categorial hierarchy is in-                      evant superordinate and subordinate categories. It was also
duced. The work of Redington, Chater, and Finch (1998),                          successful in uncovering the kinship structure of Australian
is among the most detailed in this respect. By applying hi-                      tribes.
erarchical clustering methods on the distributional contexts
of words from the CHILDES corpus of caregiver speech,                                                   Experiments
they produce a dendrogram which captures the main part-of-                       In the first experiment, a small number of simple sentences
speech classes (noun, verb, adjective), along with some sub-                     were used to extract the three basic cooccurence relations
structure. Similar results have been obtained by others with                     postulated by the X-bar theory of syntax. According to X-
the same approach for nouns and verbs but in a smaller distri-                   bar theory, each word may select 1 − 2 obligatory arguments
butional context (Mintz, Newport, & Bever, 2002). A slightly                     (specifier and complement), and an unbounded number of op-
different variation was developed by Jeffrey Elman on a cor-                     tional adjuncts. The traditional substantive relational cate-
pus of short sentences generated by a simple artificial gram-                    gories of verb and object are interpreted as special cases of
mar. He trained a simple recurrent neural network to predict                     the specifier and complement relations, which are not limited
the next word of the input. The units of the network were                        to verbs alone. For example, most English common nouns
treated as a feature vector a subsequent hierarchical cluster                    require a determiner in the specifier position, just as verbs
analysis (Elman, 1991), which showed some representation                         require a noun in the subject position. Analogously, some
of the underlying word classes. Another approach to distri-                      nouns require prepositional phrases as complements, just as
butional clustering was investigated by Cartwright and Brent                     verbs require objects (e.g. the noun “picture” requires a com-
(Cartwright & M., 1997). While their method has certain ad-                      plement “of X” to be interpreted). In addition, a small subset
vantages over hierarchical clustering (e.g. it works incremen-                   of verbs (e.g. give) require a secondary obligatory argument
tally), its main drawback is that it results in a discrete set of                (IComp), which refers to a beneficiary or recipient. If lan-
categories and does not capture the nested structure of cate-                    guage learners are able to observe these fundamental relation-
gories. Unlike our method, hierarchical clustering relies on                     ships at the word-to-word level, would they be able to use the
only one source of information at a time and cannot combine                      relational data to form a hierarchical structure of categories?
knowledge of multiple relations and/or features to produce                          To answer this question, we picked forty words that are
the best representation for all. Furthermore, this type of clus-                 likely to figure into early vocabulary, and represent an inter-
tering results in an enormous number of nested subcategories,                    esting set of potential subcategories. These included nouns,
most of which have no natural interpretation.                                    verbs, adjectives, adverbs, prepositions and determiners. The
                                                                           632

results of our experiment show that the algorithm uncovers           (e.g., modi f ies(suddenly, go)), which were collapsed with
linguistically relelvant structure at multiple levels (Figure 1).    the adjectival modifer relations into a single relation for all
It splits the words into nouns, verbs, adjectives, determiners,      of the corpus-based data sets.
prepositions and adverbs and identifies important subclasses             One of the interesting questions we set out to investigate
within the verb class: a subclass of reflective verbs (‘think’,      whether the algorithm can discover internal structure in the
‘say’, ‘know’) and verbs that are used without direct object         relations, when the structure is not explicitely given. To do
(walk, sleep, break, go), as well as a set of transitive verbs       so, we collapsed the two types of modifier relations provided
(hit, kick, hold, get and eat). It places the ditransitive ’give’    by the relation extraction utility into a single relation. We also
in a separate category. Subclasses are also identified in the        ran an experiment where all relations were collapsed into one
noun category. It is roughly split into physical objects on one      dependency relation (‘a cooccurs with b’, where a and b are
side, and people on the other. An exception is ‘picture’, which      arbitrarily far from one another), and an experiment based on
clusters with the people category (cf. ‘queen of’/‘picture of’).     immediate adjacency (‘a adjacent to b’).
                                                                         We also investigated the effect of manually annotated se-
Corpus-based experiments
                                                                     mantic and morphological features. Semantic features were
The success of the first experiment lead us to explore more          shared by a small subset of items in the major categories. One
realistic scenarios using automatically extracted data from          morphological feature (‘ing’) was highly consistent with the
a real corpus, and less abstract relations which are easier          verb category, but also occured with items in the noun cate-
to observe on the surface. Ultimately, the relations pos-            gory, which happen to have dual status (‘work’, ‘time’). The
tulated by X-bar theory are generalized versions of sub-             second morphological feature however (’s’) was evenly split
ject/object/modifier relations which have a semantic basis.          between the noun and the verb category, denoting plural in-
In addition, these relations have a strong reflection on sur-        flection with nouns and 3rd person singular with verbs. The
face order—in English, subjects almost always precede—               question is whether the algorithm would be able to assign the
and object follow, the verb. Our next set of experiments             correct level of importance to these features, overriding their
is with a dataset of automatically extracted subject, object         effect when distributional relations strongly favor other par-
and modifier relations for categorially heterogeneous set of         titions. In addition, we ran experiments using bigram dis-
words. To obtain a dataset of manageable proportions, the re-        tributional features akin to those typically used in hierarchi-
lations were collected with a frequency threshold. The set of        cal clustering, and a set of experiments with the standard hi-
words was chosen on the basis of frequency with two crite-           erarchical clustering algorithm on the same data. The PIE
ria: First, to contain an approximately equal number of items        utility allowed us to find the most common bigrams contain-
from the four main categories (Nouns, Verbs, Adjectives, and         ing our words of interest. We gathered cooccurrence data us-
Adverbs), and second, to create a reasonably dense relation          ing this tool and represented the data as features of the form
matrix, so that each word relates to at least three other items.     preceded-by-X and f ollowed-by-Y . A subset of the datasets
The resulting set contained 105 words: 18 nouns, 29 adjec-           was then selected so as to contain a densely connected set of
tives, 36 adverbs, and 22 verbs.                                     popular words.
   We collected the data from the British National Corpus
(BNC), a corpus of 100M words. The Phrases In English                                  Results and discussion
(PIE) utility (available at http://pie.usna.edu) allowed
us to find the most popular nouns in the BNC by using its            In all of our experiments, the algorithm was successful in
n-gram tool to identify the most popular unigrams contain-           identifying the high level syntactic categories. This is partic-
ing common nouns. We selected a subset of words from                 ularly impressive when all relations identified were collapsed
this set and added a few more common words to better                 into one. Adjectives and adverbs were always recovered as
span the ontological categories of concrete nouns and liv-           distinct categories, regardless of the fact that both participate
ing things. Using another online utility, the Sketch En-             in the modifier relation (Figure 2). In addition, when all rela-
gine (Kilgarriff, Rychly, Smrz, & Tugwell, 2004, available           tions were collapsed into one, the algorithm found high level
at http://www.sketchengine.co.uk/), we produced word                 structure, separating the adjective/adverb superclass from the
sketches of these nouns – summaries of their relationships           noun/verb superclass (Figure 3). This is interesting given the
to other words within the BNC and the frequencies with               linguistic relevance of distinguishing optional and obligatory
which they occur in these grammatical and collocational re-          elements. With the exception of the first experiment how-
lations. We collected data on which words pairs occurred             ever, the algorithm found relatively little subordinate struc-
in “object of” relations (e.g., ob ject-o f (money, give)), “sub-    ture. Nevertheless, the success of the first experiment (Figure
ject of” relations (e.g., sub ject-o f (I, give)), and modifier re-  1) convinces us that the reluctance to separate subgroups is
lations (e.g., adjective modifiers like modi f ies(tall, man)).      due to noise in the corpus data.
From this data, we selected a subset of verbs and adjec-                 In general, the model made better use of semantic features
tives that were densely relationally connected to the origi-         in identifying subgroups when fewer relations were present.
nal nouns, and we repeated the word sketch process on these          For example, the motion verbs cluster in a subgroup when
words, additionally pulling out adverbial modifier relations         all relations are collapsed into one, but fail to conclusively
                                                                 633

separate when multiple relations are present. This is because      a more extensive comparison to hierarchical clustering meth-
the model places relatively little value on features versus rela-  ods. This involves increasing the model’s tolerance to noise,
tions. However, in reality subgroups are likely to share more      and developing faster inference methods.
than one feature, and including more of these features will
probably improve performance. The phonological features                                     References
introduced in the model help classification in so far as they      Cartwright, T., & M., B. (1997). Syntactic categorization in
do not contradict relational evidence. The ‘s’ suffix feature is     early language acquisition: formalizing the role of distri-
linked to two clusters, noun and verb, the ‘ing’ feature – to        butional analysis. Cognition, 63(2), 121-170.
the verb cluster.                                                  Clark, A. (2003). Combining distributional and morpholog-
   The comparison with hierarchical clustering (HC) using            ical information for part of speech induction. In Eacl’03:
local context produced comparable results. While HC is               Proceedings of the tenth conference on european chapter of
slightly better at picking out low-level subclasses, it has no       the association for computational linguistics (pp. 59–66).
principled way of associating levels of the hierarchy with         Collins, A. M., & Quillian, M. R. (1969). Retrieval Time
relations. Therefore, the results of HC have less predictive         from Semantic Memory. JVLVB, 8, 240–248.
power than the representations derived by our model. In fact,      Elman, J. L. (1991). Distributed representations, simple
one might argue that the difference in performance is due to         recurrent networks, and grammatical structure. Machine
issues of implementation. Improving the algorithm’s ability          Learning, 7, 195-224.
to handle noise will allow for a better comparison on corpus       Grimshaw, J. (1981). Form, function, and the language ac-
data. This is supported by a comparison with HC on the orig-         quisition device. In C. Baker & J. McCarthy (Eds.), The
inal dataset of X-bar relations, where our algorithm performs        logical problem of language acquisition (p. 165-182). MIT
slightly better.1                                                    Press.
   It is worth pointing out that the annotated hierarchies         Keil, F. C. (1979). Semantic and conceptual development.
model was originally developed to handle data from other             Cambridge, MA: Harvard University Press.
cognitive domains. Thus, it can be considered a domain-            Kilgarriff, A., Rychly, P., Smrz, P., & Tugwell, D. (2004).
general mechanism for acquiring hierarchical structure for           The sketch engine. In Proceedings of euralex. Lorient.
the purposes of probabilistic reasoning. It is remarkable that     Lin, D. (1998). Automatic retrieval and clustering of similar
the problem of learning syntactic categories appears to share        words. In Proceedings of the 17th international conference
high-level similarities with concept development.                    on computational linguistics.
                                                                   Macnamara, J. (1972). Cognitive basis of language learning
                         Future work                                 in infants. (Vol. 79).
We are currently working on a number of additional exper-          Mintz, T., Newport, E., & Bever, T. (2002). The distri-
iments intended to investigate the performance of the algo-          butional structure of grammatical categories in speech to
rithm on relations and features automatically extractable from       young children. Cognitive Science, 26, 393-424.
raw data. In particular, we would like to replace our results on   Pinker, S. (1982). A theory of the acquisition of lexical
subject-of and object-of relations with the ordering relations       interpre-tive grammars. In J. Bresnan (Ed.), The mental
‘precedes’ and ‘follows’. One issue is that it is not feasible       representation of grammatical relations (p. 655-726). MIT
to use linear order naively, since many important ordering re-       Press.
lations are not adjacent. For example, the order of the verb       Redington, M., Chater, N., & Finch, S. (1998). Distribu-
and its object is often interrupted by a determiner (as in ’ate      tional information:a powerful cue for acquiring syntactic
an apple’). While this is a problem for a naive automatic text       categories. Cognitive Science, 22, 425-469.
analysis, there are reasons to believe it does not present such a  Rondal, J. A., & Cession, A. (1990). Input evidence regarding
problem for children, who are probably able to filter out low-       the semantic bootstrapping hypothesis. Journal of Child
saliency unstressed words from familiar content words. Thus,         Language, 17, 711-717.
extracting precedence relations involves finding an appropri-      Roy, D., Kemp, C., Mansinghka, V., & Tenenbaum, J. (2006).
ate way of automatically filtering important words. Another          Learning annotated hierarchies from relational data. In
direction we are pursuing is the automatic identification of         Proceedings of neural information processing systems 19
semantic (pragmatic) and morphological features. Initially,          (NIPS).
the latter can be accomplished in a supervised way, while the
ultimate goal is to rely on completely unsupervised extrac-
tion. Our strategy with respect to the semantic features is to
obtain a corpus annotated with pragmatic cues (agent, action,
patient, goal). Last but not least, we are working on extend-
ing our model to scale up to larger datasets in order to im-
prove the hierarchies at the subordinate level, and to provide
    1 For example, HC clusters ’of’ with the verbs.
                                                               634

