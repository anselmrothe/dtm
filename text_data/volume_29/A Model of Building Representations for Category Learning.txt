UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Model of Building Representations for Category Learning
Permalink
https://escholarship.org/uc/item/15c2j1wg
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Vanpaemel, Wolf
Lee, Micheal D.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                    A Model of Building Representations for Category Learning
                                      Wolf Vanpaemel (wolf.vanpaemel@psy.kuleuven.be)
                                                 Department of Psychology, K.U. Leuven
                                                Tiensestraat 102, B-3000, Leuven, Belgium
                                                   Michael D. Lee (mdlee@uci.edu)
                                    Department of Cognitive Sciences, University of California, Irvine
                                                           Irvine, CA, 92697-5100
                               Abstract                                   but more fundamental representational adaptation during cat-
                                                                          egory learning is not accommodated. Similarity-based repre-
   We develop a model of the interaction between representation           sentational modeling, on the other hand, has as its end goal
   building and category learning. Our model is a hierarchical            the learning of the representations, and does not extend to ac-
   extension of Nosofsky’s (1986) Generalized Context Model of
   category learning, based on the extended set of stimulus rep-          counting for category learning behavior.
   resentation possibilities developed by Vanpaemel, Storms and              We describe a hierarchical model that relates a represen-
   Ons (2005). Using Bayesian inference, the model provides an            tation building process to category learning behavior. It
   account of the representation people are using, and what pro-          builds on the Varying Abstraction Model (VAM: Vanpaemel,
   cess generated that representation. We apply the model to data         Storms, & Ons, 2005). The VAM was designed to address the
   sets from four category learning tasks, and demonstrate how
   the results inform the prototype vs exemplar representation de-        debate regarding the merits of exemplar- and prototype-based
   bate, and the similarity- vs rule-based categorization debate.         models of category learning. The VAM does this by specify-
   Keywords: Category Representation; Hierarchical Bayesian
                                                                          ing a class of representations for learning category structures
   Models; Generalized Context Model                                      that includes exemplar and prototype representations as spe-
                                                                          cial cases. Using the category learning processes of the GCM,
                                                                          the VAM makes inferences about representations, based on
                           Introduction                                   the decisions made in a category learning task. In effect, the
A fundamental challenge for cognitive science is to under-                VAM provides a model of what representations might be used
stand the interaction between building stimulus representa-               for category learning. Our extension is to include a represen-
tions and learning category structures. When people learn                 tational process describing how those representations might
categorical associations, they usually rely, in part, on internal         be generated. With this extension, we can make inferences
representations of stimuli. Learning whether or not a newly               about the representation building processes used to support
encountered breed of dog is dangerous is facilitated by think-            their learning of categories.
ing about the known hostilities of familiar dogs. In this sense,             In this paper, we test the hierarchical extension of the VAM
stimulus representations are the building blocks for learning             by re-analyzing data from seminal category learning tasks
category structures.                                                      (Nosofsky, 1986; Nosofsky, Clark, & Shin, 1989). These ex-
   Most representations, however, are themselves learned.                 periments involve single subjects or groups of subjects learn-
And often category membership is a key source of the stim-                ing various two-category structures over a small number of
ulus similarity that can guide the building of representations.           stimuli, all of which have two-dimensional spatial represen-
The similarity between Las Vegas and Atlantic City derives,               tations. The collected data measure the way the trained stim-
in large part, from their shared association to the gambling in-          uli, and a set of additional stimuli not seen in training, were
dustry. In this sense, category structures are building blocks            categorized.
for learning stimulus representations.
   Many successful models of category learning and stim-                            A Representation Building Process
ulus representation make assumptions along these lines.
                                                                          The Class of Representations
Exemplar-based models of category learning, like the Gen-
eralized Context Model (GCM: Nosofsky, 1986), make cate-                  The VAM considers all possible representations that can be
gorization decisions using memory representations of related              obtained by merging the stimuli belonging to each category.
stimuli. Similarity-based models of representation, including             The exemplar representation is the one where no stimuli are
dimensional and featural models (Goldstone, 1999), rely on                merged. The prototype representation is the one where all
similarity judgments sensitive to shared categorical associa-             stimuli are merged. Intermediate possibilities involve some
tions between stimuli.                                                    of the stimuli being merged.
   Rarely, though, is the tight coupling of representational and             The bottom row of Figure 1 provides a concrete example
category learning processes modeled. Models of category                   of VAM representations. Panel A shows the exemplar repre-
learning usually assume a fixed stimulus representation. Of-              sentation of four stimuli in two-dimensional space. Panel B
ten these representations are derived using similarity-based              shows the representation created when two of the stimuli are
models, and sometimes the representations are manipulated                 merged, with the original stimuli shown as smaller squares,
during category learning by processes like selective attention,           joined by lines to their merged representation.
                                                                     1605

    A        B         C         D           E      F      G        H          I       J       K        L        M       N        O
Figure 1: The bottom row shows the 15 possible VAM representations for a four-stimulus category structure. The top five rows
give the probability distribution over these 15 representations for the generation process, corresponding to parameterizations
θ = .99, γ = 1 (white), θ = .01, γ = 1 (black), θ = .7, γ = 0 (light gray), θ = .7, γ = 1 (medium gray), and θ = .7, γ = 10 (dark
gray).
   The remaining panels in the bottom row of Figure 1 show              The parameter γ ≥ 0 controls the level of emphasis given
the VAM representations resulting from averaging other stim-            to similarity in determining the pair to be merged. As γ in-
uli. Panels B–G show the results of single merge, leaving               creases, the maximally similar pair dominates the others, and
three representations, while Panels H–N show the results of             will be chosen as the pair to be merged with probability ap-
two merges, leaving two representations. The final VAM rep-             proaching one. At the other extreme, when γ = 0, similarity is
resentation in Panel O shows the prototype representation in            not taken into account. All choices of pairs to merge then are
which all four stimuli are merged into a single representation.         equally likely, and the merge is essentially chosen at random.
                                                                        Values of γ between these two extremes result in intermediate
The Generation of VAM Representations                                   behavior.
We propose an iterative process for generating the class of
VAM representations. The process has two parts; one control-
                                                                           Given a value for the θ and γ parameters, every VAM rep-
ling how many merges are made, and another deciding which
                                                                        resentation has some probability of being generated by the
stimuli are merged. Formally, 0 ≤ θ ≤ 1 is a parameter giving
                                                                        process just described. The top five rows in Figure 1 show
the probability that an additional merge will take place, and
                                                                        the probability of the VAM representations being generated
the iterative process will continue. This means, at any stage,
                                                                        for different parameters. In the top row θ = 0.99, so merging
there is a 1 − θ probability that the current representation will
                                                                        is very likely, and hence the prototype representation almost
be maintained as the final one.
                                                                        always results. In the second row θ = 0.01, so merging is very
   When an additional merge is undertaken, it is based on the
                                                                        unlikely, and hence the exemplar representation is almost al-
similarities between all of the current representations (i.e., the
                                                                        ways retained.
original stimuli, or their merged replacement). The similarity
between the ith and jth representations is, consistent with the
GCM, modeled as an exponentially decaying function of the                  The third, fourth and fifth rows show, for a fixed θ = 0.7,
distance between their points, according to a Minkowski r-              the effect of the γ parameter. When γ = 0 in the third row,
metric:                                                                 the exemplar and prototype representations are most likely,
                          "                      #1/r                 but all others are possible. In particular, any representation
                                                                     arising from a single merge is equally likely, and any repre-
              si j = exp − ∑ vik − v jk
                                               r
                                                         .     (1)      sentation arising from two merges is equally likely, because
                               k
                                                       
                                                                        the pair of stimuli to be merged is chosen at random. In the
Given these similarities, across all pairs in the current repre-        fourth row, when γ = 1, representations like B and L that in-
sentation, the probability, mi j , of choosing to merge the pair        volve merging similar stimuli become much more likely, al-
(i, j) is given by an exponentiated Luce-choice rule                    though some other possibilities remain. Once γ = 10 in the
                                                                        fifth row, only the most similar stimuli are merged, and B and
                                  (exp si j )γ                          L are the only intermediate possibilities between exemplar
                      mi j =                     γ.            (2)
                             ∑x ∑y≥x (exp sxy )                         and prototype representation with non-negligible probability.
                                                                   1606

                                               θ   γ                       multidimensional scaling analysis. The training stimuli are
                                                                           the ones assigned to categories, and so are the basis for the
                                                                           representation building process.
                                                                              Formally, the parameters θ and γ determine the index x of
                              j = 1, . . ., Nx        i = 1, . . ., N      the VAM representational class, which we write
                                  k = 1, 2               k = 1, 2                                                            
                         v jk                    x        pik                                     x ∼ Merge θ, γ .                           (3)
                                                                           This index defines the Nx points in the VAM representation,
                                                                           with vi denoting the ith of these representations. We used
                              j = 1, . . ., Nx                             Monte Carlo estimates of p (x | θ, γ) to define the Merge dis-
                                                     i = 1, . . ., M       tribution, found by simulating the iterative process over the
                                                         k = 1, 2          stimuli and category structures used in the applications across
       w                 di j                             p0ik             the grid θ = (0.025, 0.05,..., 0.975) and γ = (0, 0.1, . .., 10).
                                                                              For this representation component of the model, we use
                                                                           priors
                                                                                                                              
                                                                                                θ ∼ Uniform 0, 1 ,
       c                 ηi j                                                                                              
                                                                                                γ ∼ Erlang 1 .                               (4)
                                                                           The uniform prior for the rate θ is an obvious choice. The
                            i = 1, . . ., N + M
                                                                           Erlang prior for γ gives support to all positive values, but has
                                                                           most density around the modal value one, corresponding to
                                                                           our prior expectations.
                         ri
                                                                           Categorization Having generated the VAM representation,
                                                                           the remainder of the model deals with the categorization pro-
                                                                           cess, and follows closely the GCM. The only difference is
                                                                           that the category similarity of the N + M stimuli presented to
       aj                ki            ti                                  participants is formed by summing over their similarities to
                                                                           the Nx representations constituting the VAM representation.
   j = 1, . . ., N     i = 1, . . ., N + M                                    First, the attention-weighted distances between the stimuli
                                                                           and representations are calculated, according the Minkowski
                                                                           r-metric, so that
Figure 2: Graphical model representation of the hierarchical
VAM used in the applications.                                                                             r                        r  1
                                                                                  di j =    w (pi1 −v j1 ) + (1−w) (pi2 −v j2 ) r ,          (5)
                                                                           for the training stimuli, and analogously for the additional
                                                                           stimuli, where w is the attention weight parameter measuring
                   The Hierarchical VAM                                    the relative emphasis given to the first stimulus dimension
Graphical Model Notation Figure 2 shows as a graphical                     over the second.
model the specific adaptation of the hierarchical VAM we ap-                  From the distances, the generalization gradient with scale
plied to re-analyze the Nosofsky (1986) and Nosofsky et al.                parameter c and shape α determines the similarities,
(1989) data. Figure 2 uses a standard graphical model rep-                                                          
                                                                                                ηi j = exp −cdiαj .                          (6)
resentation (Jordan, 2004). Nodes represent the labeled vari-
ables. The directed graph structure indicates dependencies                    The assignment of the representations to the two categories
between the variables, with children depending on their par-               is defined by the category structure, and is given by indicator
ents. Stochastic variables have single-borders and determin-               variables, so that a j = 1 means the jth representation belongs
istic variables have double-borders. Observed variables have               to Category A, with a j = 0 otherwise. For the current re-
shading and unobserved variables have no shading. Continu-                 analyses we ignore the possibility of response bias, and so
ous variables have circular nodes and discrete variables have              the probability of the ith stimulus being chosen as a member
square nodes. Independent replications in the model are rep-               of the Category A is determined by the sum of similarities
resented by enclosing parts of the graph in square boundaries              between the ith stimulus to the Nx representations in each cat-
called plates, and are labeled by the indexing of the replica-             egory, according to a Luce choice rule,
tions.
                                                                                                               ∑ j a j ηi j
                                                                                            ri =                                    .        (7)
                                                                                                  ∑ j a j ηi j + ∑ j (1 − a j )ηi j
Representation At the top of Figure 2 are the coordinate
locations pik and p0ik for the N training and M additional stim-              Finally, the response probabilities are used to account for
uli, respectively, in k = 1, 2 dimensions, as found by previous            the observed data, D, which are the counts, ki of the number
                                                                      1607

         1.00                        1.00                                       0.05
      Subject 1                    Subject 2
                                                                                0.95                         1.00
                                                           10
      Subject 1
      Subject 2
                                                           5   γ              Subject 1                    Subject 2
                                                                                                                                     10
                                                                               Subject 1
                                                                               Subject 2
                                                           1
                                                           0
                                                                                                                                     5   γ
0       0.2       0.4        0.6      0.8      1
                        θ
                                                                                                                                     1
Figure 3: Hierarchical VAM account of the Nosofsky (1986)                                                                            0
Interior-Exterior category data.
of times the ith stimulus was chosen in Category A out of                 0       0.2      0.4       0.6    0.8        1
                                                                                                 θ
the ti trials it was presented. The counts ki follow a Binomial
distribution
                                                                         Figure 4: Hierarchical VAM account of the Nosofsky (1986)
                     ki ∼ Binomial ti , ri .                (8)           Diagonal category data.
   For this categorization component of the model, we use
priors
                                                                           Our primary interest is on two posterior distributions: x | D,
                   w ∼ Uniform 0, 1 ,                                     which describes the inferences made by the model about what
                                                                         VAM representation is being used; and (θ, γ) | D, which de-
                  c2 ∼ Gamma ε, ε .                    (9)                scribes inferences about what process people used to generate
The uniform distributions for w is again an obvious choice.               that representation.
The c parameter functions as an inverse scale (i.e., 1/c scales           Interior-Exterior from Nosofsky (1986) Figure 3 summa-
the distances), implying c2 functions as a precision, and so              rizes the results from the Interior-Exterior category struc-
is given the standard near non-informative Gamma prior with               ture. The upper panels show the VAM representations in-
ε = .001 set near zero.                                                   ferred from the categorization decisions made by each sub-
                                                                          ject. The stimuli for the two categories are shown by circles
                            Applications                                  and squares, together with any merged representations. For
This section presents four applications of our model to the               both subjects, there is a single VAM representation that cap-
seminal category learning data reported and analyzed by the               tured virtually all of the posterior probability. These repre-
GCM in Nosofsky (1986) and Nosofsky et al. (1989). We                     sentations are extremely similar, and involve a single merge
consider seven of the eight available data sets, leaving the ‘Di-         of two of the stimuli in the interior category.
mensional’ category structure, because its heavy reliance on                 The lower panels of Figure 3 show the joint posterior distri-
selective attention manipulations is not well accommodated                bution over θ and γ inferred from the data. The main central
by our current model. We follow the original analyses in us-              panel shows samples from this joint posterior for each sub-
ing r = α = 2 for Nosofsky (1986), and r = α = 1 for Nosof-               ject. The side panels show the marginal distributions for θ
sky et al. (1989).                                                        and γ for each subject. These distributions are also extremely
                                                                   1608

       0.12                                                                                        0.22
                                                                                1.00               0.78                 1.00
       0.88                         1.00
                                                                                None               Rule 1           Rule 2
                                                                                                                                       10
                                                                                 None
                                                                                 Rule 1
                                                                                 Rule 2
    Subject 1                     Subject 2
                                                            10
      Subject 1
      Subject 2
                                                                                                                                       5   γ
                                                            5   γ
                                                                                                                                       1
                                                                                                                                       0
                                                            1
                                                            0
                                                                            0          0.2   0.4            0.6   0.8          1
                                                                                                     θ
                                                                            Figure 6: Hierarchical VAM account of the Nosofsky et al.
0       0.2       0.4       0.6    0.8        1                             (1989) category data.
                        θ
Figure 5: Hierarchical VAM account of the Nosofsky (1986)
Criss-Cross category data..                                                 Figure 6), will be categorized quite differently by the two sub-
                                                                            jects.
                                                                            Criss-Cross from Nosofsky (1986) Figure 5 summarizes
                                                                            the results from the Criss-Cross category structure. The rep-
similar for both subjects. In both cases, the value of θ is likely          resentations are similar for both subjects, and involve merg-
to be relatively low, indicating the use of a near-exemplar rep-            ing dissimilar stimuli within each category. The joint posteri-
resentation.                                                                ors are also similar for both subjects, with the posterior for γ
Diagonal from Nosofsky (1986) Figure 4 summarizes the                       taking small values (i.e., less than one) indicating the merging
results from the Diagonal category structure. Subject 1 has                 of dissimilar stimuli.
two VAM representations with posterior probabilities of 0.95                   These results suggest a deficiency in our representation
and 0.05, while the second subject has a single representation.             building process. A more natural generative account of the
In this case, there are significant individual differences, with            representations in Figure 5 would involve the deletion of
Subject 1 using some merging, but maintaining many stimuli,                 stimuli, rather than relying solely on merging for building
while Subject 2 uses a prototype representation. The θ pa-                  representations.
rameter captures these individual differences, taking values
close to one for Subject 2, but lower values for Subject 1.                 Nosofsky et al. (1989) Nosofsky et al. (1989) consid-
                                                                            ered a single category learning task, similar in structure to
   This model analysis provides a useful explanation of some                the Nosofsky (1986) Interior-Exterior task, but using differ-
striking features of the raw data in Nosofsky (1986, Table 3).              ent stimuli, and giving different instructions to three groups
In particular, the use of prototype representations results in              of subjects. The first group was given no special instructions,
a loss of sensitivity to the diagonal structure of the category             and so was expected to learn the category structure using
boundary, and so explains the observation that the additional               the similarity-based principles that underly the GCM. The re-
stimuli labeled ‘7’ and ‘10’, located just below and right, and             maining two groups were instructed to use one of two simple
just above and left of center respectively (see Nosofsky, 1986,             rules accurately describing the category structure.
                                                                     1609

   Figure 6 summarizes the results from the hierarchical VAM            totype representations, as well as a bias towards similarity-
analysis of each group. The group with no special instructions          based representational compression. Our last two applica-
are accounted for by a VAM representation that does follow              tions show that these biases can be overcome by data, and so
stimulus similarity, by collapsing the similar stimuli in the           we believe the hierarchical VAM strikes the right balance of
interior category to a prototype, and largely preserving the            having theoretically-based expectations, without losing flexi-
less similar stimuli as exemplars in the exterior category.             bility by simply assuming the basic tenets of those theories.
   The groups given the rule instructions, however, do not fol-            Against these strengths, the applications presented here
low stimulus similarity closely, especially through their merg-         suggest a deficiency in the particular representation building
ing of the same two dissimilar exterior stimuli. An examina-            process we proposed. It seems, at least for some categoriza-
tion of the rules reveals that both had in common a logical             tion tasks, people ignore a subset of the stimuli to learn the
proposition that directly corresponds to these two dissimilar           category structure. Future work intends to refine the current
stimuli, and so encouraged this merging.                                model with a more general representation building process
   As in the previous example, the posterior for γ neatly dis-          that allows for this possibility.
tinguishes whether or not representations were similarity-
based, taking large values for the group given no special in-                               Acknowledgments
structions, and values less than one for the two rule groups.           WV was supported by the Research Council of the University
                                                                        of Leuven (IDO/02/004).
                          Discussion
The extension to the VAM developed here provides a process                                       References
model for the class of representations it previously just as-           Goldstone, R. L. (1999). Similarity. In R. A. Wilson &
sumed. The obvious benefit of this extension is that it permits                F. C. Keil (Eds.), MIT Encyclopedia of the Cognitive
inferences about the process of representation building. The
                                                                               Sciences (pp. 763–765). Cambridge, MA: MIT Press.
applications demonstrated the ability of the model to make
inferences about two theoretically interesting parameters: θ,           Jordan, M. I. (2004). Graphical models. Statistical Science,
measuring the extent of compression, which is relevant to the                  19, 140–155.
exemplar vs prototype debate; and γ, measuring to what ex-              Nosofsky, R. M. (1986). Attention, similarity, and the
tent compression is based on stimulus similarity, which is rel-                identification-categorization relationship. Journal of
evant to the similarity- vs rule-based categorization debate.                  Experimental Psychology: General, 115(1), 39–57.
   A further, perhaps less obvious, contribution of the repre-          Nosofsky, R. M., Clark, S. E., & Shin, H. J. (1989). Rules and
sentation building process is that it naturally overlays a sen-                exemplars in categorization, identification, and recog-
sible inductive bias on the VAM class of representations. A                    nition. Journal of Experimental Psychology: Learning,
strength of the VAM is that it considers a wide range of rep-                  Memory, and Cognition, 15(2), 282–304.
resentational possibilities, but a reasonable criticism is that it      Vanpaemel, W., Storms, G., & Ons, B. (2005). A vary-
gives each of these equal prior status. Intuitively, some the
                                                                               ing abstraction model for categorization. In B. Bara,
VAM representations seem more reasonable than others, and
the prior predictions made by the hierarchical extension, as                   L. Barsalou, & M. Bucciarelli (Eds.), Proceedings of
shown in Figure 1 seem intuitively satisfying. In particular,                  the 27th Annual Conference of the Cognitive Science
there is a strong inductive bias towards the exemplar and pro-                 Society (pp. 2277–2282). Mahwah, NJ: Erlbaum.
                                                                   1610

