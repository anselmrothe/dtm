UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Two Views of the World: Active Vision in Real-World Interaction
Permalink
https://escholarship.org/uc/item/7ms4z979
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
yu, Chen
Smith, Linda B.
Christensen, Mark
et al.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                Two Views of the World: Active Vision in Real-World Interaction
               Chen Yu, Linda B. Smith, Mark Christensen, Alfredo Pereira (chenyu@indiana.edu)
              Department of Psychological and Brain Sciences, and Cognitive Science Program, Indiana University
                                                  Bloomington, IN, 47405 USA
                            Abstract                                 active movement –points, head turns, and eye gaze – in
                                                                     social dynamics and particularly in establishing joint
  An important goal in cognitive development research is
                                                                     attention (see Smith & Breazeal, 2007, for a review).
an understanding of the real-world physical and social
                                                                     Computational theorists and roboticists (e.g. Ballard et al.,
environment in which learning takes place. However, the
                                                                     1997) have also demonstrated the computational advantages
relevant aspects of this environment for the learner are only
                                                                     of what they call “active vision”, how an observer – human
those that make contact with the learner’s sensory system.
                                                                     or robot – is able to understand a visual environment more
In light of this, we report new findings using a novel method
                                                                     effectively and efficiently by interacting with it. This is
that seeks to describe the visual learning environment from
                                                                     because perception and action form a closed loop;
a young child’s point of view. The method consists of a
                                                                     attentional acts are preparatory to and made manifest in
multi-camera sensing environment consisting of two head-
                                                                     action while also constraining perception in the next
mounted mini cameras that are placed on both the child’s
                                                                     moment.
and the parent’s foreheads respectively. The main result is
                                                                       Nonetheless, most previous studies of children’s attention
that the adult’s and child’s views are fundamentally
                                                                     and learning have been conducted using macro-level
different in (1) the spatial distributions of hands and
                                                                     behaviors and in constrained situations, without considering
everyday objects in the child’s visual field and where they
                                                                     the role of active vision and the perception-action loop. This
are in the parent’s field; (2) the salience of individual
                                                                     is in part a consequence of the typical method which uses a
objects and hands in those two visual fields; and (3) the
                                                                     third-person camera (or several) to record the child’s stream
temporal dynamic structures of objects and hands in two
                                                                     of activities in context. Such recordings provide the view of
views. These findings have broad implications for how one
                                                                     an outside observer but not the view of the actively engaged
studies and thinks about developmental processes.
                                                                     cognitive system. Further, these views are typically coded by
  Keywords: Cognitive Development, Embodied Cognition,
                                                                     human coders who watch these third person views, a
Joint Attention
                                                                     process which is both time consuming and biased, as these
                                                                     coders are outside observers with their own psychology and
                        Introduction
                                                                     parsing of the events. Understanding how developmental
    Children learn about their world – about objects, actions,       process emerges in second-by-second and minute-by-minute
other social beings, and language -- through their second-           sensorimotor interactions requires capturing (and describing
by-second, minute-by-minute sensorimotor interactions.               without bias) the first-person view as it is actively generated
Visual information plays a critical role in this early learning.     by the young learner.
Before babies with normal vision can talk or walk, and                 The larger goal of this research enterprise is to understand
before the emergence of any social intelligence to guide             the building blocks for fundamental cognitive capabilities
their everyday interaction with caregivers, babies are able to       and, in particular, to ground social interaction and the theory
perceive and parse their visual environment and are able to          of mind in sensorimotor processes. To these ends, we have
move their eyes and head to select visual targets (objects or        developed a new method for studying the structure of
people) in space. Infants have the opportunity to                    children’s dynamic visual experiences as they relate to
continuously process complex visual input, and accumulate            children’s active participation in a physical and social
knowledge from the visual environment. This real time                world. In this paper, we report results from a study that
visual information, plus its control through gaze direction          implemented a sensing system for recording the visual input
and visual attention, contributes to the development of other        from both the child’s point of view and the parent’s
sensory, cognitive and social capabilities. Indeed,                  viewpoint as they engage in toy play. With this new
developmentalists such as Gibson (Gibson, 1969) and Ruff             methodology, we compare and analyze the dynamic
(Ruff, 1989) have documented the powerful dynamic visual             structure of visual information from these two views. The
information that emerges as infants and children move their          results show that the dynamic first-person perspective from
eyes, heads and bodies, and as they act on objects in the            a child is substantially different from either the parent’s or
world. In addition, Bertenthal and Campos (1987) have                the third-person (experimenter) view commonly used in
shown how movement – crawling and walking over, under,               developmental studies of both the learning environment and
and around obstacles – creates dynamic visual information            parent-child social interaction. The key differences are
crucial to children’s developing knowledge about space.              these: the child’s view is much more dynamically variable,
Researchers studying the role of social partners in                  more tightly tied to the child’s own goal-directed action, and
development and problem solving also point to the body and           more narrowly focused on the momentary object of interest.
                                                                 731

                                                                        eyes. The angle of the camera was adjustable. Input power
                                                                        and video output to these cameras went through a camera
                                                                        cable connected to a wall socket, which was long enough to
                                                                        not cause any movement restriction while participants were
                                                                        sitting down. Both cameras were connected via standard
                                                                        RCA cables to a digital video recorder card in a computer in
                                                                        the room adjacent to the experiment room.
                                                                             The head camera field is approximately 90 degrees,
                                                                        which is comparable to the visual field of older infants,
                                                                        toddlers and adults (van Hof van Duin & Mohn, 1986;
                                                                        Mohan, Dobson, Harvey, Delaney, & Leber, 1999). One
                                                                        possible concern in the use of a head camera is that the head
                                                                        camera image changes with changes in head movements but
                                                                        not in eye movements. This problem is reduced by the
                                                                        geometry of table-top play. Yoshida & Smith (2007)
                                                                        documented this in a head-camera study of toddlers by
                                                                        independently recording eye-gaze, and showed that small
                                                                        shifts in eye-gaze direction unaccompanied by a head shift
Figure 1: Multi-camera sensing system. The child and the mother         do not yield distinct table-top views. Indeed, in their study
play a set of toys at a table. Two mini cameras are placed onto the     90% of head camera video frames corresponded with
child’s and the mother’s heads respectively to collect visual           independently coded eye positions.
information from two first-person views. A third camera mounted
on the top of the table records the bird-eye view of the whole
interaction.
                                                                        Bird-Eye View Camera. A high-resolution camera was
                                                                        mounted right above the table and the table edges aligned
                                                                        with edges of the bird-eye image. This view provided visual
           Multi-Camera Sensing Environment                             information that was independent of gaze and head
 The method uses multi-camera sensing system in a                       movements of a participant and therefore it recorded the
 laboratory environment wherein children and parents are                whole interaction from a third-person static view. An
 asked to freely interact with each other. As shown in Figure           additional benefit of this camera lied in the high-quality
 1, participant interactions are recorded by three cameras              video, which made our following image segmentation and
 from different perspectives – one head-mounted camera                  object tracking software work more robustly compared with
 from the child’s point of view to obtain an approximation of           two head-mounted mini cameras. Those two were light-
 the child’s visual field, one from the parent’s viewpoint to           weighted but with a limited resolution and video quality due
 obtain an approximation of the parent’s visual field, and one          to the small size.
 from a top-down third-person viewpoint that allows a clear
                                                                            Parent-Child Joint Interaction Experiment
 observation of exactly what was on the table at any given
 moment (mostly the participants’ hands and the objects                 Participants. The target age period for this study was 18 to
 being played with).                                                    20 months. We invited parents in the Bloomington, Indiana
 Interaction Environment. The study was run in a 3.3m ×                 area to participate in the experiment. Nine dyads of parent
 3.1m room. At the center of the room a 61cm × 91cm ×                   and child were part of the study. One child was not included
 64cm table was placed. The table surface was covered in a              because of fussiness before the experiment started. For the
 white soft blanket and the edges were clearly marked with              child participants included, the mean age was 18.2, ranging
 black tape. A high chair for the child and a small chair for           from 17.2 to 19.5 months. Three of the included children
 the parent was placed facing each other. The walls and floor           were female and five were male. All participants were white
 of the room were covered with white fabrics. Both                      and middle-class.
 participants were asked to wear white T-shirts as well. In             Stimuli. Parents were given six sets (three toys for each set)
 this way, from any image collected from any camera, white              in this free-play task. The toys were either rigid plastic
 pixels can treated as background while non-white pixels are            objects or plush toys (three of the total 18). Most of them
 either objects on the table, the edges of the table, the hands,        had simple shapes and either a single color or an overall
 or the faces of participants.                                          main color. Some combinations of objects were selected to
    Head-Mounted Cameras. Two light-weight head-                        elicit an action, especially evident to an adult asked to play
 mounted mini cameras (one for the child and another for the            with them. Figure 2 shows all the stimuli used in this study.
 parent) were used to record the first-person view from both            Procedure. The study was conducted by three
 the child and the parent’s perspectives. These cameras were            experimenters: one to distract the child, another to place the
                                                                        head-mounted cameras and a third one to control the quality
 mounted on two everyday sports headbands, each of which
                                                                        of video recording. Parents were told that the goal of the
 was placed on one participant’s forehead and close to his
                                                                        study was simply to observe how they interacted with their
                                                                    732

   Figure 2: Objects used in the parent-child       Figure 3: the overview of data processing using computer vision techniques. We
   joint interaction study. The ruler shown in      first remove background pixels from an image and then spot objects and hands in
   each set is 12’’ in length.                      the image based on pre-trained object models. The visual information from two
                                                    views is then aligned for further data analyses.
child while playing with toys and that they should try to             objects, hands, and faces, from sensory data in each of three
interact as naturally as possible. Upon entering the                  cameras. These are based on computer vision techniques,
experiment room, the child was quickly seated in the high             and include three major steps (see Figure 3). Given raw
chair and several attractive toys were placed on top of the           images from multiple cameras, the first step is to separate
table. One experimenter played with the child while the               background pixels and object pixels. This step is not trivial
second experimenter placed a sports headband with the                 in general because two first-view cameras attached on the
mini-camera onto the forehead of the child at a moment that           heads of two participants moved around all the time during
he appeared to be well distracted. Our success rate in                interaction causing moment-to-moment changes in visual
placing sensors on children is now at over 80%. After this,           background. However, since we designed the experimental
the second experimenter placed the second head-mounted                setup (as described above) by covering the walls, the floor
camera onto the parent’s forehead and close to her eyes.              and the tabletop with white fabrics and asking participants
   To calibrate the horizontal camera position in the forehead        to wear white cloth, we simply treat close-to-white pixels in
and the angle of the camera relative to the head, the                 an image as background. Occasionally, this approach also
experimenter asked the parent to look into one of the objects         removes small portions of an object that have light
on the table, placed close to the child. The third                    reflections on them as well. (This problem can be fixed in
experimenter controlling the recording in another room                step 3). The second step focuses on the remaining non-
confirmed if the object was at the center of the image and if         background pixels and breaks them up into several blobs
not small adjustments were made on the head-mounted                   using a fast and simple segmentation algorithm. This
camera gear. The same procedure was repeated for the child,           algorithm first creates groups of adjacent pixels that have
with an object close to the child’s hands. After this                 color values within a small threshold of each other. The
calibration phase, the experimenters removed all objects              algorithm then attempts to create larger groups from the
from the table, asked the parent to start the experiment and          initial groups by using a much tighter threshold. This
left the room. The instructions given to the parent were to           follow-up step of the algorithm attempts to determine which
take all three objects from one set, place them on the table,         portions of the image belong to the same object even if that
play with the child and after hearing a command from the              object is broken up visually into multiple segments. For
experimenters, remove the objects in this trial and move to           instance, a hand may decompose a single object into several
the next set to start the next trial. There were a total of six       blobs. The third step assigns each blob into an object
trials, each about 1 minute long. The entire study, including         category. In this object detection task, we used Gaussian
initial setup, lasted for 10 to 15 minutes.                           mixture models to pre-train a model for each individual
   Image Segmentation and Object Detection                            object. By applying each object model to a segmented
The recording rate for each camera is 10 frames per second.           image, a probabilistic map is generated for each object
In total, we have collected approximately 10800 (10 × 60 ×            indicating the likelihood of each pixel in an image belongs
6 × 3) image frames from each interaction. The resolution of          to this special object. Next, by putting probabilistic maps of
image frame is 320 × 240.                                             all the possible objects together, and by considering spatial
   The first goal of data processing is to automatically              coherence of an object, our object detection algorithm
extract visual information, such as the locations and sizes of        assign an object label for each blob in a segmented image as
                                                                 733

     Figure 4: A comparison of the child’s and the parent’s visual fields. Each curve represents a proportion of an object in
     the visual field over the whole trial. The total time in a trial is about 1 minute (600 frames). The three snapshots show the
     image frames from which the visual field information was extracted.
shown in Figure 3. As a result of the above steps, we extract
useful information from image sequences, such as what
objects are in the visual field at each moment, and what are
the sizes of those objects, which will be used in the
following data analyses.
               Data Analyses and Results
The multi-camera sensing environment and computer vision
software components enable fine-grained description of
child-parent interaction and from two different viewpoints.
In this section, we report our preliminary results while
focusing on comparing sensory data collected
simultaneously from two views. We are particularly
interested in the differences between what a child sees and
what the mature partner sees.
           Visual Field in a First-Person View
The first analyses concern the overall distribution of objects           Figure 5: The top row shows the distribution of objects in two
and hands in both the child’s and the parent’s visual fields:            first-person views. The second row shows the distribution of
(1) where are hands in the child’s visual field and where are            hands and faces. The third row is the combination of the two
they in the parent’s field; (2) how those objects occupy the             showing the overall distribution of non-background visual
                                                                         information in two visual fields.
visual fields in two views respectively; and (3) what are the
differences of the spatial distributions of objects in those           moment by moment. In light of this general observation, we
visual fields. Figure 5 shows the overall occupation of                developed several metrics to quantify three aspects of the
objects and hands in two visual fields. The child’s view is a          differences between these two views.
closer “zoom-in” view of the physical environment such                    First, we measure the composition of visual field shown
that objects and hands are all over his visual field. In               in Figure 6(a). From the child’s perspective, objects occupy
contrast, there is more structure and regularities in the              about 20% of his visual field. In contrast, they take just less
parent’s view. For instance, objects are at the center of her          than 10% of the parent’s visual field. Although the
visual field and hands are most often in the peripheral field.         proportions of hands and faces are similar between these
Overall, objects and hands are more clustered in the parent’s          two views, a closer look of data suggests that the mother’s
view compared with the one from the child.                             face rarely occurs in the child’s visual field while the
       A Quantitative Comparison of Two Views                          mother’s and the child’s hands occupy a significant
Figure 4 shows the proportion of each object or hand in                proportion (~15%-35%) in some image frames. From the
one’s visual field over a whole trial (three snapshots taken           mother’s viewpoint, the child’s face is always around the
from the same moments from these two views). Clearly, the              center of the field while the hands of both participants occur
child’s visual field is substantially different from the               frequently but occupy just a small proportion of visual field.
parent’s. Objects and hands occupy the majority of the                    Second, Figure 6(b) compares the salience of the
child’s visual field and the whole field changes dramatically          dominating object in two views. The dominating object for a
                                                                  734

      Figure 6: We quantify and compare visual information from two views in three ways. Left: the occupation and
    composition of visual field. Middle: the salience of dominating objects. Right: the dynamics of visual field over time.
frame is defined as the object that takes the largest              attending at each moment to just one object. Parents, on the
proportion of visual field. Our hypothesis is that the child’s     other hand, don’t switch attended objects very often and all
view may provide a unique window of the world by filtering         the objects on the table are in their visual fields almost all
irrelevant information (through movement of the body close         the time.
to the object) enabling the child to focus on one object (or          The dynamics of their visual fields in terms of the change
one event) at a single moment. To support this argument,           of objects in visual field makes the same point. In the
the first metric used here is the percentage of the dominating     child’s view, on average, in each frame, 6% of the visual
object in the visual field at each moment. In the child’s          field consists of new objects, objects that are different from
view, the dominating object takes 12% of the visual field on       the just previous frame to frame. Only less than 2% of the
average while it occupies just less than 4% of the parent’s        parent’s visual field changes this way frame to frame over
field. The second metric measures the ratio of the                 time. The child’s view is more dynamic and that offers
dominating object vs. other objects in the same visual field,      potentially more spatio-temporal regularities that may be
in terms of the occupied proportion in an image frame. A           utilized by leading young learners to pay attention to the
higher ratio would suggest that the dominating object is           more informative (from their point of view) aspects of a
more salient and distinct among all the objects in the scene.      cluttered environment.
Our results show a big difference between two views. In
                                                                                          General Discussion
more than 30% of frames, there is one dominating object in
the child’s view which is much larger than other objects                                      Embodiment
(ratio > 0.7). In contrast, in less than 10% of time, the same     There   are  two  practical  reasons that the child’s view is quite
phenomenon happens in the parent’s view.                           different   from   the  parent’s   view. First, because they are
   This result suggests not only that children and parents         small,   their  head  is  close  to  the tabletop. Therefore, they
have different views of the environment but also that the          perceive    a  “zoom-in”,    more   detailed,  and more narrowed
child’s view may provide more constrained and clean input          view   than   taller parents.   Second,    at the   behavioral level,
to facilitate learning processes which don’t need to handle a      children   move    objects   and  their  own   hands    close to their
huge amount of irrelevant data because there is just one           eyes   while   adults  rarely  do  that.  Both   explanations   above
object (or event) in view at a time. We also note that this        can   account    for  dramatic    differences   between     these two
phenomena doesn’t happen randomly and accidently.                  views.   Both   factors  highlight  the  crucial  role of the  body in
Instead, the child most often intentionally moves his body         human     development     and  learning.  The  body    constrains and
close to the dominating object and/or uses his hands to bring      narrows visual information perceived by a young learner.
the object closer to his eyes which cause one object to            One challenge that young children face is the uncertainty
dominate the visual field. Thus, the child’s own action has        and ambiguity inherent to real-world learning contexts: In
direct influences on his visual perception and most likely         object recognition, learners need to select the features that
also on the underlying learning processes that may be tied to      are reliably associated with an object from all possible
these perception-action loops.                                     visual features; and in word learning, they need to select the
   The third measure is the dynamics of visual field, shown        relevant object (at the moment) from among all possible
in Figure 6(c). The dominating object may change from              referents on a table. In marked contrast to the mature
moment to moment, and also the locations, appearances and          partner’s view, the visual data from the child’s first-person
the sizes of other objects in the visual field may change as       view camera suggests a visual field filtered and narrowed by
well. Thus, we first calculated the number of times that the       the child’s own action. Whereas parents may selectively
dominating object changed. From the child’s viewpoint,             attend through internal processes that increase and decrease
there are on average 23 such object switches in a single trial     the weights of received sensory information, young children
(about 1 minute or 600 frames). There are only 11 per trial        may selectively attend by using the external actions of their
from the parent’s view. These results together with the            own body. This information reduction through their bodily
measures in Figure 6(b) suggest that children tend to move         actions may remove a certain degree of ambiguity from the
their head and body frequently to switch attended objects,         child’s learning environment and by doing so provide an
                                                               735

advantage to bootstrap learning. This suggests that an adult         technology). From this perspective, the head-mounted
view of the complexity of learning tasks may often be                camera is complimentary to the remote eye-tracking
fundamentally wrong. Young children may not need to deal             technique which can obtain precise eye gaze location but
with all the same complexity from an adult’s viewpoint –             just in a 2-dimensional pre-defined screen.
some of them that complexity may be automatically solved
by bodily action and the corresponding sensory constraints.
                                                                                             Conclusion
                                                                       The first goal in this work is to see the world as the child
                      Joint Interaction
                                                                     sees it and not filtered through our own adult expectations
Previous joint-attention research has focused on the
temporal synchrony of different participants in real-time            about the structure in that world. The second and equally
interaction. For instance, Butterworth (1991) showed that            important goal is to understand how the child’s own actions
children and parents share visual attention through social           –and coupled actions to a social partner --create regularities
cues signaled by their eyes. Yu, Ballard & Aslin (2005)              in visual information. This paper reports beginning progress
provided a formal model of the role of gaze in language              in reaching these goals and moreover suggests that progress
learning. The present work extends these studies in two              in achieving these goals will bring unexpected new
important ways. First, our results suggest the importance of         discoveries about the visual environment, about the role of
spatial information. Children need to not only share visual          the body, and the structure of the learning task –from the
attention with parents at the right moment; they also need to        learner’s point of view.
perceive the right information at the moment. Spatio-
temporal synchrony encoded in sensorimotor interaction               Acknowledgments: This research was supported by
may provide this. Second, hands (and other body parts, such          National Science Foundation Grant BCS0544995 and by
as the orientation of the body trunk) play a crucial role in         NIH grant R21 EY017843.
signaling social cues to the other social partner. The parent’s                               References
eyes are rarely in the child’s visual field but the parent’s and
                                                                     Ballard, D. H., Hayhoe, M. M., Pook, P. K., & Rao, R. P. N.
the child’s own hands occupy a big proportion of the child’s            (1997). Deictic codes for the embodiment of cognition.
visual field. Moreover, the change of the child’s visual field
                                                                        Behavioral and Brain Sciences, 20 (4), 723-767.
can be caused by gaze and head movement, but this change             Bertenthal, B. I., Campos, J. J. and Kermoian, R. (1994) An
can also be caused by both his own hand movements and
                                                                        Epigenetic Perspective on the Development of Self-
the social partner’s hand movements. In these ways, hand                Produced Locomotion and Its Consequences. Current
movements directly and significantly change the child’s
                                                                        Directions in Psychological Science, 3 (5) 145-140.
view.                                                                Butterworth (1991) The ontogeny and phylogeny of joint
              A New Window of the World                                 visual attention In Natural theories of mind: Evolution,
The first-person view is visual experience as the learner sees          development, and simulation of everyday mind reading,
it and thus changes with every shift in eye gaze, every head            A. Whitten (Eds.), pp. 223- 232, Oxford, England;
turn, every observed hand action on an object. This view is             Blackwell.
profoundly different from that of an external observer, the          Delaney SM, Dobson V, Harvey EM, Mohan KM,
third-person view who watches the learner perform in some               Weidenbacher HJ, Leber NR. Stimulus motion increases
environment, precisely because the first person view                    measured visual field extent in children 3.5 - 30 months
changes moment-to-moment with the learner’s own                         of age. Optom Vis Sci 2000; 77:82-9.
movements. The systematic study of this first person view --         Gibson, E. J. (1969). Principles of perceptual learning and
- of the dynamic visual world through the developing                    development. Appleton-Century-Crofts, East Norwalk,
child’s eyes -- seems likely to reveal new insights into the            CT: US.
regularities on which learning is based and on the role of           Ruff, H.A. (1989). Infants’ manipulative exploration of
action in creating those regularities. The present findings             objects: Effects of age and object characteristics.
suggest that the visual information from a child’s point of             Developmental Psychology,20, 9-20.
view is dramatically different from the parent’s (or an              Smith, L.B. & Breazeal, C. (2007) The dynamic lift of
experimenter’s) viewpoint. This means analyses of third-                developmental process. Developmental Science, 10, 61-
person views from an adult perspective may be missing the               68.
most significant visual information to a young child’s               van Hof-van Duin & Mohn (1986) The development of
learning.                                                               visual acuity in normal fullterm and preterm infants.
   The head camera method used here provides a new look                 Vision Research, 26 (6) 909-16.
on the structure of the learning environment, and how that           Yoshida, H. & Smith, L.B. (2007). Hands in view: Using a
structure is generated by the child’s own actions. In general,          head camera to study active vision in toddlers. Infancy.
a head camera can provide information about what is in that          Yu, C., Ballard, D.H., & Aslin, R.N. (2005). The role of
field --and available to attention -- but does not provide              embodied intention in early lexical acquisition. Cognitive
fine-grained information on what the specific focus of the              Science, 29 (6), 961–1005.
child’s attention in that field (as does eye-tracking
                                                                 736

