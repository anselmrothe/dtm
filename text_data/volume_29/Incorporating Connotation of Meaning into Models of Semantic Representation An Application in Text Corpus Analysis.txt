UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Incorporating Connotation of Meaning into Models of Semantic Representation: An Application
in Text Corpus Analysis
Permalink
https://escholarship.org/uc/item/1cc8p34v
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Mueller, Shane T.
Shiffrin, Richard M.
Publication Date
2007-01-01
Peer reviewed
  eScholarship.org                                 Powered by the California Digital Library
                                                                     University of California

  Incorporating Connotation of Meaning into Models of Semantic Representation:
                                     An Application in Text Corpus Analysis
                                              Shane T. Mueller (smueller@ara.com)
                                                       Klein Associates Division
                                                               A. R. A. Inc.
                                               1750 Commerce Center Boulevard North
                                                       Fairborn, OH 45434 USA
                                           Richard M. Shiffrin (shiffrin@indiana.edu)
                                Department of Psychological and Brain Sciences, 1101 E. 10th Street
                                                     Bloomington, IN 47404 USA
                              Abstract                                 important aspect of our knowledge, for linguistic and non-
                                                                       linguistic stimuli and for extreme and subtle cases.
   Connotation of meaning is an important aspect of human se-
   mantic knowledge, and it cannot be captured in simple pro-             Yet many psychological models of knowledge and concept
   totype representations of concepts. Yet models of human             representations and fail to capture connotation. For example,
   episodic memory typically rely on prototype representations,        prototype approaches typically consider information to be en-
   as do statistical techniques for extracting meaningful repre-
   sentations from text corpora (such as LSA). We will demon-          coded as a set of features, and accumulate average or typical
   strate how REM-II (a model of human episodic and semantic           feature values across many individual events to form a com-
   memory) allows connotation of meaning to be represented, and        posite, ignoring systematic variation and correlation among
   demonstrate that model can be develop and learn reasonable
   semantic representations by processing the Mindpixel project’s      features. Such an approach is not unreasonable, because it al-
   80,000-statement GAC corpus. The success of the model at            lows a rich composite of central tendency to be formed from
   developing meaningful and contextual representations from a         a set of noisy individuals. But if there are consistent patterns
   text corpus provides a demonstration of the importance and
   utility of our assumptions.                                         in the co-occurrence of features, a prototype will not be sen-
                                                                       sitive to them and will not be able to regenerate these distinct
   Keywords: episodic memory; semantic memory; text corpus             contextual representations. A prototype for the concept taxi
   analysis
                                                                       would be a concept that never occurs in the world: a vehi-
                                                                       cle that is a mixture between a sedan and a compact car in a
   Connotation of meaning has been shown to be important
                                                                       color somewhere between yellow and green. And consider
in language learning (Corrigan, 2002), meaning disambigua-
                                                                       adding rickshaws, airport shuttles, limousine services, and
tion (e.g., Swinney, 1979) and even latent emotional content
                                                                       horse-drawn carriages to the prototype: the result is nearly
(e.g., Cato et al., 2004). As a rough guide to its prevalence in
                                                                       impossible to imagine.
English, the Merriam-Webster’s Collegiate Dictionary, 11th
edition (2003) contains 165,000 entries with 225,000 defini-              Despite the inadequacy of prototype techniques for rep-
tions. Thus, there are approximately 1.36 meanings for each            resenting knowledge, techniques for extracting meaningful
word, even though homonyms are given distinct entries and              representations from text corpora typically use prototypes.
the dictionary is likely to contain large numbers of infrequent        For example, HAL (Burgess & Lund, 1997) uses a graded
and specialized terms with only one definition.                        word co-occurrence vector to represent semantic space; LSA
   Connotation of meaning describes the fact that the con-             (Landauer & Dumais, 1997) uses co-occurrences as input and
cepts we understand have multiple context-specific forms. If           projects this information onto a lower dimensional space us-
we consider linguistic concepts, extreme versions of connota-          ing statistical optimization procedures similar to factor anal-
tion encompass homophony, homonymy and polysemy: sin-                  ysis. Likewise, the Topics model (Griffiths & Steyvers, 2004)
gle word forms sharing multiple distinct meanings. Words               uses a bayesian approach to place constraints on the statisti-
exhibiting these properties make connotation a challenge for           cal distribution taken by features, and as a byproduct gener-
automated systems attempting to understand language, be-               ates features that are often interpretable. And recently, Jones
cause the context of the word must be considered in order              and Mewhort (2007) demonstrated that order and meaning
to understand its proper meaning. But even subtler forms of            can be incorporated into a composite holographic trace using
connotation can be important, and this importance can tran-            a convolution/correlation process. Of these, only Jones and
scend purely linguistic contexts. For example, consider how            Mewhort (2007) use a representation of knowledge that is not
taxi cabs in different cities and countries differ substantially       a simple prototype; instead they use a complex holographic
from one another. In Manhattan, a typical taxi is a yellow             representation in which information is distributed.
four-door sedan built by an American car company; in Mex-                 In order to move beyond a simple prototype knowledge
ico City, a typical taxi may be a small green compact vehi-            representation, we propose that knowledge accumulates in
cle. Thus, what we are calling connotation of meaning is an            the form of feature co-occurrences. Thus, if one considers all
                                                                   497

                                                                                                             Statements/Events
experienced exemplars of a concept, one would determine for
each pair of features how many times those features occurred                                    He ate the bologna sandwich.
together to form a composite trace. Such a representation                                       The ship sailed the seven seas.
maintains a set of conditional representations, and enables                                     .....
each distinct meaning to be maintained independently. We
have implemented these notions in a computational model of
human we describe next. Following this, we will demonstrate
the utility of our assumptions by allowing the model to read                     Example Event           Encoded Features       Unlikely Features
text corpora and develop meaningful representations based on
information in the text.                                                                   HE
                                                                                                             [1 1 1 0 0]            [0 0 0 0 0]
                                                                                          ATE                [4 0 0 1 0]            [1 0 0 0 0]
      REM-II: A Bayesian Model of Episodic                                                THE                [1 0 1 1 0]
                                                                                                             [5 0 0 0 1]
                                                                                                                                    [0 0 0 0 0]
                                                                                                                                    [1 0 0 0 1]
                                                                                       BOLOGNA
   Memory Retrieval and Semantic Knowledge                                             SANDWICH
                                                                                                             [3 1 0 0 0]            [1 0 0 0 0]
                         Formation
REM-II (Mueller & Shiffrin, 2006) is an extension of REM
Retrieving Effectively from Memory, Shiffrin & Steyvers,                        Composite
                                                                              Representation          Knowledge Matrix          Update Knowledge
1997), a bayesian model of human episodic memory. REM-
II was developed in to explain empirical phenomena in which
                                                                                                                                          HE
polysemous words encoded with bias would activate earlier                                               9
                                                                                                        0
                                                                                                          0
                                                                                                          0
                                                                                                            0
                                                                                                            0
                                                                                                                0
                                                                                                                0
                                                                                                                  3
                                                                                                                  0                      ATE
memories associated with one, but not both connotations of                      [3 0 0 0 1]             0 0 0   0 0                      THE
                                                                                                        0 0 0   0 0
the probe word. A more precise mathematical description                                                 3 0 0   0 1
                                                                                                                                      BOLOGNA
                                                                                                                                      SANDWICH
of the model is available in Mueller and Shiffrin (2006), but
for the present demonstrations in corpus analysis, we will
highlight three critical assumptions. First, rather than us-
                                                                    Figure 1: Basic steps performed by REM-II to process state-
ing a global optimization process to produce representations,
                                                                    ments in language or events in the world. (1) the physical
we implement a psychological model of sensemaking (e.g.,
                                                                    identity of the component objects in the event activate knowl-
Klein, Moon, and Hoffman, 2006) that interprets events ac-
                                                                    edge structures and (2) generates traces by sampling features.
cording to its knowledge and grows knowledge because of
                                                                    (3) These traces are compared to the base rate distribution
those events. The second assumption is related to a result of
                                                                    to determine which are unlikely to have occurred by chance.
the corpus techniques described earlier: words that are simi-
                                                                    (4) Then, a composite representation of the local semantic
lar to one another tend to appear in close proximity. We as-
                                                                    context is formed from these unlikely features, and (5) a co-
sume that the opposite relation holds as well: concepts that
                                                                    occurrence matrix is formed from that composite representing
appear in the same context grow more similar because of this
                                                                    the features that occurred together in the current context. (6)
co-occurrence. And finally, we assume (as discussed before)
                                                                    Finally, this composite matrix is added back into the semantic
that knowledge accrues as feature co-occurrences, enabling
                                                                    knowledge matrix for each word in the event.
connotation and contextual meaning to be represented. The
basic steps involved in allowing REM-II to interpret a sen-
tence of text is shown in Figure 1.
   In REM-II, an event consists of a set of concepts that occur     the current semantic context, at first by sampling a feature
at the same time and place. In the context of corpus analysis,      from the current context, selecting that row in the knowledge
we treat each individual sentence or statement as a distinct        matrix and sampling a feature from the selected row. We as-
event. An episode is formed through a “sensemaking” pro-            sume that greater study time would allow more features to be
cess by which each event is interpreted through past knowl-         sampled, generating a richer representation of the concept.
edge, and is represented as a set of features that were present        In the original REM model, memory matches are com-
in the event. In contrast to this flat representation, semantic     puted by computing a likelihood ratio based on a probabilistic
knowledge of a concept is maintained as a symmetric matrix          model of memory encoding. The model assumes that a fea-
that encodes the co-occurrence of features within individual        tures can appear in a memory trace either because they are
events. Each row of that matrix keeps track of a prototype of       were correctly encoded, or because an error was made. The
a conditional representation of that concept, conditioned on        distribution of errors is assumed to follow the base rate of
the presence of each feature.                                       features in the environment, and so for any memory probe,
   To encode a new episode, we assume that the proper se-           one can compute the probability that it “matches” an episodic
mantic knowledge matrix is identified based on perceptual           trace by computing the likelihood that the trace arose from the
and contextual information. The model then samples addi-            memory structure associated with the probe. When events are
tional features from the knowledge matrix to enhance and            encoded, we go through a similar process to determine which
give meaning to the representation. Sampling is biased by           encoded features are important carriers of the unique infor-
                                                                498

       Group A versus Group B      Non co−occurrent words
         episodic encodings             (Words 4 and 8)             look like: A1 A3 P1 A2 A1 P1 .B1 B3 P2 P2 B1 B2 P2. The model
                                                                    made 5000 iterations through each of four sentence types
                                                                    (A, P1 ,A, P2 ,B, P1 ,A, P2 ), at which point we determined that
                                                                    the representations had converged to be highly similar within
                                                                    each meaning set, and the two polysemous words had also
                                                                    converged to nearly identical representations.
                         Group A                      Word 4            We were especially interested in whether the representa-
                         Group B                      Word 8
                                                                    tions of the polysemous words would indeed keep the mean-
         Polysemous Word 4           Polysemous Word 8
        biased toward A or B         biased toward A or B           ings associated with the distinct contexts separate, or whether
                                                                    the representation would simply converge to an average of
                                                                    the two contexts. To test this, we used probabilistic encod-
                                                                    ing process described earlier to generate biased and unbiased
                                                                    episodic traces from different words in this small corpus. To
                                                                    encode an unbiased representation, a row is initially sam-
                         A Bias
                         B Bias
                                                      A Bias
                                                      B Bias
                                                                    pled unconditionally from the base rate distribution, and a
                                                                    feature is sampled from that row, but for following samples
                                                                    row are chosen probabilistically from the he representation
Figure 2: MDS solution for biased and unbiased episodes en-         being built. We encoded 100 unbiased episodes from group
coded from each meaning group and from the two polyse-              A, group B, and the two polysemous words, and 100 biased
mous words.                                                         episodes from the two polysemous words, biased by “A” or
                                                                    “B” contexts. Once encoded, we computed a distance matrix
                                                                    over the complete set of sampled episodes by calculating the
mation about the episode. For each encoded trace, we com-
                                                                    root-mean-square deviation between episodes normalized to
pare its distribution to the base rate distribution of features
                                                                    sum to 1.0. We then submitted this distance matrix to a single
across the entire history of the model. Only those features
                                                                    multi-dimensional scaling (MDS) solution using the isoMDS
with density greater than expected by chance are selected. A
                                                                    function of the R statistical computing language. We present
co-occurrence matrix is formed from the outer product of the
                                                                    the data from the global MDS solution in multiple panels of
index features, and this co-occurrence matrix is added back
                                                                    Figure 2 to assist visualization.
into the semantic knowledge matrix for each concept occur-
                                                                        The upper left panel of Figure 2 shows that unbiased en-
ring in the episode.
                                                                    coding of pure A or B words segregate in the space, with
   Although this is a model of the interpretation of events and
                                                                    little overlap. At the same time, unbiased episodes encoded
formation of knowledge from those events, we have found
                                                                    from the two polysemous words (upper right panel) cover the
that it can go beyond modeling simple laboratory experimen-
                                                                    entire space and are indistinguishible from one another (even
tal situations, and be deployed on meaningful text to learn
                                                                    though they never appeared together). When episodes were
useful representations. In the remainder of the paper, we will
                                                                    encoded from the polysemous words biased by either A or B
describe several demonstrations in which the model was al-
                                                                    (lower panels) the resulting episodes clustered in the spaces
lowed to read a corpus of text and develop semantic represen-
                                                                    corresponding to unbiased encodings of words from those
tations based on the co-occurrence patterns in the text.
                                                                    two groups. Thus the two polysemous words which appeared
          Application: Text Corpus Analysis                         in two distinct contexts retained the information separately,
                                                                    and appropriate versions of these traces could be extracted
If the assumptions of REM-II are accurate, we should be able        using a biased encoding process.
to present information to the model and have it grow represen-
tations that produce natural semantic spaces. We first tested       Demonstration 2: “Fly” Subset of the GAC Corpus
some of the assumptions using a small hand-generated cor-           We next attempted to scale up the model to a larger naturally-
pus. We then scaled the model to a large targeted corpus, and       occurring corpus. To increase the efficiency of the learning
finally to a broad corpus of knowledge. Results from each           process, we replaced the sampling process used to generate
demonstration are described below.                                  an episodic trace in Demonstration 1 with the probabilistic
                                                                    computation of the expected distribution. This is simply a
Demonstration 1: Small Polysemous Corpus                            weighted sum of a normalized context vector and a normal-
We began with a small toy corpus generated with sim-                ized knowledge matrix. This was compared to the base rate
ple probabilistic rules. The corpus contained eight dis-            distribution for features, and only unlikely features were se-
tinct words with two sets of three words that tended to             lected, and so this remained a fairly similar process, but in-
appear together, and two polysemous words that appeared             creased the efficiency of the process substantially.
with each set but not together. So, if A, B and P de-                   We attempted to identify a text corpus which could pro-
note whether a word was from Set A, B, or a polyse-                 vide fairly dense information, to reduce the processing re-
mous word, a typical automatically set of sentences might           quirements for this exploratory project. One of the better
                                                                499

                                                                                                                                            Dendrogram of agnes(x = d)                                                                                                                                GAC REM−II Solution
                                                                                                         0.10
                                      bee                                                                                                                                                                                                             airplane
                                                                moth                                                                                                                                                                                  propeller
                             0.06
                                                                                                                                                                                                                                                        airport
                                                                                                         0.08
                                                                                                                                                                                                                                                            jet
                             0.04                                                                                                                                                                                                                    helicopter
                                                                                                                                                                                                                                        bee   moth
                                                                                                                                                                                                                                                          duck
                                                  bug                                                    0.06
                                                                                                                                                                                                                                                         dove
                             0.02                           insect
                                                                                                                                                                                                                              airport
                                                                                                                                                                                                                                                        goose
                                                                                                Height
                                                                                                                                                                              bug
                                                   beetle                                                0.04                                                                                                                                           pigeon
                                                                                                                                                            insect   beetle
                             0.00                                             pigeon                                                                                                                                                                     hawk
                                                                            duck
                                                                                                                                                                                    helicopter
                                airport                              fly             dove
                                                                                   hawk                                                                                                                                                                  moth
                                                                                                                                                                                                                  propeller
                                                                                                                fly
                                                                                                         0.02         goose
                             −0.02
                                                                                                                                                                                                 jet
                                                                                                                                                                                                       airplane
                                                                                                                                                                                                                                                           bee
                                                        helicopter          goose
                                                                                                                                            pigeon
                                                                                                                                                     duck
                                             jetairplane
                                                                                                                                                                                                                                                        beetle
                                                                                                                                                                                                                                                           bug
                                                                                                         0.00
                             −0.04                                                                                                                                                                                                                      insect
                                                                                                                              hawk   dove
                                        propeller                                                                                                                                                                                                           fly
                                                                                                                                                                                                                                                                  fly
                                                                                                                                                                                                                                                                        insect
                                                                                                                                                                                                                                                                                 bug
                                                                                                                                                                                                                                                                                       beetle
                                                                                                                                                                                                                                                                                                bee   moth   hawk
                                                                                                                                                                                                                                                                                                                    pigeon   goose   dove   duck
                                                                                                                                                                                                                                                                                                                                                   helicopter
                                                                                                                                                                                                                                                                                                                                                                jet
                                                                                                                                                                                                                                                                                                                                                                      airport
                                                                                                                                                                                                                                                                                                                                                                                propeller   airplane
                                     −0.04         −0.02      0.00         0.02          0.04
                                                                                                                                                                d
                                                                                                                                                 Agglomerative Coefficient = 0.58
Figure 3: Results of Demonstration 2: An MDS solution, agglomerative hierarchical clustering tree, and visual depiction of
similarity matrix for words related to three connotations of “fly”.
                                                                                                                                                                                    sources we identified was a corpus produced by the Mind-
                                                                                                                                                                                    pixel project. The Mindpixel project was an internet-based
                  Similarity of conditional representations of FLY                                                                                                                  collaborative project to generate verifiable statements about
                                                                                                                                                                                    the world. Users submitted statements or questions about
           0.35                                                                                                                                                                     the world (e.g., “Is a dog is a mammal?” and other users
                                                                                    4
                                                                                    9
                                                                                    13
                                                                                                                                                                                    would verify if the statement was correct. Each such state-
                                                                                    17
                                                                                    7
                                                                                    8
                                                                                    10
                                                                                    5
                                                                                                                                                                                    ment was considered a “mindpixel”. The project began in the
           0.30                                                                                                                                                                     year 2000, and had putatively collected 1.4 million “mind-
                     24                                                             15
                     3
                     20                                                             22
                                                                                    18
                                                                                    14
                     6
                     23
                     11
                     2
                                                                                    12
                                                                                    16                                                                                              pixels” by 2004, in a database called GAC (General Artificial
                                                                                    21
                     1
                     15
                     25
                     21
                                                                                    25
                                                                                    6
                                                                                                                                                                                    Consciousness). Although the project appears to have been
           0.25                                                                                                                                                                     abandoned with the death of its founder in 2006, a database
                     12
                     16
                     14                                                             2
                     18
                     22
                                                       4                            1                                                                                               of 80,000 verified statements was released on the internet. We
                     19
                     10
                     5                                19                            3
                                                                                    19
                     8
                                                                                    23
                                                                                    24
                                                                                                                                                                                    view these statements as a rich yet broad source of semantic
           0.20
                                                      5
                                                      13                            11
                                                      9                             20                                                                                              content that could be used by our REM-II model to grow rep-
Distance
                                                      7
                                                      17
                     7
                                                      25
                                                                                                                                                                                    resentations resembling human knowledge.
                                                      1
                                                      6
                                                                                                                                                                                       We have found that when the model is applied to typical
           0.15
                                                      8
                                                      2
                                                      20
                     17                               23
                                                      16
                                                      11                                                                                                                            text corpora, common function words which appear in many
                                                      15
                                                      24
                                                      3                                                                                                                             contexts end up developing representations that resemble the
                     9
                                                                                                                                                                                    base rate distribution substantially, and so their information is
           0.10      13
                                                      10
                                                                                                                                                                                    ’filtered out’ by the likelihood comparison process. Thus, in
                     4
                                                      22
                                                      18
                                                      12
                                                      14
                                                                                                                                                                                    order to further increase the speed of the algorithm, we per-
                                                      21
           0.05
                                                                                                                                                                                    formed some simple pre-processing to the GAC corpus, elim-
                                                                                                                                                                                    inating common function words and mapping distinct word
                                                                                                                                                                                    forms onto the same base word according to the lemmas in
           0.00
                                                                                                                                                                                    the CELEX database. As a result of this preprocessing, the
                                                                                                                                                                                    80,000 statement corpus containing approximately 660,000
                  AIRPLANE                          BIRD                          INSECT                                                                                            tokens and 29,000 unique words was reduced to 78,745 state-
                                                    Word                                                                                                                            ments containing 269,000 tokens and 11,859 unique words.
                                                                                                                                                                                       Our initial target for corpus analysis was a subset of the
Figure 4: Dissimilarity of each conditional representation of                                                                                                                       GAC corpus: statements which contained either the word
“fly” to three key words: “airplane”, “bird”, and “insect”.                                                                                                                         “fly” or one of its close associates (e.g., airplane, bird, insect,
Each conditional representation is indicated by the index of                                                                                                                        etc.). This resulted in 3992 statements containing a total of
the feature used to form the conditional representation. Re-                                                                                                                        15,583 tokens and 2907 unique words. To monitor progress,
sults demonstrate that the matrix representation segregates                                                                                                                         we selected 16 words in three groups describing three con-
meanings related to each context, enabling connotation and                                                                                                                          notations of “fly”: the word fly, words related to insects, air-
polysemy to emerge.                                                                                                                                                                 planes, and birds. We used 25 features as a basis for the rep-
                                                                                                                                                                                    resentation. Reasonable representations for these 16 words
                                                                                                                                                                                    developed fairly quickly, but the meaning groups continued
                                                                                                                                                            500

to develop over more than 100 consecutive readings of the
text. We computed a similarity matrix across the 16 words,                  0.02
                                                                                                                                  travel
and show three methods for visualizing it in Figure 3. On                                                     baby
                                                                                                    fly       child
the left, an MDS solution shows that the word “fly” is in the               0.01
center of the space, and the three different meanings clus-                                                     mother
                                                                                                       tea
                                                                                                   coffee
ter together in three corners of the space. The center panel                0.00
                                                                                    wine               beer
                                                                                                                  father
shows how an agglomerative hierarchical clustering solution                                                                        car
tends to cluster the like words together. Finally, the rightmost                                     cat animal                                   drive
                                                                            −0.01
panel shows the pairwise similarity between each word, with
darker entries indicating greater similarity. These solutions
                                                                            −0.02
compared favorably to the ones produced by LSA using the                                   pet            dog
TASA corpus (r = .584), and the solution produced by LSA                                   −0.02                0.00       0.02            0.04
on the exact same GAC corpus (r = .403). In fact, the REM
solution was more similar to both LSA solutions than they
were to one another (r = .202).                                           Figure 5: Multi-dimensional scaling solution for four clus-
                                                                          ters of four words, based on REM-II learning of the complete
   These graphical visualizations show that REM-II placed                 GAC corpus. Semantically similar words tend to cluster in
“fly” and its semantic neighbors into a reasonable semantic               similar regions of space.
space, with related subsets of words clustering together and
“fly” being somewhat similar to all concepts. Yet such a phe-
nomenon could occur even if a prototype representation was
                                                                          Demonstration 3: Complete GAC Corpus
used. To determine whether the representation of “fly” con-               Finally, we wanted to demonstrate that the model could be
tains conditional representations that segregate these differ-            used to learn representations of wider knowledge in the com-
ent meanings, we examined each row of the matrix represen-                plete GAC corpus. In this demonstration, we used 40 fea-
tation. Recall that each row (or column) of a co-occurrence               tures, and allowed the model to read the complete 80,000-
matrix can be interpreted as a conditional representation, con-           statement database multiple times, randomizing the order of
ditioned on the presence of a specific feature. For the 25 fea-           the statements between each pass, and monitoring the inter-
tures in this demonstration, there were thus 25 conditional               mediate solutions.
representations for “fly”. We examined each of these in turn,                To assess whether the representations of different words
comparing them to the composite representations for “air-                 humans judge as similar grow similar to one another, we se-
plane”, “bird”, and “insect” using a root-mean-square devi-               lected 16 high frequency words in four target areas to monitor
ation over normalized vectors. This produced 25 distance                  as the representations grew. These included: pet, cat, dog, an-
scores for each comparison word, which are all shown in Fig-              imal, child, baby, father, mother, travel, car, drive, fly, beer,
ure 4, denoted by the index of each feature.                              tea, coffee, and wine. A multi-dimensional scaling solution
                                                                          for these target words is shown in Figure 5 after twelve passes
   This analysis reveals several things. Similar to Figure 2, it          through the database. Semantically similar words tended to
shows that conditional representations of “fly” are on average            cluster together, with the curious exception of the word “fly”.
closer to the word “bird” than “airplane” or “insect”, and al-            This apparent anomaly was completely unrelated to its role in
though several are close to “airplane”, none are very close to            the earlier analysis.
“insect”. Additionally, representations that were close to “air-             Considering just the 16 target words, REM-II was able to
plane” tended to be farther from “bird” and “insect”. Over-               produce between-word similarities that compared well with
all, the dissimilarity of conditional representations of “fly” to         those produced by LSA on the large TASA corpus (r = .439),
“airplane” was negatively correlated with the dissimilarities             in ways similar to that produced by LSA on the same GAC
of “fly” to “bird” (r = −.5) and insect (r = −.8), whereas the            corpus (r = .459). The between-word similarities from REM-
dissimilarities of “fly” to “bird” was slightly positively corre-         II and LSA analyses of the GAC corpus were also correlated,
lated with those of “fly” to “insect” (R = .17). This indicates           but to a lesser extent (r = .324). The dissimilarity matrices
that several of the features (e.g., 4, 9, 13, and 17) in “fly”            for these three analyses are depicted visually in Figure 6.
tended to encode an “airplane” connotation, whereas others                   Finally, because the analysis was completed for a larger
(e.g., 10, 12, 21, 22) tended to encode “bird” or “insect” con-           corpus with approximately 29,000 words, we are able to gen-
notations. The features of “airplane” that had the greatest               erate similarity-based queries and evaluate their fitness quali-
densities were 4,9,10, 13, 17, 18, and 22, and the features               tatively. To demonstrate, we present the closest ten represen-
of “bird” that had the greatest densities were 10, 12, 14, 18,            tations to a variety of key probes in Table 1.
21, and 22, which maps closely onto those conditional repre-
sentations of “fly” that were similar to the each word. Inter-                                                     Discussion
estingly, the two connotations of “fly” appear to map onto a              In this paper, we have shown how a model of human memory
natural/man-made distinction fairly nicely.                               can be deployed to grow the same types of representations
                                                                    501

            GAC REM−II Solution                  Canonical LSA Solution                        GAC LSA SOlution
  wine                                 wine                                          wine
 coffee                               coffee                                       coffee
    tea                                  tea                                          tea
   beer                                 beer                                         beer
     fly                                  fly                                          fly
  drive                                drive                                        drive
    car                                  car                                          car
  travel                               travel                                       travel
mother                               mother                                        mother
 father                               father                                        father
  baby                                 baby                                         baby
  child                                child                                         child
 animal                               animal                                       animal
   dog                                  dog                                           dog
    cat                                  cat                                          cat
    pet                                  pet                                          pet
               pet
               cat
              dog                                   pet
                                                    cat
                                                   dog                                           pet
                                                                                                 cat
                                                                                                dog
           animal
             child
             baby                               animal
                                                  child
                                                  baby                                       animal
                                                                                               child
                                                                                               baby
            father
           mother
            travel                               father
                                                mother
                                                 travel                                       father
                                                                                             mother
                                                                                              travel
               car
             drive
                fly                                 car
                                                  drive
                                                     fly                                         car
                                                                                               drive
                                                                                                  fly
             beer
               tea
           coffee                                 beer
                                                    tea
                                                coffee                                         beer
                                                                                                 tea
                                                                                             coffee
             wine                                 wine                                         wine
Figure 6: Depiction of dissimilarity matrices for sixteen target words, created using REM-II on the GAC corpus (left panel),
LSA on the TASA corpus (middle panel), and LSA on the GAC corpus (rightmost panel)
                                          Table 1: Ten most similar words to eight probes.
  color            food           europe     man         fly         fire        car                              earth       animal
  color            food           europe     man         fly         fire        car                              earth       animal
  blue             eat            italy      woman       flap        match       drive                            around      breath
  violet           cereal         locate     physically airplane flame           motorcycle                       close       worm
  combination      rice           portugal strong        air         touch       ride                             spin        predator
  red              regularly      rome       attractive  plane       start       driving                          mars        usually
  orange           restaurant     germany virgin         balloon wise            automobile                       planet      human
  primary          hamburger      belgium average        lighter     term        form                             sun         intelligent
  yellow           mouth          music      naked       african     off         move                             spherical   eat
  combine          order          spain      crave       pop         hot         vehicle                          rotate      curious
  purple           usually        japan      reach       fan         lightbulb consume                            moon        cockroach
that statistical corpus analysis techniques can produce. The                    Griffiths, T. & Steyvers, M. (2004). Finding Scientific Topics. Pro-
success demonstrated here shows that the psychological as-                        ceedinngs of the National Academy of Sciences, 101 (suppl. 1),
                                                                                  5228–5235.
sumptions we based the model on are sufficient to develop                       Jones, M. N. & Mewhort, D. J. K. (2007). Representing word mean-
rich knowledge representations, providing a clear demonstra-                      ing and order information in a composite holographic lexicon.
                                                                                  Psychological Review, 114,1–37.
tion of the conference theme: “CogSci in the Real World”. By                    Klein, G. Moon, B., and Hoffman, R. R. (2006). Making Sense
taking our psychological model out of the laboratory and al-                      of Sensemaking 2: A Macrocognitive model. IEEE Intelligent
lowing it to learn from natural artifacts, we are able to demon-                  Systems, 21, 88-92.
                                                                                Landauer, T. K.. & Dumais, S. T. (1997) A solution to Plato’s
strate the utility of the model, and at the same time create a                    problem: The Latent Semantic Analysis theory of acquisition, in-
tool that can be useful for automated processing and interpre-                    duction, and representation of knowledge. Psychological Review,
                                                                                  104, 211–240.
tation of text. By taking these psychological processes and                     Merriam-Webster’s Collegiate Dictionary, 11th Ed. (2003). Spring-
representations seriously, we believe that new and more in-                       field, MA: Merriam-Webster, Inc.
                                                                                Mueller, S. T. & Shiffrin, R. M. (2006). REM-II: A Model of the
telligent tools can be developed for a range of applications in                   developmental co-evolution of episodic memory and semantic
knowledge management and understanding.                                           knowledge. Paper presented at the International Conference on
                                                                                  Learning and Development (ICDL), Bloomington, IN, June, 2006.
                                                                                Shiffrin, R. M. & Steyvers, M. (1997). A model for recognition
                            References                                            memory: REM–retrieving effectively from memory. Psycho-
                                                                                  nomic Bulletin and Review, 4, 141–166.
Burgess, C., & Lund, K. (1997). Modelling parsing constraints with              Swinney, D. A. (1979). Lexical access during sentence compre-
  high-dimensional context space. Language and Cognitive Pro-                     hension: (Re)consideration of context effects. Journal of Verbal
  cesses, 12, 1–34.                                                               Learning and Verbal Behavior, 18, 645–659.
Corrigan, R. (2002). The acquisition of word connotations: asking
  ’What happened?’ Journal of Child Language, 31: 381-398.
Cato, M. A., Crosson, B., Gkay, D., Soltysik, D., Wierenga, C.,
  Gopinath, K., Himes, N., Belanger, H., Bauer R. M., Fischler I.
  S., Gonzalez-Rothi, L. & Briggs, R. W. (2004). Processing Words
  with Emotional Connotation: An fMRI Study of Time Course and
  Laterality in Rostral Frontal and Retrosplenial Cortices. Journal
  of Cognitive Neuroscience, 16, 167-177.
                                                                          502

