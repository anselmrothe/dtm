UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Social Facilitation Effects of Virtual Humans

Permalink
https://escholarship.org/uc/item/8kh6f73k

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)

Authors
Park, Sung
Catrambone, Richard

Publication Date
2007-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Social Facilitation Effects of Virtual Humans
Sung Park (gtg116s@mail.gatech.edu)
School of Psychology, Georgia Institute of Technology, 654 Cherry Street
Atlanta, GA 30332 USA

Richard Catrambone (rc7@prism.gatech.edu)
School of Psychology, Georgia Institute of Technology, 654 Cherry Street
Atlanta, GA 30332 USA
behaviors (e.g., direct requests for evaluations elicit more
positive responses, other-praise is perceived as more valid
than self-praise) even though users assume machines do not
possess emotions, feelings, or “selves”. Nass' work, and the
work of other researchers (e.g., Lester et al., 1997; Nass et
al., 1994; Reeves & Nass, 1996), suggests that there is a
striking similarity between how humans interact with one
another and how a human and a virtual human interact. An
implication of this research is that designers of virtual
humans should consider research on human social
interaction when making decisions about virtual humans’
behavior, appearance, speech, and other factors.
One social phenomenon of interest is whether virtual
humans can elicit a social facilitation effect. Social
facilitation is demonstrated when the mere presence of
another person facilitates or inhibits task performance.
Various researchers have offered theories to account for this
effect (e.g., Cottrell, 1972; Zajonc, 1965; for a most up-todate review, see Aiello & Douthitt, 2001). Social facilitation
is generally referred to as performance enhancement on a
simple or well learned task, and performance impairment on
a complex or novel task. We behave differently, and
presumably process information differently, when there is
someone else near than when we are alone.
The aim of the present study is to investigate whether
social facilitation can be observed with virtual humans.
Such facilitation has implications for whether virtual
humans should be "present" while a person is studying
instructional material or attempting to do various tasks.
Should the virtual human’s face always present on the
screen? Should it be invoked to appear only when
necessary? An ever-present virtual human might make users
feel uneasy by providing a sense of evaluation and hence
impair task performance. Conversely, the presence of a
virtual human might encourage the learner or help the
learner focus when a task is easy, thereby enhancing
performance.

Abstract
When people do an easy task, and another person is nearby,
they tend to do that task better than when they are alone.
Conversely, when people do a hard task, and another person
is nearby, they tend to do that task less well than when they
are alone. This phenomenon is referred to as "social
facilitation". The present study investigated whether people
show social facilitation in the presence of a virtual human.
Participants were given different tasks to do that varied in
difficulty. The tasks involved anagrams, mazes, and modular
arithmetic. They did the tasks either alone, in the company of
another person, or in the company of a virtual human on a
computer screen. The virtual human produced the social
facilitation effect: for easy tasks, performance in the virtual
human condition was better than in the alone condition, and
for difficult tasks, performance in the virtual human condition
was worse than in the alone condition. These results suggest
guidelines for the use of virtual humans in computer
interfaces.
Keywords: Embodied conversational agent; virtual agent;
animated character; avatar; anthropomorphism.

Introduction
Interest in virtual humans or embodied conversational
agents (ECAs) is growing in the realm of human computer
interaction. Our informal count of articles and papers
appearing in peer-reviewed journals and conference
proceedings that focus on the interaction between virtual
humans and humans shows a steady increase from 2000 to
2006 (from 9 in 2000 to 54 in 2006). Recent improvements
in computation have facilitated the use of virtual humans in
various applications such as entertainment (Jeong,
Hashimoto, & Makoto, 2004), clinical practice (Glantz,
Rizzo, & Graap, 2003), and military (Hill et al., 2003).
Anthropomorphizing an interface means adding human-like
characteristics such as speech, gestures, and facial
expressions. These components are remarkable in conveying
information and communicating emotion. The human face,
especially, is powerful in transmitting a great deal of
information efficiently (Collier, 1985). For example, a
virtual human with a confused face might be better (e.g.,
faster) at letting a user know that the virtual human does not
understand the user’s command than simply displaying “I
don’t understand” on the screen.
Nass, Steuer, and Tauber (1994) argue that individuals’
interactions with computers are fundamentally social. Their
evidence suggests that users can be induced to elicit social

Research on Social Facilitation
of Virtual Humans
Social facilitation theory is one of the oldest theories in
psychology. A large body of research has been conducted
over more than 100 years, however, the development of
social facilitation theory has been fragmented. While there
are some prominent explanations, no single theory has

557

emerged that can unequivocally account for this
phenomenon.
Only a handful of studies have investigated the social
facilitation effect in the context of virtual humans. Walker,
Sproull, and Subramani (1994) investigated participants’
responses to a synthesized talking face displayed on a
computer screen in the context of a questionnaire study.
Compared to participants who answered questions presented
via a text display on a screen, participants who answered the
same questions spoken by a talking face spent more time,
made fewer mistakes, and wrote more comments. Walker et
al. claimed that this enhancement in task performance was
due to social facilitation. However, one major aspect of the
social facilitation effect is that performance is facilitated
only if the task is simple or well learned. The researchers
never explicitly stated or demonstrated whether the
questionnaire task in their study was a simple task. Secondly,
spending more time with the talking face does not
necessarily mean enhancement in task performance. This
might simply mean that it took longer to listen to a question
than to read it and the study did not address this issue.
Rickenberg and Reeves (2000) investigated the effects of
different virtual character presentations on user anxiety, task
performance, and subjective evaluations in the context of
web sites. Users felt more anxious when virtual characters
monitored their web site work and this effect was strongest
for users with an external control orientation. The presence
of a monitoring character decreased task performance.
Zanbaka, Ulinski, Goolkasian, and Hodges (2004)
attempted to investigate social facilitation due to virtual
humans. Participants first learned a task and were then
randomly assigned to perform the same task or a novel task
either alone, in the presence of a real human, or in the
presence of a virtual human. In general, Zanbaka et al. were
unable to replicate the social facilitation effect. There was
no significant improvement for easy tasks in the presence of
the virtual human. The research reported some significant
decrement on task performance for the complex task, which
is in accordance with the social facilitation effect. However,
this was observed for only some female participants within
some blocks of the performance. Gender was not a planned
factor in this experiment and thus not evenly distributed
across conditions.
One reason that Zanbaka et al.’s (2004) research failed to
replicate the social facilitation effect was, as noted in their
paper, due to a ceiling effect. Participants were able to learn
the correct pattern in the learning stage, which left little
room for improvement later on. This is also a common
problem in social facilitation research in social psychology
(Bond & Titus, 1983).
Given the debate in social psychology on the causes of
the social facilitation effect, and given the difficulties with
the dependent measures used in prior studies investigating
social facilitation with virtual humans, the present study was
designed to examine whether the social facilitation effect
will occur with a virtual human for tasks that are
demonstrably different in difficulty.

558

As mentioned earlier, there is no single unified theory that
can parsimoniously account for the social facilitation effect.
In addition, a number of factors such as task complexity,
type of presence, and the context of evaluation seem to
moderate the strength of the presence effect (Aiello &
Douthitt, 2001). The present study was not designed to
distinguish among the major social facilitation theories.
Instead, we wished to examine whether the social
facilitation effect can be evoked by virtual humans under
conditions that map on to the signature finding with real
humans. The key hypothesis is that the presence of a virtual
human enhances simple task performance and impairs
complex task performance.

Experiment Design
The experiment tasks needed both breadth and depth to test
the social facilitation effect but at the same time be
applicable to the realm of virtual humans. Hence, the
following two criteria were used for selecting experimental
tasks.
1) Is the task something that a user might do with
the assistance of a virtual human?
2) Is the task scalable in terms of difficulty?
With respect to the first criterion, virtual humans can
assist users in many different tasks. Some tasks can be
opinion-like (e.g., choosing what to bring on a trip) while
others can be more objective (e.g., implementing edits in a
document) (Catrambone, Stasko, & Xiao, 2004). These
tasks are "high-level" cognitive tasks. Hence, low level
sensory motor tasks were excluded from this experiment.
With respect to the second criterion, the present study
examined differences in task performance between simple
and complex tasks.
Using these two criteria, the present study incorporated
the following three cognitive tasks: anagrams, mazes, and
modular arithmetic. These three tasks provided a good
mixture of verbal, spatial, mathematical, and high level
problem solving skills. All three tasks were cognitive tasks
and had an objective and therefore, were within the range of
tasks that a virtual human might assist. It was also possible
to produce both easy and difficult instances of these tasks.
As part of a larger study, participants also did an additional
task, the Tower of Hanoi; it was done after the other tasks
were completed and was a between-subject task and
excluded from the analyses below.

Anagram Task
Social facilitation (due to a human being present) in
anagram tasks has been studied in the context of electronic
performance monitoring (EPM), a system whereby every
task performed through an electronic device may be
analyzed by a remotely located person (Davidson &
Henderson, 2000). The social facilitation effect was clearly
observed in the presence of EPM; easy anagrams being
performed with greater proficiency and difficult anagrams

being performed with less proficiency. In the present study,
anagrams were divided into two categories (easy or
difficult) using normative solution times from Tresselt and
Mayzner’s (1966) anagram research.

Maze Task
Research has suggested that participants tend to perform
better in the presence of a human on simple maze tasks
(Rajecki, Ickes, Corcoran, & Williams, 1977). In the present
study, simple mazes included wide paths and few blind
alleys so that the correct route was readily perceivable,
whereas difficult mazes included narrow paths with many
blind alleys. Materials for the maze task were similar to the
ones of Jackson and Williams (1985). Participants were
given a maze and a cursor on the screen and were asked to
draw a path to the exit.

Figure 1: Virtual Human in the present study.

Design and Procedure
The present study was a 2 x 3 within subject design. The
complexity factor had two levels; simple and complex, and
the presence factor had three levels; alone, presence of a
human, and presence of a virtual human. These two within
subjects factors were crossed to produce six types of trials:
participants doing a simple task alone, a simple task in the
presence of a human, a simple task in the presence of a
virtual human, a complex task alone, a complex task in the
presence of a human, and a complex task in the presence of
a virtual human. Every participant experienced multiple
instances of each of the six trial types.
The order of the presence factor was varied across
participants using a Latin Square. That is, some participants
did the first set of tasks in the presence of a human (H), the
next set in the presence of the virtual human (VH), and the
third set alone (A). The other two orders were A -> H -> VH
and VH -> A -> H.
Within a particular presence situation (e.g., virtual
human), participants did a block of anagrams, a block of
mazes, and a block of modular arithmetic problems. Task
order was manipulated using a Latin Square resulting in
three possible orders (anagram -> maze -> mod arith;
maze -> mod arith -> anagram; mod arith -> anagram ->
maze).
Within each task block, participants conducted a
combination of easy and difficult trials for that particular
task (e.g., anagrams). The number of easy and difficult trials
was the same (six) in each block; however, the order of easy
and hard trials was one of the three predetermined pseudorandomized orders.
In the anagram tasks, a five letter anagram appeared on
the screen and the participants were asked to solve the
anagram quickly and accurately by typing in the answer
using the keyboard and then pressing the Enter key.
Completion time and error rates were measured.
In the maze tasks, a maze appeared on the screen.
Participants were asked to move the cursor by dragging the
mouse through each maze and find the exit as fast as
possible. Completion time was measured.
In modular arithmetic tasks, a problem statement (e.g., 50
≡ 20 (mod 4)) appeared on the screen. Participants were
asked to decide whether the statement was true or false by
pressing the corresponding button in the keyboard. There
was a “Y” marked key corresponding “true” and an “N”

Modular Arithmetic Task
The object of Gauss’s modular arithmetic is to judge if a
problem statement such as 50 ≡ 22 (mod 4) is true. In this
case, the statement’s middle number is subtracted from the
first number (i.e., 50 – 22) and the result (i.e., 28) is divided
by the last number (i.e., 28 ÷ 4). If the quotient is a whole
number (as here, 7) then the statement is true. Difficulty of
the task was manipulated by controlling the number of
digits presented to participants for the first two numbers of a
given problem; one for an easy task (7 ≡ 2) and two (with
borrowing) for a difficult task (51 ≡ 19). Beilock, Kulp,
Holt, and Carr (2004) claimed that modular arithmetic is
advantageous as a laboratory task because it is unusual and,
therefore its learning history can be controlled.

Method
Participants
One hundred and eight participants were recruited from the
Georgia Institute of Technology. Participants were
compensated with course credit.

Materials
Participants did all tasks (anagrams, maze, and modular
arithmetic) on a computer. Java application and Java script
were used to implement the tasks on the computer. An
additional computer was used to present the virtual human.
Haptek Corporation’s 3-D character was loaded on this
computer (Figure 1); the appearance of the virtual human
was held constant. The character displayed life-like
behaviors such as breathing, blinking, and other subtle facial
movements. The monitor was positioned so that the virtual
human was oriented to the task screen and not the
participant and was located about 4 feet from the task
monitor and about 3.5 feet from the participant. This is also
the location where the human observer would sit.

559

marked key corresponding “false”. Response time and error
rates were measured.
Each participant was briefed on each task prior to the
actual experiment. Briefing consisted of a demonstration by
the experimenter and four hands-on practice trials for the
participants so that they could familiarize themselves with
the computer and the task.
For conditions involving a human or a virtual human, the
participants were told that a human or a virtual human was
there to “observe” the task and not the participant.
Specifically, when a human was present, participants were
told:
An observer will be sitting near you to observe the
tasks you will do. The observer will be present to
learn more about the tasks and try to catch any
mistakes we made in creating the tasks. The
observer is not trying to learn how you go about
working on the tasks and, in fact, will not be
allowed to communicate with you while he is sitting
here.

Complexity interaction that is consistent with the social
facilitation effect, F(2, 162) = 53.0, MSE = .21, p < .001.
Significant two-way interactions were found between
complexity and task type, F(2, 162) = 42.7, MSE = .56, p
< .001, and between presence and task type, F(2, 162) =
6.66, MSE = .21, p < .001. There was a main effect of
complexity (easy, hard), F(1, 81) = 807.77, MSE = .99, p
< .001, and presence (alone, virtual human, human), F(2,
162) = 8.6, MSE = .21, p < .001, but no main effect of task
type, F < 1.

Post hoc Analyses for Each Task Type
The three-way Presence x Task x Complexity interaction
suggests that each task type should be further analyzed for
the relationship between presence and complexity. We
conducted a post hoc Dunnett’s test to compare each
presence condition to the alone condition separately for each
task type. For each task type, the social facilitation effect for
virtual humans was demonstrated and, for each task type,
the social facilitation effect for humans was demonstrated.
The analyses for each of the above observations are
presented below.

When a virtual human was present, participants were told:
A virtual human will observe the task. The virtual
human is an artificial intelligence that attempts to
analyze events that happen on the computer screen.
The virtual human will be present to learn more
about the tasks and try to catch any mistakes we
made in creating the tasks. The virtual human is not
trying to learn how you go about working on the
tasks and, in fact, will not be allowed to
communicate with you while he is present.

Results
A three-way repeated-measures ANOVA (complexity factor,
presence factor, and task factor) was initially conducted and
was followed by simple effects analyses. Analysis was
conducted on response times because it has been the most
frequently measured dependent variable in social facilitation
research (Bond & Titus, 1983). The pseudo order and the
Latin Square orders had no effect on task performance and
all results were collapsed over these variables.
Data were transformed into z-scores for each task to
perform the analysis involving complexity, presence, and
task. The results (summarized in Figure 2) show that the
effect of presence on task completion time was conditional
upon the combination of the task and task complexity,
resulting in a significant three-way interaction of Presence x
Task type x Complexity (F(4, 324) = 4.39, MSE = .23, p
< .01). Of particular importance, the results show that
combined across task types (anagram, maze, and modular
arithmetic), if the task was easy then completion times in
the presence of the virtual human and the real human tended
to be faster than in their absence, whereas, if the task was
hard then mean completion times were slower in the
presence of the virtual human and the real human than in
their absence. This observation is supported by a Presence x

560

Virtual Human vs. Alone. For the anagram task, response
time for easy tasks was shorter in the virtual human
condition than the alone condition, t(214) = 3.17, MSE = .36,
p < .001, and the response time for hard tasks was longer in
the virtual human condition than the alone condition, t(214)
= 3.09, MSE = .25, p < .01 (see Figure 2a). For the maze
task, response time for easy tasks was shorter in the virtual
human condition than the alone condition, t(214) = 2.60,
MSE = .02, p < .01, and the response time for hard tasks
was longer in the virtual human condition than the alone
condition, t(214) = 1.90, MSE = .25, p < .05 (see Figure 2b).
For the modular arithmetic task, response time for easy
tasks was shorter in the virtual human condition than the
alone condition, t(214) = 2.00, MSE = .05, p < .05, and the
response time for hard tasks was longer in the virtual human
condition than the alone condition, t(214) = 4.33, MSE = .21,
p < .001 (see Figure 2c).
Human vs. Alone. For the anagram task, response time for
easy tasks was shorter in the human condition than the alone
condition, t(214) = 4.57, MSE = .23, p < .001, and the
response time for hard tasks was longer in the human
condition than the alone condition, t(214) = 3.48, MSE = .48,
p < .001 (see Figure 2a). For the maze task, response time
for easy tasks was shorter in the human condition than the
alone condition, t(214) = 3.71, MSE = .03, p < .001, and the
response time for hard tasks was longer in the human
condition than the alone condition, t(214) = 2.14, MSE = .30,
p < .05 (see Figure 2b). For the modular arithmetic task,
response time for easy tasks was shorter in the human
condition than the alone condition, t(214) = 2.00, MSE = .04,
p < .05, and the response time for hard tasks was longer in
the human condition than the alone condition, t(214) = 7.81,
MSE = .31, p < .001 (see Figure 2c).

Virtual Human vs. Human. For the anagram task,
response time for easy tasks was shorter in the human
condition than the virtual human condition, t(214) = 1.75,
MSE = .08, p < .05, but the response time for hard tasks was
not significantly different, t(214) < 1 (see Figure 2a). For
the maze task, response time for easy tasks was shorter in
the human condition than the virtual human condition,
t(214) = 2.21, MSE = .01, p < .05, but the response time for
hard tasks was not significantly different, t(214) < 1 (see
Figure 2b). For the modular arithmetic task, response time
for easy tasks was not significantly different, t(214) < 1, but
the response time for hard tasks was longer in the human
condition than the virtual human condition, t(214) = 4.56,
MSE = .36, p < .001 (see Figure 2c).
Figure 2: Mean completion time in seconds for each
condition for the different tasks (n = 108).

Discussion
The key hypotheses in this study were supported: For easy
tasks, performance in the virtual human condition was better
than in the alone condition, and for difficult tasks,
performance in the virtual human condition was worse than
in the alone condition. We replicated the social facilitation
effect of humans and expanded this effect to the presence of
virtual humans.
How would a presence of virtual human evoke a social
facilitation response? Drive theory (Zajonc, 1965) would
argue that the presence of a virtual human elevated the
participants’ drive levels. This increased drive supposedly
enhances performance of easy tasks and inhibits difficult
tasks. The evaluation apprehension theory (Cottrell, 1972)
would argue that participants were concerned with how the
virtual human would evaluate them and this increased drive
level. However, these theories are built on findings with
humans and might have limitations in explaining social
facilitation of virtual humans.
It is worth considering how the presence of a virtual
human is similar to or different from the presence of a
human. What factors might lead someone to treat a virtual
human as a person versus treating the virtual human as a
program? On the one hand, users might unconsciously
regard virtual humans as social entities (Nass et al., 1994).
On the other hand, users are aware of the limitations of
virtual humans based on their knowledge with computer
systems.
We suggest that the social nature of human-virtual human
interaction is somewhere between human-human interaction
and human-computer interaction. Users' behaviors when
interacting with virtual humans are different from when they
interact with windows, menus, and icons. Yet, users react to
virtual humans in the same way as to humans under certain
conditions (Xiao, 2006). The present results seem to support
this idea. For easy tasks participants were fastest in the
presence of a human and slowest when alone; the virtual
human trials fell in the middle although the difference

561

between the human and virtual human trials was not
significant for the modular arithmetic task. Similarly, for
hard tasks participants were fastest alone and slowest with a
human present; the virtual human trials fell in the middle.
The difference between the human and virtual human trials
was not significant for the anagram and the maze task. The
present experiment was not designed to explore differences
due to the presence of virtual human and human, but future
studies can begin to do this.
The results have implications for the design of
instructional systems as well as other systems involving
human-computer interaction. They suggest that designers of
such systems should consider that users behave differently
in the presence of virtual humans and that the nature of the
behavior depends on the task (such as task difficulty). A
design decision to present a virtual human should be a
deliberate and thoughtful one. An ever-present virtual
human might make users uneasy, thus diminishing
performance for a challenging task. Presenting a virtual
human only when it is called upon by a user or when a task
is easy might be a good idea.

Conclusion
This study examined whether the social facilitation effect
can be evoked by virtual humans. The study found that
virtual humans do produce the social facilitation effect: for
easy tasks, performance in the virtual human condition was
better than in the alone condition, and for difficult tasks,
performance in the virtual human condition was worse than
in the alone condition. This was observed for a range of
verbal, spatial, and mathematical tasks. Designers and
practitioners of interaction systems should be mindful about
the likely social nature of virtual humans.

Reference
Aiello, J. R. & Douthitt, E. A. (2001). Social facilitation
from Triplett to electronic performance monitoring.
Group Dynamics, 5, 163-180.
Beilock, S. L., Kulp, C. A., Holt, L. E., & Carr, T. H. (2004).
More on the fragility of performance: Choking under
pressure in mathematical problem solving, Journal of
Experimental Psychology: General, 133(4), 584-600.
Bond, C. F. & Titus, L. J. (1983). Social facilitation: A
meta-analysis of 241 studies. Psychological Bulletin, 94,
265-292.
Catrambone, R., Stasko, J., & Xiao, J (2004). ECA user
interface paradigm: Experimental findings within a
framework for research. In C. Pelachaud & Z. Ruttkay
(Ed.), From brows to trust: Evaluating embodied
conversational
agents,
239-267.
NY:
Kluwer
Academic/Plenum Publishers.
Collier, G. (1985). Emotional Expression. Hillsdale, NJ:
Lawrence Erlbaum Associates.
Cottrell, N. B. (1972). Social Facilitation. In C. McClintock
(Ed.), Experimental Social psychology, 185-236.
Davidson, R. & Henderson, R. (2000). Electronic
performance monitoring: A laboratory investigation of

562

the influence of monitoring and difficulty on task
performance, mood state, and self-reported stress levels,
Journal of Applied Social Psychology, 30(5), 906-920.
Glantz, K., Rizzo, A., & Graap, K. (2003). Virtual reality
for psychotherapy: current reality and future possibilities.
Psychotherapy: Theory, Research, Practice, Training,
40(1/2), 55-67.
Hill, R. W., Gratch, J., Marsella, S., Rickel, J., Swartout, W.,
& Traum, D. (2003). Virtual humans in the mission
rehearsal exercise system. Kynstlich Intelligenz, 17(4),
32-38.
Jackson, J. M. & Williams, K. D. (1985). Social loafing on
difficult tasks: Working collectively can improve
performance. Journal of Personality and Social
Psychology, 49, 937-942.
Jeong, S., Hasimoto, N., & Makoto, S. (2004). A novel
interaction system with force feedback between real- and
virtual human: an entertainment system: “virtual catch
ball”, Proceedings of the 2004 ACM SIGCHI
International Conference on Advances in computer
entertainment technology, 61-66.
Lester, J., Converse, S., Kahler, S., Barlow S., Stone, B., &
Bhogal, R. (1997). The persona effect: affective impact of
animated pedagogical agents. Proceedings of the SIGCHI
conference on Human Factors in computer systems, 359366.
Nass, C., Steuer, J., & Tauber, E. (1994). Computers are
social actors. Proceedings of the ACM Computer Human
Interaction 1994 Conference, 72-78.
Rajecki, D. W., Ickes, W., Corcoran, C., & Lenerz, K.
(1977). Social facilitation of human performance: mere
presence effects, The Journal of Social Psychology, 102,
297-310.
Reeves, B. & Nass, C. (1996). The Media Equation. New
York, NY: Cambridge University Press.
Rickenberg, R., & Reeves, B. (2000). The effects of
animated characters on anxiety, task performance, and
evaluations of user interfaces. Proceedings of the ACM
Computer Human Interaction 2000 Conference, 49-56.
Tresselt, M. E., & Mayzner, M. S. (1966). Normative
solution times for a sample of 134 solution words and 378
associated
anagrams.
Psychonomic
Monograph
Supplements, 1, 293-298.
Walker, J., Sproull, L., & Subramani, R. (1994). Using a
human face in an interface. Proceedings of the
Conference on Human Factors in Computers 1994, 85-91.
Xiao, J. (2006). Empirical studies on embodied
conversational agents. Doctoral dissertation, Georgia
Institute of Technology, Atlanta, GA.
Zajonc, R. B. (1965). Social facilitation. Science, 149, 269274.
Zanbaka, C. Ulinski, A., Goolkasian, P., & Hodges, L. F.
(2004). Effects of virtual human presence on task
performance. Proceedings of the International
Conference on Artificial Reality and Telexistence, 174 –
181.

