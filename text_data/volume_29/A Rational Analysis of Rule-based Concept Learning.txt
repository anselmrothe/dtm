UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Rational Analysis of Rule-based Concept Learning
Permalink
https://escholarship.org/uc/item/37n016f5
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Goodman, Noah D.
Griffiths, Thomas
Feldman, Jacob
et al.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                            A Rational Analysis of Rule-based Concept Learning
                       Noah D. Goodman1 (ndg@mit.edu), Thomas Griffiths2 (tom griffiths@berkeley.edu),
                         Jacob Feldman3 (jacob@ruccs.rutgers.edu), Joshua B. Tenenbaum1 (jbt@mit.edu)
                         1 Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology
                                      2 Department of Psychology, University of California, Berkeley
                             3 Department of Psychology, Center for Cognitive Science, Rutgers University
                               Abstract                                     However, existing rule-based models are primarily
                                                                         heuristic—no rational analysis has been provided, and they
   We propose a new model of human concept learning that pro-
   vides a rational analysis for learning of feature-based concepts.     have not been tied to statistical approaches to induction. A ra-
   This model is built upon Bayesian inference for a grammat-            tional analysis for rule-based models might assume that con-
   ically structured hypothesis space—a “concept language” of            cepts are (represented as) rules, and ask what degree of be-
   logical rules. We compare the model predictions to human
   generalization judgments in two well-known category learning          lief a rational agent should accord to each rule, given some
   experiments, and find good agreement for both average and             observed examples. We answer this question by formulat-
   individual participants’ generalizations.                             ing the hypothesis space of rules as words in a “concept lan-
   Keywords: concept learning; categorization; Bayesian induc-           guage” generated by a context-free grammar. Considering the
   tion; probabilistic grammar; rules.                                   probability of productions in this grammar leads to a prior
                                                                         probability for words in the language, and the logical form
                           Introduction                                  of these words motivates an expression for the probability of
Concepts are a topic of perennial interest to psychology, par-           observed examples given a rule. The methods of Bayesian
ticularly concepts which identify kinds of things. Such con-             analysis then lead to the Rational Rules model of concept
cepts are mental representations which enable one to discrim-            learning. This grammatical approach to induction has ben-
inate between objects that satisfy the concept and those which           efits for Bayesian rational analysis: it compactly specifies an
do not. Given their discriminative use, a natural hypothesis is          infinite, and flexible, hypothesis space of structured rules and
that concepts are simply rules for classifying objects based             a prior that decreases with complexity. The Rational Rules
on features. Indeed, the “classical” theory of concepts (see             model thus makes contributions to both rule-based concept
Smith and Medin, 1981) takes this viewpoint, suggesting that             modeling and rational statistical learning models: to the for-
a concept can be expressed as a simple feature-based rule: a             mer it provides a rational analysis, and to the latter it provides
conjunction of features that are necessary and jointly suffi-            the grammar-based approach. Across a range of experimen-
cient for membership. Early models based on this approach                tal tasks, this new model achieves comparable fits to the best
failed to account for many aspects of human categorization               rule-based models in the literature, but with fewer free pa-
behavior, especially the graded use of concepts (Mervis and              rameters and arbitrary processing assumptions.
Rosch, 1981). Attention consequently turned to models with
a more statistical nature: similarity to prototypes or to exem-                           An Analysis of Concepts
plars (Medin and Schaffer, 1978; Kruschke, 1992; Love et al.,
2004). The statistical nature of many of these models has                A general approach to the rational analysis of inductive learn-
made them amenable to a rational analysis (Anderson, 1990),              ing problems has emerged in recent years (Anderson, 1990;
which attempts to explain why people do what they do, com-               Tenenbaum, 1999; Chater and Oaksford, 1999). Under this
plementing (often apparently ad-hoc) process-level accounts.             approach a space of hypotheses is posited, and beliefs are as-
   Despite the success of similarity-based models, recently re-          signed using Bayesian statistics—a coherent framework that
newed interest has led to more sophisticated rule-based mod-             combines data and a priori knowledge to give posterior de-
els. Among the reasons for this reconsideration are the inabil-          grees of belief. Uses of this approach, for instance in causal
ity of similarity-based models to provide a method for con-              induction (Griffiths and Tenenbaum, 2005) and word learn-
cept combination, common reports by participants that they               ing (Xu and Tenenbaum, 2005), have successfully predicted
“feel as if” they are using a rule, and the unrealistic mem-             human generalization behavior in a range of tasks.
ory demands of most similarity-based models. The RULEX                      In our case, we wish to establish a hypothesis space of
model (Nosofsky et al., 1994), for instance, treats concepts             rules, and analyze the behavior of a rational agent trying to
as conjunctive rules plus exceptions, learned by a heuristic             learn those rules from labeled examples. Thus the learn-
search process, and has some of the best fits to human ex-               ing problem is to determine P(F|E, `(E)), where F ranges
perimental data—particularly for the judgments of individ-               over rules, E is the set of observed example objects (possibly
ual participants. Parallel motivation for reexamining the role           with repeats) and `(E) are the observed labels. (Through-
of logical structures in human concept representation comes              out this section we consider a single labeled concept, thus
from evidence that the difficulty of learning a new concept is           `(x) ∈ {0, 1} indicates whether x is an example or a non-
well predicted by its logical complexity (Feldman, 2000).                example of the concept.) This quantity may be expressed
                                                                     299

(through Bayes’ formula):                                                                    S    →     (B) ∨ S
                                                                                             S    →     (B)
                                                                                             B    →     B∧P
               P(F|E, `(E)) ∝ P(F)P(E, `(E)|F)               (1)                             B    →     P
                                                                                             P    →     D1
To use this relationship we will need, in addition to a hy-                                        ..
                                                                                                    .
pothesis space, the prior probability, P(F), and a likelihood                                P    →     DN
function, P(E, `(E)|F).                                                                     D1    →      f1 (x) = 1
                                                                                            D1    →      f1 (x) = 0
Concept Representation                                                                              ..
                                                                                                     .
The hypothesis space of rules is given by well formed formu-                                DN    →      fN (x) = 1
                                                                                            DN    →      fN (x) = 0
lae of a concept language, which is specified by a context-
free grammar over an alphabet of terminal symbols. In our
case the terminal symbols are logical connectives (¬, ∧, ∨),         Figure 1: The DNF Grammar. S is the start symbol, and
grouping symbols, and a set of feature predicates. The fea-          B, P, Di the other non-terminals. fi (x) is the value of the ith
ture predicates are formed from functions fi (x), which report       feature.
the value of a physical feature, and the operator =c, which
represents comparison with constant c: each feature predi-           where G is the grammar, s ∈ T are the productions of parse
cate is of the form fi (x)=c (read “the ith feature of object        T , and τ(s) their probability. (τ(s) sums to one over the
x has value c”). For brevity we consider here only Boolean           productions of each non-terminal.) The DNF grammar is a
features: fi (x) ∈ {0, 1} and c ∈ {0, 1}. (The extension to con-     unique production grammar—there is a single production for
tinuous features may be made in a straightforward manner,            each well-formed formula—so the probability of a formula is
replacing the equality operator by inequality.)                      also given by Eq. 2. (We write F below for both the formula
   The DNF grammar (Fig. 1) generates formulae in dis-               and its parse.) Note that this prior captures a simplicity bias:
junctive normal form (ie. disjunctions of conjunctions of            syntactically shorter formulae have smaller parse trees, thus
features). Each concept in this language consists of a set           higher prior probability.
of “definitions”, the B non-terminals, and each definition              We have no a priori reason to prefer one set of values for
consists of necessary and sufficient features (the P non-            τ to another, so we assume a uniform prior over the possible
terminals, which become feature predicates). For instance,           values of τ. The probability of a parse becomes:
from the start symbol S, we might generate two “definitions”,                                  Z
(B) ∨ (B); from these we perhaps reach (P ∧ P) ∨ (P); each                          P(T |G ) =    P(τ) ∏ τ(s)dτ
                                                                                                        s∈F
P is specialized to a feature: (D2 ∧ D1 ) ∨ (D4 ); and, finally                                Z
( f2 (x) = 1 ∧ f1 (x) = 0) ∨ ( f4 (x) = 0). We will focus on the                             =    ∏ τ(s)dτ                        (3)
DNF grammar, a natural interpretation of the classical theory                                     s∈F
of concepts, for the present exercise, but it is by no means                                 =  ∏ β(Count(Y, F) + 1),
the only (or most interesting) possibility. For instance, one                                  Y ∈N
may formulate a grammar inspired by the representations of
                                                                     where β(·) is the normalizing constant of the Dirichlet distri-
Nosofsky et al. (1994), which generates a rule plus excep-
                                                                     bution, and Count(Y, F) is the vector of counts of the produc-
tions, or a grammar of implications, inspired by Feldman
                                                                     tions for non-terminal symbol Y in the unique parse of F.
(2006), which represents causal structure.
   The concept language generates well formed formulae of            Likelihood: Evaluation and Outliers
first-order logic, which allows us to recursively evaluate a         To derive a likelihood function, we begin by making the weak
formula on a given object (which is the usual approach in            sampling assumption, that the set of observed examples is
mathematical logic (Enderton, 1972)). Briefly, each term in-         independent of the concept:
volving a logical connective can be evaluated in terms of its
constituents, and, presuming that we know the feature values                        P(E, `(E)|F) = P(`(E)|F, E)P(E).              (4)
for the object x, we can evaluate all of the feature predicates.
                                                                     The term P(E) will cancel from our calculations when all fea-
This assigns a truth value F(x) to formula F for each object
                                                                     ture values are observed for all objects. Next we assume that
x.
                                                                     the label is true exactly when an object satisfies the hypoth-
A Syntactic Prior                                                    esized formula. Thus, if we knew that the observed labels
                                                                     were correct, and we required an explanation for each ob-
By supplementing the grammar with a probability for each             servation, this likelihood would reduce to evaluation of the
production we get a prior over the formulae of the language.         formula for each example (logical true is interpreted as prob-
The probability of a given parse (sequence of productions) is:       ability 1, etc.):
                                                                                                       ^
                       P(T |G , τ) = ∏ τ(s),                 (2)                       P(`(E)|F, E) =        `(x)↔F(x).           (5)
                                      s∈T                                                              x∈E
                                                                 300

However, to allow concepts that explain only some of the ob-                      who take A to be the main category, and B the contrast cat-
servations, we assume that there is a probability e−b that any                    egory, with vice versa. Since these experiments have similar
given observation is an outlier (ie. an unexplainable obser-                      numbers of A and B examples, this is probably a reasonable
vation which should be excluded from induction). Writing S                        initial assumption.
for the set of examples which are not outliers, the likelihood                                   1
becomes:
                                                                                                0.9
                              −b |S|   −b |E|−|S|                                                                                                                 A3=0100
                                                        ^
 P(`(E)|F, E) =    ∑ (1 − e     ) (e )                      `(x)↔F(x)                           0.8                                                         A2=0101   T3=0000
                   S⊆E                               x∈S                                                                                                            A1=0001
                                                 −b |S| −b |E|−|S|                              0.7
               =          ∑            (1 − e ) (e )                                                                                        A4=0010
                                                                                   Human mean
                   S⊆{x∈E|`(x)↔F(x)}                                                            0.6
                                                                                                                                          T1=0110         A5=1000
                    −b|{x∈E|¬(`(x)↔F(x))}|
               =e                            ,                                                  0.5                                                 T6=1100
                                                                      (6)                       0.4
                                                                                                                              T2=0111       B2=1001
                                                                                                                              T4=1101       B1=0011
                                                                                                                              T5=1010
where the second step follows from the Binomial Theorem.                                        0.3
                                                                                                          T7=1011
                                                                                                0.2
The Rational Rules Model                                                                                                  B3=1110
                                                                                                0.1       B4=1111
The above likelihood and prior, combined using Bayes’ rule,
constitute a model of concept learning, which we call the Ra-                                    0
                                                                                                      0    0.1      0.2      0.3    0.4      0.5    0.6      0.7    0.8   0.9   1
tional Rules model (RRDNF , to indicate the grammar). The                                                                           RR predictions
posterior probability for this model is:
                                         !                                        Figure 2: The category structure of Medin and Schaffer
                                                     −b|{x∈E|¬(`(x)↔F(x))}|       (1978), plotted on model vs. human results. Each example
P(F|E, `(E)) ∝     ∏ β(|{y ∈ F}| + 1)            e
                                                                                  has four binary features; A1-A5 are examples of category
                   Y ∈N
                                                          (7)                     A, B1-B4 of B, and T1-T7 are transfer objects. The human
There is a trade-off in this posterior between explanatory                        mean generalization rates of Nosofsky et al. (1994) are plot-
completeness and conceptual parsimony. Though some ex-                            ted against the predictions of the RRDNF model (b=1). The
amples may be ignored as outliers, concepts that explain more                     model accounts for R2 =0.98 of the variance in human data.
data are preferred by the likelihood function. On the other
hand, simpler (i.e. syntactically shorter) formulae are pre-                          Participants were trained on the category structures in a
ferred by the prior.                                                              blocked-learning paradigm: each example in the training set
   Using this posterior belief function, the generalization                       was presented once per block, and blocks were presented un-
probability that a test object “t is an L” is:                                    til the training set could be classified accurately (relative to
                                                                                  a predetermined threshold). It is often the case that different
           P(`(t)|E, `(E)) = ∑ F(t)P(F|E, `(E)).                      (8)         effects occur as training proceeds, and these effects can be
                                F                                                 tricky to capture in a rational model. However, it is apparent
                                                                                  from Eq. 6 that the Rational Rules model with outlier param-
This generalization probability represents inference by the                       eter b and N identical blocks of examples is equivalent to the
“ideal learner”. Initially we assume, consistent with standard                    model with one block and parameter N · b. This makes intu-
practices of Bayesian modeling, that the average of the hu-                       itive sense: the more often an example is seen, the less likely
man population matches this ideal. At the end of the next                         it is to be an outlier. Thus we may roughly model the course
section we consider the relationship between the ideal learner                    of human learning by varying the b parameter – effectively
and individual human learners.                                                    assuming a constant outlier probability while increasing the
                                                                                  number of trials.
 Comparison with Human Category Learning
                                                                                      The model was approximated by Monte Carlo simulation
In the preceding sections we have presented a rational anal-                      (30,000 samples for each run). Except where otherwise noted
ysis of concept learning when concepts are represented in a                       we have coarsely optimized over b by taking the best fitting
conceptual language of propositional rules. In this section we                    result from among b={1, 2, ..., 8}.
explore the extent to which this rational analysis can explain
human learning. We will consider two experiments from the                         Prototype Enhancement and Typicality Effects
concept learning literature that have often been used as initial                  The second experiment of Medin and Schaffer (1978), among
tests for modeling efforts.                                                       the first studies of ill-defined categories, used the category
   In the experiments considered below participants were re-                      structure shown on Fig. 2 (we consider the human data from
quired to distinguish between two categories, A and B, which                      the Nosofsky et al. (1994) replication of this experiment,
were mutually exclusive. For simplicity in fitting the model                      which counter-balanced physical feature assignments). This
we assume that the population is an even mixture of people                        experiment is a common first test of the ability of a model
                                                                            301

                          0.7                                                    0.4
                                                     Posterior feature weight
                                                                                0.35                         Table 1: The category structure of Medin et al. (1982), with
  Posterior probability
                          0.6
                          0.5
                                                                                 0.3
                                                                                                             initial and final block mean human responses of McKinley
                          0.4
                                                                                0.25
                                                                                                             and Nosofsky (1993), and the predictions of RRDNF at b=1
                                                                                 0.2
                          0.3
                                                                                                             and b=8.
                                                                                0.15
                          0.2
                                                                                 0.1                                Object    Human: Initial   Final    RRDNF : b=1    b=8
                          0.1                                                   0.05
                                                                                                              A1    1111         0.64          0.96        0.84          1
                                                                                                              A2    0111         0.64          0.93        0.54          1
                           0
                                1    2   3   4   5
                                                                                  0
                                                                                       1   2   3   4          A3    1100         0.66            1         0.84          1
                                    Complexity                                             Feature            A4    1000         0.55          0.96        0.54        0.99
                                                                                                              B1    1010         0.57          0.02        0.46          0
                                                                                                              B2    0010         0.43            0         0.16          0
Figure 3: Posterior complexity and feature weight distri-                                                     B3    0101         0.46          0.05        0.46        0.01
butions for RRDNF on the category of Medin and Schaffer                                                       B4    0001         0.34            0         0.16          0
(1978) (see Fig. 2). The model focuses on simple rules along                                                  T1    0000         0.46          0.66        0.2         0.56
                                                                                                              T2    0011         0.41          0.64        0.2         0.55
dimensions 1 and 3.                                                                                           T3    0100         0.52          0.64        0.5         0.57
                                                                                                              T4    1011          0.5          0.66        0.5         0.56
                                                                                                              T5    1110         0.73          0.36        0.8         0.45
to predict the pattern of generalizations on novel stimuli, and                                               T6    1101         0.59          0.36        0.8         0.44
demonstrates two important effects: prototype enhancement                                                     T7    0110         0.39          0.27        0.5         0.44
                                                                                                              T8    1001         0.46           0.3        0.5         0.43
(Posner and Keele, 1968), and (a certain flavor of) typicality.
    The overall fit of the Rational Rules model (Fig. 2) is good:
R2 =0.98. Other models of concept learning are also able to                                                  often. The Rational Rules model also predicts this typicality
fit this data quite well: for instance R2 =0.98 for RULEX, and                                               effect, in a manner similar to prototype enhancement.
R2 =0.96 for the context model (Medin and Schaffer, 1978).
However, RRDNF has only a single parameter (the outlier pa-                                                  Correlated Dimensions
rameter), while each of these models has four or more free                                                   Medin et al. (1982) studied the category structure shown in
parameters; indeed, the full RULEX model has nine free pa-                                                   Table 1. This structure affords two strategies: the first two
rameters (whose interpretation is not entirely clear).                                                       features are individually diagnostic of category membership,
    In Fig. 3 we have plotted the posterior probability mass that                                            but not perfectly so, while the correlation between the third
the RRDNF model places on all formulae of a given complex-                                                   and fourth features is perfectly diagnostic. It was found that
ity and the posterior feature weights—the expected impor-                                                    human learners relied on the perfectly diagnostic, but more
tance of each feature. (Complexity is measured by Boolean                                                    complicated, correlated features. McKinley and Nosofsky
complexity: the number of feature symbols in the formula).                                                   (1993) replicated this result, studying both early and late
We see that the RRDNF model solves this concept learning                                                     learning by eliciting transfer judgments after initial and fi-
problem, as human learners do, by placing most of its weight                                                 nal training blocks. They found that human participants re-
on simple formulae along features 1 and 3. It has been noted                                                 lied primarily on the individually diagnostic dimensions in
before (Navarro, 2006) that selective attention effects, like                                                the initial stage of learning, and transitioned to the correlated
this one, emerge naturally from the Bayesian framework.                                                      features later in learning. The RRDNF model explains most of
    The object T3=0000 is the prototype of category A, in the                                                the variance in human judgments in the final stage of learn-
sense that most of the examples of category A are similar to                                                 ing, R2 =0.95 when b=8; see Fig. 4. Correlation with hu-
this object (differ in only one feature) while most of the exam-                                             man judgments after one training block is also respectable:
ples of category B are dissimilar. Though it never occurs in                                                 R2 =0.69 when b=1. Thus, the course of human learning in
the training set, the importance of this prototype is reflected in                                           this experiment can be modeled quite well, as predicted, by
the human transfer judgments: T3 is, by far, the most likely                                                 varying the outlier parameter b. By comparison RULEX best
transfer object to be classified as category A. The Rational                                                 fits are R2 =0.99 for final, and R2 =0.67 for initial learning
Rules model predicts this prototype enhancement. The sim-                                                    block, but with several extra free parameters.
ple formulae f1 (x)=0 and f3 (x)=0 each have high posterior
probability, these agree on the categorization of T3 and so                                                  Individual Generalization Patterns
combine (together with many lower probability formulae) to                                                   Nosofsky et al. (1994) investigated the pattern of general-
enhance the probability that T3 is in category A.                                                            izations (i.e. the sequence of responses to transfer questions)
    The degree of typicality, or recognition rate for training ex-                                           made by individual participants, in addition to the group av-
amples, is often taken as a useful proxy for category centrality                                             erages. One may wonder whether it is necessary to con-
(Mervis and Rosch, 1981) because it correlates with many of                                                  sider these individual differences. As noted in Nosofsky and
the same experimental measures (such as reaction time). In-                                                  Palmeri (1998), even the best binomial model (a model that
deed we see greater typicality for the prototype of category B,                                              predicts the averaged data perfectly, but assumes that all in-
the object B4=1111, than for other training examples: though                                                 dividuals behave identically) does very poorly at predicting
presented equally often it is classed into category B far more                                               individual generalization patterns (in this case R2 =0.24).
                                                                                                       302

      1
                                                                                       Discussion and Conclusion
     0.9
                                                                          We have suggested an approach for analyzing human concept
     0.8                                           Final block.           learning: assume that concepts are represented in a concept
                                                   Initial block.         language, propose a specific grammar and inductive seman-
     0.7
                                                                          tics for this language, then describe rational inference from
     0.6                                                                  examples to words of the language. Carrying out this scheme
R2                                                                        using a grammar for DNF formulae, we derived the Rational
     0.5                                                                  Rules (RRDNF ) model of concept learning. This model was
                                                                          shown to predict human judgments in several key category
     0.4
                                                                          learning experiments, and to do so with only one, readily in-
     0.3                                                                  terpretable, parameter. The model was also used to predict
                                                                          the generalization judgments of individual learners, extend-
     0.2                                                                  ing the usual reach of rational analysis.
     0.1                                                                     To model individual judgments we assumed that each
        0   1     2     3      4       5    6     7       8         9
                                   b                                      learner arrives at a small set of rules, chosen and weighted in
                                                                          accord with the complete rational model. This raises a ques-
Figure 4: Comparison of human judgments with RRDNF                        tion: how are individuals able to find such samples? One
model predictions. Fit of RRDNF model to the initial and final            answer, which has been given by the RULEX model, is that
block mean human generalization pattern for several values                learners apply a variety of hypothesis testing and generation
of the outlier parameter, see Table 1. The model fits the early           heuristics. To understand how such heuristics are related to
learning data well at small values of b, and the late learning            rational modeling, particularly to the RRDNF model, investi-
data well for larger values.                                              gation is needed into on-line algorithms which can approxi-
                                                                          mate the Bayesian posterior while maintaining only a small
                                                                          number of hypotheses—that is, “rational process models”.
                                                                          A useful initial exploration has been made by Sanborn et al.
                                                                          (2006), who focused on similarity-based models. (We have,
                                                                          in fact, verified some of the simulation results presented in
                                                                          this paper using a similar on-line sequential Monte Carlo al-
   To model individual generalizations using the Rational                 gorithm, based upon Chopin (2002), but have not evaluated
Rules model we make an additional assumption about how                    this algorithm for psychological relevance.)
individuals approximate the full posterior distribution over
                                                                             As noted in Love et al. (2004), many of the ideas under-
formulae. The simplest assumption (similar in spirit to
                                                                          lying SUSTAIN, and other modern descendants of exemplar
RULEX) is that each participant uses a single formula, sam-
                                                                          models, are similar to those underlying RULEX. In particu-
pled randomly from the posterior. Note that this predicts the
                                                                          lar, each attempts to provide a rich set of possible concepts,
same group averages, for large enough populations, but pro-
                                                                          while controlling their complexity in any given instance. The
vides a psychological model more similar to existing process-
                                                                          grammar-based approach introduced here has a similar goal,
oreinted models of rule use. This interpretation of the RRDNF
                                                                          but at a different descriptive level, and thus complements
model explains R2 = 0.83 of the variance in human general-
                                                                          much of the earlier modeling work.
ization for the 36 individual generalization patterns reported
in Nosofsky et al. (1994). However, it is troubling to re-                   Unlike many approaches, the basic idea of grammar-based
introduce deterministic rule use in this way, considering evi-            induction can be easily extended to new situations, such as
dence of graded category centrality effects even within indi-             role-governed or adjective-like concepts. Indeed, an impor-
viduals. A similar, but more flexible, possibility is that each           tant direction for future work concerns the representations of
individual considers a small number of rules, and weights                 other types of concepts by extending the concept language to
them appropriately. This is similar to the idealized cogni-               a larger fragment of first-order, or higher-order, logic.
tive models suggested by Lakoff (1987), and explains how                     The proposal that concepts are represented by words in a
individuals could exhibit graded behavior, if weight is spread            concept language is not new in cognitive science—indeed this
among several rules, or almost deterministic behavior, if a               is a principal component of the language of thought hypoth-
single rule predominates. A model in which individuals take               esis (Fodor, 1975). Nor is the idea that cognition can be an-
two or three samples from the RRDNF posterior still fits the              alyzed by considering an optimally rational agent new: ideal
data of Nosofsky et al. (1994) well: R2 = 0.87. RULEX does                observers have been prominent in vision research (Geisler,
similarly well, R2 = 0.86, but again uses several additional              2003) and cognitive psychology (Anderson, 1990; Chater and
parameters. As with RULEX, the qualitative match of the                   Oaksford, 1999). However, the combination of these ideas
Rational Rules model to human judgments is quite good, as                 leads to an exciting, and neglected, project: rational analysis
show in Fig. 5.                                                           of the language of thought. We have shown in this paper that
                                                                    303

0.35
 0.3
                                                                                                                         Model
0.25                                                                                                                     Human
                                                                                                                                                                                                                                                                           Figure 5: Individual
                                                                                                                                                                                                                                                                           generalization pat-
 0.2
                                                                                                                                                                                                                                                                           terns: the portion
                                                                                                                                                                                                                                                                           of participants re-
0.15                                                                                                                                                                                                                                                                       sponding with the
                                                                                                                                                                                                                                                                           indicated categoriza-
 0.1                                                                                                                                                                                                                                                                       tions for the seven
                                                                                                                                                                                                                                                                           transfer stimuli of
                                                                                                                                                                                                                                                                           Fig. 2. Human data
0.05
                                                                                                                                                                                                                                                                           from Nosofsky et al.
                                                                                                                                                                                                                                                                           (1994), Experiment
  0                                                                                                                                                                                                                                                                        1. The model values
       AAAAAAA   AAAABAB
                           AAABAAB
                                     AAABBAB
                                               AAABBBA
                                                         AABBABA   AABBBBB
                                                                             ABAAAAB
                                                                                       ABABAAA
                                                                                                 ABABAAB
                                                                                                           ABABABB
                                                                                                                     ABABBAB
                                                                                                                               ABBABAB   ABBBAAB
                                                                                                                                                   ABBBBBA
                                                                                                                                                             BAABBAB
                                                                                                                                                                       BAABBBB
                                                                                                                                                                                 BBAAAAA
                                                                                                                                                                                           BBAAAAB
                                                                                                                                                                                                     BBAABBB   BBABAAA
                                                                                                                                                                                                                         BBABAAB
                                                                                                                                                                                                                                   BBABABB
                                                                                                                                                                                                                                             BBABBAB
                                                                                                                                                                                                                                                       BBABBBB   BBBAABB
                                                                                                                                                                                                                                                                           assume 2 samples for
                                                                                                                                                                                                                                                                           each simulated partic-
                           AAABABB             AAABBBB                       ABAABAB             ABABABA             ABABBBB                       BAAABAB             BABBBBB             BBAABAB                       BBABABA             BBABBBA                       ipant, b=4.
rigorous results are possible in this program, and that they can                                                                                                         Medin, D. L., Altom, M. W., Edelson, S. M., and Freko, D. (1982).
provide accurate models of basic cognitive processes.                                                                                                                      Correlated symptoms and simulated medical classification. Jour-
                                                                                                                                                                           nal of Experimental Psychology: Learning, Memory, and Cogni-
                                                                                                                                                                           tion, 8:37–50.
                                                         References                                                                                                      Medin, D. L. and Schaffer, M. M. (1978). Context theory of classi-
Anderson, J. R. (1990). The adaptive character of thought. Erlbaum,                                                                                                        fication learning. Psychological Review, 85:207–238.
  Hillsdale, NJ.                                                                                                                                                         Mervis, C. B. and Rosch, E. H. (1981). Categorization of natural
Chater and Oaksford (1999). Ten years of the rational analysis of                                                                                                          objects. Annual Review of Psychology, 32:89–115.
  cognition. Trends in Cognitive Science, 3(2):57–65.                                                                                                                    Navarro, D. J. (2006). From natural kinds to complex categories.
Chopin, N. (2002). A sequential particle filter method for static                                                                                                          In Proceedings of the Twenty-Eighth Annual Conference of the
  models. Biometrika, 89(3):539–552.                                                                                                                                       Cognitive Science Society.
Enderton, H. B. (1972). A mathematical introduction to logic. Aca-                                                                                                       Student Version
                                                                                                                                                                         Nosofsky,  R. M.ofand
                                                                                                                                                                                            MATLAB
                                                                                                                                                                                                Palmeri, T. J. (1998). A rule-plus-exception
  demic Press, New York.                                                                                                                                                   model for classifying objects in continuous-dimension spaces.
Feldman, J. (2000). Minimization of Boolean complexity in human                                                                                                            Psychonomic Bulletin & Review, 5:345–369.
  concept learning. Nature, 407:630–633.                                                                                                                                 Nosofsky, R. M., Palmeri, T. J., and McKinley, S. C. (1994). Rule-
Feldman, J. (2006). An algebra of human concept learning. Journal                                                                                                          plus-exception model of classification learning. Psychological
  of Mathematical Psychology, 50:339–368.                                                                                                                                  Review, 101(1):53–79.
Fodor, J. A. (1975). The language of thought. Harvard University                                                                                                         Posner, M. I. and Keele, S. W. (1968). On the genesis of abstract
  Press: Cambridge, MA.                                                                                                                                                    ideas. Journal of Experimental Psychology, 77(3):353–363.
Geisler, W. S. (2003). Ideal observer analysis. In Chalupa, L. and                                                                                                       Sanborn, A., Griffiths, T., and Navarro, D. (2006). A more rational
  Werner, J., editors, The Visual Neurosciences, pages 825–837.                                                                                                            model of categorization. In Proceedings of the Twenty-Eighth
  MIT press.                                                                                                                                                               Annual Conference of the Cognitive Science Society.
Griffiths, T. L. and Tenenbaum, J. B. (2005). Structure and strength                                                                                                     Smith, E. and Medin, D. (1981). Categories and Concepts. Cam-
  in causal induction. Cognitive Psychology, 51:285–386.                                                                                                                   bridge, MA: Harvard University Press.
Kruschke, J. K. (1992). ALCOVE: An exemplar-based connectionist                                                                                                          Tenenbaum, J. B. (1999). Bayesian modeling of human concept
  model of category learning. Psychological Review, 99(1):22–44.                                                                                                           learning. In Advances in Neural information Processing Systems
Lakoff, G. (1987). Women, fire, and dangerous things: What cat-                                                                                                            11.
  egories reveal about the mind. University of Chicago Press,                                                                                                            Xu, F. and Tenenbaum, J. B. (2005). Word learning as Bayesian
  Chicago.                                                                                                                                                                 inference: Evidence from preschoolers. In Proceedings of the
Love, B. C., Gureckis, T. M., and Medin, D. L. (2004). SUSTAIN:                                                                                                            Twenty-Seventh Annual Conference of the Cognitive Science So-
  A network model of category learning. Psychological Review,                                                                                                              ciety.
  111(2):309–332.
McKinley, S. C. and Nosofsky, R. M. (1993). Attention learning
  in models of classification. (Cited in Nosofsky, Palmeri, and
  McKinley, 1994).
                                                                                                                                                              304

