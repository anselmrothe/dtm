UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning the Structure of a Mathematical Group
Permalink
https://escholarship.org/uc/item/2c9702hk
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Jamrozki, Anna
Shultz, Thomas R.
Publication Date
2007-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                              Learning the Structure of a Mathematical Group
                                       Anna Jamrozik (anna.jamrozik@mail.mcgill.ca)
                                Department of Psychology, McGill University, 1205 Penfield Avenue
                                                     Montreal, QC H3A 1B1 Canada
                                         Thomas R. Shultz (thomas.shultz@mcgill.ca)
              Department of Psychology and School of Computer Science, McGill University, 1205 Penfield Avenue
                                                     Montreal, QC H3A 1B1 Canada
                             Abstract
   A mathematical group is a set of operations that satisfies the                            The Klein 4-group
   properties of identity, inverse, associativity, and closure.          A mathematical group is a set of operations that satisfies
   Understanding of group structure can be assessed by changing          four criteria: identity, inverse, associativity, and closure.
   elements and operations over different versions of the same           The Klein 4-group, named after German mathematician
   underlying group. Participants learned this structure more
                                                                         Felix Klein, is one of only two possible four-element groups
   quickly over four different, successive versions of a subset of
   the Klein 4-group, suggesting some understanding of the               (the other is the cyclic 4-group). The four elements of the
   group structure (Halford, Bain, Mayberry, & Andrews, 1998).           Klein 4-group are operations that, when combined, create
   Because an artificial neural network learning the task failed to      other operations that are also members of the group. Table 1
   improve, it was argued that such models are incapable of              shows how the Klein 4-group might be used to structure a
   learning abstract group structures (Phillips & Halford, 1997).        task like that used by Halford et al. (1998), which can be
   Here we show that an improved neural model that adheres               visualized as moving from one position to another in a two-
   more closely to the task used with humans does speed its
   learning over changing versions of the task, showing that             dimensional plane.
   neural networks are capable of learning and generalizing
   abstract structure.                                                               Table 1: Example of the Klein 4-group.
   Keywords: generalization; mathematical groups; Klein 4-                             Identity     Horizontal     Vertical    Diagonal
   group; systematicity; neural networks; sibling-descendant             *
   cascade-correlation.                                                  Identity      Identity     Horizontal     Vertical    Diagonal
                                                                         Horizontal   Horizontal      Identity    Diagonal     Vertical
                          Introduction
                                                                         Vertical      Vertical      Diagonal      Identity   Horizontal
Generalization allows application of what was learned in
one task to new tasks, thus distinguishing understanding                 Diagonal      Diagonal      Vertical     Horizontal    Identity
from mere memorization (Shultz, 2001). Phillips and
Halford (1997) questioned the ability of current feedforward
                                                                            The identity criterion is satisfied by an operation that does
neural networks to learn abstract structures and to generalize
                                                                         not change the operation it acts on. Notice that the first row
to related problems. Mathematical groups offer a
                                                                         and first column in Table 1 each use an identity operation
challenging context to test for such generalization.
                                                                         and thus preserve the effect of the other operation. For
Generalization is achieved if the structure underlying a
                                                                         example, a horizontal operation followed by an identity
group can be extracted and applied to new instances.
                                                                         operation is simply a horizontal operation. The inverse
   Human participants demonstrated some ability to
                                                                         criterion is satisfied when every operation has an inverse
extrapolate the structure underlying a series of four different
                                                                         that reverses the action of that operation. Inverse operations
versions of what was described as a Klein 4-group task
                                                                         are designated in the diagonal of Table 1, marked by cells
(Halford et al., 1998). Attempts to create a feedforward
                                                                         labeled identity. For example, a horizontal operation can be
network able to do the same were unsuccessful (Phillips &
                                                                         undone by a reverse horizontal operation, leaving the system
Halford, 1997). The modeling failure was cited approvingly
                                                                         in the same place it started.
by Marcus (1998) as part of a general argument against the
                                                                            The associative law of operations specifies that two
ability of neural networks to generalize outside of the
                                                                         operations applied in a certain order result in the same final
training set as well as humans do. Here, we identify some
                                                                         state even if their order is reversed. For example, a
serious shortcomings of that unsuccessful model and present
                                                                         horizontal operation followed by a vertical operation yields
a more appropriate neural-network model that exhibits
                                                                         the same result as does a vertical operation followed by a
generalization by learning the Klein 4-group structure more
                                                                         horizontal operation. Finally, closure states that the result of
quickly with each new version of the task, as Halford et al.’s
                                                                         any combination of operations can also be produced by a
participants did.
                                                                         different, single operation. For example, combining a
                                                                    1115

vertical operation with a horizontal operation is equivalent          Participants completed four versions of the task, each one
to using a diagonal operation.                                    containing a maximum of six repetitions of the eight
   The Klein 4-group, and foregoing examples, can be              possible triads. All four versions of the task shared the same
visualized as in Figure 1, with four states at the corners and    underlying structure but differed in the shapes and strings
the six edges representing the three non-identity operations.     used in the triads. Each triad was composed of an initial pair
                                                                  formed of one element and operation. Participants were
                       horizontal                                 asked to predict the element resulting from the
         A                                     B identity         transformation by selecting from a list of four possible final
                                                                  elements. Following each identification attempt, participants
         v                                     v                  were given feedback regarding the correct answer. After
         e                                     e                  identifying every pair correctly within a trial or after a
         r                                     r                  maximum of six trials, participants began the next version
         t        diagonal                     t                  of the task, with a different set of shapes and strings. By the
         i                                     i                  beginning of the fourth version of the task, participants’
         c                                     c                  error had decreased, suggesting that participants had
         a                                     a                  extracted something about the abstract relationship between
         l                                     l                  the elements and operations.
         D             horizontal
                                               C                        Previous Klein 4-group Network Models
                                                                   Phillips and Halford (1997) argued that neither a simple
                                                                   recurrent nor feedforward neural-network model could
  Figure 1: A graphical representation of the Klein 4-group.       capture the same degree of generalization found in humans.
                                                                   They claimed that this was because the Klein 4-group
    Human Extrapolation of Group Structure                         problem exhibited systematicity, which they defined as the
In a series of experiments, Halford et al. (1998)                  “property whereby cognitive capacities are grouped on the
demonstrated people’s ability to extrapolate an abstract           basis of common structure” (Phillips & Halford, 1997, p.
structure underlying a series of four tasks involving a            614). The basic idea is that the same structure underlies all
modified and simplified subset of the Klein 4-group.               four task versions and this structure can be learned and used
Curiously, the diagonal and identity operations, present in        to speed the learning of each successive version. They
the full Klein 4-group, were missing from this subset. The         argued that connectionist networks are context-specific, not
Halford et al. subset is pictured in Figure 2. This structure      structure-specific. Because the context of the problem
allows for eight possible triads, or element-operation-            changes from task to task, they expected that networks
element sequences (e.g., A horizontal D). Three-letter             would be unable to improve their performance across
pronounceable but meaningless strings of letters (e.g.,            different versions of the Klein 4-group task.
‘MIW’) were substituted for each of the four elements in the          Phillips and Halford (1997) created a fixed-architecture
group and simple shapes (e.g., ∆) were substituted for each        feedforward network to test this hypothesis: 6 input units
of the two operations. An example of a triad could be ‘MIW         leading to a layer of 3 hidden units, these leading to 2
∆ VOL’.                                                            hidden units, and finally to 4 output units. A localist coding
                                                                   scheme was used for both inputs and outputs. This is a
                                                                   binary scheme in which an input or output pattern contains
                                                                   one non-zero bit. For example, the four group elements
           D                                      A                could be represented as ‘1000’, ‘0100’, ‘0010’, and ‘0001’.
                         horizontal
                                                                   After the network successfully predicted each pattern in the
                v                            v
                                                                   first task, then one, two, or all three of the weights
                e                            e                     stemming from one input unit were reset, the network was
                r                            r                     retrained on all other patterns, and then tested on the pattern
                t                            t                     corresponding to the reset input unit. This was repeated for
                i                            i                     10 trials for each weight reset (1, 2, or 3). With one weight
                c                            c                     reset, the network was able to generalize to 7 of the 10 test
                a                            a                     trials. With three reset weights, the network was unable to
                l                            l                     generalize to the test input patterns.
                         horizontal
                                                                             Shortcomings of Previous Models
           C                                       B               Phillips and Halford (1997) did not test network
                                                                   generalization ability in the same way that humans were
   Figure 2: Halford et al.’s (1998) version of the Klein 4-       tested. Humans were tested for improvement in learning
                             group.                                speed over four tasks all sharing an underlying subset of the
                                                              1116

Klein 4-group structure. Phillips and Halford concluded that                                   Method
participants were generalizing the structure of the group and
using it to solve later tasks because they became better at         Coding the Task
learning to predict the correct final elements by the fourth
task. Their networks were tested in a different fashion. After      The task used in Phillips and Halford’s (1997) simulation
resetting the weights between one input unit and the hidden         was modified to more fairly test network learning ability
units, the network’s ability to find the solution to the            and to better correspond with the task used in the human
element represented by that input unit was tested. No new           experiments conducted by Halford et al. (1998).
versions of the task were used. The resetting of weights               Human participants in the Halford et al. (1998)
could be seen as erasing memory. The human experiment               experiment had to choose which of four elements best
involved learning new tasks by generalizing the structure of        completed an element-operation pair. The difference
earlier tasks, not as learning one task, having this knowledge      between elements and operations was stressed by coding
erased, and trying to learn a similar task.                         elements as three-letter strings and operations as shapes. In
   Using a local input/output representation is known to            addition, participants were given cards depicting each of the
hamper generalization, because different elements are               four elements in order to facilitate solving the task (Halford
represented on different units and weights. By using a              et al., 1998). We implemented the difference between
coding scheme known to limit generalization, feedforward            elements and operators by coding each element with a
networks’ ability to generalize the Klein 4-group structure         different integer between 5 and 16 and each operation with a
was compromised. In order to more fairly assess this ability,       different integer between 1 and 4.
a coding scheme that facilitates generalization should be              Participants in the Halford et al. (1998) experiment
used. Analog representation facilitates generalization by           completed four different versions of the Klein 4-group task,
representing different elements on the same units and               but networks in the Phillips and Halford (1997) simulation
weights and has allowed for generalization and other                only completed two versions. In our simulation, networks
psychological effects to emerge naturally within neural             completed four different versions of the task as humans did.
networks (Shultz, 2003).                                            Participants’ performance in the Halford et al. (1998) task
   The feedforward network used in Phillips and Halford’s           was measured by their error rate. Participants were found to
(1997) simulation was designed by hand, such that the               have a lower error rate by the beginning of the fourth
number of hidden units and their topology was fully                 version of the task than they had at the beginning of the first
determined before training began. This prevented the                version, suggesting that they were learning the task faster in
network from growing to meet the demands of the task.               later versions. To mimic this, we recorded networks’
People were under no such restrictions. With the alternative        learning speed on each of the four tasks.
of constructive learning, a learning algorithm can build its           As noted, we used an analog coding scheme to facilitate
own network topology to suit the problem (Shultz, 2006).            generalization rather than the localist coding scheme used in
The topology can start with minimal structure and new               Phillips and Halford’s (1997) simulation, which is known to
hidden units can be recruited as needed and placed in a             limit generalization.
network topology that learns to minimize network error.                Our networks were given an input of a triad, and asked to
   Finally, if the goal is to test human and network ability to     predict if the given triad was or was not a possible member
learn the abstract structure of the Klein-4-group, then the         of the Klein 4-group. There are 64 ways to combine one of
full Klein 4-group should be used, not an arbitrary subset          four possible initial elements, one of four possible
that is not technically a group.                                    operations, and one of four possible final elements. Of these
                                                                    64 possible combinations, 16 are valid members of the
SDCC                                                                Klein 4-group and 48 are not. All 64 triads (e.g., element 1,
                                                                    horizontal, element 2; element 1, horizontal, element 3)
In order to allow the networks in our simulation to construct
                                                                    were created and paired with the corresponding output (+0.5
their own topologies, the Sibling-descendant Cascade-
                                                                    if the triad was a valid example of the Klein 4-group; -0.5 if
correlation (SDCC) algorithm (Baluja & Fahlman, 1994)
                                                                    it was not). This approximated the limited choice given to
was employed. As in ordinary cascade-correlation (CC),
                                                                    participants in the Halford et al. (1998) experiment, who
SDCC allows networks to recruit hidden units as they are
                                                                    selected one of four possible elements to form a valid triad.
needed in order to minimize error. But rather than installing
                                                                       Each of the four elements was randomly designated with
each hidden unit on its own layer as in CC, a new unit is
                                                                    a different integer between 5 and 16 and each of the four
either installed on a new layer (descendant) or in the current
                                                                    operations was randomly designated with a different integer
highest layer (sibling), depending on which candidate’s
                                                                    between 1 and 4. There are 285,120 ways in which to pair
activation correlates best with network error. Networks
                                                                    elements and operations with that coding. Operations and
created with SDCC are shallower than CC networks but
                                                                    elements were chosen from different sets of numbers to
have a similar ability to learn and generalize (Baluja &
                                                                    mirror the distinction between operations and elements
Fahlman, 1994). CC and more recently SDCC have been
                                                                    found in the Halford et al. (1998) study, in which elements
used to successfully simulate many phenomena in human
                                                                    were designated with letter strings and operations were
learning and development (Shultz, 2003, 2006).                      designated with shapes.
                                                               1117

   By designating elements and operations with numbers,           needed in later training sets. Hidden units were arranged on
element-operation-element triads could be created and             a mean of 11 layers by the end of the fourth training set.
paired with the appropriate output for that triad. For               A one-way repeated measures ANOVA was conducted to
example, if the triad ‘element 1, horizontal, element 2’,         test whether a difference existed in the number of epochs
translated say to 12 2 7, is a possible member of the Klein 4-    needed to reach victory across the four successive training
group, this input would be paired with the output +.5. By         sets. This ANOVA revealed an overall difference between
pairing triads and outputs, a training set of 64 input-output     the means, F(3, 33) = 26, p < .001. There were both linear,
pairs, or training patterns, was created. Four sets of training   F(1, 11) = 112, p < .001, and quadratic, F(1, 11) = 11, p <
patterns were formed. A training epoch consisted of the           .01, trends in these data, reflecting increased learning speed
network processing each of the 64 training patterns.              across versions. Bonferroni–corrected pairwise comparisons
   Our network task thus approximated the task given to           revealed that the mean of first training set was greater than
human participants by Halford et al. (1998). Elements and         the mean of the third training set (p < .05), as for humans
operations were differentiated and networks were given a          (Halford et al., 1998). Additionally, the mean of the first
choice of answer. Networks were required to learn and thus        training set was also greater than the mean of the second
generalize over four different consecutive versions of the        training set (p < .05).
task, allowing us to measure learning speed. The fresh               An analogous ANOVA was conducted to test whether a
coding for each successive training set required as much          difference existed between the numbers of hidden units
extrapolation as did the human experiment. Networks were          recruited in successive training sets. This ANOVA also
allowed to build their own topology and weights were not          revealed an overall difference between the means, F(3, 33)
erased when moving to a new version of the task.                  = 39, p < .001. Again, there were linear, F(1, 11) = 244, p <
                                                                  .001, and quadratic, F(1, 11) = 19, p < .002, trends in these
Testing for Generalization                                        data, indicative of increased learning speed over successive
Twelve SDCC networks, corresponding to the 12                     training sets.
participants in the Halford et al. (1998) study, were trained                                   2000
on four successive training sets, representing each of the
four versions of the task. Networks varied from each other
                                                                       Mean epochs to learn
                                                                                                1500
because of unique random initializations of their weights,
and because of unique random codings of elements and                                            1000
operations.
   Networks were trained on the first training set until                                                       500
reaching victory. Victory was declared if all of the output
activations generated by the network for each training                                                                      0
pattern were within a certain threshold of the correct output.                                                                   1   2                        3       4
In this case, this threshold was 0.4, the standard value for                                                                                 Training set
sigmoid output units (Shultz, 2003). All other network
parameters were also left at their default values. Once the           Figure 3: Mean number of epochs needed to reach victory
network completed this first training set, the network began            during consecutive training sets, along with standard
training on the second set. At the start of this second phase,           deviation bars and linear and quadratic trend lines.
the weights created during the first phase remained intact.
This allowed the network to preserve the knowledge used to
reduce error during the first training set and then to continue                                                             16
learning from that point. The algorithm continued to recruit
                                                                                              Mean hidden units recruited
                                                                                                                            14
hidden units as needed. Once victory for this second training                                                               12
set was reached, a third training set was substituted, etc.                                                                 10
Upon completing four training sets, the network’s topology                                                                  8
and knowledge representations were examined.                                                                                6
   If the networks reached victory faster for later versions of                                                             4
the task than for earlier versions, then we would conclude                                                                  2
that networks had abstracted something of the underlying                                                                    0
                                                                                                                                 1       2                        3       4
structure of the task.
                                                                                                                                               Training set
                          Results                                       Figure 4: Mean number of hidden units added during
The mean numbers of epochs needed to learn the successive            consecutive training sets, along with standard deviation bars
training sets are pictured in Figure 3; and the mean numbers                     and linear and quadratic trend lines.
of hidden units recruited in successive training sets are
pictured in Figure 4. Fewer epochs and hidden units were               To understand the networks’ knowledge representations,
                                                                     the contributions of input and hidden units were analyzed
                                                              1118

using Principal Component Analysis (PCA). Network                                                                               another moderately complex problem, continuous exclusive-
contributions are the products of sending-unit activations                                                                      or (Shultz & Elman, 1994) before learning one training set
and connection weights entering into output units (Shultz &                                                                     of the Klein 4-group task. This comparison was used to
Elman, 1994). Although the full results cannot be elaborated                                                                    examine whether networks reached victory for later training
here because of space limitations, it was found that element                                                                    sets because of the increased complexity of the networks
input units loaded onto one principal component while the                                                                       learning later sets. The pre-trained networks had recruited
operation input unit loaded onto another principal                                                                              an average of 6.17 hidden units during pre-training. When
component. Component scores suggested that networks                                                                             training on the Klein 4-group task, the pre-trained networks
distinguished the different element and operation values.                                                                       did not reach victory faster than networks without pre-
   An example of the relationship between the four                                                                              training. These results suggest that mere network
operations and the component the operation unit loaded onto                                                                     complexity cannot account for the increase in performance
is pictured in Figure 5 for one representative network. Mean                                                                    across successive training sets.
scores on this component distinguished the four operations
from each other. An example of the relation between the                                                                                                 Discussion
four elements and the component representing those                                                                              It is evident that the networks abstracted something of the
elements is shown in Figure 6 for this same network. Both                                                                       structure underlying all four of the training tasks because
initial and final element values were distinguished from                                                                        they reached victory faster for later versions of the task than
each other by these component scores. Such representations                                                                      for earlier versions. This simulates human performance in
became somewhat more precise with each successive                                                                               the Halford et al. (1998) experiment, in which people were
version of the task.                                                                                                            faster in learning later than earlier tasks. The number of
                                                                                                                                trials needed by human participants and the number of
                                1.5                                                                                             epochs needed by networks to reach victory are not directly
                                                                                                                                comparable because there is no way of knowing how much
  Mean component score
                                                1
                                                                                                                                processing humans are doing on each trial. What is
                                0.5                                                                                             important is that both humans and networks exhibited faster
                                                0                                                                               learning in successive versions of the task.
                         -0.5                                                                                                      Knowledge-representation analyses consisting of PCAs of
                                                                                                                                network contributions revealed that the element input units
                                           -1
                                                                                                                                loaded onto one component and that the operation input unit
                         -1.5                                                                                                   loaded onto the other component. In addition, the specific
                                                          Vertical   Identity             Diagonal       Horizontal
                                                                                                                                elements and operations could be distinguished by variation
                                                                                Operation
                                                                                                                                in component scores. The fact that these representations
                                                                                                                                became more precise with each new version of the task
   Figure 5: Mean component-2 scores for network 6 after
                                                                                                                                supports the idea that networks were gradually abstracting
the last training set. The operation input unit loaded heavily
                                                                                                                                some of the structure of the Klein 4-group.
 onto component 2, which distinguished the four different
                                                                                                                                   These learning speed and knowledge-representation
                          operations.
                                                                                                                                results contradict the claims by Phillips and Halford (1997)
                                                                                                                                and Marcus (1998) about the inability of neural networks to
                                                     1                                                                          generalize abstract relations outside of the training set.
                                                                                                                                   As demonstrated by Browne (2002), the coding scheme
                         Mean component score
                                                    0.5
                                                                                                              Element
                                                                                                                                employed in training and testing networks plays a large role
                                                                                                                      Initial
                                                                                                                                in a network’s ability to generalize. The binary coding
                                                     0
                                                                                                                      Final     scheme used by Phillips and Halford (1997) prevented
                                                                                                                                successful network generalization, as did their erasing of
                                                -0.5
                                                                                                                                weights when moving from one phase to another. Our
                                                                                                                                changing the identity of elements and operations in each
                                                     -1
                                                              1       2               3              4
                                                                                                                                new training phase, while preserving the weights,
                                                                     Element numbers
                                                                                                                                corresponded more closely to the method of Halford et al.’s
                                                                                                                                human experiment. Plus, our use of analog representations
    Figure 6: Mean component-1 scores for network 6 after                                                                       allowed for generalization across the four different versions
the last training set. The initial and final element input units                                                                of the problem. Another factor in the current networks’
 loaded heavily onto component 1, which distinguished the                                                                       ability to generalize may have been the use of SDCC, which
                 four different element values.                                                                                 allowed networks to configure their own architecture, but a
                                                                                                                                direct test of this would require comparing the present
   The performance of the 12 Klein 4-group networks                                                                             results to static networks (Shultz, 2006).
training on the first training set was also compared to the                                                                        The possibility that the networks were not abstracting the
performance of 12 networks that had been pre-trained on                                                                         Klein 4-group structure but simply solving the task faster as
                                                                                                                           1119

their architectures became more complex was refuted by
comparing the performance of networks only learning the                                      References
Klein 4-group task and networks pre-trained on a                      Baluja, S. & Fahlman, S. E. (1994). Reducing network
continuous exclusive-or problem before learning the Klein               depth in the Cascade-Correlation learning architecture.
4-group task. Because the learning speed of the pre-trained             Tech Report CMU-CS-94-209, School of Computer
networks did not differ from networks without pre-existing              Science, Carnegie Mellon University.
hidden units, more complex architectures could not explain            Browne, A. (2002). Representation and extrapolation in
increases in learning speed.                                            multilayer perceptrons. Neural Computation, 14, 1739-
   The use of PCA to analyze knowledge representations in               1754.
our networks provided some understanding of how network              Halford, G. S., Bain, J. D., Maybery, M. T., & Andrews G.
knowledge was organized. The PCAs revealed that                         (1998). Induction of relational schemas: Common
networks distinguished between operations and elements                  processes in reasoning and complex learning. Cognitive
and between different values on each of these two                       Psychology, 35, 201-245.
dimensions. It would be interesting to determine if networks         Marcus, G. F. (1998). Rethinking eliminative
trained on other mathematical group tasks would develop a               connectionism. Cognitive Psychology, 37, 243–282.
similar pattern of loadings, perhaps suggesting similar              Phillips, S., & Halford, G. S. (1997). Systematicity:
solutions to the tasks.                                                 Psychological evidence with connectionist implications.
   Other future work might use the present simulations as               In M. G. Shafto & P. Langley (Eds.), Proceedings of the
predictions for a psychology experiment in which                        Nineteenth Annual Conference of the Cognitive Science
participants would be asked to classify example triads of               Society. Mahwah, NJ: Erlbaum.
two elements and a transforming operation as instances or             Shultz, T. R. (2001). Assessing generalization in
non-instances of the full Klein 4-group.                                connectionist and rule-based models under the learning
   A better experimental control for network complexity                 constraint. In J. Moore & K. Stenning (Eds.), Proceedings
than the continuous exclusive-or task used here might                   of the Twenty-third Annual Conference of the Cognitive
involve randomizing the weights of an SDCC network that                 Science Society. Mahwah, NJ: Erlbaum.
was trained on a version of the Klein 4-group. Such a                Shultz, T. R. (2003). Computational developmental
randomized network would be exactly as complex in                       psychology. Cambridge, MA: MIT Press.
topology as the network from which it was derived but lack           Shultz, T. R. (2006). Constructive learning in the modeling
any content knowledge of the Klein 4-group.                             of psychological development. In Y. Munakata & M. H.
   Although Phillips and Halford (1997) claimed that the                Johnson (Eds.), Processes of change in brain and
Klein 4-group task is one of systematicity, this systematic             cognitive development: Attention and performance XXI.
structure may only appear at a level of abstraction achieved            Oxford: Oxford University Press.
by rather skilled mathematicians. Shultz and Bale (2001,             Shultz, T. R., & Bale, A. C. (2001). Neural network
2006) presented similar examples of CC networks                         simulation of infant familiarization to artificial sentences:
exhibiting rule-like behavior without directly representing             Rule-like behavior without explicit rules and variables.
or manipulating symbolic rules. Symbolic rules can be seen              Infancy, 2, 501-536.
as emerging from the subsymbolic processing of neural                Shultz, T. R., & Bale, A. C. (2006). Neural networks
networks (Smolensky, 1988). In an analogous fashion, our                discover a near-identity relation to distinguish simple
networks learned something about the structure of the Klein             syntactic forms. Minds and Machines, 16, 107-139.
4-group without actually representing and processing the             Shultz, T. R., & Elman, J. L. (1994). Analyzing cross
explicit rules of identity, inverse, associativity, and closure.        connected networks. In J. D. Cowan, G. Tesauro, & J.
                                                                        Alspector (Eds.), Advances in Neural Information
                    Acknowledgements                                    Processing Systems 6. San Francisco, CA: Morgan
This research was supported by a grant from the Natural                 Kaufmannn.
Sciences and Engineering Research Council of Canada to               Smolensky, P. (1988). On the proper treatment of
the second author. We are grateful to Dirk Schlimm for                  connectionism. Behavioral and Brain Sciences, 11, 1-74.
helpful comments on an earlier draft.
                                                                 1120

