UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Unifying Rational Models of Categorization via the Hierarchical Dirichlet Process
Permalink
https://escholarship.org/uc/item/5p6505mf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Griffiths, Thomas L.
Canini, Kevin R.
Sanborn, Adam N.
et al.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                      University of California

 Unifying Rational Models of Categorization via the Hierarchical Dirichlet Process
                                          Thomas L. Griffiths (tom griffiths@berkeley.edu)
                    Department of Psychology, University of California, Berkeley, Berkeley, CA 94720-1650 USA
                                                Kevin R. Canini (kevin@cs.berkeley.edu)
               Department of Computer Science, University of California, Berkeley, Berkeley, CA 94720-1776 USA
                                              Adam N. Sanborn (asanborn@indiana.edu)
                Department of Psychological and Brain Sciences, Indiana University, Bloomington, IN 47405, USA
                                        Daniel J. Navarro (daniel.navarro@adelaide.edu.au)
                                School of Psychology, University of Adelaide, Adelaide SA 5005, Australia
                                Abstract                                   gory learning leave a number of questions open. In particular,
                                                                           many categorization experiments have explored whether peo-
   Models of categorization make different representational as-
   sumptions, with categories being represented by prototypes,             ple represent categories with exemplars or prototypes. One
   sets of exemplars, and everything in between. Rational mod-             desideratum for a rational account of category learning might
   els of categorization justify these representational assumptions        be that it can indicate when a learner should choose to use one
   in terms of different schemes for estimating probability distri-
   butions. However, they do not answer the question of which              of these forms of representation over the other. The greater
   scheme should be used in representing a given category. We              flexibility of nonparametric density estimation has motivated
   show that existing rational models of categorization are spe-           the claim that exemplar models are to be preferred as ratio-
   cial cases of a statistical model called the hierarchical Dirichlet
   process, which can be used to automatically infer a represen-           nal models of category learning (Nosofsky, 1998). However,
   tation of the appropriate complexity for a given category.              nonparametric and parametric methods have different advan-
   Keywords: rational analysis, categorization, Dirichlet process          tages and disadvantages: the greater flexibility of nonpara-
                                                                           metric methods comes at a cost of requiring more data to es-
   Rational models of cognition aim to explain human be-                   timate a distribution. The decision as to which representation
havior as an optimal solution to the computational problems                scheme to use should be determined by the stimuli presented
posed by our environment (Anderson, 1990). Examining                       to the learner, and existing rational analyses do not indicate
these computational problems provides a deeper understand-                 how this decision should be made (although a similar argu-
ing of the assumptions behind successful models of human                   ment is made by Briscoe & Feldman, 2006).
cognition, and can lead to new models. In this paper, we
pursue a rational analysis of category learning: inferring the                The question of how to represent categories is complicated
structure of categories from a set of stimuli labeled as be-               by the fact that prototype and exemplar models are not the
longing to those categories. The knowledge acquired through                only options. A number of models have recently explored
this process can ultimately be used to make decisions about                possibilities between these extremes, representing categories
how to categorize new stimuli. Existing rational analyses of               using clusters of several exemplars (Anderson, 1990; Van-
category learning (Anderson, 1990; Ashby & Alfonso-Reese,                  paemel, Storms, & Ons, 2005; Rosseel, 2002; Love, Medin,
1995; Rosseel, 2002) agree that the computational problem                  & Gureckis, 2004). The range of representations possible
involved is one of density estimation: determining the proba-              in these models emphasizes the importance of being able to
bility distributions over stimuli associated with different cat-           identify an appropriate representation for a category from the
egory labels.                                                              stimuli themselves: with more options for the representation
   Viewing category learning as density estimation helps to                of categories, it becomes more important to be able to say
clarify the assumptions behind the two main classes of psy-                which option a learner should choose.
chological models: exemplar models and prototype models.                      Our goal in this paper is to build on previous rational analy-
Exemplar models assume that a category is represented by a                 ses of category learning to provide not just a unifying frame-
set of stored exemplars, and categorization involves compar-               work which can be used to understand the assumptions be-
ing new stimuli to the set of exemplars in each category (e.g.,            hind existing models of categorization, but a unifying model
Medin & Schaffer, 1978; Nosofsky, 1986). Prototype models                  of which these models are special cases. This model goes
assume that a category is associated with a single prototype               beyond previous unifying models of category learning (e.g.,
and categorization involves comparing new stimuli to these                 Rosseel, 2002; Vanpaemel et al., 2005) by providing a ratio-
prototypes (e.g., Reed, 1972). These approaches to category                nal solution to the question of which representation should
learning correspond to different strategies for density estima-            be chosen based purely on the structure of a category. These
tion, being nonparametric and parametric density estimation                results are achieved by identifying connections between mod-
respectively (Ashby & Alfonso-Reese, 1995).                                els of human category learning and ideas from nonparametric
   Despite providing insight into the assumptions behind                   Bayesian statistics. In particular, we show that all of the mod-
models of categorization, existing rational analyses of cate-              els mentioned above can be viewed as variants of a stochastic
                                                                       323

process called the hierarchical Dirichlet process (Teh, Jor-               category j is defined to be
dan, Beal, & Blei, 2004).
   Identifying the connection between models of human cate-                                           ηN, j = ηN,p j                    (3)
gory learning and nonparametric Bayesian density estimation
extends the scope of the rational analysis of category learning.           where p j is the prototypical instance of the category and ηN,p j
It also provides a different perspective on human category                 is a measure of the similarity between stimulus N and the
learning. Rather than suggesting that people use one form                  prototype p j , as used in the exemplar model.
of representation or another, our approach indicates how it                   Realizing that these two models are opposite ends of a
might be possible (and, in fact, desirable) for people to switch           spectrum, Vanpaemel et al. (2005) observed that we can for-
between representations based upon the structure of the stim-              malize a set of interpolating models by allowing the instances
uli they observe, choosing the representation best justified by            of each category to be partitioned into clusters, where the
the available data. We illustrate this by modeling data from               number of clusters Kc ranges from 1 to Nc . Then each cluster
Smith and Minda (1998), in which people seem to shift from                 is represented by a prototype, and the similarity of a stimulus
using a prototype representation early in training to using an             N to category j is defined to be
exemplar representation late in training.                                                                    Kj
   The plan of the paper is as follows. The next section sum-
marizes exemplar and prototype models, and the idea of in-
                                                                                                   ηN, j =  ∑ ηN,p j,k                  (4)
                                                                                                            k=1
terpolating between the two. We then discuss existing ratio-
nal models of categorization. This raises the question of how              where p j,k is the prototype of cluster k in category j. When
the models might be unified, which we address by turning to                Kc = 1 for all c, this is equivalent to the prototype model,
some ideas from nonparametric Bayesian statistics. Having                  and when K = Nc for all c, this is equivalent to the exemplar
established these ideas, we define a unifying rational model               model. Thus, this generalized model, the Varying Abstraction
of categorization based on the hierarchical Dirichlet process,             Model (VAM), is more flexible than both the exemplar and
and show that this model can capture the shift from prototypes             prototype models, although it raises the problem of estimat-
to exemplars in the data of Smith and Minda (1998).                        ing which clustering people are actually using in a particular
                                                                           categorization task (for details, see Vanpaemel et al., 2005).
                Exemplars and prototypes
                                                                                     Rational models of categorization
Exemplar and prototype models were originally developed
as accounts of the cognitive processes involved in catego-                 Following the methodology outlined by Anderson (1990), ra-
rization, incorporating different assumptions about how cat-               tional models of categorization explain human behavior in
egories are represented and how this information is used.                  terms of adaptive solutions to a computational problem posed
These models share the basic assumption that people assign                 by the environment rather than the underlying cognitive pro-
stimuli to categories based on similarity. Given a set of N − 1            cesses. Existing analyses tend to agree that the basic prob-
stimuli with features xN−1 = (x1 , x2 , . . . , xN−1 ) and category        lem is one of prediction – identifying the category label or
labels cN−1 = (c1 , c2 , . . . , cN−1 ), the probability that stimulus     some other unobserved property of an object using its ob-
N with features xN is assigned to category j is given by                   served properties (Anderson, 1990; Ashby & Alfonso-Reese,
                                                                           1995; Rosseel, 2002). Focusing for the moment on the case
                                                   ηN, j β j               of predicting category labels, as in most categorization ex-
             P(cN = j|xN , xN−1 , cN−1 ) =                         (1)     periments, the problem can be formulated as one of Bayesian
                                                  ∑c ηN,c βc
                                                                           inference: computing the probability that object N belongs
where ηN,c is the similarity of the stimulus xN to category c              to category j given the features and category labels of N − 1
and βc is the response bias for category c. The key difference             objects. Applying Bayes’ rule, we can write
between the models is in how ηN,c is computed.
   In an exemplar model (e.g., Medin & Schaffer, 1978;                           P(cN = j|xN , xN−1 , cN−1 ) =                          (5)
Nosofsky, 1986), a category is represented by all of the stored                         P(xN |cN = j, xN−1 , cN−1 )P(cN = j|cN−1 )
instances of that category. The similarity of stimulus N to cat-                      ∑c P(xN |cN = c, xN−1 , cN−1 )P(cN = c|cN−1 )
egory j is calculated by summing the similarity of the stimu-
lus to all stored instances of the category. That is,                      with the posterior probability of category j being proportional
                                                                           to the product of the probability of an object with features xN
                          ηN, j =     ∑      ηN,i                  (2)     being produced from that category and the prior probability
                                                                           of choosing that category, taking into account the features and
                                    i|ci = j
                                                                           labels of the previous N − 1 objects (assuming that only cate-
where ηN,i is a symmetric measure of the similarity between                gory labels influence the prior). Category learning, then, be-
the two stimuli xN and xi . In a prototype model (e.g., Reed,              comes a matter of determining these probabilities – a problem
1972), a category j is represented by a single prototypical                that is known as density estimation. Different rational models
instance. In this formulation, the similarity of a stimulus N to           vary in how they approach this problem.
                                                                       324

Exemplar and prototype models                                            generating a new object from cluster k in category j. The
Ashby and Alfonso-Reese (1995) observed a connection be-                 clusters can either be shared between categories, or specific
tween the Bayesian solution to the problem of categoriza-                to a single category (in which case P(zN = k|zN−1 , cN = j) is
tion presented in Equation 5 and the way that choice prob-               0 for all clusters not belonging to category j). It is straight-
abilities are computed in exemplar and prototype models                  forward to show that this reduces to kernel density estima-
(i.e. Equation 1). Specifically, ηN, j can be identified with            tion when each object has its own cluster and the clusters
P(xN |cN = j, xN−1 , cN−1 ), while β j corresponds to the prior          are equally weighted, and parametric density estimation when
probability of category j, P(cN = j|cN−1 ). The difference be-           each category is represented by a single cluster. By a similar
tween exemplar and prototype models thus comes down to                   argument to that used for the exemplar model above, we can
different ways of estimating P(xN |cN = j, xN−1 , cN−1 ).                connect Equation 7 with the definition of ηN, j in the VAM
   The definition of ηN, j used in an exemplar model (Equation           (Equation 4), providing a rational justification for this method
2) corresponds to estimating P(xN |cn = j, xN−1 , cN−1 ) as the          of interpolating between exemplars and prototypes.
sum of a set of functions (known as “kernels”) centered on
                                                                         Anderson’s Rational Model of Categorization
the xi already labeled as belonging to category j, with
                                                                         The MMC elegantly resolves the question of how to define a
            P(xN |cN = j, xN−1 , cN−1 ) ∝    ∑      f (xN , xi ) (6)     rational model between exemplars and prototypes, but leaves
                                           i|ci = j                      open the issue of determining how many clusters are used in
                                                                         representing each category – a question about which of these
where f (x, xi ) is a probability distribution centered on xi . This
                                                                         kinds of representations might be more appropriate based on
is a method that is widely used for approximating distribu-
                                                                         the available data. Anderson (1990) introduced a model that
tions in statistics, being a simple form of nonparametric den-
                                                                         he called the Rational Model of Categorization (RMC), which
sity estimation (meaning that it can be used to identify distri-
                                                                         presents a partial solution to this problem.
butions without assuming that they come from an underlying
                                                                            The RMC differs from the other models discussed in this
parametric family) called kernel density estimation.
                                                                         section in assuming that category labels should be treated like
   The definition of ηN, j used in a prototype model (Equa-
                                                                         features. Thus, the RMC specifies a joint distribution on fea-
tion 3) corresponds to estimating P(xN |cn = j, xN−1 , cN−1 ) by
                                                                         tures and category labels, rather than assuming that the dis-
assuming that the distribution associated with each category
                                                                         tribution on category labels is estimated separately and then
comes from an underlying parametric family, and then find-
                                                                         combined with a distribution on features for each category.
ing the parameters that best characterize the instances labeled
                                                                         As in the MMC, this distribution is a mixture, with
as belonging to that category. The prototype corresponds to
                                                                                         P(xN , cN ) = ∑ P(xN , cN |zN )P(zN )
these parameters. Again, this is a common method for esti-
                                                                                                                                              (8)
mating a probability distribution, known as parametric den-                                             zN
sity estimation, in which the distribution is assumed to be of
a known form but with unknown parameters.                                where P(zN ) is a distribution over clusterings of the N ob-
                                                                         jects. The key difference from the MMC is that this distribu-
The Mixture Model of Categorization                                      tion allows the number of clusters to be unbounded, with
The interpretation of exemplar and prototype models as dif-
ferent schemes for density estimation suggests that a sim-                                               αK         K
ilar interpretation might be found for interpolating mod-                               P(zN ) =
                                                                                                   ∏i=0
                                                                                                      N−1          ∏
                                                                                                           [α + i] k=1
                                                                                                                      (Mk − 1)!               (9)
els. Rosseel (2002) proposed one such model – the Mixture
Model of Categorization (MMC) – in which it is assumed that              where α is a parameter of the distribution and Mk is the num-
P(xN |cN = j, xN−1 , cN−1 ) is a mixture distribution. Specif-           ber of objects assigned to cluster k.1 This is the distribu-
ically, the model assumes that each object xi comes from a               tion that results from sequentially assigning objects to clus-
cluster zi , and each cluster is associated with a probability           ters with probability
distribution over the features of the objects generated from
that cluster. When evaluating the probability of a new object                                         Mk
                                                                                                
                                                                                                    i−1+α     Mk > 0 (i.e., k is old)
xN , it is necessary to sum over all of the clusters from which             P(zi = k|zi−1 ) =         α                                     (10)
                                                                                                    i−1+α     Mk = 0 (i.e., k is new)
that object might have been drawn, with
                                                                         where the counts Mk are accumulated over zi−1 . Thus, each
P(xN |cN = j, xN−1 , cN−1 ) =                                    (7)     object can be assigned to an existing cluster with probability
    Kj
                                                                         proportional to the number of objects already assigned to that
    ∑ P(xN |zN = k, xN−1 , zN−1 )P(zN = k|zN−1 , cN = j, cN−1 )          cluster, or to a new cluster with probability determined by α.
   k=1
                                                                             1 Due to space constraints, we have defined this distribution in the
where K j is the total number of clusters for category j,
                                                                         form associated with the Dirichlet process, rather than using the idea
P(xN |zN = k, xN−1 , zN−1 ) is the probability of xN under clus-         of a “coupling probability” from Anderson’s (1990) treatment (see
ter k, and P(zN = k|zN−1 , cN = j, cN−1 ) is the probability of          Neal, 1998, and Sanborn, Griffiths, & Navarro, 2006, for details).
                                                                     325

                                                                                                                γ ∈ (0, ∞)                    γ→∞
   Despite having been defined in terms of the joint distri-                                             categories share clusters categories share no clusters
bution of xN and cN , the assumption that features and cate-                          α→0
                                                                                                                 HDP0,+
                                                                                                                                            HDP0,∞
                                                                             one cluster per category                                     (prototype)
gory labels are independent given clusters makes it possible
                                                                                    α ∈ (0, ∞)
to write P(xN |cN = j, xN−1 , cN−1 ) in the same form as Equa-           intermediate number of clusters
                                                                                                                HDP+,+                     HDP+,∞
tion 7. The probability of cluster k is simply                                       α→∞                         HDP∞,+                    HDP∞,∞
                                                                             one stimulus per cluster            (RMC)                    (exemplar)
      P(zN = k|zN−1 , cN = j, cN−1 ) ∝                       (11)
           P(cN = j|zN = k, zN−1 , cN−1 )P(zN = k|zN−1 )               Figure 1: Unifying rational models of categorization. Each
                                                                       model is specified as HDPα,γ , where + is a value in (0, ∞).
where the second term on the right hand side is given by
Equation 10. This defines a distribution over the same K
clusters regardless of j, but the value of K depends on the            to all of the clusters in its group, with the prior probability of
number of clusters in zN−1 . The RMC can thus be viewed as             each cluster determined by Equation 10. If the observation is
a form of the mixture model in which all clusters are shared           to be assigned to a new cluster, the new cluster is drawn from
between categories but the number of clusters is inferred from         a second Dirichlet process that compares the stimulus to all
the data. However, the two models are not directly equivalent,         of the clusters that have been created across groups. This
because assuming that features and category labels are gen-            Dirichlet process is governed by parameter γ, analogous to α,
erated based on the clustering induces a dependency between            and the prior probability of each cluster is proportional to the
the two, meaning that cN depends on xN−1 as well as cN−1 , vi-         number of times that cluster has been selected by any group,
olating the (arguably sensible) assumption made by the other           instead of the number of observations in each cluster. The
models and embodied in Equation 5.                                     new observation is only assigned a completely new cluster if
   The RMC thus comes close to our goal of specifying a uni-           both Dirichlet processes select a new cluster.
fying rational model of categorization, capturing many of the              The HDP provides a way to model probability distributions
ideas embodied in other models and making it possible to               across groups of observations. Each distribution is a mixture
infer a representation warranted by the data. However, the             of an unbounded number of clusters, but the clusters can be
model is still significantly limited. First, the analysis given        shared between groups. Furthermore, the number of clus-
in the previous paragraph shows that the model assumes that            ters in each group can vary independently. A priori expecta-
every category is represented using the same set of clusters           tions about the number of clusters in a group and the extent
(and thus the same number), an assumption that is inconsis-            to which clusters are shared between groups are determined
tent with many models that interpolate between prototypes              by the parameters α and γ. When α is small, each group will
and exemplars (e.g., Vanpaemel et al., 2005). Second, the              have few clusters, but when α is large, the number of clus-
idea that category labels should be treated like other features        ters will be closer to the number of observations. When γ is
has some odd implications, such as the dependency between              small, groups are likely to share clusters, but when γ is large,
features and category labels mentioned above. These lim-               the clusters in each group are likely to be unique.
itations leave room for a model in which each category is
directly represented by a different number of clusters, with                                  A unifying rational model
the appropriate number being inferred from the data. We de-            We can now define a unifying rational model of categoriza-
velop and test such a model in the remainder of the paper,             tion, based on the HDP. If we identify each category with
by drawing on connections between the RMC and work in                  a “group” for which we want to estimate a distribution, the
nonparametric Bayesian statistics.                                     HDP instantly becomes a model of category learning, provid-
                                                                       ing us with a way to formulate models in which the number
           Dirichlet processes and beyond                              of clusters in each category is learned, and subsuming all pre-
The RMC defines a probability distribution as a mixture of             vious rational models through different settings of α and γ.
an unbounded number of clusters. The same idea appears in              Figure 1 identifies six models we can obtain by considering
nonparametric Bayesian statistics, in the form of the Dirichlet        limiting values of α and γ.2
process mixture model (Antoniak, 1974; Neal, 1998). In fact,               Three of the models shown in Figure 1 are exactly isomor-
the distribution defined by the RMC is exactly the same as             phic to existing models. HDP∞,∞ is an exemplar model, with
that defined by this model (Neal, 1998; Sanborn et al., 2006).         one cluster per object and no sharing of clusters. HDP0,∞ is a
This equivalence means that we can use recent results gener-           prototype model, with one cluster per category and no shar-
alizing the Dirichlet process to identify a richer class of ratio-     ing of clusters. HDP∞,+ is the RMC, provided that category
nal models of categorization.                                          labels are treated as features. In HDP∞,+ , every object has its
   Teh, Jordan, Blei, and Beal (2004) introduced a general-            own cluster, but those clusters are generated from the higher-
ization of the Dirichlet process known as the hierarchical             level Dirichlet process. Consequently, group membership is
Dirichlet process (HDP). The basic idea is simple. Observa-                 2 The case of γ → 0 is omitted, since it simply corresponds to a
tions are divided into groups, and each group is modeled us-           model in which all observations belong to the same cluster across all
ing a Dirichlet process. A new observation is first compared           categories, for all values of α.
                                                                   326

ignored and the model reduces to a Dirichlet process.                given its cluster, meaning that we can write
   There are also several new models. HDP0,+ makes the
                                                                       P(xN |zN = k, xN−1 , zN−1 ) = ∏ P(xN,d |zN = k, xN−1 , zN−1 )
same basic assumptions as the prototype model, with a single                                          d
cluster per category, but makes it possible for different cate-
gories to share the same prototype – something that might be         where xN,d is the value of the dth feature of object N. Given
appropriate in an environment where the same category can            the cluster, the value on each dimension is assumed to have
have different labels. However, the most interesting models          a Bernoulli distribution (although other distributions can be
are HDP+,+ and HDP+,∞ . These models are essentially the             used for continuous features). Integrating out the parameter
MMC, with clusters shared between categories or unique to            of this distribution with respect to a Beta(µ0 , µ1 ) prior, we
different categories respectively, but the number of clusters in     obtain
each category can differ and be learned from the data. Conse-                                                    Mk,v + µv
quently, these models make it possible to answer the question              P(xN,d = v|zN = k, xN−1 , zN−1 ) =                     (12)
                                                                                                               Mk + µ0 + µ1
of whether a particular category is best represented using pro-
totypes, exemplars, or something in between, simply based on         where Mk,v is the number of stimuli with value v on the dth
the structure of that category. In the remainder of the paper,       feature that zN identifies as belonging to cluster k.
we show that one of these models – HDP+,∞ – can capture the             All three models were exposed to the same training stimuli
shift that occurs from prototypes to a more exemplar-based           as the human participants, and used to categorize each stimu-
representation in a recent categorization experiment.                lus after each segment of 4 blocks. The cluster structures for
                                                                     the prototype and exemplar models are fixed, so the probabil-
   Modeling the prototype-exemplar transition                        ity of each category is straightforward to compute. However,
                                                                     since HDP+,∞ allows arbitrary clusterings, the possible clus-
Smith and Minda (1998) argued that people seem to produce            terings need to be summed over when computing the proba-
responses that are more consistent with a prototype model            bilities used in categorization (as in Equation 7). We approx-
early in learning, later shifting to exemplar-based represen-        imated this sum by sampling from the posterior distribution
tations. The models discussed in the previous section poten-         on clusterings using the Markov chain Monte Carlo (MCMC)
tially provide a rational explanation for this effect: the prior     algorithm described by Teh et al. (2004). Each set of predic-
specified in Equation 9 prefers fewer clusters and is unlikely       tions is based on an MCMC simulation with a burn-in of 1000
to be overwhelmed by small amounts of data to the contrary,          steps, followed by 100 samples separated by 10 steps each.
but as the number of objects consistent with multiple clusters       The parameter α was also estimated by sampling.
increases, the representation should shift. These results thus          As in Smith and Minda’s original modeling of this data,
provide an opportunity to compare the HDP to human data.             a guessing parameter was incorporated to allow for the pos-
                                                                     sibility that participants were randomly responding for some
   We focused on the non-linearly separable structure ex-
                                                                     proportion of the stimuli. The guessing parameter was fit for
plored in Experiment 2 of Smith and Minda (1998). In this
                                                                     each participant, being fixed across every instance of every
experiment, 16 participants were presented with six-letter
                                                                     stimulus for that participant. The values of µ0 and µ1 were
nonsense words labeled as belonging to different categories.
                                                                     also fit for each participant, with the restriction that µ0 = µ1 ,
Each letter could take one of two values, producing the bi-
                                                                     resulting in two free parameters for each of the models.
nary feature representation shown in Table 1. Each category
                                                                        The predictions of the three models are shown in Figure 2,
contains one prototypical stimulus (000000 or 111111), five
                                                                     averaged across participants, and model fits appear in Figure
stimuli with five features in common with the prototype, and
                                                                     3. As might be expected, the prototype model does poorly
one stimulus with only one feature in common with the pro-
                                                                     in predicting the categories of the exceptions, while the ex-
totype, which we will refer to as an “exception”. No linear
                                                                     emplar model is more capable of handling these stimuli. We
function of the features can correctly classify every stimulus,
                                                                     replicated the results of Smith and Minda (1998) in finding
meaning that a prototype model will not be able to distinguish
                                                                     that the prototype model fit better early in training (for seg-
between the categories exactly. Participants were presented
                                                                     ments 1-4), and the exemplar model better later in training.
with a random permutation of the 14 stimuli and asked to
                                                                     However, we also found that HDP+,∞ provided an equiva-
identify each as belonging to either Category A or Category
                                                                     lent or better account of human performance than the other
B, receiving feedback after each stimulus. This block of 14
                                                                     two models from segment 4 onwards. In particular, only this
stimuli was repeated 40 times for each participant, and the re-
                                                                     model captured the shift in the treatment of the exceptions
sponses were aggregated into 10 segments of 4 blocks each.
The results are shown in Figure 2 (a). The exceptions were
initially identified as belonging to the wrong category, with         Table 1: Categories A and B from Smith and Minda (1998)
performance improving later in training.
   We tested three models: the exemplar model HDP∞,∞ , the                                           Stimuli
                                                                       A    000000, 100000, 010000, 001000, 000010, 000001, 111101
prototype model HDP0,∞ , and HDP+,∞ . In all three models,             B    111111, 011111, 101111, 110111, 111011, 111110, 000100
we assumed that the features of each object are independent
                                                                 327

                                (a)                          1                                  (b)       1                                       (c)    1                            (d)    1
                                                            0.8                                       0.8                                               0.8                                 0.8
                                Probability of Category A
                                                            0.6                                       0.6                                               0.6                                 0.6
                                                            0.4                                       0.4                                               0.4                                 0.4
                                                            0.2                                       0.2                                               0.2                                 0.2
                                                             0                                            0                                              0                                   0
                                                                  0          5          10                    0          5                   10               0      5           10               0      5           10
                                                                          Segment                                     Segment                                     Segment                             Segment
Figure 2: Human data and model predictions. (a) Results of Smith & Minda (1998, Experiment 2). (b) Prototype model,
HDP∞,0 . (c) Exemplar model, HDP∞,∞ . (d) HDP+,∞ . For all panels, white plot markers are stimuli in Category A, and black are
in Category B. Triangular markers correspond to the exceptions to the prototype structure (111101 and 000100 respectively).
                                       −250
                                                                                                                                                    across all contexts, but instead select a representation whose
                                                                                                                                                    complexity is warranted by the available data.
                                       −300
                                       −350
                                                                                                                                                                              References
                                                                                                                                                    Anderson, J. R. (1990). The adaptive character of thought. Hills-
 Log likelihood of human data
                                       −400
                                                                                                                                                           dale, NJ: Erlbaum.
                                                                                                                                                    Antoniak, C. (1974). Mixtures of Dirichlet processes with appli-
                                                                                                                                                           cations to Bayesian nonparametric problems. The Annals of
                                       −450
                                                                                                                                                           Statistics, 2, 1152-1174.
                                                                                                                                                    Ashby, F. G., & Alfonso-Reese, L. A. (1995). Categorization as
                                       −500                                                                                                                probability density estimation. Journal of Mathematical Psy-
                                                                                                                                                           chology, 39, 216-233.
                                       −550
                                                                                                                                                    Briscoe, E., & Feldman, J. (2006). Conceptual complexity and the
                                                                                                                                                           bias-variance tradeoff. In Proceedings of the 28th Annual
                                                                                                                                                           Conference of the Cognitive Science Society. Mahwah, NJ:
                                       −600                                                                                 prototype
                                                                                                                            exemplar
                                                                                                                                                           Erlbaum.
                                                                                                                            HDP+,∞                  Love, B. C., Medin, D. L., & Gureckis, T. M. (2004). SUSTAIN: A
                                       −650
                                                             1        2    3        4   5             6           7   8     9           10
                                                                                                                                                           network model of category learning. Psychological Review,
                                                                                            Segment                                                        111, 309-332.
                                                                                                                                                    Medin, D. L., & Schaffer, M. M. (1978). Context theory of classifi-
Figure 3: Model fits, measured as log-likelihood of the human                                                                                              cation learning. Psychological Review, 85, 207-238.
responses under each model, for each segment of training.                                                                                           Neal, R. M. (1998). Markov chain sampling methods for Dirichlet
                                                                                                                                                           process mixture models (Tech. Rep. No. 9815). Department
                                                                                                                                                           of Statistics, University of Toronto.
                                                                                                                                                    Nosofsky, R. M. (1986). Attention, similarity, and the identification-
over training. This shift occurred because the number of clus-                                                                                             categorization relationship. Journal of Experimental Psy-
ters in the HDP changes around segment 4: categories are ini-                                                                                              chology: General, 115, 39-57.
tially represented with one cluster, but then become a more                                                                                         Nosofsky, R. M. (1998). Optimal performance and exemplar models
                                                                                                                                                           of classification. In M. Oaksford & N. Chater (Eds.), Rational
complex two cluster representation, with one for the stimuli                                                                                               models of cognition (p. 218-247). Oxford: Oxford University
close to the prototype and one for the exception.                                                                                                          Press.
                                                                                                                                                    Reed, S. K. (1972). Pattern recognition and categorization. Cogni-
                                                                                                                                                           tive Psychology, 3, 393-407.
                                                                                    Conclusion                                                      Rosseel, Y. (2002). Mixture models of categorization. Journal of
One of the most valuable aspects of rational models of cog-                                                                                                Mathematical Psychology, 46, 178-210.
                                                                                                                                                    Sanborn, A. N., Griffiths, T. L., & Navarro, D. J. (2006). A more
nition is their ability to establish connections across different                                                                                          rational model of categorization. In Proceedings of the 28th
fields. Here, we were able to exploit the correspondence be-                                                                                               Annual Conference of the Cognitive Science Society. Mah-
tween Anderson’s (1990) Rational Model of Categorization                                                                                                   wah, NJ: Erlbaum.
                                                                                                                                                    Smith, J. D., & Minda, J. P. (1998). Prototypes in the mist: The
and the Dirichlet process to draw on recent work in nonpara-                                                                                               early epochs of category learning. Journal of Experimental
metric Bayesian statistics that allowed us to define a more                                                                                                Psychology: Learning, Memory, and Cognition, 24, 1411-
general rational model, based on the hierarchical Dirichlet                                                                                                1436.
                                                                                                                                                    Teh, Y., Jordan, M., Beal, M., & Blei, D. (2004). Hierarchical
process. This model subsumes previous rational analyses of                                                                                                 Dirichlet processes. In Advances in Neural Information Pro-
human category learning, and provides a general solution to                                                                                                cessing Systems 17. Cambridge, MA: MIT Press.
the problem of selecting the number of clusters to represent                                                                                        Vanpaemel, W., Storms, G., & Ons, B. (2005). A varying abstraction
                                                                                                                                                           model for categorization. In Proceedings of the 27th Annual
a category. The result is a picture of human categorization in                                                                                             Conference of the Cognitive Science Society. Mahwah, NJ:
which people do not use a fixed representation of categories                                                                                               Erlbaum.
                                                                                                                                              328

