UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Goal Inference as Inverse Planning
Permalink
https://escholarship.org/uc/item/5v06n97q
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Baker, Chris L.
Tenenbaum, J.B.
Saxe, Rebecca R.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                           Goal Inference as Inverse Planning
                                        Chris L. Baker, Joshua B. Tenenbaum & Rebecca R. Saxe
                                                      {clbaker,jbt,saxe}@mit.edu
                                               Department of Brain and Cognitive Sciences
                                                   Massachusetts Institute of Technology
                             Abstract                                   many authors (e.g. Nichols and Stich (2003); Baker, Tenen-
                                                                        baum, and Saxe (2006)) have argued that the qualitative de-
   Infants and adults are adept at inferring agents’ goals from in-     scriptions of the principle of rationality that have been pro-
   complete or ambiguous sequences of behavior. We propose
   a framework for goal inference based on inverse planning,            posed are insufficient to account for the complexities of hu-
   in which observers invert a probabilistic generative model of        man goal inference. Further, the qualitative predictions of
   goal-dependent plans to infer agents’ goals. The inverse plan-       noncomputational models lack the resolution for fine-grained
   ning framework encompasses many specific models and rep-
   resentations; we present several specific models and test them       comparison with people’s judgments.
   in two behavioral experiments on online and retrospective goal          Here, we propose a computational version of this approach
   inference.
                                                                        to goal inference, in terms of inverse probabilistic planning. It
   Keywords: theory of mind; action understanding; Bayesian             is often said that “vision is inverse graphics”: computational
   inference; Markov Decision Processes
                                                                        models of visual perception – particularly in the Bayesian tra-
                                                                        dition – often posit a causal physical process of how images
                          Introduction                                  are formed from scenes (i.e. “graphics”), and this process
A woman is walking down the street, when suddenly she                   must be inverted in perceiving scene structure from images.
pauses, turns, and begins running in the opposite direction.            By analogy, in inverse planning, planning is the process by
Why? Is she crazy? Did she complete an errand unknown to                which intentions cause behavior, and the observer infers an
us (perhaps dropping off a letter in a mailbox) and rush off to         agent’s intentions, given observations of an agent’s behav-
her next goal? Or did she change her mind about where she               ior, by inverting a model of the agent’s planning process.
was going? These inferences derive from attributing goals to            Like much work in computer vision, the inverse planning
the woman and using them to explain her behavior.                       framework provides a rational analysis (Anderson, 1990) of
   Adults are experts at inferring agents’ goals from obser-            goal inference. We hypothesize that people’s intuitive the-
vations of behavior. Often these observations are ambiguous             ory of goal-dependent planning approximates scientific mod-
or incomplete, yet we confidently make goal inferences from             els of human decision making proposed by economists and
such data many times each day. Developmental psychologists              psychologists, and that bottom-up information from inverting
have shown that infants also perform simple forms of goal               this theory, given observations of behavior, is integrated with
inference. In experiments using live-action stimuli, Wood-              top-down prior knowledge of the space of goals to allow ra-
ward found evidence that 6-month old infants attribute goals            tional Bayesian inference of goals from behavior.
to human actors, and look longer when subsequent behav-                    The inverse planning framework includes many specific
ior is inconsistent with the old goal (1998). Meltzoff (1995)           models that differ in the complexity they assign to the be-
showed that 18-month olds imitate intended acts of human                liefs and desires of agents. Prior knowledge of the space of
actors rather than accidental ones, and Csibra and colleagues           other agents’ goals is necessary for induction, and in this pa-
found evidence that infants infer goals from incomplete tra-            per, we will present and test several models that differ in their
jectories of moving objects in simple two-dimensional anima-            representations of goal structure. Our experimental paradigm
tions (Csibra, Biró, Koós, & Gergely, 2003), both suggesting          tests each model with a wide range of action trajectories in a
that children infer goals even from incomplete actions.                 simple space for which our models make fine-grained predic-
   The apparent ease of goal inference masks a sophisticated            tions. (Our stimuli resemble those of Gergely et al. (1995)).
probabilistic induction. There are typically many goals logi-           Some of these stimuli display direct paths to salient goals, and
cally consistent with an agent’s actions in a particular context,       have simple intentional interpretations. Other stimuli display
and the apparent complexity of others’ actions invokes a con-           more complex behaviors, which may not have simple inten-
fusing array of explanations, yet observers’ inductive leaps to         tional interpretations. These sorts of trajectories allow us to
likely goals occur effortlessly and accurately. How is this feat        distinguish between alternative models that differ in their rep-
of induction possible?                                                  resentation of complex goal structure. By varying the length
   A possible solution, proposed by several philosophers and            of the trajectories, we measure how subjects’ goal inferences
psychologists, is that these inferences are enabled by an intu-         change over time, and by eliciting both online and retrospec-
itive theory of agency that embodies the principle of rational-         tive inferences, we measure how subjects integrate informa-
ity: the assumption that rational agents tend to achieve their          tion over time.
desires as optimally as possible, given their beliefs (Dennett,            To illustrate the space of models we present, consider the
1987; Gergely, Nádasdy, Csibra, & Biró, 1995). However,               introductory example. Each of the three queries raised about
                                                                    779

the woman’s goals correspond to a particular representation              incurs a cost as well). The goal state is absorbing and cost-
of goal structure that we test. The first model (M1) assumes             free, meaning that the agent incurs no cost once it reaches the
a single invariant goal across a trajectory, and explains any            goal and stays there. Thus, rational agents will try to reach
deviation from the optimal behavior as noise, or bounded ra-             the goal state as quickly as possible.
tionality. The second model (M2) assumes that agents can                    The value function Vg,w    π (s) is defined as the infinite-horizon
have subgoals along the way to their final goal, and is able             expected cost to the agent of executing policy π starting from
to explain indirect paths. The third model (M3) assumes that             state s (with no discounting):
agents’ goals can change over time, and can also explain in-
                                                                                                ∞
                                                                                             "                                            #
direct paths or changes in direction.
    The plan for the paper is as follows. We first describe
                                                                                π
                                                                             Vg,w   (s) = Eπ   ∑ ∑ Pπ (at |st , g, w)C(at , st ) s1 = s     .      (1)
                                                                                               t=1 at
our framework for inverse planning, and present three spe-
                                                                                                                       π
cific inverse planning models for goal inference. We then                Qπg,w (st , at ) = ∑st+1 P(st+1 |st , at )Vg,w   (st+1 ) + Cg,w (at , st ) is
describe two new behavioral experiments designed to distin-              the state-action value function, which defines the infinite-
guish between our specific inverse planning models, and pro-             horizon expected cost of taking action at from state st , with
vide quantitative results of our model for each experiment.              goal g, in world w, and executing policy π afterwards. The
                                                                         agent’s probability distribution over actions associated with
               Inverse planning framework                                policy π is defined as Pπ (at |st , g, w) ∝ exp βQπg,w (st , at ) ,
                                                                                                                                                    
Although the definition of rationality has been left informal            sometimes called a Boltzmann policy. The optimal Boltz-
in prior work on intentional reasoning, formal models of ra-             mann policy and the value function of this policy can be com-
tionality have been well developed in the field of decision              puted efficiently using value iteration (Bertsekas, 2001). This
theory. Markov Decision Problems (MDPs) are the standard                 policy embodies a “soft” principle of rationality, where the
formalism for sequential decision making, or planning, un-               parameter β controls how likely the agent is to deviate from
der uncertainty. Solving an MDP entails finding an optimal               the rational path for unexplained reasons. The β parameter
policy, or rule of action, that leads to the maximum expected            plays an important role in each of our models, weighing ran-
discounted reward, given the environment. A rational agent               domness against high-level goal structure, and we vary its
is one that follows an optimal policy.                                   value for each of our models to determine its effect on pre-
                                                                         diction in our experiments.
    At its core, the inverse planning framework assumes that
                                                                            Next, we describe three candidate representations for peo-
human observers represent other agents as rational planners
                                                                         ple’s prior knowledge about goals in our framework, roughly
solving MDPs. The causal process by which goals cause be-
                                                                         corresponding to the three kinds of explanations we offered
havior is generated by probabilistic planning in MDPs with
                                                                         for the woman’s anomalous behavior in our introductory ex-
goal-dependent reward functions. Using Bayesian inference,
                                                                         ample. These candidate models, denoted M1(β), M2(β,κ),
this causal process can be integrated with prior knowledge of
                                                                         and M3(β,γ), are formalized in the subsections below.
likely goal structures to yield a probability distribution over
agents’ goals given their behavior. Our framework builds on              Model 1: single underlying goal
previous work by Baker et al. (2006) and Verma and Rao
                                                                         Our first candidate model assumes that the agent has one un-
(2006), who propose similar inverse planning frameworks.
                                                                         derlying goal that it pursues across all timesteps. We denote
Here we consider a wider range of hypothesis spaces for goal
                                                                         this model M1(β). Unlike M2 and M3, this model must ex-
structures, and present the first quantitative tests of this frame-
                                                                         plain all deviations from the shortest path to the goal in terms
work as an account of human goal inference.
                                                                         of unlikely choices by the agent, governed by the parameter
    Let S be the set of agent states, let W be the set of environ-       β. Given a state sequence of length T , the distribution over
mental states, let G be the set of goals, and let A be the set of        the agent’s goal in this model is obtained using Bayes’ rule:
actions. Let st ∈ S be the agent’s state at time t, let w ∈ W be
the world state (assumed constant across trials), let g ∈ G be                             P(g|s1:T , w) ∝ P(s1:T |g, w)P(g|w),                    (2)
the agent’s goal, and let at ∈ A be the agent’s action at time t.
                                                                                                        T −1
Let P(st+1 |st , at , w) be the state transition distribution, which     where P(s1:T |g, w) = ∏t=1           P(st+1 |st , g, w). The probabil-
specifies the probability of of moving to state st+1 from state          ity of the next state st+1 , given the current state st , the
st , as a result of action at , in world w. In general, the dynam-       goal g, and the environment w, is computed by marginal-
ics of state transitions depend on the environment, but for the          izing over actions, which are only partially observable
stimuli considered in this paper, state transitions are assumed          though their effect on the agent’s state: P(st+1 |st , g, w) =
to yield the desired outcome deterministically.                          ∑at ∈Ast P(st+1 |st , at , w)Pπ (at |st , g, w). M1(β) is a special case
    Let Cg,w (a, s) be the cost of taking action a in state s for        of both M2(β,κ) and M3(β,γ), with κ and γ equal to 0.
an agent with goal g in world w. In general, cost functions
may differ between agents and environments. For our 2D                   Model 2: complex goals
motion scenarios, action costs are assumed to be proportional            The next model we consider is based on the complex goal
to the negative length of the resulting movement (staying still          model of Baker et al. (2006). We denote this model M2(β,κ).
                                                                     780

In this model, with prior probability κ, the agent picks a com-            The marginal probability of goal gt given the state sequence
plex goal, which includes the constraint that the agent must               s1:T is the product of the forward and backward messages:
pass through a particular “via-point” on the way to its end
goal. With prior probability 1−κ the agent picks a simple                            P(gt |s1:T ) ∝ P(gt |s1:t+1 )P(st+2:T |gt , s1:t+1 ). (6)
goal, which is just a single goal point, as in M1. Given the
agent’s type of goal (simple or complex), the distribution over            This distribution allows us to infer what the agent’s goal was
goals within each type is assumed to be uniform. Inferences                at time t, given its past movements from 1:t+1, and future
about an agent’s end goal are obtained by marginalizing over               movements from time t+1:T , allowing us to model subjects’
goal types, and within the complex goal type, marginalizing                retrospective inferences in Experiment 2. The parameter γ
over possible via-points.                                                  plays a key role in how information from the past and future
   The evidence that people represent and reason about com-                is integrated into the distribution over current goals. When
plex goals (Baker et al., 2006) has lead us to consider M2 as a            γ = (k − 1)/k, past and future movements carry no informa-
hypothesis for explaining people’s goal inferences. However,               tion about the current goal. When γ = 0, changing goals is
in the stimuli we consider, there may be less evidence for                 prohibited, and future information constrains the probability
complex goals than in the experiments of Baker et al. (2006).              of all past goals to be equal to P(gT −1 |s1:T ). For each exper-
Thus, we vary the parameter κ to assess the effect of greater a            iment, we tested the range of predictions of M3 across this
priori probability of complex goals. The next model we con-                parameter space.
sider also represents sequences of goals, but in a way that is
more generic, and only depends on the agent’s tendency to                                            Experiments
“change its mind”.                                                         As candidate models for our experiments, we tested each of
                                                                           M1, M2, and M3 with β values in {0.25,0.5,1.0,1.5,2.0,4.0}.
Model 3: changing goals                                                    For M2 and M3, which each have an additional free parame-
Our final model assumes that agents’ goals can change over                 ter, we tested a range of values for these parameters as well.
time for reasons unknown to observers. This model takes the                These values are listed in Tables 1 and 2. For M2, we omit
form of a Dynamic Bayes net, which we denote as M3(β,γ),                   the full range of β values from Tables 1 and 2 for readability.
where γ is the probability of changing goals. (In this section,
we omit the conditional dependence of probability distribu-                Experiment 1
tions on w for readability). Let k be the number of goals, and             Our first experiment tested the power of our alternative mod-
let P(g1 ) be the prior over initial goals at time t=1. P(gt+1 |gt )       els to predict people’s judgments in a task of inferring agents’
is the conditional distribution over changing to goal gt+1 at              goals from observations of partial action sequences.
time t+1 given the goal gt at time t:
                                  (                                        Participants Participants were 16 members of the MIT
                                     1−γ        if i = j                   community.
          P(gt+1 =i|gt = j) =                                      (3)
                                     γ/(k − 1) otherwise.                  Materials and Procedure Subjects were told they would
                                                                           watch 2D videos of intelligent aliens moving around in sim-
When γ = 0, this model reduces to M1. When γ = (k − 1)/k,                  ple environments with visible obstacles, with goals marked
the conditional distribution P(gt+1 |gt ) is uniform; in this case         by capital letters.
the model is equivalent to choosing a new goal at random                      There were 100 stimuli in total. An illustrative subset is
at each time step. Intermediate values of γ between 0 and                  shown in Fig. 1(a). Each stimulus contained 3 goals. There
(k − 1)/k interpolate between these extremes.                              were 4 different goal configurations, and two different obsta-
   To compute the posterior distribution over goals at time t,             cle conditions: gap and solid, for a total of 8 different envi-
given a state sequence s1:t+1 , we recursively define the for-             ronments. There were 11 different complete paths: two paths
ward distribution:                                                         headed toward ‘A’, two paths headed toward ‘B’, and 7 paths
  P(gt |s1:t+1 ) ∝ P(st+1 |gt , st ) ∑ P(gt |gt−1 )P(gt−1 |s1:t ), (4)     headed toward ‘C’ (to account for C’s varying location). Par-
                                    gt−1                                   tial segments of these paths starting from the beginning were
                                                                           shown in each different environment. Because many of the
where the recursion is initialized with P(g1 ). This allows us             paths were initially identical, and because many of the paths
to model subjects’ online inferences in Experiment 1.                      were not possible in certain environments (i.e. collided with
   To compute the marginal probability of a goal at time t                 walls), the total number of unique stimuli was reduced to 100.
given s1:T , t<T , we use a variant of the forward-backward                   Stimuli were presented shortest lengths first in order to not
algorithm. The forward distribution is defined by Eq. 4 above.             bias subjects toward particular outcomes. Stimuli of the same
The backward distribution is recursively defined by:                       length were shown in random order. After each stimulus pre-
                                                                           sentation, subjects were asked to rate which goal they thought
   P(st+2:T |gt , s1:t+1 ) =                                               was most likely (or if two or more were equally likely, to pick
      ∑ P(st+2 |gt+1 , st+1 )P(st+3:T |gt+1 , st+2 )P(gt+1 |gt ).  (5)     one of the most likely). After this choice, subjects were asked
     gt+1                                                                  to rate the likelihood of the other goals relative to the most
                                                                       781

(a)
                           C                       13   A                           C            A                          C          A                                    A                                    A                                 C
                                                                                                                                                                                                                                                   15   A
Example
                                                 12                                                                                                                                                                                                14
                                        7 8 9 1011                                          13                          7 8 9 10                              7 8 9 10                                                                             13
                                                                                                                                                                                                             1112 C
                                    6                                                      12                          6       11                            6       11                                         1314                               12
                           34
                                5                                                        11
                                                                          3 4 5 6 7 8 9 10                         34
                                                                                                                      5          12
                                                                                                                                  13                     34
                                                                                                                                                            5      C 1213                     3 4 5 6 7 8 9 10
                                                                                                                                                                                                                                                   11
                                                                                                                                                                                                                                     3 4 5 6 7 8 9 10
 Stimuli
                       2                                              2                                        2                                     2                                    2                                      2
                   1                                    B         1                              B         1                           B         1                          B         1                          B           1                          B
                                    A
(b)           1                     B                        1                                        1                                     1                                    1                                      1
                                    C
Subject      0.5                                            0.5                                      0.5                                   0.5                                  0.5                                    0.5
Rating        0                                              0                                        0                                     0                                    0                                      0
               3                    7         1011 13         3                 7       1011 13        3                7       1011 13      3               7     1011 13        3               7     1011 14          3               7 101113 15
(c)           1                                              1                                        1                                     1                                    1                                      1
  Model      0.5                                            0.5                                      0.5                                   0.5                                  0.5                                    0.5
Prediction    0                                              0                                        0                                     0                                    0                                      0
               3                    7         1011 13         3                 7       1011 13        3                7       1011 13      3               7     1011 13        3               7     1011 14          3               7 101113 15
                           Time step                                       Time step                               Time step                              Time step                            Time step                             Time step
Figure 1: Experiment 1. (a) Example stimuli. Plots show all 4 goal conditions and both obstacle conditions. Both ‘A’ paths are shown, one
of two ‘B’ paths is shown and 2 of 7 ‘C’ paths are shown. Dark colored numbers indicate displayed lengths. (b) Average subject ratings with
standard error bars for above stimuli. (c) Model predictions. Model predictions closely match people’s ratings. Displayed model: M3(1.5,.5).
                                                                                                                                Model
                                        M1(β)                      M2(1.5,κ)                      M2(2.0,κ)                  M3(β,0.67)                   M3(β,0.50)            M3(β,0.25)                    M3(β,0.10)
                                      β      r                     κ      r                       κ      r                     β      r                     β     r               β     r                       β     r
                                    0.25 0.82                     0.10 0.89                      0.10 0.89                   0.25 0.92                    0.25 0.92             0.25 0.92                     0.25 0.91
                                     0.5   0.83                   0.25 0.90                      0.25 0.91                    0.5  0.93                    0.5  0.94             0.5  0.95                     0.5  0.94
                                     1.0   0.82                   0.50 0.92                      0.50 0.93                    1.0  0.95                    1.0  0.95             1.0  0.96                     1.0  0.96
                                     1.5   0.82                   0.67 0.92                      0.67 0.93                    1.5  0.96                    1.5  0.96             1.5  0.97                     1.5  0.97
                                     2.0   0.81                   0.75 0.93                      0.75 0.94                    2.0  0.96                    2.0  0.97             2.0  0.98                     2.0  0.98
                                     4.0   0.78                   0.90 0.93                      0.90 0.94                    4.0  0.96                    4.0  0.96             4.0  0.96                     4.0  0.96
Table 1: Experiment 1 results. M1, M2 and M3 were tested with β values in {0.25,0.5,1.0,1.5,2.0,4.0}. Odd columns contain parameter
settings for the various models. Even columns contain r-values of the various models’ correlations with people’s ratings. For M2 and M3,
which each have an additional free parameter, we tested a range of values for these parameters as well. We omit the full range of β values for
M2 for readability. The M3(β, 0.67) column corresponds to the condition in which a new goal is sampled at random at each time step.
likely goal, on a 9-point scale from “Equally likely”, to “Half                                                                            viable model for people’s judgments in this experiment.
as likely”, to “Extremely unlikely”. Ratings were normalized
to sum to 1 for each stimulus, then averaged across all sub-                                                                               Experiment 2
jects and renormalized to sum to 1. Example subject ratings
are plotted with standard error bars in Fig. 1(b).                                                                                         Our second experiment sought to provide a context within
   Each model makes strong predictions about people’s rat-                                                                                 which predictions of M2 and M3 could be distinguished. We
ings in this experiment. If M1 is correct, then people should                                                                              showed subjects long trajectories and asked them to make ret-
weigh evidence from old and recent movements equally, and                                                                                  rospective judgments about the agent’s goal at several earlier
react slowly to new evidence that conflicts with past evi-                                                                                 points in the action sequence. As explained below, in cases
dence. Conversely, M2 and M3 predict that people should                                                                                    where the early and late stages of a trajectory are locally best
react quickly to recent movements strongly indicating a par-                                                                               explained by different goals, only the changing goal model
ticular goal. M2 achieves this by inferring that a subgoal has                                                                             (M3) predicts that people’s retrospective goal inferences will
been reached, and that recent movements reflect the end goal.                                                                              vary accordingly.
M3 achieves this by inferring that the agent has changed its                                                                               Participants Participants were 16 members of the MIT
goal. Example model predictions from M3(1.5, 0.5) are plot-                                                                                community (distinct from the first group).
ted in Fig. 1(c); these match subjects’ ratings very closely.
                                                                                                                                           Materials and Procedure The procedure of Experiment 2
Results The results of Experiment 1 are summarized in Ta-                                                                                  was similar to that of Experiment 1, except now subjects were
ble 1. All instances of M3 correlate highly with subjects’                                                                                 told they would see an alien’s movement, and that after this
ratings, indicating that subjects were quick to respond to ev-                                                                             movement, an earlier point along the alien’s path would be
idence of a new goal. Because of this, M2 also correlates                                                                                  marked. Subjects were told they would then be asked to indi-
highly with people’s judgments. M1 clearly does a poorer                                                                                   cate “which goal the alien had in mind” at the marked point,
job of predicting people’s judgments. Fig. 3(a) shows scat-                                                                                in light of the entire subsequent path they observed.
ter plots of model predictions versus subject ratings for the                                                                                 There were 95 stimuli in this experiment. Stimuli were
model with the highest correlation from each class. Although                                                                               taken from Experiment 1 as follows. Each path from each en-
the predictions of M3 correlate slightly higher with subjects’                                                                             vironment was used. However, only paths of maximal length
ratings than the predictions of M2, only M1 is ruled out as a                                                                              were displayed. The marked points were taken to be evenly
                                                                                                                                   782

(a)
                   C           A                    C     A                     C     A                           A                           A              C       A
Example                                                                                                                                       C
                                                                                                            C
 Stimuli                       B                          B                           B                           B                           B                      B
                       A
(b)           1                       1                          1                           1                           1                         1
                       B
                       C
Subject      0.5                     0.5                        0.5                         0.5                         0.5                       0.5
Rating        0                       0                          0                           0                           0                         0
               3       7     11 13     3        7       11 13     3         7       11 13     3         7       11 13     3        7    11 14       3    7    11 13 15
(c)           1                       1                          1                           1                           1                         1
  Model      0.5                     0.5                        0.5                         0.5                         0.5                       0.5
Prediction    0                       0                          0                           0                           0                         0
               3       7     11 13     3        7       11 13         3     7       11 13         3     7       11 13         3    7    11 14       3    7       11 13 15
                   Time step                  Time step                   Time step                   Time step                   Time step             Time step
Figure 2: Experiment 2. (a) Example stimuli. Dashed line corresponds to the movement subjects saw prior to rating the likelihood of each
goal at the marked point. Black +’s correspond to test points in the stimuli. Compare to corresponding column of Fig. 1. (b) Subjects’ ratings:
compare to Fig. 1. (c) Model predictions. Displayed model: M3(1.5, 0.5).
                                                                                   Model
                           M1(β)            M2(1.5,κ)      M2(2.0,κ)            M3(β,0.67)            M3(β,0.50)        M3(β,0.25)        M3(β,0.10)
                         β      r           κ      r       κ      r               β      r              β     r           β     r           β     r
                       0.25 0.60           0.10 0.78      0.10 0.77             0.25 0.91             0.25 0.95         0.25 0.94         0.25 0.87
                        0.5   0.58         0.25 0.78      0.25 0.77              0.5  0.91             0.5  0.95         0.5  0.93         0.5  0.86
                        1.0   0.57         0.50 0.78      0.50 0.77              1.0  0.92             1.0  0.94         1.0  0.92         1.0  0.84
                        1.5   0.56         0.67 0.78      0.67 0.77              1.5  0.92             1.5  0.93         1.5  0.90         1.5  0.83
                        2.0   0.56         0.75 0.78      0.75 0.77              2.0  0.91             2.0  0.92         2.0  0.88         2.0  0.82
                        4.0   0.55         0.90 0.78      0.90 0.77              4.0  0.90             4.0  0.90         4.0  0.86         4.0  0.78
Table 2: Experiment 2 results. M1, M2 and M3 were tested with β values in {0.25,0.5,1.0,1.5,2.0,4.0}. Odd columns contain parameter
settings for the various models. Even columns contain r-values of the various models’ correlations with people’s ratings. For M2 and M3,
which each have an additional free parameter, we tested a range of values for these parameters as well. We omit the full range of β values for
M2 for readability. The M3(β, 0.67) column corresponds to the condition in which a new goal is sampled at random at each time step.
spaced points corresponding to shorter path lengths previ-                                  M3 continues to correlate most highly with people’s judg-
ously displayed in Experiment 1. Thus, each rating in Exper-                                ments. Interestingly, as predicted earlier, the correlation of
iment 2 had a corresponding rating in Experiment 1. Fig. 2(a)                               people’s ratings in Experiment 2 with people’s ratings from
shows illustrative stimuli that directly correspond to the stim-                            the corresponding stimuli from Experiment 1 was 0.89; fairly
uli in Fig. 1(a).                                                                           high given the difference in tasks. Fig. 3(b) shows scat-
   In this experiment, M2 and M3 make opposite predictions.                                 ter plots of model predictions versus subject ratings for the
M2 (and M1) predict that the strong evidence provided for                                   model with the highest correlation in each class.
particular end goals by the long paths shown in Experiment 2
will have a large effect on people’s inferences about agents’                                                                 Discussion
goals earlier in their paths. This is because both M1 and M2
assume the agent’s goal (simple or complex) to be constant                                  The high correlations between our models and subjects’
throughout its path. Thus, if an agent gives a strong indica-                               predictions from Experiment 1 and Experiment 2 provide
tion of pursuing a particular goal at the end of its path, these                            strong quantitative evidence in support of the inverse plan-
models assume that it must have been headed for this goal all                               ning framework. These results also provide support for the
along. M3, however, uses Eq. 6 to compute the probability of                                goal structures of M3 as plausible representations for human
the agent’s goal at the marked point. As discussed earlier, the                             goal inference. However, the lower correlations of M2 with
parameter γ controls how much information the distribution                                  subjects’ predictions from Experiment 2 do not rule out sub-
over goals in the future provides about goals in the past. M3                               goals as a possible goal structure representation. Subgoals
predicts that people’s prior assumptions about the likelihood                               could be useful in some cases, such as in our earlier work,
of changing goals will have a large effect on their judgments.                              where we showed that people’s action predictions are well-
If γ is close to (k − 1)/k, M3 predicts that people’s ratings                               explained by M2 when an agent persistently pursues complex
in Experiment 2 will correlate highly with the corresponding                                goals (Baker et al., 2006). As mentioned previously, the stim-
ratings from Experiment 1.                                                                  uli used in the current paper provide less evidence for sub-
                                                                                            goals than the stimuli used in Baker et al. (2006). People’s
Results The results of Experiment 2 are summarized in Ta-                                   use of different models to explain and reason about different
ble 2. M1 continues to perform poorly. Now, however, M2 is                                  data might be captured by a hierarchical Bayesian model that
also a relatively poor predictor of people’s judgments, while                               incorporates both M2 and M3 as submodels, as well as many
                                                                                      783

(a)
      Experiment 1
                                        r = 0.83                 r = 0.93                 r = 0.94                 r = 0.96                 r = 0.97                 r = 0.98                 r = 0.98
                               1                        1                        1                        1                        1                        1                        1
                     People
                              0.5                      0.5                      0.5                      0.5                      0.5                      0.5                      0.5
                               0                        0                        0                        0                        0                        0                        0
                                    0     0.5      1         0     0.5      1         0     0.5      1         0      0.5     1         0     0.5      1         0     0.5      1         0     0.5      1
                                    M1(0.50)             M2(1.5,0.90)                M2(2,0.90)               M3(2,0.67)                M3(2,0.50)               M3(2,0.25)               M3(2,0.10)
      Experiment 2
(b)                                     r = 0.60                 r = 0.78                 r = 0.77                 r = 0.92                 r = 0.95                 r = 0.94                 r = 0.87
                               1                        1                        1                        1                        1                        1                        1
                     People
                              0.5                      0.5                      0.5                      0.5                      0.5                      0.5                      0.5
                               0                        0                        0                        0                        0                        0                        0
                                    0     0.5      1         0     0.5      1         0     0.5      1         0      0.5     1         0     0.5      1         0     0.5      1         0     0.5      1
                                    M1(0.25)             M2(1.5,0.90)                M2(2,0.90)               M3(1,0.67)           M3(0.50,0.50)            M3(0.25,0.25)            M3(0.25,0.10)
Figure 3: Example scatter plots of model predictions against subject ratings. Plots of model predictions use the parameter settings with the
highest correlation from each model column of Tables 1 and 2. (a) Experiment 1 results. (b) Experiment 2 results.
other submodels with different goal representations, since M2                                                      tations of the models. Our experiments yielded high resolu-
and M3 merely scratch the surface of possible goal structures.                                                     tion data, which provided empirical support for the inverse
   Although our results suggest that people explain apparent                                                       planning framework, and gave quantitative evidence for our
deviations from the most direct path by inferring a change                                                         changing goals model as a plausible goal representation. It
in goals, inferring that an agent’s goal has changed does not                                                      will be important to test whether inverse planning will gen-
account for all the complexities of behavior – the parameter                                                       eralize to explain human goal inference outside of the lab-
β also plays an important role in generating accurate predic-                                                      oratory, but we believe that the power of the rationality as-
tions of subjects’ ratings in our model. Thus, people’s goal                                                       sumption, combined with rich representations of goal struc-
inferences in our experiments reflect a tradeoff between ex-                                                       ture, can account for much of people’s everyday reasoning
plaining complex behavior in terms of unlikely deviations                                                          about agents’ actions and goals.
from the shortest path and attributing a change in goals.
                                                                                                                   Acknowledgments: This work was supported by AFOSR MURI
   Two well-known qualitative accounts of action understand-                                                       contract FA9550-05-1-0321, the James S. McDonnell Foundation
ing, theory-theory (Gopnik & Meltzoff, 1997) and simulation                                                        Causal Learning Collaborative Initiative, the Department of Home-
theory (Goldman, 2006), can be seen as cases of inverse plan-                                                      land Security Graduate Fellowship (CLB) and the Paul E. Newton
                                                                                                                   Chair (JBT).
ning. On a theory-theory interpretation, inverse planning con-
sists of inverting a theory of rational action to arrive at a set of                                                                                References
goals that could have generated the observed behavior, and in-                                                     Anderson, J. R. (1990). The adaptive character of thought. Hills-
ferring individual goals based on prior knowledge of the kinds                                                        dale, NJ: Lawrence Erlbaum Associates.
of goals the observed agent prefers. On a simulation theory                                                        Baker, C. L., Tenenbaum, J. B., & Saxe, R. R. (2006). Bayesian
                                                                                                                      models of human action understanding. Advances in Neural In-
account, goal inference is performed by inverting one’s own                                                           formation Processing Systems, Vol. 18 (pp. 99–106).
planning process to narrow down the set of goals that could                                                        Bertsekas, D. P. (2001). Dynamic programming and optimal control.
have generated the observed behavior, and inferring individ-                                                          Belmont, MA: Athena Scientific, 2nd edition.
ual goals from this set according to their desirability under                                                      Csibra, G., Biró, S., Koós, O., & Gergely, G. (2003). One-year-old
one’s own preferences.                                                                                                infants use teleological representations of actions productively.
   Unlike previous proposals, our computational framework                                                             Cognitive Science, 27, 111–133.
shows precisely how to integrate top-down prior knowledge                                                          Dennett, D. C. (1987). The intentional stance. Cambridge, MA:
                                                                                                                      MIT Press.
about goals with bottom-up observations of behavior using
                                                                                                                   Gergely, G., Nádasdy, Z., Csibra, G., & Biró, S. (1995). Taking the
Bayesian inference. Similar ideas have been sketched out                                                              intentional stance at 12 months of age. Cognition, 56, 165–193.
qualitatively, but our inverse-planning models are the first to                                                    Goldman, A. I. (2006). Simulating minds. Oxford University Press.
quantitatively describe people’s probabilistic goal inferences,                                                    Gopnik, A., & Meltzoff, A. N. (1997). Words, thoughts, and theo-
and to explain rationally how these inductive leaps can be suc-                                                       ries. Cambridge, MA: MIT Press.
cessful given only sparse, incomplete observation sequences.                                                       Meltzoff, A. N. (1995). Understanding the intentions of others: Re-
                                                                                                                      enactment of intended acts by 18-month-old children. Develop-
                                                Conclusion                                                            mental Psychology, 31(5), 838–850.
                                                                                                                   Nichols, S., & Stich, S. (2003). Mindreading. Oxford University
We presented a computational framework for explaining peo-                                                            Press.
ple’s goal inferences based on inverse planning. Within                                                            Verma, D., & Rao, R. (2006). Goal-based imitation as probabilistic
this framework, we presented three specific inverse planning                                                          inference over graphical models. Advances in Neural Information
                                                                                                                      Processing Systems, Vol. 18 (pp. 1393–1400).
models, each using a different representation of goal struc-
ture. We tested these models with two novel experiments                                                            Woodward, A. L. (1998). Infants selectively encode the goal object
                                                                                                                      of an actor’s reach. Cognition, 69, 1–34.
designed to distinguish between the different goal represen-
                                                                                                         784

