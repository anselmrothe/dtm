UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Human Performance in Statistical Word Segmentation
Permalink
https://escholarship.org/uc/item/03m6p1h1
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Frank, Micheal C.
Goldwater, Sharon
Mansinghka, Vikash
et al.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                 Modeling Human Performance in Statistical Word Segmentation
 Michael C. Frank1 (mcfrank@mit.edu), Sharon Goldwater2 (sgwater@stanford.edu), Vikash Mansinghka1
    (vkm@mit.edu), Tom Griffiths3 (tom_griffiths@berkeley.edu), and Joshua Tenenbaum1 (jbt@mit.edu)
      1
        Department of Brain and Cognitive Sciences, MIT; 2Department of Linguistics, Stanford University; 3Department of
                                             Psychology, University of California, Berkeley
                              Abstract                                 word segmentation should succeed in recovering the
   What mechanisms support the ability of human infants,
                                                                       complete lexicon of the original Saffran, Newport, and
   adults, and other primates to identify words from fluent            Aslin (1996) experiment, for example. However, despite the
   speech using distributional regularities? In order to better        simplicity of artificial language experiments, human adults
   characterize this ability, we collected data from adults in an      largely do not achieve perfect performance in these tasks. In
   artificial language segmentation task similar to Saffran,           the work presented here, we take advantage of this fact by
   Newport, and Aslin (1996) in which the length of sentences          manipulating the difficulty of an artificial segmentation
   was systematically varied between groups of participants. We        experiment and then evaluating a number of computational
   then compared the fit of a variety of computational models—
   including simple statistical models of transitional probability     models on their fit to human performance in this paradigm.
   and mutual information, a clustering model based on mutual             The plan of the paper is as follows. We first describe an
   information by Swingley (2005), PARSER (Perruchet &                 artificial language learning experiment with adult
   Vintner, 1998), and a Bayesian model. We found that while           participants in which we parametrically varied the length of
   all models were able to successfully complete the task, fit to      sentences in our language in an attempt to vary the difficulty
   the human data varied considerably, with the Bayesian model         of the segmentation task. In the next section, we describe
   achieving the highest correlation with our results.
                                                                       the criteria for evaluation of the models and the details of
   Keywords: Statistical learning; word segmentation; language         the implementation of each model (in cases in which the
   acquisition; Bayesian modeling.                                     details of a model are already described in another
                                                                       publication we only note where our implementation differs
                          Introduction                                 from that description). Because all systems tested were
How do young infants learn words from fluent speech?                   extremely effective at finding the correct segmentation, we
Research on this topic has identified a number of                      compared the models on two measures: 1) the best linear fit
information sources which aid in word segmentation,                    of their performance to the experimental data, and 2) the
including phonotactic, prosodic, and allophonic cues, as               relative contribution of target and distractor scores to the
well as lexical knowledge (Jusczyk, 1999). However, these              performance of each model. We conclude by discussing the
information sources vary across languages. Some languages              relative merits of the different models in fitting human data.
(like English) have a predominant stress pattern which                    While our selection of models is by no means a complete
allows relatively robust segmentation even in the absence of           survey of the field, we have attempted to test models which
lexical knowledge, while others do not.                                have been influential or distinctive in the psychological
   One fact remains constant across languages: the use of a            literature. We start with models based on the suggestion by
small subset of the possible sound sequences in combination            Saffran, Newport, and Aslin (1996) that boundaries between
to create many different meanings. This property—                      words can be effectively found through the use of simple
essentially, the existence of words which are combined                 bigram statistics. We evaluate three models of this type:
together to form sentences—implies certain statistical                 local minima in transitional probability (TP); minima in TP
properties of the speech stream, including an increase in              with smoothed counts; local minima in pointwise mutual
predictability from the beginning of a word to its end.                information. We then evaluate three other models which
Recent work by Saffran and colleagues suggests that human              focused on finding a lexicon to fit the input corpus: a
learners can make use of these statistical properties to               clustering model by Swingley (2005) which also uses
distinguish words from non-words in a novel artificial                 pointwise mutual information; PARSER (Perruchet &
language (Saffran, Aslin, & Newport, 1996). Many other                 Vinter, 1998), a memory-decay model of segmentation; and
studies have replicated and extended these results to a                a Bayesian model in the style of Brent (1999) by Goldwater,
variety of other domains (e.g., Fiser & Aslin, 2002) and               Griffiths, and Johnson (2006).
other populations, including non-human primates (Hauser,
Newport, & Aslin, 2001).                                                                   Experimental Data
   However, despite the existence of a large number of                 When learning a foreign language, longer sentences often
statistical models of word segmentation (reviewed in Brent,            seem more difficult to understand than shorter sentences.
1999), there have been relatively few attempts to integrate            Certainly, in the limit, individually presented words are easy
these models with human experimental results. One reason               to learn and remember, while those presented in long
for this lack is the extreme simplicity of many of the human           sentences with no boundaries are more difficult, perhaps
artificial language learning results: every available model of
                                                                   281

because of problems in segmentation. In order to test the
hypothesis that segmentation performance decreases as
sentence length increases, we exposed adults to sentences
constructed from a simple artificial lexicon. We assigned
participants to one of eight sentence-length conditions so
that we could estimate the change in their performance as
sentence length increased.
Methods
Participants We tested 96 MIT students and members of
the surrounding community, but excluded 5 participants
from the final sample based on performance greater than
two standard deviations below the population mean.
Materials Each participant in the experiment heard a unique
and randomly generated sample from an artificial language.
The lexicon of this language was generated by                               Figure 1. Segmentation performance as a function of
concatenating 18 syllables (ba, bi, da, du, ti, tu, ka, ki, la, lu,          sentence length. Dots show mean performance for
gi, gu, pa, pi, va, vu, zi, zu) into six words, two with two                                       individuals.
syllables, two with three syllables, and two with four
syllables. Sentences in the language were created by
randomly concatenating words together without adjacent                                  Computational Modeling
repetition of words. Each participant heard a randomly                  In order to evaluate models of word segmentation, we
generated language sample consisting of 1200 words.                     compared their performance on our experimental materials
   Participants were randomly placed in one of eight                    to that of our adult participants.
sentence length conditions (1, 2, 3, 4, 6, 8, 12, or 24 words
per sentence). All speech in the experiment was synthesized             Materials We compiled a corpus of ten randomly generated
using the MBROLA speech synthesizer (Dutoit, Pagel,                     training sets in each of the eight sentence length conditions.
Pierret, Bataille, & van der Vrecken, 1996) with the us3                Each training set was of the same length as those presented
diphone database, in order to produce an American male                  to our experimental participants (1200 words) and was
speaking voice. All consonants and vowels were 25 and                   accompanied by 30 pairs of test items, the same number of
225ms in duration, respectively. The fundamental frequency              trials as our participants received. Test items were, as in the
of the synthesized speech was ~100 Hz. No breaks were                   experimental section, words in the generating lexicon of the
introduced into the sentences: the synthesizer created equal            training set or part-word distractors.
co-articulation between every phone. There was a 500ms
break between each sentence in the training set. Test                   Evaluation Our metric of evaluation was simple: each
materials consisted of a word from the lexicon paired with a            model was required to generate a score of some kind for
part-word distractor (a set of syllables of the same length             each of the two forced-choice test items. We turned these
which also appeared—with lower frequency—in the                         scores into probabilities by applying the Luce choice rule
corpus).                                                                (Luce, 1963):
Procedure. Participants were given instructions that they                                                      s(x)
were going to listen to a nonsense language for 15 minutes,                                   pchoice (x) =
                                                                                                            s(x) + s(y)
after which they would be tested on how well they learned
the words of the language. All participants listened on                 where s(x) and s(y) denote the scores of the two words in the
headphones in a quiet room. After they had heard the                    forced choice. (Note that in the case where the scores are
training set, they were instructed to make forced choice                          ! under the language, this is exactly what we
                                                                        probabilities
decisions between pairs of words from the test set by                   should do to condition on the fact that one outcome of the
indicating which one of the two sounded more like a word                two forced choice options must be in the language). Having
in the language they just heard. No feedback was given                  produced a choice probability for each test trial, we then
during testing.                                                         averaged these probabilities across test trials and training
                                                                        corpora to produce a set of average choice probabilities for
Results                                                                 each sentence-length condition.
Performance by condition is shown in Figure 1. We
observed a significant main effect of sentence length on                Boundary-finding approaches
performance (F[7,88]=5.57, p < .001), resulting from the                One approach to segmentation employs simple bigram
gradual decrease in performance as sentences grew longer.               statistics to measure the relationship between units such as
                                                                    282

syllables or phonemes. This approach is originally due to          syllable pairs (Swingley, 2005; Brent, 1999). Pointwise
Harris (1951) but has been the focus of much recent interest.      mutual information is defined as:
We chose syllables as the primary level of analysis for our
models; all syllables had the same structure (consonant-                                                p(a,b)
                                                                                   MI(a,b) = log 2
vowel), so there was no difficulty in segmenting words into                                        # p(a, y) # p(x,b)
syllables. We examined three models of this type, beginning                                        y "V     x "V
with the suggestion of Saffran, Newport, and Aslin (1996)
to use local minima in transitional probability as word            We create scores for words using the same method as we
boundaries. In order to ascertain that our problems in fitting     used with TP above: taking local minima in MI across
                                                                        !
human data did not stem from the 0 and 1.0 TPs found in            words. In a less uniform corpus, this measure would differ
our corpus, we further tested a transitional probability model     significantly from the result of summing mutual information
with smoothed counts. We also tested a model using                 across words. However, given that part-words spanned
pointwise mutual information (MI), a bidirectional measure         exactly one word boundary and all syllables appeared in
of association.                                                    only one word, summing mutual information produced the
                                                                   same result as taking local minima.
Transitional probability We calculated transitional
probability (TP) by creating bigram syllable counts over the       Lexicon-finding approaches
training sentences in our corpus with a symbol appended to         We evaluated three other models. These models were
the beginning and end of each sentence to indicate a               distinguished by the assumption that a lexicon—a list of
boundary. The transitional probability of a syllable b given       words—is the fundamental representation to be optimized
a was defined as:                                                  with respect to the input corpus. The first was a recent
                                                                   model by Swingley (2005), which clusters syllables based
                                     p(a,b)                        on their frequency and the mutual information of their
                      p(b | a) =
                                 # p(a, y)                         syllables. The second was a Bayesian model of
                                 y "V
                                                                   segmentation proposed by Goldwater, Griffiths, and
                                                                   Johnson (2006). The third was PARSER (Perruchet &
where p(a,b) was the probability of the bigram ab appearing        Vinter, 1998), a model based on simple memory principles
and V was the complete set of bigrams observed in the              of decay and interference. We describe each briefly because
            !
corpus.                                                            the relevant details are available in the respective
   The score of a word under this model was defined as the         publications.
minimum transitional probability within that word (as in
Saffran, Newport, and Aslin, 1996). However, given that in         Swingley (2005) This model is a heuristic clustering model
our stimuli, transitional probabilities between syllables in       which calculates n-gram statistics and pointwise mutual
the words in the language were equal to one, the same              information over a corpus, then takes as words those strings
probabilities for targets and distractors would have been          which exceed a certain threshold value both in their
computed if the dependent measure were the product of the          frequency and in the mutual information of their constituent
probabilities within a word rather than the minimum.               bisyllables. In order to run the model on the language of our
                                                                   experiment, we added support for four syllable words. We
Smoothed transitional probability We additionally                  then defined the score of a string under the model (given
calculated transitional probabilities using a simple add-          some input corpus) as the maximum threshold value at
lambda smoothing scheme in order to eliminate zero counts          which that string appeared in the lexicon found by the
for unobserved bigrams. We did this by calculating the             model. In other words, the highest-scoring strings were
probability of a bigram p(a,b) as:                                 those that had the highest percentile rank both in mutual
                                                                   information and in frequency. It should be noted that, unlike
                              count(a,b) + "                       the next two models, Swingley’s model relies on purely
                 p(a,b) =
                          " # V + % count(a, x)                    local and word-based statistics (frequency and MI); thus,
                                    x $V                           unlike either PARSER or GGJ2006, a word’s score is
                                                                   unrelated to the size of the lexicon.
In other words, we incremented each count by a small
constant, lambda, and then divided by lambda times the
       !                                                           Goldwater, Griffiths, & Johnson (2006) This model uses
number of words in the vocabulary. We tested using a range
                                                                   Bayesian inference to optimize a lexicon with respect to an
of values for lambda but found equivalent results for all
                                                                   observed corpus. Its lexicon is generated according to a
values, thus we report values with a standard value, λ = 1.
                                                                   Dirichlet process, a probability distribution which gives
                                                                   higher probability to small lexicons containing short words.
Point-wise mutual information Mutual information is                We use the implementation of the unigram model described
sometimes suggested as an alternative to transitional              in GGJ2006 since there were no bigram syllable
probability for computing the association strength between
                                                                   dependencies in our materials.
                                                               283

   The score for a word under this model was the posterior         differences in forced-choice performance. Therefore, we
probability of the word, estimated using a Gibbs sampler as        report results using the same parameter settings used by
in the original paper. Because the posterior probability of        Perruchet and Vintner.
the correct solution was normally so high (indicating a high          We made one modification to the model to allow it to run
degree of confidence in the solution the model found), we          on our data: rather than iterating through the entire input
ran the Gibbs sampler using a range of temperatures to             corpus, our implementation of the model iterated through
encourage the model to consider alternate solutions.               each sentence until reaching the end and then began anew at
(Temperature is a parameter which controls the degree to           the beginning of the next sentence. Scores for words under
which the Gibbs sampler prefers more probable lexicons,            this model were the average scores from the lexicon
with higher temperature indicating greater willingness to          obtained by averaging the lexicons from 20 PARSER runs
consider lower-probability lexicons). The model had one            on each training set.
further parameter: the parameter of the Dirichlet process, α,
which was kept constant at the value used in the original          Comparison 1: Linear fit to experimental data
paper.                                                             Because all of the models we evaluated gave high choice
                                                                   probabilities to the targets (indicating near-perfect
PARSER We implemented the PARSER model described                   segmentation), absolute performance was not useful in
in Perruchet and Vintner (1998). This model is organized           comparing models. Instead, we examined performance
around a lexicon, that is, a set of words and scores for each      across the eight sentence-length conditions relative to the
word. The model receives input sentences and parses them           performance of adult participants. In other words, we were
according to the current lexicon and then adds sequences to        interested in whether the models extracted a similar amount
the lexicon at random from the parsed input. Each lexical          of information from a corpus with e.g., three-word sentences
item decays at a constant rate and similar items interfere         relative to what they extracted from a corpus with two-word
with each other. The model as described has six parameters:        sentences. In order to compute the relationship between
the maximum length of an added sequence, the weight                conditions in different models relative to the human data,
threshold for a word being used to parse new sequences, the        we first scaled the performance of each model using a linear
forgetting and interference rates, the gain in weight for          regression, finding the best linear adjustment of the scale of
reactivation, and the initial weight of new words. Because of      the curve from each model. We then computed simple
the large number of parameters in this model, it was not           correlation coefficients (r values) between the best fit of
possible to complete an exhaustive search of the parameter         each model and the experimental data. See figure 2 for
space; however, we experimented with a variety of different        results.
combinations of interference and forgetting rates and                 We found that GGJ2006 best fit our experimental data,
maximum sequence lengths without finding any major                 succeeding in particular in modeling the decrease in
       Figure 2. Best linear fit of each model’s performance to human data, graphed by sentence length. The vertical axis
      represents decision probabilities for models and percentage correct for human data; the horizontal, sentence length.
                                                               284

information between sentences of length 12 and sentences              system. Run in this fashion, PARSER is actually quite
of length 24. Here we plot results from this model at                 similar to a probabilistic model such as GGJ2006 or Brent
temperature 5, but for temperatures between 50 and 3, there           (1999) in that it is an algorithm for sampling a posterior
was relatively little difference in r values (ranging                 distribution over lexicons, albeit one that incorporates a
predictably between .920 and .968).                                   number of free parameters and ad hoc approximations.
   Interestingly, the next most effective models were the                In the following section we examine further the reasons
boundary-finding models using MI and TP. These models                 why performance differed between models by examining
produce curves that are noticeably too shallow, changing              the contribution of target and distractor scores to each
little in performance between sentences with length 12 and            model’s choice probabilities.
those with length 24; however, they do show the same
dependency between sentence length and score as the                   Comparison 2: Target and distractor scores
human data do. Surprisingly, once TP, smoothed TP, and                Why did some models match the drop in human
MI were fit to the human data, there was no appreciable               performance as sentence length increased, while others did
quantitative difference between the models’ fit, suggesting           not? One way in which models differed from one another
that these models’ underlying difficulty in fitting this dataset      was whether the score assigned to targets and distractors
may be a fundamental deficit of the bigram statistic                  changed as the sentence length changed. For example,
approach.                                                             because TPs within words were always 1, target scores
   Finally, both Swingley2005 and PARSER performed very               under the two TP models remained constant no matter what
poorly on this task, producing choice probabilities that did          the sentence length. In order to investigate this factor more
not decrease as utterance length increased. One issue in the          systematically, we plotted target and distractor scores for
Swingley model is that because it relies on percentile                each model, normalized by the maximal target score (so that
rankings of frequency rather than raw frequency, its                  all scores varied between 0 and 1). See figure 3 for results.
performance can vary highly with very small changes in                   Although we have no empirical data which address
frequency. In addition, because the model is deterministic,           whether participants make errors at longer sentence lengths
this noise could not be averaged out by multiple runs                 because targets are less attractive or because distractors are
through the input corpus.                                             more so, it seems plausible that both are true. However, in
   PARSER was similarly variable in its performance, but              each of the four models based on bigram statistics, nearly all
for different reasons. In any given run, PARSER very rarely           of the change in performance across sentence lengths was
assigned any score to a given distractor; thus it was                 caused by changes in the score assigned to distractor
necessary to run the model a large number of times on each            elements. In contrast, there was almost no change in the
different corpus in order to estimate choice probabilities            score of target items. This lack of change in target word
despite its intent to be a single-pass, online segmentation           scores seems unrealistic in a psycholinguistic model. Words
       Figure 3. Target and distractor scores for each model, normalized by highest target probability. The vertical axis is in
                                     arbitrary units; the horizontal represents sentence length.
                                                                 285

differ in their frequency and acceptability, and if models of       seems useful in models which make contact with other
segmentation are to make contact with models of the lexicon         aspects of the word learning task and should be pursued in
they should be able to take this fact into account.                 future modeling. Second, the close correspondence between
   Perhaps this difference is ultimately a difference between       the predictions of the Bayesian model and the human data
boundary-finding models and models which search for a               suggests that there may be a congruence between the
globally good lexicon (such as GGJ2006 and PARSER).                 assumptions of the model and the assumptions of the human
The performance of these latter two models is characterized         word learning system.
by a steep dip in target probabilities as sentence length
increases, corresponding to a decrease in confidence in the                             Acknowledgments
target words. However, unlike PARSER (inasmuch as we                The first author was supported by a Jacob Javits Graduate
could determine given our computational constraints),               Fellowship; this research was also supported by NSF grant
GGJ2006 also increased the probability of distractor items          #0631518. The authors would like to acknowledge Ted
as sentence length increased. This combined drop in target          Gibson and the members of Tedlab and Cocosci for helpful
probability and increase in distractor probability led to the       comments.
close fit between GGJ2006’s performance and our
participant data.
                                                                                             References
                        Conclusions                                 Brent, M. R. (1999). An Efficient, Probabilistically Sound
                                                                      Algorithm for Segmentation and Word Discovery.
We collected data on word segmentation by adults in an
                                                                      Machine Learning, 34(1), 71-105.
artificial language task in which words were presented as
                                                                    Dutoit, T., Pagel, V., Pierret, N., Bataille, F., & van der
part of unsegmented sentences. Our data showed a clear and
                                                                      Vrecken, O. (1996). The MBROLA Project: Towards a
dramatic decrease in performance as sentence length
                                                                      Set of High-Quality Speech Synthesizers Free of Use for
increased. In order to better characterize the mechanisms
                                                                      Non-Commercial Purposes. Proccedings of the
involved in segmentation, we attempted to fit a variety of
                                                                      International Conference on Spoken Language
computational models to our participants’ data. We initially
                                                                      Processing, 3, 1393-1396.
implemented three boundary-finding models based on
                                                                    Fiser, J., & Aslin, R. N. (2002). Statistical learning of new
bigram statistics: local minima in unsmoothed transitional
                                                                      visual feature combinations by infants. Proceedings of the
probability, smoothed TP, and mutual information. We
                                                                      National Academy of Sciences, 99(24), 15822-15826.
additionally tested three other models which focused not on
                                                                    Goldwater, S., Griffiths, T. L., & Johnson, M. (2006).
finding boundaries but on developing a lexicon: a clustering
                                                                      Contextual Dependencies in Unsupervised Word
model by Swingley (2005); PARSER, an online model by
                                                                      Segmentation. Proceedings of the 44th Annual Meeting of
Perruchet & Vintner (1998); and a Bayesian model by
                                                                      the Association for Computational Linguistics.
Goldwater, Griffiths, and Johnson (2006).
                                                                    Harris, Z. S. (1951). Methods in structural linguistics:
   We found that the Bayesian model—GGJ2006—achieved
                                                                      University of Chicago Press [Chicago.
a significantly higher fit to the human empirical data than
                                                                    Hauser, M. D., Newport, E. L., & Aslin, R. N. (2001).
any of the other models, reflecting the change in both target
                                                                      Segmentation of the speech stream in a human primate:
and distractor probabilities under the model across different
                                                                      statistical learning in cotton-top tamarins. Cognition, 78,
sentence length conditions. Surprisingly, the next highest-
                                                                      B53-B64.
performing models were the three based on simple bigram
                                                                    Jusczyk, P. W. (1999). How infants begin to extract words
statistics: TP, smoothed TP, and MI. These models were
                                                                      from speech. Trends in Cognitive Sciences, 3(9), 323-328.
very nearly equivalent to one another once they had been fit
                                                                    Luce, R. D. (1963). Detection and recognition. In R. D.
to the data, and they correctly predicted the decrease in
                                                                      Luce, R. R. Bush & E. Galanter (Eds.), Handbook of
performance as sentence lengths increased, although the
                                                                      Mathematical Psychology. New York: Wiley.
shape of the predicted curve did not exactly match the
                                                                    Perruchet, P., & Vinter, A. (1998). PARSER: A model for
empirical data. Finally, neither PARSER nor Swingley2005
                                                                      word segmentation. Journal of Memory and Language,
adequately fit the human data, although they failed for
                                                                      39(246-263).
different reasons. Swingley2005’s weakness was its extreme
                                                                    Saffran, J. R., Aslin, R. N., & Newport, E. L. (1996).
sensitivity to small differences in frequency combined with
                                                                      Statistical Learning by 8-Month-Old Infants. Science,
its deterministic character. PARSER’s weakness, in
                                                                      274(5294), 1926-1928.
contrast, may have been its excessive variability, which
                                                                    Saffran, J. R., Newport, E. L., & Aslin, R. N. (1996). Word
made evaluating the scores of distractor items quite difficult.
                                                                      segmentation: The role of distributional cues. Journal of
   The success of the Bayesian model in fitting our empirical
                                                                      Memory and Language, 35(4), 606-621.
data suggests several conclusions, both about modeling and
                                                                    Swingley, D. (2005). Statistical clustering and the contents
about segmentation. First, two of the lexicon-finding
                                                                      of the infant vocabulary. Cognitive Psychology, 50, 86-
models, PARSER and GGJ2006, have the property of
                                                                      132.
assigning varying scores to target words depending on the
model’s degree of confidence in that word; this property
                                                                286

