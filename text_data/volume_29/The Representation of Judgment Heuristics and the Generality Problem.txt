UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Representation of Judgment Heuristics and the Generality Problem

Permalink
https://escholarship.org/uc/item/6hv4k2dx

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)

Author
Lee, Carole J.

Publication Date
2007-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

The Representation of Judgment Heuristics and the Generality Problem
Carole J. Lee (CJLEE@Mtholyoke.Edu)
Department of Philosophy, 50 College Street
South Hadley, MA 01033 USA
Abstract

that are inappropriately characterized and should not be
used in the epistemic evaluation of belief. Second,
Gigerenzer’s critiques of Daniel Kahneman and Amos
Tversky’s judgment heuristics help to recast the generality
problem in naturalized epistemology in constructive ways.
In the first part of this paper, I will argue that naturalized
epistemologists and cognitive psychologists like Gigerenzer
have explanatory goals and conundrums in common: in
particular, both seek to explain the production and epistemic
status of beliefs by referring to the reliable cognitive
processes from which they arise; and both face the
generality problem in trying to do so. Gigerenzer helps to
address the generality problem by proposing a stricter
standard for the characterization of cognitive processes.
This stricter standard disqualifies some judgment heuristics
as candidates for the epistemic evaluation of belief. In what
follows, I will defend Gigerenzer’s criteria for the
explanatory utility of cognitive process types by contrasting
his methodology with that of Kahneman and Tversky, and
by comparing the utility of the processes to which
Gigerenzer adverts, with the judgment heuristics posited by
Kahneman and Tversky. I will argue and defend the claim
that Gigerenzer constructively recasts the generality
problem in cognitive psychology and naturalized
epistemology as the methodological problem of how
cognitive psychologists should characterize cognitive
processes for the explanation and epistemic evaluation of
beliefs.

In his debates with Daniel Kahneman and Amos Tversky,
Gerd Gigerenzer puts forward a stricter standard for the
proper representation of judgment heuristics. I argue that
Gigerenzer’s stricter standard contributes to naturalized
epistemology in two ways. First, Gigerenzer’s standard can be
used to winnow away cognitive processes that are
inappropriately characterized and should not be used in the
epistemic evaluation of belief. Second, Gigerenzer’s critique
helps to recast the generality problem in naturalized
epistemology and cognitive psychology as the methodological
problem of identifying criteria for the appropriate
specification and characterization of cognitive processes in
psychological explanations. I conclude that naturalized
epistemologists seeking to address the generality problem
should turn their focus to methodological questions about the
proper characterization of cognitive processes for the
purposes of psychological explanation.
Keywords: Naturalized epistemology; relativism; generality
problem; heuristics and biases; cognitive algorithms;
evolutionary psychology; ecological rationality.

Introduction
Pocket calculators are reliable tools for multiplying numbers
represented as Arabic numerals. However, they are
unreliable at multiplication when you type in binary
numerals expressed as sequences of zeros and ones. There is
no important mathematical difference between Arabic and
binary numerals: they are mathematically equivalent insofar
as they can be mapped onto each other one to one. But, the
calculator is good at using Arabic rather than binary
numerals because the calculator’s multiplication algorithm
is designed to work on Arabic numerals: the calculator’s
algorithm is tuned to a particular representation of the
information. The way in which the numerical information is
represented – that is, the information format – can have a
huge effect on how reliable the calculator’s computations
are.
Cognitive psychologists Gerd Gigerenzer and Ulrich
Hoffrage argue that, like a pocket calculator, the mind is
programmed with algorithms designed to work on some
representations of information and not others (1995). They
propose a stricter standard for the representation of
judgment heuristics to capture the connection between
algorithms and information formats: in order to be
descriptively and explanatorily adequate, judgment
heuristics should be characterized in a way that accounts for
how they are functionally related to information formats.
Gigerenzer and Hoffrage’s stricter standard contributes to
naturalized epistemology in two important ways. First, the
standard can be used to winnow away cognitive processes

Naturalized Epistemology and Cognitive
Psychology
Gigerenzer’s points about the descriptive and epistemic
evaluation of judgment heuristics and beliefs go hand in
hand. But, in order to appreciate this, it is important to
understand the explanatory goals and problems that
naturalized epistemologists and cognitive psychologists
have in common.

Shared Explanatory Goals
The central insight reliabilism brings to naturalized
epistemology is that the distinguishing feature justified
beliefs share is that they are causally initiated or sustained in
ways that reliably produce true beliefs. Reliabilism links
facts about the causal, functional processes underlying
beliefs with the epistemic status of beliefs. This allows the
deliverances of a reliabilist theory of justification to be
explanatory in two senses. First of all, a belief-warranting
cognitive process explains how someone has arrived at a
belief by describing the functional procedure responsible for

1211

transforming the inputs into the output belief. Second, a
belief-warranting cognitive belief-forming process explains
the epistemic status of an output-belief by referring to “the
underlying source of justificational status:” namely, the
reliability of the type of belief-forming process.
Some cognitive psychologists interested in the rationality
of judgment adopt a similar explanatory program. Cognitive
psychologists explain judgments by characterizing cognitive
functions as the processes responsible for transforming
inputs to output-beliefs. Gigerenzer’s stricter standard for
the characterization of judgment heuristics requires that
heuristics be specified to account for how the information
format of an input is functionally and causally implicated in
producing the output belief. The explanatory advantages of
his stricter descriptive standard are twofold. On the one
hand, cognitive processes so specified can explain why
judgment improves and worsens with changes in
information format. On the other hand, cognitive processes
so specified can provide concrete predictions that can be
falsified, thus allowing for the possibility of improving or
disproving a psychological explanation.
For cognitive psychologists like Gigerenzer, the epistemic
status of a belief is explained by the reliability of the
cognitive process involved. In his fast and frugal heuristics
research program, Gigerenzer seeks to discover
“ecologically rational” heuristics that exploit the
information occurring in natural environments to support a
disproportionately high frequency of true beliefs (in relation
to the total frequency of true and false beliefs) for a given
reference class (Goldstein & Gigerenzer, 2002). Gigerenzer
aims to vindicate the rationality of human judgment by
discovering and promoting ecologically rational heuristics
that reliably produce true beliefs. Gigerenzer is a reliabilist
in the sense that he invokes reliable cognitive processes to
explain both the production and epistemic status of beliefs
(Gigerenzer, 2001).

epistemic status of the beliefs to which it gives rise when
characterized at an inappropriate level of generality.
Cognitive psychologists also face the generality problem.
For any pattern of judgment, there may be multiple
cognitive processes one might invoke to explain that pattern
of judgment. For cognitive psychologists like Gigerenzer,
the determination of a belief-token’s epistemic status
depends on the reliability or validity of the underlying
cognitive process. However, cognitive psychologists
disagree over which cognitive process we should prefer in
explaining a pattern of judgment. These competing
cognitive processes have different degrees of reliability.
Thus, the epistemic status of a belief or pattern of beliefs
depends on the explanatory story one adopts: the epistemic
status of a pattern of beliefs depends entirely on how the
underlying cognitive process gets characterized.
If we can rule out a cognitive process because it fails to
meet a crucial standard of descriptive and explanatory
adequacy, then we can rule out using it in determining the
epistemic status of a token-belief. So, cognitive
psychologists can approach the generality problem by
identifying criteria that give preference to some
characterizations of cognitive processes over others. If a
cognitive process is not descriptively or explanatorily
adequate, then the supposed reliability or unreliability of
that process will not be relevant to the epistemic evaluation
of the beliefs in question.

The Representation of Judgment Heuristics
Gigerenzer’s account of how judgment heuristics ought to
be represented provides a significant criterion that both
cognitive psychologists and naturalized epistemologists can
use to prefer some cognitive process descriptions to others.
Gigerenzer’s stricter standard for the proper specification
and representation of judgment heuristics is motivated by
his frustrations with the judgment heuristics and
explanations put forward by Kahneman and Tversky’s
heuristics and biases research program.

The Generality Problem
Both reliabilists and cognitive psychologists must grapple
with the generality problem. For reliabilists, the challenge is
to figure out how to classify a belief under the most relevant
cognitive belief-forming type. Since a belief-token may be
classified under any number of belief-process types,
reliabilism leaves us with “no idea which process types are
relevant to the evaluation of any particular belief” (Feldman,
1985).
The problem is not simply that there are multiple ways of
describing the same process token. The problem is that
different process types have different degrees of reliability;
and, unless we can identify which process type is the most
relevant, we have no way of explaining or determining the
epistemic status of a belief-token. This is a “generality”
problem because the generality with which one
characterizes the cognitive process producing a belief
determines that belief’s epistemic status. It is a problem
because a proposed belief-forming type fails to capture the

The Representativeness Heuristic
In their canonical work “Judgment Under Uncertainty:
Heuristics and Biases,” Kahneman and Tversky summarized
an ambitious scope of studies demonstrating systematic
biases in human judgment. To explain these biases,
Kahneman and Tversky proposed three major heuristics, the
most debated of which is the representativeness heuristic.
The representativeness heuristic calculates the conditional
probability that A is true given B (or vice versa) on the basis
of simpler judgments about the degree to which event A is
similar to event B: when A is judged to be highly similar to
B, the conditional probability of A given B is judged to be
high (Tversky and Kahneman, 1982). Kahneman and
Tversky explain both base rate and conjunction effects by
invoking the representativeness heuristic.
To do Kahneman and Tversky’s heuristic the most justice,
I turn to the paper and Tom W. study they recently
identified as providing the strongest evidence for the

1212

representativeness heuristic” (Kahneman & Tversky, 1996).
In the experiment, the researchers use two control groups.
The similarity group was presented with a personality
description of Tom W. – a description resembling a
stereotypical engineer – and was asked to judge how similar
Tom W.’s personality description was to the typical
graduate student in nine different fields of graduate school
specialization (Kahneman & Tversky, 1982). The base-rate
group, was asked to rank the relative probability with which
a graduate student (with no personality description) would
be specializing one of those nine fields. The experimental
group, the prediction group, was given the personality
description of Tom W. and instructed to rank the fields of
graduate specialization by the probability that Tom W. is a
student of those specializations.
Kahneman and Tversky found that over 95% of graduate
students judged that Tom W. is more likely to study
computer science than humanities or education, even though
“they were surely aware of the fact that there are many more
graduate students in the latter field.” Because Kahneman
and Tversky found the correlation between judged
likelihood and similarity to be 0.97, and the correlation
between judged likelihood and estimated base rate to be 0.65, they concluded, “judgments of likelihood essentially
coincide with judgments of similarity and are quite unlike
the estimates of base rates.” They took this result as
providing “a direct confirmation of the hypothesis that
people predict by representativeness, or similarity” rather
than by base rates.
Kahneman and Tversky invoke other heuristics to explain
different cases of quantitative judgments under uncertainty.
Their availability heuristic is supposed to explain how
subjects calculate the size of a class of events or the
probability of an event according to the degree of ease with
which instances or occurrences of those event-types can be
brought to mind: “[f]or example, one may assess the risk of
heart attack among middle-aged people by recalling such
occurrences among one’s acquaintances.” And, their
anchoring heuristic is supposed to explain how subjects
calculate a value along some criterion on the basis of an
initial value or starting point suggested in the formulation of
a problem: for example, subjects seemed to use arbitrary
numbers provided in the wording of an experimental task to
estimate the percentage of African countries in the United
Nations (Tversky & Kahneman, 1974).

is as easily “explained” by saying the process is anchoring
(on the base rate)” (Gigerenzer, 1996). Because Kahneman
and Tversky’s heuristics are underspecified, they can be
selectively invoked to explain nearly any kind of judgment:
if one heuristic fails to account for an observed pattern of
judgment, another can be invoked to save the day. As a
result, theorists have plenty of flexibility to selectively
invoke heuristics in ways that “resist attempts to prove,
disprove, or even improve them” (Gigerenzer, 1996).
Kahneman and Tversky’s heuristics explain “[t]oo little
because we do not know when these heuristics work and
how” (Gigerenzer, 1996). In particular, they say nothing
about how heuristic processes are functionally and causally
tuned to specific representations of information. Thus, they
lend no help in explaining why judgments vary across
information formats.
Gigerenzer seeks a new level of specificity and
explanatory power for heuristics proposed in contemporary
psychological theorizing. The stricter standard looks
something like this: psychological explanations should
invoke process models of cognition that specify how the
processes functionally relate to the representation of
information (i.e., information formats).
This stricter standard accommodates important insights
Gigerenzer brings to bear on the relationship between the
representation of judgment heuristics and psychological
explanation (Feynman, 1967). The primary lesson
Gigerenzer draws is this: two judgment heuristics or
algorithms may be mathematically or computationally
equivalent, yet be psychologically different. The cognitive
psychologists’ task, then, is to discover which (if any) of
these algorithms can be found in the human mind.
The second, connected insight is that two different
representations of numerical or statistical information may
be equivalent, yet be psychologically different. If the mind
has an algorithm tuned to a particular information format,
then human judgment might make good use of one
representation of the information and not the other
(Gigerenzer, 1996). For example, numerical information can
be represented in Roman, Arabic, and binary systems.
Although these representations are mathematically
equivalent, they are not psychologically equivalent: we have
learned to calculate arithmetic algorithms under only
particular representations of numerical information. To
underscore this point, Gigerenzer and Hoffrage ask the
reader to “[c]ontemplate for a moment long division in
Roman numerals” (1995).
In order to be properly specified, cognitive algorithms and
judgment heuristics must be characterized so that they are
tuned to specific representations of information in the way
that algorithms in a pocket calculator are tuned to Arabic
numerals. Cognitive algorithms specified in this way
provide legitimate explanations because they do not explain
too much or too little. They do not explain “too much”
because their specific conditions of use cannot be fitted to
any experimental result. Nor do they explain “too little:”
they provide detailed conditions of use that explain when

Heuristics and Biases: Heuristic Explanations
The problem with Kahneman and Tversky’s heuristics, as
Gigerenzer sees it, is that they are too vague to provide
legitimate psychological explanations: the heuristics are
vague with respect to the kind of inputs and information
formats required by the heuristics. As a result, the heuristics
“at once explain too little and too much.” They explain too
much because “post hoc, one of them can be fitted to almost
any experimental result. For example, base-rate neglect is
commonly attributed to representativeness. However, the
opposite result, overweighting of base rates (conservatism),

1213

the heuristic process can be used and how the heuristic
process transforms input-information into output-judgments.

tasks were stated in terms of frequencies rather than
percentages.
Gigerenzer’s frequency algorithm is preferable over
Kahneman and Tversky’s representativeness heuristic
because it is descriptively and explanatorily more adequate.
It does not explain “too much:” it cannot be fitted to any
experimental result because it has specific conditions of use
(statistical information must be represented in terms of
frequencies for the frequency algorithm to work). Nor does
the algorithm explain “too little:” it provides detailed
conditions of use that explain when the heuristic process
gets used (i.e., when the right frequency information is
provided). Finally, Gigerenzer and Hoffrage’s judgment
heuristic predicts and explains why probability judgments
improve when probability information is represented as
frequencies rather than single-event probabilities. In
contrast, the representativeness heuristic cannot explain this
information format effect on probability judgment.
Gigerenzer and Hoffrage’s cognitive process, which
conforms to the stricter standard they propose, has the
explanatory advantage of being able to account for
information format effects.

The Frequency Algorithm
Gigerenzer and Hoffrage’s proposed judgment heuristic
meets this stricter standard for the representation of
judgment heuristics and psychological explanation. As a
result, their judgment heuristic is capable of doing
something Kahneman and Tversky’s representativeness
heuristic could not: viz., explain why probability judgments
vary with different kinds of information formats.
The fact that Kahneman and Tversky demonstrated base
rate and conjunction effects with a particular kind of
information format does not imply that the human mind has
no judgment heuristics capable of performing conditional
probability calculations. In order to discover whether
humans have cognitive algorithms that carry out conditional
probability judgments, Gigerenzer and Hoffrage surmise
that the kind of probabilistic information to which cognitive
algorithms could be tuned were not single-event
probabilities or percentages, but the “sequential encoding
and updating of event frequencies without access or
reference to the base rate” (Gigerenzer & Hoffrage, 1995).
To illustrate, they ask the reader to suppose there is a
physician in an illiterate society who has discovered a
symptom that signals a severe disease that has begun to
afflict her people: In her lifetime, she has seen 1,000 people,
10 of whom had the disease. Of those 10, 8 showed the
symptom; of the 990 not afflicted, 95 did. A new patient
appears. He has the symptom. What is the probability that
he actually has the disease? To calculate the conditional
probability that the patient has the disease given the
symptom, all she needs is the number of cases that had both
the symptom and disease (8) and the number of symptom
cases (8 + 95):
The Bayesian algorithm for computing the posterior
probability p(H|E) from the frequency format involves
solving the equation:
e&h
8
p(H | E ) =
=
(e & h )+ (e & !h ) 8 + 95
where e & h (evidence and hypothesis) is the number of
cases with symptom and disease, and e & –h is the number
of cases having the symptom but lacking the disease. This
algorithm is formally equivalent to Bayes Rule in the sense
that it produces the same conditional probability solutions
(population statistics) for fixed samples. However, the
algorithm is computationally simpler and does not require
explicit reference to base rate information.
If humans have a judgment heuristic like the frequency
algorithm and not like the traditional formulation of Bayes’
law, then we would expect conditional probability
judgments to improve when probability problems are stated
in terms of frequencies of e, h, and -h rather than in terms of
single-event probabilities or percentages that include
explicit base rate information. Gigerenzer and Hoffrage
discovered that correct conditional probability judgments
jumped from 16% to 50% when conditional probability

Judgment Heuristics and Naturalized
Epistemology
Gigerenzer proposes a stricter standard for what counts as
explanatorily adequate cognitive processes, and thus
provides a criterion we can use in winnowing away
descriptions of cognitive process types relevant to the
epistemic
evaluation
of
belief.
Because
the
representativeness heuristic fails to meet the appropriate
descriptive and explanatory standard put forward by
Gigerenzer, it fails to stand as a candidate cognitive process
type in the epistemic evaluation of belief.
Ruling out the representativeness heuristic in this way
suggests a more general strategy for coping with the
generality problem in cognitive psychology and naturalized
epistemology: cognitive psychologists can decide what
cognitive processes are most relevant in the epistemic
evaluation of belief by creating stricter standards for what
counts as a legitimate or adequate cognitive process from an
explanatory point of view. This would make methodological
debates about how cognitive processes ought to be specified
directly relevant to naturalized epistemologists’ questions
about the epistemic evaluation of cognitive processes and
beliefs.

Reliability versus Content-Blind Norms
Some might object that the normative appropriateness of
using the frequency algorithm in the experimental tasks is
grounded, not on the reliability of the frequency algorithm,
but on the judgment’s conformance to Bayes Rule.
In response, I would argue that there is nothing about the
functional characterization of the frequency algorithm by
itself that guarantees that one’s acquired sample data
appropriately or reliably grounds statistical claims about a
new sample.

1214

The normativity of the frequency algorithm is not built
into the algorithm itself, but gets determined, in part, by its
fit with the environment or context of use. I agree with
Gigerenzer that a priori rules of probability and statistics
should not be applied to concrete situations in a contentblind way since doing so disregards the relevant structural
properties of the given situation. Instead, norms need to be
constructed and justified for a specific class of situations
(Gigerenzer, 2001).
I think that the more pressing epistemic question to ask is
this: what is the nature of the normative “fit” between the
frequency algorithm and the experimental task’s context of
use? For Gigerenzer, the normative appropriateness of the
“fit” between the frequency algorithm and the experimental
task could be construed in either of two ways.
On the one hand, the “fit” between the frequency
algorithm and the experimental task could have to do with
the statistical validity of using Bayes Rule in the task’s
context of use (Gigerenzer, 2001). Subjects were presented
a mammography problem asking them to calculate the
relative frequency with which “a new representative
sample” of forty year old women will have cancer on the
basis of the following acquired sample of forty year old
women: 103 out of every 1,000 women had a positive
mammography, but only 8 of these women actually turned
out to have breast cancer (Gigerenzer & Hoffrage, 1995).
The problem is set in a medical context where the reference
class is clearly specified and the samples (it is reasonable to
presume) were randomly selected and sufficiently large for
the purpose of making predictions about new cases. So,
Gigerenzer
might
suggest
that
the
normative
appropriateness of using Bayes Rule here has to do with the
concrete specification of reference classes in the
experimental task, the representativeness of the new sample,
and the quality of the samples involved (Gigerenzer, 2001).1
On the other hand, Gigerenzer could construe the
frequency algorithm’s “fit” with the environment or context
as an ecologically rational one. Ecologically rational
heuristics manipulate the information structure in the
environment to support a disproportionately high frequency
of true beliefs in relation to the total frequency of true and
false beliefs for a given reference class (Goldstein &
Gigerenzer 2002). The ecological rationality of a heuristic is
not determined a priori but is contingent upon its accuracy
for the information environment in which it operates (Todd
& Gigerenzer, 2000).
Ecologically rational cognitive processes reliably produce
true beliefs. And, a reliabilist account of justification would
prefer ecological rationality’s way of characterizing the
normativity and fit of the frequency algorithm with the
experimental tasks involved. There are empirical and
epistemic advantages to this position. Modeling the

algorithm’s ecological rationality for different contents and
contexts of reasoning might provide surprising predictions
about the conditions in which the reliability of judgment
improves or degrades in different environments (Todd &
Gigerenzer, 2000). Such work would open up the possibility
of discovering contents and contexts for which the
reliability of the frequency algorithm does not improve or
degrade with changes in sample size or sampling technique
due to features of the reference class objects or the
environment’s information structure.

Is Gigerenzer’s Standard Too Strict?
It might be objected that the explanatory standard
Gigerenzer proposes is too strong for psychological
theorizing. Kahneman and Tversky’s heuristics are couched
at a level of generality common in psychology. Consider,
for example, psychological explanations for cognitive
dissonance and stereotype threat. The cognitive dissonance
literature explains the resistance to adopt a belief in terms of
the degree to which that belief is dissonant or
psychologically uncomfortable for a person given her
beliefs and actions (Festinger, 1957). But, how it is that
cognitive dissonance is triggered in the mind is left
unspecified. More recently, in the stereotype threat
literature, psychologists have proposed that the implicit
activation of a sociocultural stereotype influences the
performance of the stereotyped individual (Shih, Pittinsky &
Ambady, 1999). How the sociocultural stereotype gets
activated within the mind is left unspecified.
Indeed, even Gigerenzer’s fast and frugal heuristics suffer
from these forms of underspecification. For example, the
recognition heuristic, “the most frugal” of all the fast and
frugal heuristics, directs one to choose the recognized object
when one of a set of objects is recognized, while the others
are not (Goldstein & Gigerenzer, 2002). However, the
conditions under which the recognition heuristic or
recognition memory gets “triggered” remains undefined.
Until the recognition heuristic becomes more refined, we
have no way of explaining why individuals do not use
recognition to decide which German city has the larger
handball team or lies farther West.2 Nor do we have an
explanation for why individuals use the recognition
heuristic rather than some other fast and frugal heuristic in
the “adaptive toolbox” for a specific judgment or task
(Cooper, 2000).
Gigerenzer recognizes the value of these somewhat
vague, yet informative, psychological explanations. With
respect to heuristics proposed in the heuristics and biases
research program, he says, “[i]t is understandable that when
heuristics were first proposed as the underlying cognitive
processes in the early 1970s, they were only loosely
characterized.” His complaint is that “25 years and many
experiments later, explanatory notions such as
representativeness remain vague, undefined, and

1

If the frequency algorithm refers to an existing cognitive
process that is tuned to frequency representations of probabilistic
information, then we should also expect subjects to rely on the
frequency algorithm in statistically inappropriate contexts. Thanks
to Jay Garfield for pointing out this empirical implication.

2

Thanks to Norbert Schwarz and Hilary Kornblith respectively
for these examples.

1215

unspecified with respect both to the antecedent conditions
that elicit (or suppress) them and also to the cognitive
processes that underlie them” (Gigerenzer, 1996).
Gigerenzer’s critique, then, is not that psychological
explanations invoking underspecified cognitive processes
are not useful in the course of theoretical development and
progress. Rather, the concern is that researchers should – in
the design of their experimental tasks – aim to meet a
stricter standard in the specification of cognitive processes
for the sake of achieving determinate, falsifiable
psychological explanations.

should we decide such matters? For reliabilists, these are
intriguing and crucial questions to attend to.

Acknowledgements
This paper has greatly benefited from comments by Peter
Railton and Jay Garfield.

References
Cooper, R (2000). Simple Heuristics Could Make Us Smart:
But Which Heuristics Do We Apply When? Behavioral
and Brain Sciences, 23, 746.
Cosmides, L. & Tooby, J. (1996). Are Humans Intuitive
Statisticians After All? Rethinking Some Conclusions
from the Literature on Judgment Under Uncertainty.
Cognition, 58, 1-73.
Feldman, R. (1985). Reliability and Justification. The
Monist, 68, 159-174.
Festinger, L. (1957). A Theory of Cognitive Dissonance.
Evanston, IL: Row, Peterson.
Feynman, R. (1967). The Character of Physical Law.
Cambridge, MA: The MIT Press.
Gigerenzer, G. (1991) How to Make Cognitive Illusions
Disappear: Beyond “Heuristics and Biases.” European
Review of Social Psychology, 2, 83-115.
Gigerenzer, G. (1996). On Narrow Norms and Vague
Heuristics: A Reply to Kahneman and Tversky (1996).
Psychological Review, 103(3), 592-596.
Gigerenzer, G. (2001). Content-blind Norms, No Norms, or
Good Norms? A Reply to Vranas. Cognition, 81, 93-103.
Gigerenzer, G., & Hoffrage, U. (1995). How to Improve
Bayesian Reasoning Without Instruction: Frequency
Formats. Psychological Review, 102(4), 684-704.
Goldstein, D. G., & Gigerenzer, G. (2002). Models of
Ecological Rationality: The Recognition Heuristic.
Psychological Review, 109(1), 75-90.
Kahneman D., & Tversky, A. (1982). On the Psychology of
Prediction (1973). In D. Kahneman, P. Slovic & A.
Tversky (Eds.), Judgment Under Uncertainty: Heuristics
and Biases (pp. 48-68). New York, NY: Cambridge
University Press.
Kahneman, D., & Tversky, A. (1996). On the Reality of
Cognitive Illusions. Psychological Review, 103(3), 582591.
Shih, M., Pittinsky, T. L. & Ambady, N. (1999). Stereotype
Susceptibility: Identity Salience and Shifts in Quantitative
Performance. Psychological Science, 10(1), 80-83.
Todd, P. & Gigerenzer, G. (2000). Precis of Simple
Heuristics that Make Us Smart. Behavioral and Brain
Sciences, 23, 727-741.
Tversky, A., & Kahneman, D. (1974). Judgment Under
Uncertainty: Heuristics and Biases. Science, 185, 11241131.
Tversky, A., & Kahneman, D. (1982). Judgments of and by
Representativeness. In D. Kahneman, P. Slovic & A.
Tversky (Eds.), Judgment Under Uncertainty: Heuristics
and Biases (pp. 84-98). New York, NY: Cambridge
University Press.

Conclusions
Naturalized epistemologists and cognitive psychologists
share explanatory goals that intimately tie together the
explanation and epistemic evaluation of belief. And, in
explaining the production and epistemic status of beliefs,
naturalized epistemologists and cognitive psychologists
both face the generality problem. I have argued that
naturalized epistemologists and cognitive psychologists
need to address the generality problem by focusing on the
methodological question of how cognitive processes should
be characterized for the sake of psychological explanation. I
analyzed how Gigerenzer and Hoffrage rule out the
representativeness heuristic’s relevance to the epistemic
evaluation of belief by invoking a stricter standard for the
representation of judgment heuristics. And, I defended
Gigerenzer and Hoffrage’s stricter standard by contrasting
the utility of processes proposed by Gigerenzer to those
proposed by Kahneman and Tversky.
Evolutionary psychologists like Gigerenzer have called
for a return to seeing humans as good intuitive statisticians
(Cosmides & Tooby, 1996; Gigerenzer, 1991). Gigerenzer
has been especially vociferous in arguing for a shift in the
disciplinary focus towards rational cognitive processes: he
suggests that “after 40 years of toying with the notion of
bounded rationality, it is time to overcome the opposition
between the rational and the psychological and to reunite the
two” (Gigerenzer & Goldstein, 1996). The shift towards
identifying conditions and cognitive processes underlying
rational (rather than irrational) judgment is a boon for
naturalized epistemologists interested in identifying reliable
strategies of reasoning.
However, Gigerenzer’s work also brings with it new
conundrums. Gigerenzer and Daniel Goldstein provide an
interesting model to measure the ecological rationality or
reliability of the recognition heuristic for different contexts
of reasoning. However, as I argued above, the recognition
heuristic is not sufficiently specified to provide a
determinate psychological explanation – at least, not
according to Gigerenzer’s stricter standard.
This inconsistency in approach raises many questions.
What are we doing when we seek to evaluate the epistemic
status of a cognitive process that is not yet adequately
specified from an explanatory point of view? What does it
mean to have a sufficiently specified cognitive process or a
determinate explanation in cognitive psychology? And, how

1216

