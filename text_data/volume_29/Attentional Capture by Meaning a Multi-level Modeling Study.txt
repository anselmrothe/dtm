UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Attentional Capture by Meaning, a Multi-level Modeling Study
Permalink
https://escholarship.org/uc/item/9hk8q667
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Su, Li
Bowman, Howard
Barnard, Philip
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                  Attentional Capture by Meaning, a Multi-level Modelling Study
                               Li Su, Howard Bowman (ls68@kent.ac.uk, hb5@kent.ac.uk)
                        Centre for Cognitive Neuroscience and Cognitive Systems, University of Kent, UK
                                    Philip Barnard (philip.barnard@mrc-cbu.cam.ac.uk)
                                       MRC Cognition and Brain Sciences Unit, Cambridge, UK
                              Abstract                                Kerszberg et al. 1998). These two levels of explanation
                                                                      really reflect different capacities to observe systems; that is,
   We present a computational study of attentional capture by
   meaning, based on Barnard et al's key-distractor attentional       the extent to which the system is viewed from outside or
   blink task. We highlight a sequence of models, from an             inside, i.e., as a black or white box. There are clear pros and
   abstract black-box to a structurally detailed white-box model.     cons to these forms of modelling, which we discuss now.
   Each of these models reproduces the major findings from the
   key-distractor blink task. We argue that such multi-level          Black-box (Extensionalist) Modelling. With this approach,
   modelling gives greater confidence in the theoretical position     no assumptions are made about the internal structure of the
   encapsulated by these models.                                      system and there is no decomposition at all of the black-box
   Keywords: Attentional blink; LSA; semantic modulation;
                                                                      into its constituent components. Thus, the point of reference
   multi-level modeling.                                              for the modeller is the externally visible behaviour, e.g. the
                                                                      stimulus-response pattern. That is, such models are
                         Introduction                                 extensionalist in nature. A critical benefit of black-box
                                                                      modelling is that a minimal set of assumptions are made,
There are now many different approaches to the
                                                                      especially in respect of the system structure. Consequently,
computational modelling of cognition, e.g. symbolic models
                                                                      there are less degrees of freedom and fewer hidden
(Newell 1990; Kieras and Meyer 1997), cognitive
                                                                      assumptions, making data fitting and parameter setting both
connectionist models (McLeod, Plunkett et al. 1998) and
                                                                      well founded and, typically, feasible. For example, if the
neurophysiologically prescribed connectionist models
                                                                      system can be described in closed form, key parameters can
(O'Reilly and Munakata 2000). The relative value of
                                                                      be determined by solving a set of equations, if not,
different approaches is a hotly debated topic, with each
                                                                      computational search methods can be applied.
presented as an alternative to the others, suggesting that they
are in opposition to one another, e.g. (Fodor and Pylyshyn            White-box (Intensionalist) Modelling. In contrast, the
1988; Hinton 1990). However, another perspective is that              internal (decompositional) structure of the system is
these reflect different levels of abstraction / explanation of        asserted with this approach. That is, such models are
the same system that are complementary, rather than                   intensionalist in nature. Although we can bring theories of
fundamentally opposed.                                                cognitive architecture and (increasingly) neural structure to
   Computer science, which has often been used as a                   bear in proposing white-box models, a spectrum of
metaphor in the cognitive modelling domain, gives a clear             assumptions (necessarily) needs to be made. Furthermore,
precedent for thinking in terms of multiple views of a single         typically, many of these assumptions concern the internal
system. An illustration of this is what is now probably the           structure of the system. While structurally detailed models
most widely used design method, the Unified Modelling                 of cognition are likely to be the most revealing (especially
Language (UML) (Booch, Rumbaugh et al. 1999). It is not               with the current emphasis on neurophysiological correlates),
that this perspective has been completely lost on cognitive           deduction from these models is more slippery and
scientists; indeed, Marr famously elaborated a version of             potentially less well founded. Most importantly, many
this position in his three levels of cognitive description            assumptions, such as settings of key parameters, need to be
(Marr 2000). However, despite Marr's observations,                    made, many of which may, at best, require complex
concrete modelling endeavours rarely, if ever, consider               justification and, at worst, be effectively arbitrary. As a
multiple abstraction levels in the same context and                   result, parameter setting and data fitting is more difficult
particularly how to relate those levels.                              and, arguably, less well founded with white-box models.
                                                                         We can summarise then by saying that black-box
         Multiple Level Cognitive Modelling                           modelling describes what a cognitive system does and it
In this paper, we can distinguish between the following two           describes it in a relatively contained and well-founded
levels of explanation of a cognitive phenomenon. Firstly,             manner. However, white-box modelling cannot be ignored,
high-level abstract descriptions of the mathematical                  since it enables us to describe how a cognitive system
characteristics of a pattern of data, e.g. (Stewart, Brown et         functions, which is a concern for both traditional
al. 2005). Secondly, low-level detailed models of the                 information processing and more recent neurophysiological
internal structure of a cognitive system, e.g. (Dehaene,              explanations. Thus, when tackling the computational
                                                                      modelling of a particular cognitive phenomenon, one should
                                                                  1521

                    (a)                           H u m a n D a ta                                             (b)           Ex te n sio n a list M o d e l - D a ta F i tti n g
                    80%                                                                                        80%
                                                                                                                                                                                                H S - C o rre c t ID .
                                                                                                                                                                                                HS - "No"
                    70%                                                                                        70%
                                                                                                                                                                                                HS - "Y es "
                    60%                                                                                        60%                                                                              LS - C o rrec t ID .
                    50%                                                                                        50%                                                                              LS - " N o "
       Proportion                                                                                 Proportion
                                                                                                                                                                                                LS - " Y e s "
                    40%                                                                                        40%
                    30%                                                                                        30%
                    20%                                                                                        20%
                    10%                                                                                        10%
                     0%                                                                                         0%
                          0   1       2       3        4      5       6       7      8   9   10                      0   1        2        3       4        5       6         7    8   9   10
                                                            L ag                                                                                          Lag
                    (c)           In te rm e d i a te M o d e l - In tr in sic I D                             (d)                 I n te n sio n a li st M o d e l - L S A
                    80%                                                                                        80%
                    70%                                                                                        70%
                    60%                                                                                        60%
                    50%                                                                                        50%
       Proportion                                                                                 Proportion
                    40%                                                                                        40%
                    30%                                                                                        30%
                    20%                                                                                        20%
                    10%                                                                                        10%
                     0%                                                                                         0%
                          0   1       2       3        4      5       6       7      8   9   10                      0   1        2        3       4       5        6         7    8   9   10
                                                            Lag                                                                                           L ag
  Figure 1 Proportion of different types of responses. HS and LS denote high and low salient condition respectively;
  Correct ID denotes correct report of target identity. “Yes” denotes response if subject was confidant a job word had
  been there but could not say exactly what it was. “No” denotes responses if subject did not see a target.
start with an abstract black-box analysis of the observable                                                                  presented at fixation in Rapid Serial Visual Presentation
behaviour arising from the phenomenon. Then, from this                                                                       (RSVP) format, at around 10 items per second. Targets were
solid foundation, one could develop increasingly refined and                                                                 only distinguishable from background items in terms of
concrete models, in a progression towards white-box                                                                          their meaning. This variant of the paradigm did not rely on
models. Importantly though, this approach enables cross                                                                      dual target report. Rather, participants were simply asked to
abstraction level validation, showing, for example, that the                                                                 report a word if it refers to a job or profession for which
white-box model is correctly related to the black-box model.                                                                 people get paid, such as waitress, and these targets were
   This paper provides an initial step in the direction of                                                                   embedded in background words that all belonged to the
multilevel cognitive modelling. In particular, the refinement                                                                same category, e.g. nature words. However, streams also
we present is more from black to dark-gray, then to light-                                                                   contained a key-distractor item, which, although not in the
gray! More complete instantiation of our approach awaits                                                                     target category, was semantically related to that category.
further theoretical work on how to relate the sorts of models                                                                The serial-position that the target appeared after the key-
developed in the cognitive modelling setting.                                                                                distractor was varied.
   A key contribution of the article will be the identification                                                                 Participants could report the target word (accurate report),
of analogous parameter manipulations in all the three                                                                        say “Yes” if they were confident a job word had been there
models. These cross-model relationships effectively serve as                                                                 but could not say exactly what it was, or say “No” if they
a verification that the theoretical claims we make of our                                                                    did not see a target, and there were trials on which no target
most intensionalist model are well-founded.                                                                                  was presented. When key-distractors were household items,
                                                                                                                             a different category from both background and target words,
                     Key-distractor Attentional Blink                                                                        there was little influence on target report. However, key-
We illustrate our approach in the context of a study of                                                                      distractors that referenced a property of a human agent, but
temporal attention. To do this, we reproduce data on the                                                                     not one for which they were paid, like tourist or husband,
key-distractor attentional blink task (Barnard, Scott et al.                                                                 gave rise to a classic and deep blink as shown in Figure 1a
2004), which considers how attention is drawn to                                                                             (e.g. the HS - Correct ID curve) & Figure 2b. The horizontal
semantically salient items. A particular reason for focusing                                                                 axis denotes lag, which indicates the serial position of the
on this task is that it maps out the profile of attentional                                                                  target relative to the key-distractor. The vertical axis denotes
capture by meaning over time. This is encapsulated in the                                                                    the proportion of each types of responses used. Thus,
serial position curve; see Figure 1.                                                                                         (Barnard, Scott et al. 2004) showed that the level of salience
   In order to examine semantic effects, (Barnard, Scott et al.                                                              of the key-distractor, i.e. how related it is to the target
2004) used a variant of the Attentional Blink (AB) paradigm                                                                  category, modulates how strongly attention is captured. In
in which no perceptual features were present to distinguish                                                                  this paper, we will concentrate on quantitatively modeling
targets from background items. In this task, words were                                                                      this key effect that semantic similarity modulates the blink
                                                                                                                     1522

(a)                                                 (b)                                                    baseline; and y(x) denotes the GD, which also has
                                                            80%
Internal Structure of the Two Sub-systems                   70%                                            parameters. However, b is the only parameter that changes
Model with Buffer at the Implicational Sub-                 60%
                                                                                                           significantly when different key-distractors are used in the
                                               Proportion
                  system                                    50%
                                                                      Pimp ( ¬LS ∧ Targ ) = 0.54
                                                            40%
                                                            30%
                                                                                                           experiment. The function becomes the baseline if b is 0, i.e.
RSVP
                                                            20%
                                                                       Pimp ( ¬HS ∧ Targ ) = 0.34
                                                                                                           complete absence of the blink and baseline performance at
           Buffer                       WM                  10%
                                                                                                           all lags. Hence, we argue that b is related to salience of the
 …                    …                …                     0%
                                                                  0 1 2 3 4 5 6 7 8 9 10                   key-distractor and thus characterises the attentional capture
                                                                                Lag
         Implic            Prop                                                                            by salience effect we are interested in.
                                                                    Human - HS             Human - LS
                                                                                                              A simple search of the parameter space has proved
(c)                                                                                                        sufficient to yield a good fit to the experimental data. We
                      Implicational Salience                                                               show this fit in Figure 1b. Note, the ratio of the b parameter
                      Assignment Threshold       Job word category
High salience key-                                                                                         between low and high salient conditions is around
distractor category                                                         Low salience key-
                                                                            distractor category            0.4 / 0.9 ≈ 0.44 . Moreover, the GD shape parameter is
                                                                                                           relatively small for all curves. This suggests that the blink
                                                                                                           curves are asymmetrical. It will become clear that this
                                                                                                           relationship is consistent among our different models.
Propositional Salience                                                                                         Intermediate Model – Intrinsic Identification
Assignment Threshold                                                        Background word
                                                                            category                       In this section, we model the internal structure of the system
                                                                                                           as shown in Figure 2a. Three principles underlie our model:
Figure 2 (a) Internal structure. (b) Target report accuracy                                                sequential processing, 2-stages and serial allocation of
by lag in humans for high and low salient key-distractors                                                  attention. We discuss these principles in turn.
with intrinsic identifications. (c) Salience assignment.
                                                                                                           Sequential Processing. With any RSVP task, items arrive
Semantics in LSA are expressed in a high dimensional
                                                                                                           in sequence and need to be correspondingly processed.
space. This illustration is 2D for ease of depiction.
                                                                                                           Thus, we require a basic method for representing this
depth as shown in Figures 1a & 2b, and present both (black-                                                sequential arrival and processing of items. At one level, we
box) extensionalist and (white-box) intensionalist models.                                                 can view our approach as implementing a pipeline. New
                                                                                                           items enter the front of the pipeline from the visual system;
                                                                                                           they are then fed through until they reach the back of the
           Extensionalist Model – Data Fitting
                                                                                                           pipeline, where they enter working memory (WM). Every
The most extensionalist approach begins with behavioural                                                   cycle, a new item enters the pipeline and all items currently
data from Barnard’s key-distractor task. Accordingly, this                                                 in transit are pushed along one place. The key data structure
model fits the behavioural data using a closed-form                                                        that implements this pipeline metaphor is a delay-line as
equation. This approach has been applied to almost every                                                   shown in Figure 2a. It could also be viewed as a symbolic
branch of science in order to characterise the observed                                                    analogue of a sequence of layers in a neural network; a
behaviour and formulate mathematical models of the                                                         particularly strong analogue being with synfire chains
underlying mechanisms. This technique has also been                                                        (Abeles, Bergman et al. 1993). It is a very natural
widely used in modelling response time distributions (Van                                                  mechanism to use in order to capture the temporal properties
Zandt 2000) and, more recently, in modelling serial position                                               of a blink experiment, which is inherently a time
curves of AB tasks (Cousineau, Charbonneau et al. 2006).                                                   constrained order task.
  In our context of exploring the key-distractor AB task, the
human data has a sharp blink onset and shallow recovery as                                                 2-Stages. Like (Chun and Potter 1995; Bowman and Wyble
shown in Figure 1a (e.g. the HS - Correct ID curve) &                                                      2007), (Barnard, Scott et al. 2004) and (Barnard and
Figure 2b. This shape matches an inverted Gamma                                                            Bowman 2004) argued for a two-stage model, but this time
distribution (GD). (Note, there is a shape parameter in the                                                recast to focus exclusively on semantic analysis and
GD, which determines the skewness of the distribution.                                                     executive processing. In particular, (Barnard and Bowman
Increasing the shape parameter, moves the GD towards a                                                     2004) modelled the key-distractor blink task using a two-
normal distribution; decreasing it, moves the GD towards an                                                stage model. In the first stage, a generic level of semantic
exponential distribution.) Hence, we use the following                                                     representation is monitored and initially used to determine if
equation to model our AB curves.                                                                           an incoming item is salient in the context of the specified
                                                                                                           task. If it is found to be so, then, in the second stage, the
                                  f ( x) = a + b ⋅ y ( x)                                                  specific referential meaning of the word is subjected to
                                                                                                           detailed semantic scrutiny; thus, a word’s meaning is
where x denotes lag; a is the baseline parameter, which sets                                               actively evaluated in relation to the required referential
baseline performance and, thus, performance following                                                      properties of the target category. If this reveals a match,
blink recovery; b is the depth parameter, which sets the                                                   then the target is encoded for later report. The first of these
difference between the deepest point of the blink and the                                                  stages is somewhat akin to first taking a “glance” at generic
                                                                                                        1523

meaning, with the second akin to taking a closer “look” at             When Prop is buffered and detects an implicationally
the relationship to the meaning of the target category. These       uninterpreted word, the buffer is passed back to Implic,
two stages are implemented in two distinct subsystems as            which can assign salience to its items again. After this,
shown in Figure 2a: the implicational subsystem or Implic           target words entering the system will be detected as
and the propositional subsystem or Prop (Barnard 1999).             implicationally and propositionally salient and thus will be
(We consider how these subsystems fit into a larger                 reported. Hence, the blink recovers.
cognitive framework, ICS, in the conclusion.)
                                                                    Generating a Blink Curve. Humans though perceive
   These two subsystems process qualitatively distinct types
                                                                    information imperfectly; as a result, salient items may be
of meaning. One, implicational meaning, is holistic, abstract
                                                                    missed. In the current model, we assume that the ease of
and schematic, and is where affect is represented and
                                                                    detecting that the key-distractor is implicationally salient
experienced (Barnard 1999). The other is classically
                                                                    determines the depth of the blink curve. We work here with
“rational”, being based upon propositional representation,
                                                                    what we call “intrinsic probabilities of identification”, i.e. if
capturing referentially specific semantic properties and
                                                                    an item (distractor or target) is presented alone in an RSVP
relationships. Semantic errors make clear that sometimes we
                                                                    stream, what is the probability that it will be seen. Thus,
only have (referentially non-specific) semantic gist
information available to us, e.g. the Noah illusion illustrates      Pimp ( Dist ∧ Targ ) is not the probability that both the key-
implicational meaning (Erickson and Mattson 1981).                  distractor and target are seen in an AB setting, but rather the
Serial Allocation of Attention. Our third principle is a            probability that both would be seen in two separate idealised
mechanism of attentional engagement. It is only when                “single target events”. The intrinsic probability of judging
attention is engaged at a subsystem that it can assess the          targets to be implicationally salient, Pimp (Targ ) = 0.67 , is set
salience of items passing through it. Furthermore, attention        by the baseline performance of human subjects. (Barnard et
can only be engaged at one subsystem at a time.                     al stated that humans correctly report the target’s identity on
Consequently, semantic processes cannot glance at an                average on 67% of target only trials; furthermore, at high
incoming item, while looking at and scrutinising another.           lags, the blink curve also recovers to this baseline
This constraint will play an important role in generating a         performance (Barnard, Scott et al. 2004).) We assume that
blink in our models. When attention is engaged at a                 the intrinsic probability of detecting a background word as
subsystem, we say that it is buffered (Barnard 1999). (In the       implicationally salient, Pimp (Back ) , is zero. (This sort of
context of this paper, the term buffer refers to a moving
                                                                    error is so rare as to be effectively zero.) The intrinsic
focus of attention.) Thus, salience assignment can only be
                                                                    probability of detecting a key-distractor as implicationally
performed if the subsystem is buffered and only one
                                                                    salient is Pimp (HS ) in the high salient condition and
subsystem can be buffered at a time as shown in Figure 2a.
The buffer mechanism ensures that the central attentional            Pimp (LS ) in the low salient condition. According to our
resources are allocated serially, while items pass                  model, the likelihood of correct report at the deepest point in
concurrently, i.e. all items throughout the overall delay-line      the blink curve reflects the joint probability of missing the
are moved on one place on each time step.                           key-distractor and detecting the target. This is because the
How the Model Blinks. In this model, words are expressed            way the model is constructed, there is indeed no other way
by their roles in Barnard et al’s blink task, i.e. background,      that a target can be detected during the blink. From Figure
target, and key-distractor, which has two subtypes: high            2b, Pimp (¬HS ∧ Targ ) = 0.34 and Pimp (¬LS ∧ Targ ) = 0.54
salient and low salient. The buffer movement dynamic                can be obtained. We assume detecting targets and the key-
provides the underlying mechanism for the blink.                    distractors are independent, in particular, in both cases we
   Initially, Implic is buffered as shown in Figure 2a. When,       assume the buffer is at Implic when the assessment is made.
in response to the key-distractor being found implicationally       So, Pimp ( HS ) = 0.49 and Pimp ( LS ) = 0.19 .
salient, the buffer moves from Implic to Prop, salience
                                                                       This calculation quantitatively determines how the model
assessment cannot be performed on a set of words (i.e. a
                                                                    generates a blink curve. As a reflection of the relatively high
portion of the RSVP stream) entering Implic following the
                                                                    level of abstraction of this model, randomness is imposed
key-distractor. So, when these implicationally uninterpreted
                                                                    globally and externally using a convolution. This technique
words are passed to Prop, propositional meaning (which
                                                                    does not require specification of either the dynamics or the
builds on implicational meaning) cannot be accessed. Target
                                                                    source of noise inside the model. As a result, assumptions
words falling within this window will not be detected as
                                                                    about the internal structure of the system are minimised and
implicationally salient and thus will not be reported.
                                                                    also the number of simulation runs is reduced. Thus, we
   There is normally lag-1 sparing in key-distractor AB
                                                                    convolve Gaussian-distributed noise (GDN) with the (noise
experiments, i.e. a target word immediately following the
                                                                    free) simulation results. We also gradually increase the
key-distractor is likely to be reported. This arises in our
                                                                    deviation of the GDN by serial position, i.e. the GDN is
model because buffer movement takes time, hence, the word
                                                                    narrower at earlier lags and broader at later lags. We call
immediately following the key-distractor may be
                                                                    this a convolution with sliding noise. (Note, we explored
implicationally interpreted before the buffer moves to Prop.
                                                                    simpler convolution strategies, but none of these generated a
                                                                1524

suitable blink curve, see (Bowman, Su et al. 2006) for               weighted sum of these five LSA values. Effectively, we
details). The intuition behind this approach is that there is        "skew" the LSA space according to the extraction of
less noise in earlier phases of processing than in later phases      implicational meaning. The five weights characterise this
of processing, which influence blink onset and recovery              skewing, reflecting the relative emphasis that the
respectively. Application of such a convolution with sliding         implicational schema puts on each of the five dimensions.
noise results in a good fit to the human data as shown in               We constructed a two layer neural network to determine
Figure 1c. Note, our extensionalist model achieves this blink        these weights. The input layer contained five neurons, one
curve asymmetry by setting the GD shape parameter, which             for each of the five categories. The output layer was a single
determines how skewed it is from a normal distribution.              neuron. We trained the network using all the words we used
   In our simulations, the meaning of a target word can be           in the AB experiment. The learning algorithm used was the
processed to three different degrees, which, we argue,               delta rule (O'Reilly and Munakata 2000). The inputs were
reflect different types of response. Words that are both             LSA cosines and the expected output was 1 for targets and 0
implicationally and propositionally fully interpreted can be         for non-targets. The learning finished when the weights
reported correctly with their identity. Some target words can        settled, i.e. their changes were smaller than a given value
be implicationally fully un-interpreted, reflecting complete         (0.0001). Using the trained network, we calculated the new
unawareness of the presence of target words, i.e. the “No”           LSA values for all words. The results were: 52.5% of high
responses. Finally, some target words can be partially               salient and 22.2% of low salient key-distractors were
processed, reflecting the “Yes” response.                            implicationally salient. Nature words were mainly
   The resulting percentages of correct report of target             implicationally unsalient, except for one word (so, we
identities, “No” responses and “Yes” responses are shown in          excluded this word from our simulation). 63.4% of target
Figure 1c. These graphs also illustrate the difference in            words were implicationally salient. Interestingly, the ratio
performance between the high and low salience conditions.            between low and high salient key-distractor LSA
The results are consistent with the experimental results from        calculations was 22.2 / 52.5 ≈ 0.42 , which is consistent with
humans (Barnard, Scott et al. 2004) shown in the same                the depth parameters (0.44) and intrinsic probabilities (0.39)
graph. Moreover, the ratio between low and high salient              derived from our previous model.
key-distractor intrinsic probabilities of identification is             As a reflection of the fact that this is a more concrete
 0.19 / 0.49 ≈ 0.39 , which is similar to the ratio of the depth     model than the previous ones, convolutions are not used
parameters (0.44) in the previous model.                             here. Instead, different amounts of variance are added to the
                                                                     buffer movement delay at different stages, i.e. less variance
              Intensionalist Model – LSA                             is added to the delay of buffer movement from Implic to
In previous models, parameters were derived from human               Prop (which regulates blink onset) than the delay of buffer
performance on the AB task and assumptions about the                 movement in the opposite direction (which regulates blink
internal structure were minimized. However, in this model,           offset). Our extensionialist and intermediate models justify
word meanings are represented using Latent Semantic                  this, i.e. GD is a skewed distribution and the sliding noise
Analysis (LSA) (Landauer and Dumais 1997), which was                 ensures that the variance increases by lag. Partial responses
developed outside the AB. In this sense, this model’s key            are modelled in a similar way as the intermediate model.
parameters were constrained by a general theory that will be         The simulation results are shown in Figure 1d. Full details
used to explain the intrinsic probability and the depth              of these models can be found in (Bowman, Su et al. 2006).
parameter in our previous models.
   We hypothesize that a word is assigned to be salient if the                                Conclusion
semantic distance (an LSA cosine) between the word and               Attentional Capture by Meaning. We have provided a
the target category is smaller than a specified threshold. As        concrete account of attentional capture by meaning and the
shown in Figure 2c, the target words are within the                  temporal dynamics of that process. A number of key
propositional salience threshold. Hence, they are both               findings have arisen from our modelling. Firstly, we have
implicationally and propositionally salient. On the other            provided further evidence for the applicability of LSA in the
hand, background words are outside the implicational                 context of attentional capture by meaning. That is, we have
salience threshold. Hence, they are both implicationally and         shown that a model that measures semantic distance using
propositionally unsalient. Key-distractors can be either             LSA can reproduce the key-distractor blink and semantic
implicational salient or unsalient. However, they cannot be          modulations of blink depth. Furthermore, we have shown
propositionally salient. Only job words can be reported and          that these LSA calculations are consistent with more
only implicationally salient key-distractors can cause blinks.       extensionalist approaches in which the difference in
   In this model, the depth of the blink curve depends on the        observable behaviour is captured by either the GD depth
percentage of key-distractors above the implicational                parameter, or intrinsic probabilities of ascribing
threshold. We calculated the LSA cosines in relation to the          implicational salience derived directly from the blink curve.
meanings: generic human, generic occupation, generic                 Importantly, in all three cases, i.e. GD depth parameter,
payment, generic household and nature categories (Barnard,           intrinsic probabilities of implicational salience and LSA
Scott et al. 2004). Then, we integrated these cosines as a           measures of implicational salience, the ratio between high
                                                                 1525

and low salience has been almost identical (around 0.42).            Booch, G., J. Rumbaugh, et al. (1999). The Unified
This is an illustration of how multilevel modelling can                Modelling Language User Guide, Addison-Wesley.
provide converging evidence for a theoretical position.              Bowman, H. and G. Faconti (1999). Analysing Cognitive
   Secondly, we have clarified the characteristics of                  Behaviour using LOTOS and Mexitl. Formal Aspects of
attentional redeployment when meaning captures attention.              Computing 11: 132-159.
In particular, at an extensionalist level, a skewed                  Bowman, H., L. Su, et al. (2006). Semantic Modulation of
distribution was used to characterise the asymmetry of the             Temporal Attention: Distributed Control and Levels of
blink curve. At an intermediate level, the need to use a               Abstraction in Computational Modelling, Technical
convolution with sliding noise suggests that temporal noise            Report 9-06, Computing Lab, University of Kent.
increases systematically by serial position. At an                   Bowman, H. and B. Wyble (2007). The Simultaneous Type,
intensionalist level, this sliding noise is realised as variance       Serial Token Model of Temporal Attention and Working
in the buffer movement delay. This finding suggests that               Memory. Psychological Review 114(1):38-70.
there is less variance in extracting semantic gist (at Implic)       Chun, M. M. and M. C. Potter (1995). A Two-Stage Model
than extracting referential meaning (at Prop), since Implic            for Multiple Target Detection in Rapid Serial Visual
does not have to fully analysis and generate a concrete                Presentation. Journal of Experimental Psychology:
referent, which is likely to be affected by many variables.            Human Perception and Performance 21(1): 109-127.
This consistency is again an illustration of converging              Cousineau, D., D. Charbonneau, et al. (2006). Parametering
evidence from different levels of modelling.                           the attential blink effect. Canadian Journal of
                                                                       Experimental Psychology 60(3): 175-189.
Cognitive Architectures. The general applicability of our
                                                                     Dehaene, S., M. Kerszberg, et al. (1998). A neuronal model
models is enhanced since the approach can be placed within
                                                                       of a global workspace in effortful cognitive tasks. Proc
the context of a broad cognitive theory: the Interacting
                                                                       Natl Acad Sci U S A 95(24): 14529-34.
Cognitive Subsystems (ICS) architecture (Barnard 1999).
                                                                     Erickson, T. D. and M. E. Mattson (1981). From words to
Distributed control is inherent in ICS: subsystems are
                                                                       meaning: a semantic illusion. Journal of Verbal Learning
independent components, which interact through exchange
                                                                       and Verbal Behavior 20: 540-551.
of data representations over communication channels
                                                                     Fodor, J. A. and Z. W. Pylyshyn (1988). Connectionism and
(Barnard 1999; Bowman and Faconti 1999; Barnard and
                                                                       cognitive architecture: a critical analysis. Cognition 28: 3-
Bowman 2004). ICS asserts that cognition emerges as the
                                                                       71.
product of the interaction between a set of autonomous
                                                                     Hinton, G. E. (1990). Special Issue of Journal Artificial
subsystems. Both the delay-line and buffering concepts that
                                                                       Intelligence on Connectionist Symbol Processing (edited
we use have their roots in ICS. However, most significantly,
                                                                       by Hinton, G.E.). Artificial Intelligence 46(1-4).
the implicational - propositional distinction reflects ICS'
                                                                     Kieras, D. E. and D. E. Meyer (1997). An overview of the
dual-subsystem central engine (Teasdale and Barnard 1993).
                                                                       EPIC architecture for cognition and performance with
Multi-level Cognitive Modelling. We have provided a case               application to human-computer interaction. Human-
study for how multilevel modelling can be applied in the               Computer Interaction 12: 391-438.
cognition setting. Viewing systems from different                    Landauer, T. K. and S. T. Dumais (1997). A Solution to
perspectives and levels of abstraction is just a useful                Plato's Problem: The Latent Semantic Analysis Theory of
exploratory method for understanding systems, and it is one            the Acquisition, Induction and Representation of
that the cognitive modelling domain should not miss.                   Knowledge. Psychological Review 104: 211-240.
                                                                     Marr, D. (2000). Vision. Minds, Brains and Computers, The
                         References                                    Foundation of Cognitive Science (An Anthology). R.
                                                                       Cummins and D. D. Cummins, Blackwell: 69-83.
Abeles, M., H. Bergman, et al. (1993). Spatiotemporal
                                                                     McLeod, P., K. Plunkett, et al. (1998). Introduction to
   Firing Patterns in the Frontal Cortex of Behaving
                                                                       Connectionist Modelling of Cognitive Processes, OUP.
   Monkeys. Journal of Neurophysiology 70: 1629-1638.
                                                                     Newell, A. (1990). Unified Theories of Cognition.
Barnard, P. J. (1999). Interacting Cognitive Subsystems:
                                                                       Cambridge, Massachusetts, Harvard University Press.
   modelling working memory phenomena within a multi-
                                                                     O'Reilly, R. C. and Y. Munakata (2000). Computational
   processor architecture. Models of Working Memory:
                                                                       Explorations in Cognitive Neuroscience: Understanding
   Mechanisms of active maintenance and executive control:
                                                                       the Mind by Simulating the Brain. MIT Press.
   298-339.
                                                                     Stewart, N., G. D. A. Brown, et al. (2005). Absolute
Barnard, P. J. and H. Bowman (2004). Rendering
                                                                       Identification by Relative Judgment. Psychological
   information processing models of cognition and affect
                                                                       Review 112(4): 881-911.
   computationally explicit: Distributed executive control
                                                                     Teasdale, J. D. and P. J. Barnard (1993). Affect, Cognition
   and the deployment of attention. Cognitive Science
                                                                       and Change: re-modelling depressive thought. Hove,
   Quarterly 3(3): 297-328.
                                                                       Lawrence Erlbaum Associates.
Barnard, P. J., S. Scott, et al. (2004). Paying attention to
                                                                     Van Zandt, T. (2000). How to fit a response time
   meaning. Psychol Sci 15(3): 179-86.
                                                                       distribution. Psychol Bulletin & Review 7(3): 424-465.
                                                                 1526

