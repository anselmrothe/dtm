UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Hypothesis Testing and Associative Learning in Cross-Situational Word Learning: Are They
One and the Same?
Permalink
https://escholarship.org/uc/item/40c145pc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Yu, Chen
Smith, Linda B.
Klein, Krystal A.
et al.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

   Hypothesis Testing and Associative Learning in Cross-Situational Word Learning:
                                               Are They One and the Same?
          Chen Yu, Linda B. Smith, Krystal A. Klein and Richard M. Shiffrin ({chenyu}@indiana.edu)
              Department of Psychological and Brain Sciences, and Cognitive Science Program, Indiana University
                                                       Bloomington, IN 47405 USA
                             Abstract                                   cluttered than this with many candidate objects for a word
                                                                        and many candidate words for an object, and in the
   Recent studies (e.g. Yu & Smith, in press; Smith & Yu,
   submitted) show that both adults and young children possess          discourse context many shifts in attention among the
   powerful statistical computation capabilities -- they can infer      candidate words and referents. These highly ambiguous
   the referent of a word from highly ambiguous contexts                learning environments with many words and many objects
   involving many words and many referents. This paper goes             may significantly limit the plausibility of fast-mapping
   beyond demonstrating empirical behavioral evidence -- we             solutions.
   seek to systematically investigate the nature of the underlying         There is, however, an alternative way that learners might
   learning mechanisms. Toward this goal, we propose and
   implement a set of computational models based on three
                                                                        solve the indeterminacy problem, not in a single encounter
   mechanisms: (1) hypothesis testing; (2) dumb associative             with a word its referent, but across many trials and
   learning; and (3) advanced associative learning. By applying         simultaneously for many words and referents. This solution
   these models to the same materials used in learning studies          is possible if learners can accumulate the statistical evidence
   with adults and children, we first conclude that all the models      across multiple learning situations. A learner who is unable
   can fit behavioral data reasonably well. The implication is          to unambiguously decide the referent of a word on any
   that these mechanisms – despite their seeming difference --          single learning trial could nonetheless store possible word-
   may be fundamentally (or formally) the same. In light of this,
   we propose a formal unified view of learning principles that is      referent pairings across trials, evaluate the statistical
   based on the shared ground between them. By doing so, we             evidence, and ultimately map individual words to the right
   suggest that the traditional controversy between hypothesis          referents through this cross-trial evidence. For example, a
   testing and associative learning as two distinct learning            young learner (an infant perhaps) might hear the words
   machineries may not exist.                                           “bat” and “ball” in the context of seeing a BAT and BALL.
   Keywords:       language     acquisition,    word      learning,     Without other information, the learner cannot know whether
   computational modeling.                                              the word form “ball” refers to one or the other visual object.
                                                                        However, if subsequently, while viewing a scene with the
                          Introduction                                  potential referents of a BALL and a DOG, the learner hears
There are an infinite number of possible word-to-world                  the words “ball” and “dog” and if the learner can combine
pairings in naturalistic learning environments. Quine (1960)            the conditional probabilities of co-occurrences from two
illustrated this indeterminancy problem with this example:              streams of data across trials, the learner could correctly
Imagine an anthropologist who goes to a foreign country                 map “ball” to BALL. This mechanism seems to be quite
and observes a native speaker saying “gavagai” while                    straightforward. However, until recently, there was no
pointing in the general direction of a field with a rabbit in it.       evidence as to whether human learners perform these kinds
The intended referent (rabbit, grass, the field, or rabbit ears,        of statistical computations.
etc.) is indeterminate from this experience. Hard as it seems              In a series of recent experiments, we showed that both
to be to infer referents correctly from such data, typically            adults (Yu & Smith, in press) and 12 month-old infants
developing children have no problem using data of this sort             (Smith & Yu, submitted) do calculate cross-trial statistics
to learn their native vocabulary smoothly and effortlessly.             and find correct word-referent mappings amidst highly
   For 30 years, research on the indeterminacy problem has              ambiguous learning contexts, and they do so with
concentrated on single trial learning such that a language              impressive accuracy over relatively few trials. Cross-
learner – despite the logical ambiguity pointed out by Quine            situational statistical learning is clearly within the repertoire
– nonetheless correctly and rapidly maps the word to the                of human learners. This paper intends to go beyond
intended referent on that trial and by most accounts does so            demonstrating what language learners can do, and focus on
on the basis of social, linguistic and/or representational              investigating the internal learning mechanisms that may
constraints (e.g. Gleitman, 1990; Tomasello, 2000).                     underlie their powerful statistical learning capabilities. What
   However, most previous experiments showing such fast-                is the nature of the underlying learning processes? Can we
mapping of a word to a referent were conducted in highly                provide a formal account of cross-situational learning?
constrained laboratory environments. A typical scenario is                 Traditionally, two classes of cross-situational learning
like this: an experimenter presents one or two objects to               mechanisms have been considered. One is associative
young subjects and utters a simple phrase, such as “look,               learning. Across trials, the learner could accrue associations
this is a toma!” Everyday learning contexts are much more               between words and their potential referents by strengthening
                                                                        and weakening associative links between experiences of
                                                                    737

names and objects (see Plunkett, 1997, for review and               referent associations per trial. Although there is no
discussion). Building on the “ball/bat” example above, the          information on any individual trial as to which label goes
learner could on trial 1, equally associate “ball” with BALL        with which word, across trials the underlying word-referent
and BAT. But after trial 1, and based on the experience of          mappings are certain in that individual labels are presented
“ball” in the context of BALL and DOG, the association              in a training trial if and only if the referent is also presented.
between “ball” and BALL would be much stronger than that               The stimuli were slides containing pictures of uncommon
between “ball” and BAT. Over enough trials, these                   objects (e.g. canister, facial sauna, and hitch haul) paired
association strengths would converge on the real world              with auditorally presented artificial words. The three
statistics and yield the right word-referent pairs. The success     conditions were presented within subjects and so in total,
of such a learning mechanism would seem to depend                   there were 54 unique objects and 54 unique pseudowords
heavily on constraints on the kind of associations potentially      partitioned into the three sets of 18 words and referents for
formed given the logically infinite number of referents in          each condition. By design, the three learning conditions
any scene (e.g. Regier, 2003).                                      differed in the number of words and referents presented on
   An alternative way that learners could use cross-trial           each training trial (2, 3 or 4) and the number of times each
information is through hypothesis testing, by formulating           word and referent pair was presented across trials was held
and evaluating hypotheses about which names map to which            constant at 6. Order of trials within a condition was
referents (e.g., Siskind, 1996; Tennenbaum & Xu, 2000).             randomly determined. Order of the three conditions (a
Building on the “ball/bat” example above, the learner could         within-subject manipulation) was counterbalanced across
wrongly hypothesize on the initial trial that “ball” refers to      subjects. Training was passive. The adult participants
BAT but correct that hypothesis on trial 2 which presents           (n=38) just watched and listened as the trials were
disconfirming evidence. Across trials, the co-occurrence            presented; they were not told that there is a one-word-one-
probabilities would support the “right” hypotheses for the          referent correspondence. After training in each condition,
language over others. These kinds of learning mechanisms            learning was assessed via a four-alternative forced-choice
also require constraints on the kinds of hypotheses that can
                                                                    test; presented with one word, participants were asked to
be formed, given the infinite number of logically correct
                                                                    choose the picture to which the word referred. The three
hypotheses true of any single datum (see also Quine, 1960).
                                                                    foils were all drawn from the set of 18 training pictures.
  Based on the above two learning principles, this paper
                                                                    Participants learned more word-referent pairs in each
first presents three computational models as simulated
                                                                    condition than expected by chance (t(37)=8.785, p<0.001,
learners who receive the same training data that human
                                                                    one-tailed, for 4 × 4). They discovered on average more
subjects received in Yu and Smith (in press) experiments.
                                                                    than 16 of the 18 pairs in the 2 × 2 condition (M=16.2,
Given highly ambiguous learning trials with many possible
                                                                    SD=2.5) and more than 13 of the 18 pairs in the 3 × 3
words to be learned and many possible referents, simulated
                                                                    condition (M=13.6; SD=3.5), all this in less than 6 minutes
learners need to keep track of and memorize many word-
                                                                    of training per condition. Even in the 4 × 4 condition with 16
referents pairs and accrue cross-situational evidence just as
                                                                    potential associations per trial, subjects discovered almost
the human learners did. At the end of training, the
                                                                    10 of the 18 word-referent pairs (M=9.5; SD=2.9). The level
simulated learners are tested on the same tests that were
                                                                    of performance in the three conditions is remarkable -- in a
used with the human learners. In this way, we can directly
                                                                    very short time, over relatively few trials, each highly
compare human and simulated learners. Moreover, we use
                                                                    ambiguous, subjects nonetheless found the underlying
simulated learners to explore a variety of potential
                                                                    word-referent pairs. Human learners can and do keep track
constraints and to systematically evaluate their importance
                                                                    of the simultaneous co-occurrences of many labels and
in word learning. Based on this comparative study of three
                                                                    referents across trials such that they can find individual
computational models, we propose a unified view of
                                                                    mappings. Moreover, this is readily accomplished in
hypothesis testing and associative learning, suggesting these
                                                                    relatively few learning trials.
two can be treated as variants of the very same learning
                                                                       Experiment 2 of Yu & Smith (in press) explored adult
mechanism.
                                                                    learning in the condition of high within-trial ambiguity --
          Experimental Data for Simulation                          the 4 × 4 condition. We were particularly interested in
   This section presents the results in (Yu & Smith, in press)      learning under such high within-trial uncertainty as a
which are used as empirical data in the current simulation          function of the number of word-referent pairs to be learned.
study. In our first experiment, we asked how easily adults          Accordingly, in this experiment, each condition is a version
could simultaneously learn 18 word-referent pairs from              of the original 4 x 4 condition above. We manipulated: (1)
learning trials that are individually highly ambiguous. The         the total number of word-referent pairs to be learned (9 or
experiment included three conditions that manipulated               18) and (2) the number of repetitions of each word-referent
within-trial ambiguity: 2 words and 2 possible referents, or        pair (6, 8 or 12). In the 9 words/8 repetitions condition,
3 words and 3 possible referents, or 4 words and 4 possible         subjects attempt to discover a total of 9 word-referent pairs
referents on each trial. The 2 × 2 condition yields 4 possible      each repeated 8 times over the course of training. In the 9
word-referent associations per trial. The 3 × 3 condition           words/12 repetitions condition, subjects attempt to discover
yields 9 potential associations per trial. The 4 × 4 condition      9 word-referent pairs but are given 4 additional repetitions
yields the seemingly overwhelming number of 16 word-
                                                                738

of each word-referent pair. Finally, the third condition is a          works because this condition has been tested in two
replication of the original 4 × 4 condition, 18 word-referent          completed experiments. The simulations on other conditions
pairs to be learned and 6 repetitions of each. Intuitively, the        are achieved by applying the corresponding stimuli to the
9 words/12 repetitions condition should improve the                    same model.
learning performance because, compared with the 18                              In the 4 × 4 condition, the 18 novel word-picture pairs
words/6 repetitions condition, the number of words needed              can be represented as {( p1 , w1 ), ( p2 , w2 ),......( p18 , w18 )} . In the
to be learned are reduced while their occurrence frequencies           ith trial, the stimuli are Ti = { pi , pi , pi , pi , wi , wi , wi , wi }
are doubled. All aspects of the experiment are identical to                                                            1  2  3    4  1    2    3    4
the first experiment except for the composition of the three           while i1 , i2 , i3 and i4 can be selected from 1 to 18. And there is
training conditions.                                                   no information as to which picture goes with which name.
   In terms of the proportion of word-referent pairs to be             We also assume that the simulated learner maintains a list of
discovered, participants performed comparably in the three             hypothesized pairings as learned results from previous trials.
conditions, (F(2,54) = 0.52; p > 0.5), discovering more pairs          Thus, its lexical knowledge at the ith trial can be then
than expected by chance (t(27) > 6.4 in all three conditions,          represented as a list of pairs                               M = {( pn1 , wm1 ),
p < 0.001). In terms of the total number of pairs learned,
subjects actually learned more pairs in the 18 word-referent           ( pn2 , wm2 ),......, ( pnk , wmk )} while n j and   m j can be selected
condition (M=9.461, SD=2.907) than in the two 9 word-                  separately from 1 to 18, and the equivalence of these two
referent conditions (8 repetitions: M=5.111, SD=1.706; 12              indicates a correct pairing. At the beginning, the simulated
repetitions: M=5.481, SD=2.089). The 18 word condition                 learner randomly picks one word and one picture from a
presents the same within-trial ambiguity, more word-                   trial and builds a hypothesized pairing. With more trials,
referent pairs to be learned, and fewer repetitions of the             more pairings are built and stored in the memory. Two
individual word-referent pairs than the other two learning             additional mechanisms are utilized to make this learning
conditions. If numbers of co-occurrences were all that                 process more effective. First, one important constraint in
mattered, this condition should lead to the poorest overall            adding new pairs is to maintain the consistency of
performance. However, for statistical learners, smaller data           hypothesized pairings so that one word can be associated
sets are not as good as large ones because spurious                    with only one picture. This constraint explicitly encodes
correlations are more likely to occur (and thus also in the            such proposals as mutual exclusivity (Markman, 1990) and
lower foil probabilities at test).                                     contrast (Clark, 1987) into the learning machinery and by
   Overall, our experimental results show the power of                 doing so makes learning more efficient because without
cross-situational statistical learning: Even when the referent         such a constraint, the simulated learner would randomly
of a word cannot be unambiguously determined on any                    select many conflicting (and therefore incorrect) word-
single learning trial, across multiple trials involving many           picture pairs across multiple trials. Second, the model keeps
different words and many different potential referents, the            track of the frequency of each hypothesized pair. When the
word and referent will occur most systematically than any              number of occurrences of a pair is above a certain threshold,
other. The more words and referents there are to learn and             this pair will be treated as a learned lexeme and then used to
that may co-occur together on any learning trial, the more             filter out the input in subsequent trials; this significantly
discernible the systematicity – across trials – of the                 simplifies the learning task. For instance, if a learned pair
underlying correct mappings.                                           occurs in a new trial, it will be removed from the stimuli to
           Hypothesis Testing Model (HTM)                              reduce a 44 condition into a 33 condition. More
   There is no information at the beginning of learning to             importantly, subjects in empirical studies informed
guide learners; thus it is quite plausible that on the first trial     experimenters that they used a similar filtering strategy in
learners randomly select word-referent pairs as their initial          the later part of the training phase when they were confident
hypotheses, gradually justifying or replacing these                    that some word-picture pairs were correct.
hypotheses as more trials ensue. Following this general                   We applied the same training and testing data in the two
principle, the specific questions for such a hypothesis                adult experiments to the model. For each condition, the
testing mechanism are (1) how hypothesized pairs are                   simulation was run for 5000 times. Thus, we had 5000
selected and stored from a trial? (2) how subjects justify             simulated subjects (with the same set of parameters) for
whether a word-object pair is correct? (3) whether they use            each condition. Note that the fundamental mechanism
the mutual exclusivity constraint if two working                       encoded in our model is to randomly select and store
hypothesized pairs are not compatible? and (4) whether they            hypothesized pairs. Therefore, quite different results could
use previously learned pairs to help the learning of new               be obtained on each run depending on what pairs were
pairs in subsequent trials?                                            selected from trial to trial. We used 5000 simulated subjects
   Our first simulation study attempts to answer these                 to ensure the statistical power of this simulation study and
questions and also to generate a dynamic picture of the real-          the results are shown in Figure 1. We observed that in
time learning when the simulated learner is fed with the               general the results in simulation are quite in line with those
same set of ordered trials as the adult learners. Here we use          of human subjects, suggesting that if subjects apply a simple
the 44 condition as an example to show how the model                  statistical learning machinery like the one in our model, then
                                                                       this could explain their superior performance in learning
                                                                   739

from individually ambiguous learning trials. The similarities                         obtain the results from 5000 simulated associative learners
of the results between human subjects and simulated                                   for each method. Figure 2 (b) and (c) shows examples of
subjects are consistent not only in one condition but among                           association matrices built by one-pair and two-pairs learners
five conditions of two experiments, indicating that the                               respectively. Clearly, these two matrices are quite different
learning principles encoded in our model are plausibly                                and far from perfect compared with the matrix shown in
similar to those that guide the learning of human subjects.                           Figure 2 (a). Nonetheless, when the simulated learners were
                                                                                      asked to do the same forced-choice tests, they still
                            1
                                                      human subjects
                                                                                      demonstrated learning based on partial and incomplete
                           0.8
                                                      simulated learners              matrices, as shown in Figure 3.
                                                                                       word
      proportion correct
                                                                                                                 referent
                           0.6
                                                                                        12         6   6   8     3   3   3   3     4   0 1 0 0 1 0 0 0 0                   1 1 1 0 0 0 0 1 0
                           0.4
                                                                                         6     12 5        7     4   4   4   3     3   1 1 0 0 1 0 0 0 0                   1 2 0 2 0 1 0 0 1
                                                                                         6      5 12       4     3   3   6   4     5   0 0 1 0 0 1 0 0 0                   2 1 2 0 0 0 3 2 0
                           0.2                                                           8         7   4   12 2      4   4   4     3   0 0 0 1 0 0 0 0 0                   1 0 0 2 1 0 0 0 0
                                                                                         3         4   3    2 12     5   6   7     6   0 0 1 0 0 0 0 1 0                   0 0 0 1 2 1 1 0 1
                            0
                                                                                         3         4   3   4     5   12 5    5     7   0 0 1 0 1 1 0 0 0                   0 1 1 0 1 4 0 0 1
                                  2x2   3x3   4x4   9 words/ 8    9 words/ 12            3         4   6   4     6    5 12   5     3   0 0 1 0 0 1 1 1 0                   0 0 2 0 1 0 1 0 0
                                                    repetitions   repetitions            3         3   4   4     7   5   5   12 5      1 0 1 1 0 0 1 1 1                   0 0 0 1 1 0 0 3 0
                                                                                         4         3   5   3     6   7   3    5 12     1 1 0 0 0 0 0 0 2                   3 0 0 0 1 0 1 0 1
   Figure 1: Simulated learners and human learners achieve similar
results in all conditions.                                                                (a) an ideal learner     (b) a simulated learner   (c) a simulated learner
                                                                                                                          (one pair)                (two pairs)
                                 Dumb Associative Model (DAM)                         Figure 2: the learning results in 9 words/18 repetitions condition. The row is a
   The hypothesis testing model fits behavioral data                                            list of words and the column is a list of referents. Each cell
qualitatively well. One major assumption in the HTM is that                                     represents the co-occurrence frequency of a word-referent pair. The
it applies the constraints, such as Mutual Exclusivity, in                                      diagonal items count relevant co-occurrences.
real-time learning, and ignores other information. In
                                                                                              1
contrast to this type of explicit learning, an alternative                                                                                human subjects
mechanism would be to associate one word with one                                                                                         associative learner ( 1 pairs)
                                                                                                                                          associative learner (2 pairs)
                                                                                             0.8
referent at a time and to accumulate this evidence across                                                                                 associative learner (3 pairs)
multiple trials. During testing, this associative model picks
                                                                                             0.6
out the object most strongly associated with the test word.
Along this line, an associative learner who kept track and
                                                                                             0.4
stored all co-occurrences on all trials and at test chose the
most strongly associated referent would be an ideal learner,                                 0.2
internally representing the matrix of input, as in the 9
words/12 repetitions condition shown in Figure 2 (a).                                         0
Human subjects may well be able to approximate this,                                                       2x2                   3x3          4x4               9 words/ 8
                                                                                                                                                               repetitions
                                                                                                                                                                                9 words/ 12
                                                                                                                                                                                repetitions
storing many, if not all, of the associations on a trial and                             Figure 3: A comparison between human learners and three different
accruing them across trials. Such an associative learning                             associative learners.
mechanism would, in fact, do quite well in our experimental
tasks. However, it is also possible that human learners are                                            Associative Translation Model (ATM)
more selective associative learners, that due to processes of                              In contrast to a dumb association model that accrues co-
competition, inhibition, and attention shifting (e.g.,                                occurrence frequencies trial by trial and updates the strength
                                                                                      of the connections between a word and an object, we also
Kruschke, 2001), they may build a few one-word-one-object
                                                                                      developed a more advanced associative model based on
associations exhibiting a form of mutual exclusivity.
                                                                                      machine translation techniques. Briefly, machines “learn” to
       Thus far, three simple associative methods have been                           automatically translate one language into another through
developed. These only pursue models based on the general                              statistical regularities across large parallel corpora (e.g.,
principle that the system randomly selects and accumulates                            statistical regularities across Anna Karenina in English and
word-referent pairs without applying any constraints in real-                         Russian). The basic assumption behind this approach is that
time learning. Hence, dumb associative model can be                                   there are latent meanings that both languages point to, and
viewed as based on Hebbian learning principles – the                                  the machine learning techniques attempt to discover these
connection between a word and an object is increased if the                           latent structures through the statistical regularities. Here, we
pair co-occurs in a trial. One associative model simply                               use this same computational approach but conceptualize the
accumulates one single word-referent pair from a trial.                               object stream as one language and the audio stream as the
Thus, the total number of pairs selected is equal to the                              other, attempting to find the latent word-referent pairings
number of trials. Figure 2 (b) shows one instance of this                             between these two streams. More specifically, we used the
model. The second and the third models select two and three                           translation model in (Brown, Pietra, Pietra, & Mercer, 1994)
pairs from a trial. Similar to the hypothesis-testing model,                          and applied an Expectation-Maximization (EM) based
these three methods are also based on random selection of                             learning algorithm. Our algorithm assumes that word-
pairs. Therefore, the simulation was run for 5000 times to                            referent pairs are hidden factors underneath the
                                                                                740

observations, which consist of spoken words and                               two approaches. For example, if the threshold is 19, then a
extralinguistic contexts. Thus, association probabilities are                 set of 19 pairs should be treated as hypothesis testing and a
not directly observable, but they somehow determine the                       set of 20 should be based on associative learning.
observations because spoken language is produced based on                     Nonetheless, it is not clear that this kind of threshold exists
the caregiver’s lexical knowledge. Therefore, the objective                   at all.
of language learners or computational models is to figure                        Second, the hypothesis testing model stores the accrued
out the values of these underlying association probabilities                  information in a winner-take-all way – the pairs in the list
so that they can interpret the observations better. Correct                   are equally treated as correct while the pairs not in the list
word-meaning pairs are those which can maximize the                           are excluded from consideration. In contrast, the associative
likelihood of the observations. Technical detailed can be                     learning method accumulates and stores the information in a
found in Yu, Ballard & Aslin (2005).
                                                                              probabilistic and graded way. Every co-occurring word-
      1                                                                       referent pair is assigned to an association probability based
                                             human
    0.8                                      1 pair                           on co-occurrence frequency while some pairs have high
                                             2 pairs                          probabilities and others are assigned with low probabilities.
    0.6                                                                       The dumb associative model purely relies on co-occurrence
    0.4
                                                                              frequencies while associative translation model computes
                                                                              association probabilities by considering various correlations
    0.2                                                                       between words and objects to find an overall optimal
                                                                              solution.
      0
            2×2        3×3     4×4   9 words / 8 9 words / 12                    Conceptualizing the differences in this way makes clear
                                     repetitions repetitions                  that associative models can be converted into hypothesis
Figure 4: A comparision between human learners and assciative translation     testing models and that hypothesis testing models can be
model in the 4 conditions.                                                    converted into associative models. More specifically, a set
Similar to the dumb associative model, we compare three                       of hypotheses can be considered as a special type of
variants of ATM with empirical results. As shown in Figure                    associative representation with the probabilities equal to
4, this sort of associative mechanism can easily extract the                  either 1 or 0 but nothing in between. As such, the hypothesis
proper mappings of words to referents – the most effective                    set can be treated as a special case of associative
learning mechanism compared with HTM and DAM.                                 representations and converted into a sparse and binary
                           A Unified View                                     association matrix as shown in Figure 5. Similarly, even if
So far, we have considered three computational models,                        lexical knowledge is stored in a probabilistic mode in an
Hypothesis Testing Model (HTM), Dumb Associative                              association matrix, the associative learner will need to make
Model (DAM) and Associative Translation Model (ATM).                          decisions at testing, which may force the learner to retrieve
As radically different as associations versus hypotheses may                  the strongest relevant associations. For example, in our
seem to be, all these models can be viewed as variants of the                 word-learning task (or any other task of the same sort, for
same kinds of processes. Critically, from both an associative                 example, naming the object), the learner needs to pick out
and a hypothesis testing perspectives, the relevant
                                                                              an object after hearing a word. To do that, the learner finds
mechanisms for the cross-situational learning of words and
                                                                              the most relevant referent and ignores others. Thus, one can
referents are almost the same: (1) what information is stored
on any individual learning trial, (2) how past knowledge                      also extract a hypothesis set from an association matrix by
constrains future learning, and (3) how accrued information                   picking out strongest associations (converting probabilistic
is evaluated. In the following, we will compare associative                   associations into explicit hypotheses). In doing so, different
versus hypothesis testing mechanisms in terms of these                        thresholds used in the conversion may determine the
three aspects.                                                                number of pairs in the hypothesis set. But again there is no
Representations and Learning Results                                          clear threshold of the number of pairs that can be applied to
The representations and learning results based on                             separate two mechanisms.
associative learning and hypothesis testing appear to be
radically different. As shown in Figure 5, one builds a big
two-dimensional matrix to count all possible co-occurrences
between words and objects while the other just keeps track
of a short list of word-referent pairings. There are two ways
to quantify these differences in representations: (1) the
number of word-object pairs and (2) as probabilistic versus
all-or-none representations.
   First, the hypothesis testing model maintains a clean and
short list while the associative models store many co-
occurring pairs. If the number of pairs really matters, one                      Figure 5: we can select the strong associations in an association matrix
would expect that there should, quantitatively, be a clear                    to form a hypothesis set. Similarly, we can represent a hypothesis set as a
boundary (threshold) that can be drawn to differentiate these                 sparse association matrix. The numbers in the lists are indexes of words
                                                                              and referents.
                                                                          741

Learning Mechanisms                                                   insights about learning processes that fall between these two
In general, all three mechanisms use cross-situational                classic extremes. Second, the learning literature is replete
information accumulatively and all three mechanisms need              with cases in which one or the other approach appears
to include some constraints in learning. One may attempt to           better. Conceptualizing the two approaches as special cases
differentiate the three mechanisms based on real-time versus          of the same principles may be a first step to understanding
batch processing -- at what moments are the constraints               how certain tasks, contexts, and past history select for
added to reduce the degree of ambiguity in the data? Does             specific learning solutions. Third, there are reasons to
this happen in trial-by-trial learning or only at the retrieval       suspect that young learners (in some domain) are more
of accumulated information during testing? HTM is a real-             likely to appear to be associative learners whereas older (or
time learning process wherein the simulated learner                   more expert) learners appear to be hypothesis testers. The
continuously builds and justifies hypothesized pairs trial by         present conceptualization offers a framework within which
trial. The decisions are made in real time on whether a               to theoretically understand the developments. Finally, this
selected pair is included in or excluded from the hypothesis          conceptualization suggests that many of the heated debates
list. When the training phase ends, the learner acquires a list       about associations versus hypotheses may be fundamentally
of hypothesized pairs. In contrast, DAM functions in a batch          –and mechanistically –misguided.
mode because it accumulates data during training to form an           Acknowledgments: This research was supported by
association matrix including all the possible pairs that the          National Science Foundation Grant BCS0544995.
model can select and store. The decisions are made in
testing when the model needs to retrieve an object given a
                                                                                              References
spoken word and several options. Different from the above
two, ATM’s learning mechanism is in between batch and                 Clark, E.V. (1987). The Principle of Contrast: a constraint
real-time processing – similar to hypothesis testing, it                 on language acquisition. In B. MacWinney (Ed.),
estimates association probabilities in real time and similar to          Mechanisms of language acquisition (pp. 1-33): Hillsdale,
DAM, the association probabilities in the association matrix             NJ: Lawrence Erlbaum Associates.
                                                                      Gleitman, L. (1990). The structural sources of verb
jointly determine the referent of a word during testing.
                                                                         meanings. Language Acquisition, 1 1-55.
Recent advances in machine learning suggest that many
                                                                      Markman, E. M. (1990). Constraints Children Place on
learning algorithms can be converted between batch mode
                                                                         Word Learning. Cognitive Science, 14, 57-77.
and real-time mode. Thus, even if real-time and batch
                                                                      Plunkett, K. (1997). Theories of early word learning. Trends
processings are fundamentally different, associative learning
                                                                         in Cognitive Sciences, 1, 146-153.
and hypothesis testing mechanisms are at least compatible
                                                                      Quine, W. V. O. (1960). Word and Object. Cambridge, MA:
(and convertible) and can be grouped under a more general
                                                                         MIT Press.
learning system.
                                                                      Siskind, J.M. (1996). A computational study of cross-
Retrieving accrued information at Testing                                situational techniques for learning word-to-meaning
The hypothesis testing model uses the accrued knowledge in               mappings. Cognition, 61, 39-61.
a straightforward way. After hearing a word at each trial, it         Smith, L.B. & Yu, C (submitted). Infants rapidly learn
checks the hypothesis list to match the word with those in               words from noisy data via cross-situational statistics.
the hypothesized pairs. For associative models, lexical               Tenenbaum, J.B. & Xu, F. (2000) Word learning as
knowledge is represented as latent information and as a                  Bayesian inference. In L. Gleitman and A. Joshi (eds.),
system of associations, which can be activated in response               Proceedings of the 22nd Annual Conference of the
to a specific input. In fact, there are several different ways to        Cognitive Science Society. Hillsdale, NJ: Erlbaum.
extract and utilize latent knowledge from an association              Regier, T. (2003). Emergent constraints on word-learning:
matrix. For example, a hypothesis set can be extracted by                A computational review. Trends in Cognitive Sciences, 7,
picking out the strongest associations from the association              263-268.
matrix. Similar to the hypothesis testing model, the mutual            Tomasello, M. (2000). Perceiving intentions and learning
exclusivity constraint can be added to ensure the word-                  words in the second year of life. In M. Bowerman & S.
object pairs in the list are consistent. Another method is to            Levinson (Eds.), Language acquisition and conceptual
decompose the association matrix into several hypothesis                 development (pp. 111-128): Cambridge University.
sets, each of which forms a consistent set of word-object             Yu, C., Ballard, D. H., and Aslin, R. N. (2005). The role of
pairs. At test, the overall decision is based on hypothesis test         embodied intention in early lexical acquisition. Cognitive
averaging. Thus, this is the main conclusion -- there is no              Science, 29, 961-1005.
fundamental difference in these two learning principles.              Yu, C. & Smith, L.B. (in press). Rapid Word Learning
                          Conclusion                                     under Uncertainty via Cross-Situational Statistics.
This idea that hypothesis testing and associative learning               Psychological Science.
may be special cases of a unified set of learning
mechanisms is theoretically significant on various grounds.
First, the theoretical and empirical exploration of learning
mechanisms within such a unified view may reveal new
                                                                  742

