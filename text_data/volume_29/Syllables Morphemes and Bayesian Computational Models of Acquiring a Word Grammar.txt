UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Syllables, Morphemes and Bayesian Computational Models of Acquiring a Word Grammar
Permalink
https://escholarship.org/uc/item/08v1f00r
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Çoltekin, Ça?ri
Bozşahin, Cem
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                     Syllables, Morphemes and Bayesian Computational Models
                                               of Acquiring a Word Grammar
                                 Çağrı Çöltekin                                               Cem Bozşahin
                                Cognitive Science                            Cognitive Science and Computer Engineering
    Middle East Technical University (METU), Ankara 06531 Turkey                      METU, Ankara 06531 Turkey
                               cagri@xs4all.nl                                           bozsahin@metu.edu.tr
                               Abstract                                      There are indeed phonological and prosodic cues for dis-
                                                                          cerning substrings smaller than words, namely syllables
   We report a computational study on the CHILDES database for            (rhythm), stress and pitch accents. In this work, we report
   learning a word grammar of Turkish nouns. The syllable-based
   model converges to a morpheme-based model in terms of over-            a computational study which starts with the ability to iden-
   laps in the set of lexical hypotheses. Morphology is a hidden          tify syllables, and learns the meaning and category of words
   variable in all models, and the search problem for hypotheses          and morphemes without the assumption that only words and
   is narrowed down by a probabilistic conception of universal
   grammar à la Combinatory Categorial Grammar. The conver-              morphemes have a meaning. The kind of meanings that the
   gence of the syllable model suggests that morphemehood can             system starts with and learns more of is not lexical meanings,
   be an emergent computational property.                                 such as what it means to be a dog or to sleep (see Tenen-
   Keywords: Morphology, grammar, learning, Bayesian model.               baum & Xu 2000 for a Bayesian way to tackle that prob-
                                                                          lem), but the combinatory meaning and its syntactic reflex
                           Introduction                                   in the form of a category, as a lexical hypothesis, for example
                                                                          how pisi-ler-e (kitty-PLU-DAT, Turkish), with the syllables
How can the meaning and category of words arise in the mind               pi·si·le·re, can come to be associated with a logical form such
of a child? On one hand, we have the problem of identifying               as to 0 (plu 0 cat 0 ) and the syntactic type N for nouns (and oth-
segments of speech as word-like units. On the other, we have              ers, such as VP modifiers). We show that under a Bayesian
the problem of identifying which meanings go with which                   scenario of hypothesis revision with the Universal Grammar
substrings in speech. The assumption, common to both gener-               as the provider of likelihoods and priors, starting with sylla-
ative and cognitive linguistics, is that the child has the innate         bles and the assumed ability to associate forms with mean-
capacity to associate forms with meaning, and it is a question            ings converges to a lexicalized grammar of words and mor-
of acquisition to tackle the problem of deciding which forms              phemes, by showing a significant overlap with the lexical hy-
go with which meanings.                                                   potheses of a learning model which works with the assump-
   A quick glance over the Turkish fragment of the child-                 tion that only morphemes and words constructed from them
directed speech in the CHILDES database reveals that 44%                  have meanings.
of the nouns are uninflected; the remaining 56% are inflected                Crucially, morphology of words is a hidden variable in our
by means of affixes and clitics. The question then arises                 model, and the input to the system are pairs of sequence of
as to how the meaning and category of the inflected words,                syllables (in lieu of phonological form, PF) and a logical form
which constitute the majority, are acquired by the child. A               (LF), without any indication as to which syllable contributes
common concept, influential at least since Bloomfield, is that            to what part of the LF, or which part of the meaning of a mor-
morpheme is the minimal meaning-bearing element in natu-                  pheme is covered by a syllable. This is unlike the approach of
ral languages. Nevertheless, although there are clear phono-              Jack et al. (2006), another syllable-based acquisition model,
logical and prosodic cues for word boundaries (e.g. Jusczyk,              in which a sequence of syllables is paired not with a possi-
1999; Thiessen & Saffran, 2003), there are no apparent cues               bly ambiguous LF but with a disambiguated representation
for morpheme boundaries, hence the task of learning mor-                  of world meanings. We do not assume that the child knows
pheme meanings to come up with word meanings is not made                  pisi is kitty, and ler is plural; she might (wrongly) hypoth-
easier by labeling some items as morphemes in the compe-                  esize pisi is plural and ler means kitty, or the first syllable
tence grammar of adults.1                                                 of pisi (pi ) means kitty, etc. We also differ from Aronoff et
    1                                                                     al. (2006), whose model detects frequently-occurring sound
      Aksu-Koc & Slobin (1985); Peters & Menn (1993) report pro-
duction data of respectively Turkish and English children of age 2;6      sequences and hypothesizes that they are morphemes. Our
and younger, during which the child produces meaningless filler syl-      model aims to learn the correct LF of the purported mor-
lables. Peters & Menn data show this is not idiosyncratic to verbs.       pheme as well, not just its form.
Contra the remarks of both work for Turkish without a statistic, mor-
pheme and syllable boundaries do not generally coincide. Only 23%
of the syllables in nouns (out of 20,433 syllables) are also mor-                                 Universal Grammar
phemes in the CHILDES database. If we only match boundaries (the
beginning and end of a morpheme align with a syllable boundary, ir-       What allows our system to learn with reasonable efficiency is
respective of whether the syllable and the morpheme are the same),
e.g. araba-lar (car-PLU, Turkish) versus the syllables a·ra·ba·lar        that the search problem for lexical hypotheses is kept man-
providing two matches out of 4 syllables, the overlap is 57%.             ageable by a Universal Grammar (UG) and the current lexi-
                                                                      887

calized grammar, and that the input words are relatively short                   (4) The Principle of Categorial Type Transparency (PCTT):
compared to adult input, so that considering all possibilities                      (Steedman, 2000)
of syllable-LF associations is tolerable computationally, as                        “For a given language, the semantic type of the interpre-
suggested by Steedman & Hockenmaier (2007) for learnabil-                           tation together with a number of language-specific direc-
ity of short utterances involving multiple words.2                                  tional parameter settings uniquely determines the syntactic
   We shall assume that UG comprises a set of principles and                        category [syntactic type] of a category.”
a set of universal combinatory rules, which are completely
type-dependent, rather than structure-dependent. Steedman                           In the pair (σ, µ) of a lexical category, the syntactic type
(2000) shows how Combinatory Categorial Grammar (CCG)                               σ and the semantic type µ are co-determined: µ is of type
can fulfill that role, which gives CCG its explanatory edge                         T σ, and σ is of type T −1 µ, where T is a relation with
compared to structure-dependent accounts, to explain the so-                        inverse. If σ is a syntactic functor α\β or α/β, then its
called nonstandard constituency in bounded and unbounded                            semantic type is T σ = T β 7→ T α.
dependencies such as in coordination and relativization asym-                       The Prin. of Combinatory Type Transparency:
metries, to integrate intonation structure, information struc-
                                                                                    “All syntactic combinatory rules are type-transparent ver-
ture and constituent structure as arising from the same deriva-
                                                                                    sions of one of a small number of simple semantic oper-
tional system of projecting (PF, LF) pairs from the lexicon to
                                                                                    ations over functions.” [They are called B, T and S in
phrases. Steedman & Hockenmaier (2007) show how CCG
                                                                                    Curry’s Combinatory Logic.]
can bootstrap and facilitate learning a lexicalized grammar of
a natural language, with examples involving the use of words                        The Principle of Consistency (PC):
from the stage of 2-word syntax onwards.                                            “All syntactic combinatory rules must be consistent with
   We will confine ourselves to a few principles related to our                     the directionality of the principal functor.”
work, and to one combinatory rule that seems most relevant
to acquiring a word grammar, viz. function application. It is                    For example, (5a–c) are not viable lexical hypotheses (assum-
defined as follows:                                                              ing for 5a that the child has not been constantly exposed to
(1) a. Forward Application:                                                      *ler-pisi ‘PLU kitty’ as well by an unduly sarcastic adult).
          X/Y: f Y: a ⇒ X: f a                                           (>)     The first one violates PC, and the others violate PCTT: a syn-
                                                                                 tactic functor has to correspond to a predicate, not to a propo-
    b. Backward Application:                                                     sition as in (5b); a 2-place syntactic functor cannot originate
          Y: a X\Y: f ⇒ X: f a                                           (<)     from a 1-place predicate as in (5c).
The elements of a CCG lexical hypothesis are:                                    (5) a. {pisiler := N, pisi := N\N, ler := N}                 (*)
                                            category
                                                                                     b. ler := N\N: plu 0 (t)                                 (*)
                    z                          }|                       {
      string          syn. type                   lambda term
      z}|{          z    }|      { z                  }|                {
                                                           0
(2) like |{z}  := (S\NP)/NP :             λxλy.       like (e, (e, t)) xy            c. tut (catch) := S\NP\NP: λx.catch 0 (e,t) x            (*)
                                          | {z }             | {z }
             string                 correspondence
              type                                            sem.type
                                                      |         {z      }                                 The Models
                                                          logicalf orm
                                                                                 We have developed three models: 1) A syllable-based model
In a lexicon of morphemes and words, these type assignments
                                                                                 (SBM) in which an LF is associated with a sequence of syl-
and rules engender a derivation of pisi-ler-e as follows:
                                                                                 lables, 2) a morpheme-based model (MBM) where an LF is
(3)     pisi            ler                     e                                associated with a sequence of morphemes, and 3) a random
        kitty         -PLU                    -DAT                               model (RM) in which a randomly-segmented word is associ-
      N: cat 0 N\N: λx.plu 0 x Ndat \N: λy.to 0 y                                ated with an LF.
                                 <
              N: plu 0 cat 0                                                        All models use the same statistical learning mechanism.
                                                        <                        Each input is a word segmented according to above, depend-
                    Ndat : to 0 (plu 0 cat 0 )                                   ing on the model, with the LF paired with the entire sequence
                       ‘to the kitties’
                                                                                 of units in the word. We assume that the number of units per
The following principles of UG narrow down possibilities for                     word is always greater than or equal to the number of terms
universal rules and lexical types, independent of whether the                    in the LF, so that for example a polysyllabic word can in prin-
type assignment is to a word, morpheme, affix, clitic, syllable,                 ciple be associated with a one-term LF. We thus distinguish
sign or tone:                                                                    zero-morphemes from root forms. The plural sheep would
    2
      The numbers are as follows: pisilere := to 0 (plu 0 cat 0 ) example        have the LF sheeps 0 , not plu 0 sheep 0 if we used English data
provided earlier requires 4 morpheme-LF pairings to be considered                (we consider relaxing this assumption as future work; Turk-
in the morpheme model, and 8 syllable-LF pairings in the syllable                ish seems to have no zero-morphemes, and composite suffixes
model. Average number of pairings are respectively 3.24 and 5.63 in
CHILDES. In contrast, an “adult word” such as kitabındakilerdeki                 such as -leri ‘-POSS.3PERS.PLU’ are indeed polysyllabic).
would require 49 and 343 pairings respectively.                                  The output is a lexicon containing the lexical hypotheses as
                                                                             888

items of a lexicalized grammar, such as (2). A lexical hypoth-          not increase much, and in the second case, it will continue to
esis is a 4-tuple (PF, σ, LF, w), where σ is the syntactic type         increase, albeit slowly. This is a more serious impediment to
(e.g., N, N\N), and w is the outcome of the system’s belief in          approximating the acquisition of grammar by the child in real
the hypothesis (0 ≤ w ≤ 1).                                             life, and short of faithfully approximating P (E), the issue
    All models use Algorithm 1 for learning. Learning is                remains controversial.
achieved by updating the weights based on new input. The
model follows a simple statistical method for updating the              Algorithm 1 Training the three models.
weights. The weights in the lexicon are the probability, or            1. Inputs: 1) The initial lexicon L0 . 2) A pair (PF,LF). PF is
system’s belief, that the lexical item in question is correct.             the segmented word. LF is the logical form for the entire
Each weight update consists of determining the new weight,                 PF.
the probability of the lexical hypothesis h given the new ev-
                                                                       2. Output: The final lexicon Lf . The procedure stops when
idence E. The new evidence E is the input word segmented
                                                                           no more hypotheses are added to the lexicon.
one of three ways, depending on the model. The weight of
the lexical item after seeing the input is updated by (6).             3. After the nth input, the updated lexicon Ln is determined
(6) w = w0 + αw0 L(1 − w0 )                                                by the following procedure:
                                                                         (a) All possible lexical hypotheses from the input are gen-
where w0 is the probability (or weight) of the lexical hypoth-
                                                                               erated by CCG rules.
esis before seeing the input E. If the hypothesis is already in
the lexicon, w0 is the weight of the hypothesis in the current           (b) Generated hypotheses are placed in a temporary lexicon,
lexicon, otherwise an arbitrary initial value is assigned. L in                LT . The weights of the items are obtained from the cur-
the formula is the likelihood P (E | h) in Bayesian terms (7).                 rent lexicon Ln−1 . If the lexical hypothesis is not in
                                                                               Ln−1 , an initial weight ws is assigned for the weight of
                    P (E|h)P (h)
(7) P (h | E) =         P (E)    ∝ P (E | h)P (h)                              the item in LT . In the experiments, an initial weight of
                                                                               0.1 is used.
L is calculated as the number of parses in which hypothesis              (c) All possible parses of PF using LT are produced. Each
h is used, divided by the total number of parses of the word.                  parse is assigned a weight proportional to the weights of
This determines the contribution of the new input to the pos-                  all the lexical items used in the derivation.
terior probability. The higher the number of parses that the
                                                                         (d) All the hypotheses used in the derivation of PF with the
hypothesis supports, the higher the likelihood value will be.
                                                                               highest weight are added to Ln , with the new weight
If the hypothesis is used by all possible parses of the input,
                                                                               determined by (6).
the value is 1. The value gets smaller due to the parses that
do not include the hypothesis. The final term in the formula,
1 − w0 , normalizes the result so that the new weight is in the
range (0,1]. α is a constant that is used to control the learn-            We use Algorithm 1 to train all models. It is adopted from
ing rate. Throughout the experiments it is kept at 0.01 (small          Zettlemoyer & Collins (2005), who also use CCG as a frame-
perturbations in the neighborhood did not have any effect in            work, with a different update mechanism. The crucial point in
preliminary runs).                                                      their algorithm is to allow any contiguous substring of the in-
    The final weight, the posterior probability of the hypothe-         put to be a lexical item. We use the principles of CCG (4) for
sis is increased with a value directly proportional to the prior        eliminating the illicit hypotheses, whereas they can eschew
P (h) and the likelihood P (E | h), as shown in (6). Rewrit-            the principles because their inventory of types is specific to a
ing (6) and (7) together, we get:                                       geography database, providing a similarly constrained behav-
                                                                        ior without UG.
(8) P (h | E) = P (h) + αP (h)P (E | h)(1 − P (h))
                                                                        An example
Our approach is inspired by Bayesian hypothesis revision, but           To exemplify the different behavior of the models, starting
it is not strictly Bayesian. Firstly, the implicit assumption is        with an empty lexicon, we go through the process of learning
that there is no negative evidence, as the probabilities do not         two related words: oda (room) and oda-ya (room-DAT). We
decrease. One can see no increase in the weight of a hypothe-           chose short words to save space, one with two syllables (o·da)
sis as less belief in it, compared to its alternatives with higher      and a single morpheme (oda), and the other with three sylla-
weight. The problem can be alleviated if we can fit a distribu-         bles (o·da·ya) and two morphemes (oda-ya); longer words are
tion for P (E), but this is rather difficult if not impossible. We      attested in CHILDES. For example adam-lar-a (man-PLU-
can assume that it is constant for all real word experiences E,         DAT) produces 20 hypotheses in MBM, and a staggering
therefore it can be ignored in the search for maximum poste-            number (49) in SBM.3
riors (cf. Step 3d of Algorithm 1).
                                                                            3
    Secondly, the system has no grounds to distinguish infre-                 Interestingly, the notorious -ki suffix, which causes recursion
                                                                        in morphology to produce indefinitely long words, is nonexistent in
quent but correct hypotheses from incorrect but frequent hy-            recursive form in CHILDES. We counted 30 instances of single use
potheses. In the first case, the belief in a hypothesis would           of -ki , out of 20,000 morphemes. 17 of them are word-final.
                                                                   889

   For the first word oda, the input to SBM is the pair (o·da,                  grammar as the trigger of negative feedback is crucial in this
room 0 ), while MBM gets as PF the whole word as one unit.                      respect: The child has the current set of hypotheses at her
Step 3a generates the single hypothesis (9) in both MBM and                     disposal to realize in a new experience that she might have
SBM. The input contains only a basic LF, hence no attempt is                    assumed wrongly about which part meant what, as calculated
made to find smaller units in PF.                                               in steps 3b–c of Algorithm 1.
(9) (oda, N, room 0 , 0.1)
                                                                                                        Experiments
With the hypothesis (9) placed in the temporary lexicon in                      We measure the success of the models with usual metrics over
Step 3b, the algorithm generates a single parse of the input.                   the final lexicons (precision, recall, f-score), and with two
As the only lexical item in the winning parse, the hypothesis                   sets of tests: recognize and generate. The first test measures
is inserted into the lexicon with a weight adjustment accord-                   each model’s ability to deal with unseen PFs, and the second,
ing to (6), which increases it to 0.1009.                                       unseen LFs. We use the following items for comparison:
   The second input is segmented as (oda-ya, dat 0 room 0 ) for                    Lr : The reference lexicalized grammar. This is a
MBM, and (o·da·ya, dat 0 room 0 ) for SBM. The morpheme                         manually-derived MBM-type adult competence grammar of
model maintains the hypotheses (10), after Step 3c of Al-                       Turkish nouns in CHILDES. It contains all free and bound
gorithm 1 eliminates potential hypotheses such as oda :=                        morphemes in the data. Lm : The lexicalized grammar
Ndat \N: λx.dat 0 x and ya := Ndat /N: λx.dat 0 x, because no                   learned by MBM. Ls : The lexicalized grammar learned by
universal rule can use them in any derivation of this experi-                   SBM. Lrm : The lexicalized grammar learned by RM.
ence.
(10)     (oda, N, room 0 , 0.1009)           (ya, Ndat \N, λx.dat 0 x, 0.1)
                                                                                Data
         (oda, Ndat /N, λx.dat 0 x, 0.1)     (ya, N, room 0 , 0.1)              Our data is the Turkish noun fragment of CHILDES
Hypotheses such as oda := Nplu /N: λx.dat x are eliminated    0                 (MacWhinney & Snow, 1990). It contains 51 recording ses-
by a currently oversimplistic closed-world assumption, by                       sions with 33 children. The ages of the children vary between
which the child’s linguistic world is embodied in the lexicon                   2;0 to 4;8. The average age of children is 3;4.
and the current experience, neither of which includes plural-                      We use the child directed speech (CDS) in the corpus. All
ity at this stage.                                                              the nouns in the CDS have been segmented at morpheme
   All hypotheses except (oda, N, room 0 , 0.1009) have the                     boundaries. Each segmented word is tagged with an LF to
weight 0.1, because none of them were in the lexicon. Sim-                      establish Lr . We left out derivational suffixes as future work.
ilarly, the syllable model produces the following set for the                   All derivational morphemes are considered part of the nomi-
input (o·da·ya, dat 0 room 0 ) :                                                nal root. Due to the nature of CHILDES transcriptions, auto-
                                                                                mated segmentation and tagging was not practical; they were
(11)   (o, N, room 0 , 0.1)                (daya, Ndat \N, λx.dat 0 x, 0.1)     done mostly by hand.
       (o, Ndat /N, λx.dat 0 x, 0.1)       (daya, N, room 0 , 0.1)
       (oda, N, room 0 , 0.1009)           (ya, Ndat \N, λx.dat 0 x, 0.1)          The data for training and testing contains 12,274 nouns
       (oda, Ndat /N, λx.dat 0 x, 0.1)     (ya, N, room 0 , 0.1)                out of 33,450 words in the CDS. The total number of mor-
After Step 3d, the lexicon contains the items in (12) and (13),                 phemes (nominal roots/stems with possible derivations, and
with the updated weights respectively for MBM and SBM.                          inflectional morphemes) is 20,433. The number of syllables
Lower weights for SBM are due to likelihood, which is in-                       is 27,497.
versely proportional to the total number of parses, which is
                                                                                Test Measures
higher in this example for SBM.
                                                                                The standard measures translate to the following in our case:
(12)   (oda, N, room 0 , 0.101354)     (ya, Ndat \N, λx.dat 0 x, 0.10045)       precision(p)=hits/(hits + noise), recall(r)=hits/(hits +
                                                                                misses), f-score=2pr/(p + r), where for any lexicon Lx , an
(13)   (oda, N, room 0 , 0.101127)     (ya, Ndat \N, λx.dat 0 x, 0.100225)
                                                                                entry that is both in Lx and Lr is a hit, an entry that is not in
The algorithm depends on the occurrences of isolated forms                      Lx but in Lr is a miss, and an entry that is in Lx but not in
to start up, which are generally the root forms. However, the                   Lr is noise. We also compare Lm and Ls .
system makes use of frequently occurring forms to learn other                      All models have been trained with equal amount of input.
forms without having seen them in isolation. For example, a                     Table 1 shows the value of measures and the size of the lexi-
third input masaya (table-DAT), segmented as masa-ya for                        cons (the number of lexical items with a syntactic type).
MBM and ma·sa·ya for SBM, would cause both systems to                              The lexicon learned by MBM (Lm ) is very similar to Lr .
add (masa, N, table 0 ) into their lexicon, as well as increasing               MBM fails to learn 5 of the inflections in the input, and in-
the weight of (ya, Ndat \N, λx.dat 0 x).                                        correctly learns 4 items which are not in Lr . The misses are
   In closing we note that if initial assumptions that are put in               due to infrequent salient occurrence of the morphemes. The
the lexicon are incorrect, they would nevertheless be licensed                  other errors are due to ambiguous inflections. All the differ-
by UG, to be corrected only by further experience in a Siskin-                  ences in Lm and Lr are due to affixes; the model learned the
dian (1995; 1996) scenario. The role of the current lexicalized                 complete set of root/stem forms in Lr .
                                                                            890

           Table 1: Test measures over the lexicons.                           Table 3: Results of the 10× generation tests.
     Lexicon #of items precision             recall f-score             Lexicon       precision          recall           f-score
     Lr               1041      100.00 100.00 100.00                                  µ        σ       µ        σ        µ        σ
     Lm               1040       99.61       99.51     99.55            Lr          16.20 0.92       92.90 1.66       27.59 1.30
     Ls                909       81.73       71.37     76.19            Lm          16.10 0.88       92.90 1.66       27.44 1.24
     Lrm              1697       51.73       83.57     63.90            Ls          15.20 1.14       71.70 1.83       25.08 1.53
                                                                        Lrm          0.90 0.32       91.20 1.52         1.78 0.63
         Table 2: Results of the 10× recognition tests.
   Lexicon       precision          recall          f-score          is considered a hit. If no parse producing the target LF is
                 µ        σ       µ        σ       µ        σ        found, it is a miss. Any parse that did not lead to expected LF
                                                                     is noise.
   Lr          87.00 1.63       92.90 1.66       89.83 0.79
                                                                        Because of the words that are in the test set but not in the
   Lm          86.70 1.42       92.90 1.66       89.67 0.66
                                                                     training set due to 10-folding, parsing all the input in the test
   Ls          84.20 4.32       72.80 2.04       78.02 2.06
                                                                     sets was not always possible, even for Lr . Ambiguous mor-
   Lrm         57.10 3.35       82.00 2.98       67.21 1.71
                                                                     phemes, on the other hand, caused multiple parses, hence the
                                                                     low precision values for all the models in Table 2 .
                                                                        Similar to the agreement statistics, Lm performs very close
   As expected, SBM’s performance is lower than MBM.                 to Lr in recognition. Due to the smaller number of lexical
However, it is significantly more precise than RM. SBM               items SBM learns in these experiments, its recall performance
misses to learn 268 (28%) morphemes while learning an ad-            is slightly worse than all others. However, it is almost as
ditional 166 (18%) morphemes which were not in Lr . Like             precise as Lm and Lr .
MBM, SBM could not learn a number of morphemes that                     The generation test produces the possible PFs for the input
were not frequent enough in the input. However, the majority         LF. Due to multiple phonetic alternations, and lack of phono-
of the morphemes that SBM fails to learn are the morphemes           logical knowledge in the models, the generation test always
whose boundaries do not match with syllable boundaries. The          overgenerates. For example, the logical form plu 0 man 0 gen-
additional items that are learned mistakenly also follow a           erates both adam-lar and adam-ler, the latter only violating
similar pattern. Most of them are due to morpheme-syllable           vowel harmony. On average, Lr and Lm generate 5.61 PFs
boundary mismatch. For example, for the plural suffix lar, Ls        per LF, Ls 4.62 and Lrm 80.92.
contains the lexical item la := Nplu \N: λx.plu 0 x, in addition        Except the high rate of noise generated by all models, the
to the adult reference entry lar := Nplu \N: λx.plu 0 x. Ignor-      results of generation tests are again similar to those of agree-
ing single-phoneme differences, like in the example above,           ment statistics. Lm performs very close to Lr , and Ls does
the number of misses by SBM drops to 165 (15%), and the              comparably, but slightly worse.
number of mistakenly learned items drops to 31 (3%).
                                                                                    Discussion and Conclusion
Recognition and Generation
                                                                     With varying degrees of success, all three models we investi-
CCG’s lexicalized grammars can act both as recognition               gated do learn a syntactic type for the (LF, PF) pair, that is, a
mechanisms, deriving an input PF to its possible LFs, and            fragment of grammar, in our case a word grammar.
as production mechanisms that output the possible PFs for               Our main result is as follows: Although we expected the
an LF. To test the success of the models and the differences         morpheme model MBM to approximate an adult reference
between them, all models have been run through these tests           word grammar, the syllable model SBM is unexpectedly not
after equal amount of training. We use the same data set of          too far behind, at least in the circumstances where word-
the lexicon comparison tests.                                        level ambiguity is kept to a minimum. This is promising
   For both generation and recognition experiments, we em-           for grounding early development of language in perception.
ployed 10-fold cross-validation. The data set is divided into        SBM’s success seems to depend on its ability to consider any
10 subsets with equal number of words, and the models have           contiguous substring of syllables as potential bearer of an LF
been trained 10 times, each time leaving a different subset as       (or part of it). This is not always possible since morpheme
test set and training set. The numbers reported in Table 2 and       boundaries do not always coincide with syllable boundaries.
Table 3 are average and standard deviation of measures for 10        With a reasonable margin of error (one phoneme), SBM
experiments.                                                         shows comparable performance with MBM in all tests.
   The recognition test can produce multiple parses leading             The success of the morpheme model is also noteworthy.
to the same or different LFs. Our criteria for hit, miss, and        Even though the input is segmented at morpheme boundaries
noise in this experiment are as follows: If the lexicon was          in MBM, it still needed to match the correct (PF, LF) pairs in
able to engender the intended semantic form at least once, it        the input. It does this successfully, only failing in case of am-
                                                                 891

                                                                          cues in child-directed speech is clear. As future work we in-
Table 4: Overall comparison of the lexicons. EM (Exact
                                                                          tend to relax the close-world assumption to observe the learn-
match) is the count of matching items with identical (PF, syn.
                                                                          ing rate of generating more hypotheses early in the course
type, LF). NM (Near match) ignores a single phoneme dif-
                                                                          of development. We think that universal grammar will still
ference in PF. LFS (LF/syn.type match) ignores the PF com-
                                                                          be indispensable for narrowing down the hypothesis space,
pletely.
                                                                          but other cues such as stress, intonation and access to possi-
                                Roots
                                                                          bly ambiguous extra-linguistic world (scenes, objects, events
                             & Stems Inflections Total
                                                                          etc.) will have to be incorporated to limit the search to obtain
       # items in Lr               886          155 1041                  reported early development of word grammar.
       # items in Lm               886          154 1040
       # items in Ls               802          107      909                                       References
       EM: Lr & Lm                 886          150 1036                  Aksu-Koc, A. A., & Slobin, D. I. (1985). The acquisition of
       EM: Lm & Ls                 684           59      743                 Turkish. In D. I. Slobin (Ed.), The crosslinguistic study
       NM: Lm & Ls                 774          101      875                 of language acquisition, vol.I: The data. New Jersey:
       LFS: Lr & Lm                886          150 1036                     Lawrence Erlbaum.
       LFS: Lm & Ls                719           83      802              Aronoff, J. M., Giralt, N., & Mintz, T. H. (2006). Stochastic
                                                                             approaches to morphology acquisition. In Selected pro-
                                                                             ceedings of the 7th conference on the acquisition of Span-
biguous morphemes. Given the lack of disambiguating cues                     ish and Portuguese as first and second languages (p. 110-
in the models, it seems that word learning in the model is fur-              121).
ther facilitated by less ambiguity in word structure, compared            Fodor, J. D. (1998). Unambiguous triggers. Linguistic In-
to syntactic structure.4                                                     quiry, 29, 1–36.
   10-fold cross-validation does not reveal all the similari-             Jack, K., Reed, C., & Waller, A. (2006). From sylla-
ties between morpheme and syllable models. Taken all to-                     bles to syntax: Investigating staged linguistic development
gether as one lexicalized grammar, the numbers are as in Ta-                 through computational modelling. In Proceedings of the
ble 4. It is significant that there is a 71% exact match of                  28th annual conference of the cognitive science society.
lexical hypotheses of the syllable model and the morpheme                 Jusczyk, P. W. (1999). How infants begin to extract words
model. Granted that the exact match of bound morphemes is                    from speech. Trends in Cognitive Science, 3, 323-328.
low (around 40%), we have to keep in mind that the syllable               MacWhinney, B., & Snow, C.(1990). The child language data
model does not come with root/stem boundaries, therefore the                 exchange system: An update. Journal of Child Language,
exact match of these forms (77%) is very significant.                        17, 457-472.
   In the course of development of the child, there will cer-             Peters, A. M., & Menn, L. (1993). False starts and filler syl-
tainly be more novel free morphemes in the lexicalized gram-                 lables: Ways to learn grammatical morphemes. Language,
mar than novel bound morphemes. One estimate for Turkish                     69(4).
is that children master the nominal paradigm by 24 months                 Siskind, J. (1995). Grounding language in perception. Artifi-
or earlier (Aksu-Koc & Slobin, 1985), therefore allomorphy                   cial Intelligence Review, 8, 371-391.
of bound morphemes (all lexical hypotheses for morpheme-                  Siskind, J.(1996). A computational study of cross-situational
like units in our terms, including their correct PF) is already              techniques for learning word-to-meaning mappings. Cog-
intact by that age in real life. This can only help the syllable             nition, 61, 39-91.
model to converge to the morpheme model more, if the same                 Steedman, M. (2000). The syntactic process. Cambridge,
relative success rate can be maintained. We can then surmise                 MA: MIT Press.
that morphemehood need not be a theoretical primitive, since              Steedman, M., & Hockenmaier, J.(2007). The computational
computation might deliver the morphemes without having to                    problem of natural language acquisition. University of Ed-
start with that assumption.                                                  inburgh. (ms.)
   Both models stand in sharp contrast with the random                    Tenenbaum, J. B., & Xu, F.(2000). Word learning as Bayesian
model. Its performance shows that the success of morpheme                    inference. In Proc. of the 22nd annual conf. of the cognitive
and syllable models is not due to chance. Since all models use               science society. Philadelphia.
the same universal grammar to control the explosion of num-               Thiessen, E. D., & Saffran, J. R. (2003). When cues collide:
ber of hypotheses, the importance of adequately capturing the                Use of stress and statistical cues to word boundaries by 7-
    4                                                                        to 9-month-old infants. Developmental Psychology, 39(4),
      There is no room for parameters in CCG, because UG is not
conceived as the initial state of competence grammar; it is invariant        706-716.
and only the lexicalized grammar is learned. Therefore, a trigger-        Zettlemoyer, L. S., & Collins, M. (2005). Learning to map
based scenario of acquisition via parameters such as that of Fodor           sentences to logical form: Structured classification with
(1998) where knowledge of structural unambiguity by the learner is
assumed is incompatible with CCG. A potential explanation lies to-           Probabilistic Categorial Grammars. In Proc. of the 21st
ward understanding the work of perceptual cues in narrowing down             conf. on Uncertainty in Artificial Intelligence. Edinburgh.
the hypotheses space allowed by a probabilistic universal grammar.
                                                                      892

