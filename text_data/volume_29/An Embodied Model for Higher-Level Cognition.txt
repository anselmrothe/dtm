UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Embodied Model for Higher-Level Cognition

Permalink
https://escholarship.org/uc/item/4kt834hp

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)

Author
Bittencourt, Guilherme

Publication Date
2007-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

An Embodied Model for Higher-Level Cognition
Guilherme Bittencourt (gb@das.ufsc.br)
Departamento de Automação e Sistemas
Universidade Federal de Santa Catarina
88040-900 - Florianópolis - SC - Brazil
based on evolutionary computation (Baum, 2004), to model
cognition it is necessary to define the elements of an evolutionary environment where such evolutionary computation
can take place. In natural evolution, the DNA codes for proteins, but the protein behavior depends on its three dimensional structure that is not coded in the DNA. The match between code and “good” behavior is computed by selection,
staying alive (and reproducing) is the positive feedback to the
significant matching. To model cognition using an evolutionary algorithm, we need to define, on the one hand, the code,
and we propose to adopt a specific logical representation –
prime implicants/implicates –, and, on the other hand, the
necessary feedback modeled in a minimalist approach, only
by “good” or “bad” emotions. Finally, we need variability and
selection, and time to let them work. Although no particular
evolutionary algorithm is described, the paper formally defines a possible computational framework that supports these
necessary features.
The model to be defined is a first approximation. Several
aspects of the model could and should be extended to include
more complex mechanisms, but this presentation remains as
simple as possible, in order to stress the solution schema proposed by the model to a specific conceptual problem: how a
cognitive agent is to learn and explore meaning in its environment.
The proposed model can be viewed from two perspectives,
an external one, that consists of the model’s desiderata and
includes the description of what would be interesting for a
cognitive mechanism to do, given the restrictions imposed by
the environment definition, and an internal one, that describes
how the interesting features of the cognitive mechanism —
learning, reasoning, remembering – are in fact implemented.
In the sequel, italic expressions should be understood as technically defined terms in the model and “quoted” expressions
are intended to evoke a certain “meaning” whose precise definition is out of the scope of the model.

Abstract
In this paper we describe a cognitive model based on the systemic approach and on the Autopoiesis theory. The syntactical
definition of the model consists of logical propositions but the
semantic definition includes, besides the usual truth value assignments, what we call emotional flavors, that correspond to
the state of the agent’s body translated into cognitive terms.
The combination between logical propositions and emotional
flavors allows the agent to learn and memorize relevant propositions that can be used for reasoning. These propositions are
represented in a specific format – prime implicants/implicates
– which is enriched with annotations that explicitly store the
internal relations among their literals. Based on this representation, learning, reasoning and memory mechanisms are described.
Keywords: Artificial Intelligence, Cognitive Science, Learning, Memory, Reasoning, Situated cognition, Knowledge representation, Logic.

Introduction
In recent years the interest in logical models applied to practical problems such as planning (Bibel, 1997) (Pollock, 2000)
and robotics (CogRobo, 2003) has been increasing. Although
the limitations of the sense-model-plan-act approach have
been greatly overcome (Giacomo, Lespérance, Levesque, &
Sardina, 2002), the gap between the practical ad hoc path
to “behavior-based artificial creatures situated in the world”
(Brooks, 1991) and the logical approach is yet to be filled.
In this paper we define a logic-based generic model for
a cognitive agent. This model found inspiration in several
sources. From the systemic approach (Morin, 1991) and
from the Autopoiesis theory (Varela, 1989) came the hypothesis that cognition is an emergent property of a cyclic dynamic self-organizing process. From the Theory of Evolution (Darwin, 1998) and from the Memetics theory (Dawkins,
1976) came the belief that variability and selection is the
base of both life and cognition. From (dilettante) neurobiology (Changeux, 1983) (Changeux, 2002) (Damasio, 1994)
(Damasio, 2000) came the guidelines for pursuing psychological plausibility. From Piaget’s Genetic Epistemology (Piaget, 2001) came the requirement that learning and cognition
should be closely related and, therefore, that the cognitive
modeling process should strongly depend on the cognitive
agent’s particular history. From the logicist school (Newell,
1980) (Newell, 1982) (Brachman & Levesque, 1985) and
the work on Cognitive Robotics (Levesque, Pirri, & Reiter,
1998) came the focus on computational logical models. From
Wittgenstein (Wittgenstein, 1933) came the intended epistemological status of logic.
Assuming that cognition can be captured by a computational model (Scheutz, 2002) and that this model is somehow

External View
The proposed cognitive agent is immersed in an unknown environment and is going through an “experiential flux”. This
flux, its domain according to the Autopoiesis theory nomenclature, is represented as a set of primitive propositional symbols – P = {p1 , . . . , pn } –, the simple unities that compose the
cognitive agent. In a first approximation, the only property of
each propositional symbol is its truth value.1 Therefore, the
space is defined as the set of all possible truth assignments
1 This property could be generalized to allow fuzzy (Yager &
Filev, 1994) or multiple values (Belnap, 1977). The adopted logic

107

agent has the motivation that good emotional flavors be true
and bad ones false.3
These emotional flavors correspond to what Damasio call
“emotions” (Damasio, 2003) and define the raison d’être of
the agent’s cognitive mechanism: to control the truth values of emotional flavors using controllable symbol manipulations. Damasio has suggested that while the senses of vision,
hearing, touch, taste and smell (the primitive propositional
symbols of the proposed model) function by nerve activation
patterns that correspond to the state of the external world;
emotions are nerve activation patterns that correspond to the
state of the internal world. If we experience a state of fear,
then our brains will record this body state in nerve cell activation patterns obtained from neural and hormonal feedback,
and this information (the emotional flavors of the proposed
model) may then be used to adapt behavior appropriately.
Clearly the binary nature of the adopted emotional flavors
is a coarse approximation of the concept intended by Damasio: based on the notion of somatic markers, he describes a
range of qualitatively different phenomena, from genetically
determined emotions to socially learned feelings.
From the cognitive point of view, any motivation is directly
or indirectly derived from the desire to control the truth values
of emotional flavors and therefore, beyond their truth values,
propositions only have “meaning” with respect to the emotional flavors to which they are associated.
To satisfy this motivation, the cognitive agent should be
able to learn the relevant relations between specific emotional
flavors and propositions built up from primitive propositional
symbols. Once a proposition is known to be true whenever
a given “good” emotional flavor is true, this proposition can
be associated with an abstract propositional symbol that becomes the cognitive counterpart of the emotional flavor. Using this proposition, the agent can “rationally” act on the truth
values of the proposition’s controllable symbols in such a way
that the proposition truth status is preserved when the values
of the uncontrollable symbols change. This relation between
an emotional flavor and an abstract propositional symbol (and
its learned proposition) is called a thought.
A thought combines an emotional flavor and a logical proposition. Such a proposition, because of its logical properties, can possibly be factored into simpler subpropositions and these sub-propositions can be associated
with abstract propositional symbols. This factoring mechanism can give rise to new derived thoughts, composed by
the sub-propositions and “refined” versions of the original
emotional flavor. These new emotional flavors are refined
in the sense that, although they share the same motivating character of the original emotional flavor, they are restricted by their associated sub-propositions and, therefore,
they can be recognized as “different” from one another with
respect to the environment. The new derived thoughts “en-

to this set of propositional symbols – A P . We also suppose
that, as time goes by, the environment drifts along the possible states through flips of the primitive propositional symbols
truth values.
Therefore, from the agent point of view, the environment,
including the agent itself, is “conceived” as an unbound
temporal sequence of assignments . . . , εt−1 , εt , εt+1 , . . . where
εi ∈ A P , εi : P → {true, f alse} are semantic functions that
map propositional symbols into truth values. The primitive
propositional symbols can be of two kinds: controllable and
uncontrollable. Roughly, uncontrollable symbols correspond
to perceptions and controllable ones to actions. Perceptions
may include internal perceptions, i.e., internal properties of
the agent that are “felt”, such as proprioceptive information
(Berthoz, 1997), and actions may include orders to the agent
body. Both controllable and uncontrollable symbols are “neutral”, in the sense that, a priori, the agent is indifferent to
which semantic values (true or false) they assume.
Primitive propositional symbols can be combined into
propositions that correspond to the composite unities, according to the Autopoiesis theory nomenclature. The organization that defines these propositions are simply the rules of
propositional logic syntax, i.e., a proposition is simply a well
formed formula of propositional logic and its semantics is derived from the truth values of its components as usual. The
structure associated with a specific proposition is the actual
syntactical expression by which it is represented. It should be
noted that a proposition can change its structure through any
logically valid transformation, e.g., the application of an inference rule. Because the proposed cognitive agent is a computational one, the particular adopted syntactical representation can have an important impact on the computational properties of the cognitive mechanism, such as efficiency, modularity and reuse potential. The composite unities can be used
as components of other composite unities, i.e., it is possible to
associate an abstract propositional symbol with a proposition
and to use it as an ordinary proposition symbol in another
proposition. Let Q = {q1 , q2 , . . .} be the set of all abstract
propositional symbols. Propositions in which abstract propositional symbols occur are called abstract propositions.
In order to “embody” the agent, we assume that the state
of the agent’s “body” can be perceived through emotional flavors.2 An emotional flavor has two complementary aspects:
from the cognitive point of view it can be true or false and
from the agent’s body point of view it can be, in a first approximation, either “good” or “bad”, in the sense that the
could also be first-order or even a higher-order logic, instead of
propositional logic.
2 It should be noted that the proposed cognitive model is intended
as the highest level of a multi-level architecture (Bittencourt, 1997).
The emotional flavors should be seen as the result of lower level processes associated with the internal functioning of the agent and with
its interaction with the external environment (e.g., (Bryson, Tanguy,
& Willis, 2004)). These processes are out of the scope of the present
cognitive model.

3 The

representation and use of motivation have been intensively
study in the multi-agent community. A popular approach is the
so-called Beliefs, Desires and Intentions (BDI) (Rao & Georgeff,
1995).

108

tangle”4 the “meaning” of the original emotional flavor with
the environment properties captured by their associated subpropositions, whose truth values ultimately depend on the
primitive propositional symbols.
The formalism that supports this entanglement is the central point of the proposed solution to the problem of extracting meaning from the environment. On the one hand, its syntactical abstraction mechanism allows for the construction of
complex control mechanisms, articulated with the environment properties through the progressive automatizing of the
lower level abstract propositions. On the other hand, semantically it allows that general emotional flavors such as hunger
and sexual attraction, be refined in principle into complex
thought structures able to control behaviors such as search
for eatable fruits and build a nest to show you are a good sexual partner; or even go to a French restaurant and buy a red
Ferrari.

clause entailment, implicants, equivalence, sentential entailment and model enumeration (Darwiche & Marquis,
2001).
4. Prime implicates and implicants of a proposition present a
holographic relationship, where each literal in a clause is
associated with a dual clause, and conversely. This allows
the identification of which dual clauses are “critically” affected by a given clause.

PIP = {(PIψ IPψ ) | ψ ∈ L (P)}

The first and most important property – the uniqueness of
the prime representations – deserves more comments. The
prime representations in PIP are unique in the sense that,
given a set P of propositional symbols, any proposition built
up with symbols of P has one and only one representation in
PIP, but the structure of any pair in PIP depends only on the
relations between the propositional symbols and not on their
identity with respect to the set P. Therefore, each pair represents a whole family of structurally identical propositions
that only differ in the names that are attributed to its variable
symbols. Propositions that belong to the same family can be
transformed among them simply by renaming their propositional symbols, possibly by a negated value.
The psychological properties of these proposition families,
such as the ease with which they are learned from examples,
were studied since the 1950s. In (Feldman, 2003), a review of
this research and a complete catalog of the propositions with
up to four propositional symbols are presented. The propositions are shown in complete DNF, in Polya’s hypercubebased notation, and in a (heuristically determined) minimal
form. To our knowledge, the prime forms were not yet considered either in the Boolean functions or in the psychological
research contexts.

This choice is due to the following properties of these normal forms:

Example 1 Consider the following propositional theory, represented in complete disjunctive normal form:

1. The prime form representations are unique (up to the order of literals, clauses and terms that occur in them). The
only other unique representation for propositions is complete DNF6 which is usually much larger than prime forms.

(a ∧ ¬b ∧ c) ∨ (¬a ∧ b ∧ ¬c) ∨ (a ∧ b ∧ ¬c)

Working Hypothesis
The main hypothesis underlying the proposed cognitive
model consists in restricting the organization and structure of
the propositions that participate in thoughts in such a way that
these propositions are always represented using prime normal forms. Given a proposition ψ, its prime normal forms
consist of a set of prime implicates (PIψ ) and a set of prime
implicants (IPψ ), defined as special cases of conjunctive normal forms (CNF) and disjunctive normal forms (DNF), respectively, that consist of the smallest sets of clauses (or
terms) closed for inference, without any subsumed clauses
(or terms), and not containing a literal and its negation.5 We
call PIP the set of all such pairs of prime representations:

It is one of the 162 possible Boolean functions with 3
propositional symbols. It also belongs to one of the 13 families in which these 162 functions are distributed. It has 3
models and its negation has 5 models, adding up to 23 = 8,
the number of possible models with 3 propositional symbols.
The prime implicates and prime implicants associated with
this family can be represented by the following PIP pair:

2. Given both prime representations of a given formula ψ, the
prime representations of its negation can be obtained directly:7 PI¬ψ = IPψ , IP¬ψ = PIψ .
3. The prime implicates and implicants of a proposition can
be queried in polynomial time for consistency, validity,

0 : (¬b{0} ∨ ¬c{1} )∧
 1 : (a{0} ∨ ¬c{1} )∧

 2 : (b{1} ∨ c{0} )∧
3 : (a{0} ∨ b{1} )


4 Although

“entangle” is not a technically defined term of the
model, it would be if the model is extended to emotional flavors, because it represents the necessary combination operation that would
have to be defined in any emotional flavor theory.
5 Due to space limitations the formal definitions of logical concepts are not included. They can be found, for instance, in (Fitting,
1990), (Herzig & Rifi, 1999), (Kean & Tsiknis, 1990).
6 The disjunction of all the models of the proposition represented
as conjunctions of literals.
7 We note A the formula A with the truth values of all its literals
flipped.



0 : (a{3,1} ∧ ¬b{0} ∧ c{2} )∨ 


1 : (b{3,2} ∧ ¬c{1,0} )

in which each literal is annotated with coordinates, that contain the clauses/terms to which they belong in the dual form.
A possible interpretation of the propositional symbols of the
theory could be:

109

• a - true if a prey is attractive as food. An abstract
propositional symbol, derived from uncontrollable primitive propositional symbols.

metaphorical relations can be used, for instance, to guide the
decomposition of new learned propositions into significant
sub-propositions.
These two dimensions can be compared with Jerry Fodor’s
basic methodological principle (Fodor, 1983) that considers
two fundamentally different types of mental processing: vertical processing, taking place in specialized systems that carry
out a restricted set of operations on a limited type of input,
and horizontal processing that are neither domain-specific nor
informationally encapsulated. Thoughts that share the same
emotional flavors are similar to the specialized Fodor’s vertical modules, and thoughts that share the same PIP pairs are
similar to the general horizontal modules, but in the proposed
model both types share the same representational structure.
These dimensions can also be seen as analogous to the two
judging (or, rational) functions of consciousness – Thinking
and Feeling – proposed by C. G. Jung (Jung, 1971). The other
two Jung’s functions of consciousness – Sensation and Intuition are perceiving (or, non-rational) functions and therefore
out of the scope of a cognitive model.
The cognitive agent performs two types of activities: the
actual interaction with the environment, its “daylight” activities, and the reorganization of its internal structure in order to
optimize storage space and reasoning efficiency, its “dreaming” activities.
During its interaction with the environment, at each moment there are several active emotional flavors. Each emotional flavor evokes a set of memorized thoughts in which it
participates (a thought may “entangle” more than one emotional flavor). To say that the agent is aware of a given
thought means that the agent wants to control the emotional
flavor entanglement associated with the thought and has available an instance of the PIP pair that represent the abstract
propositional symbol associated with that thought. Let the
present thought be τ. To attain the goal of controlling the
truth value of the associated emotional flavor entanglement,
the agent executes the following actions:

• b - true if a prey is too big. An abstract propositional symbol, derived from uncontrollable primitive propositional
symbols.
• c - true if the action of catching is adequate in the situation. An abstract propositional symbol, derived from controllable primitive propositional symbols.
This proposition could be associated with the emotional
flavor hunger leading to a thought that could be expressed
as: “To appease hunger, if there is a prey that is too big,
independently of it being attractive or not, do not catch it. If
it is not too big and attractive, then catch it.”

Internal View
Applying the adopted hypothesis to the proposed cognitive
model leads to a more precise definition of the organization
and structure of the agent’s cognitive mechanism. A thought τ
is (re-)defined as a relation between an abstract propositional
symbol (associated with a logical proposition represented by
a PIP pair) and an emotional flavor. This relation consists of
three elements:8
• A generic pair πτ ∈ PIP with variable propositional symbols V (πτ ) = {x1 , . . . , xk }.
• A set of propositional symbols Pτ = {p1 , . . . , pk } associated with an emotional flavor9 and with its cognitive counterpart, the abstract propositional symbol qτ .
• A mapping µτ : Pτ → V (πτ ) that associates with each pi a
xi that occur in the PIP pair.
It should be noted that: (i) every thought is an operational
recipe to control the truth value of an emotional flavor; (ii)
each emotional flavor can be associated with different PIP
pairs (different ways to control its value) and thus can participate in different thoughts; (iii) the PIP pairs are independent
of the thought contents and can also participate in different
thoughts.
This last property shows that, besides the semantic relation
that associates thoughts that share the same emotional flavors,
thoughts can also be associated through a syntactical relation,
when they share the same PIP pair. Because the syntactical
relation occurs at the proposition level and not at the propositional symbol level, it can lead to significant metaphorical
relations across different domains (Johnson, 1987). These

• Instantiate the PIP pair πτ using the propositional symbols
in Pτ and the mapping µτ .
• Apply the appropriate reasoning method on the PIP pair.
• Decide to act according to the deduced controllable propositional symbols or continue to analyze new thoughts.
These new thoughts may have three sources: (i) If one or
more of the propositional symbols in Pτ are abstract then
their associated thoughts can become new thoughts. (ii)
Other thoughts that share the same PIP pair πτ can become
new thoughts. (iii) Another emotional flavor may become
active and invoke new thoughts.

8 These three elements are analogous to the three “subjects” in
the semiosis definition: “(...) an action, or influence, which is, or
involves, a cooperation of three subjects, such as a sign, its object,
and its interpretant; this tri-relative influence not being in any way
resolvable into actions between pairs.” (Peirce, 1974)
9 We explicitly avoid representing the emotional flavor in the formalism, because only its syntactical counterpart actually belongs to
the proposed cognitive model.

During the reorganization period, the agent does not interact with the environment. It executes an internal activity
that can be defined as a search for the “most suitable” definitions of abstract propositional symbols with respect to storage
space and reasoning efficiency. According to the principles

110

sented in example 1. Intellectual learning consists in receiving a CNF representation of the proposition.
During the “dreaming” activities, the learned DNF and
CNF forms are transformed by the learning mechanism into
the prime implicants and prime implicates of the proposition.
Once one of the forms in available the other can be calculated to form a PIP pair that can be integrated into the memory contents according to the semantic and syntactical dimensions.
The PIP pair can now be used to reason. For instance,
given a partial assignment that falsifies the symbol b, a disjunctive reasoning would reduce the thought proposition to
a ∧ c, guiding the agent to verify the value of a to decide
which value of c is adequate. Given a partial assignment
that attributes the value true to the symbol a, a conjunctive
reasoning would result in two rules ¬b ∨ ¬c and b ∨ c, that
read “if big do not catch, if not big then catch”.
In the semantic dimension, the memory mechanism would
associate the thought in the example to other thoughts that
can satisfy the emotional symbol hunger, e.g., a thought for
selecting eatable fruits if the agent is omnivore. In the syntactical dimension, the associated thought would share the PIP
pair but not the emotional flavor, e.g., the emotional flavor
“relieve the stress” could use the same PIP pair with the following interpretation: a - true if you feel angry, b - true if you
are talking with the boss and c - true if you criticize the one
you are talking to.

adopted in (Bittencourt, 1997), we assume that this optimization mechanism is implemented through an internal evolutionary algorithm that evolves the best representations.
The interaction activity uses the environment semantics
compressed into memorized thoughts, together with all its relations with similar thoughts that share either the same emotional goals or the same operational patterns, to learn and generate behavior intended to control the truth values of active
emotional flavors. The reorganization activity builds and refines these relations.
To implement these two types of activities it is necessary
to define a mechanism that acquires the agent propositions, a
learning method; a mechanism that uses the acquired propositions in order to recognize and predict states of the environment, a reasoning method; and, finally, a maintenance mechanism that optimizes, according to some given criteria, the
proposition representation, a memory management method.10
Two learning mechanisms are possible: practical learning,
in which the the patterns of perceptions (uncontrollable symbol values) and actions (controllable symbol values), that result in an intended truth value for a given emotional flavor are
directly learned from the environment; and intellectual learning, in which a trusted oracle communicates all the rules that
define the relevant proposition.
To reason means to take into account the effect of a partial assignment describing the environment on the PIP pair
associated with the thought and to calculate a new resulting
PIP pair. This new pair can be seen as a complex action rule,
specific to the situation described by the partial assignment:
the disjunctive part indicates which are the closest situations
in which the proposition is true and the conjunctive part can
be used as local rules to choose an appropriate action.
The agent’s memory contains thoughts. These thoughts are
“indexed” according to two different dimensions: a semantic
one, that associates thoughts that share the same emotional
flavor, and a syntactical one, that associates thoughts that
share the same PIP pair. The “daylight” activity of the memory is to provide relevant thoughts to be used to control active
emotional flavors. The “dreaming” activity consists of organizing the structure of memorized thoughts in such a way that
the “remembering” mechanism works effectively. The goal
of the organizing mechanism is to find (or better, to evolve)
sensible abstract propositional symbol definitions that facilitate the storage, inference and communication of thoughts.
The search for these definitions is done by some evolutionary
algorithm.

Conclusion
The paper has described a logical cognitive model that allows
an agent to learn, reason and remember using “significant
propositions”. The main hypothesis underlying the model
is that prime implicants/implicates are good candidates for
“cognitive building blocks”. The model also assumes that the
agent is “embodied” and that its “body” state is reported to
the cognitive mechanism through emotional symbols, whose
exact nature is not specified. Using these assumptions the
concept of thought is formally defined and its properties explored.
Future work includes implementing experiments where the
whole model, including emotional flavors and evolutionary
optimization, could be tested. Some possible application domains that are being investigated are robot soccer (Costa &
Bittencourt, 2000) and Internet search (Freitas & Bittencourt,
2003). We also intend to refine the logical part of the model
introducing modal and first-order formalisms (Bittencourt &
Tonin, 2001).

Example 2 Consider the thought introduced in example 1,
practical learning consists in being exposed to the eight situations that correspond to the models of the proposition and to
memorize the models associated with situations that best satisfy the emotional symbol hunger. Syntactically, these models
are represented by the complete DNF of the proposition pre-

References
Baum, E. B. (2004). What is thought? A Bradford Book The
MIT Press. (p. 478)
Belnap, N. (1977). A useful four-valued logic. In J. Dunn
& G. Epstein (Eds.), Modern uses of multiple-valued logics
(pp. 8–37). D. Reidel Pub. Co., Dordrecht, Holland.
Berthoz, A. (1997). Le sens du mouvement. Editions Odile
Jacob, Paris.

10 A more formal presentation of the mathematical details of these
mechanism is done elsewhere (Marchi & Bittencourt, 2004).

111

Bibel, W. (1997). Let’s plan it deductively. In Proceedings
of IJCAI 15 (p. 1549-1562). Nagoya, Japan, August 23-29:
Morgan Kaufmann (ISBN 1-55860-480-4).
Bittencourt, G. (1997). In the quest of the missing link. In
Proceedings of IJCAI 15 (p. 310-315). Nagoya, Japan, August 23-29: Morgan Kaufmann (ISBN 1-55860-480-4).
Bittencourt, G., & Tonin, I. (2001). An algorithm for dual
transformation in first-order logic. Journal of Automated
Reasoning, 27(4), 353-389.
Brachman, R., & Levesque, H. (Eds.). (1985). Readings in
knowledge representation. Morgan Kaufmann Publishers,
Inc., Los Altos, CA.
Brooks, R. A. (1991, January). Intelligence without representation. Artificial Intelligence (Special Volume Foundations
of Artificial Intelligence), 47(1-3), 139-159.
Bryson, J. J., Tanguy, E. A. R., & Willis, P. J. (2004). The
role of emotions in modular intelligent control. AISB Quarterly The Newsletter of the Society for the Study of Artificial
Intelligence and Simulation of Behaviour, 117.
Changeux, J.-P. (1983). L’homme neuronal. Collection
Pluriel, Librairie Arthème Fayard.
Changeux, J.-P. (2002). L’homme de vérité. Harvard University Press Odile Jacod. (p. 402)
Costa, A. C. P. L., & Bittencourt, G. (2000). Dynamic social knowledge: A comparative evaluation. In
Proceedings of the International joint conference IBERAMIA’2000/SBIA’2000 (Vol. 1952, p. 176-185). Lecture
Notes in Artificial Intelligence, Springer Verlag.
Damasio, A. R. (1994). Descartes’ error: Emotion, reason,
and the human brain. G.P. Putnam’s Sons, New York, NY.
Damasio, A. R. (2000). The feeling of what happens:
Body and emotion in the making of consciousness. Harvest
Books.
Damasio, A. R. (2003). Looking for Spinoza joy, sorrow, and
the feeling brain. Harcourt Books, Orlando, Florida, USA.
Darwiche, A., & Marquis, P. (2001, August). A perspective on knowledge compilation. In Proceedings of the 17th
international joint conference on artificial intelligence (IJCAI’01) (p. 175-182). Seattle, Washington, USA.
Darwin, C. (1998). The origin of species. New York: Modern
Library.
Dawkins, R. (1976). The selfish gene. Oxford University
Press.
Feldman, J. (2003). A catalog of boolean concepts. Journal
of Mathematical Psychology, 47, 75-89.
Fitting, M. (1990). First-order logic and automated theorem
proving. Springer Verlag, New York.
Fodor, J. A. (1983). The modularity of mind. A Bradford
Book The MIT Press.
Freitas, F. L. G., & Bittencourt, G. (2003). An ontologybased architecture for cooperative information agents. In
Proceedings of IJCAI 18 (p. 37-42). Acapulco, Mexico,
August 9-15: Professional Book Center, Denver, Colorado
(ISBN 0-127-05661-0).

Giacomo, G. D., Lespérance, Y., Levesque, H., & Sardina,
S. (2002). On the semantics of deliberation in Indigolog
- from theory to implementation. In D. M. D. Fensel
F. Giunchiglia & M.-A. Williams (Eds.), Principles of
knowledge representation and reasoning (kr2002) (p. 603614). Morgan Kaufmann. (Toulouse, France, April 22-25)
Herzig, A., & Rifi, O. (1999). Propositional belief base update and minimal change. Artificial Intelligence, 115(1),
107-138.
Johnson, M. (1987). The body in the mind the bodily basis
of meaning, imagination, and reason. The University of
Chicago Press. (p. 233)
Jung, C. G. (1971). Psychological types (collected works,
vol. 6). Princeton, NJ: Princeton University.
Kean, A., & Tsiknis, G. (1990). An incremental method for
generating prime implicants/implicates. Journal of Symbolic Computation, 9, 185-206.
Levesque, H. J., Pirri, F., & Reiter, R. (1998). Foundations for
the situation calculus. Electronic Transactions on Artificial
Intelligence, 2, 159-178.
Marchi, J., & Bittencourt, G. (2004). Propositional reasoning for an embodied cognitive model. In Proceedings
of the XVII Brazilian symposium on artificial intelligence
(SBIA’04). September 29 - October 1, São Luís, Maranhão,
Brazil: Lecture Notes in Artificial Intelligence, Springer
Verlag.
Morin, E. (1991). La méthode 4, les idées. Editions du Seuil,
Paris.
Newell, A. (1980). Physical symbol systems. Cognitive Science, 4, 135-183.
Newell, A. (1982). The knowledge level. Artificial Intelligence, 18, 87- 127.
Peirce, C. (1974). The collected papers of C.S. Peirce. Harvard University Press, Cambridge, Mass.
Piaget, J. (2001). The psychology of intelligence. Routledge
Classics. (Malcolm Piercy and D. E. Berlyne (translators))
Pollock, J. L. (2000). In ATAL’99: 6th international workshop on intelligent agents, agent theories, architectures,
and languages (pp. 71–90). London, UK: Springer-Verlag.
Rao, A. S., & Georgeff, M. P. (1995). BDI agents: from theory to practice. In Proceedings of the first international
conference on multi-agent systems (pp. 312–319). San
Francisco, CA: MIT Press.
Scherl, R., & Levesque, H. J. (2003, March). Knowledge, action, and the frame problem. Artificial Intelligence, 1(144),
1–39.
Scheutz, M. (Ed.). (2002). Computationalism new directions.
A Bradford Book The MIT Press.
Varela, F. J. (1989). Autonomie et connaissance: Essai sur le
vivant. Editions du Seuil, Paris.
Wittgenstein, L. (1933). Tractatus logico-philosophicus.
Routledge & K. Paul, London. (Originally published in
1922)
Yager, R., & Filev, D. (Eds.). (1994). Essentials of fuzzy
modeling and control. John Wiley & Sons, Inc.

112

