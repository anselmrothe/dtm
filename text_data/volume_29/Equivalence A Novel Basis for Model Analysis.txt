UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Equivalence: A Novel Basis for Model Analysis
Permalink
https://escholarship.org/uc/item/4qq353dw
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 29(29)
Authors
Stewart, Terrence C.
WEst, Robert L.
Publication Date
2007-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                 Equivalence: A Novel Basis for Model Analysis
                                               Terrence C. Stewart (terry@ccmlab.ca)
                                             Robert L. West (robert_west@carleton.ca)
                                            Institute of Cognitive Science, Carleton University
                                       1125 Colonel By Drive, Ottawa, Ontario, K1S 5B6 Canada
                             Abstract                                    Repeated Binary Choice
                                                                         One of the simplest tasks in experimental psychology is the
   As cognitive models are developed that are meant to apply to
   a broad range of phenomena, it is necessary to evaluate how           Repeated Binary Choice (RBC) task. Here, participants
   successfully they do so. This is commonly done by measures            make a series of either/or decisions, usually represented by
   such as the Mean Squared Error.              We propose and           pressing one of two buttons.           After each decision,
   demonstrate an alternate approach based on a measure of               reinforcement feedback of some kind is provided to the
   statistical equivalence. Instead of using sample means, this          participant, indicating how good that choice was. This
   method uses confidence intervals, and places an upper bound           allows them to refine their choices. For example, choice A
   on how wrong the model may be, given the uncertainties in             may be correct 70% of the time, while choice B is correct
   the data. We apply this to the RELACS model in various
                                                                         the remaining 30% of the time. By giving feedback as to
   different repeated binary choice tasks. We show that the
   equivalence measure identifies ranges of canonical parameter          whether the correct choice was made, the subject will
   settings that are equally equivalent. It also identifies              eventually choose A more often than B. Of particular
   experimental conditions that are not yet modelled well.               interest is how much more often A is chosen over B, and
   Keywords: modeling; statistics; equivalence; repeated binary          how this changes over time.
   choice; RELACS; parameter spaces; philosophy of modeling                 In much of the early work on this task (e.g., Myers et al.,
                                                                         1963), the feedback was limited to a simple correct/incorrect
                          Introduction                                   indication. In such situations, the overall result in human
When evaluating a cognitive model of a phenomenon, it is                 participants is generally characterized as “probability
common practice to determine how closely the model's                     matching”. That is, if A is correct 70% of the time, then the
behaviour matches that of the real system being modelled.                participant will choose A around 70% of the time. This is
This is typically done through finding the mean squared                  somewhat surprising, given that the optimal behaviour in
error, and there is often a determination of a “best fit”                this condition is to choose A 100% of the time.
parameter setting for the model. Other approaches (e.g.,                    It is also possible to include numerical rewards within the
Bayesian methods) requiring assumptions about statistical                task feedback. This can either involve just a single number
distributions and prior probabilities are not considered here.           indicating the reward given for the button that was pressed
   In (Stewart, 2006) and (Stewart, in press), we have argued            (the “minimal information” paradigm), or two values can be
for supplementing this approach by also determining the set              provided, indicating both the reward the participant has
of equivalent models. This set consists of all models that               received as well as the reward they would have received if
cannot be statistically distinguished from the original data at          they had have pressed the other button (the “complete
a given probability (p<0.05). This set of models (one for                feedback” paradigm). These rewards can be probabilistic;
each possible combination of parameter settings) determines              for example, pressing button A may give 1 point half of the
the space of potential explanations for the phenomenon at                time and 10 points the other half of the time, while button B
hand.1       In contrast, the best fitting model (however                may always give 7 points.
determined) can be seen as the best predictor of the                        This work has resulted in a large collection of
particular data set that was used.                                       experimental results about the frequency of choices in
   In this paper, we apply this approach to a well-established           various conditions. However, the precise pattern of these
model of repeated binary choice behaviour in humans.                     choices and the mechanism(s) underlying this performance
Specifically, we examine the Reinforcement Learning                      are unclear. Although the idea that participants tend to
Among Cognitive Strategies (RELACS) model (Erev &                        match probabilities is prevalent, Friedman and Massaro
Barron, 2005), and determine what potential explanations                 (1998) note that “probability matching in binary choice ... is
arise from it. This analysis is contrasted with the more                 less robust than most psychologists seem to believe.” The
typical fitting analysis performed by the original developers            actual observed behaviour is much more complex, and a
of the model. This work is a part of a project examining                 variety of models have been proposed to account for it.
various cognitive models of this phenomenon to determine
key experiments that may distinguish between them.                       Existing Models
                                                                         Given the simplicity of this task, a wide variety of existing
1
  There are, of course, further criteria that would be needed before     cognitive models could be applied.              Indeed, any
a model can be safely considered to be an explanation. Statistical       reinforcement-based model would be a natural approach,
equivalence (to a specified degree) on the set of relevant measures      including all Reinforcement Learning (RL) systems.
is necessary for an explanation, but is not sufficient.
                                                                     659

However, there are a number of robust behaviours observed             model should be compared to) is equally likely to be
in Repeated Binary Choice experiments that do not                     anywhere within the confidence interval of the observed
generally occur in RL systems. This has prompted a variety            data. The equivalence measure is meant to take this into
of specific models to account for these effects. Recently,            account, and can generalize to any other statistic (such as
attention has been focused on the RELACS model by Erev                the variance, median, skew, or kurtosis).
and Barron (2005), which was developed to be a descriptive               As presented here, the equivalence measure is
account of this range of results.                                     conservative. In (Tryon, 2001), it is shown that if means are
   The basis of this model is inherent in its name:                   being measured and if we assume the data is normally
Reinforcement Learning Among Cognitive Strategies. It                 distributed, these confidence intervals can be reduced by a
consists of three separate learning systems, each of which            factor of
uses a slightly different approach (Fast Best Reply; Loss
Aversion and Case-Based Reasoning; and Diminishing                              CI scaling=
                                                                                              S  2
                                                                                                  M S H
                                                                                                         2
                                                                                                                             (Eq 2)
Random Choice and Slow Best Reply with Exploration). A                                         S M S H
fourth system is in charge of learning which of the three                However, this scaling factor is not used in our work. The
strategies is currently doing the best job (i.e. resulting in the     first reason for this is that there is generally much more
most reward), and on the basis of this deciding which of the          model data available than real world data (i.e. much higher
three strategies should currently be followed. For more               N). This means the scaling factor is generally close to 1
details, see (Erev & Barron, 2005).                                   (0.9~0.95), and so has little effect. The main reason,
   It should be noted that the RBC task has also been                 however, is that avoiding this assumption makes the
investigated using ACT-R models (e.g., Fu & Anderson,                 technique applicable to non-normally distributed data, and
2006). As part of our ongoing work, we are examining not              can thus be applied when comparing statistics other than the
only RELACS, but also various ACT-R models, including                 mean. This allows us to compare the standard deviations of
ones both procedural-memory based and declarative-                    the model and human performance (or any other measure
memory based via sequential dependencies. We are also                 for which we can determine confidence intervals).
examining the use of Clarion on this task. For space                     Since the equivalence measure relies on confidence
reasons, we restrict this paper to the RELACS model only.             intervals, attaining accurate intervals is important. When
                                                                      raw data is available, we use the bootstrap non-parametric
                Measuring Equivalence                                 confidence interval (Davison and Hinkley, 1997). This
The key measure used here to compare model and human                  makes no assumptions about the underlying distribution of
performance on this task involves the equivalence threshold           the data, and is suitable for any statistic. When only
(E). This indicates how wrong the model could be, given               summary data is available (as it is for the experimental
the available information. This is calculated by examining            results discussed herein), we use the standard method for
the maximum difference between the 95% confidence                     estimating confidence intervals for the mean (using the t
intervals, as shown in Equation 1.                                    distribution) and standard deviation (using χ2 ).
   E=max M U −H L , H U −M L                                        Multiple Measures
                                                          (Eq 1)
                                                                      Since repeated binary choice behaviour is revealed only by
  Here, the model confidence interval is ML to MU and the             examining a large number of different experimental
human data's confidence interval is HL to HU. As noted in             conditions, we must have a method for combining data from
(Tryon, 2001), this is a conservative version of an inverted          these conditions. The basic equivalence measure described
t-test, where H0:|μM-μH|>E rather than H0:μM-μH=0 (called an          above is suitable for comparing model performance in one
equivalence test in Barker et al, 2002). So, if we use                experimental condition to that of human participants in that
Equation 1 with 95% confidence intervals and find an                  same condition. In order to combine across multiple
equivalence of 150 milliseconds, then we can be 95%                   conditions, there must be some way of determining how
confident that the model differs from the human                       well the model performs overall.
performance by no more than 150 milliseconds.                            In the standard MSD approach, the measures across the
  It should be noted that this measure differs considerably           different conditions are combined to their mean value. This
from the standard approach of looking at the squared                  indicates how close the model is to the real data on average.
difference between the model's mean performance and the               Thus it is possible for the model to be very close on some
human mean performance. Such a comparison is made in                  measures, but much worse than average on others.
the common Mean Squared Difference (MSD) measure and                     Our approach is that, instead of taking the average
even in the (less common) use of the correlation between              difference across measures, we use the maximum difference.
model and human performance. The key limitation with                  That is, if a model is very close on two measures, but highly
these measures is that they rely on the sample means, and             different on a third measure, then the overall equivalence for
are thus a measure of how well the model matches to the               that model should be the value for the third measure. The
particular set of individuals being studied. It must be               overall equivalence is thus an indication that all measures
remembered that the population mean (which is what a                  under consideration are at worst different from the human
                                                                  660

data by the given amount.2 It is also possible to scale these           many other parameter settings perform just as well, in a
values before finding the maximum. Scaling by a factor                  statistical equivalence sense. The fact that one model gives
such as the size of the real-world confidence interval                  a slightly better match than the others is not necessarily an
ensures that measures with high uncertainty do not dominate             indication of a better parameter setting, as it may be an
the result. This also provides a simple interpretation to the           indication of over-fitting to the particular sample data.
resulting number: if it is below 1, then the model is                      From an equivalence testing perspective, every model
statistically indistinguishable from the real-world results at          with an Er less than 1 is equally good (i.e. they are all within
the chosen confidence level.3 The resulting measure is                  the same confidence level). If the results produce an area of
termed relativized equivalence (Er).                                    equivalent models within the model space, then the
                 max M i ,U −H i , L , H i , U −M i , L               parameter values within that area can be seen as canonical
   E r =max i                                                (Eq 3)     parameter ranges. This is the range of parameter values
                              H i ,U −H i , L                           over which the model behaves similarly to human
   We are unaware of any other research using this measure              participants. For a discussion of the concept of canonical
for evaluating the quality of a cognitive model. However, a             parameter values, see (Anderson & Lebiere, 1998).
suggestion that this sort of measure might be possible was                However, it is also possible to find disjoint areas of the
independently noted in a footnote in (Axtell et al, 1996) for           parameter space which provide equivalent models. These
use in model/model comparisons.                                         represent alternate explanations of the human behaviour.
                                                                        Once these alternatives are identified, future experiments
Evaluating RELACS                                                       can be developed to differentiate them.
As with most cognitive models, RELACS has a variety of                    Also, when dealing with such a large set of measures, it is
parameters that govern its behaviour (α, β, λ, and κ)4. α               quite possible that certain parameter settings will result in
determines how quickly the exploration strategy learns                  models that are equivalent on some measures but not others,
(larger values are faster). β does the same for the fast reply          and vice versa for other parameter settings. In this situation,
strategy. λ controls the balance between exploration and                the model cannot explain both measures, but can explain
exploitation, with larger values indicating less exploration.           either. These occurrences must be identified so that model
κ adjusts the loss aversion system, with higher values being            developers can resolve them (perhaps by adding
more accurate in estimating previous losses.                            mechanisms that adjust parameters based on some change
   The different combinations of parameters define a space              between the conditions).
of different models, each of which may behave differently.                Since it is impossible to evaluate every parameter setting,
In the original work with RELACS, Erev and Barron                       in this work we sample the parameter space using the values
searched this parameter space and identified one “best                  shown in Table 1, for a total of 3,456 settings. We have
fitting” model. This was the model at α=0.00125, β=0.2,                 performed explorations outside of this space, but have not
λ=8, and κ=4, which had a MSD of 0.0036.                                found significant changes in behaviour outside these values.
   Two things are unclear from this result. First, since MSD
is being used, we do not know how accurate the model is on                      Table 1: RELACS Parameter Values Examined
any particular measure. There may be a few conditions for                             α (exploration learning rate)
which the model gives extremely different results. Indeed, a                               0 0.01 0.02 0.03 0.04 0.05
visual inspection of the plots in (Erev & Barron, 2005,
Figures 2-4) reveals that measures 8, 28, and 33 differ by                            β    (quick learning rate)
significantly more than the average difference of 0.06.                                    0 0.01 0.02 0.05 0.1 0.2 0.5 1
However, none of the original analysis makes any use of                               λ    (conservativeness)
confidence intervals, so it is impossible to determine how                                 1 2 4 8 16 32 64 128
well the model is performing on these conditions. It may be
that a difference this large is merely due to statistical                             κ    (loss aversion)
sampling (especially since many of the studies used by Erev                                0 1 2 4 8 16 32 64 128
and Barron have only 10 to 14 subjects).
   The second ambiguity in the original analysis is how well                                    Human Data
other parameter settings perform. One parameter setting is              To simplify the comparison between our approach and the
given as the best numerical match. However, it may be that              standard one, we use the same set of human data as found in
2
  The risk of a bad set of data eliminating a good model is handled     (Erev & Barron, 2005). This is a set of 40 different
by selectively removing conditions from consideration.                  conditions, mostly consisting of previous studies by Erev
3
  It is also possible to choose some predefined scaling factor for      and his colleagues. Precise details on all these conditions
each measure, indicating how close we require the model to be for       can be found in their paper. The majority of these
a particular purpose. This is highly recommended when the               conditions use the minimal information paradigm (a single
empirical data has small confidence intervals.                          numerical reward value shown after each choice), and the
4
  Other possible changes to RELACS, such as eliminating one of          rewards are generally particular values with different
the three strategies, or choosing randomly between them, can be
                                                                        probabilities. For example, in condition #23, pressing A
treated as non-numerical parameters, but are not discussed here.
                                                                    661

gives a reward of 32 10% of the time and 0 the remaining            black line has been added indicating Er=1. Every point
90%, while pressing B gives a reward of 3 all the time.             inside that contour (coloured pure white) indicates a model
  Two conditions (#29 and #30) had to be removed from               that gives performance statistically equivalent to the human
consideration, since no information was available on their          performance (at p<0.05).
standard deviations, making it impossible to estimate
confidence intervals. Also, standard deviation information          Payoff Variability
was only available for the 2nd block of 100 trials in each          There were no parameter values that matched for all 16
condition (with the exception of conditions #15 to #20,             conditions examining payoff variability. At most 14
which give the 4th block), so only this block is considered.        conditions are matched; in these cases, #1, #7, #11, and #13
  The remaining conditions are divided into four categories,        are the most problematic. Conditions #1 and #7 are the
based on the effects being demonstrated. Most of the first          simplest cases (A always giving a reward of 11 and B
22 conditions (with the exception of #15 to #20)                    always giving 10), while #11 and #13 are the most complex,
demonstrate variations on the Payoff Variability Effect.            giving real-value rewards (with a Gaussian distribution), as
This involves adjusting the variation in the outcomes for a         opposed to a one of a small fixed number of rewards.
given choice, without adjusting the mean outcome.                      If these four conditions are eliminated from consideration,
Observed behaviour changes from risk-seeking to risk-               many parameter settings fit the remaining conditions (72 out
aversion depending on whether the variability is associated         of 3,456). Figure 1a) shows that when κ and λ are low (0
with the overall best option                                        and 1, respectively), there are a large group of equivalent
  Conditions #15 to #20 examine changing the magnitude              models with α at 0.01~0.02, and β anywhere from 0.02 to
of the reward. Choice A is correct 60%, 70%, or 80% of the          0.2. Figure 1b) shows another view of this same cluster,
time, and the reward is 1 or 10. This translates into different     indicating that it extends up to κ of at least 8 and λ≈2.
monetary rewards given to the participants.                         Figure 1d) shows three separate clusters with different
   Conditions #23 to #25 investigate the under-weighting of         values of κ and λ. Figure 1c) shows how the lower-right
rare outcomes. Here, events that are very rare (<10%) seem          cluster in 1d) is shaped as α and β vary.
to not be considered when determining expected outcomes.               The main result from Figure 1 is that a wide variety of
   The remaining 15 conditions (#26 to #40) deal with the           parameter settings produce models that are statistically
Loss Rate Effect. In these cases, “when the action that             indistinguishable from the human participants for these
maximizes expected value increases the probability of               conditions. If we note that setting either α or β to zero
losses, people tend to avoid it” (Erev & Barron, 2005, p.           effectively turns off that component of the RELACS model,
917). That is, a choice that has a higher expected value in         we can see that these models function wildly differently,
the long run may be chosen less often if it is comprised of         and yet are still overtly similar to the empirical data.
many small losses and few large gains.
                            Results
These four sets of conditions can be examined separately
before combining them for an overall understanding of the
RELACS model's performance. The goal here is to
understand what conditions RELACS can explain, and to
see what can be learned about the parameter values.
   Since there are 3,456 different parameter settings to be
considered, we cannot present all the gathered data about
exactly which parameter settings lead to equivalent models.
Instead, we present cross-sections of the model parameter
space. These cross-sections are created by holding two of
the parameters constant and allowing the other two to vary.
For consistency, the same parameters are held to the same
values to create the cross-sections in each set of graphs.
Cross-section (a) is λ=1, κ=0, (b) is α=0.01, β=0.05, (c) is
λ=2, κ=32, and (d) is α=0, β=0.1. These values were chosen
to maximize the amount of space shown as equivalent to the
human data. Of course, a three dimensional display could
further improve the data visualization, but there will always
be problems with four or more parameters. Showing
optimal cross-sections with this methodology allows for
identification of interesting areas in any case.                     Figure 1: Er for 10 payoff variability conditions. All points
   These cross-sections are shown as contour plots of Er,                 inside the black line (Er<1) indicate models that are
where darker shading is less equivalence (larger Er). A               equivalent to human participants, and are shown in white.
                                                                662

Adjusting Reward                                                 good results for this is not surprising given the failure for
The six conditions where reward is adjusted (#15 to #20) are     #11 and #13. Also, #33 and #34 come from a series of
not graphed here. This is because the RELACS model turns         studies where the only adjustment is the absolute value of
out to not change its behaviour at all when reward values        the reward. RELACS does not change behaviour in such
are scaled. For confirmation, examining (Erev & Barron,          situations, explaining its failure here.
Figure 2, p. 915) reveals identical model performance, but          However, there seems to be no clear reason why
changing human data. This fact is not commented on in that       RELACS would fail on condition #40. This measure is a
paper. It is clear, then, that RELACS cannot account for         fairly standard experiment where choice A always gives a
this aspect of human performance. It should be noted that        reward of -3, and choice B gives a reward of -4 80% of the
the MSD approach to analysis used by Erev and Barron did         time and 0 the rest of the time. This is merely the opposite
not highlight this fact.                                         of #21, which was modelled well by RELACS.
Underweighting
A wide selection of parameter settings are equivalent in
terms of the three measures for underweighting. These are
shown in Figure 2. Note that 2d) shows that every model
with α=0 and β=0.1 is equivalent to the human performance,
regardless of λ and κ values.
                                                                    Figure 3: Er for 13 loss rate effect conditions. All points
                                                                      inside the black line (Er<1) indicate models that are
                                                                   equivalent to human participants, and are shown in white.
                                                                 Overall Results
                                                                 If the measures indicated in the previous figures are
                                                                 combined, we attain Figure 4: an overall plot showing the
   Figure 2: Er for 3 underweighting conditions. All points      parameters which give equivalent models on all of the above
     inside the black line (Er<1) indicate models that are       conditions (except those explicitly eliminated above).
  equivalent to human participants, and are shown in white.         This reveals two small regions of equivalent models.
                                                                 Figures 4a) and 4b) show that the models near (α=0.01,
Loss Rate Effect                                                 β=0.05, λ=1, κ=0) are indistinguishable from the human
                                                                 data. Figure 4c) and 4d) indicate a separate parameter
As with the payoff variability conditions, there were no
                                                                 setting that is also equivalent: (α=0, β=0.2, λ=2, κ=8).
parameter settings that were equivalent on all 13 loss rate
                                                                 Looking at these values, we can see that these models
effect conditions. However, if #28, #33, #34, and #40 are
                                                                 perform very different internal processes, yet both result in
removed, then equivalent models are found for the
                                                                 equally convincing accounts of human performance over
remaining conditions. These are shown in Figure 3. Note
                                                                 this set of RBC task conditions. The first model makes no
that 3b) indicates only one equivalent model in the very
                                                                 use of the loss aversion system (κ=0), while the second
bottom-left of the graph (λ=1, κ=0), and 3c) shows a very
                                                                 never adjusts from its initial state in the exploration system
few equivalent models (α=0, β=0.1~0.5).
                                                                 (α=0). Both of these models are also better (from an
   Condition #28 involves a reward with a Gaussian
                                                                 equivalence standpoint) than the “best fit” model identified
distribution (as did #11 and #13), so the failure to produce
                                                                 in (Erev & Barron, 2005).
                                                             663

                                                                                                Conclusion
                                                                      The equivalence method introduced here supplements the
                                                                      standard “fitting” approach to model evaluation by taking
                                                                      into account confidence intervals and by treating all models
                                                                      that are statistically indistinguishable from the real data as
                                                                      equally good potential explanations.            We chose the
                                                                      RELACS model to demonstrate this process because the
                                                                      authors had done what few do; they evaluated their model
                                                                      by comparison to a large and diverse set of real-world
                                                                      results. In our opinion, this is critical for evaluating models
                                                                      past a certain level of development. What we have shown
                                                                      in this paper is the value of using an equivalence testing
                                                                      approach for this type of evaluation. Particular conditions
                                                                      can be identified as problematic, and canonical parameter
                                                                      ranges can be identified.
                                                                         To facilitate further investigation into the RELACS model
                                                                      and other uses of the equivalence approach, all source code,
                                                                      data, and analysis tools used are freely available at
                                                                      <http://ccmlab.ca> as part of the CCMSuite tool-kit.
                                                                                                References
                                                                      Anderson, J.R. and Lebiere, C. (1998). The Atomic
   Figure 4: Er for all 26 remaining conditions. All points              Components of Thought. Mahwah, NJ: Erlbaum.
     inside the black line (Er<1) indicate models that are            Axtell, R., Axelrod, R., Epstein, J.R., and Cohen, M.D.
  equivalent to human participants, and are shown in white.              (1996). Aligning Simulation Models: A Case of Study
                                                                         and Results. Computational Mathematical Organization
                                                                         Theory, 1(2), 123-141.
                         Discussion                                   Barker L.E., Luman E.T., McCauley M.M., Chu Y.R.
The equivalence method provides us with a collection of                  (2002) Assessing equivalence: An alternative to time use
models, all of which must be treated equally. These are all              of difference tests for measuring disparities in vaccination
parameter settings for models that cannot be distinguished               coverage. American J. of Epidemiology; 156:1056-1061.
from the real data (at p<0.05). These models have differing           Davison, A.C. and Hinkley, D.V. (1997). Bootstrap
mean squared error values, but there is no sense in which                Methods and Their Application. Cambridge University.
any of these settings are more equivalent than others (i.e. a         Erev, I. and Barron, G. (2005). On Adaptation,
better fit may indicate a better model or it may indicate                Maximization, and Reinforcement Learning Among
over-fitting the data). This process identifies sets of                  Cognitive Strategies. Psych. Review, 112(4), 913-931.
canonical parameter values that constrain the use of the              Friedman D., and Massaro D. W. (1998) Understanding
RELACS model. These parameter values turn out to not                     variability in binary and continuous choice. Psychonomic
include the “best ” model found by the original researchers.             Bulletin & Review, 5, 370–389.
   We have also identified those conditions that are not              Fu, W-T. & Anderson, J.R. (2006). From recurrent choice to
                                                                         skill learning: A reinforcement-learning model. Journal
modelled well. RELACS was unable to simultaneously
                                                                         of Experimental Psychology: General, 135(2), 184-206.
model all of these conditions at p<0.05. Instead of averaging
                                                                      Myers, J. L., Fort, J. G., Katz, L., and Suydam, M. M.
across them, we identified those for which RELACS failed.                (1963). Differential monetary gains and losses and event
For these, RELACS needs different parameter values for                   probability in a two-choice situation. Journal of
different conditions. Future extensions of RELACS may                    Experimental Psychology, 66, 521–522.
incorporate mechanisms for detecting these situations and             Stewart, T.C. (2006) Tools and Techniques for Quantitative
then adjusting its own parameters, but none currently exist.             and Predictive Cognitive Science. 28th Annual Meeting of
   It is also possible that the failure of RELACS on any of              the Cognitive Science Society.
these conditions is due to statistical error, as any of the real-     Stewart, T.C. (in press) Model-Based Science and Artificial
world data sets could be outside the confidence interval                 Cognitive Systems: The Philosophy of Computational
(indeed, up to 5% of the conditions may be). Gathering                   Modelling. Theoria et Historia Scientiarum: Special
more human data resolves this by exposing atypical results.              Issue on Artificial Life.
   We have also restricted ourselves to the mean                      Tryon, W. (2001). Evaluating statistical difference,
performance only. All of this analysis could also be applied             equivalence, and indeterminacy using inferential
to any other statistic, such as the variance. This has not               confidence intervals: An integrated alternative method of
been done here, as RELACS is known to have a much                        conducting null hypothesis statistical tests. Psychological
smaller variance than the human subjects                                 Methods, 6(4):371-386.
                                                                  664

