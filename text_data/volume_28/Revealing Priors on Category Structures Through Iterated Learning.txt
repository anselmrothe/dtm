UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Revealing Priors on Category Structures Through Iterated Learning
Permalink
https://escholarship.org/uc/item/0563b7zh
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Christian, Brian R.
Griffiths, Thomas L.
Kalish, Michael L.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

   Revealing Priors on Category Structures Through Iterated Learning
                             Thomas L. Griffiths (Thomas Griffiths@brown.edu)
                              Brian R. Christian (Brian Christian@brown.edu)
              Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912
                                     Michael L. Kalish (kalish@louisiana.edu)
                Institute of Cognitive Science, University of Louisiana at Lafayette, Lafayette, LA 70504
                          Abstract                                learners. The basic idea behind this method is simple:
                                                                  having people solve a series of inductive problems where
   We present a novel experimental method for identifying         the hypothesis selected on one trial is used to generate
   the inductive biases of human learners. The key idea           the data observed on the next. We call this method “iter-
   behind this method is simple: we use participants’ re-         ated learning”, due to its close correspondence to a class
   sponses on one trial to generate the stimuli they see on
   the next. A theoretical analysis of this “iterated learn-      of models that have been used to study language evolu-
   ing” procedure, based on the assumption that learners          tion (Kirby, 2001). Our use of iterated learning is mo-
   are Bayesian agents, predicts that it should reveal the        tivated by a theoretical analysis that shows that, in the
   inductive biases of the learners, as expressed in a prior      case where the learners are Bayesian agents, the proba-
   probability distribution. We test this prediction through
   two experiments in iterated category learning.                 bility that a learner chooses a particular hypothesis will
                                                                  ultimately be determined by their inductive biases, as ex-
                                                                  pressed in a prior probability distribution over hypothe-
   Many of the cognitive challenges faced by human be-            ses (Griffiths and Kalish, 2005). We tested this predic-
ings can be framed as inductive problems, in which ob-            tion in two experiments with stimuli for which people’s
served data are used to evaluate underdetermined hy-              inductive biases are well understood, examining whether
potheses. To take two common examples, in language                the outcome of iterated learning is consistent with previ-
acquisition the hypotheses are languages and the data             ous work on the difficulty of learning different category
are the utterances to which the learner is exposed, while         structures (Shepard et al., 1961; Feldman, 2000).
in category learning the hypotheses are category struc-              The plan of the paper is as follows. First, we outline
tures and the data are the observed members of a cate-            the theoretical background behind our approach, laying
gory. Analyses of inductive problems in both philosophy           out the formal framework that justifies the use of iter-
(Goodman, 1955) and learning theory (Geman, Bienen-               ated learning as a method for determining the biases
stock, & Doursat, 1992; Kearns & Vazirani, 1994; Vap-             of learners. We then provide a more detailed analysis
nik, 1995) stress the importance of combining the evi-            of the specific case of inferring category structures from
dence provided by the data with a priori biases about the         observed members, presenting a Bayesian model of this
plausibility of hypotheses. These biases prevent learners         task. The predictions of this model, and of our more
from jumping to outlandish conclusions that might be              general theoretical framework, are tested through two
consistent with the data, and can produce successful in-          experiments. We close by discussing the implications of
ductive inferences so long as they approximately capture          these experiments for iterated learning as a method for
the nature of the learner’s environment.                          revealing inductive biases, and some future directions.
   If we want to understand how people solve inductive
problems, we need to understand the biases that con-
strain their inferences. However, identifying these biases
                                                                  Iterated learning reveals inductive biases
can be a challenge. Inductive biases can result from bi-          Iterated learning has been discussed most extensively in
ological constraints on learning, general-purpose prin-           the context of language evolution, where it is seen as
ciples such as a preference for simplicity, or previous           a potential explanation for the structure of human lan-
domain-specific experience, and in many cases will be             guages. Language, like many other aspects of human
a mixture of all three. Not all of these factors are avail-       culture, can only be learned from other people, who were
able to introspection, and as a consequence assessment            once learners themselves. The consequences of this fact
of the biases of learners has tended to be indirect. In the       have been studied using what Kirby (2001) termed the
past, people’s inductive biases have been evaluated using         iterated learning model, in which several generations of
experiments that examine whether, for example, certain            one or more learners each learn from data produced by
category structures are easier or harder to learn (e.g.,          the previous generation. For example, with one learner
Shepard, Hovland, & Jenkins, 1961), or by assessing how           per generation, the first learner is exposed to some initial
well models that embody particular biases correspond to           data, forms a hypothesis about the language it repre-
human judgments (e.g., Tenenbaum, 1999).                          sents, and generates new data from that language. This
   In this paper, we explore a novel experimental method          new data are passed to the second learner, who infers a
that makes it possible to directly determine the biases of        hypothesis and generates data from it that are provided
                                                             1394

to the third learner, and so forth. Through simulations,                            Type I            Type II               Type III
Kirby and his colleagues have shown that languages                   a
with properties similar to those of human languages can
emerge from iterated learning with simple learning algo-
rithms (Kirby, 2001; Smith, Kirby, & Brighton, 2003).
   Griffiths and Kalish (2005) provided a formal analy-
sis of the consequences of iterated learning for the case                           Type IV           Type V                Type VI
where learners are Bayesian agents. Assume that a
learner has a set of hypotheses, H, and that their biases
are encoded through a prior probability distribution,
P (h), specifying the probability a learner assigns to the
truth of each hypothesis h ∈ H before seeing some data
d. Bayesian agents evaluate hypotheses using a principle
                                                                         Type                     Iteration/Block
of probability theory called Bayes’ rule. This principle
states that the posterior probability P (h|d) that should            b          0    1   2    3   4     5       6   7   8      9   10
                                                                           I
                                                                                                                                        Dependent chains
be assigned to each hypothesis h after seeing d is
                                                                          II
                            P (d|h)P (h)                                  III
             P (h|d) = P             0     0
                                                        (1)
                         h0 ∈H P (d|h )P (h )                             IV
                                                                          V
where P (d|h), the likelihood, indicates the probability of               VI
the data d under hypothesis h.
                                                                          I
                                                                                                                                        Independent chains
   We can now formally analyze the consequences of iter-
ated learning with Bayesian learners. Each learner uses                   II
Bayes’ rule to compute a posterior distribution over the                  III
hypothesis of the previous learner, samples a hypothesis                  IV
from this distribution, and generates the data provided                   V
to the next learner using this hypothesis. The probabil-                  VI
ity that the nth learner chooses hypothesis hn given that
the previous learner chose hypothesis hn−1 is                     Figure 1: (a) Types of category structures for stimuli
                          X                                       defined on three binary dimensions. Vertices are objects,
          P (hn |hn−1 ) =     P (hn |d)P (d|hn−1 )      (2)       with color indicating category membership. (b) Design
                           d                                      of iterated category learning experiments (see Method).
where P (hn |d) is the posterior probability obtained from
Equation 1. This specifies the transition matrix of               comes from the literature on category learning. Shep-
a Markov chain, since the hypothesis chosen by each               ard et al. (1961) conducted an experiment exploring the
learner depends only on that chosen by the previous               relative difficulty of learning different kinds of category
learner. Griffiths and Kalish (2005) showed that when             structures defined on objects that vary along three bi-
the learners share a common prior, P (h), the stationary          nary dimensions, such as shape, color, and size. Cate-
distribution of this Markov chain is simply the prior as-         gories are defined in terms of which subsets of the eight
sumed by the learners. The Markov chain will converge             possible objects they contain. In principle, there are 256
to this distribution under fairly general conditions (e.g.,       different category structures, but if we restrict ourselves
Norris, 1997). This means that the probability that the           to categories with four members, this number is reduced
last in a long line of learners chooses a particular hypoth-      to 70. If we collapse together structures that are identi-
esis is equal to the prior probability of that hypothesis,        cal up to rotation and negation, this number is reduced
regardless of the data provided to the first learner.             still further, giving us a total of six different types of
                                                                  category structures. Examples of categories belonging
      Testing convergence to the prior                            to these six types are shown in Figure 1(a).
The theoretical results summarized in the previous sec-              Shepard et al. (1961) found that there is great varia-
tion raise a tantalizing possibility: if iterated learning        tion in the ease with which people learn different types
converges to the prior, perhaps we can reproduce it in the        of category structures. Type I, in which membership
laboratory as a means of determining people’s inductive           is defined along a single dimension, is easiest to learn,
biases. However, these results are based on the assump-           followed by Type II, in which two dimensions are suffi-
tion that the learners are Bayesian agents. Whether the           cient to identify members. Next come Types III, IV, and
predictions of this account will be borne out with human          V, which all correspond to a one-dimensional rule plus
learners is an empirical question.                                an exception, and are about equally difficult to learn.
   To test whether iterated learning with human learners          Type VI, in which no two members share a value along
will converge to an equilibrium reflecting people’s induc-        more than one dimension, is hardest to learn. Similar
tive biases, we need to use a set of stimuli for which            results have been obtained by Nosofsky, Gluck, Palmeri,
these biases are well understood. One such set of stimuli         McKinley, and Glauthier (1994) and Feldman (2000).
                                                              1395

   Since difficulty in learning a hypothesis is an indica-         distribution over hypotheses, represented as a column
tion that it may be inconsistent with the inductive biases         vector, the distribution over hypotheses at each subse-
of the learner, these stimuli provide a way to test the pre-       quent iteration can be computed by multiplying this vec-
dictions of our theoretical account of iterated learning:          tor by the transition matrix. This can be used to make
we can examine whether iterated category learning using            predictions not just about the asymptotic distribution
these stimuli converges to a distribution over hypotheses          over hypotheses, which we know to be the prior, P (h),
consistent with the results of Shepard et al. (1961). How-        but also the dynamics of iterated learning. The asymp-
ever, in order to do this efficiently, we need to introduce       totic distribution will not be affected by the amount of
one more innovation. Our discussion so far has focused            data seen by the learners, but the dynamics will change
on cases where iterated learning occurs “between sub-             significantly depending on the degree to which the data
jects”, with each learner seeing data generated by a pre-         constrain the choices of the learners.
vious learner. Iterated learning experiments using such              Our two experiments examine iterated category learn-
a design can be cumbersome, requiring a large number              ing in two regimes: with two positive examples (m = 2),
of participants in order to have chains of learners of any        and with three positive examples (m = 3). The goals
appreciable length. Fortunately, the same analysis ap-            of these experiments are twofold. First, to determine
plies to iterated learning with a “within subjects” design,       whether iterated learning converges to a distribution
where a single learner responds to stimuli that are based         over hypotheses consistent with people’s inductive bi-
on his or her own previous responses. In the remainder of         ases, and second, to establish whether the fine-grained
the paper, we discuss two experiments in within-subjects          dynamics of this process are consistent with the Bayesian
iterated category learning. However, before we present            framework presented above.
these experiments, we will describe a formal model that
we will use to make quantitative predictions about the               Experiment 1: Two positive examples
dynamics of iterated category learning.
                                                                  Method
    Modeling iterated category learning
                                                                  Participants Participants were 20 members of the
Our account of category learning is based on a model de-          Brown University community, paid $8 per hour for
veloped by Tenenbaum (1999) and Tenenbaum and Grif-               their participation, and 97 University of Louisiana at
fiths (2001). The data, d, that people observe will consist       Lafayette undergraduates participating for course credit.
of m positive examples – objects that belong to a cate-
gory. If we assume that objects are drawn by sampling             Stimuli Following Feldman (2000), stimuli were
without replacement, then the probability of a particular         “amoebae” with a wavy cell wall and an internal nu-
set of m positive examples is                                     cleus. Nuclei varied along three binary dimensions:
                                                                  shape (round/square), color (black/white), and size
                                                                  (large/small). Categories were “species” of amoebae.
                    
                       (|h| − m)!/|h|! d ⊂ h
          P (d|h) =                                       (3)
                               0              otherwise           Procedure The design of the experiment is shown in
                                                                  Figure 1(b). Each participant completed 120 trials of
where |h| denotes the number of objects in the category           category learning, being presented with two positive ex-
associated with hypothesis h and d ⊂ h indicates that             amples from a category and selecting one of the fifteen
all m objects in d are members of h (and m < |h|).                category structures that were consistent with those ex-
We can now use Bayes’ rule to evaluate the posterior              amples. To remove memory demands, examples and hy-
probability of any hypothesis h given some set of objects         potheses were presented simultaneously on a computer
d. Combining the likelihood given by Equation 3 with a            screen, as shown in Figure 2 (a), and participants se-
prior on hypotheses, P (h), we obtain                             lected category structures using a mouse. The 120 trials
                                                                  were divided into 10 blocks of 12, corresponding to 10
                           P (h) (|h| − m)!/|h|!
          P (h|d) = P                                     (4)     iterations of learning. Within each block, six trials be-
                        h0 ⊃d P (h0 ) (|h0 | − m)!/|h0 |!         longed to “dependent” chains, with the objects being
for all hypotheses h such that d ⊂ h, and 0 otherwise.
All categories are of the same size (|h| = 4), so we have
                                                                      a                                 b
                                    P (h)
                   P (h|d) = P                0
                                                          (5)
                                  h ⊃d P (h )
                                   0
which is simply the prior, normalized over all hypotheses
consistent with d.
   The likelihood given by Equation 3 and posterior dis-
tribution from Equation 5 can be substituted into Equa-
tion 2 to find the transition matrix of the Markov chain
on hypotheses induced by iterated learning. The result is
a square matrix where the number of rows and columns               Figure 2: Sample displays showing stimuli and possible
is equal to the number of hypotheses. Given an initial             responses for (a) Experiment 1 and (b) Experiment 2.
                                                              1396

                          All chains                  Type I chains           Type II chains          Type III chains          Type IV chains          Type V chains           Type VI chains
               1
              0.8
Probability
              0.6
              0.4
              0.2
               0
                    0         5           10      0         5          10 0         5          10 0          5          10 0         5          10 0         5          10 0         5          10
                        Bayesian model                Bayesian model          Bayesian model          Bayesian model           Bayesian model          Bayesian model          Bayesian model
               1
              0.8
Probability
              0.6
                                       Type I          Type IV
              0.4                      Type II         Type V
                                       Type III        Type VI
              0.2
               0
                    0          5          10      0          5         10 0          5         10 0          5          10 0          5         10 0          5         10 0          5         10
                           Iteration                     Iteration               Iteration               Iteration                Iteration               Iteration               Iteration
  Figure 3: Results of Experiment 1. The leftmost panel shows data aggregated over all chains, while the remaining
  panels break this down by the type of category structure used to initialize each chain. Each point represents the
  contribution of 69 subjects, so the maximum standard error is 0.025 for the aggregate data, and 0.060 otherwise.
  generated at random from the hypothesis selected on                                                       algorithm (Dempster et al., 1977) was used to simulta-
  the previous trial in that chain. Each dependent chain                                                    neously estimate the prior and probabilistically classify
  was initialized with a hypothesis corresponding to a dif-                                                 participants as either responding at random or in ac-
  ferent type of category structure (these hypotheses are                                                   cord with the model. The resulting parameter estimates
  referred to as iteration 0). The other six trials within                                                  gave Types I-VI prior probabilities of 0.687, 0.136, 0.048,
  each block were part of “independent” chains, with the                                                    0.012, 0.079, and 0.039, respectively. Computing the ac-
  objects being generated from a randomly selected hy-                                                      tual prior probability of a category structure of each type
  pothesis corresponding to one of the six types. Trials                                                    requires dividing by the number of categories of each
  were randomized within blocks.                                                                            type, being 6, 6, 24, 8, 24, and 2, respectively. These
                                                                                                            probabilities are consistent with previous findings con-
  Results and Discussion                                                                                    cerning the relative difficulty of learning different types
  While previous work makes qualitative predictions about                                                   of category structures, with the only possible exception
  the relative prior probabilities of the six different types of                                            being the relatively high probability of Type VI struc-
  category structure, the model presented above provides                                                    tures. Using these parameters, we computed the proba-
  the opportunity to estimate these quantities directly, and                                                bility that each participant was responding at random.
  use them to make quantitative predictions about the dy-                                                   The remainder of our analyses use only the 69 partici-
  namics of iterated learning. We specified a prior over                                                    pants for whom this probability was less than 0.5.
  hypotheses, P (h), by assuming that the prior probabil-                                                      The leftmost panel of Figure 3 shows how the propor-
  ity was affected only by the type of category structure                                                   tion of participants selecting a hypothesis of each type
  to which a hypothesis corresponds, being uniform within                                                   varies as a function of the number of iterations, aggregat-
  types. Since there are six such types, the prior can be                                                   ing over all six dependent chains. To evaluate whether
  completely specified by five parameters, giving the prob-                                                 iterated learning was having an effect on responses, we
  abilities of Types I-V (the probability of Type VI follows                                                ran a χ2 test comparing the proportions of the six types
  from the fact that probabilities sum to one).                                                             across the independent and dependent chains at each
     The parameters of the prior were estimated from the                                                    iteration. The results of these tests were statistically
  frequencies with which participants selected hypotheses                                                   significant for all iterations after the third, with p < .01.
  given different sets of examples, aggregated across de-                                                      Figure 3 also shows the predictions of the Bayesian
  pendent and independent chains, rather than by optimiz-                                                   model outlined in the previous section when applied to
  ing the fit of the model to the dynamics of the data. Ac-                                                 this task. As can be seen from the figure, there is a re-
  cording to the Bayesian model, people’s choices should                                                    markably close correspondence between the predictions
  follow the distribution given by Equation 5. Parameters                                                   of the model and the human data, with a linear correla-
  were found using maximum-likelihood estimation. Pre-                                                      tion of r(58) = 0.997. In particular, both the model and
  liminary analyses indicated that a subset of the partici-                                                 the human data converge to an asymptotic distribution
  pants were responding at random, so a variant of the EM                                                   over hypotheses consistent with the prior. This close cor-
                                                                                                      1397

                          All chains              Type I chains           Type II chains          Type III chains           Type IV chains          Type V chains           Type VI chains
               1
              0.8
Probability
              0.6
              0.4
              0.2
               0
                    0         5          10   0         5          10 0         5          10 0          5          10 0          5          10 0         5          10 0         5          10
                        Bayesian model            Bayesian model          Bayesian model          Bayesian model           Bayesian model           Bayesian model          Bayesian model
               1
              0.8
Probability
              0.6
              0.4
              0.2
               0
                    0          5         10   0          5         10 0          5         10 0          5          10 0           5         10 0          5         10 0          5         10
                           Iteration                 Iteration               Iteration               Iteration                 Iteration               Iteration               Iteration
  Figure 4: Results of Experiment 2. The leftmost panel shows data aggregated over all chains, while the remaining
  panels break this down by the type of category structure used to initialize each chain. Each point represents the
  contribution of 64 subjects, so the maximum standard error is 0.026 for the aggregate data, and 0.062 otherwise.
  respondence prevails despite the fact that the model pa-                                              Stimuli            Stimuli were those used in Experiment 1.
  rameters were estimated from the hypotheses that people                                               Procedure The procedure was that of Experiment 1,
  selected across all trials, rather than explicitly attempt-                                           but three positive examples of each category were pre-
  ing to capture the dynamics of iterated learning. The                                                 sented on each trial. Since this meant only one member
  remaining panels of Figure 3 show a more fine-grained                                                 of the category was unknown, participants had to choose
  analysis of the correspondence between model and data,                                                a response from just five consistent hypotheses. Figure
  breaking the aggregate data shown in the leftmost panels                                              2 (b) shows a sample display from the experiment.
  up based on the type of category structure with which
  the chains were initialized. The model and data still
                                                                                                        Results and Discussion
  exhibit a strong correlation, with r(358) = 0.990, and
  both converge to a distribution consistent with the prior                                             The procedure developed for Experiment 1 was used to
  regardless of initial conditions.                                                                     estimate the parameters of the prior, resulting in prob-
     The results of this first experiment bear out the pre-                                             abilities of 0.651, 0.195, 0.040, 0.008, 0.062, and 0.044
  dictions of our theoretical framework, with human learn-                                              for Types I-VI respectively. These parameters were con-
  ers converging to a distribution over hypotheses consis-                                              sistent with both previous research and the estimates
  tent with their inductive biases. Furthermore, the dy-                                                from Experiment 1. Using these parameters, 64 partic-
  namics of this process correspond well with the quanti-                                               ipants were classified as responding non-randomly, and
  tative predictions of our Bayesian model, with conver-                                                were used in the remainder of our analyses. The left-
  gence occurring extremely rapidly regardless of initial                                               most panel of Figure 4 shows how the proportion of par-
  conditions. However, the speed of convergence prevents                                                ticipants selecting a hypothesis of each type varies as a
  a detailed analysis of the dynamics of iterated learning,                                             function of the number of iterations. χ2 tests found a sig-
  with very little variation in behavior following the sec-                                             nificant difference between dependent and independent
  ond or third iteration. To address this problem, our sec-                                             chains for every iteration after the third, with p < .01.
  ond experiment examined iterated category learning in                                                    Figure 4 also shows the predictions of the Bayesian
  a context where the data provided stronger constraints                                                model, which again correlated extremely well with the
  on hypotheses, reducing the rate of convergence.                                                      human data, r(58) = 0.992. The stronger constraints
                                                                                                        on choices imposed by using more examples resulted in
              Experiment 2: Three positive examples                                                     much slower convergence towards the prior for both the
                                                                                                        Bayesian model, and the human data. In particular, it
  Method
                                                                                                        seems that iterated learning has not fully converged after
  Participants Participants were 20 members of the                                                      10 iterations, with the prevalence of Type I still increas-
  Brown University community, paid $8 per hour for                                                      ing, and the prevalence of other types still decreasing.
  their participation, and 53 University of Louisiana at                                                One small difference from the predictions of the Bayesian
  Lafayette undergraduates participating for course credit.                                             model appears for the Type II structures, which should
                                                                                                  1398

still be decreasing at the end of the experiment, but ap-            There are a number of directions in which the experi-
pear to have stabilized at a slightly higher probability         ments presented in this paper could be extended. First,
than predicted by the model.                                     a more complete test of the predictions of our frame-
   The remaining panels in Figure 4 show the data broken         work, and of the Bayesian model outlined above, could
down across the six dependent chains. The slower con-            be conducted by considering a wider range of category
vergence results in some interesting differences in the dis-     learning tasks, potentially producing a deeper picture
tribution over hypotheses across chains. For example, in         of the dynamics of iterated category learning. Second,
the data for the Type VI chains, the dominance of Type           given the original proposal of iterated learning as a mode
I categories only emerges after a period in which Type V         of intergenerational knowledge transmission, exploration
increases in popularity. As can be seen from the figure,         of whether similar dynamics are observed when iterated
the Bayesian model does a good job of capturing these            learning occurs “between subjects” could provide insight
dynamics, with a correlation of r(358) = 0.990. The              into questions relating to the consequences of cultural
slight over-prevalence of Type II category structures rel-       evolution. However, perhaps the most exciting direction
ative to the model predictions is more pronounced for the        for future research is the investigation of people’s induc-
Type II and III chains, and seems to be complemented             tive biases in contexts where they remain unknown. By
by under-prevalence in Type I and IV chains. With                reproducing iterated learning in the laboratory, we may
sufficiently many iterations, the probabilities across all       be able to map out the implicit biases that are at the
chains should converge. The fact that they remain quite          heart of the remarkable human ability to solve problems
different at the end of the experiment suggests that these       requiring inductive inference.
discrepancies may be the result of noise rather than a           Acknowledgements We thank Anu Asnaani, Rebecca Cre-
systematic failure of the model.                                 mona, Alana Firl, and Vikash Mansinghka for discussions
                                                                 about this project and assistance in running experiments.
                 General Discussion
                                                                                         References
The results of our experiments bear out the predictions          Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977).
of both our theoretical framework, and our Bayesian                Maximum likelihood from incomplete data via the EM al-
model of iterated category learning. In both exper-                gorithm. Journal of the Royal Statistical Society, B, 39.
iments, the distribution over category structures con-           Feldman, J. (2000). Minimization of Boolean complexity in
                                                                   human concept learning. Nature, 407:630–633.
verged towards an equilibrium consistent with previous
                                                                 Geman, S., Bienenstock, E., and Doursat, R. (1992). Neural
research on learning difficulty (Shepard et al., 1961; Feld-       networks and the bias-variance dilemma. Neural Computa-
man, 2000; Nosofsky et al., 1994), with Type I structures          tion, 4:1–58.
being most prevalent, followed by Type II, and then the          Goodman, N. (1955). Fact, Fiction, and Forecast. Harvard
other four types. The dynamics of this convergence, as            University Press, Cambridge.
represented by the distribution over category structures         Griffiths, T. L. and Kalish, M. L. (2005). A Bayesian view
at each iteration, were also strongly in accord with our          of language evolution by iterated learning. In Bara, B. G.,
Bayesian model: the greater constraints on hypotheses             Barsalou, L., and Bucciarelli, M., editors, Proceedings of the
                                                                  Twenty-Seventh Annual Conference of the Cognitive Science
provided by three positive examples resulted in slower            Society, pages 827–832. Erlbaum, Mahwah, NJ.
convergence to the prior, and the Markov chains initial-         Kearns, M. and Vazirani, U. (1994). An introduction to com-
ized with different types of category structures showed           putational learning theory. MIT Press, Cambridge, MA.
fine-grained dynamics that closely matched the predic-           Kirby, S. (2001). Spontaneous evolution of linguistic struc-
tions of the Bayesian model. These results suggest that           ture: An iterated learning model of the emergence of regu-
iterated learning may provide a viable method for deter-          larity and irregularity. IEEE Journal of Evolutionary Com-
mining the inductive biases of learners.                          putation, 5:102–110.
                                                                 Norris, J. R. (1997). Markov Chains. Cambridge University
   One interesting aspect of our data is the persistently         Press, Cambridge, UK.
high probability of Type VI category structures. The             Nosofsky, R. M., Gluck, M., Palmeri, T. J., McKinley, S. C.,
previous research mentioned above suggested that Type             and Glauthier, P. (1994). Comparing models of rule-based
VI structures are hardest to learn, but the prior that            classification learning: A replication and extension of Shep-
seemed to characterize people’s inferences in our ex-             ard, Hovland, and Jenkins (1961). Memory and Cognition,
periments gave these structures higher probability than           22:352–369.
Types III-V. One possible explanation for this difference        Shepard, R. N., Hovland, C. I., and Jenkins, H. M. (1961).
                                                                  Learning and memorization of classifications. Psychological
is the lack of memory demands in our task. The experi-            Monographs, 75. 13, Whole No. 517.
ments that suggest Type VI structures are hard to learn          Smith, K., Kirby, S., and Brighton, H. (2003). Iterated learn-
required participants to remember a set of examples from          ing: A framework for the emergence of language. Artificial
the category, while in our experiments participants could         Life, 9:371–386.
see both the examples and the full set of possible cat-          Tenenbaum, J. B. (1999). A Bayesian framework for concept
egory structures. Type VI structures actually have far            learning. PhD thesis, Massachussets Institute of Technology,
                                                                  Cambridge, MA.
greater symmetry and simplicity than Types III-V, being
                                                                 Tenenbaum, J. B. and Griffiths, T. L. (2001). Generalization,
describable as the structures for which every two mem-            similarity, and Bayesian inference. Behavioral and Brain
bers have the same value on exactly one dimension. Our            Sciences, 24:629–641.
presentation format could have made this property more           Vapnik, V. N. (1995). The nature of statistical learning the-
apparent, resulting in a stronger preference for Type VI.         ory. Springer, New York.
                                                             1399

