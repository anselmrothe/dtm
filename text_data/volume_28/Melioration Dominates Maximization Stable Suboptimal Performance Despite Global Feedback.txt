UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Melioration Dominates Maximization: Stable Suboptimal Performance Despite Global
Feedback
Permalink
https://escholarship.org/uc/item/66q6p0nq
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Gray, Wayne D.
Neth, Hansjörg
Sims, Chris R.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                       Melioration Dominates Maximization:
                        Stable Suboptimal Performance Despite Global Feedback
                                       Hansjörg Neth, Chris R. Sims & Wayne D. Gray
                                                      Cognitive Science Department
                                                     Rensselaer Polytechnic Institute
                                                          Troy, NY 12180 USA
                                                      [nethh; simsc; grayw]@rpi.edu
                              Abstract                                     In this paper, we sketch the outline of a framework for
                                                                        understanding this phenomenon from an information
   Situations that present individuals with a conflict between          processing perspective. After presenting the results of two
   local and global gains often evoke a behavioral pattern known        empirical studies that demonstrate suboptimal choice
   as melioration — a preference for immediate rewards over
                                                                        behavior in humans, we develop a computational model that
   higher long-term gains. Using a variant of a binary forced-
   choice paradigm by Tunney & Shanks (2002), we explored
                                                                        explains these results in terms of capacity limitations and a
   the potential role of global feedback as a means to reduce this      competition between local and global feedback.
   bias. We hypothesized that frequent explicit feedback about
   future expected and optimal gains might enable decision              Melioration in Theory and Practice
   makers to overcome the documented tendency to meliorate              In an extensive series of experiments, Herrnstein and
   when choices are rewarded probabilistically. Our results             colleagues (1997) have documented many instances of
   suggest that the human tendency to meliorate is tenacious and
                                                                        motivated and systematic deviations from the rational ideal.
   even prospective normative feedback is insufficient to reliably
   overcome inefficient choice allocation. We identify human            When faced with a dilemma between short- and long-term
   memory limitations as a potential source of this problem and         rewards, both animals and humans appear to reliably favor
   sketch a reinforcement learning model that mimics the effects        high immediate reinforcements over a higher overall gain.
   of a variable feedback horizon on performance. We conclude              One reason why these findings are not yet widely known
   that melioration is a powerful explanatory mechanism that can        is that melioration and maximization only predict different
   account for a wide range of human behavior.                          behaviors in environments in which local and global
                                                                        optimization conflict. Imagine an environment in which one
                                                                        alternative (call it L) is always better than another (X);
                           Introduction                                 however, the more L is chosen, the worse both options
A specter is haunting psychology, decision sciences and                 become. Under certain circumstances, the optimal strategy
economic theory — the specter of maximization. It is an                 in this environment is to always choose the locally worse
intuitively appealing assumption that rational organisms                option, X. Figure 1 presents the details of such an
maximize their expected reward when making decisions.                   environment, where X stands for maximization, and L for
The idea of optimal choice allocation to available                      melioration. The two parallel lines are produced by the
alternatives (or maximization of utility) is often equated              functions Pmax ( h ) = Ah + B and Pmel ( h ) = Ah + B + C and
with the very concept of rationality and is one of the main
guiding principles of contemporary cognitive science.
   Despite its intuitive appeal, this notion of utility
maximization may be mistaken. One now familiar criticism
is encapsulated in Simon’s (1956) notion of satisficing. A
satisficing organism aspires to meet some subjective
satisfaction criterion, thus replacing the optimal solution
with a solution that is deemed ‘good enough’.
   The goal of this paper is to promote a less familiar but
just as profound alternative—a phenomenon known as
melioration (Herrnstein & Vaughn, 1980). In a nutshell, the
molecular mechanism underlying melioration is not the
achievement of maximal utility or subjective satisfaction,
but rather a general preference for high immediate rewards
over higher long-term gains. While the origins of this
research lie in studies of choice behavior in pigeons, the
tendency to meliorate has equally been documented for
humans (see Herrnstein, 1997).
                                                                                Figure 1: Environmental contingencies known to
                                                                               induce melioration behavior. (See text for details.)
                                                                    627

indicate the probability of receiving a reward by choosing            were probabilistic (Exp-2). This bias to focus on immediate
option X or L as a function of the choice history h, which is         gains was alleviated when payoffs were negative (Exp-3) or
defined as the percentage of choices to L over the w most             when the test phase was preceded by an exploration phase
recent trials. As both functions only differ by a constant C,         (Exp-4). In the absence of a principled account, these results
choosing L at any moment yields a higher expected payoff              appear like an assortment of unrelated phenomena,
than choosing X. While this makes L a locally dominating              suggesting that people’s choice allocation is heavily
alternative, we also need to consider the long-term effects of        context-dependent and subject to relatively random
choosing it. As every single choice of L increases the                situational constraints.
number of recent choices to L by 1 for the next w trials (i.e.,          As this seems unsatisfactory, we advocate a framework
shifts h by 1/w units to the right, relative to having chosen         for understanding melioration in terms of information
                                                                      processing and cognitive limitations. The conflict between
X), it results in a delayed and repeated cost of A/w on each
                                                                      melioration and maximization is a consequence of a
of the next w trials. Whenever the absolute magnitude of A
                                                                      competition on two different timescales: attention to short-
exceeds C, the global costs of choosing L outweigh its local          term rewards (on a local timescale) favors option L, whereas
benefits. (For values of w = 10, A = –2/3, B = 2/3, and C =           attention to long-term gains (or adopting a global
1/3, the long-term costs 2/3 of any choice of L exceed its            perspective) favors option X. While pigeons may be
immediate benefit 1/3 by 1/3.)                                        doomed to meliorate due to their inability to comprehend
   Another way of seeing the overall inferiority of option L          the long-term consequences of an action, a fundamental
despite its universal local dominance is to consider the              difference between pigeons and people is that the latter use
expected reward for a stable mix of choice allocations.               language to describe and abstract from properties of task
Always choosing L would yield a reward 33% of the time                environments. Experimenters routinely rely on this ability
(P1). The optimal long-term strategy is indicated by the              by providing verbal instructions to communicate aspects of
position on the abscissa at which the weighted average of             the task that are not directly observable or experienced, e.g.,
reward probabilities (drawn as a dashed line) is maximal.             hidden properties about task dynamics or extrapolations of
This is the case when X is chosen 100% of the time (P2).              the current performance into the future.
   Environmental contingencies like these may appear                     In our research, we focus on probabilistic rewards and
artificial, but there is nothing unusual per se about choices         investigate the use of feedback to direct attention away from
being rewarded probabilistically and incurring both short-            immediate outcomes and towards the global consequences
and long-term benefits and costs. Outside the experimental            of an action. Under this approach, the phenomenon of
laboratory, meliorating behavior has been demonstrated in a           melioration is cast as a competition between two sources of
wide range of tasks and domains. For instance, even highly            reward, with the goal of understanding how we can tip the
experienced users of interactive software packages routinely          balance in favor of globally optimal performance.
use inefficient procedures (Bhavani & John, 2000) and
novice typists prefer locally efficient visually-guided typing                  Lessons from a Failed Experiment
to a superior touch typing strategy (Yechiam et al., 2003).
                                                                      In a previous experiment (Neth, Sims & Gray, 2005) we
Fu and Gray (2004) have recently explained this ‘paradox of           explored the role of feedback frequency in a task modeled
the active user’ in terms of cost-benefit tradeoffs that favor        on Tunney and Shanks’ (2002) forced-choice paradigm,
small incremental gains of an interactive nature over less            using the reward contingencies described above. In addition
interactive but globally more efficient strategies.                   to the immediate reward obtained after each choice, we
   Beyond the realms of software applications, discounting            provided periodic global feedback designed to inform
local rewards in favor of higher global ones is notoriously           participants of the relative optimality of their recent choices
difficult—otherwise, nobody would ever drive without a                on a larger timescale (every 10 or 100 trials). Our aim was
seatbelt, postpone a dentist’s appointment, pollute the               to counteract the local push towards melioration by
environment, smoke cigarettes, or gamble.                             introducing an additional reward that favored maximization.
   At the core of meliorating behavior lies an inability or
unwillingness to discount high local rewards in favor of              Results Much to our surprise, our feedback manipulation
even higher global ones. Whereas previous research has                did not have the desired result. Instead, we were baffled by a
often cast this in clinical terms of self control, addiction, and     complete lack of maximization strategies.
impulsiveness (see Herrnstein, 1997, Ch. 5–9), we approach            Critique Our choice of providing feedback over 10 trials
the phenomenon as a problem of incomplete knowledge and               may have been inadequate. While focusing on 10-trial
a challenge to human information-processing limits.                   segments may encourage a more global perspective and
                                                                      facilitate a task representation on the scale that actually
Adopting a Global Perspective                                         determines the reward contingencies, it can be shown that it
In a series of experiments using the repeated forced-choice           is still advantageous to consistently meliorate when merely
paradigm described above, Tunney and Shanks (2002)                    extrapolating over units of 10 trials.
demonstrated that small changes in the type of payoffs can               Another potential problem was the counterfactual nature
have large effects on behavior. Whereas participants                  of the feedback provided to participants. Feedback of the
maximized when payoffs systematically varied in                       form “You won $x on the last n trials. If you had pursued
magnitude (Exp-1), they tended to meliorate when payoffs              the optimal strategy all along, you would have won $y.”
                                                                  628

implicitly directed attention to what participants did not do
so far and provided little indication of what they should do
instead. The hypothetical antecedent of the if-clause may
also have conveyed the misleading impression that
participants could not recover from past misallocations of
choices. The emphasis on what participants could have done
(given optimal performance) also rendered the feedback of
the optimal value entirely static, i.e., insensitive to the
current choices distribution of an individual. This also
created the possibility of nonsensical (or ‘contra-optimal’)
feedback when the sum of actually received rewards x (e.g.,
$0.24) exceeded the alleged ‘optimal’ reward y ($0.20).
                        Experiment:
           Providing Prospective Feedback
The current study attempted to address the above                     Figure 2: Screenshot of the experimental task window.
shortcomings by making several changes. First, rather than
providing retrospective and counterfactual feedback we
provided prospective feedback, for example, “If you               Design All participants received local feedback on the
continue the same strategy you can expect to win $x on the        presence or absence of a reward after each of 800 choices
next n trials. By adopting the optimal strategy, you could        and were displayed their cumulative winnings so far. The
expect to win $y instead.” Apart from a change in emphasis,       additional availability of global feedback distinguished
this change has the additional advantage that it allows to        between three conditions: Whereas a ‘No-Feedback’ control
compute and contrast the exact values of expected wins for        group did not receive any additional feedback, two groups
consistent continuation and maximization, based on the            received verbal feedback every 10 trials. For a ‘Future-20’
actual and current choice history of the individual.              group the current choice history was extrapolated 20 trials
   Second, we increased the minimal global feedback               into the future to contrast the expected payoff for continuing
horizon n to 20 trials, which we found to be the smallest         the current choice allocation ratio with the expected payoff
number of choices for which consistent maximization               for consistent maximization on those trials. For a ‘Future-
always outperforms not only melioration, but also all             All’ group the same rationale was applied over a larger
alternative choice allocation strategies.                         horizon, spanning from the current trial t to the end of the
   Third, we added a control condition that did not receive       session, i.e., the remaining 800–t trials. Thus, our
any verbal (global) feedback in addition to the rewards           experiment employed a mixed design of three between-
received on individual trials.                                    subjects conditions, each of which made 80 blocks of 10
   Fourth, and finally, we increased the number of trials         choices.
from 500 to 800.
                                                                  Procedure Participants were tested individually in a quiet
Method                                                            room. During the instructions, participants were informed
                                                                  that their choices could earn them a cash payment of up to
Participants Thirty RPI undergraduate students volunteered        $11, depending on their performance.
to participate to earn a performance-related cash reward.            Each individual choice was indicated by pressing either
Task Environment As shown in Figure 2, two buttons                the left or the right button. After each choice, both buttons
marked ‘Left’ and ‘Right’ were displayed at the bottom of         were disabled for .5 sec and the feedback from the previous
the task window. The top of the window listed the                 trial was updated. After the buttons were re-enabled the
participant’s cumulative winnings. The middle showed the          participant was free to make the next choice.
previous trial number, their choice on that trial, and the           Every 10 trials, the two global feedback conditions saw a
reward received for that choice.                                  feedback screen that occluded the task window and
   For each participant, the maximizing choice alternative X      contained the verbal feedback message.
was randomly assigned to either the left or right button. The        An experimental session was completed in 45 minutes on
possible payoff for each choice was a fixed $.02 reward that      average, including instructions.
was probabilistically received or not received on each trial.
   The current probability of receiving a reward upon             Predictions As the explicit global feedback was designed to
selecting an alternative was based on the participant’s           overcome the local bias towards melioration, we predicted
distribution of choices over the last 10 trials, using the        that both global feedback groups would select the
reward functions illustrated above. Over the course of 800        maximization choice more frequently than the No-Feedback
trials, consistent maximization would yield an expected           control group, which would result in higher overall gains. In
reward of $10.67, whereas consistent melioration would            addition, we expected maximization to be most facilitated
yield an expected reward of $5.33.                                for the Feedback-End group.
                                                              629

    Table 2: Choice allocations of individual participants on 40 blocks (of 20 trials each). Blocks with 17 or more L-choices
    were classified as melioration blocks, blocks with 17 or more X-choices were classified as maximization blocks, and all
    other blocks as indeterminate (–). The overall classification of individuals in the final column is based on their total
    number of choices. (If the sum to either alternative exceeds 437 out of 800, random allocation can be rejected at p < .01).
Group No.            1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   Tmax: Class.
        1           –    –    –    X    –    –    –    –    –    –    –    –    –    X    –    –    –    –    –     L   –    –     L   –     L   X    –    –    –     L    L   –    –    –    –    –    –    –    –    –     309    L
        2           –    –    –    –    –    –    –    –    X    –    X    –    –    X    –    –    –    X    –    X    –    –    X    –    X    –    –    –    –    –    X    –    –    –    X    –    X    –    –    –     472    X
        3           –    –    –    –    –    –    –    –    –     L    L   –    –    –    –    –     L   –    –    –    –    –    –    –    –    –    –     L   –    –    –    –    –    –    –     L    L   –    –    –     297    L
 No-Feedback
        4           –    –    –    –    –    –    –    –    –    –    –    –    –     L    L   –    –    –     L   –     L   –    –    –     L   –    –    –     L    L   –     L   –     L   –    –    –    X    X    X     298    L
        5           –    –    –    –    –    –    –    –    –    –    –    –     L   –     L   –     L   –     L    L    L   –     L    L    L    L    L   –    –    –    –    –    L     L    L   –     L    L   –    –     207    L
        6           –    –    –    –    –    –    L    –    –    –    –     L   –    –    –    –    –    –    –    –     L   –     L   –    –    –    –    –    –    –    –    –    –     L   –    –     L   –    –    –     247    L
        7           –    –    –    X    –    –    –    –    –    –    –    –    –     L   –     L   –    –    –    –    –     L   –    –     L   –    –    –    –    –    –    –    –    –    –     L   –    –    –    –     260    L
        8           –    –    –    –    –    –    –    –    –    –    –     L   –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –     295    L
        9           –    –    –    –    –    –    L    –    X    –     L    L    L   –    –    X    –     L   –    –    –    –    –     L   –     L    L    L   –     L    L   –    X    X    X    X    X    X    X    X     369    –
       10           –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –     L   –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –     269    L
       11           –    –    –    –    –    –    –    –    –     L   –    –    –    –    –    –    –    –    –    X    –    X    X    –    –    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X     589    X
       12           –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –     L   –    –     L   –    –     302    L
       13           –    –    –    –    –    –    –    –    L    –    –    –     L   –    –    –     L    L   –    –    –    –    –    –    –    –    –    –    –    –     L   –    –    –    –    –    –    –    –    –     264    L
 Feedback-20
       14           –    –    –    –    –    –    –    –    –    –    –    –    –     L   –    –    –    –    –    –    –    –    –     L   –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –     348    L
       15           –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –     L   –    –    –     L   L    –    –    –    –     L   –    –     304    L
       16           –    –    –    –    –    –    –    –    X    –     L   X    –    –    –    X    –    X    X    –    X    –    –    –    –    –    –    –    X     L   –     L   –     L   –    –     L    L   –    –     407    –
       17           –    –    –    –    –    –    –    –    L    –     L   –    –    –    –    –     L    L   –    –    –    –    –    –    –    –    –    –    –    –    –     L   –    –    –     L   –    –    –    X     298    L
       18           –    –    –    L    –    L    –    L    –    –    –     L   –    –    –    –     L   –    –    –    –     L   –    –     L    L   –    –    –    –     L   –    –    –    –     L    L    L   –     L    195    L
       19           –    –    –    –    –    –    L    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X     713    X
       20           –    –    –    –    –    –    L    L    L    –    –    –    –    –    –     L   –     L    L   –    –    –    –    –    –     L   X    –    –    –    –    –    –    –    –    –     L   –     L   –     281    L
       21           –    L    –    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X     758    X
       22           –    –    –    X    X    X    X    X    X    X    X    –    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X     760    X
 Feedback-End
       23           –    –    –    –    –    –    –    –    L    –     L   –    –    –    –    –    –    –    –    –    –    –     L    L   –    –    –     L   –    –     L   –    –    –     L   –    –    –    –    –     202    L
       24           –    –    L    L    –    –    –    –    L    –    –    –    –    –    –    –    –    –    –    –    –    –    –     L   –    –    –    –    X    X    X    X    X    X    X    X    X    X    X    X     431    –
       25           X    X    X    –    X    X    –    X    L    –    –    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X    X     748    X
       26           –    –    –    –    –    –    –    –    –    –    –    –     L   –    –    –    –     L   –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –     L   –    –     L   –     236    L
       27           –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –     300    L
       28           –    –    –    –    –    –    –    –    –    –    –    –    –    –    X    –    –    –    –    –    –    –    X     L   –    –    –    –    –    X    X    –    –     L   –    –     L   –     L   –     359    L
       29           –    –    –    –    –    –    –    L    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –    –     394    –
       30           –    –    –    –    –    –    –    –    –    –    –    –    –     L   –    –    –    –    –    –    –    –    –     L   –    –    –    –    –    –    –    –    –     L   –    –    –    –    –    –     225    L
                                                  Results                                                                            Table 2 (above) reveals structural regularities that were
We will first present performance results on an aggregate                                                                         obscured by the group averages. Consistent with the
level before considering individual choice allocations.                                                                           average trends, the total number of maximization blocks
   Table 1 (below) contains the overall wins and                                                                                  appears to be higher for the groups that received
percentages of maximization choices by experimental                                                                               prospective feedback. This applies particularly to the
condition. Despite consistent trends in the predicted                                                                             Feedback-End group in which three individuals
direction, the group differences are relatively small and                                                                         discovered the maximization strategy by the 4th block.
the within group variability is high. Comparing overall                                                                              If we count the total number of maximization blocks
wins and maximizations by group yielded two non-                                                                                  per participant the average count of 12.4 in the Feedback-
significant ANOVAs, F(2, 29) = 1.6, MSE = 1.33, p = .22                                                                           End group is more than twice that of the 6.0 average in
and F(2, 29) = 1.7, MSE = 434.6, p = .20, respectively,                                                                           the Feedback-20 group and more than 4 times the value of
suggesting that our feedback manipulations have failed                                                                            2.6 blocks for the No-Feedback group.
yet again. Even though two planned comparisons between                                                                               Similarly, when classifying the overall performance of
the extreme No-Feedback and Feedback-End conditions                                                                               each individual participant (by a binomial test rejecting
are marginally significant (p = .087 and p = .073,                                                                                the assumption of random choice allocation at p < .01) the
respectively) we cannot claim on the basis of group                                                                               Feedback-End group contained 3 maximizers, whereas
means that global feedback elicits a larger proportion of                                                                         the Feedback-20 group contained 2, and the No-Feedback
maximization choices and higher overall wins.                                                                                     group contained 1 (last column of Table 2). Although
   On the other hand, this conclusion is strangely at odds                                                                        these numbers are only descriptive, they still show that
with the impression gained when studying participants’                                                                            individual decision-makers were able to benefit from the
performance profiles. To illustrate the sequential choices                                                                        global feedback provided.
of individual decision makers we classified each block of                                                                            Another interesting pattern emerging from Table 2 is
20 choices as instances of unambiguous melioration or                                                                             that participants rarely switched back to a meliorating or
maximization if the number of corresponding decisions                                                                             intermediate strategy after having once maximized. This
significantly deviated from chance levels in that direction                                                                       may have been facilitated by the feedback received (in
(i.e., less than 4 or more than 16 maximizations, in which                                                                        which the projected actual gains would closely
case a sign-test assuming a random binomial random                                                                                approximate the projected optimal gains) but also
distribution yielded p < .01).                                                                                                    suggests that maximizers typically realized that they had
                                                                                                                                  found the optimal strategy.
                Table 1: Performance by experimental condition.                                                                      But beyond all qualitative accounts we cannot disregard
                                                                                                                                  the fact that at least half of the participants in either group
                                        Wins (in $):                        Max choices (%):                                      were classified as overall meliorators. Although our
        Group:                          Mean (SD)                           Mean    (SD)                                          provision of clear and prospective feedback may have
        No Feedback:                     7.27 (0.49)                         37.8    (9.2)                                        budged a few individuals, our results demonstrate yet
        Feedback-20:                     7.81 (1.12)                         46.3 (20.0)                                          again that melioration, rather than maximization, seems to
        Feedback-End:                    8.18 (1.58)                         55.2 (28.6)                                          dominate human choice.
                                                                                                                        630

     Modeling Variable Feedback Horizons
To develop a formal understanding of the impact of local
rewards on choice performance, we developed a
reinforcement learning (Sutton & Barto, 1998) model
designed to examine the effects of adopting a local versus
global perspective on feedback. While reinforcement
learning is increasingly used in the cognitive modeling
community as a process model of human learning (e.g.,
Fu & Anderson, 2006), our use of the technique instead
reflects a desire to form quantitative predictions of
performance under known or hypothesized processing
limits. This approach mirrors the Ideal Performer
Analysis approach (Gray, Sims, Fu, & Schoelles, in press)
in terms of seeking a theory of optimal human
performance under constraints. In our case, the relevant
constraint is the extent to which the model adopts a local       Figure 3: The percentage of maximizing choices of
or global perspective on its trial-to-trial feedback.            reinforcement learning agents for various settings of the
   On each trial, the model chooses the button with the          eligibility trace parameter λ.
highest utility based on its experience with each button.
Following each action, the model probabilistically               recent action, but also to at least the four preceding
receives a reward r using the same contingencies as our          actions (and possibly much greater), a span that could
human participants. This reward is then used to update the       easily overwhelm human working memory capacity.
model’s utility estimate for the chosen button. This is             A further point demonstrated by the reinforcement
accomplished using a simple linear difference equation:          learning model is that even adopting the appropriate
                                                                 global perspective on choice outcomes does not guarantee
                     U ! " U + # [r $ U ] ,                      that the maximizing strategy will be discovered quickly.
                                                                 With λ=8, the reinforcement learning models require over
where α is a learning rate parameter determining how             200 trials of experience before 80% of the agents discover
much the error between the current estimate and observed         the optimal strategy, and roughly 10% of the agents never
reward is reduced after each outcome. By itself, the above       learn to maximize. If the learning problem faced by the
equation would quickly learn to meliorate, as by                 model is great, then humans face an even greater
definition, the average return on any single choice is           challenge, as they must somehow learn or guess the
greater for the melioration button than the maximization         appropriate global perspective, as well as deal with the
button. In order to shift the model’s focus from local           working memory demands imposed by that perspective.
rewards to a global perspective, we added eligibility               While it is impossible to directly measure anything like
traces (Sutton & Barto, 1998) to the model’s utility             a “λ parameter” in humans by looking at behavioral data,
calculation. The effect of adding the eligibility trace is       it is possible to examine the extent that each decision
that after each action is taken, a temporary record is made      reflects past outcomes over various timescales. Figure 4
of that action. This record is used to update the utility        shows the likelihood of receiving a reward over the past
                                                                 10 trials and the decision to switch buttons or stay on the
estimates for an action based not just on its immediate
                                                                 current trial. For a stay decision, there is a high likelihood
outcome, but also the resulting outcomes for subsequent
choices. The duration in trials that the eligibility trace
remains in memory is governed by a parameter (λ) that
can be used to shift the model’s perspective from local to
global performance. For example, by setting λ=5, the
model’s utility estimate for each action will consist of not
just the immediate reward, but rather the average rewards
obtained for the five choices following each action.
   Figure 3 shows the average performance of 500 runs of
the reinforcement learning model using various settings of
the parameter λ. As would be expected, with λ=1 the
model quickly learns to meliorate. However, as the
parameter increases the model gradually shifts towards
maximization. The most obvious result obtained by the
model is a demonstration of the memory demands
required by any human participant to learn the
maximizing strategy in our experiment. In order to                Figure 4: Likelihood of having received a reward on the
reliably discover a maximizing strategy, participants             preceding 10 trials and deciding to switch or stay on the
would have to attribute each reward not just to the most          current trial, contrasted with the overall reward likelihood.
                                                             631

that the participant was rewarded on the previous trial             Our findings imply that humans, like pigeons,
(and low likelihood for participants who switched).              systematically favor local over globally optimal rewards.
However the correlation rapidly diminishes between               Although some humans, some of the time, under some
choices and outcomes more than two trials apart. This            conditions are able to steer against local optima this
result strongly suggests that participants in our                clearly does not come easily. In fact, the tenaciousness of
experiment attributed the utility of each action mainly to       the melioration phenomenon may suggest that local
its local consequences, and failed to learn the connection       optimization is an evolutionary adaptive mechanism that
between local choices and their long-term consequences.          is only dysfunctional in very special environments.
The value of our computational model is to suggest that             As many phenomena of addictive and impulsive
this failure may represent not just the choice of an             behavior patterns can be explained from a melioration
inappropriate perspective on feedback, but more                  perspective, it would be worrying if humans could not in
fundamentally, a working memory limitation that could            principle overcome this tendency. Future research should
prevent the adoption of a more global perspective.               concentrate on the interaction between conflicting local
                                                                 and global feedback. A better model of this interaction
                       Discussion                                would not only benefit our theoretical understanding of
                                                                 behavioral mechanisms, but would bear great potential for
Our results provide yet another demonstration of the
                                                                 applications that range from interactive software tools to
persistence of the tendency to meliorate rather than
                                                                 the prevention or cure of self-destructive behaviors.
maximize. Even with a feedback manipulation that clearly
highlighted the global suboptimality of their choice
allocations, the majority of our participants meliorated.                            Acknowledgments
We interpret these findings as both partial success and          The work reported was supported by the Air Force Office
successful failure. Although group means did not show            of Scientific Research (AFOSR #F49620-03-1-0143).
any systematic effects, individual performance profiles
suggested that our manipulation has helped some                                           References
individuals to maximize their rewards.
                                                                 Bhavnani, S. K., & John, B. E. (2000). The strategic use of
   The success in our failure is that our theoretical model           complex computer systems. Human-Computer Interaction,
allows us to account for those findings to a certain extent.          15(2-3), 107–137.
Even with perfect attribution of rewards to past choices         Fu, W.-T., & Anderson, J. R. (2006). From recurrent choice to
the model needs to consider sequences of six or more                  skill learning: A reinforcement-learning model. Journal of
choices in order to learn to maximize. By contrast,                   Experimental Psychology: General, 135(2).
people’s choice allocations seem to be governed by local         Fu, W.-T. & Gray, W. D. (2004). Resolving the paradox of the
events like the presence or absence of rewards on the                 active user: Stable suboptimal performance in interactive
immediately preceding trials.                                         tasks. Cognitive Science, 28(6), 901–937.
   At present our model does not take into account the           Gray, W. D., Sims, C. R., Fu, W.-T. & Schoelles, M. J. (in
global feedback provided to participants. However, the                press). The soft constraints hypothesis: A rational analysis
important contribution of the model is its ability to place           approach to resource allocation for interactive behavior.
both local and global perspectives on a continuous scale              Psychological Review.
(via the parameter λ), whereas our experiments have only         Herrnstein, R. J. (1997). The matching law. H. Rachlin & D. I.
manipulated this dimension by providing qualitatively                 Laibson (Eds.). Cambridge, MA: Harvard University Press.
different types of feedback. An interesting question is          Herrnstein, R. J. & Vaughn, W. (1980). Melioration and
                                                                      behavioral allocation. In Staddon, J. E. R. (Ed.), Limits to
whether a particular combination of local and global
                                                                      action: The allocation of behavior (pp. 143–176), New
feedback would tip the balance sufficiently that a
                                                                      York: Academic Press.
maximizing strategy could be learned using a lower               Neth, H., Sims, C. R. & Gray, W. D. (2005). Melioration
demand on working memory (concretely, a smaller                       despite more information: The role of feedback frequency
parameter λ). If so, the model might shed light on the                in stable suboptimal performance. Proceedings of the 49th
cognitive mechanisms that underlie melioration and may                annual meeting of the Human Factors and Ergonomics
guide the design of experiments in which decision makers              Society (pp. 357–361). Orlando, FL.
reliably manage to maximize their rewards.                       Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning:
                                                                      An introduction. Cambridge, MA: The MIT Press.
                      Conclusion                                 Simon, H. A. (1956). Rational choice and the structure of the
                                                                      environment. Psychological Review, 63, 129–138.
Maximization is not just an obsolete ideal in need of            Tunney, R. J., & Shanks, D. R. (2002). A re-examination of
retirement and remains an important benchmark for                     melioration and rational choice. Journal of Behavioral
understanding human behavior. But as individual choice                Decision Making, 15(4), 291–311.
allocations often defy the notion of utility maximization        Yechiam, E., Erev, I., Yehene, V., & Gopher, D. (2003).
an alternative explanatory mechanism is needed: We                    Melioration and the transition from touch-typing training to
propose that current list of contenders (including notions            everyday use. Human Factors, 45(4), 671–684.
of ‘bounded rationality’ and an ‘adaptive toolbox’) needs
to be extended to include melioration.
                                                             632

