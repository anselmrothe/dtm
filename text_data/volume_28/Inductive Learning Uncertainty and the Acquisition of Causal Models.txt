UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Inductive Learning, Uncertainty and the Acquisition of Causal Models

Permalink
https://escholarship.org/uc/item/2gs955q3

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)

Authors
Drewitz, Uwe
Thuring, Manfred
Urbas, Leon

Publication Date
2006-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Inductive Learning, Uncertainty and the Acquisition of Causal Models
Manfred Thüring (thuering@gp.tu-berlin.de)
Department of Cognitive Psychology, Berlin University of Technology,
Franklinstr. 5-7, 10587 Berlin, Germany

Uwe Drewitz (drewitz@gp.tu-berlin.de)
Department of Cognitive Psychology, Berlin University of Technology,
Franklinstr. 5-7, 10587 Berlin, Germany

Leon Urbas (leon.urbas@tu-berlin.de)
Center of Human-Machine-Systems, Berlin University of Technology,
Jebensstr. 1, 10623 Berlin, Germany
Abstract
Causal models can be regarded as fundamental knowledge
bases consisting of rules for generating explanations and
predictions. As we all know, such inferences are not free from
uncertainty. In an experiment about acquiring causal models
by induction, we investigate the impact of the validity of such
models on the certainty of inferences. The results indicate that
preliminary models are revised in the light of new information, and that the degree of validity considerably influences
the certainty of predictions.
Keywords: Mental models; conditional
inferences; induction; uncertainty.

rules;

causal

Introduction
Causal reasoning is one of the cornerstones of human
cognition and seems as fundamental as our concepts of
space and time. Explanations and predictions are crucial for
everyday thinking as well as for scientific research. They
heavily influence our understanding of the world and guide
our judgments, decisions and actions in many respects.
Well aware of the central role of causality, philosophy
and psychology have been exploring the mental mechanisms that underlie causal thinking for many years (White,
1990). Examples of research issues are the perception of
causal relations, the structure of causal knowledge or the
mechanisms of learning and reasoning in causal contexts.
In this paper, we address the issue of acquiring causal
models by inductive learning. In particular, we investigate
how a causal model may gradually evolve from observations and how its structure may influence the content and
certainty of predictive inferences.

Basic causal models
To start with, what do we mean when we say “A causes Z”,
as in the statement “smoking causes lung cancer”? Mackie
(1974) proposes four possible interpretations:
• A is a necessary and sufficient condition for Z. Smoking
(A) is the only cause for lung cancer (Z) and always
leads to that effect.
• A is a necessary but insufficient condition for Z. Only
smokers get lung cancer, but some of them do not,

because they are lacking the particular genetic
predispositions (C) that cause lung cancer in conjunction
with smoking.
• A is a sufficient but unnecessary condition for Z since
other conditions lead to the same effect. Smoking causes
lung cancer, but the inhalation of asbestos particles (B)
does as well.
• A is an insufficient but necessary part of a condition
which itself is unnecessary but sufficient for Z (i.e.,
either A and C or B cause Z). Smoking in conjunction
with certain genetic predispositions causes lung cancer
or – alternatively - cancer is caused by asbestos particles.
Mackie’s taxonomy of causal statements points to four
basic knowledge structures constituting the different meanings of “A causes Z”. These structures can be regarded as
causal models in the sense of Craik (1943) who coined the
term “mental model” for domain specific knowledge of predictive and explanatory power. To represent the (in)sufficiency and (un)necessity of conditions for an effect, a causal
model can be expressed in terms of conditional rules. Based
on Mackie’s definition, four types of models can be
distinguished. In this paper, we address three of them.

Model of Unique Causation (MUC)
A is the only cause for Z. Whenever A occurs Z will occur,
and whenever A is missing Z will be missing as well. This
implies two conditional rules.
RMUC1: A Æ Z
RMUC2: -A Æ -Z

Model of Complex Causation (MCC)
A in conjunction with another condition C forms a complex
cause that leads to Z. When A and C occur, Z will occur.
When either A or C are missing, Z will not appear. This can
be represented by three rules.
R MCC1: (A & C) Æ Z
R MCC2: -A Æ -Z
R MCC3: -C Æ -Z

Model of Multiple Causation (MMC)

2251

A or B cause Z. When A or B are present, Z will occur too.
When both A and B are missing, Z will not appear either.
Again, three rules express these relationships.

R MMC1: A Æ Z
R MMC2: B Æ Z
R MMC3: (-A & -B) Æ -Z
Model four is a combination of MMC and MCC, called
Model of Multiple Complex Causation (Thüring, 1991).
All these models represent different types of causal
knowledge that people acquire by gathering experience in a
domain and that are used to explain or to forecast events of
interest. In our example for instance, a person who believes
that a certain predisposition is necessary in conjunction with
smoking for developing lung cancer (MCC) will tend to
other predictions than a person who believes that smoking
on its own will lead to this disease (MUC). Hence, the
structure of a model and the contents of its rules determine
the inferences that a persons draws about a case.
Compared to the vast amount of causal relationships and
their complex intertwining stored in human memory, the
MUC, MCC and MMC are apparently simple and limited,
and of course we do not assume that they represent comprehensive knowledge bases. Instead, we propose to regard
them as basic structures from which such knowledge bases
are built. The exact structure and size of an extensive causal
model depends on the number of conditions it contains and
the relations between these conditions (i.e. terms as “A”,
“B” and “C” in the rules of basic models are meant as placeholders for sets of conjunctively or disjunctively combined
conditions). This conceptualization makes it is possible to
describe elaborated causal models as combinations and
extensions of the basic models given above.

Uncertainty of causal inferences
When the conditions of a rule in a causal model are matched
by adequate information, it produces an inference predicting
either the occurrence or the non-occurrence of the event the
rule is about. From the perspective of propositional logic,
such deductions leave no room for uncertainty. On the other
hand, we all know that explanations and predictions are
usually far from being certain. This raises the question what
factors may lead to the uncertainty of a causal inference
deduced from the rule of a mental model.
Two factors have been proposed to influence the uncertainty of explanations and predictions: (a) the ambiguity of
the information available for a causal inference, and (b) the
validity of the causal model (Thüring, 1991).
Ambiguity can be defined as the perceived amount of
missing information (Frisch & Baron, 1988). This amount
depends on the structure of the causal model and the extent
to which given information matches the conditions of its
rules. To illustrate this relationship, imagine a person who
relies on the MCC to predict an event Z. When this person
receives confirming or disconfirming information about “A”
and “C” then the situation is unequivocal (i.e., all data are
available required for the model to forecast the occurrence
or non-occurrence of Z). On the other hand, imagine this
person receiving only information confirming “A” while
“C” remains unknown. This case is a typical example for an
ambiguous situation, since the occurrence of Z depends on
whether condition “C” is fulfilled or not. If “C” is true in
addition to “A” the condition of rule R MCC1 is completely
matched and “Z” should be inferred. If “not C” is true the

condition of R MCC3 is fulfilled and “not Z” should be
predicted.
The experienced validity of a causal model depends on
the number of correct and false inferences derived from the
model in the past. Imagine a person with rather limited
causal knowledge as in the case of the mono-causal model
MUC. Whenever this person receives the information “A”
she will predict Z, and whenever she receives the
information “not A”, she will predict “not Z”. In both cases,
the confidence in the rules of the model should increase
when the prediction is right. However, when the causal
model does not represent all causal conditions which are
really responsible for Z, such inferences may be wrong.
Firstly, “A” may not be a sufficient condition for Z to
appear. In this case, the “real” causal relationship may be
that “A” in conjunction with another condition is required
for Z. Secondly, “A” may not be a necessary condition for Z
(i.e., although “not A” is true, Z appears). In this case, the
real causal relationship may be that an alternative cause “B”
may lead to Z, too. In both cases, the person’s model lacks
important knowledge and the wrong predictions should
decrease her confidence in the rules she uses.
Contingency information as in this example has been
investigated in a multitude of studies concerned with the
“strength” of a causal relation, and a number of theories
have been proposed to predict causal strength from the
frequencies in the cells of a contingency table. Prominent
approaches are the ΔP rule (Jenkins & Ward, 1965), the
Power PC theory (Cheng, 1997), the pCI rule (White, 2004)
and the Belief Revision Model (Catena et al., 1998). All of
them focus on covariation and try to predict the perceived
strength of a causal relationship. None of them addresses the
problems of how contingency information might be
understood on the basis of preexisting causal knowledge or
how such information might influence the certainty of a
prediction or explanation.
Causal models are a promising conceptual framework to
tackle these problems. Since they capture both, the ambiguity of given data as well as the validity of causal rules,
they can be used to predict the content as well as the
certainty of a causal inference. In this respect, they are
similar to causal networks (Pearl, 2000), but in contrast to
this approach, they stress the heuristic, non-bayesian nature
of human probability assessment. Based on theoretical
assumptions about heuristics for judging the likelihood of
events (Einhorn & Hogarth, 1986), Thüring (1991) proposes
a theory which describes the processes by which inferences
are derived from causal knowledge. This theory also specifies a formal algorithm to predict the certainty of causal inferences from the validity of causal rules represented in the
model and the perceived ambiguity of the available data.
The influence of ambiguity on the certainty of causal
inferences could be demonstrated in a number of experiments (Thüring, 1991; Thüring & Jungermann, 1992;
Jungermann & Thüring, 1993). Participants learned models
of fictional diseases which were structured as the MCC,
MMC or MMCC. After ensuring that the diseases were
properly understood, sets of data about hypothetical patients
were presented. While the validity of the models was held
constant, the data about the cases differed with respect to

2252

ambiguity (i.e., the degree of matching between data and
causal rules was varied.) Data sets contained either positive
evidence – for example information matching rules with
positive conditions as in RMUC1 - or they contained negative
evidence, thus matching rules with negative conditions, such
as RMUC2. For both types of evidence and for explanations as
well as predictions, the experiments showed a pronounced
impact of ambiguity on the certainty of causal inferences.
Increasing the degree of matching (and thus decreasing
ambiguity) led to higher subjective probabilities of any
prediction or explanation derived from a rule.
Further evidence comes from experiments reported by
Molz (2002) who also found an increase of the certainty of
causal inferences when the ambiguity of information
decreased. Moreover, he detected an influence of the
validity of causal models on the subjective probabilities of
inferences. Conclusions based on models of low validity
were considered as less certain than conclusions derived
from models of high validity. This influence, however, was
much less pronounced than assumed. A reason for this
unexpected result may lie in the way subjects were trained
in these experiments. Causal models as well as their validity
were not acquired by observing sets of data, but were
described in texts. Information about validity was given by
sentences, such as “this diagnosis proved right in one of
three cases”. Since this information was only given once per
model, its influence might be rather minor compared to
ambiguity which had to be accounted for in each judgment
about a patient.
To summarize, the knowledge base of explanations and
predictions can be conceptualized as a causal model
consisting of a number of rules. These rules represent
knowledge about positive and negative evidence with
respect to the occurrence or non-occurrence of the
phenomenon the model is about. The ambiguity of data and
the validity of rules are regarded as factors which influence
the certainty of inferences derived from the model. Both
factors can be expressed as model parameters. While the
extent of ambiguity is given by the degree of matching
between data and rule conditions, the validity depends on
the relation between correct and incorrect inferences that
were deduced from the rules of the model in the past. While
the influence of ambiguity on the certainty of causal
inferences has been documented convincingly, the influence
of validity needs further clarification.

dict its state. The system was a fictional pump system regulating the cooling of a power plant. In each trial, participants
received information about the states of four components of
the pump system and had to predict the future state of the
overall system. In addition, they had to judge the certainty
of their prediction. At the end of each trial, they were
informed about the correctness of their forecast.
The experiment consisted of three successive blocks of
trials. The first block endorsed a simple causal model (the
MUC), the second block provided evidence against that
model to reduce its validity, and the third block offered
information to revise the MUC by transforming it into a
more complex model, such as the MMC or the MCC. This
fixed sequence of blocks corresponds to a common situation
in inductive learning where people start with a simple
hypotheses which proves as deficient and must be modified
in the face of new evidence.

Participants
Forty two participants were recruited for the experiment,
twenty four of them were women. Only lay people with
respect to the technology of power plants were chosen. All
persons were paid for their participation.

Materials

Experiment

The instruction informed people that they were participating
in a learning experiment. A short scenario was presented in
which the pump system was characterized as an important
part of a power plant. People were told that it consisted of
four subsystems and that its state depended on the states of
these components – but they were not informed about the
specific causal relationships between those states. Instead,
they were told that their task was to find out which states of
the subsystems entailed a proper functioning of the whole
system, and which states led to a malfunction.
The state of each subsystem was indicated on a dial (see
figure 1). Each dial could be “on” (subsystem A and C) or
“off” (subsystem B and D). Only if the dial was on, the state
of the subsystem could be read. In this case, the needle of
the dial was either on the right side - indicating that the
subsystem was “up” and functioning properly (subsystem
A) - or the needle was on the left side – indicating that the
subsystem was “down” (subsystem C). If the dial was off,
no information about the corresponding subsystem was
available. In this case, the dial was shown in grey
(subsystem B and D). Altogether, the state of each
subsystem was either “up”, “down” or “unknown”.
Participants could judge the state of the whole system by
pressing one of two buttons labeled “ok” or “malfunction”.
For estimating the certainty of their prediction, they had to
use the scale shown in figure 1. The state of the whole
system was indicated in a separate field displaying either the
message “ok” or “malfunction”.
All materials were evaluated in three pretests with a total
number of 20 persons. Instruction, technical scenario and
graphics were revised twice according to the participants’
comments. The third pretest finally ensured that the scenario
provided a plausible cover story and that instruction, task as
well as all materials were precise and comprehensible.

To investigate the influence of validity on the certainty of
causal inferences, we set up an experiment in which causal
models were gradually acquired by inductive learning. The
goal of the experiment was to answer three questions:
(1) Does the certainty of a causal inference increase with
growing validity of the rule it is derived from?
(2) What happens to the certainty of inferences drawn
from a well proven model when the validity of the
model starts to decrease?
(3) In which way do people revise the rules of their
model when its validity is diminishing?
The task for the participants of the experiment was to
acquire knowledge about a technical system in order to pre2253

Subsystem A

Subsystem B
OFF

OFF

ON

ON

Subsystem D

Subsystem C

OFF

OFF

ON

ON

Malfunction

Malfunction
System

OK

OK

?

OK

Next

Figure 1: Overview of materials used in all three blocks.

Procedure
The experiment was run on an IBM PC under Windows XP.
Each participant read the instruction and could ask any
questions before the experiment began. In each trial, the participant was first shown the dials of the subsystems. The
field indicating the state of the whole pump system was
grey. To predict the state of the whole system, the participant pressed either the button labeled “ok” or the button
labeled “malfunction”. Then she judged the certainty of her
prediction by adjusting the slide shown in figure 1. After
this judgment, she was informed about the correctness of
her response by displaying either the message “ok” or “malfunction” in the field labeled “system”. After this feedback,
she started the following trial by pressing a “next” button.
The experiment consisted of three blocks of trials in a
fixed sequence. Within each block, trials followed a random
order. The first block aimed at inducing a model of unique
causation (MUC) in which the proper functioning of
subsystem A appeared as a sufficient and necessary
condition for the proper functioning of the whole pump
system. The block consisted of twelve trials per rule. The
combination of “subsystem A is up” and “the whole system
is functioning properly” was presented to induce and
reinforce the rule RMUC1. The combination of “subsystem A
is down” and “the whole system has a malfunction” was
presented to induce and reinforce the rule RMUC2. To support
an early formation of rules, the dials of the other systems
were turned off during the first three trials for both rules.
This was changed for the subsequent trials. The dials of
other systems were turned on as well, but the information
they provided was always consistent to the rules of the
MUC. Although this consistency ensured that no evidence

was presented conflicting to the MUC, information about
the other three subsystems are distractors in this context.
The second block consisted of six trials per rule and was
designed differently for two groups of our participants. The
first group received information contradicting the
sufficiency of the MUC. People were provided with data
which were inconsistent with RMUC1 (i.e., the whole system
was not “ok” although subsystem A was “up”). The second
group received information contradicting the necessity of
the MUC. People were provided with data which were
inconsistent with RMUC2 (i.e., the whole system was “ok”
although subsystem A was “down”).
The third block had ten trials per rule and was again different for the two groups. The first group was given information that could be used to reestablish the sufficiency of
the causal model by expanding the model of unique causation (MUC) into a model of complex causation (MCC). To
do so, people had to recognize that subsystem A had to be
“up” in conjunction with subsystem C. To represent this
relation, the three rules of the MCC had to be acquired.
Only data were provided which were in accordance with
these three rules. The second group was given information
that could be used to reestablish the necessity of the causal
model by expanding the MUC into a model of multiple
causation (MMC). To build up that model, people had to
recognize that a failure of subsystem A was compensated
when subsystem B was “up”. Hence, proper functioning of
subsystem B was an alternative cause for the proper functioning of the whole system. The representation of these relations required the generation of the three rules of the MMC.
For this group, only data were presented which were in
accordance with these three rules.
Throughout the whole procedure, two basic conditions
were fulfilled. Firstly, it was ensured from the beginning
that no data sets were presented which were inconsistent to
the final model which had to be acquired by each group.
This means that even the data sets of the first trials aiming at
building up the preliminary model of unique causation were
chosen in a way that neither contradicted the MCC nor the
MMC. Secondly, no ambiguous data sets were used. This is
important for block three because this block was the first
one where ambiguity could arise. For the MMC, an
ambiguous situation would arise if rule R MCC1 was only
partially matched by the information of a data set. For the
MCC, data would be ambiguous which do not match rule
RMMC3 completely. Since any partial matching of that kind
was completely avoided, ambiguity – as defined above –
was ruled out in this experiment.

Design

2254

Different independent variables were relevant for the three
blocks. For the first block, a 2-factorial design with repeated
measurement was realized. The first factor was called “rule”
with the treatments “ok” (for rules predicting a proper
functioning of the system) and “malfunction” (for rules
predicting a failure of the system). The second factor was
the number of reinforcing trials, called “reinforcement”,
with twelve treatments. For block two, the factors were
“rule” (as in block 1) and “discrediting” with six treatments
(i.e., the number of trials with information inconsistent to

the rules of the MUC). For the third block, the factors were
“rule” with two treatments and “reinforcement” with ten
treatments. All designs were designs of repeated measures
and used certainty judgments as major dependent variable.

Certainty
75

OK
66%

MAL
65%

50

25

Results
Figure 2 to 5 show the mean certainty judgments as well as
the percentage of participants producing the inference
predicted by the causal model which was supposed to be
used. An analysis of the percentages revealed that an
inference opposite to the one proposed by the model was
drawn in four trials (i.e., in trial 6 of block one, in trial 4 of
block two and in trial 4 in block three for the MMC as well
as for the MCC). Since these trials failed to reinforce or to
discredit the model as planned we decided to exclude them
from the statistical analysis. In each case, the reason for the
opposite inference is a distractor which will be explained in
the discussion.
First block: Reinforcing the MUC
Figure 2 illustrates the gradual increase of certainty with the
growing number of reinforcing trials for both rules. This
trend is interrupted in trial 6 where certainty judgments
suddenly drop. As mentioned above, this dramatic change is
caused by a distractor which led 74% of the participants to
an opposite inference.
An ANOVA revealed a significant main effect of the
factor “reinforcement” (F(10,410) = 12.00, p < 0.001, η² =
0.226) and a significant interaction of “reinforcement” and
“rule” ( F(10,410) = 4.51, p < 0.001, η² = 0.099). The factor
“rule” had no significant effect.

0
1

2

3

4

5

6

-25

-50

-75

Trials

Figure 3: Mean certainty judgments for the MUC depending
on the number of discrediting trials.
Third block of trials: Reinforcing MCC and MMC
Percentages of predicted inferences and mean certainty
judgments given by participants who learned the MCC are
shown in figure 4. An opposite inference was drawn in the
fourth trial. An ANOVA revealed two significant effects: a
main effect of the factor “reinforcement” (F(9, 180)=9.15, p
< 0.001, η² = 0.314) and an interaction effect of “reinforcement” and “rule” (F(9,180)=4.75, p < 0.001, η² = 0.192).
Certainty

100

80

MAL
97%

60

40

Certainty

OK
93%

20

100
0
80

1

MAL
95%

20

0

-20

4

5

6

7

8

9

10

Figure 4: Mean certainty judgments for the MCC depending
on the number of reinforcing trials.
The other group of participants who learned the MMC
produced mean certainties that are illustrated in figure 5.
Again, an inference opposite to the model appeared in trial
4.

OK
84%

1

3

Trials

60

40

2

2

3

4

5

6

7

8

9

10

11

12

Certainty

Trials

100

Figure 2: Mean certainty judgments for the MUC depending
on the number of reinforcing trials.
Second block: Discrediting the MUC
Figure 3 shows the percentage of predicted inferences and
the development of mean certainty judgments over six trials
discrediting the rules of the MUC. This time, the fourth trial
produced an opposite inference.
Data were analyzed by an ANOVA. The factor
“discrediting” had a significant effect on mean certainty
judgments (F(3.3, 65.9)=15.70, p < 0.001, η² = 0.440.
Moreover, a significant interaction between “discrediting”
and “rule” was shown F(3.3, 62.6)=3.42, p < 0.021, η² =
0.146). There was no main effect of the factor “rule”.

2255

80

60

MAL
95%

40

20

OK
75%

0
1

2

3

4

5

6

7

8

9

10

Trials

Figure 5: Mean certainty judgments for the MMC
depending on the number of reinforcing trials.
The ANOVA of the data of this group showed an effect of
“rule” (F(1,20)=10.29, p < 0.004, η² = 0.340) and of
“reinforcement” (F(9,180)=8.96, p < 0.001, η² = 0.309) as

well as a significant interaction of these two factors
(F(9,180)=7.59, p < 0.001, η² = 0.275).

Discussion
Let us now return to the three questions raised at the beginning of this chapter.
The results of the first experimental block support the
assumption that people can easily build up a basic causal
model, such as the MUC. As the high percentage of
inferences in concurrence with the causal model show, in
most people infer the states of the technical system as
predicted. The certainty of these judgments increases with
the number of trials confirming the rules of the model. This
can be equally observed for rules whose conditions are
matched by positive evidence, such as RMUC1, and for rules
employed to evaluate negative evidence, such as RMUC2.
For inferences deduced from such rules, a relatively high
level of certainty is reached pretty fast and can be preserved
as long as no “noise” is generated by information about
irrelevant conditions. In block one, such information is
provided in the sixth trial and concerns subsystem D which
is causally irrelevant for the MUC. In the preceding trials,
the display for D had always been turned off with the
exception of one data set, where subsystem D had been “up”
together with subsystem A. Since in that case the resulting
state of the entire pump system had been “ok”, people might
have formed the hypothesis that D in conjunction with A
was necessary for this state (i.e., that the pump system
would malfunction if either A or D were “down”). This
could explain why our participants drew an inference
opposite to the MUC in the sixth trial, where D was
presented as “down”, and why their certainty judgments
dropped accordingly. Very similar data constellations and
corresponding hypotheses were probably responsible for the
other conspicuous values mentioned in the result section,
such as the mean certainties of trial four in figure 3, 4 and 5.
Such deviations from the expected subjective certainties hint
at the existence of more than one cognitive mechanism for
inductive rule formation and the derivation of certainties.
While the validity of a rule might heavily influence the
certainty of predictions under regular circumstances,
competing rules obviously come into play when data
constellations leave room for new hypotheses.
At the end of block one, the MUC seems to be pretty
stable with respect to the certainty of inferences. Although
the data sets remain “noisy”, judgments of certainty are
mostly close to 0.80 or above and do not fall below 0.73.
This changes considerably in the second block which aims
to discredit the MUC. As soon as data sets occur, which
violate the rules of the model, the certainty of inferences
starts dropping. Moreover, the percentage of people
producing the predicted inference decreases as well. These
observations provide the answer to our second question. A
decrease of the validity of a causal model has two effects.
Firstly, the certainties of inferences deduced from the model
decrease. Secondly, people seem to start forming rules
producing alternative conclusions even though conclusive
information for revising the model is still lacking.
Such information is not provided before the third block
which aims at substituting the simple MUC by a MCC for

one group of persons and by a MMC for the other group. In
answer to our third question, this block suggest that people
use such information to revise and extend the model they
started with. In the course of inductive learning, a significant increase of the certainty of predictions is found with a
growing number of trials reinforcing the models.
Although the development of certainty judgments over all
three blocks leaves the impression of a regular pattern, it
should not be forgotten that a variety of deviations occurred
from what we had expected. This not only concerns the
influence of distractors, but also the interaction between the
factors “rules” and “reinforcement” respectively “discrediting”. This interaction shows that some trials led to certainty
judgments departing from the general trend induced by
reinforcing or discrediting a model. It may indicate that
people do not value the information about different cases
equally and that they start looking for alternative rules much
earlier than expected. Further research is required to investigate the cognitive processes underlying these phenomena
and to describe them in terms of rule-based causal models.

References

2256

Catena, A., Maldonado, A. & Cándido, A. (1998). The
effect of the frequency of judgement and the type of trial
on covariation learning. Journal of Experimental Psychology: Human Perception and Performance, 24, 481-495.
Cheng, P. W. (1997). From covariation to causation: a causal power theory. Psychological Review, 104, 2, 367-405.
Craik, K. (1943). The nature of explanation. Cambridge,
MA.: Cambridge University Press.
Einhorn, H. J. & Hogarth, R. M. (1986). Judging probable
cause. Psychological Bulletin, 99, 1, 3-19.
Frisch, D. & Baron, J. (1988). Ambiguity and rationality.
Journal of Behavioral Decision Making, 1, 149-157.
Jenkins, H.M. & Ward, W.C. (1965). Judgement of contingency between responses and outcomes. Psychological
Monographs: General and Applied, 79, 1, 1-17.
Jungermann, H. & Thüring, M. (1993). Causal knowledge
and the expression of uncertainty. In G. Strube & K. F.
Wender (Eds.), The cognitive psychology of knowledge.
Amsterdam: Elsevier.
Mackie, J. L. (1974). The cement of the universe. Oxford:
Clarendon Press.
Molz, G. (2002). Knowledge reliability and information
ambiguity: two determinants of causal inferences.
Cognitive Processing, 3-4, 31-42.
Pearl, J. (2001). Causality: Models, reasoning, and
inference. Cambridge: Cambridge University Press.
Thüring, M. (1991). Probabilistisches Denken in kausalen
Modellen. Weinheim: Psychologie Verlags Union.
Thüring, M. & Jungermann, H. (1992). Who will catch the
Nagami Fever? Causal inferences and probability judgment in mental models of diseases. In D. A. Evans & V.
L. Patel (Eds.), Advanced models of cognition for medical
training and practice (pp. 307-325). Berlin: Springer.
White, P.A. (1990). Ideas about causation in philosophy and
psychology. Psychological Bulletin, 108, 1, 3-18.
White, P.A. (2004). Causal judgement from contingency
information: A systematic test of the pCI rule. Memory &
Cognition, 32, 3, 353-368.

