UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Synapse Plasticity Model for Conceptual Drift Problems
Permalink
https://escholarship.org/uc/item/7979b54f
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Mappus IV, Rudolph L.
Ram, Ashwin
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

            A Synapse Plasticity Model for Conceptual Drift Problems
                                 Rudolph L Mappus IV (cmappus@cc.gatech.edu)
                                          Ashwin Ram (ashwin@cc.gatech.edu)
                                      College of Computing, Georgia Institute of Technology
                                                      Atlanta, GA 30332 USA
                           Abstract                                 the component such as the RPM of component shafts
                                                                    or bearing temperatures. The goal of the learner in this
   Traditional supervised learning techniques do not ad-
   dress online learning problems such as concept drift, due        domain is to classify the working state of the component
   to the fact that learning is offline when using these meth-      in real time, given a stream of samples from the sensor
   ods. Associative neural networks using Hebbian learning          array. Suppose that during the daytime hours, bear-
   rules show robust performance in classification tasks in-        ing temperatures rise from ambient heat, to levels that
   volving concept drift. Biologically plausible neural net-
   works represent a set of computational models designed           would, under other conditions, raise warning. In this
   to be more strongly related to biological neuron models.         case, the previous values used to classify nominal oper-
   In this paper, we apply a biologically inspired plasticity       ation do not reflect the new operating parameters. A
   model of synapse dynamics to a concept drift classifi-           supervised learner that is not modified after training re-
   cation problem. The motivation for this method is to             quires retraining on the new instances. We are interested
   provide more biologically plausible networks in cognitive
   tasks.                                                           in applying biologically plausible computational models
                                                                    to this problem.
                       Introduction                                 Concept Drift
Concept drift is a real world problem associated with               For the purpose of this discussion, we define concep-
concept learning and classification: over time or as an             tual drift to be the change in distribution parameters
agent gains more information about a concept and re-                that define a classification. For example, suppose that
lated concepts, the parameters of the features defining             the probability density of the an instance belonging to a
the concept change. Traditional supervised learners do              classification was defined as Gaussian. The parameters
not address the issue. Learning in problem domains with             defining that distribution would change over time, indi-
static concepts also avoids the problem, but it is a prob-          cating a conceptual drift. This drift can be correlated,
lem in dynamic learning problems and agents.                        so that concepts at time t are related to concepts at time
   Intuitively, neural networks seem to have properties             t + n. Clearly, even in a binary decision task, if drift oc-
that would be helpful in concept drift problems. Partic-            curs, the decision boundary generated for the concepts
ularly, associative neural networks seem appropriate to             at t would not be the decision boundary generated at
the problem. Traditional artificial neural networks how-            t + n.
ever were designed as computational models first based                  Concept drift can increase without repetition (i.e.
on observed neuron behavior second.                                  never returning to a previous distribution). Essentially,
Previous Work                                                        a concept’s change is continual, and while the rate of
                                                                     change can slow, the change always leads to a new pa-
Computational modeling of neuron processes has re-
                                                                     rameterization of the concept probability distribution in
ceived a wealth of research (O’Reilly and Munakata,
                                                                     feature space. A good example of this might be bear-
2000). Some well researched models have been ap-
                                                                     ing temperature whose value distribution permanently
plied to learning problems such as concept drift. Biehl
                                                                     changes after seating.
and Schwarze (Biehl and Schwarze, 1993) demonstrate a
Hebbian learning model for handling random as well as                Repeating and cyclic concept patterns There are
correlated concept drift. Widmer and Kubat (Widmer                   some special cases worth mentioning. Concept drift can
and Kubat, 1996) show how latent variables can influ-                repeat. Suppose that a concept, after initial training,
ence concept drift in the form of context. In this paper,            is explained by a parameterization b1 . In online learn-
we show results of timing dependent synapses, whose po-              ing, more instances arrive, inducing parameterization b2 .
tentiation is governed by firing rate patterns of pre- and           Still more instances arrive, driving the concept param-
post-synaptic neurons. Schlimmer and Granger (Schlim-                eterization back to b1 . The learner, having learned pa-
mer and Granger, 1986) show how learning incrementally               rameterization b1 would do best to take advantage of
from noisy data can still generate classification results.           the previously learned concept. Concept drift can also
   Consider the real world problem of classifying the                be cyclic, where a concept repeats a set of parameteriza-
working state of a power plant component (e.g. a tur-                tions in an order, and this ordering repeats. In general,
bine generator), given a set of sensor array readings for            these special cases point to the fact that a latent variable
                                                                1777

affects the concept learning. All of these cases seem to
require some memory, and the need for dynamic weight-
ing of features.
Cognitive Motivation
There is growing evidence that synaptic behavior plays a
fundamental role in many cognitive tasks. Hippocampal
neurons responsible for working spatial memory seem to
be highly plastic, relative to other cortical neurons. The       Figure 1: Plastic synapse neuron model (pre- and post-
implication is that a high level of plasticity is needed to      synaptic neurons).
handle dynamic spatial environments such as is needed
to support navigation. Performance decay of learned
processes that are not practiced seems to be attributable
to weakening (depressed) potentiation between connec-
tions salient to the neuron processes related to the task.       difference (i.e. a pre-synaptic action potential precedes
This point is noticed acutely again in spatial tasks, typ-      a post-synaptic action potential). A synapse potential is
ically attributed to hippocampal areas of the brain. As-        depressed if the difference is negative.
pects of motion detection also seem to apply to synaptic            Consider an integrate and fire neuron model, where
plasticity in the visual pathways. There has been some          the instantaneous sum of activation arriving from the
behavioral evidence that certain forms of dyslexia may          dendrite is what is considered in action potential propa-
be attributable to synapse plasticity in magnocellular          gation. This decision function is modeled using a logis-
visual pathway (layers I and II of the lateral geniculate       tic transfer function 1/1 + e−u . In this case, a neuron
nucleus) (Stein, 2001).                                         soma represents the decision point for action potential
   The human capacity to handle concept drift seems in-         propagation. Propagation of signals can be expressed in
nate to many cognitive processes. The problem of clas-          terms of the synapses and their relation to the soma. For
sification in concept drift situations seems to share some      each synapse, the distance from the presynaptic soma to
of the dynamic demands required of plastic neural areas         the synapse represents the time for an action potential
in the brain. We believe the use of stronger biological         to propagate from the presynaptic soma to the synapse.
models in neural networks provides insight into cognitive       Likewise, the latency from the synapse to the postsy-
computational phenomena and cognitive function.                 naptic neuron soma represents the time for an action
                                                                potential to propagate from the synapse to the soma as
                 Synaptic Plasticity                            well as the variability in the conductance properties of
Synaptic plasticity has been used to refer to the relative      individual connections. Here, we make the assumption
tendency for a neuron (dendrite) to form new synapses           that the conductance properties of the network are uni-
as well as the potentiation of a synapse to propagate           form. Suppose a fully connected network (i.e. a synapse
action potentials. In this paper we refer to plasticity as      exists between each neuron, but not recursive), where
the latter. In Hebbian models of learning, the timing of        the latencies between synapses are chosen from a uni-
action potential arrivals at the pre- and post- synaptic        form random distribution. A synapse exists at each con-
neurons governs the potentiation or depression of the           nection, and has a potential value. The potentiation of
synapse.                                                        each synapse is initialized using a uniform random distri-
                                                                bution. Neurons are organized in a laminar fashion, as
Computational model of plasticity                               in traditional networks, but latencies between neurons
What properties of synapse plasticity are salient to a          varies, so action potential propagation is not laminar.
computational model? First, conductance of action po-           Inputs are delivered to the input layer of neurons at the
tentials varies from region to region in the brain. At the      soma, and arrive simultaneously. We define the resulting
neuron level, electrical properties vary from dendrite to       network by the set of synapse potential values, the con-
dendrite, creating latencies in action potential propaga-       nection latencies (weights), the synapse update function,
tion. However, the processes governing potentiation of          using an integrate and fire neuron model.
synapses should be constant for all synapses. In other              Action potentials are propagated through the network
words, a constant factor is used to potentiate synapses         at each timestep. The sigmoid decision function is used
when necessary. The same is true for annealing synapse          to propagate action potentials to the axon (i.e. inte-
potential due to a lack of arriving action potentials (i.e.     grate and fire). Each synapse’s potentiation is updated,
use it or lose it policy). Synapse potentials are bounded       based on the pre- and postsynapse neurons’ firing re-
in their values, where the minimum value represents no          sults. Qualitatively, neurons that fire in order (pre then
action potential propagation, and maximum potential             post), increase potentiation, and lower the threshold for
means any arriving action potential is propagated.              a signal to propagate at the synapse. Neurons that
   Both pre- and post-synaptic neurons process action           fire out of order depress the synapse potential, increas-
potentials S where {spre , spost } ∈ S (figure 1). Action       ing the threshold signal required to propagate a signal.
potentials arrive at each synapse, creating a difference        Synapses with potential 0 are not able to propagate sig-
between pairs of action potentials d = tspre − tspost . Po-     nals. Synapses with max potential are able to propagate
tentiation is updated according to a window of positive         any arriving signal (signal with amplitude a > 0).
                                                            1778

Network output                                                  work components may be organized in a laminar fashion,
                                                                action potential propagation is not.
The network is allowed to propagate action potentials
until all action potentials have propagated in the net-         Biological plausibility
work (reached output neurons or were filtered). At each
timestep, the values of the output nodes are noted, and         An important aspect of the model is that it is biolog-
once complete, the resulting “spike train” is analyzed          ically plausible in the sense that potentiation is a key
for output. Biological networks of neurons output spike         feature of biological synapse dynamics. In a set of ex-
trains to indicate output from the network. In the sim-         periments meant to demonstrate the ability of TD learn-
ulated case, the spike train is analyzed for significant        ing to model the potentiation behavior of spike timing
action potentials output within a window of time to de-         dependent plastic synapses, (Rao and Sejnowski, 2000)
termine positive output.                                        employ a neuron model meant to capture many biologi-
                                                                cal behaviors. The experimental results support the use
   We define a positive window of activity in terms of          of a TD(0) function to capture the Hebbian activation
the output population over all of the output windows.           window for a synapse. The attraction of using TD(0) is
If any window shows activity above the mean activity,           that it is computationally tractable. Shon et al. (Shon
then we attribute a positive output by the network on           et al., 2004) employ a similar neuron model to model mo-
that instance. We define a threshold decision rule for de-      tion detection and prediction as observed in the visual
termining output activity of the network. If more than          cortex. Both experiments point out that it seems plau-
a threshold value of spikes are detected in the time win-       sible the brain is adaptive for and reacting to statistical
dow, then the network output is considered positive. The        properties of the natural world.
most sensitive decision rule would react to one action po-
                                                                   Another reason why this is a more biologically plausi-
tential of minimal amplitude.
                                                                ble network than a traditional network is it does not use
   A network composed of these neurons behaves simi-            a backpropagation function at the neuron level as the
larly to traditional networks: instances generate an ac-        direct means to propagate error through the network.
tivation pattern at the input nodes. Signal propagation         Backpropagating signals do so at the synapse level, and
through the network is governed by the potentiation of          do not propagate beyond the synapse (i.e. action poten-
synapses and a propagation decision function at each            tials do not flow backwards through the network). Re-
neuron. The decision function is meant to model the             current connections, rather, are extensive in many areas
fact that threshold signal must reach the soma before the       of the brain, and seem to represent an important method
neuron propagates an action potential (integrate and fire       that neurons use to receive feedback. The plastic model
model). Training the network consists of passing the net-       is more similar to this model of feedback updating than
work positive instances of classification, so that common       traditional networks.
synapses salient to the task will potentiate as a result of
the training input.                                                                  Experiments
   Processing action potentials in the network occurs as
a discrete simulation of a neuron model, such that ac-          The biological and cognitive motivations for the model
tion potentials propagate through the network at each           leads to a testable hypothesis: plasticity in terms of po-
timestep. The maximum time for an input action po-              tentiation and Hebbian learning seems to be behind (at
tential to produce output is the maximum latency of a           least short term) associative memory. If this is the case,
signal from input to output. In this case we assume             then in concept drift problems where the drift is not
action potentials propagate at a constant rate over the         random, but correlated to the original concept, then a
network (i.e. this does not model the chemical properties       plastic network (a network whose synapses are sensitive
of action potential propagation).                               to activation patterns in the data) should perform better
                                                                in classification accuracy than traditional networks.
Synapse potentiation A synapse potentiates (lowers
the potential to propagate a signal across the synapse)         Usenet Corpus
based on firing pattern and rate of the pre- and post-          A Usenet corpus was used to test the network on real
synaptic neurons. Qualitatively, if the pre- and post-          world data. The corpus consisted of samples of messages
synaptic neurons fire sequentially, the affected synapse        from twenty newsgroups (see table 1) ordered chronolog-
potentiates. If they fire out of sequence, the synapse is       ically. A bag of words generator was used to represent
depressed. Synapse potential anneals from lack of ac-           each message (McCallum, 1996), and each document was
tivity as well. The potentiation window (Hebbian win-           normalized using the L2 vector norm. A collection of
dow) is modeled in the plastic synapse network using a          1000 words representing each message was used for the
quadratic equation.                                             experiments. The vocabulary for the training set was
Laminar networks Traditional neural networks are                ranked using infogain, and the top 1000 terms were se-
laminar in structure and processing. Nodes are assigned         lected, common to both training and test sets. Messages
to layers: typically input, hidden and output and activa-       were classified by the news group it belonged to. The ex-
tion is computed per layer, for each node in a layer. The       perimental networks were trained using a training set of
propagation of action potentials for use in tuning synapse      instances, predetermined in the corpus body, and occur-
potentiation is asynchronous with respect to the set of         ring chronologically before for test set. This arrangement
action potentials propagating in the network. While net-        of messages creates a distinct “drift” in the discrete word
                                                           1779

               12                                                       12                                                              12                                                          12
               10                                                       10                                                              10                                                          10
                8                                                        8                                                               8                                                           8
       count    6                                               count    6                                                      count    6                                                  count    6
                4                                                        4                                                               4                                                           4
                2                                                        2                                                               2                                                           2
                0                                                        0                                                               0                                                           0
                    0   200   400    600    800   1000   1200                0   200   400    600    800   1000   1200                       0   200    400    600    800   1000   1200                  0   200    400    600    800   1000   1200
                                    words                                                    words                                                            words                                                       words
               12                                                       12                                                              12                                                          12
               10                                                       10                                                              10                                                          10
                8                                                        8                                                               8                                                           8
       count    6                                               count    6                                                      count    6                                                  count    6
                4                                                        4                                                               4                                                           4
                2                                                        2                                                               2                                                           2
                0                                                        0                                                               0                                                           0
                    0   200   400    600    800   1000   1200                0   200   400    600    800   1000   1200                       0   200    400    600    800   1000   1200                  0   200    400    600    800   1000   1200
                                    words                                                    words                                                            words                                                       words
                        comp.graphics                                             misc.forsale                                                         rec.autos                                                   sci.crypt
Figure 2: Sample discrete category distributions for training (top) and testing (bottom) sets showing qualitative
differences in distributions (drift). Word set is ranked infogain vocabulary of training set (common in testing set).
distributions of training and test sets for each group. A                                                                   duce propagated signals at a synapse cause synapse po-
sample of group distributions for training and test sets                                                                    tentiation that lower the threshold amplitude of future
can been seen in figure 2.                                                                                                  arriving presynaptic action potentials. In these exper-
                                                                                                                            iments, the networks were initialized with a fully con-
Neural network model                                                                                                        nected hidden layer which fed to a set of output nodes.
In order to facilitate discussion, we compare the results                                                                   The networks were trained using only positive instances,
of the plastic synapse network to performance of a tradi-                                                                   in order to potentiate the appropriate synapses. During
tional artificial neural network (ANN) in the same task.                                                                    testing, synapse potentials were not changed in response
The network was configured to the same structure as the                                                                     to input.
plastic synapse network (n input nodes, n hidden nodes,
1 output node, learning rate α = 0.001) using the Joone                                                                     Learning
neural network toolkit (Marrone, 2005). The network                                                                         The task in all of the experiments was a binary classifica-
was fully connected, composed of sigmoid nodes in the                                                                       tion task. Instances were labeled according to the group
input and hidden layers. Output ω of the network was                                                                        being tested (members of the group in the training set
positive where ω >= 0.5 and negative otherwise.                                                                             were assigned a positive label, nonmembers a negative
   The network learning rate was tuned, using a range of                                                                    one). The task is one that is real in terms of concept
values from 0.001 to 0.1. Over this range, the number                                                                       drift: over time, the features describing documents that
of nodes in the hidden layer was tuned from its initial                                                                     belong to a particular group change in parametrization.
value n up to n + 200. The number of epochs was tuned                                                                       The task for the classifier is decide whether a new doc-
using discrete values (100,1000,10000) to 100. In each of                                                                   ument belongs to the goal concept or not.
these cases for each of the Usenet groups, there was not
a significant difference in performance between param-                                                                                                                                    Results
eterizations. Two training strategies were used: train                                                                      Table 1 shows the training and test accuracy for the tra-
using the entire dataset, and train using only positive                                                                     ditional and plastic synapse networks in each category
group examples.                                                                                                             of the Usenet corpus. The task was binary classifica-
                                                                                                                            tion, where for each category, the category messages were
Plastic Neural Network                                                                                                      positive instances, and all other instances were negative.
The plastic neural network was initialized using the same                                                                   Positive instance set sizes for training and test sets are
neuron layer structure as the ANN, so that the number                                                                       shown for each category.
of neurons in both cases was the same. The TD win-
dow was set to unit time (maximum sensitivity to firing                                                                     Plastic synapse neural network
sequence); the synapse potential rate was initialized to                                                                    The plastic synapse neural network was trained using
0.0001.                                                                                                                     only positive examples. The network used was feedfor-
   Action potentials were modeled by signal amplitude                                                                       ward, with recurrent connections (no reflective connec-
(strength of signal using the normalized input vector),                                                                     tions). A hidden layer was used that was fully connected
and their arrival time at the soma or the synapse.                                                                          to both the input and output nodes. While training ac-
Computation of action potential propagation produces a                                                                      curacy was relatively high for all categories, test accu-
qualitative rule: presynaptic action potentials that pro-                                                                   racy was insignificant, indicating overfitting and little
                                                                                                                         1780

Table 1: Usenet corpus results. Size are the positive instance train and test set sizes, respectively. PSN training
is the training accuracy of the plastic synapse network (* for all categories test accuracy was insignificant). ANN
Positive is test set accuracy of the traditional neural network, using a training set of all positive instances. ANN
Complete is the test set accuracy using the complete training set (positive and negative instances).
             Newsgroup                    Train    Test   PSN Training*      ANN Positive     ANN complete
             alt.atheism                  480      319    0.9962             0.4280           0.9747
             comp.graphics                584      389    0.9537             0.4335           0.9692
             comp.os.ms-windows.misc      591      394    0.9685             0.4339           0.9688
             comp.sys.ibm.pc.hardware     590      392    0.9721             0.4339           0.9689
             comp.sys.mac.hardware        578      385    0.9487             0.4332           0.9695
             comp.windows.x               593      395    0.9518             0.4340           0.9687
             misc.forsale                 585      390    0.9647             0.4336           0.9691
             rec.autos                    594      396    0.9684             0.4340           0.9686
             rec.motorcycles              598      398    0.9767             0.4342           0.9684
             rec.sport.baseball           597      397    0.9973             0.4342           0.9685
             rec.sport.hockey             600      399    0.9839             0.4343           0.9683
             sci.crypt                    595      280    0.9820             0.4341           0.9686
             sci.electronics              591      393    0.9931             0.4339           0.9688
             sci.med                      594      396    0.9645             0.4340           0.9686
             sci.space                    593      394    0.9834             0.4340           0.9687
             soc.religion.christian       599      398    0.9978             0.4343           0.9684
             talk.politics.guns           546      364    0.9409             0.4315           0.9712
             talk.politics.mideast        564      376    0.9763             0.4324           0.9702
             talk.politics.misc           465      310    0.9713             0.4272           0.9754
             talk.religion.misc           377      251    0.9566             0.4226           0.9801
generalization power in the classifier. As the distribu-       ing problems display properties that mimic neuron pro-
tions in figure 2 demonstrate, there does not seem to          cesses, a neural network approach seems applicable.
be significant overlap in terms between the training and          The results indicate that potentiated synapses in neu-
test sets in many of the categories (see table 2). Qual-       ral networks seem to be able to classify a concept in a
itatively, less overlap produces fewer action potentials       drift condition, but with little generalization power. In
that are propagated in the network for positive instances,     the constructive learning task, when training the learner
leading to outputs that are not detectable above negative      with positive examples, the learner can only indicate how
instance input. Of course, allowing for online learning,       strongly a query instance matches (activates) the trained
even in the case where there is little or no feature over-     concept. We believe this explains the generalization per-
lap, produces high accuracy.                                   formance of the network. One possible way to address
                                                               this problem is to use the trained networks together to
Traditional neural network                                     generate a classification. In the past, committee meth-
The traditional neural network did not appear to learn         ods (e.g. bagging or boosting) have been used to in-
the binary classification task for any of the newsgroups       crease performance of weak classifiers. In terms of the
when trained with either all positive instances or the         neural structure, an output neuron would be connected
complete dataset. High accuracy in the complete train-         to the output of each individually trained network, and
ing set cases was due to the fact that the network per-        would serve as the voting mechanism (in the case of bag-
formed as a majority classifier (100% false negative rate).    ging) governing the output of the network. The implica-
To provide some insight into training the plastic synapse      tions for concept generalization would be that the classi-
network, a traditional network was also trained using          fier would be conservative in its estimates of new query
only positive instances. When trained with only posi-          points: only instances with small distance from the clus-
tive examples, the network generated low classification        ter would get a strong output from a particular network.
accuracy, with similar responses for all document cate-           The traditional neural network solution is backprop-
gories SSD < 0.001.                                            agation of error through the network. Clearly, this
                                                               method is less biologically plausible than an integrative
                       Conclusions                             approach. An important outcome of this work would
We have introduced a model biologically plausible model        be a biologically plausible model that can be applied to
of synapse behavior and applied the model to a real world      problem domains similar to the cognitive functions that
learning problem. In situations where real world learn-        mechanisms like synaptic plasticity support. The initial
                                                           1781

                                                                                     References
Table 2: Category inner product angles between training
and test sets for each Usenet category.                         Biehl, M. and Schwarze, H. (1993). Learning drifting
                                                                     concepts with neural networks. Journal of Physics
       Newsgroup                     Angle(degrees)                  A: Math. Gen, 26:2651–2665.
       alt.atheism                   84.18                      Marrone, P. (2005). Joone: Java object oriented neural
       comp.graphics                 80.13                           engine the complete guide. http://www.joone.org.
       comp.os.ms-windows.misc       72.54
       comp.sys.ibm.pc.hardware      85.71                      McCallum, A. K. (1996). Bow: A toolkit for statis-
       comp.sys.mac.hardware         86.00                           tical language modeling, text retrieval, classifica-
       comp.windows.x                79.55                           tion and clustering. http://www.cs.cmu.edu/ mc-
                                                                     callum/bow.
       misc.forsale                  83.06
       rec.autos                     84.93                      O’Reilly, R. and Munakata, Y. (2000). Computational
       rec.motorcycles               82.11                           Explorations in Cognitive Neuroscience. MIT Press.
       rec.sport.baseball            82.03
       rec.sport.hockey              76.87                      Rao, R. P. and Sejnowski, T. (2000). Spike-timing-
                                                                     dependent hebbian plasticity as temporal difference
       sci.crypt                     84.10
                                                                     learning. Neural Computation, 13:2221–2237.
       sci.electronics               79.76
       sci.med                       74.01                      Schlimmer, J. and Granger, R. (1986). Incremental
       sci.space                     73.83                           learning from noisy data. Machine Learning, 1:317–
       soc.religion.christian        65.21                           354.
       talk.politics.guns            85.76
                                                                Shon, A. P., Rao, R. P., and Sejnowski, T. (2004). Mo-
       talk.politics.mideast         86.87                           tion detection and prediction through spike-timing
       talk.politics.misc            81.89                           dependent plasticity. Network: Computational Neu-
       talk.religion.misc            75.41                           ral Systems, 15:179–198.
                                                                Stein, J. (2001). The magnocellular theory of develop-
                                                                     mental dyslexia. Dyslexia, 7(1):12–36.
results shown here are encouraging for further experi-
mentation. We want to emphasize this is not a cognitive         Widmer, G. and Kubat, M. (1996). Learning in the pres-
model for concept drift, but rather through observing                ence of concept drift and hidden contexts. Machine
some likenesses of neural properties to the problem do-              Learning, 23(1):69–101.
main, we are generating a mechanism to handle concept
drift. Using a model of synapse behavior in conjunction
with a nonlaminar method of action potential propaga-
tion, the timing of action potentials (not rate coded)
determines the potentiation of a synapse. Action po-
tentials begin at the input nodes of the network, and
propagate based on the latency of propagation in the
axons and dendrites of the modeled network.
                      Future Work
To make a stronger claim about the importance of plas-
ticity in dynamic learning problems, a well formed anal-
ysis is needed of the network behavior. A formal analy-
sis of the network should provide better insight into the
types of learning problems for which it is best suited. In-
corporating backpropagation to the network would im-
prove learning. While this seems clear, the solution is
not straightforward and less biologically plausible. In a
network where signals are time dependent, the instan-
taneous update based on error used in backpropagation
is not easily addressed. Finally, generating neural struc-
tures that produce ensemble method behavior can im-
prove classifier performance.
                  Acknowledgments
We would like to thank anonymous reviewers for helpful
suggestions and comments.
                                                            1782

