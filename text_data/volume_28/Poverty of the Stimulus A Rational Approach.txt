that are hierarchically organized within sentences, which           Because this analysis takes place within an ideal learn-
are recursively generated by a grammar. The issue is             ing framework, we assume that the learner is able to ef-
whether knowledge about this structure is learned or in-         fectively search over the joint space of G and T for gram-
nate. An approach that lacks an explicit representa-             mars that maximize the Bayesian scoring criterion. We
tion of structure has two problems addressing this issue.        do not focus on the question of whether the learner can
First of all, many linguists and cognitive scientists tend       successfully search the space, instead presuming that an
to discount these results because they ignore a principal        ideal learner can learn a given G, T pair if it has a higher
feature of linguistic knowledge, namely that it is based         score than the alternatives. Because we only compare
on structured symbolic representations. Secondly, con-           grammars that can parse our corpus, we first consider
nectionist networks and n-gram models tend to be diffi-          the corpus before explaining the grammars.
cult to understand analytically. For instance, the mod-
els used by Reali and Christiansen (2004) and Lewis and          The corpus
Elman (2001) measure success by whether they predict
the next word in a sequence, rather than based on ex-            The corpus consists of the sentences spoken by adults in
amination of an explicit grammar. Though the models              the Adam corpus (Brown, 1973) in the CHILDES data-
perform above chance, it is difficult to tell why and what       base (MacWhinney, 2000). In order to focus on gram-
precisely they have learned.                                     mar learning rather than lexical acquisition, each word
   In this work we present a Bayesian account of lin-            is replaced by its syntactic category.1 Ungrammatical
guistic structure learning in order to engage with the           sentences and the most grammatically complex sentence
PoS argument on its own terms – taking the existence             types are removed.2 The final corpus contains 21792 in-
of structure seriously and asking whether and to what            dividual sentence tokens corresponding to 2338 unique
extent knowledge of that structure can be inferred by a          sentence types out of 25876 tokens in the original cor-
rational statistical learner. This is an ideal learnability      pus.3 Removing the complicated sentence types, done
analysis: our question is not whether a learner without          to improve the tractability of the analysis, is if anything
innate language-specific biases must be able infer that          a conservative move since the hierarchical grammar is
linguistic structure is hierarchical, but rather whether it      more preferred as the input grows more complicated.
is possible to make that inference. It thus addresses the           In order to explore how the preference for a grammar
exact challenge posed by the PoS argument, which holds           is dependent on the level of evidence in the input, we
that such an inference is not possible.                          create six smaller corpora as subsets of the main corpus.
   The Bayesian approach provides the capability of com-         Under the reasoning that the most frequent sentences
bining structured representation with statistical infer-         are most available as evidence,4 different corpus Levels
ence, which enables us to achieve a number of important          contain only those sentence forms that occur with a cer-
goals. (1) We demonstrate that a learner equipped with           tain frequency in the full corpus. The levels are: Level
the capacity to explicitly represent both hierarchical and       1 (contains all forms occurring 500 or more times, cor-
linear grammars – but without any initial biases – could         responding to 8 unique types); Level 2 (300 times, 13
infer that the hierarchical grammar is a better fit to typ-      types); Level 3 (100 times, 37 types); Level 4 (50 times,
ical child-directed input. (2) We show that inferring this       67 types); Level 5 (10 times, 268 types); and the com-
hierarchical grammar results in the mastery of aspects of        plete corpus, Level 6, with 2338 unique types, includ-
auxiliary fronting, even if no direct evidence is available.     ing interrogatives, wh-questions, relative clauses, prepo-
(3) Our approach provides a clear and objectively sensi-         sitional and adverbial phrases, command forms, and aux-
ble metric of simplicity, as well as a way to explore what       iliary as well as non-auxiliary verbs.
sort of data and how much is required to make these
hierarchical generalizations. And (4) our results suggest
                                                                     1
that PoS arguments are sensible only when phenomena                    Parts of speech used included determiners (det), nouns
are considered as part of a linguistic system, rather than       (n), adjectives (adj), comments like “mmhm” (c, sentence
                                                                 fragments only), prepositions (prep), pronouns (pro), proper
taken in isolation.                                              nouns (prop), infinitives (to), participles (part), infinitive
                                                                 verbs (vinf), conjugated verbs (v), auxiliary verbs (aux), com-
                         Method                                  plementizers (comp), and wh-question words (wh). Adverbs
                                                                 and negations were removed from all sentences.
                                                                     2
We formalize the problem of picking the grammar that                   Removed types included topicalized sentences (66 utter-
best fits a corpus of child-directed speech as an instance       ances), sentences containing subordinate phrases (845), sen-
of Bayesian model selection. The model assumes that              tential complements (1636), conjunctions (634), serial verb
                                                                 constructions (459), and ungrammatical sentences (444).
linguistic data is generated by first picking a type of              3
                                                                       The final corpus contained forms corresponding to 7371
grammar T , then selecting as an instance of that type           sentence fragments. In order to ensure that the high num-
a specific grammar G from which the data D is gener-             ber of fragments did not affect the results, all analyses were
ated. We compare grammars according to a probabilistic           also performed for the corpus with those sentences removed.
score that combines the prior probability of G and T and         There was no qualitative change in any of the findings.
                                                                     4
the likelihood of corpus data D given that grammar, in                 Partitioning in this way, by frequency alone, allows us
                                                                 to stratify the input in a principled way; additionally, the
accordance with Bayes’ rule:                                     higher levels include not only rarer forms but also more com-
                                                                 plex ones, and thus levels may be thought of as loosely cor-
            p(G, T |D) ∝ p(D|G, T )p(G|T )p(T )                  responding to complexity.
                                                             664

The grammars                                                                        Context-free grammar
                                                                   NP → NP PP | NP CP | NP C | N | det N | adj N
Because this work is motivated by the distinction be-
tween rules operating over linear and hierarchical rep-                     pro | prop
resentations, we would like to compare grammars that               N → n | adj N
differ structurally. The hierarchical grammar is context-                              Regular grammar
free, since CFGs generate parse trees with hierarchical            NP → pro | prop | n | det N | adj N
structure and are accepted as a reasonable “first approxi-                  pro PP | prop PP | n PP | det NP P | adj NP P
mation” to the grammars of natural language (Chomsky,                       pro CP | prop CP | n CP | det NCP | adj NCP
1959). We choose two different types of linear (structure-                  pro C | prop C | n C | det NC | adj NC
independent) grammars. The first, which we call the flat           N → n | adj N                      NP P → n PP | adj NP P
grammar, is simply a list of each of the sentences that            NCP → n CP | adj NCP               NC → n C | adj NC
occur in the corpus; it contains zero non-terminals (aside
from S) and 2338 productions corresponding to each of             Table 1: Sample NP productions from two grammar types.
the sentence types. Because Chomsky often compared               of right-hand-side items each production contains. Fi-
language to a Markov model, we consider a regular gram-          nally, for each item, a specific symbol is selected from
mar as well.                                                     the set of possible vocabulary (non-terminals and ter-
   Though the flat and regular grammars may not be of            minals). The prior probability for a grammar with V
the precise form envisioned by Chomsky, we work with             vocabulary items, n nonterminals, P productions and
them because they are representative of simple syntac-           Ni symbols for production i is thus given by:6
tic systems one might define over the linear sequence of                                                           Ni
words rather than the hierarchical structure of phrases;                                               YP         Y   1
                                                                                p(G|T ) = p(P )p(n)        p(Ni )                (1)
additionally, it is straightforward to define them in prob-                                            i=1        j=1
                                                                                                                      V
abilistic terms in order to do Bayesian model selection.
All grammars are probabilistic, meaning that each pro-              Because of the small numbers involved, all calculations
duction is associated with a probability and the probabil-       are done in the log domain. For simplicity, p(P ), p(n),
ity of any given parse is the product of the probabilities       and p(Ni ) are all assumed to be geometric distributions
of the productions involved in the derivation.                   with parameter 0.5.7 Thus, grammars with fewer pro-
   The probabilistic context-free grammar (PCFG) is the          ductions and symbols are given higher prior probability.
most linguistically accurate grammar we could devise                Notions such as minimum description length and Kol-
that could parse all of the forms in the corpus: as such, it
                                                                 mogorov complexity are also used to capture inductive
contains the syntactic structures that modern linguists
employ, such as noun and verb phrases. The full gram-            biases towards simpler grammars (Chater and Vitanyi,
mar, used for the Level 6 corpus, contains 14 terminals,         2003; Li and Vitanyi, 1997). We adopt a probabilistic
14 nonterminals, and 69 productions. All grammars at             formulation of the simplicity bias because it is efficiently
other levels include only the subset of productions and          computable, derives in a principled way from a clear
items necessary to parse that corpus.                            generative model, and integrates naturally with how we
   The probabilistic regular grammar (PRG) is derived            assess the fit to corpus data, using standard likelihood
directly from the context-free grammar by converting all         methods for probabilistic grammars.
productions not already consistent with the formalism of
regular grammar (A → a or A → aB). When possible to              Scoring the grammars: likelihood
do so without loss of generalization ability, the resulting      Inspired by Goldwater et al. (2005), the likelihood is
productions are simplified and any unused productions
                                                                 calculated assuming a language model that is divided
are eliminated. The final regular grammar contains 14
terminals, 85 non-terminals, and 390 productions. The            into two components. The first component, the gram-
number of productions is greater than in the PCFG be-            mar, assigns a probability distribution over the poten-
cause each context-free production containing two non-           tially infinite set of syntactic forms that are accepted in
terminals in a row must be expanded into a series of             the language. The second component generates a finite
productions (e.g. NP → NP PP expands to NP → pro                     6
PP, NP → n PP, etc). To illustrate this, Table 1 com-                  This probability is calculated in subtly different ways
                                                                 for each grammar type, because of the different constraints
pares NPs in the context-free and regular grammars.5             each kind of grammar places on the kinds of symbols that
                                                                 can appear in production rules. For instance, with regu-
Scoring the grammars: prior probability                          lar grammars, because the first right-hand-side item in each
                                                                 production must be a terminal, the effective vocabulary size
We assume a generative model for creating the gram-              V when choosing that item is # terminals  1
                                                                                                                  . However, for the
mars under which each grammar is selected from the               second right-hand-side item in a regular-grammar produc-
space of grammars by making a series of choices: first,          tion or for any item in a CFG production, the effective V
the grammar type T (flat, regular, or context-free); next,       is # terminals + 1# non-terminals , because that item can be ei-
the number of non-terminals, productions, and number             ther a terminal or a non-terminal. This prior thus slightly
                                                                 favors linear grammars over functionally equivalent context-
   5
     The full grammars are available at http://www.mit.edu/      free grammars.
∼perfors/cogsci06/archive.html.                                      7
                                                                       Qualitative results are similar for other parameter values.
                                                             665

observed corpus from the infinite set of forms produced           adding so many new productions that this early cost is
by the grammar, and can account for the characteristic            never regained. The context-free grammar is more com-
power-law distributions found in language (Zipf, 1932).           plicated than necessary on the smallest corpus, requiring
In essence, this two-component model assumes separate             17 productions and 7 nonterminals to parse just eight
generative processes for the allowable types of syntac-           sentences, and thus has the lowest relative prior proba-
tic forms in a language and for the frequency of specific         bility. However, its generalization ability is sufficiently
sentence tokens.                                                  great that additions to the corpus require few additional
   One advantage of this approach is that grammars are            productions: as a result, it quickly becomes simpler than
analyzed based on individual sentence types rather than           either of the linear grammars.
on the frequencies of different sentence forms. This par-            What is responsible for the transition from linear to
allels standard linguistic practice: grammar learning is          hierarchical grammars? Smaller corpora do not con-
based on how well each grammar accounts for the set of            tain elements generated from recursive productions (e.g.,
grammatical sentences rather than their frequency dis-            nested prepositional phrases, NPs with multiple adjec-
tribution. Since we are concerned with grammar com-               tives, or relative clauses) or multiple sentences using the
parison rather than corpus generation, we focus here on           same phrase in different positions (e.g., a prepositional
the first component of the model.                                 phrase modifying an NP subject, an NP object, a verb,
   The likelihood p(D|G, T ) reflects how likely the corpus       or an adjective phrase). While a regular grammar must
data D was generated by the grammar G. It is calculated           often add an entire new subset of productions to ac-
as the product of the likelihoods of each sentence type S         count for them, as is evident in the subset of the gram-
in the corpus. If the set of sentences is partitioned into        mar shown in Table 1, a PCFG need add few or none.
k unique types, the log likelihood is given by:                   As a consequence, both linear grammars have poorer
                               Xk
                                                                  generalization ability and must add proportionally more
             log(p(D|G, T )) =     log(p(Si |G, T ))      (2)
                                                                  productions in order to parse a novel sentence.
                               i=1
   The probability p(Si |G, T ) of generating any sentence        Likelihoods
type i is the sum of the probabilities of generating all pos-     The likelihood scores for each grammar on each corpus
sible parses of that sentence under the grammar G. The            are shown in Table 2. It is not surprising that the flat
probability of a specific parse is the product of the prob-       grammar has the highest likelihood score on all six cor-
ability of each production in the grammar used to derive          pora – after all, as a list of each of the sentence types,
that parse. We assume for simplicity that all productions         it does not generalize beyond the data at all. This is
with the same left-hand side have the same probability,           an advantage when calculating strict likelihood, though
in order to avoid giving grammars with more produc-               of course a disadvantage for a language learner wishing
tions more free parameters to adjust in fitting the data;         to make generalizations that go beyond the data. An-
a more complex analysis could assign priors over these            other reason that the flat grammar is preferred is that
production-probabilities and attempt to estimate them             grammars with recursive productions are penalized when
or integrate them out.                                            calculating likelihood scores based on finite input. This
                                                                  is because recursive grammars will generate an infinite
                          Results                                 set of sentences that do not exist in any finite corpus,
The posterior probability of a grammar G is the product           and some of the probability mass will be allocated to
of the likelihood and the prior. All scores are presented         those sentences.
as log probabilities and thus are negative; smaller ab-              The likelihood preference for a flat grammar does not
solute values correspond to higher probabilities.                 mean that it should be preferred overall. Preference is
                                                                  based on the the posterior probability rather than like-
Prior probability                                                 lihood alone. For larger corpora, the slight disadvan-
Table 2 shows the prior probability of each grammar type          tage of the PCFG in the likelihood is outweighed by the
on each corpus. When there is little evidence available in        large advantage due to its simplicity. Furthermore, as
the input the simplest grammar that accounts for all the          the corpus size increases, all the trends favor the hierar-
data is the structure-independent flat grammar. How-              chical grammar: it becomes ever simpler relative to the
ever, by Level 4, the simplest grammar that can parse             increasingly unwieldy linear grammars.
the data is hierarchical. As the number of unique sen-
tences and the length of the average sentence increases,          Generalizability
the flat grammar becomes too costly to compete with the           Perhaps most interestingly for language learning, the hi-
abstraction offered by the PCFG. The regular grammar              erarchical grammar generalizes best to novel items. One
has too many productions and vocabulary items even on             measure of this is what percentage of larger corpora a
the smallest corpus, plus its generalization ability is poor      grammar based on a smaller corpus can parse. If the
enough that additional sentences in the input necessitate         smaller grammar can parse sentences in the larger cor-
                                                              666

                                    Prior                       Likelihood                      Posterior
              Corpus       Flat     PRG      PCFG        Flat       PRG        PCFG       Flat    PRG      PCFG
              Level 1      -68       -116     -133        -17         -19         -29     -85      -135     -162
              Level 2     -112       -165     -180        -33         -36         -56    -145      -201     -236
              Level 3      -405      -394     -313       -134        -179        -243    -539      -573     -556
              Level 4      -783      -560     -384       -281        -398        -522    -1064     -958     -906
              Level 5     -4087     -1343     -541      -1499       -2379       -2891    -5586    -3722    -3432
              Level 6    -51505     -5097     -681     -18128      -36392 -38421        -69633   -41489 -39102
      Table 2: Log prior, likelihood, and posterior probabilities of each grammar for each level of evidence in the corpus.
pus that did not exist in the smaller corpus, it has gen-            a normal verb phrase. It would require further evidence
eralized beyond the input in the smaller corpus. Table 3             from the input – namely, examples of exactly the sen-
shows the percentage of sentence types and tokens in the             tences that Chomsky argues are lacking – to be able to
full corpus that can be parsed by each grammar corre-                make the correct generalization.
sponding to each of the smaller levels of evidence. The
context-free grammar always shows the highest level of               Discussion and conclusions
generalizability, followed by the regular grammar. The               Our model of language learning suggests that there may
flat grammar does not generalize: at each level it can               be sufficient evidence in the input for an ideal rational
only parse the sentences it has direct experience of.                learner to conclude that language is structure-dependent
                                                                     without having an innate language-specific bias to do so.
                      % types                  % tokens              Because of this, such a learner can correctly form inter-
  Grammar       Flat     RG      CFG      Flat    RG CFG             rogatives by fronting the main clause auxiliary, even if
   Level 1      0.3%    0.7%     2.4%     9.8%    31% 40%            they hear none of the crucial data Chomsky identified.
   Level 2      0.5%    0.8%     4.3%     13%    38% 47%             Our account suggests that certain properties of the in-
   Level 3      1.4%    4.5%      13%     20%    62% 76%             put – namely sentences with phrases that are recursively
   Level 4      2.6%    13%       32%     25%    74% 88%             nested and in multiple locations – may be responsible
   Level 5      11%     53%       87%     34%    93% 98%             for this transition. It thus makes predictions that can
Table 3: Proportion of sentences in the full corpus that are         be tested either by analyzing child input or studying ar-
parsed by smaller grammars of each type. The Level 1 gram-           tificial grammar learning in adults.
mar is the smallest grammar of that type that can parse the             Our findings also make a general point that has some-
Level 1 corpus. All Level 6 grammars parse the full corpus.
                                                                     times been overlooked in considering stimulus poverty
                                                                     arguments, namely that children learn grammatical rules
   PCFGs also generalize more appropriately in the case              as a part of a system of knowledge. As with auxiliary
of auxiliary fronting. The PCFG can parse aux-fronted                fronting, most PoS arguments consider some isolated lin-
interrogatives containing subject NPs that have relative             guistic phenomenon and conclude that because there is
clauses with auxiliaries – Chomsky’s critical forms – de-            not enough evidence for that phenomenon in isolation,
spite never having seen an example in the input, as il-              it must be innate. We have shown here that while there
lustrated in Table 4. The PCFG can parse the critical                might not be direct evidence for an individual phenom-
form because it has seen simple declaratives and inter-              enon, there may be enough evidence about the system
rogatives, allowing it to add productions in which the               of which it is a part to explain the phenomenon itself.
interrogative production is an aux-initial sentence that
                                                                        One advantage of the account we present here is that
does not contain the auxiliary in the main clause. The
                                                                     it allows us to formally engage with the notion of sim-
grammar also has relative clauses, which are parsed as
                                                                     plicity. In making the simplicity argument Chomsky ap-
part of the noun phrase using the production NP → NP
                                                                     pealed to the notion of a neutral scientist who rationally
CP. Thus, the PCFG will correctly generate an inter-
                                                                     should first consider the linear hypothesis because it is
rogative with an aux-containing relative clause in the
                                                                     a priori less complex (Chomsky, 1971). The question
subject NP.
                                                                     of what a “neutral scientist” would do is especially in-
   Unlike the PCFG, the PRG cannot make the correct                  teresting in light of the fact that Bayesian models are
generalization. Although the regular grammar has pro-                considered by many to be an implementation of induc-
ductions corresponding to a relative clause in an NP, it             tive inference (Jaynes, 2003). Our model incorporates
has no way of encoding whether or not a verb phrase                  an automatic notion of simplicity that favors hypothe-
without a main clause auxiliary should follow that NP.               ses with fewer parameters over more complex ones. We
This is because there was no input in which such a verb              use this notion to show that, for the sparsest levels of
phrase did occur, so the only relative clauses occur either          evidence, a linear grammar is simpler; but our model
at the end of a sentence in the object NP, or followed by            also demonstrates that this simplicity is outweighed by
                                                               667

                                                                                                                 Can parse?
  Type    Subject NP      in input?                                Example                                    Flat RG CFG
  Decl       Simple           Y                           He is happy. (pro aux adj)                           Y      Y        Y
   Int       Simple           Y                           Is he happy? (aux pro adj)                           Y      Y        Y
  Decl      Complex           Y       The boy who is reading is happy. (det n comp aux part aux adj)           Y      Y        Y
   Int      Complex           N       Is the boy who is reading happy? (aux det n comp aux part adj            N      N        Y
 Table 4: Ability of each grammar to parse specific sentences. Only the PCFG can parse the complex interrogative sentence.
the improved performance of a hierarchical grammar on             grammars that differ in structure and the ability to find
larger quantities of realistic input. Interestingly, the          the best fitting grammars of various types, can in prin-
input in the first Adam transcript at the earliest age            ciple infer the appropriateness of hierarchical phrase-
(27mo) was significantly more diverse and complicated             structure grammars without the need for innate biases
than the frequency-based Level 1 corpus; indeed, of the           to that effect. How well this ideal learnability analysis
three, the hierarchical grammar had the highest poste-            corresponds to the actual learning behavior of children
rior probability on that transcript. This suggests that           remains an important open question.
even very young children may have access to the infor-
mation that language is hierarchically structured.                Acknowledgments Thanks to Virginia Savova for
   This work has some limitations that should be ad-              helpful comments. Supported by an NDSEG fellowship
dressed with further research. While we showed that a             (AP) and the Paul E. Newton Chair (JBT).
comparison of appropriate grammars of each type results
in a preference for the hierarchically structured gram-                                    References
                                                                  Brown, R. (1973). A first language: The early stages. Harvard
mar, these grammars were not the result of an exhaustive             University Press.
search through the space of all grammars. It is almost            Chater, N. and Vitanyi, P. (2003). Simplicity: A unifying
certain that better grammars of either type could be                 principle in cognitive science? TICS, 7:19–22.
found, so any conclusions are preliminary. We have ex-            Chomsky, N. (1959). On certain formal properties of gram-
                                                                     mars. Information and Control, 2:137–167.
plored several ways to test the robustness of the analysis.
                                                                  Chomsky, N. (1965). Aspects of the Theory of Syntax. MIT
First, we conducted a local search using an algorithm                Press, Cambridge, MA.
inspired by Stolcke and Omohundro (1994), in which                Chomsky, N. (1971). Problems of Knowledge and Freedom.
a space of grammars is searched via successive merg-                 Fontana, London.
ing of states. The results using grammars produced by             Chomsky, N. (1980). In Piatelli-Palmarini, M., editor, Lan-
                                                                     guage and learning: The debate between Jean Piaget and
this search are qualitatively similar to the results shown           Noam Chomsky. Harvard Univ Press, Cambridge, MA.
here. Second, we tried several other regular grammars,            Crain, S. and Nakayama, M. (1987). Structure dependence
and again the hierarchical grammar was preferred. In                 in grammar formation. Language, 24:139–186.
general, the poor performance of the regular grammars             Goldwater, S., Griffiths, T., and Johnson, M. (2005). Inter-
                                                                     polating between types and tokens by estimating power
appears to reflect the fact that they fail to maximize               law generators. NIPS, 18.
the tradeoff between simplicity and generalization. The           Jaynes, E. (2003). Probability theory: The logic of science.
simpler regular grammars buy that simplicity only at the             Cambridge University Press, Cambridge.
cost of increasing overgeneralization, resulting in a high        Legate, J. and Yang, C. (2002). Empirical re-assessment of
                                                                     stimulus poverty arguments. Ling. Review, 19:151–162.
penalty in the likelihood.                                        Lewis, J. and Elman, J. (2001). Learnability and the statis-
   Are we trying to argue that the knowledge that lan-               tical structure of language: Poverty of stimulus arguments
guage is structure-dependent is not innate? No. All we               revisited. In Proc. of the 26th BU Conf. on Lang. Devel.
                                                                     Cascadilla Press.
have shown is that, contra the PoS argument, structure            Li, M. and Vitanyi, P. (1997). An Intro. to Kolmogorov com-
dependence need not be a part of innate linguistic knowl-            plexity and its applications. Springer Verlag, NY.
edge. It is true that the ability to represent PCFGs is           MacWhinney, B. (2000). The CHILDES project: Tools for
“given” to our model, but this is a relatively weak form             analyzing talk. Lawrence Erlbaum Ass., third edition.
                                                                  Pullum, G. and Scholz, B. (2002). Empirical assessment of
of innateness: few would argue that children are born                stimulus poverty arguments. Linguistic Review, 19:9–50.
without the capacity to represent the thoughts they later         Reali, F. and Christiansen, M. (2004). Structure dependence
grow to have, since if they were no learning would occur.            in language acquisition: Uncovering the statistical richness
Furthermore, everything that is built into the model –               of the stimulus. In Proc. of the 26th conference of the
                                                                     Cognitive Science Society.
the capacity to represent each grammar as well as the             Redington, M., Chater, N., and Finch, S. (1998). Distribu-
details of the Bayesian inference procedure – is domain-             tional information: A powerful cue for acquiring syntactic
general, not language-specific as the original PoS claim             categories. Cognitive Science, 22:425–469.
suggests.                                                         Stolcke, A. and Omohundro, S. (1994). Introducing proba-
                                                                     bilistic grammars by bayesian model merging. ICGI.
   In sum, we have demonstrated that a child equipped             Zipf, G. (1932). Selective studies and the principle of relative
with both the resources to learn a range of symbolic                 frequency in language. Harvard University Press.
                                                              668

