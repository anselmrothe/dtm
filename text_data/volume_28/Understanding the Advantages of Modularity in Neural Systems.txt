UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Understanding the Advantages of Modularity in Neural Systems
Permalink
https://escholarship.org/uc/item/6rj4t7c6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Author
Bullinaria, John A.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               Understanding the Advantages of Modularity in Neural Systems
                                          John A. Bullinaria (j.a.bullinaria@cs.bham.ac.uk)
                                           School of Computer Science, University of Birmingham,
                                                           Birmingham, B15 2TT, UK
                               Abstract                                    that a modular architecture was able to generate more
                                                                           efficient internal representations and learned more easily
   Understanding, or even defining, modularity in the human brain          than fully distributed networks. It is debatable whether this
   is not as straightforward as one might expect. It is natural to
                                                                           could be the real reason for modularity in the human visual
   assume that modularity offers computational advantages, and
   that evolution by natural selection would translate those               system (Mishkin, Ungerleider & Macko, 1983; Ungerleider
   advantages into the kind of modular neural structures familiar          & Haxby, 1994), but given the obvious potential for
   to cognitive scientists. However, explicit simulations of the           disruptive interference in the simultaneous processing of
   evolution of neural systems have shown that, in many cases, it          two independent tasks, it should be no surprise if, in general,
   is actually non-modular architectures that are most efficient. In       dedicated modules for two tasks work better than a single
   this paper, I present a further series of simulations that reveal a
                                                                           homogeneous system. It is then easy to imagine how
   crucial dependence on the details of the tasks that are being
   modelled, and the importance of taking into account physical            evolution by natural selection could enable that advantage to
   brain constraints, such as the degree of neural connectivity.           lead to the emergence of modularity.
   Eventually, we end up finding modularity emerging reliably                 The obvious next step was to simulate such an
   from evolution across a range of neural processing tasks.               evolutionary process and watch the modularity emerge.
                                                                           Although such simulations did show that modularity could
                           Introduction                                    evolve in that way if learning and performance were based
                                                                           on the Sum-Squared Error (SSE) measure, they also showed
Cognitive neuropsychology and fRMI research have made
                                                                           that even better non-modular systems could emerge if based
great progress in elucidating the structure of the human
                                                                           on the Cross Entropy (CE) error measure, thus throwing this
brain, and, although much controversy remains, it appears to
                                                                           whole approach into doubt (Bullinaria, 2001).
involve some degree of modularity. However, the reasons
                                                                              Other evolutionary neural network simulations involving
for that modularity are not very well understood, and it is
                                                                           the same what-where tasks (Di Ferdinando et al., 2001;
not clear how much of the modularity is innate and how
                                                                           Calabretta et al., 2003) have confirmed the increasingly
much arises through learning, nor even how one should best
                                                                           wide-spread belief that for complex tasks it is most efficient
define modularity (e.g., Jacobs, 1999; Elman et al., 1996).
                                                                           to have the neural architectures largely innate and the
A natural assumption is that modular systems have some
                                                                           connection weights largely learned (e.g., Elman et al.,
advantage over non-modular systems, and that either
                                                                           1996). These simulations have also elucidated further the
evolution or learning will therefore result in the emergence
                                                                           emergence of modularity in the SSE case, but they didn’t
of modules. The big question for cognitive scientists then
                                                                           consider CE based learning.
is: what are those advantages, and what mechanisms enable
                                                                              In another series of evolutionary neural network
those advantages to translate into modular architectures?
                                                                           simulations, Hüsken, Igel & Toussaint (2002) introduced
One promising approach has been to simulate or model the
                                                                           finer grained measures of modularity and also found that the
evolution of some appropriately simplified neural systems
                                                                           requirement for fast learning increased the selective pressure
performing simplified cognitive tasks, and study the
                                                                           for modularity in the SSE case, but could not reproduce
structures that emerge. In the next section of this paper I
                                                                           those results for the CE case. Most recently, Bowers &
review some of the previous attempts to do this, and identify
                                                                           Bullinaria (2005) took a computational embryogeny
some of their shortcomings. I will then present a new series
                                                                           approach to model the evolution of modularity at an even
of simulations that clarify many of the relevant issues. I end
                                                                           lower level of description, involving neural stem cells and
with some discussion and conclusions.
                                                                           connections growing along simulated chemical gradients.
                                                                           In these simulations, no sign of modularity arose until limits
                    Previous Explorations                                  were placed on dendritic distances and the output neurons
The earliest systematic study in this area was the Rueckl,                 corresponding to the two tasks were physically separated by
Cave & Kosslyn (1989) computational investigation of the                   sufficient distance. This was seen to be consistent with the
separation of “what” and “where” processing in the human                   consequences of a bias towards short connections discussed
brain. They carried out a series of simulations of simple                  by Jacobs & Jordan (1992).
neural network models trained to perform what and where                       In a non-evolutionary study, Jacobs, Jordan & Barto
classifications from simplified ‘retinal images’, and found                (1991) explored the emergence of modules in a gated
                                                                       119

mixtures of experts network, but it is difficult to set up those                 Task 1 Outputs             Task 2 Outputs
systems in such a way that there is no inherent bias towards
modularity. If one removes the bias towards modularity,
and evolves the gating parameters, we end up with the same
SSE versus CE differences as above (Bullinaria, 2002).
   Three factors clearly need further work: the dependence                   Nhid1               Nhid12                Nhid2
on the learning algorithm (e.g., SSE or CE), the effect of
physical constraints, and the dependence on the task (i.e.,
how general are the what-where results). The following
three sections will address each of these issues.                                                 Inputs
           Evolving the Learning Algorithm                           Figure 1: The simplified neural network architecture used to
                                                                     study the evolution of modularity.
The Bullinaria (2001, 2002) neural network simulations
showed that modularity was advantageous for the simplified
what-where problem if the SSE cost function was used for             as shown in Figure 1 is appropriate, with architecture
learning, as in the Rueckl et al. (1989) study, but not if the       parameters Nhid1, Nhid12 and Nhid2 that specify how many
CE cost function was used. For either cost function there            hidden units connect to each set of output units. If Nhid12
will be a trade-off between employing modularity to reduce           tends to zero, we have a modular architecture, with modules
the cross-task interference, and the additional flexibility and      consisting of a separate set of hidden units dedicated to each
free parameters arising from the full connectivity of non-           of the two tasks. If Nhid1 and Nhid2 both tend to zero, the
modular architectures. The question is: under which                  architecture is totally non-modular. The natural innate
circumstances will the trade-off favor modularity?                   learning parameters here are the random initial weight
   There is a well known problem when using the SSE cost             distributions [-lL, +uL] and learning rates η L for the four
function in neural networks with sigmoidal outputs and               network components L (input to hidden weights IH, hidden
binary targets. During learning, the gradient descent weight         unit biases HB, hidden to output weights HO, and output
updates are proportional to the output sigmoid derivatives,          biases OB). Previously, the learning algorithm has been
which are close to zero near totally incorrect outputs, as well      fixed to be standard gradient descent learning using either
as for correct outputs. This means that if the weight updates        the SSE or CE cost function (Bullinaria, 2001). Here we
from distinct training patterns interfere with each other in         shall let the learning algorithm itself evolve too by using a
such a way as to cause incorrect outputs, then it can be             cost function that can be SSE, CE, or anything in between:
difficult, if not impossible, to correct them later. Attempts
to evolve solutions to this problem for general single task
                                                                                          E = (1− µ )E SSE + µECE
binary mappings (Bullinaria, 2003) consistently resulted in          The parameter µ is bounded to lie in the range [0, 1], so the
the SSE learning algorithm evolving into the CE learning             extreme values of 0 and 1 correspond to the SSE and CE
algorithm. The problematic sigmoid derivatives cancel out            learning algorithm, while a value around 0.1 corresponds to
of the weight updates for the CE cost function, and there are        the traditional ‘sigmoid prime offset’ approach for avoiding
also good theoretical reasons why the CE cost function is            the SSE learning problem (Bullinaria, 2003). If we have a
more appropriate for classification tasks anyway (Bishop,            fixed total number of hidden units, that gives us two
2001). It is not surprising then, that the Bullinaria (2001)         architecture and thirteen learning parameters to evolve.
study found that the interference prone SSE case favored                We shall start with the same what-where training data set
modularity, while the superior CE algorithm preferred the            as used by Rueckl et al. (1989) and most subsequent studies,
extra flexibility of non-modularity. The question remains:           with nine 3×3 patterns that may appear in nine positions in a
will non-modularity always be the preferred option? In the           5×5 input space. Fitness here corresponds to the number of
remainder of this paper I shall present a further series of          training epochs required to correctly classify all 81 input
simulations that explore this issue.                                 patterns. The simulation results have been found to be
   The general idea of evolving neural networks is now well          extremely robust with respect to the evolutionary details.
established (e.g., Yao, 1999). One takes a whole population          All the results presented here are for populations of 100
of individual neural networks, each specified by a series of         individuals. At each generation, half the children copy the
innate parameters. Then at each generation, the least fit            innate parameters of a parent, and half have each parameter
individuals are replaced by children produced from the               value chosen from the range spanned by their two parents,
fittest individuals (using appropriate forms of cross-over and       plus random Gaussian mutations that allow parameters
mutation). Such repeated natural selection causes useful             outside that range. The initial population was started with
innate characteristics to proliferate in the population, and         random innate parameters, and the evolutionary process
fitness levels improve towards some local optimum.                   continued until all the parameters had clearly settled down.
   For our purposes, a standard feed-forward neural network             Figure 2 shows the evolutionary simulation results for
                                                                 120

       3                                                                    1.0
                                                                            0.8
       2
                                                        etaHO               0.6
   eta                                                                    mu
                                                        etaIH               0.4
       1
                                                                            0.2
                                               etaHB etaOB
       0                                                                    0.0
               0      2000   Generation 4000               6000                        0     2000
                                                                                                    Generation 4000           6000
     36
      1                                                                      60
                                                   Nhid12
     241                                                                     40
   Units                                                                  Epochs
     120                                                                     20
                                                       Nhid1
                                               Nhid2
           0                                                                       0
               0      2000   Generation 4000               6000                        0     2000   Generation 4000           6000
Figure 2: The evolution of the standard what-where neural network with 36 hidden units: the learning rates (top left), the CE
versus SSE parameter µ (top right), the architecture parameters (bottom left), and epochs of training required (bottom right).
neural networks with 36 hidden units, with mean values and              computational power available. It seems that the optimality
standard deviations over ten runs. The learning rates and               of non-modular architectures for this what-where task is
initial weight distributions don’t tell us much, apart from the         quite robust with respect to network complexity too.
fact that they differ somewhat from the kinds of parameters
traditionally used in hand-built networks. The parameter µ               Physical Constraints on Neural Connectivity
ends up very close to 1, corresponding to a purely CE
learning algorithm. The evolved architecture parameters                 In building cognitive models, one naturally needs to take
correspond to totally non-modular networks. Together the                into account the physical properties of the brain, in addition
evolved parameters result in the training data being learned            to the computations they are performing. Of particular
in around 18 epochs, and provide a solid confirmation of the            relevance to us here is the fact that the significant volume
earlier results (Bullinaria, 2001) that the requirement for             occupied by neural connections (i.e. axons and dendrites)
faster learning in this what-where task leads reliably to the           precludes full neural connectivity (Chklovskii et al., 2002;
emergence of non-modular neural architectures.                          Sporns et al., 2004). Jacobs & Jordan (1992) and Bowers &
   A further important consideration is the relation between            Bullinaria (2005) have already looked at the emergence of
the computational power of the neural network compared                  restricted connectivity resulting from a bias towards short
with the complexity of the task. It is easy to check this by            connections in models where the neurons have positions
repeating the above simulations with different total numbers            specified in a three dimensional space. In this section I will
of hidden units. Figure 3 shows how the evolved network                 show that modularity will emerge simply by restricting the
architecture and performance varies with the computational              proportion of connections, without regard to the neuron
power. On the left we see that the evolved architecture                 positions and connection lengths. With a given pattern of
remains non-modular from the minimal network required to                connectivity, evolution will surely arrange the neurons and
perform the given task (9 hidden units) to over a hundred               connections to minimize the volume of connections, but
times that size (1000 units). On the right we see how the               restrictions on the connectivity proportions alone is
required number of epochs of training varies with the                   sufficient to lead to the evolution of modularity.
                                                                  121

     1.0                                                                            90
                    Nhid12
     0.8
                                                                                    60
     0.6
   Prop.                                                                        Epochs
     0.4
                                                                                    30
     0.2
                        Nhid1
                    Nhid2
     0.0                                                                                 0
             1               10   Hidden Units 100               1000                        1      10     Hidden Units 100         1000
Figure 3: Dependence of the evolved what-where neural network results on the total number of hidden units. The architecture
parameters as a proportion of the total number of hidden units (left), and the number of epochs of training required (right).
       1.0                                                                          90
                                                        Nhid12
       0.8
                                                                                    60
       0.6
    Prop.                                                                       Epochs
       0.4
                                                        Nhid1                       30
       0.2
                                               Nhid2
       0.0                                                                               0
          0.25               0.50
                                    Connectivity 0.75            1.00                     0.25      0.50
                                                                                                           Connectivity 0.75        1.00
Figure 4: Dependence of the evolved what-where neural network results on the degree of connectivity between the network
layers. The architecture parameters as proportions (left), and the number of epochs of training required (right).
   The above simulations can easily be modified to test these                 constraints on the proportion of neural connectivity, but it is
ideas – one simply has to repeat them with the degree of                      not obvious that this will be true of all tasks. A particular
connectivity between layers restricted to some fraction f of                  worry is that learning a small set of input-output mappings
full connectivity. Figure 4 shows the architectures that                      for the what-where task is very different to most realistic
emerge if we have 72 hidden units in total. As we reduce f,                   human cognitive tasks in which we are typically required to
the number of hidden units shared by both output tasks,                       generalize from, and respond to, an unpredictable stream of
Nhid12, falls almost linearly until f reaches 0.5 and then                    inputs drawn from some continuous data distribution.
stays around zero for all lower levels of connectivity. This                    A typical problem humans are faced with is to classify in
means that a modular architecture makes the most efficient                    various ways new inputs drawn from some continuous space
use of the available connections if they are limited to the                   by learning to generalize from different examples they have
extent that is found in real brains. Throughout, Nhid2,                       experienced before. To keep things simple for simulation
corresponding to the easier ‘where’ task, is lower than                       purposes, suppose we have just two continuous valued
Nhid1, as was found in the modular SSE simulations                            inputs that are normalized to lie in the range [0, 1], and we
(Bullinaria, 2001) and the original Rueckl et al. (1989)                      need to perform two distinct classifications based on those
study, but the appropriate relative size of the two modules                   input values. For example, the inputs could correspond to
varies with the connectivity proportion.                                      two crucial measurable characteristics of animals, and the
                                                                              two output tasks could be to classify them as being good
                 More Realistic Learning Tasks                                food (or not) and dangerous (or not). We require our neural
                                                                              networks to learn the classification boundaries in our two
We have now established that modularity will only be an                       dimensional input space for each output task, from a
advantage for learning the what-where task when there are                     continuous stream of examples. Obviously, even for this
                                                                        122

      1.0                                                                        1.0
      0.8                                                                        0.8
      0.6                                                                        0.6
  Prop.                                                                       Prop.
                    Nhid1                                                                      Nhid1
      0.4                                                                        0.4
                      Nhid2                                                                            Nhid2
      0.2                                                                        0.2
                                                                                                                     Nhid12
                        Nhid12
      0.0                                                                        0.0
         0.25                 0.50
                                     Connectivity 0.75     1.00                     0.25                   0.50
                                                                                                                  Connectivity 0.75   1.00
    450                                                                        450
    300                                                                        300
   Epochs                                                                     Epochs
    150                                                                        150
            0                                                                          0
             0.25             0.50                  0.75   1.00                         0.25               0.50
                                     Connectivity                                                                 Connectivity 0.75   1.00
Figure 5: Results for two online generalization problems. The two pairs of classification boundaries (top), the architecture
parameters as functions of connectivity proportion (middle), and the number of epochs of training required (bottom).
simplified set-up, there are an infinite number of possible               blocks required before a full block of items was classified
tasks corresponding to different classification boundaries.               correctly before training on each item.
The question is: will a separate module for each output task                 It did not take many simulations to find the answer that
consistently work better or worse than a fully distributed                “the advantage of modularity is problem dependent”, and
network, or will the need for modularity be problem                       that the advantage depends on many factors, in particular,
dependent? I attempted to answer that by re-running the                   the overlap of the two classification tasks, the relative
above simulations with everything else the same except for                difficulties of the two tasks, the complexity of the decision
the training data and the fitness measure. Here fitness is the            boundaries, and the number of classes. The two simple
ability to learn quickly to generalize, i.e. to produce the right         problems shown in Figure 5 illustrate this. The case on the
outputs for each new item before training on it, rather than              left has one two class task and one three class task. We see
producing the right outputs for each item after many epochs               that for full neural connectivity, the optimal architecture is
of training on it. In practice, the infinite sequence of                  non-modular, and that as the degree of neural connectivity is
possible inputs was presented in blocks (or epochs) of 400                reduced, the degree of modularity increases, as for the what-
training items, and fitness was measured as the number of                 where case earlier. The case on the right has two two class
                                                                    123

tasks. In this case, a modular architecture is found to evolve       Bullinaria, J.A. (2001). Simulating the evolution of modular
for any degree of neural connectivity. As one would expect,            neural systems. In Proceedings of the Twenty-third
for both cases, the average amount of training data required           Annual Conference of the Cognitive Science Society, 146-
to reach a block of perfect performance increases as the               151. Mahwah, NJ: Lawrence Erlbaum.
connectivity, and hence computational power, is reduced.             Bullinaria, J.A. (2002). The evolution of gated sub-networks
   A final complication is that we need to check that the              and modularity in the human brain. In: J.A. Bullinaria &
evolution has not become stuck with an architecture worse              W. Lowe (Eds), Connectionist Models of Cognition and
than the global optimum. So, all the simulations were run              Perception, 27-39. Singapore: World Scientific.
again with the architecture constrained to be modular, and           Bullinaria, J.A. (2003). Evolving efficient learning
again with it constrained to be non-modular. These runs                algorithms for binary mappings. Neural Networks, 16,
confirmed that the evolved architectures did indeed produce            793-800.
the fastest learning performance for each task.                      Calabretta, R., Di Ferinando, A., Wagner, G.P. & Parisi, D.
                                                                       (2003). What does it take to evolve behaviourally
              Discussion and Conclusions                               complex organisms? BioSystems, 69, 245-262.
                                                                     Chklovskii, D.B., Schikorski, T. & Stevens, C.F. (2002).
This paper began by reviewing the previous attempts to                 Wiring optimization in cortical circuits. Neuron, 34, 341-
understand the advantages of modularity in neural systems,             347.
and how evolution could translate those advantages into              Di Ferdinando, A., Calabretta, R, & Parisi, D. (2001).
brain structures. These studies were extended here by                  Evolving modular architectures for neural networks. In
allowing the neural learning algorithms to evolve alongside            R.F. French & J.P. Sougne (Eds), Connectionist Models
the architectures, and by investigating more realistic                 of Learning, Development and Evolution, 253-262.
learning tasks. We found that for many tasks there is no               London: Springer-Verlag.
learning advantage for modularity because the reduction in           Elman, J.L., Bates, E.A., Johnson, M.H., Karmiloff-Smith,
cross-task interference that modularity provides is out-               A., Parisi, D. & Plunkett, K. (1996). R e t h i n k i n g
weighed by the extra computational power allowed by full               innateness: A connectionist perspective on development.
connectivity. For other tasks, the problem of interference is          Cambridge, MA: MIT Press.
more important than computational power, and modularity              Hüsken, M., Igel, C. & Toussaint, M. (2002). Task-
does evolve. For artificial systems then, the need for                 dependent evolution of modularity in neural networks.
modularity is problem dependent, and it is proving difficult           Connection Science, 14, 219-229.
to formulate general purpose heuristics to tell us when there        Jacobs, R.A. (1999). Computational studies of the
is an advantage to modularity, and when there isn’t.
                                                                       development of functionally specialized neural modules.
   Cognitive scientists, of course, are interested in biological
                                                                       Trends in Cognitive Science, 3, 31-38.
brains, rather than artificial systems, and their models are
                                                                     Jacobs, R.A. & Jordan, M.I. (1992). Computational
further constrained by various physical properties. Once we
                                                                       consequences of a bias toward short connections. Journal
incorporate the known physical constraints on the degree of
                                                                       of Cognitive Neuroscience, 4, 323-336.
neural connectivity (Chklovskii et al., 2002) into those brain
                                                                     Jacobs, R.A., Jordan, M.I. & Barto, A.G. (1991). Task
models, modular architectures are found to have a clear
                                                                       decomposition through competition in modular
advantage when it comes to learning efficiently, and
                                                                       connectionist architecture: The what and where vision
simulated evolution does lead to the emergence of modular
                                                                       tasks. Cognitive Science, 15, 219-250.
structures, for all the pairs of simplified tasks considered.
                                                                     Mishkin, M., Ungerleider, L.G. & Macko, K.A. (1983).
Understanding how these modules actually come about in
                                                                       Object vision and spatial vision: Two cortical pathways.
real brains is something that still requires more detailed
                                                                       Trends in Neurosciences, 6, 414-417.
simulations, including more realistic neural structures and
                                                                     Rueckl, J.G., Cave, K.R. & Kosslyn, S.M. (1989). Why are
connectivity patterns, as well as incorporation of the known
stages in human brain evolution. I hope to report on such              “what” and “where” processed by separate cortical visual
simulations in the near future.                                        systems? A computational investigation. Journal of
                                                                       Cognitive Neuroscience, 1, 171-186.
                                                                     Sporns, O., Chialvo, D.R., Kaiser, M. & Hilgetag, C.C.
                         References                                    (2004). Organization, development and function of
Bishop, C.M. (2001). Neural networks for pattern                       complex brain networks. Trends in Cognitive Sciences, 8,
   recognition. Oxford, UK: Oxford University Press.                   418-425.
Bowers, C.P. & Bullinaria, J.A. (2005). Embryological                Ungerleider, L.G. & Haxby, J.V. (1994). ‘What’ and
   modelling of the evolution of neural architecture. In: A.           ‘where’ in the human brain. Current Opinion in
   Cangelosi, G. Bugmann & R. Borisyuk (Eds), Modeling                 Neurobiology, 4, 157-165.
   Language, Cognition and Action, 375-384. Singapore:               Yao, X. (1999). Evolving artificial neural networks.
   World Scientific.                                                   Proceedings of the IEEE, 87, 1423-1447.
                                                                 124

