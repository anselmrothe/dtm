UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Auxiliary Fronting with Grammatical Inference
Permalink
https://escholarship.org/uc/item/44k200cm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Clark, Alexander
Eyraud, Remi
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

              Learning Auxiliary Fronting with Grammatical Inference
                                       Alexander Clark (alexc@cs.rhul.ac.uk)
                                              Department of Computer Science
                                            Royal Holloway University of London
                                                  Egham, Surrey TW20, UK
                                Rémi Eyraud (remi.eyraud@univ-st-etienne.fr)
                                                             EURISE
                                              23, rue du Docteur Paul Michelon
                                            42023 Saint-Étienne Cedex 2, France
                          Abstract                                 about language; no knowledge of X-bar schemas, no hid-
                                                                   den sources of information to reveal the structure. It
   We present a simple context-free grammatical inference          operates purely on unannotated strings of raw text. Ob-
   algorithm, and prove that it is capable of learning an
   interesting subclass of context-free languages. We also         viously, as all learning algorithms do, it has an implicit
   demonstrate that an implementation of this algorithm            learning bias. This very simple algorithm has a particu-
   is capable of learning auxiliary fronting in polar inter-       larly clear bias, with a simple mathematical description,
   rogatives (AFIPI) in English. This has been one of the          that allows a remarkably simple characterisation of the
   most important test cases in language acquisition over          set of languages that it can learn. This algorithm does
   the last few decades. We demonstrate that learning can
   proceed even in the complete absence of examples of             not use a statistical learning paradigm that has to be
   particular constructions, and thus that debates about           tested on large quantities of data. Rather it uses a sym-
   the frequency of occurrence of such constructions are ir-       bolic learning paradigm, that works efficiently with very
   relevant. We discuss the implications of this on the type       small quantities of data, while being very sensitive to
   of innate learning biases that must be hypothesized to
   explain first language acquisition.                             noise. We discuss this choice in some depth below.
                                                                      For reasons that were first pointed out by Chomsky
                                                                   (Chomsky, 1975, pages 129–137), algorithms of this type
                      Introduction                                 are not capable of learning all of natural language. It
For some years, a particular set of examples has been              turns out, however, that algorithms based on this ap-
used to provide support for nativist theories of first lan-        proach are sufficiently strong to learn some key prop-
guage acquisition (FLA). These examples, which hinge               erties of language, such as the correct rule for forming
around auxiliary inversion in the formation of questions           polar questions.
in English, have been considered to provide a strong ar-              In the next section we shall describe the dispute
gument in favour of the nativist claim: that FLA pro-              briefly; in the subsequent sections we will describe the al-
ceeds primarily through innately specified domain spe-             gorithm we use, and the experiments we have performed.
cific mechanisms or knowledge, rather than through the
operation of general-purpose cognitive mechanisms. A                                    The Dispute
key point of empirical debate is the frequency of occur-           We will present the dispute in traditional terms, though
rence of the forms in question. If these are vanishingly           later we shall analyse some of the assumptions implicit
rare, or non-existent in the primary linguistic data, and          in this description. In English, polar interrogatives
yet children acquire the construction in question, then            (yes/no questions) are formed by fronting an auxiliary,
the hypothesis that they have innate knowledge would               and adding a dummy auxiliary “do” if the main verb is
be supported. But this rests on the assumption that                not an auxiliary. For example,
examples of that specific construction are necessary for
learning to proceed. In this paper we show that this as-           Example 1a The man is hungry.
sumption is false: that this particular construction can
be learned without the learner being exposed to any
examples of that particular type. Our demonstration                Example 1b Is the man hungry?
is primarily mathematical/computational: we present a                 When the subject NP has a relative clause that also
simple experiment that demonstrates the applicability of           contains an auxiliary, the auxiliary that is moved is not
this approach to this particular problem neatly, but the           the auxiliary in the relative clause, but the one in the
data we use is not intended to be a realistic representa-          main (matrix) clause.
tion of the primary linguistic data, nor is the particular
algorithm we use suitable for large scale grammar induc-           Example 2a The man who is eating is hungry.
tion.
   We present a general purpose context-free grammat-              Example 2b Is the man who is eating hungry?
ical algorithm that is provably correct under a certain               An alternative rule would be to move the first occur-
learning criterion. This algorithm incorporates no do-             ring auxiliary, i.e. the one in the relative clause, which
main specific knowledge: it has no specific information            would produce the form
                                                              1127

Example 2c Is the man who eating is hungry?                      heuristics: frequency, information theoretic measures of
   In some sense, there is no reason that children should        constituency, and finally substitutability. 2 The first
favour the correct rule, rather than the incorrect one,          rests on the observation that strings of words generated
since they are both of similar complexity and so on. Yet         by constituents are likely to occur more frequently than
children do in fact, when provided with the appropriate          by chance. The second heuristic looks for information
context, produce sentences of the form of Example 2b,            theoretic measures that may predict boundaries, such as
and rarely if ever produce errors of the form Example            drops in conditional entropy. The third method which
2c (Crain & Nakayama, 1987). The problem is how to               is the foundation of the algorithm we use, is based on
account for this phenomenon.                                     the distributional analysis of Harris (Harris, 1954). This
   Chomsky claimed first, that sentences of the type in          principle has been appealed to by many researchers in
Example 2b are vanishingly rare in the linguistic envi-          the field of grammatical inference, but these appeals have
ronment that children are exposed to, yet when tested            normally been informal and heuristic (Zaanen, 2000).
they unfailingly produce the correct form rather than               In its crudest form we can define it as follows: given
the incorrect Example 2c. This is put forward as strong          two sentences “I saw a cat over there”, and “I saw a dog
evidence in favour of innately specified language specific       over there” the learner will hypothesize that “cat” and
knowledge: we shall refer to this view as linguistic na-         “dog” are similar, since they appear in the same context
tivism.                                                          “I saw a __ there”. Pairs of sentences of this form can
   In a special volume of the Linguistic Review, Pullum          be taken as evidence that two words, or strings of words
and Scholz (Pullum & Scholz, 2002), showed that in fact          are substitutable.
sentences of this type are not rare at all. Much discus-
sion ensued on this empirical question and the conse-            Preliminaries
quences of this in the context of arguments for linguis-         We will present a formal analysis here using a minimal
tic nativism. These debates revolved around both the             amount of notation; for complete definitions and a full
methodology employed in the study, and also the conse-           proof the reader is referred to (Clark & Eyraud, 2005).
quences of such claims for nativist theories. It is fair to      We will use some basic ideas from formal language the-
say that in spite of the strength of Pullum and Scholz’s         ory. We have a finite alphabet, which is a non-empty
arguments, nativists remained completely unconvinced             set of symbols, which we denote by Σ. In this case the
by the overall argument.                                         symbols will be words. We consider all finite sequences
   (Reali & Christiansen, 2004) present a possible so-           of these symbols; the set of all such finite symbols is de-
lution to this problem. They claim that local statis-            noted by Σ∗ . A formal language L is a subset of Σ∗ ,
tics, effectively n-grams, can be sufficient to indicate to      and since we are concerned here with syntax, we will be
the learner which alternative should be preferred. How-          considering L to be the set of all grammatical sentences.
ever this argument has been carefully rebutted by (Kam,             We can represent languages, which will generally be
Stoyneshka, Tornyova, Fodor, & Sakas, 2005), who show            infinite or at least very large sets, in a variety of ways.
that this argument relies purely on a phonological coinci-       Here we will use context free grammars, which consist of
dence in English. This is unsurprising since it is implau-       a set of non-terminal symbols V which will correspond
sible that a flat, finite-state model should be powerful         normally to constituents like NP, VP and so on. We will
enough to model a phenomenon that is clearly structure           have a set of productions, or rewrite rules which we write
dependent in this way.                                           as N → α, where N ∈ V and α is a non empty string
   In this paper we argue that the discussion about the         of non-terminal and terminal symbols, which will be the
rarity of sentences that exhibit this particular structure      alphabet Σ. We will also have a distinguished element
is irrelevant: we show that simple grammatical inference        S of V which represents the sentence symbol. If we can
algorithms can learn this property even in the complete         derive a string of symbols β from another string α, by
absence of sentences of this particular type. Thus the          using one application of a production in the grammar,
                                                                                                                       ∗
issue as to how frequently an infant child will see them        then we will say α ⇒ β, and we will write α ⇒ β if
is a moot point.                                                 the derivation uses zero or more derivation steps. This
                                                                 defines a language, a subset of Σ∗ written L(G) = {w ∈
                       Algorithm                                           ∗
                                                                 Σ∗ : S ⇒ w}, which is just the set of all strings that can
Context-free grammatical inference algorithms are ex-            be derived from the senence symbol using the rules in
plored in two different communities: in grammatical in-          the grammar. This is the most standard representation
ference and in NLP. The task in NLP is normally taken            for defining languages, but it is important to note that
to be one of recovering appropriate annotations (Smith &         there are many other forms.
Eisner, 2005) that normally represent constituent struc-
ture (strong learning), while in grammatical inference,          Learning
researchers are more interested in merely identifying the        We now define our learning criterion. This is identifica-
language (weak learning). In both communities, the               tion in the limit from positive text (Gold, 1967), with
best performing algorithms that learn from raw positive          polynomial bounds on data and computation, but not
data only 1 , generally rely on some combination of three        on errors of prediction (de la Higuera, 1997).
    1                                                                2
      We do not consider in this paper the complex and con-            For completeness we should include lexical dependencies
tentious issues around negative data.                           or attraction.
                                                            1128

                                                                                                                         .
   This means that our learning algorithm is going to be           We call this weak substitutability and write it as u =L
                                                                                                   .
presented with examples from sentences in the language          v. Clearly u ≡L v implies u =L v when u is a sub-
(grammatical sentences) in sequence, one at a time, and         string of the language. Any two strings that do not occur
that it must hypothesize a grammar for the language             as substrings of the language are obviously syntactically
at each step. In the identification in the limit frame-         congruent but not weakly substitutable.
work, there is only one constraint: every grammatical              First of all, observe that syntactic congruence is a
sentence must eventually appear. This means that this           purely language theoretic notion that makes no refer-
model of learning is unreasonably difficult, as we have         ence to the grammatical representation of the language,
to learn even when the examples are being ordered in            but only to the set of strings that occur in it. However
a deliberately misleading way. Since we would expect            there is an obvious problem: syntactic congruence tells
the examples to be either random or helpfully chosen,           us something very useful about the language, but all we
this limits the sorts of language classes that we can get       can observe is weak substitutability.
learnability results for.                                          When working within a Gold-style identification in the
   We say that an algorithm identifies in the limit a class     limit (IIL) paradigm, we cannot rely on statistical prop-
of languages, if eventually it will converge to exactly         erties of the input sample, since they will in general not
the right answer; i.e. it will make only a finite num-          be generated by random draws from a fixed distribution.
ber of errors. We further require that the algorithm            This, as is well known, severely limits the class of lan-
needs only polynomially bounded amounts of data and             guages that can be learned under this paradigm. How-
computation. We use the definition of de la Higuera             ever, the comparative simplicity of the IIL paradigm in
(de la Higuera, 1997). Since the ordering of the data is        the form when there are polynomial constraints on size of
potentially misleading, the requirement for polynomial          characteristic sets and computation(de la Higuera, 1997)
amounts of data translates into the requirement for a           makes it a suitable starting point for analysis.
polynomial characteristic set – a set of sentences, of size
                                                                   Given these restrictions, one solution to this problem
polynomial in the size of the representation, the number
                                                                is simply to define a class of languages where substi-
of symbols in the grammar in this case, such that once
                                                                tutability implies congruence. We call these the sub-
we have seen this set of sentences, the learner will have
                                                                stitutable languages: A language L is substitutable if
converged to the right answer.                                                                                 .
                                                                and only if for every pair of strings u, v, u =L v implies
   The other important constraint is that the amount of
                                                                u ≡L v. This rather radical solution clearly rules out
computation is limited. Human children do not have
                                                                the syntax of natural languages, at least if we consider
infinite computational power, and thus we must restrict
                                                                them as strings of raw words, rather than as strings of
our attention to algorithms that can run efficiently; thus
                                                                lexical or syntactic categories. Lexical ambiguity alone
we require worst case polynomial complexity, where the
                                                                violates this requirement: consider the sentences “The
polynomial is in the total length of the data presented.
                                                                rose died”, “The cat died” and “The cat rose from its
We require rapid learning, and this shows up both in the
                                                                basket”. A more serious problem is pairs of sentences
limited amount of data we allow the learner access to,
                                                                like “John is hungry” and “John is running”, where it is
and the limited amount of computational time that we
                                                                not ambiguity in the syntactic category of the word that
allow the learner to use. There are better formal models
                                                                causes the problem, but rather ambiguity in the con-
of first language acquisition than this, but the simplicity
                                                                text. Using this assumption, whether it is true or false,
of this one allows for very simple proofs.
                                                                we can then construct a simple algorithm for grammati-
Distributional learning                                         cal inference, based purely on the idea that whenever we
                                                                find a pair of strings that are weakly substitutable, we
The key to the Harris approach for learning a language          can generalise the hypothesized language so that they
L, is to look at pairs of substrings u and v and to see         are syntactically congruent.
whether they occur in the same contexts; that is to say,
                                                                   The algorithm proceeds by constructing a graph where
to look for pairs of strings of the form lur and lvr that
                                                                every substring in the sample defines a node. An arc is
are both in L. This can be taken as evidence that there
                                                                drawn between two nodes if and only if the two nodes
is a non-terminal symbol that generates both strings. In
                                                                are weakly substitutable with respect to the sample, i.e.
the informal descriptions of this that appear in Harris’s
                                                                there is an arc between u and v if and only if we have
work, there is an ambiguity between two ideas. The
                                                                observed in the sample strings of the form lur and lvr.
first is that they should appear in all the same contexts;
                                                                Clearly all of the strings in the sample will form a clique
and the second is that they should appear in some of
                                                                in this graph (consider when l and r are both empty
the same contexts. We can write the first criterion as
                                                                strings). The connected components of this graph can
follows:
                                                                be computed in time polynomial in the total size of the
                                                                sample. If the language is substitutable then each of
             ∀l, r lur ∈ L if and only if lvr ∈ L       (1)     these components will correspond to a congruence class
This has also been known in language theory by the              of the language.
name syntactic congruence, and can be written u ≡L v.              There are two ways of doing this: one way, which is
   The second, weaker, criterion is                             perhaps the purest involves defining a reduction system
                                                                or semi-Thue system which directly captures this gen-
                   ∃l, r lur ∈ L and lvr ∈ L            (2)     eralisation process. The second way, which we present
                                                            1129

here, will be more familiar to computational linguists,          Proof (Sketch) We can assume without loss of gen-
and involves constructing a grammar.                             erality that the target grammar is in Chomsky normal
                                                                 form. We first define a characteristic set, that is to say
Grammar construction                                             a set of strings such that whenever the sample includes
Simply knowing the syntactic congruence might not ap-            the characteristic set, the algorithm will output a correct
pear to be enough to learn a context-free grammar, but           grammar.
in fact it is. Given the syntactic congruence, and a sam-           Using some shortest first order on strings, we define
ple of the language, we can simple write down a grammar          w(α) ∈ Σ∗ to be the smallest word, generated by α ∈
in Chomsky normal form, and under quite weak assump-             (Σ ∪ V )+ . For each non-terminal N ∈ V define c(N ) to
tions this grammar will converge to a correct grammar            be the smallest pair of terminal strings (l, r), such that
                                                                     ∗
for the language.                                                S ⇒ lN r.
   This construction relies on a simple property of the             We can now define the characteristic set CS =
syntactic congruence, namely that is in fact a congru-           {lwr|(N → α) ∈ P, (l, r) = c(N ), w = w(α)}. The car-
ence: i.e.,                                                      dinality of this set is at most |P | which is clearly poly-
                                                                 nomially bounded. We observe that the computations
               u ≡L v implies ∀l, r lur ≡L lvr                   involved can all be polynomially bounded in the total
   We can construct a grammar in the following trivial           size of the sample.
way, from a sample of strings where we are given the                We next show that whenever the algorithm encoun-
syntactic congruence.                                            ters a sample that includes this characteristic set, it out-
                                                                 puts the right grammar. We write Ĝ for the learned
                                                                                                ∗
• The non-terminals of the grammar are identified with           grammar. Suppose [u] ⇒Ĝ v. Then we can see that
   the congruence classes of the language.                       u ≡L v by induction on the maximum length of the
                                                                 derivation of v. At each step we must use some rule
• For any string w = uv , we add a production [w] →              [u0 ] ⇒ [v 0 ][w0 ]. It is easy to see that every rule of this
   [u][v].                                                       type preserves the syntactic congruence of the left and
• For all strings a of length one (i.e. letters of Σ), we        right sides of the rules. Intuitively, the algorithm will
   add productions of the form [a] → a.                          never generate too large a language, since the languages
                                                                 are substitutable. Conversely, if we have a derivation
• The start symbol is the congruence class which con-            of a string u with respect to the target grammar G,
   tains all the strings of the language.                        by construction of the characteristic set, we will have,
                                                                 for every production L → M N in the target grammar,
   This defines a grammar in CNF. At first sight, this           a production in the hypothesized grammar of the form
construction might appear to be completely vacuous,              [w(L)] → [w(M )][w(N )], and for every production of the
and not to define any strings beyond those in the sample.        form L → a we have a production [w(L)] → a. A simple
The situation where it generalises is when two different         recursive argument shows that the hypothesized gram-
strings are congruent: if uv = w ≡ w0 = u0 v 0 then we will      mar will generate all the strings in the target language.
have two different rules [w] → [u][v] and [w] → [u0 ][v 0 ],     Thus the grammar will generate all and only the strings
since [w] is the same non-terminal as [w0 ].                     required (QED).
   A striking feature of this algorithm is that it makes
no attempt to identify which of these congruence classes         Related work
correspond to non-terminals in the target grammar. In-           This is the first provably correct and efficient gram-
deed that is to some extent an ill-posed question. There         matical inference algorithm for a linguistically interest-
are many different ways of assigning constituent struc-          ing class of context-free grammars (but see for example
ture to sentences, and indeed some reputable theories            (Yokomori, 2003) on the class of very simple grammars).
of syntax, such as dependency grammars, dispense with            It can also be compared to Angluin’s famous work on
the notion of constituent structure all together. De facto       reversible grammars (Angluin, 1982) which inspired a
standards, such as the Penn treebank annotations are a           similar paper(Pilato & Berwick, 1985).
somewhat arbitrary compromise among many different
possible analyses. This algorithm instead relies on the                                   Experiments
syntactic congruence, which expresses the combinatorial          We decided to see whether this algorithm without mod-
structure of the language in its purest form.                    ification could shed some light on the debate discussed
                                                                 above. The experiments we present here are not intended
Proof
                                                                 to be an exhaustive test of the learnability of natural
We will now present our main result, with an outline             language. The focus is on determining whether learning
proof. For a full proof the reader is referred to (Clark &       can proceed in the absence of positive samples, and given
Eyraud, 2005).                                                   only a very weak general purpose bias.
Theorem 1 This algorithm polynomially identifies in              Implementation
the limit the class of substitutable context-free lan-           We have implemented the algorithm described above.
guages.                                                          There are a number of algorithmic issues that were ad-
                                                            1130

  the man who is hungry died .                                     it rains
  the man ordered dinner .                                         it may rain
  the man died .                                                   it may have rained
  the man is hungry .                                              it may be raining
  is the man hungry ?                                              it has rained
  the man is ordering dinner .                                     it has been raining
  is the man who is hungry ordering dinner ?                       it is raining
  ∗is the man who hungry is ordering dinner ?                      it may have been raining
                                                                   ∗it may have been rained
Table 1: Auxiliary fronting data set. Examples above               ∗it may been have rain
the line were presented to the algorithm during the train-         ∗it may have been rain
ing phase, and it was tested on examples below the line.
                                                                 Table 2: English auxiliary data. Training data above the
                                                                 line, and testing data below.
dressed. First, in order to find which pairs of strings
are substitutable, the naive approach would be to com-
pare strings pairwise which would be quadratic in the            Again the algorithm correctly learns.
number of sentences. A more efficient approach main-
tains a hashtable mapping from contexts to congruence                                   Discussion
classes. Caching hashcodes, and using a union-find al-           Chomsky was among the first to point out the limita-
gorithm for merging classes allows an algorithm that is          tions of Harris’s approach, and it is certainly true that
effectively linear in the number of sentences.                   the grammars produced from these toy examples over-
   In order to handle large data sets with thousands of          generate radically. On more realistic language samples
sentences, it was necessary to modify the algorithm in           this algorithm would eventually start to generate even
various ways which slightly altered its formal properties.       the incorrect forms of polar questions.
However for the experiments reported here we used a                 Given the solution we propose it is worth looking again
version which performs exactly in line with the mathe-           and examining why nativists have felt that AFIPI was
matical description above.                                       such an important issue. It appears that there are sev-
                                                                 eral different areas. First, the debate has always focussed
Data                                                             on how to construct the interrogative from the declara-
For clarity of exposition, we have used extremely small          tive form. The problem has been cast as finding which
artificial data-sets, consisting only of sentences of types      auxilary should be “moved”. Implicit in this is the as-
that would indubitably occur in the linguistic experience        sumption that the interrogative structure must be de-
of a child.                                                      fined with reference to the declarative, one of the cen-
   Our first experiments were intended to determine              tral assumptions of traditional transformational gram-
whether the algorithm could determine the correct form           mar. Now, of course, given our knowledge of many differ-
of a polar question when the noun phrase had a rela-             ent formalisms which can correctly generate these forms
tive clause, even when the algorithm was not exposed to          without movement we can see that this assumption is
any examples of that sort of sentence. We accordingly            false. There is of course a relation between these two
prepared a small data set shown in Table 1. Above the            sentences, a semantic one, but this does not imply that
line is the training data that the algorithm was trained         there need be any particular syntactic relation, and cer-
on. It was then tested on all of the sentences, including        tainly not a “generative” relation.
the ones below the line. By construction the algorithm              Secondly, the view of learning algorithms is very nar-
would generate all sentences it has already seen, so it          row. It is considered that only sentences of that exact
scores correctly on those. The learned grammar also cor-         type could be relevant. We have demonstrated, if noth-
rectly generated the correct form and did not generate           ing else, that that view is false. The distinction can be
the final form.                                                  learnt from a set of data that does not include any ex-
   We can see how this happens quite easily since the            ample of the exact piece of data required: as long as the
simple nature of the algorithm allows a straightforward          various parts can be learned separately, the combination
analysis. We can see that in the learned grammar “the            will function in the natural way.
man” will be congruent to “the man who is hungry”,                  A more interesting question is the extent to which the
since there is a pair of sentences which differ only by          biases implicit in the learning algorithm are domain spe-
this. Similarly, “hungry” will be congruent to “order-           cific. Clearly the algorithm has a strong bias. One of
ing dinner”. Thus the sentence “is the man hungry ?”             the advantages of the algorithm for the purposes of this
which is in the language, will be congruent to the correct       paper is that its triviality allows a remarkably clear and
sentence.                                                        explicit statement of its bias. But is this bias specific
   Our second data set is shown in Table 2, and is a             to the domain of language? It in no way refers to any-
fragment of the English auxiliary system. This has also          thing specific to the field of language, still less specific
been claimed to be evidence in favour of nativism. This          to human language – no references to parts of speech,
was discussed in some detail by (Pilato & Berwick, 1985).        or phrases, or even hierarchical phrase structure. It is
                                                            1131

now widely recognised that this sort of recursive struc-                S. Jain, H. U. Simon, & E. Tomita (Eds.), Pro-
ture is domain-general (Jackendoff & Pinker, 2005). It                  ceedings of the 16th international conference on al-
is even possible, at some computational cost, to define                 gorithmic learning theory (p. 283-296). Springer-
this model without using a grammar at all, but merely                   Verlag.
using the congruence directly in what is called a Thue            Crain, S., & Nakayama, M. (1987). Structure depen-
system.                                                                 dence in grammar formation. Language, 63 (522-
   We have selected for this demonstration an algorithm
                                                                        543).
from grammatical inference. A number of statistical
models have been proposed over the last few years by             de la Higuera, C. (1997). Characteristic sets for poly-
researchers such as (Klein & Manning, 2002, 2004) and                   nomial grammatical inference. Machine Learn-
(Solan, Horn, Ruppin, & Edelman, 2005). These mod-                      ing(27), 125-138. (Kluwer Academic Publishers.
els impressively manage to extract significant structure                Manufactured in Netherland)
from raw data. However, for our purposes, neither of             Gold, E. M. (1967). Language indentification in the
these models is suitable. Klein and Manning’s model                     limit. Information and control, 10 (5), 447 - 474.
uses a variety of different cues, which combine with some        Harris, Z. (1954). Distributional structure. Word, 10 (2-
specific initialisation and smoothing, and an explicit con-             3), 146-62.
straint to produce binary branching trees. Though very           Jackendoff, R., & Pinker, S. (2005). The nature of the
impressive, the model is replete with domain-specific bi-               language faculty and its implications for the evo-
ases and assumptions. The model by Solan et al. would
                                                                        lution of language. Cognition, 97, 211-225.
be more suitable for this task, but again the complex-
ity of the algorithm, which has numerous components               Kam, X. N. C., Stoyneshka, I., Tornyova, L., Fodor,
and heuristics, and the lack of a theoretical justifica-                J. D., & Sakas, W. G. (2005, April). Non-
tion for these heuristics again makes the task of identify-             robustness of syntax acquisition from n-grams: A
ing exactly what these biases are, and more importantly                 cross-linguistic perspective. In The 18th annual
how domain specific they are, a very significant problem.               cuny sentence processing conference.
Since both of these approaches attempt to recover con-            Klein, D., & Manning, C. (2004). Corpus-based induc-
stituent structures, it is not clear that they will be able             tion of syntactic structure: Models of dependency
to learn these sorts of phenomena, which require either                 and constituency. In Proceedings of the 42nd an-
composition of partial constituents, or movement, to be                 nual meeting of the acl.
able to generate the correct examples.                            Klein, D., & Manning, C. D. (2002). A generative
   A more powerful variant of this algorithm can be de-
                                                                        constituent-context model for improved grammar
fined that is sensitive to the statistical properties of the
sample, and under reasonable assumptions can be proven                  induction. In Proceedings of the ACL.
to learn a larger class of languages, but the complexity of       Pilato, S. F., & Berwick, R. C. (1985). Reversible au-
the analysis makes it unsuitable for this demonstration.                tomata and induction of the english auxiliary sys-
                                                                        tem. In Proceedings of the ACL (p. 70-75).
                        Conclusion                                Pullum, G. K., & Scholz, B. C. (2002). Empirical as-
We have presented an analysis of the argument that the                  sessment of stimulus poverty arguments. The Lin-
acquisition of auxiliary fronting in polar interrogatives               guistic Review, 19 (1-2), 9–50.
supports linguistic nativism. We have shown how a very            Reali, F., & Christiansen, M. H. (2004). Structure de-
simple algorithm based on the ideas of Zellig Harris,                   pendence in language acquisition: Uncovering the
can explain this problem with a simple domain-general                   statistical richness of the stimulus. In Proceedings
heuristic. we show that the empirical question as to the                of the 26th annual conference of the cognitive sci-
frequency of occurrence of polar questions of a certain
                                                                        ence society. Mahwah, NJ: Lawrence Erlbaum.
type in child-directed speech is irrelevant, since the dis-
tinction in question can be learned even when no such             Smith, N. A., & Eisner, J. (2005, June). Contrastive es-
sentences are observed.                                                 timation: Training log-linear models on unlabeled
                                                                        data. In Proceedings of the 43rd annual meeting of
                   Acknowledgments                                      the ACL (pp. 354–362). Ann Arbor, Michigan.
This work has been partially supported by the EU                  Solan, Z., Horn, D., Ruppin, E., & Edelman, S. (2005).
funded PASCAL Network of Excellence on Pattern Anal-                    Unsupervised learning of natural languages. Proc.
ysis, Statistical Modelling and Computational Learning.                 Natl. Acad. Sci., 102, 11629-11634.
                                                                  Yokomori, T. (2003). Polynomial-time identification of
                        References                                      very simple grammars from positive data. Theo-
Angluin, D. (1982). Inference of reversible languages.                  retical Computer Science, 298 (1), 179-206.
       Communications of the ACM, 29, 741-765.                    Zaanen, M. van. (2000). ABL: Alignment-based learn-
Chomsky, N. (1975). The logical structure of linguistic                 ing. In Proceedings of COLING.
       theory. University of Chicago Press.
Clark, A., & Eyraud, R. (2005). Identification in the
       limit of substitutable context free languages. In
                                                             1132

