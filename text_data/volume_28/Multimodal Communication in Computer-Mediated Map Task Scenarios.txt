UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Multimodal Communication in Computer-Mediated Map Task Scenarios
Permalink
https://escholarship.org/uc/item/3707z7hz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Hoque, Mohammed E.
Jeuniaux, Patrick
Lewis, Gwenyth
et al.
Publication Date
2006-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

          Multimodal Communication in Computer-Mediated Map Task Scenarios
                                       Max M. Louwerse (mlouwerse@ memphis.edu)a
                                          Patrick Jeuniaux (pjeuniau@memphis.edu)a
                                       Mohammed E. Hoque (mhoque@memphis.edu)c
                                             Jie Wu (wjie@mail.psyc.memphis.edu)b
                                      Gwyneth Lewis (glewis@mail.psyc.memphis.edu)a
                                     Department of Psychology / Institute for Intelligent Systemsa
                                 Department of Computer Science / Institute for Intelligent Systemsb
                      Department of Electrical and Computer Engineering / Institute for Intelligent Systemsc
                                                         Memphis, TN 38152 USA
                             Abstract                                    First, from a psychological point of view it helps us
                                                                       understand how communicative processes take shape in the
   Multimodal communication involves the co-occurrence of              minds of dialog participants. Under what psychological
   different communicative channels, including speech, eye gaze        conditions are different channels aligned? Does a channel
   and facial expressions. The questions addressed in this study
                                                                       add information to the communicative process or does it
   are how these modalities correlate and how they are aligned to
   the discourse structure. The study focuses on a map task            merely co-occur with other channels? Research in
   scenario whereby participants coordinate a route on a map,          psychology has shed light on the interaction of modalities,
   while their speech, eye gaze, face and torso are recorded.          for instance comparing eye gaze (Argyle & Cook, 1976;
   Results show that eye gaze, facial expression and pauses            Doherty-Sneddon, et al. 1997), gestures (Goldin-Meadow,
   correlate at certain points in the discourse and that these         2003; Louwerse & Bangerter, 2005; McNeill, 1992) and
   points can be identified by the speaker’s intentions behind the     facial expressions (Ekman, 1979) but many questions
   dialog moves. This study thereby sheds light on multimodal          regarding multiple – i.e., more than pairs of – channels and
   communication in humans and gives guidelines for                    their alignment remain unanswered.
   implementation in animated conversational agents.
                                                                         Second, insight in multimodal communication is
                                                                       beneficial from a computational point of view, for instance
                          Introduction                                 in the development of animated conversational agents
Most communicative processes require multiple channels,                (Louwerse, Graesser, Lu, & Mitchell, 2004). The
both linguistic and paralinguistic (Clark, 1996). For                  naturalness of the human-computer interaction can be
instance, we talk on the phone while gesturing, we seek eye            maximized by the use of animated conversational agents,
contact when we want to speak, we maintain eye contact to              because of the availability of both linguistic (semantics,
ensure that the dialog participant comprehends us, and we              syntax) and paralinguistic (pragmatic, sociological) features.
express our emotional and cognitive states through facial              These animated agents have anthropomorphic, automated,
expressions. These different communicative channels play               talking heads with facial features and gestures that are
an important role in the interpretation of an utterance by the         coordinated with text-to-speech-engines (Cassell &
dialog partner. For instance, the interpretation of “Are you           Thórisson, 1999; Massaro & Cohen, 1994; Picard, 1997).
hungry?” depends on the context (e.g. just before going to a           Examples of these agents are Baldi (Massaro & Cohen,
restaurant, during dinner), depends on eye gaze (staring               1994), COSMO (Lester, Stone & Stelling, 1999), STEVE
somebody in the eyes or looking away), depends on prosody              (Rickel & Johnson, 1999), Herman the Bug (Lester, Stone,
(e.g. stress on ‘you’ or ‘hungry’), facial expressions (e.g.           Stelling, 1999) and AutoTutor (Graesser, Person, et al.,
surprised look, disgusted look) and gestures (e.g. rubbing             2001). Though the naturalness of these agents is
stomach, pointing at a restaurant). While multimodal                   progressively changing, there is room for improvement.
communication is easy to comprehend for dialog                         Current agents for instance incessantly stare at the dialog
participants, it is hard to monitor and analyze for                    partner, use limited facial features rather randomly, or
researchers.                                                           produce bursts of unpaused speech. Both psycholinguistics
  Despite the fact that we know linguistic modalities (e.g.            and computational linguistics would thus benefit from
dialog move, intonation, pause) and paralinguistic                     answers to questions regarding the interaction of
modalities (e.g. facial expressions, eye gaze, gestures) co-           multimodal channels.
occur in communication, the exact nature of their interaction            A specific and related question concerns the mapping of
remains unclear (Louwerse, Bard, Steedman, Graesser &                  these channels onto the discourse structure. Research has
Hu, 2004). There are two primary reasons why an insight in             shown that the structure of the dialog can often predict these
the interaction of modalities in the communicative process             modalities. For instance, Taylor, King, Isard, & Wright
is beneficial.                                                         (1998) and Hastie-Wright, Poesio, and Isard (2002) have
                                                                   1717

shown that speech recognition can be improved by taking                                         Method
into account the sequence of dialogue moves (for example,
                                                                    Two dialog partners collaborated in a Map Task scenario
answers following questions) and Flecha-Garcia (2002) has
                                                                    whereby their communication was recorded with
shown that eye brow movements can partly be explained by
                                                                    camcorders, speech recorders and eye trackers. The analysis
linguistic communicative functions related to dialogue
                                                                    reported below focuses on the alignment of cognitive states
structure.
                                                                    as expressed through facial expressions, the eye gaze and
  In this exploratory study, we investigate the mappings of
                                                                    pauses to the dialog structure.
modalities onto the dialog structure as well as their
correlations. The current paper reports on some initial             Participants
findings of a study investigating these questions in computer
mediated discourse between two dialog participants. The             Eight undergraduate students at the University of Memphis
study reported here is part of a series conducted in a project      participated in this study, six females and two males. The
investigating multimodal communication in humans and                participants received extra credit in an undergraduate course
agents.                                                             for participating in this study.
              Intelligent Map Task Agent                            Materials
In an ongoing project on multimodal communication in                Four different maps were created based on the maps used in
humans and agents, we are investigating the interaction             the HCRC Map Task corpus (Anderson, et al., 1991). The
between linguistic modalities, like prosody and dialog              maps used different types of objects (cars, churches, houses,
structure, and non-linguistic modalities, like eye gaze and         trees, lakes) and different colors in order to elicit dialog
facial expressions. The project aims to determine how these         with various contrasts. The average number of object types
modalities are aligned, whether, and if so, when these              was 7.8 (SD = 3.98) and 53 (SD = 5.79) object tokens. For
modalities are observed by the interlocutor and whether the         the analysis reported below, the four maps were considered
correct use of these channels actually aids the interlocutor’s      random variables.
comprehension. Answers to these questions should provide
a better understanding of the use of communicative                  Apparatus
resources in discourse and can subsequently aid the                 Participants’ communication was recorded by five
development of more effective animated conversational               Panasonic camcorders, two capturing the faces of each
agents.                                                             dialog participant (PV-GS31), two capturing the upper
    With so many variables in multimodal communication,             torsos of each participant (PV-GS150) and one capturing
it is desirable to control for genre, topic, and goals of           both participants from a bird’s eye view (PV-GS150). Eye
unscripted dialogs. We therefore used the Map Task                  gaze was recorded for the Giver only using an SMI iView
scenario (Anderson, et al., 1991). The Map Task is a                RED remote eye tracker. Speech was recorded using a
restricted-domain route-communication task which makes              Marantz PMD670 recorder whereby Giver and Follower
clear to experimenters exactly what each participant knows          were recorded on two separate (left and right) channels
at any given time. The Instruction Giver (Giver) coaches the        using two AKG C420 headset microphones.
Instruction Follower (Follower) through a route on the map.
By way of instructions, participants are told that they and
                                                                    Procedure
their interlocutors have maps of the same location but drawn        Participants were seated adjacent to each other, separated by
by different explorers and so potentially different in detail.      a divider to ensure that they could not see each other. They
They are not told where or how the maps differ. The maps            communicated through microphones and headphones, while
are of fictional locations and participants have only three         they could see the upper torso of the dialog partner and the
sources of knowledge in their initial encounter with a map:         map both on a computer monitor in front of them.
1) the instructions, 2) what appears on his/her map (cartoon        Participants were randomly assigned to a Giver or Follower
landmarks, their labels, an in the case of the giver, the           role.
location of the route) and 3) what has been expressed during          Before the conversation started, all equipment was
the map task dialogue, in terms of language, speech, eye            calibrated. The five camcorders were positioned and
contact, facial expressions, gestures. The route on the             focused in order to best capture facial expressions and upper
Giver’s map is designed, however, to maintain not only the          torso. The eye gaze of the Information Giver was calibrated
alternation of matching and mismatching landmarks, but              using nine calibration points on the screen. To avoid
also an ‘easy stages’ rule: the next landmark critical to the       interruption of the dialog, calibration only occurred once per
route will almost always be one of the landmarks closest to         map. If calibration was lost in the experiment, unbeknownst
the current landmark.                                               to the participant recording of eye tracking data was
                                                                    discontinued. The experiment started with a flash of light, in
                                                                    order to ensure alignment of the different channels for the
                                                                    data analysis.
                                                               1718

                                                                      All modalities were aligned at 250 milliseconds accurate
                                                                    time stamp. Two types of analyses were conducted: 1) a
                                                                    correlation analysis between the different modalities of
                                                                    cognitive state, eye gaze, eye blinks and pause; 2) an
                                                                    analysis of difference between dialog moves for each of the
                                                                    modalities variables.
                                                                    Dialog moves
                                                                    Three students in the Institute for Intelligent Systems
                                                                    transcribed the audio of the interactions between
                                                                    Information Giver and Information Follower. Next they
                                                                    classified and time-stamped (250ms intervals) these
                                                                    utterances in the 12 conversational game moves described
                                                                    by Carletta et al. (1997). An overview of these dialog moves
                                                                    is presented in Table 1. Kappas between the coders were
                                                                    acceptable at .53, higher than Kappa scores of .39 between
                                                                    human coders obtained in a different study (Louwerse &
                                                                    Crossley, 2006).
                                                                    Eye gaze
                                                                    Two dependent measures were recorded for the Giver’s eye
                                                                    gaze. Total fixation times were computed for the two areas
                                                                    of interest, the Follower and the Giver’s map. In addition,
                                                                    number of blinks was computed.
                                                                    Pause
                  Figure 1. Example of map                          Pause was analyzed using the upper intensity limit and
                                                                    duration. In measurement of intensity, minimum pitch
  The Giver was presented with a map in color similar to the        specifies the minimum periodicity frequency in any signal.
one presented in Figure 1, with a route drawn on it. The            In our case, 100 Hz for minimum pitch yielded a sharp
Follower had a slightly different map without the route             contour for the intensity. Audio frames with intensity values
drawn on it. Followers were able to draw a route on the map         consistently less than 50 dB and for the duration of longer
using a stylus or a mouse.                                          than .5 seconds were classified as pauses. The 50 dB for
                                                                    intensity threshold was chosen to be the optimal value based
                Results and Discussion                              on experimentation to capture all the utterances while
The average duration of the four dialogs was 354.5 seconds          rejecting the absolute silence during the interaction. The
(SD = 81.51). These durations are comparable with the               speech processing software Praat (Boersma & Weenink,
average duration of Map Task dialogs, 407.75 seconds                2006) was used to perform all the calculations to identify
(Branigan, Lickley, & McKelvie, 1999).                              the pause regions.
       Table 1. The 12 move types used in the Map Task, their frequency in percentages, a description and an example
                      Note: ‘IG’ and ‘IF’ refer to the frequency of dialog moves in Givers and Followers.
Dialog        IG     IF      Description                                               Example
Act
INSTRUCT      38     0       Commands partner to carry out action                      Start at the top and go down between the
                                                                                       blue and the red car.
EXPLAIN       28     8       States information not directly elicited by partner       Ok I went the wrong way.
CHECK         9      12      Requests partner to confirm information                   So, between the black and the grey one?
ALIGN         15     3       Checks attention, readiness, agreement of partner         Ok, do you see those two blue cars?
QUERY-YN 4           8       Yes/no question that is not CHECK or ALIGN                Do you see that?
QUERY-W       7      17      Any query not covered by the other categories             If I'm at the red car what do I do there?
ACKNOWL       14     7       Verbal response minimally showing understanding           Uh huh.
REPLY-Y       11     107 Reply to any yes/no query with yes-response                   Yeah, start at the top.
REPLY-N       4      24      Reply to any yes/no query with no-response                No, go like above the puddle.
REPLY-W       2      1       Reply to any type of query other than ‘yes or ‘no’        It goes below.
CLARIFY       13     3       Reply to question over and above what was asked           So you'll be between the blue and red car.
READY         28     8       Preparing conversation for new dialog game                Alright. We're going to move to the left.
                                                               1719

Cognitive states                                                              Table 3. Correlations modalities Giver
Cognitive states were coded by three judges using the facial                         Notes: **p < .01; *p < .05
expression video footage. Standard emotion coding                               Looking     Fixation      Fixation    Blink  Pause
schemes, like Ekman, Friesen, Wallace and Hager’s (2002)                         Away           on        on map
facial action coding scheme are problematic for Map Task                                     person
scenarios, because negative cognitive states like disgust,        Distracted      -.065       -.071         -.083      -.075  -.078
anger or sadness do not occur in these interactions. Instead,     Uncertain        .338**      .249*         .168*    .712** .703**
we used a coding scheme inspired by Craig, Graesser,              Confused         .172*        .086        -.027     .538** .443**
Sullins, and Gholson’s (2004) and Kort, Reilly and Picard’s       Confident      -.143*       -.067         -.112      -.103 -.155*
(2001) schemes for affect in learning. Nine cognitive states       Engaged         .163*      .314**       .438**     .893** .676**
were distinguished (Table 2). To account for degrees of          Encouraged        .172*      -.034         .006      .258**  .147*
these cognitive states, the average monothetic ratings of the     Interested      -.081        .006         -.020      -.028  -.097
cognitive states were used in the analysis.                         Bored         -.026       .212**        .062       .023   -.003
                                                                    Pause          .413*      .447**        .659**    .819**
  Table 2. Cognitive states determined by facial expressions
                                                                            Table 4. Correlations modalities Follower
  Cognitive                      Description
                                                                                     Notes: **p < .01; *p < .05
     state
  Distracted      Directing attention away from the task;                                         Looking       Pause
                            broken concentration                                                    away
  Uncertain        Hesitation or doubt; lack of assurance                          Distracted       -.037       -.059
  Confused                 Lack of understanding                                   Uncertain        .153*       -.086
  Frustrated               Annoyance or irritation                                 Confused         .151*        .091
  Confident        Expression of assurance and certainty                           Confident        -.050        .079
   Engaged        Heavy and uninterrupted involvement in                            Engaged         .229**       .095
                                   activity                                      Encouraged           .129       .146
 Encouraged              Inspiration and motivation                                Interested       -.038        .134
  Interested               Expression of attention                                   Bored           .336       -.139
    Bored              Lack of interest in the activity                              Pause           .025
Correlations between modalities                                   While the correlations for looking away and cognitive states
Cognitive states, eye gaze and pauses were compared within        are similar between Giver and Follower, correlations
both Giver and Follower, except for eye gaze, which was           between pause and cognitive states differ considerably. The
only recorded for the Giver.                                      most likely explanation for this is the difference in average
  Correlations between these modalities are presented in          length of an utterance between Givers and Followers. A
Table 3 (Giver) and 4 (Follower). The Givers’ eye gaze on         Giver’s turn is on average 5.87 seconds, while for Followers
the Follower correlated significantly with the cognitive          the average lies at .88 seconds. It is therefore important to
states of Engagement, Uncertainty and Boredom. In fact,           normalize the utterances for length. This is what was done
Engagement and Uncertainty also correlated with the               in the next analysis.
fixations on the map. Givers heavily involved in the task
seemed to pay more attention to both the dialog partner and       Differences between dialog moves
the map in front of them, either because they are absorbed in     To allow for a comparison between dialog moves, the
the task or because they are uncertain about an aspect of that    duration of the move must be taken into account. For
task. The same patterns can also be found for the Follower.       instance, the INSTRUCT move that describes an action to the
Though the Follower’s eye gaze was not recorded, the              dialog partner necessarily takes more time than a REPLY-Y
coding for the Follower moving their eyes away from the           move that simply states “yes”. The likelihoods of pausing in
map gives an adequate approximation of fixation on the            speech, of changing eye gaze or to express cognitive states
Giver. Again, Uncertainty and Engagement are the cognitive        are therefore necessarily more frequent in longer moves.
states during which this happens most frequently.                 The dependent variables were therefore normalized by the
    Blinks co-occur with many of the cognitive states. It is      duration of the dialog move.
noteworthy that, in addition to cognitive states like               Main effects for dialog move were found in eye gaze of
Engagement and Uncertainty, they correlate with the               the Giver on the Follower, a 500 ms. pause as well as the
cognitive state of Confusion. On the contrary, when Givers        cognitive states for Confused, Engaged, Encouraged and
feel confident, they pause less and look less at Follower, as     Distracted for the Giver and Confused, Confident,
suggested by the significant negative correlations.               Distracted and Interested for the Follower. No effects were
                                                                  found between dialog moves for the cognitive states for
                                                                  Bored or Uncertain.
                                                              1720

                                              Table 5. Main effects for dialog move.
                                                    Notes: **p < .01; *p < .05
                           Dependent            Role          F       MSe          Salient dialog move
                             variable
                        Gaze on follower        Giver       1.89*     .02            REPLY-Y, ALIGN
                         2500 ms Pause          Giver       1.83*     .01            ACKNOWLEDGE
                            Confident         Follower     2.39**     .02     CHECK, QUERY-YN, REPLY-Y
                            Confused            Giver      13.62**    .01               QUERY-W
                            Confused          Follower      3.26**    .02                REPLY-W
                            Distracted          Giver       3.65**    .01               QUERY-W
                            Distracted        Follower      1.98**    .01                 READY
                           Encouraged           Giver       2.93**    .02               INSTRUCT
                             Engaged            Giver       2.89**    .02               INSTRUCT
                            Interested        Follower      2.70**    .01           CHECK, QUERY-YN
  An overview of these main effects is given in Table 5, as        What are the effects of modalities not discussed here, like
well as the salient dialog moves significant from the other        eye brow movement, intonation contours or gestures? What
dialog moves in a Bonferroni post-hoc test. These post-hoc         are the best coding schemes for the various modalities in
tests revealed the exact nature of the differences between the     specific tasks, like the Map Task? What are the differences
dialog moves. The differences for Giver’s eye gaze on the          in our findings between computer-mediated interaction and
follower were primarily due to the REPLY-Y and the ALIGN           direct face-to-face interaction?
move. Apparently Givers look more at the Followers when               Although studies investigating multimodal communication
they confirm a YN-QUESTION or – not surprisingly – when            generate many research questions and are far from easy to
they check the attention, readiness and agreement of the           conduct, they also provide a wealth of information on how
dialog partner.                                                    humans communicate. The present study has provided an
  The expression of Distraction in the Giver’s cognitive           insight into how eye gaze, facial expression and pauses
states can particularly be found in QUERY-W. This may be           correlate with each other at certain points in the discourse
explained by the Giver moving attention away from the              and that these points can be identified by dialog moves.
conversation to formulate a question. The QUERY-W move             From a psycholinguistic perspective, using dialog moves as
differed from the other dialog moves in that it had higher         the unit of analysis helps to align the various modalities
frequencies of the cognitive state Confusion. This effect was      across various channels. From a computational linguistic
both found in Givers and Followers. In a QUERY-W move              perspective, using dialog moves helps to generate these
the speaker asks for information that goes beyond a request        modalities, which can in turn be studied for their alignment
for a confirmation from the dialog partner. It is therefore not    with discourse structure. Interdisciplinary research will
surprising that the QUERY-W move is frequent in a                  bring us progressively closer to answers to questions that
cognitive state of confusion. On the other hand, where the         dialog participants generally do not consider, because for
speaker has the information to provide to the dialog partner,      them multimodal communication is so natural and
as is the case in the INSTRUCT dialog move, the opposite is        seemingly easy.
true: the Giver expresses cognitive states of Encouragement
and Engagement. Similarly, the Follower merely needs a                                 Acknowledgments
confirmation of information the speaker has and expresses           This research was supported by grant NSF-IIS-0416128.
cognitive states of Confidence in dialog moves like CHECK,          Any      opinions,     findings,   and    conclusions    or
QUERY-YN, or confirms this information as in REPLY-Y.               recommendations expressed in this material are those of the
Finally, ACKNOWLEDGMENT moves likely mark the higher                authors and do not necessarily reflect the views of the
frequency of pauses in the speaker in the line of the partial       funding institution. We would like to thank Ellen Bard and
acknowledgement, whereby the speaker has some                       Markus Guhe for the creation of the maps and their help in
hesitations, as described in Louwerse and Mitchell (2004).          setting up the study, and Dominique Crocitto, Fatema Julia,
                                                                    Fang Yang and Megan Zirnstein for their help in the data
                        Conclusion                                  collection and analyses.
The study reported in this paper is the first in a series
investigating multimodal communication in humans and
agents. This study is very much exploratory in nature, but it
allows us to have a closer look at the complex interplay
between different modalities in conversation. Studies like
this elicit many research questions. For instance, what is the
effect of the map on multimodal communication? What role
do individual differences (personalities, sex, culture) play?
                                                               1721

                        References                               Goldin-Meadow, S. (2003). Hearing gesture: How our
                                                                   hands help us think. Cambridge, MA: Harvard University
Anderson, A., Bader, M., Bard, E., Boyle, E., Doherty, G.
                                                                   Press
  M., Garrod, S., Isard, S., Kowtko, J., McAllister, J.,
                                                                 Hastie-Wright, H., Poesio, M., & Isard, S. (2002).
  Miller, J., Sotillo, C., Thompson, H. S. & Weinert, R.
  (1991). The HCRC Map Task Corpus. Language and                   Automatically predicting dialogue structure using
  Speech, 34, pp. 351-366.                                         prosodic features. Speech-Communication, 36, 63-79.
Argyle, M., & Cook, M. (1976). Gaze and mutual gaze.             Kort, B., Reilly, R., & Picard, R.W. (2001). An Affective
                                                                   Model of Interplay Between Emotions and Learning:
  Cambridge: Cambridge University Press.
                                                                   Reengineering       Educational    Pedagogy-Building    a
Boersma, P. & Weenink, D. (2006). Praat: doing phonetics
                                                                   Learning Companion. In Proceedings of International
  by computer (Version 4.4.06) [Computer program].                 Conference on Advanced Learning Technologies (ICALT
  Retrieved January 30, 2006, from http://www.praat.org/           2001), Madison Wisconsin, August 2001.
Branigan, H. Lickley, R., & McKelvie, D. (1999). Non-            Lester, J., Stone, B., & Stelling, G. (1999). Lifelike
  Linguistic Influences on Rates of Disfluency in                  pedagogical agents for mixed initiative problem solving
  Spontaneous Speech. Proceedings of the 14th                      in constructive learning environments. User Modeling
  International Conference of Phonetic Sciences, pp. 387–          User-Adapted Interaction, 9, 1-44.
  390.                                                           Louwerse, M.M., Bangerter, A. (2005). Focusing attention
Carletta, J., Isard, A., Isard, S., Kowtko, J., Doherty-           with deictic gestures and linguistic expressions.
  Sneddon, G., & Anderson, A. (1997). The reliability of a         Proceedings of the 27th Annual Meeting of the Cognitive
  dialogue structure coding scheme. Computational                  Science Society.
  Linguistics, 23, 13-31.                                        Louwerse, M.M., Bard, E.G., Steedman, M., Hu, X., &
Cassell, J. & Thórisson, K. R. (1999). The power of a nod          Graesser,      A.C.     (2004).    Tracking    multimodal
  and a glance: Envelope vs. emotional feedback in                 communication in humans and agents. Technical report,
  animated conversational agents. Applied Artificial               Institute for Intelligent Systems, University of Memphis,
  Intelligence, 13, 519-538.                                       Memphis, TN.
Clark, H. H. (1996). Using language. Cambridge:                  Louwerse, M.M. & Crossley, S. (2006). Dialog act
  Cambridge University Press.                                      classification using n-gram algorithms. In Proceedings of
Craig, S. D., Graesser, A. C., Sullins, J., & Gholson, B.          the Florida Artificial Intelligence Research Society
  (2004). Affect and learning: An exploratory look into the        International Conference (FLAIRS). Menlo Park, CA:
  role of affect in learning with AutoTutor. Journal of            AAAI Press.
  Educational Media, 29, 241-250.                                Louwerse, M.M., Graesser, A.C., Lu, S., Mitchell, H. H.
Doherty-Sneddon, G., Anderson, A. H., O'Malley, C.,                (2005). Social cues in animated conversational agents.
  Langton, S., Garrod, S., & Bruce, V. (1997). Face-to-face        Applied Cognitive Psychology, 19, 1-12.
  and video-mediated communication: A comparison of              Louwerse, M.M., & Mitchell, H.H. (2003). Towards a
  dialogue structure and task performance. Journal of              taxonomy of a set of discourse markers in dialog: a
  Experimental Psychology: Applied, 3, 105-125.                    theoretical and computational linguistic account.
Ekman, P. (1979). About brows: Emotional and                       Discourse Processes, 35, 199-239.
  conversational signals. In M. von Cranach, K. Froppa,          Massaro, D. W., & Cohen, M. M. (1994). Visual,
  W. Lepenies, & D. Ploog (Eds.), Human ethology: Claims           orthographic, phonological, and lexical influences in
  and limits of a new discipline: Contributions to the             reading. Journal of Experimental Psychology: Human
  colloquium (pp. 169-248). Cambridge: Cambridge                   Perception and Performance, 20, 1107- 1128.
                                                                 McNeill, D. (1992). Hand and Mind: What Gestures Reveal
  University Press.
                                                                   about Thought. Chicago: University of Chicago Press.
Ekman, P., Friesen, Wallace V., & Hager, J.C. (2002).
                                                                 Picard, R. (1997). Affective computing. Cambridge, MA:
  Facial Action Coding System (FACS). CD-ROM.                      MIT Press.
Flecha-Garcia, M.L. (2002). Eyebrow raising and                  Rickel,J. & Johnson, W.L. (1999). Animated agents for
  communication in map task dialogues. Gesture: The                procedural training in virtual reality: Perception,
  Living Medium. University of Texas at Austin.                    cognition, and motor control. Applied Artificial
Graesser, A., Person, N., Harter D., & the Tutoring                Intelligence, 13, 343-382.
  Research Group. (2001). Teaching tactics and dialog in         Taylor, P., King, S., Isard, S., & Wright, H. (1998).
  AutoTutor. International Journal of Artificial Intelligence      Intonation and dialogue context as constraints for speech
  in Education, 12, 23-39.                                         recognition. Language and Speech, 41, 493-512.
                                                             1722

