UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Locally Bayesian Learning
Permalink
https://escholarship.org/uc/item/94f8z0vp
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Author
Kruschke, John K.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                                 Locally Bayesian Learning
                                             John K. Kruschke (kruschke@indiana.edu)
                                   Department of Psychological and Brain Sciences; Indiana University
                                                         Bloomington IN 47405 USA
                               Abstract                                      The system starts with some prior distribution of belief
                                                                          over the joint hypotheses, p(Î¸L , . . . , Î¸1 ). That distribution is
   This article is concerned with trial-by-trial, online learning         updated each time that an input-output datum is experienced.
   of cue-outcome mappings. In models structured as succes-
   sions of component functions, an external target can be back-          For input x1 , suppose that the correct outcome, as observed
   propagated such that the lower layerâ€™s target is the input to          in the environment, is tL . Bayesâ€™ theorem indicates that the
   the higher layer that maximizes the probability of the higher          appropriate beliefs after witnessing the item htL , x1 i are
   layerâ€™s target. Each layer then does locally Bayesian learning.
   The resulting parameter updating is not globally Bayesian, but
   can better capture human behavior. The approach is imple-                   p(Î¸L , . . . , Î¸1 |tL , x1 )
   mented for an associative learning model that first maps inputs
   to attentionally filtered inputs, and then maps attentionally fil-                                p(tL |Î¸L , . . . , Î¸1 , x1 ) p(Î¸L , . . . , Î¸1 )
                                                                                  = R                                                                       (1)
   tered inputs to outputs. The model is applied to the human-                               dÎ¸L . . . dÎ¸1 p(tL |Î¸L , . . . , Î¸1 , x1 ) p(Î¸L , . . . , Î¸1 )
   learning phenomenon called highlighting, which is challeng-
   ing to other extant Bayesian models, including the rational
   model of Anderson, the Kalman filter model of Dayan and                The probability of the outcome given the input,
   Kakade et al., the noisy-OR model of Tenenbaum and Grif-               p(tL |Î¸L , . . . , Î¸1 , x1 ), is determined by the particular functions
   fiths et al., and the sigmoid-belief networks of Courville et
   al. Further details and applications are provided by Kruschke          in each layer. The updating of the belief distribution over
   (in press); the present article reports new simulations of the         the joint parameter space is referred to as globally Bayesian
   Kalman filter and rational model.                                      learning.
        Cognition Modeled as a Succession of
                         Transformations
                                                                                              Locally Bayesian Learning
Cognitive models are often conceived to be successions of
transformations from an input representation, through vari-               An alternative approach comes from considering the local en-
ous internal representations, to an output or response repre-             vironment of each layer. Each layer only has contact with its
sentation. Each transformation is a formal operation, typi-               own input and output. If a layer had a specific target and in-
cally having various parameter values that are tuned by expe-             put, then the layer could apply Bayesian updating to its own
rience. A well-know example is Marrâ€™s (1982) modeling of                  parameters, without worrying about the other layers.
vision as a succession from a representation of image inten-                 A local updating scheme proceeds as follows. When an in-
sity to a â€œprimal sketchâ€ to a â€œ2 12 -D sketchâ€ to a 3-D model            put x1 is presented at the bottom layer, the input is propagated
representation.
                Globally Bayesian Learning                                                       yâ„“+1 âˆ¼ p(yâ„“+1 |Î¸â„“+1 , xâ„“+1 )
In Bayesian approaches to cognitive modeling, each transfor-                                       6
mation in the hierarchy takes an input and generates a dis-                                         Î¸â„“+1 âˆ¼ p(Î¸â„“+1 )
tribution of possible outputs. Figure 1 shows the input xâ„“
at layer â„“ being transformed into the output yâ„“ , which has a                                    xâ„“+1
probability distribution p(yâ„“ ). The input at the first layer is
denoted x1 , and the output at the last layer is denoted yL . The                                yâ„“ âˆ¼ p(yâ„“ |Î¸â„“ , xâ„“ )
specifics of the distribution are governed by the values of the                                    6
parameters Î¸â„“ .                                                                                     Î¸â„“ âˆ¼ p(Î¸â„“ )
   Each value of the parameters Î¸â„“ represents a particular hy-
                                                                                                 xâ„“
pothesis about how inputs (stimuli) and outputs (outcomes
or responses) are related. The combinations of all possible
values of Î¸â„“ span the possible beliefs of the model. The core             Figure 1. Architecture of successive functions. Vertical arrows in-
ontological notion in Bayesian approaches is that knowledge               dicate a mapping from input to output within a layer, parameterized
consists of the degree of belief in each possible value of the            by Î¸. The notation â€œÎ¸ âˆ¼ p(Î¸)â€ means that Î¸ is distributed accord-
parameters Î¸â„“ . That distribution of beliefs in each layer is             ing to the probability distribution p(Î¸). In the globally Bayesian
denoted p(Î¸â„“ ).                                                           approach, xâ„“+1 = yâ„“ . In the locally Bayesian approach, xâ„“+1 = yÌ„â„“ .
                                                                      453

up the layers. The input to layer â„“ + 1 is the expected value of
the output of module â„“:                                                                 Outcomes:             E        L
                                                                                                              â†‘        â†‘
                                   Z                                                                   z     }| z { }|       {
                   xâ„“+1 = yÌ„â„“ =       dyâ„“ yâ„“ p(yâ„“ |xâ„“ )         (2)                           Cues:      PE       I       PL
Equation 2 is applied recursively up the sequence of layers,
so every layer has a specific input.                                    Figure 2. Symmetric structure of cue-outcome relations in the
   A target output tL is provided at the final output layer. The        highlighting procedure. Cases of PE.Iâ†’E are trained earlier than
                                                                        cases of I.PLâ†’L, but with equal base rates overall.
belief probabilities for layer â„“ = L are updated according to
Bayes theorem,
                                                                        in which a new cue PL along with old cue I indicate a new
                                       p(tâ„“ |Î¸â„“ , xâ„“ ) p(Î¸â„“ )           outcome L. Figure 2 shows the symmetric structure of the
            p(Î¸â„“ |tâ„“ , xâ„“ )    =   R                            (3)     cue-outcome relations in highlighting. Notice that cue I is
                                     dÎ¸â„“ p(tâ„“ |Î¸â„“ , xâ„“ ) p(Î¸â„“ )
                                                                        an Imperfect predictor because both outcomes E and L can
where xâ„“ = yÌ„â„“âˆ’1 as in Equation 2.                                      occur (on different trials) when I occurs. Cue PE is a Perfect
   Then a target is selected for the next layer down. This tar-         predictor of the Earlier trained outcome E, and cue PL is a
get for the lower layer is the input to the higher layer that           Perfect predictor of the Later trained outcome L.
maximizes the probability of the higher-layer target. In other             If people learn the simple underlying symmetry of the cue-
words, when the â„“th layer has a target vector tâ„“ , we choose the        outcome correspondences, then when they are tested with cue
next lower target as:                                                   I by itself, they should choose outcomes E and L equally of-
                                                                        ten. In fact, there is a strong tendency to choose outcome E.
        tâ„“âˆ’1   = argmax p(tâ„“ |xâˆ—â„“ )                                     This response bias is not a general primacy effect, however,
                           xâˆ—â„“                                          because when people are tested with the pair of cues PE
                                 Z
                                                                        and PL, they prefer outcome L. Apparently, cue PL has been
               = argmax dÎ¸â„“ p(tâ„“ |Î¸â„“ , xâˆ—â„“ ) p(Î¸â„“ |tâ„“ , xâ„“ )    (4)     highlighted during learning I.PLâ†’L, so that cue I is not as-
                           xâˆ—â„“
                                                                        sociated strongly with L. But PL apparently is strongly asso-
Equation 4 simply states that the target for the lower layer is         ciated with PL, even more than PE is associated with E.
the input to the upper layer that would maximize the proba-                Table 1 shows details of a canonical highlighting design.
bility of the upper layerâ€™s target. The variable xâˆ—â„“ is given a su-     The learner first sees trials of cues I and PE indicating out-
perscript star to distinguish it from the input value xâ„“ = yÌ„â„“âˆ’1 .      come E, denoted I.PEâ†’E. One â€œepochâ€ of trials consists of
   The targets can then be propagated down the layers by re-            the items in that phase presented in random order. In the sec-
cursively applying Equations 3 and 4. For each layer, the be-           ond and third phases of training, trials of I.PLâ†’L are inter-
liefs are updated and then a target is determined for the layer         mixed. The canonical highlighting design equalizes the fre-
below.                                                                  quencies of the early and late outcomes. Notice in the table
   An interesting quality of this algorithm is that the target          that when N3 = N2 + N1 , the total number of I.PEâ†’E trials is
received by a lower layer depends not only on the actual exte-          3N1 + 4N2 , which equals the total number of I.PLâ†’L trials.
rior target but also on what the upper layers have learned until        This equality of base rates distinguishes highlighting from
that point in training. (As mentioned before, I am assum-               the â€œinverse base rate effectâ€ reported by Medin and Edel-
ing trial-by-trial, online learning.) The target for the lower          son (1988), which uses only the second phase of Table 1, i.e.,
layer is selected to be maximally consistent with what the up-          N1 = 0 and N3 = 0. The equality of base rates emphasizes that
per layers have already learned. In this way, the upper layer           highlighting is an order-of-learning effect, not a base rate ef-
changes the data to be consistent with its beliefs before the           fect. Simulations described below show that various Bayesian
lower layer changes its beliefs to be consistent with the data.         models of learning predict p(E|I) = p(E|PE.PL) = .5, con-
As a consequence, the system is not globally Bayesian. Nev-             trary to human behavior.
ertheless, simulations below illustrate that this is an important
characteristic for capturing human learning.                            Table 1
                                                                        Canonical highlighting design.
       A Challenging Behavior: Highlighting
                                                                           Phase         # Epochs             Items Ã— Frequency
In typical associative learning experiments, people must learn
which button to press in response to some simple cues pre-
                                                                           First             N1           I.PEâ†’E Ã—2
sented on a computer screen. The cues could be simple
words, such as â€œbrainâ€ and â€œworld.â€ In a learning trial, the
                                                                          Second             N2           I.PEâ†’E Ã—3       I.PLâ†’L Ã—1
cues are presented, the learner presses the button that s/he
thinks is correct, and then the correct response is displayed.
                                                                           Third      N3 = N2 + N1        I.PEâ†’E Ã—1       I.PLâ†’L Ã—3
The learner studies the cues and correct response and then
moves on to the next trial. At first the learner is guessing, but
                                                                            Test                                 PE.PLâ†’? (L)
predictive accuracy improves with training.
   In the highlighting procedure, people are initially trained                                                      Iâ†’? (E)
on cases in which two cues, denoted PE and I, indicate out-             Note: An item is shown in the format, Cuesâ†’Correct Response. In
come E. Later in training, people are also trained on cases             the test phase, typical response tendencies are shown in parentheses.
                                                                    454

   Highlighting has been obtained in many different experi-               The prior over the hidden weight hypotheses is uniform, and
ments using different stimuli, procedures, and cover stories,             the prior over the output weight hypotheses is Gaussian. The
such as fictitious disease diagnosis (Kruschke, 1996; Medin               prior therefore is completely neutral and provides no prefer-
& Edelson, 1988), random word association (Dennis & Kr-                   ential treatment for any cue or outcome.
uschke, 1998; Kruschke, Kappenman, & Hetrick, 2005), and                     The upper row of Figure 4 shows the results after train-
geometric figure association (Fagot, Kruschke, DeÌpy, & Vau-              ing the locally Bayesian model in the highlighting procedure
clair, 1998). Many other published experiments have ob-                   with N1 = 1, N2 = 2 and N3 = 3 in Table 1. The left panel
tained the inverse base rate effect for different relative fre-           simply lists the training items in the order presented. The
quencies and numbers of training blocks (e.g., Juslin, Wen-               right panel shows the choice preference of the model, where it
nerholm, & Winman, 2001; Medin & Bettger, 1991; Shanks,                   can be seen that the model shows a robust highlighting effect:
1992). I have run several (unpublished) experiments in my                 p(E|I) > .5 and p(E|PE.PL) < .5.
lab in which N1 = 0 and N2 = N3 , and in all of these experi-                The panel labeled â€œHidden Weightsâ€ shows that the model
ments robust highlighting has been obtained.                              has shifted all its belief to hypotheses in which cue PL inhibits
                                                                          hidden node I: The dotted line marked with a star, and labeled
      Predictions of Various Bayesian Models                              hidIâ†PL, has all its belief probability loaded over the weight
                 Applied to Highlighting                                  value of âˆ’5. But cue PE does not symmetrically inhibit hid-
The remainder of this brief article shows that several                    den node I: The solid line marked with a diamond, and la-
Bayesian models of learning cannot accommodate the high-                  beled hidIâ†PE, has all its belief probability loaded over the
lighting effect, but a simple locally Bayesian model does.                weight value of 0, not âˆ’5.
There is not space here to discuss several other phenomena                   The panel labeled â€œOutcome Weightsâ€ shows that the
in human learning that are difficult for globally Bayesian                model believes in hypotheses for which there is a positive
models but which can be addressed by a locally Bayesian                   connection from hidden node I to outcome E, but does not
model. These other phenomena, and full details of the lo-                 believe in hypotheses for which there is a negative connec-
cally Bayesian model summarized in the next section, are dis-             tion from hidden node I to outcome E: The line marked with
cussed by Kruschke (in press).                                            a square and labeled Eâ†hidI has marginal belief probability
                                                                          greater than .4 over weight value +5, but has marginal belief
Locally Bayesian Learning                                                 probability close to 0 over weight value âˆ’5. In other words,
An illustrative implementation of the locally Bayesian learn-             the locally Bayesian model has learned to believe in hypothe-
ing scheme is now presented. Figure 3 shows that the model                ses that are not symmetric across cues.
architecture has two layers of associative weights. Input                    The locally Bayesian model learns asymmetric beliefs
nodes correspond with stimulus cues, and output nodes corre-              because of the internal targets it generates while learning
spond to response choices. An essential aspect of the model               I.PLâ†’L. Because it has previously learned that cue I indi-
is that the intermediate (â€œhiddenâ€) nodes represent atten-                cates outcome E, not the currently correct outcome L, the tar-
tionally modulated copies of the corresponding input cues.                get at the hidden layer that is most consistent with the target
The weight from a cue to the corresponding hidden node                    has hidden node I de-activated. The lower layer then learns
is constrained to be positive, but weights from cues to non-              to believe in hypotheses that suppress hidden node I when
corresponding hidden nodes can be zero or negative. This al-              cue PL is present.
lows the network to entertain hypotheses that some cues can
inhibit attention to other cues.                                          Globally Bayesian Learning
   The weights from the hidden nodes to the outcome nodes                 The simplistic implementation of the locally Bayesian model
can have positive, zero, or negative values. Within each layer,           permits the analogous globally Bayesian model to be exactly
a hypothesis is a particular weight matrix, W . The model is              implemented. The globally Bayesian model crosses every
supplied with a large number of hypothetical weight matrices.             hidden-weight matrix with every output-weight matrix to cre-
                                                                          ate a large joint hypothesis space. If the locally Bayesian
                                                                          model has N hid hidden-weight hypotheses and N out output-
             Outcomes i i                yout âˆ¼ p(yout |Wout , xhid )     weight hypotheses, then it has N hid + N out hypotheses alto-
                           M6
                            BB 6         6                             gether. The globally Bayesian model, on the other hand, has
                              B             Wout âˆ¼ p(Wout )              N hid Ã— N out hypotheses. The prior on the joint space is also
                              B         xout
                                                                          just the product of the local priors, so that the marginal priors
                             B                                           on the joint space are identical to the local marginal priors.
       Attended Cues i i                                                     The lower row of Figure 4 reveals that the globally
                           M6
                            B 6        yhid âˆ¼ p(yhid |Whid , xhid )
                             B                                           Bayesian model shows no highlighting effect whatsoever, and
                              B            6                             symmetrically distributes its beliefs. The globally Bayesian
                              B             Whid âˆ¼ p(Whid )              model believes in hypotheses that have cues PE and PL
                             B          xhid                             equally associated with their respective outcomes, and have
                  Cues i i                                                cue I neutrally or equally associated with both outcomes.
                                                                             Interestingly, it turns out that the globally Bayesian model
Figure 3. Architecture for the simple model of associative learning.      learns the training items more slowly than the locally
When locally Bayesian, the input to the outcome layer is the mean         Bayesian model. In other words, accuracy on the training
output of the hidden layer, i.e., xout = yÌ„hid .                          items is better in the locally Bayesian model, throughout
                                                                      455

                                Data entered:                           Outcome Weights                             Hidden Weights                            Overt Behavior
                               [ PE I PL E ]                       1                                           1                                       1
                                  1   1   0   1
                                                                                                                                                      0.9
                                                                                              Marginal P(w)
                                  1   1   0   1
                                  1   1   0   1                   0.8
                                  1   1   0   1                                                                                                       0.8
                                                                                                              0.5
                                  1   1   0   1
                                                  Marginal P(w)
                                  0   1   1   0                                                                                                       0.7
                                  1   1   0   1                   0.6
                                  1   1   0   1
                                                                                                                                      Marginal P(E)
                                  1   1   0   1                                                                                                       0.6
                      LOCAL
                                                                                                               0
                                  0   1   1   0                   0.4                                          âˆ’5       0         5
                                  0   1   1   0                                                                      Weight Value                     0.5
                                  0   1   1   0
                                  0   1   1   0                                                                          hidPEâ†PE                     0.4
                                  1   1   0   1                   0.2                                                    hidPEâ†I
                                  0   1   1   0                                                                                                       0.3
                                  0   1   1   0
                                                                                                                         hidPEâ†PL
                                  0   1   1   0                    0                                                     hidIâ†PE
                                  1   1   0   1
                                                                                                                                                      0.2
                                                                        âˆ’5        0       5                              hidIâ†I
                                  0   1   1   0
                                  0   1   1   0
                                                                             Weight Value                                hidIâ†PL                      0.1
                                  0   1   1   0                                  Eâ†hidPE                                 hidPLâ†PE
                                  1   1   0   1                                  Eâ†hidI                                  hidPLâ†I                       0
                                                                                                                                                            PE.I I.PL I PE.PL
                                                                                 Eâ†hidPL                                 hidPLâ†PL                                 Test Item
                                Data entered:                           Outcome Weights                             Hidden Weights                            Overt Behavior
                               [ PE I PL E ]                       1                                           1                                       1
                                  1   1   0   1
                                                                                                                                                      0.9
                                                                                              Marginal P(w)
                                  1   1   0   1
                                  1   1   0   1                   0.8
                                  1   1   0   1                                                                                                       0.8
                                                                                                              0.5
                                  1   1   0   1
                                                  Marginal P(w)
                                  0   1   1   0                                                                                                       0.7
                                  1   1   0   1                   0.6
                                  1   1   0   1
                                                                                                                                      Marginal P(E)
                                                                                                                                                      0.6
                      GLOBAL
                                  1   1   0   1                                                                0
                                  0   1   1   0                   0.4                                          âˆ’5       0         5
                                  0   1   1   0                                                                      Weight Value                     0.5
                                  0   1   1   0
                                  0   1   1   0                                                                          hidPEâ†PE                     0.4
                                  1   1   0   1                   0.2                                                    hidPEâ†I
                                  0   1   1   0                                                                                                       0.3
                                  0   1   1   0
                                                                                                                         hidPEâ†PL
                                  0   1   1   0                    0                                                     hidIâ†PE
                                  1   1   0   1
                                                                                                                                                      0.2
                                                                        âˆ’5        0       5                              hidIâ†I
                                  0   1   1   0
                                  0   1   1   0
                                                                             Weight Value                                hidIâ†PL                      0.1
                                  0   1   1   0                                  Eâ†hidPE                                 hidPLâ†PE
                                  1   1   0   1                                  Eâ†hidI                                  hidPLâ†I                       0
                                                                                                                                                            PE.I I.PL I PE.PL
                                                                                 Eâ†hidPL                                 hidPLâ†PL                                 Test Item
                     Figure 4. Upper Row: The locally Bayesian model trained in the highlighting procedure (with
                     N1 = 1, N2 = 2, N3 = 3 in Table 1). Lower Row: The globally Bayesian model trained the same.
training. (A hint of this can be seen by comparing the up-                                                     to a cluster and the clusterâ€™s beliefs are updated, the posterior
per and lower rows of Figure 4, but the difference appears                                                     distribution again has the form of a Dirichlet distribution. The
to be weak because accuracies are near asymptote by this                                                       parameters of the posterior distribution are simply the prior
point in training.) One reason for the relative retardation in                                                 parameters incremented by 1 wherever a feature was present
the global model is that the global model retains some belief                                                  in the added stimulus.
distributed over many candidate hypotheses, and this dilutes                                                      When a stimulus is presented to the rational model, the
performance. A more detailed discussion can be found in                                                        model computes the probability of each cluster given the
Kruschke (in press).                                                                                           stimulus. One of the candidate clusters is always the novel
                                                                                                               cluster which has a uniform prior. The stimulus is added to
Rational Model                                                                                                 whichever cluster has highest probability. If it is added to the
The rational model of category learning, invented by Ander-                                                    until-then novel cluster, a new novel cluster is recruited for
son (1990), is a Bayesian clustering algorithm. Each cluster                                                   subsequent trials.
represents a distribution of beliefs over candidate probabili-                                                    Predictions regarding missing features are determined by
ties of feature values in that cluster. For example, if one stim-                                              computing, in each cluster, the probabilities of the values
ulus dimension is presence or absence of feature PE, a cluster                                                 of the missing feature, and adding those probabilities across
might have .10 belief that the probability of PE presence is                                                   clusters, weighted by the probability of the cluster given the
.3, and .15 belief that the probability of PE presence is .4, and                                              presented features. This is the normative Bayesian approach:
so forth. The degree of belief in the conjoined features of a                                                  The prediction is the average of the predictions of each hy-
stimulus is simply assumed to be the product of the beliefs in                                                 pothesis, weighted by the degree of belief in each hypothesis.
the individual features.                                                                                          Figure 5 shows the results of applying the rational model
   The belief distributions on each feature are continuous and                                                 to the highlighting procedure with N1 = 1, N2 = 2, N3 = 3
parameterized as Dirichlet distributions. These distributions                                                  in Table 1. The right panel reveals that the model shows no
have one parameter per feature value. A convenient charac-                                                     highlighting effect. The middle panel shows the state of the
teristic of these distributions is that when a stimulus is added                                               cluster nodes at the end of training. The model has recruited
                                                                                              456

                                              Data entered:      Internal Clusters                            Overt Behavior
                                             [ PE I PL E ]      Dirichlet Parameters              1
                                                1   1   0   1        Cluster 1:
                                                1   1   0   1        1 1 12 1                    0.9
                                                1   1   0   1       12 12 1 12
                                                1   1   0   1        Cluster 2:                  0.8
                                                1   1   0   1
                                                                    12 1 1 12
                     Rational Model, c=0.3
                                                0   1   1   0                                    0.7
                                                                     1 12 12 1
                                                1   1   0   1
                                                1   1   0   1        Cluster 3:
                                                1   1   0   1         1 1 1 1                    0.6
                                                0   1   1   0         1 1 1 1
                                                0   1   1   0                             P(E)   0.5
                                                0   1   1   0
                                                0   1   1   0                                    0.4
                                                1   1   0   1
                                                0   1   1   0                                    0.3
                                                0   1   1   0
                                                0   1   1   0
                                                1   1   0   1                                    0.2
                                                0   1   1   0
                                                0   1   1   0                                    0.1
                                                0   1   1   0
                                                1   1   0   1                                     0
                                                                                                       PE.I    I.PL       I    PE.PL
                                                                                                                 Test Item
                    Figure 5. Rational model (Anderson, 1990) trained in the highlighting procedure (with N1 = 1,
                    N2 = 2, N3 = 3 in Table 1).
two clusters. One cluster represents all the I.PEâ†’E items,                  added on each trial, but increased uncertainty can be counter-
and the other cluster represents all the I.PLâ†’L items. (The                 acted by longer training.
third cluster is the omnipresent novel cluster.) Because the                    The lower panel of Figure 6 indicates the â€œuncertaintiesâ€
clusters are completely symmetric with respect to the cues,                 on each cue, which are simply the variances (diagonal ele-
the predicted behavior is also.1                                            ments of the covariance matrix) of the Gaussian belief distri-
                                                                            bution. As training progresses, uncertainty decreases, which
                                                                            indicates that beliefs sharpen-up over particular weight val-
The Kalman Filter                                                           ues. The graph indicates that uncertainties are very nearly
The top layer of the simplistic locally Bayesian model is                   symmetric at the end of training.
closely related to a Kalman filter, which was introduced to as-                 The locally Bayesian model extends the Kalman-filter ap-
sociative learning researchers by Sutton (1992) and has been                proach by pre-pending an attentional learning layer. Whereas
used to model some aspects of attention in learning by Dayan,               the Kalman filter learns about the cues in their totality, the
Kakade and collaborators (e.g., Dayan & Kakade, 2001;                       upper layer of the locally Bayesian model learns only about
Dayan, Kakade, & Montague, 2000; Dayan & Yu, 2003;                          attentionally filtered cues at the hidden layer. The attentional
Kakade & Dayan, 2002). In a Kalman filter, continuous-scale                 filtration depends on the temporal order of training items. The
outcomes are computed as a weighted sum of input cues. The                  temporal dependencies of the two models are not incompat-
weighting coefficients have prior distributions defined as mul-             ible; future extensions of the models could incorporate both
tivariate normal. The Kalman filter uses Bayesian updating                  the uncertainty accumulation of the Kalman filter model with
to adjust the probability distribution on the weights (Mein-                the attentional selection of the locally Bayesian model.
hold & Singpurwalla, 1983). Because the model is linear,
                                                                            Other Bayesian Models
the posterior distributions on the weights are also multivari-
ate normal, and the Kalman filter equations elegantly express               Tenenbaum and collaborators (e.g., Sobel, Tenenbaum, &
the posterior mean and covariance as a simple function of the               Gopnik, 2004; Tenenbaum & Griffiths, 2003) have developed
prior mean and covariance. One difference between the mod-                  Bayesian models in which the hypotheses are noisy-OR gates.
els is that the Kalman filter can add uncertainty to the weight             The models handily address some aspects of rapid learning,
distributions on every trial. Because of the accumulation of                but are not able to exhibit highlighting because the models
noise across trials, the Kalman filter can exhibit some trial               have no time dependencies. That is, all that matters to the
order effects. Typically the amount of uncertainty added is a               model is the overall frequency of the training items, not their
constant.                                                                   training order.
   Figure 6 shows the behavior of the Kalman filter when ap-                   Courville and colleagues (Courville, Daw, Gordon, &
plied to highlighting (with N1 = 1, N2 = 2, N3 = 3 in Ta-                   Touretzky, 2004; Courville, Daw, & Touretzky, 2005) con-
ble 1). The format of the figure matches that used in reports               ceptualized both the cues and outcomes as effects to be pre-
by Dayan et al. The top panel of Figure 6 shows the mean                    dicted by latent causes (analogous to the clusters in the ra-
weight (i.e., the mean of the Gaussian distribution of beliefs              tional model). In their approach, a hypothesis is a set of
over possible weight values) on each cue, at the beginning                  weights from latent causes to cues and outcomes, with the
of each epoch of training. The means start unbiased at zero.                    1
                                                                                  Anderson (1990) reported that the rational model can capture
At the end of training, the mean on cue I is nearly zero, and               some aspects of the â€œinverse base rate effect,â€ which is the proce-
the means on cues PE and PL are nearly equal (but opposite)                 dure of Table 1 with N1 = 0 and N3 = 0. The model works in that
magnitude. Therefore, when presented with items I or PE.PL,                 situation because the more frequent cluster has a tighter Dirichlet
                                                                            distribution than the less frequent cluster. But with the equal overall
the model predicts nearly 50-50 outcomes. This behavior can                 frequencies in canonical highlighting, the two clusters have equal
be modulated somewhat by the amount of uncertainty that is                  variances.
                                                                      457

                                                        Kalman Filter (Highlighting N =1, N =2, N =3)
                                                                                       1      2          3
                                                                                                                                             References
                                                       1                                                           Anderson, J. R. (1990). The adaptive character of thought. Hills-
                                                                                                                      dale, NJ: Erlbaum.
                                                                                                                   Courville, A. C., Daw, N. D., Gordon, G. J., & Touretzky, D. S.
      Weight at begin of epoch
                                                                                                                      (2004). Model uncertainty in classical conditioning. In S. Thrun,
                                                      0.5                                                             L. K. Saul, & B. SchoÌˆlkopf (Eds.), Advances in neural informa-
                                                                                                                      tion processing systems (Vol. 16, pp. 977â€“984). Cambridge, MA:
                                                                                                                      MIT Press.
                                                       0                                                           Courville, A. C., Daw, N. D., & Touretzky, D. S. (2005). Similarity
                                                                                                                      and discrimination in classical conditioning: A latent variable
                                                                wI                                                    account. In L. K. Saul, Y. Weiss, & L. Bottou (Eds.), Advances
                                            âˆ’0.5                wPE
                                                                                                                      in neural information processing systems (Vol. 17, p. **). Cam-
                                                                                                                      bridge, MA: MIT Press.
                                                                wPL                                                Dayan, P., & Kakade, S. (2001). Explaining away in weight space.
                                                      âˆ’1                                                              In T. Leen, T. Dietterich, & V. Tresp (Eds.), Advances in neural
                                                            1    2      3      4       5      6          7            information processing systems (Vol. 13, pp. 451â€“457). Cam-
                                                                             Epoch                                    bridge, MA: MIT Press.
                                                                                                                   Dayan, P., Kakade, S., & Montague, P. R. (2000). Learning and
                                                                      U=1.0, V=0.5, W=0.010                           selective attention. Nature Neuroscience, 3, 1218â€“1223.
                                                      1.1
                                                                                                                   Dayan, P., & Yu, A. J. (2003). Uncertainty and learning. IETE (In-
                                                                                                                      stitution of Electronics and Telecommunication Engineers, India)
                     Uncertaintiy at begin of epoch
                                                       1                                          Ïƒ2I                 Journal of Research, 49, 171â€“182.
                                                                                                                   Dennis, S., & Kruschke, J. K. (1998). Shifting attention in cued
                                                                                                  Ïƒ2PE                recall. Australian Journal of Psychology, 50, 131â€“138.
                                                      0.9
                                                                                                  Ïƒ2PL             Fagot, J., Kruschke, J. K., DeÌpy, D., & Vauclair, J. (1998). Associa-
                                                      0.8                                                             tive learning in baboons (papio papio) and humans (homo sapi-
                                                                                                                      ens): species differences in learned attention to visual features.
                                                      0.7                                                             Animal Cognition, 1, 123â€“133.
                                                                                                                   Juslin, P., Wennerholm, P., & Winman, A. (2001). High level rea-
                                                      0.6                                                             soning and base-rate use: Do we need cue competition to explain
                                                                                                                      the inverse base-rate effect? Journal of Experimental Psychol-
                                                      0.5                                                             ogy: Learning, Memory and Cognition, 27, 849â€“871.
                                                      0.4
                                                                                                                   Kakade, S., & Dayan, P. (2002). Acquisition and extinction in au-
                                                            1    2      3      4       5      6          7            toshaping. Psychological Review, 109, 533â€“544.
                                                                             Epoch                                 Kruschke, J. K. (1996). Base rates in category learning. Journal
                                                                                                                      of Experimental Psychology: Learning, Memory and Cognition,
Figure 6. Kalman filter (Dayan et al., 2000) trained in the high-                                                     22, 3â€“26.
lighting procedure (with N1 = 1, N2 = 2, N3 = 3 in Table 1).                                                       Kruschke, J. K. (in press). Locally Bayesian learning with applica-
                                                                                                                      tions to retrospective revaluation and highlighting. Psychological
                                                                                                                      Review.
probability of each effect being determined by a sigmoidal                                                         Kruschke, J. K., Kappenman, E. S., & Hetrick, W. P. (2005). Eye
                                                                                                                      gaze and individual differences consistent with learned attention
function of the summed weights from activated latent causes.                                                          in associative blocking and highlighting. Journal of Experimen-
The hypothesis space consists of many weight combinations,                                                            tal Psychology: Learning, Memory and Cognition, 31, 830â€“845.
and Bayesian learning shifts belief probability among the hy-                                                      Marr, D. (1982). Vision. San Francisco: W. H. Freeman.
                                                                                                                   Medin, D. L., & Bettger, J. G. (1991). Sensitivity to changes in
potheses. Courville et al. (2004) showed that the approach                                                            base-rate information. American Journal of Psychology, 104,
can account for the dependency of conditioned inhibition on                                                           311â€“332.
the number of trials of training, by virtue of the prior prob-                                                     Medin, D. L., & Edelson, S. M. (1988). Problem structure and the
                                                                                                                      use of base-rate information from experience. Journal of Exper-
abilities being gradually overwhelmed by training data. But                                                           imental Psychology: General(117), 68â€“85.
the model would not be able to exhibit highlighting because                                                        Meinhold, R. J., & Singpurwalla, N. D. (1983). Understanding the
it has no time dependencies.                                                                                          Kalman filter. American Statistician, 37(2), 123â€“127.
                                                                                                                   Shanks, D. R. (1992). Connectionist accounts of the inverse base-
                                                                                                                      rate effect in categorization. Connection Science, 4, 3â€“18.
                                                                                                                   Sobel, D. M., Tenenbaum, J. B., & Gopnik, A. (2004). Childrenâ€™s
                                                                      Conclusion                                      causal inferences from indirect evidence: Backwards blocking
                                                                                                                      and Bayesian reasoning in preschoolers. Cognitive Science, 28,
                                                                                                                      303â€“333.
                                                                                                                   Sutton, R. S. (1992). Gain adaptation beats least squares? In Pro-
The locally Bayesian attention model produces highlighting                                                            ceedings of the seventh annual Yale workshop on adaptive and
(and other challenging phenomena) by generating internal                                                              learning systems (p. 161-166). New Haven, CT: Yale University.
target data that depend on current beliefs. When learning a                                                        Tenenbaum, J. B., & Griffiths, T. L. (2003). Theory-based causal
                                                                                                                      inference. In S. Becker, S. Thrun, & K. Obermayer (Eds.), Ad-
cue-outcome correspondence, the model first generates in-                                                             vances in neural information processing systems (Vol. 15, pp.
ternal representations that are maximally consistent with its                                                         35â€“42). Cambridge, MA: MIT Press.
current (upper-layer) beliefs before updating its (lower-layer)
beliefs. Thus, the locally Bayesian model changes the data to
fit its beliefs before changing its beliefs to fit the data. Alas,
people seem to behave that way too.
                                                                Acknowledgments
Supported in part by grant BCS-9910720 from the National
Science Foundation.
                                                                                                             458

