UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Automated Team Discourse Modeling: Test of Performance and Generalization
Permalink
https://escholarship.org/uc/item/30n8g193
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Abdelali, Ahmed
Foltz, Peter W.
Martin, Melanie J.
et al.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

     Automated Team Discourse Modeling: Test of Performance and Generalization
               Peter W. Foltz1,3 (pfoltz@crl.nmsu.edu), Melanie J. Martin2 (mmartin@cs.csustan.edu),
                 Ahmed Abdelali 1(ahmed@crl.nmsu.edu), Mark Rosenstein3 (mbr@pearsonkt.com),
                                            Rob Oberbreckling3 (rjo@pearsonkt.com)
                                1. Computing Research Laboratory and Department of Psychology
                              New Mexico State University, P.O. Box 30001, Las Cruces, NM 88003
                                                 2. Department of Computer Science
                    California State University, Stanislaus, 801 West Monte Vista Avenue, Turlock, CA 95382
                                                3. Pearson Knowledge Technologies
                                       4940 Pearl East Circle, Suite 200, Boulder, CO, 80305
                           Abstract                                    knowledge. The manual analysis of team communication
                                                                       has shown promising results, see for example, Bowers et
   Team communication provides a rich source of discourse,             al. (1998). This analysis, however, is quite costly, where
   which can be analyzed and tied to measures of team                  coding for content can take upwards of 28 hours per 1
   performance. Our goal is to better understand and model             hour of tape (Emmert, 1989) and can be subjective. Thus,
   the relationship between team communication and team                there is a need for techniques to automatically analyze
   performance to improve team process, develop                        team communications to categorize and predict
   collaboration aids, and improve the training of teams. In           performance.
   the present work, we use Latent Semantic Analysis (LSA)                The ultimate goal is to be able to automatically analyze
   for automating the analysis and annotation of team                  a team’s communication and incorporate that analysis into
   discourse. We describe two approaches to modeling team              models that can provide feedback during or after training,
   performance. The first measures the semantic content of a
                                                                       as well as predict performance. This work extends
   team’s dialogue as a whole to predict the team’s
   performance. The second categorizes each team member’s
                                                                       approaches to computational analysis of language, while
   statements using an established set of discourse tags and           also providing techniques to improve modelling of teams
   uses them to predict team performance.           In three           and measuring performance.
   experimental settings we demonstrate the ability of these              A number of AI, statistical and machine learning
   approaches to model performance and generalize to new               techniques have been applied to discourse modeling,
   datasets.                                                           generally for the purpose of improving speech recognition
                                                                       and dialogue systems. However, few have focused
                        Introduction                                   directly on just the content of the discourse of teams.
                                                                       Recent methods include decision trees (Core 1998),
  Evaluating teams of decision-makers in complex                       statistical modelling based on current utterance and
problem-solving environments requires effective models                 discourse history (Chu-Carroll 1998), and hidden Markov
of individual and team performance. Without these                      models. For example, in the work by Stolcke et al.,
models, current methods of assessing team and group                    (2000), they were able to predict the tags assigned to
performance rely largely on global outcome metrics,                    discourse within 15% of the accuracy of trained human
which often lack information rich enough to diagnose                   annotators, while Kiekel et al., (2004) developed markov
failures or suggest improvements in team cognition or                  models of communication patterns among team members
process. Modeling and assessment of teams has been                     that predicted overall performance.
hindered by the fact that the richest source of data, the                 In this work we generate predictive models using
verbal communication among team members, is difficult                  Latent Semantic Analysis (LSA) to measure free-form
to collect and analyze. Prior attempts at annotating the               verbal interactions among team members. Because LSA
content of communication have relied on tedious manual                 can measure and compare the semantic information in
techniques or have only employed limited data such as                  these verbal interactions, it can be used to characterize the
frequencies, pattern analyses and durations of                         quality and quantity of information expressed. LSA
communications. With the advent of NLP, AI and                         analysis can be used to determine the semantic content of
machine-learning techniques that can measure the                       any utterance made by a team member as well as to
semantic content of communication discourse, novel                     measure the semantic similarity of an entire team’s
methods for the analysis and modeling of team                          communication to another team.
communication can be applied.                                             This paper extends research on automated techniques
   A team’s verbal communication data provides a rich                  for analyzing the communication and predicting team
indication of cognitive processing at both the individual              performance using corpora of communication of teams
and the team level. This can be tied back to both the                  performing simulated military missions (see Kiekel et al.,
team’s and each individual team member’s abilities and                 2002; Martin & Foltz, 2004). We focus on two
                                                                       applications of this approach in order to test how well
                                                                1317

these methods generalize to different data sets and              Tasks) Lab's synthetic team task environment (CERTT
parameter conditions. The first application predicts team        UAV-STE) (see http://www.certt.com). CERTT's UAV-
effectiveness based on an analysis of the entire discourse       STE is a three-team-member task in which each team
of the team during a mission. The second application             member is provided with distinct, though overlapping,
predicts categories of discourse for each utterance made         training; has unique, yet interdependent roles; and is
by team members and uses the tags to predict                     presented with different but overlapping information
performance.          Testing generalizability permits           during the mission. The overall goal is to fly the UAV to
characterizations of the robustness of the methods. We           designated target-areas and to take acceptable photos at
look at two levels of generality. At the lower level, we         these areas. To complete the mission, the three-team
consider the range of situations for which a given               members need to share information with one another and
communications model can produce meaningful                      work in a coordinated fashion. Most communication is
predictions. At the higher level, we consider the range of       done via microphones and headsets, although some
team communication scenarios that LSA based techniques           involves computer messaging.
can successfully model. We illustrate the applicability of
                                                                                    Table 1. Corpora Statistics
the approach across a number of datasets. We conclude
with a discussion of how these techniques can account for            Corpus     Transcripts     Teams    Missions   Utterances
the role of communications in teams. Overall we                      AF1        67              11       7          20245
illustrate how NLP, AI, and machine learning techniques              AF3        85              21       7          22418
can be used to automatically model and predict team
performance using realistically sized data sets of team              AF4        85              20       5          22107
communication.                                                      The three corpora are labelled by experiment name:
                                                                 AF1, AF3, and AF4. Each corpus consists of a number of
              Latent Semantic Analysis                           team-at-mission transcripts, where mission duration is
                                                                 approximately 40 minutes. Some statistics are shown in
LSA is a fully automatic corpus-based statistical
                                                                 Table 1. All communication was manually transcribed.
modeling method for extracting and inferring relations of
                                                                   To train LSA, we added 2257 documents to the
expected contextual usage of words in discourse
(Landauer, Foltz and Laham, 1998).                               transcripts of all of the corpora to create a semantic space.
   In LSA a training text is represented as a matrix, where      These documents consisted of training documents and
each row represents a unique word in the text and each           pre- and post-training interviews related to UAVs. Unless
columns represents a text passage or other unit of context.      otherwise noted all results reported were computed using
The entries in this matrix are the frequency of the word in      300 dimensions, although results were robust across a
the context. A singular value decomposition (SVD) of the         range of dimensions.
matrix results in a 100-500 dimensional "semantic space",
where the original words and passages are represented as         Predicting Team Performance
vectors. The meaning of any passage is the average of the
vectors of the words in the passage (Landauer et al.,            Throughout the CERTT UAV-STE experiments an
1997). Words, utterances, and whole documents can then           objective performance measure was calculated to
be compared against each other by computing the cosine           determine each team’s effectiveness at completing the
between the vectors representing any two texts. This             mission. The performance score was a composite of
provides a measure of the semantic similarity of those two       objective measures including: amount of fuel/film used,
texts, even if they do not contain words in common.              number/type of photographic errors, time spent in
   LSA has been used for a wide range of applications and        warning and alarm states, and un-visited waypoints. This
for simulating knowledge representation, discourse and           composite score ranged from -200 to 1000. The score is
psycholinguistic phenomena. These approaches have                highly predictive of how well a team succeeded in
included: information retrieval (Deerwester et al., 1990),       accomplishing their mission. We used two approaches to
automated essay scoring (Landauer et al., 2001), and             predict these overall team performance scores: correlating
automated text analysis (Foltz, 1996). More recently
                                                                 entire mission transcripts with one another and by
Serafin and Di Eugenio (2004) used LSA for dialogue act
                                                                 correlating tag frequencies with the scores.
classification, finding that LSA can effectively be used
for such classification and that adding features to LSA
showed promise. In addition, Martin and Foltz (2004)             Prediction Using Whole Transcripts Our first approach
found that LSA predicted overall team performance                to measuring content in team discourse is to analyze the
scores as well as effectively classified speech acts.            transcript as a whole. Using a k-nearest neighbor method
                                                                 that has been highly successful for scoring essays with
                      Experiment 1                               LSA (Landauer et al., 1998), we used whole transcripts to
                                                                 predict the team performance score. We generate the
Three corpora of team transcripts were collected as a            predicted team performance scores as follows: Given a
result of three different experiments that simulate              subset of transcripts, S, with known performance scores,
operation of an Uninhabited Air Vehicle (UAV) in the             and a transcript, t, with unknown performance score, we
CERTT (Cognitive Engineering Research on Team                    can estimate the performance score for t by computing its
                                                            1318

similarity to each transcript in S. The similarity between        tags in team discourse has been shown to be predictive of
any two transcripts is measured by the cosine between the         team performance.
transcript vectors in the semantic space. To compute the             The 67 transcripts in AF1 were manually coded. Of
estimated score for t, we take the average of the                 these 12 were coded by two humans independently.
performance scores of the 10 closest transcripts in S,            Working within the limitations of the manual annotations,
weighted by cosines. A holdout procedure was used in              we developed an algorithm to code transcripts
which the score for a team’s transcript was predicted             automatically, resulting in some decrease in accuracy, but
based on the transcripts and scores of all other teams (i.e.      at significant savings in time and resources.
a team’s score was only predicted by the similarity to               We established a lower bound for coding performance
other teams). Tests on the AF1 corpus showed that the             of 0.27 by computing the tag frequency in the 12 AF1
LSA estimated performance scores correlated strongly              transcripts coded by two annotators. If all utterances were
with the actual team performance scores (r = 0.76, p <            coded with the most frequent tag, the percentage coded
0.01, r=0.63, p<.01 when correcting for the repeated              correctly would be 27%.
measure structure (see Martin & Foltz, 2004 for
additional details on the approach). Thus, the results            Algorithm for Automatic Annotation We tested our
indicate that we can accurately predict the overall               algorithm to automatically annotate the data, by
performance of the team (i.e. how well they fly and               computing a "corrected tag" for all turns in the 12
complete their mission) just based on our automatic               transcripts coded by two human annotators. This was
analysis of their mission transcripts.                            necessary due to the only moderate agreement between
                                                                  the humans. We used the union of the sets of tags
Generalization of Team Performance scores for                     assigned as the "corrected tag". The union better captures
Different Corpora While the results were successful for           all likely code types within a turn. A disadvantage to
the AF1 corpus, it is important to determine if similar           using “corrected tags” is the loss of sequential tag
results hold for the other two corpora. In addition, it is        information within individual turns. However the focus of
important to determine if the technique can operate               this study was on identifying the existence of relevant
successfully by training the algorithm on the performance         discourse, not on its order within the turn.
scores of one corpus in order to predict performance                 Then, for each of the 12 team-at-mission transcripts, we
scores on another corpus. This approach is equivalent to          automatically assigned "most probable" tags to each turn,
having collected N transcripts from teams flying UAVs             based on the corrected tags of the "most similar" turns in
on a set of particular missions and then trying to predict a      the other 11 transcripts, using Martin and Foltz
new set of teams performing a different set of missions.          (2004)algorithms: LSA and LSA+.
Thus, delimiting the bounds of generalization reveals how            The LSA algorithm uses only LSA to measure semantic
                                                                  similarity to predict the most probable codes, while the
robust such a system could be in more realistic contexts
                                                                  LSA+ algorithm adds two syntactic features (Martin &
where different teams may have to fly entirely novel
                                                                  Foltz 2004). Using LSA+ the performance was only 10%
missions.                                                         and 15% below human-human agreement, depending on
  We tested the generalization for the AF3 set of                 which agreement measure is used. (see Table 2).
transcripts, by training our algorithm on the performance            We used two measures of agreement: C-value
scores of the AF3 experiment and predicting the                   (Schvaneveldt, 1990) measures the proportion of inter-
performance scores from the other experiment (AF4).               coder agreement, without taking into account agreement
Using the 10 closest transcripts, as before, the LSA              by chance and Cohen’s Kappa, does accounts for chance
estimated scores strongly correlated with the actual scores       agreement, as shown in Table 2.
or AF3, showing only a four percent degradation in
performance (r=0.72 to r=0.66). Thus, there was a high                            Table 2. Kappa and C-Values.
level of generalization from one training corpus to
predicting the performance scores of another.                         Annotators-Agreement         C-Value          Kappa
                                                                      Human-Human                       0.70          0.62
                                                                      LSA-Human                         0.59          0.48
Automated Discourse Tagging                                           LSA+-Human                        0.63          0.53
Another approach to utilizing the semantic content of                The results suggest that we can automatically annotate
team dialogues to measure performance is to focus on the          team transcripts with tags. While the approach is not
dialogue in the transcripts at the turn level. We designed        quite as accurate as human coders, LSA is able to tag an
an algorithm to learn from human coded content of the             hour of transcripts in under a minute. As a comparison, it
communication data and then apply the tool to code new            can take half an hour or longer for a trained tagger to do
communication data.                                               the same task.
   We used a tag set developed by Bowers et al. (1998) to
analyze airplane cockpit team communication. The set              Predicting       Performance       from      Tags    While
consists of tags for acknowledgement, action, fact,               demonstrating that the method is able to tag, it is critical
planning,     response,     uncertainty,   and     non-task       to determine the relationship of the type of utterances to
communication. The frequency of the occurrence of these           performance. We computed correlations between the
                                                             1319

team performance score and tag frequencies in each team-           Laham & Foltz, 2001). Using a stepwise regression
at-mission transcript.                                             model, the system built a model that incorporated,
   The tags for all utterances in the 67 AF1 transcripts           typically the 4 to 6 best measures. The correlation of the
were first generated using the LSA+ method. The tag                model’s predicted scores to the SME scores using a Jack-
frequencies for each team-at-mission transcript were then          knife correlation which predicts each score for each
computed by counting the number of times each                      transcript on the basis of the other 63 transcripts is shown
individual tag appeared in the transcript and dividing by          in Table 4.
the total number of individual tags occurring in the
transcript.                                                           Table 4. Model correlations to SME ratings
    Our results indicate that frequency of certain types of          SME measure                                 Jack-knife Corr.
utterances correlate with team performance. The                                                                  to SME ratings
                                                                     Leadership (guidance + priority)                 0.73
correlations for tags predicted by computer are shown in
                                                                     Brevity                                          0.58
Table 3.                                                             Clarity                                          0.68
           Table 3. Tag to Performance Correlations.                 Completeness of reports                          0.63
                                                                     Passing information                              0.61
  TAG                     PEARSON            SIG.                    Proper phraseology                               0.78
                          CORR.              2-Tailed                Seeking sources                                  0.57
  Acknowledgement               0.335            0.006               Situation update                                 0.45
                                                                     Providing          and           requesting      0.62
  Fact                          0.320            0.008               backup/assistance
  Response                     -0.321            0.008               Error correction                                 0.57
  Uncertainty                  -0.460            0.001               Providing guidance                               0.61
   We see that the automated tagging provides useful                 Stating priorities                               0.73
results that can be interpreted in terms of team processes.          Anti-Air Warfare Team Observation                0.61
                                                                     Communication                                    0.59
Teams that tend to state more facts and acknowledge
                                                                     Information Exchange                             0.50
other team members more tend to perform better. Those                Supporting Behavior                              0.61
that express more uncertainty and need to make more
responses to each other tend to perform worse. These                  The results show significant correlations to all SME
results are consistent with those found in Bowers et al.           ratings and they show that the approach to predicting
(1998), but were generated automatically rather than by            team performance generalizes to highly different types of
the hand-coding done by Bowers. The results therefore              team communication data.
illustrate a direct link between the types of language used           In another experiment in cooperation with the Air Force
and team performance.                                              Research Lab (AFRL) in Mesa, Arizona, we analyzed
                                                                   engagement transcripts recorded from F-16 simulators.
                                                                   We analyzed communication transcribed from radio audio
             Generalization Experiments                            using automatic speech-to-text software.                     All
  While the above results show that the approach can               engagements were air-to-air scenarios against an enemy
provide accurate predictions of team performance, it is            threat involving a team of four F-16s and an airborne
important to test the generalization to different team tasks.      warning and control system aircraft (AWACS). AFRL
In this section, we present results from two additional data       provided subjective SME team performance ratings
sets: Navy TADMUS and Air Force Research Labs                      corresponding to 230 engagement transcripts.
WCAS.                                                                 One SME evaluates an engagement, but different SMEs
   We obtained 64 transcribed missions from the Navy               were used over the time of our data. SME agreement data
Tactical Decision Making Under Stress (TADMUS)                     was not available, although previous research on similar
dataset collected at the Surface Warfare Officer’s School          data found SME agreement of 0.42. (See Krusmark,
                                                                   Schreiber & Bennett, 2004)
(SWOS) (See Johnston, Poirer and Smith-Jentsch, 1998).
                                                                      As before, we created a semantic space of relevant text.
In the scenario, a ship’s air defense warfare (ADW) team
                                                                   For team performance prediction, we used the same
performs the detect-to-engage (DTE) sequence on aircraft           language measures as the TADMUS experiment
in the vicinity of the battle group, and report it to the          augmented with speaker frequency counts and speaker
Tactical Action Officer and Bridge. Associated with the            frequency ratios. Because we had utterance time stamps,
transcripts were a series of Subject Matter Expert (SME)-          we also included aggregate time measures of average time
rated performance measures (see Table 4).                          between utterances and the engagement time duration.
   As in Experiment 1, we developed a semantic space of            Predictive models were created in the same manner as in
information relevant to the domain and then computed               the TADMUS experiment. Table 5 shows the resulting
predicted scores for each of the SME ratings. Unlike               correlations between our predictions of team performance
Experiment 1, we combined the LSA k-nearest measure                and the original SME ratings of team performance and
with a series of other natural language measures that have         Table 6 shows correlations to objective measures.
been successfully used in characterizing essay quality
and account for effects of word order (See Landauer,
                                                              1320

   Table 5. Correlations to subjective SME ratings                   For example, we can now locate utterances in the
  SME measure                                   Corr              semantic space that correspond to places where teams
  Planning operations                           0.58              received high or low team scores. These can provide
  Rate the comm.. about the                     0.50              indications of the types of language that are strongly
  targeting/sorting/shots                                         correlated with good and poor performance. It can further
  Aggregate score for overall situational       0.54              identify potential knowledge gaps in teams. Because of
  awareness                                                       the highly interactive nature of the task, there are certain
  Aggregate score for overall communications    0.42
                                                                  pieces of knowledge that must flow between team
  Aggregate score for overall engagement        0.44
                                                                  members at critical points. These techniques can identify
                                                                  if this information has been conveyed. Typically,
                                                                  modelling these tasks have heretofore relied on large
   Table 6 Correlations to objective indicators
                                                                  amounts of hand-coding and SME knowledge in order to
  Objective indicator                           Corr              determine the types of knowledge and team processes
  Team identification number                    0.80              involved at any part of a large team interaction. The
  Mission identification number                 0.67              results suggest that the current approach can be used in
  Day of week of engagement                     0.63              combination with other modelling efforts of teams.
                                                                     In terms of applying this research to team dialogues, the
   These result show we could predict/identify which team         automated modeling provide cost-effective and efficient
was performing the task based solely on our                       approaches for analyzing communications data. The
communications analysis. The teams’ tasks vary in a               experiments reported here show that the LSA techniques
known way over the course of the week of the team’s               work with both transcribed and automated speech
training and the results show we can often determine the          recognition (ASR) output confirming, Foltz, Laham &
day of the week the engagement was performed using our            Derr (2003) which showed that LSA predictions derived
communications analysis alone.                                    from ASR output was highly robust. With 40 percent
   As with the TADMUS results, these AFRL results                 word error rates, LSA’s prediction ability decreased by
show significant correlations (p < 0.0001) with both the          only 10-15%. Thus, these methods can yield information
SME and the objective indicators again demonstrating              on team communication patterns that are valid, reliable,
that this approach generalizes to disparate team                  and useful to the assessment and understanding of team
communication data sets.                                          performance and cognition--necessary prerequisites to the
                                                                  development of team training programs and the design of
                                                                  technologies that facilitate team performance. In
                          Conclusions                             particular, application domains that are communications-
Overall, the results of the studies show that LSA-based           intensive and that require a high degree of team
algorithms can be used to predict team performance both           coordination can especially benefit from these streamlined
through modeling team communications directly and via             methods for assessing team communication.
tagging content. These results show that the approach                Research into modelling team cognition based on team
generalizes beyond specific corpora and training sets.            discourse is a new but growing area. However, until
The results from the tagging portion of the study are             recently, the large amounts of transcript data have limited
comparable to other efforts of automatic discourse                researchers from performing analyses of team discourse.
tagging using different methods and different corpora             The results of this study show that applying AI and NLP
(Stolcke et al., 2000), which found performance within            techniques to team discourse can provide accurate
15% of the performance of human taggers. While LSA                predictions of performance. These automated tools can
relies only on a semantic model, ignoring word order and          help inform theories of the nature of communication in
other syntactic and discourse factors, Experiment 2 shows         team performance and also aid in the development of
that it can also be incorporated with additional statistical      more effective automated team training systems.
measures of language (see also Serafin & Di Eugenio
2004).                                                                              Acknowledgements
   In addition to being able to use the LSA-based                 This research was completed in collaboration with the
approach to discourse tagging, this study demonstrates            CERTT laboratory including, Nancy Cooke, Preston
how it can be applied as a method for doing automated             Kiekel, Jamie Gorman, Susan Smith, and David Farwell
measurement of team performance. The LSA-predicted                from the Computing Research Laboratory and with data
team performance scores correlated strongly with the              provided by Joan Johnston, NAVAIR and Winston
actual team performance measures. This demonstrates               Bennett, AFRL. This work was supported by the Office
that analyses of discourse can automatically measure how          of Naval Research, DARPA, Army Research Laboratory
well a team is performing on a mission. This has                  and Air Force Research Laboratory.
implications both for automatically determining what
discourse characterizes good and poor teams as well as
developing systems for monitoring team performance in
near real-time.
                                                             1321

                       References                                Kiekel, P., Cooke, N., Foltz, P., Gorman, J., & Martin, M.
                                                                   (2002). Some Promising Results of Communication-
Bowers, C., Jentsch, F., Salas, E., & Braun, C. (1998).            Based Automatic Measures of Team Cognition. In
   Analyzing communication sequences for team training             Proceedings of the Human Factors and Ergonomics
   needs assessment. Human Factors, 40, 672-679.                   Society 46th Annual Meeting.
Chu-Carroll, J. (1998). A Statistical Model for Discourse        Kiekel, P., Gorman, J., & Cooke, N. (2004). Measuring
   Act Recognition in Dialogue Interactions. Papers from           Speech Flow of Co-located and Distributed Command
   the 1998 AAAI Spring Symposium. Jennifer Chu-                   and Control Teams During a Communication Channel
   Carroll and Nancy Green, Program Cochairs. 2001.                Glitch. Proceedings of the Human Factors and
   Technical Report SS-98-01. Published by The AAAI                Ergonomics Society 48th Annual Meeting, 683-687.
   Press, Menlo Park, California. Pp. 12-17.                     M. Krusmark, B. Schreiber, & W. Bennet (2004). The
Cohen, J. (1960). A coefficient of agreement for nominal           Effectiveness of a Traditional Gradesheet for
   scales. Educational and Psychological Measurement,              Measuring Air Combat Team Performance in Simulated
   20, 34-46.                                                      Distributed Mission Operations. (AFRL-HE-AZ-TR-
Core, M. (1998). Analyzing and Predicting Patterns of              2004-0090). Air Force Research Laboratory,
   DAMSL Utterance Tags. Papers from the 1998 AAAI                 Warfighter Readiness Research Division, May 2004.
   Spring Symposium, Jennifer Chu-Carroll and Nancy              Landauer, T., Foltz, P., & Laham, D. (1998). Introduction
   Green, Program Cochairs, Technical Report SS-98-01,              to Latent Semantic Analysis. Discourse Processes, 25,
   Published by The AAAI Press, Menlo Park, California.             259-284.
   Pp. 18-24.                                                    Landauer, T., Laham, D., & Foltz, P. (2001). Automated
Deerwester, S., Dumais, S., Furnas, G., Landauer, T., &             essay      scoring.    IEEE      Intelligent    Systems.
   Harshman, R. (1990). Indexing By Latent Semantic                 September/October 2001.
   Analysis. Journal of the American Society for                 Martin, M., and Foltz, P. (2004). Automated Team
   Information Science, 41, 391-407.                                Discourse Annotation and Performance Prediction
Emmert, V. (1989). Interaction analysis. In P. Emmert               using LSA. Proceedings of the Human Language
   and L. L. Barker (Eds.), Measurement of                          Technology and North American Association for
   Communication Behavior, pp. 218-248. White Plains,               Computational            Linguistics         Conference
   NY: Longman, Inc.                                                (HLT/NAACL), Boston, Massachusetts.
Foltz, P. W. (1996). Latent Semantic Analysis for text-          Schvaneveldt, R. (1990).           Pathfinder associative
   based research. Behavior Research Methods,                       networks:       Studies in knowledge organization.
   Instruments and Computers. 28(2), 197-202.                       Norwood, NJ: Ablex.
Foltz, P. W. Laham, D., & Derr, M. (2003). Using                 Serafin, R., & Di Eugenio, B. (2004). FLSA: Extending
  Automatic      Speech       Recognition      for    Team          Latent Semantic Analysis with features for dialogue act
  Communication Analysis. In Proceedings of the Human               classification. ACL04, 42nd Annual Meeting of the
  Factors and Ergonomics Society 47th Annual Meeting.               Association for Computational Linguistics, Barcelona,
Johnston, J.H., Poirer, J., and Smith-Jentsch, K.A. (1998).         July.
  "Decision Making Under Stress: Creating a Research             Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R.,
  Methodology". In, Cannon-Bowers, J. & Salas, E.                   Jurafsky, D., Taylor, P., Martin, R., Van Ess-Dykema,
  (Eds.), Making Decisions Under Stress: Implications               C., & Meteer, M. (2000). Dialogue Act Modeling for
  for Individual and Team Training, pp. 39-5).                      Automatic Tagging and Recognition of Conversational
  Washington, DC: APA,                                              Speech, Computational Linguistics 26(3), 339-373.
                                                            1322

