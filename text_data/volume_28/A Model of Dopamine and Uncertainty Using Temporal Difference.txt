UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Model of Dopamine and Uncertainty Using Temporal Difference
Permalink
https://escholarship.org/uc/item/4061m644
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Davey, Neil
Done, D. John
Thurman, Angela J.
et al.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               A Model of Dopamine and Uncertainty Using Temporal Difference
          Angela J. Thurnham* (a.j.thurnham@herts.ac.uk), D. John Done** (d.j.done@herts.ac.uk),
                     Neil Davey* (n.davey@herts.ac.uk), Ray J. Frank* (r.j.frank@herts.ac.uk)
                       School of Computer Science,* School of Psychology, **University of Hertfordshire,
                                College Lane, Hatfield, Hertfordshire. AL10 9AB United Kingdom
                            Abstract                                   from their electrophysiological characteristics, during a
                                                                       delay paradigm of classical conditioning to receive a fixed
   Does dopamine code for uncertainty (Fiorillo, Tobler &              juice reward, while manipulating the probability of receipt
   Schultz, 2003; 2005) or is the sustained activation recorded        of the reward. Two related but distinct parameters of reward
   from dopamine neurons a result of Temporal Difference (TD)          were identified from the activation produced, after learning
   backpropagating errors (Niv, Duff & Dayan, 2005)? An                had taken place: (i) A phasic burst of activity, or reward
   answer to this question could result in a better understanding      prediction error, at the time of the expected reward, whose
   of the nature of dopamine signaling, with implications for          magnitude increased as probability decreased; and (ii) a new
   cognitive disorders, like Schizophrenia. A computer                 slower, sustained activity, above baseline, related to
   simulation of uncertainty incorporating TD Learning
                                                                       motivationally relevant stimuli, which developed with
   successfully modelled a Reinforcement Learning paradigm
   and the detailed effects demonstrated in single dopamine
                                                                       increasing levels of uncertainty, and varied with reward
   neuron recordings by Fiorillo et al. This alternate model           magnitude. Both effects were found to occur independently
   provides further evidence that the sustained increase seen in       within a single population of dopamine neurons.
   dopamine firing, during uncertainty, is a result of averaging
   firing from dopamine neurons across trials, and is not
   normally found within individual trials, supporting the claims
   of Niv and colleagues.
   Keywords: Dopamine; Uncertainty; Single Cell Recordings;
   Temporal Difference; Computer Simulation.
               Dopamine and Uncertainty
Current theories of the effects of dopamine on behaviour
focus on the role of dopamine in Reinforcement Learning,
where organisms learn to organise their behaviour under the
influence of goals, and expected future reward is believed to
drive action selection (McClure, Daw & Montague, 2003;
Montague, Dayan & Sejnowski, 1996; Schultz, Dayan &
Montague, 1997; Suri & Schultz, 1999). Single cell
recordings of dopamine neurons have identified a phasic
dopamine burst of activity which is posited to be a reward
prediction error (Schultz, 1998; Waelti, Dickinson &
Schultz, 2001) and Temporal Difference (TD) Learning
(Sutton, 1988; Sutton & Barto, 1998), a form of
Reinforcement Learning, provides an explicit method of
modelling and quantifying this error (Hollerman & Schultz,
1998; Schultz et al., 1997). It is likely that disruption to the
dopamine system gives rise to an abnormality in
information processing by dopamine and some of the
symptoms currently associated with schizophrenia,
particularly psychosis and deficits in working memory.                   Figure 1: Sustained activation of dopamine neurons with
   It has been posited that dopamine also codes for                     uncertainty taken from Fiorillo et al. (2003) (A) Rasters and
uncertainty (Fiorillo, Tobler & Schultz, 2003), as under                histograms of single cell activity (B) Population histograms
conditions of maximum uncertainty, observations of single
cell recordings have shown a sustained increase in activity               With uncertainty, the sustained activation began on
from presentation of a conditioned stimulus (CS) to the                presentation of a CS and increased in strength until a reward
expected time of a reward. They recorded the activity of               was due, at which point the activation ceased (Figure 1B,
neurons in two primates, identified as dopamine neurons                where P = 0.25, 0.5 and 0.75). This activation was greatest
                                                                  2257

when uncertainty of reward was at a maximum, i.e., when               and only allowed travel in one direction. The aim of this
the reward was received on only 50% of occasions and                  investigation was to use TD learning to model the following
probability (p) was 0.5. Sustained activation was also seen           effects seen in dopamine neuron firing by Fiorillo and
at lower values of uncertainty, when probability was 25%              colleagues: (a) The phasic activation at the expected time of
and 75%, but to a lesser extent. No sustained activation was          reward that increased as probability decreased; (b) the
seen when probability was certain at either zero or 1,                sustained increase in activity from the onset of the CS until
suggesting that the sustained activation coded for                    the expected time of reward, during uncertainty, posited
uncertainty.                                                          either as uncertainty, or as backpropagating TD prediction
   However, this view is controversial as Niv, Duff and               errors; and (c) the sustained activation increasing with
Dayan, (2005) have suggested that the sustained activation,           increasing reward magnitude. In addition, in the discussion
or ‘ramping’ effect in the delay period, is due to                    an attempt is made to address three of the points raised by
backpropagating TD prediction errors, and not to                      Fiorillo et al. (2005) in response to Niv et al. (2005).
uncertainty. Specifically, they suggest that it is the
asymmetric coding of those prediction errors that give rise                                         Method
to the effects seen in time, over consecutive CS
presentations, due to a low baseline rate of activity in              Temporal Difference
dopamine neurons. Firing rates of positive prediction errors
typically rise to about 270% above baseline, while negative           The maze incorporated an ‘actor-critic’ architecture
                                                                      (McClure et al., 2003; Montague, Hyman & Cohen, 2004;
errors only fall to approximately 55% below baseline
(Fiorillo et al. 2003). During uncertainty, these                     Sutton & Barto, 1998), a form of reinforcement TD learning
                                                                      where an ‘adaptive critic’ computes a reward prediction
asymmetrical positive and negative errors, when summed,
will not cancel each other out, as predicted by the TD                error, which is used by the ‘actor’ to choose those actions
                                                                      that lead to reward.
algorithm, even after extensive training periods. The overall
effect, as seen in Fiorillo et al., will be of (i) a positive
                                                                      The Critic The TD algorithm is designed to learn an
response across trials at the expected time of reward, and (ii)
                                                                      estimate of a value function V*, representing expected total
a ‘ramping’ effect from presentation of the CS to the
                                                                      future reward, from any state, s, (Equation 1), where t
expected time of reward, described by Fiorillo and
                                                                      represents time and subsequent time steps t = 1, t = 2 etc; E
colleagues as sustained activation. The resulting effects
                                                                      is the expected value and r represents the value of the
arise as a result of averaging across multiple trials and are
                                                                      reward. γ is a discounting parameter between 0 and 1 and
not a within trial phenomena.
                                                                      has the effect of reducing previous estimates of reward
   Using TD, Niv and colleagues successfully modelled both
effects identified by Fiorillo et al. (2003) during uncertainty.      exponentially with time, so that a reward of yesterday is not
                                                                      worth as much as a reward of today. Equation 2 is Equation
They also showed that the shape of the ramp depended on
the learning rate, and that the difference in the steepness of        1 in a recursive form that can be used in the learning
                                                                      process.
the ramp between delay and trace conditioning could be
accounted for by the low learning rates associated with trace
                                                                           V*(st) = E[rt + γ rt + 1 + γ2 rt + 2 + γ3 rt + 3 + …] [Eqn 1]
conditioning, resulting in a smaller or even negligible ramp.
   In reply to Niv et al., Fiorillo and colleagues defend their
                                                                           V*(st) = E[rt + γV*(st + 1)]                          [Eqn 2]
original claim that dopamine encodes uncertainty about
reward (Fiorillo, Tobler & Schultz, 2005). Three of the five
                                                                         TD prediction error is a measure of the inconsistency for
points raised are of particular interest to this study. Firstly,
they give two examples as evidence of sustained activation            estimates of value at successive time steps. The error, δ(t), is
                                                                      derived by rearranging Equation 2 into Equation 3, which is
within single trials, which is contrary to the postulations of
Niv et al., and secondly, they suggest that activity in the last      a measure of the relationship between two successive states
                                                                      and the current reward. This will give estimates, V, of the
part of the delay period should reflect the activity of the
preceding trial. Finally, they suggest that other ways of             value function V*. The dopamine prediction error signal,
                                                                      δ(t), takes into account the current reward, plus the next
using TD to model dopamine as a TD error are more
biologically plausible than backpropagating TD errors. It is          prediction multiplied by the discounting parameter γ, minus
                                                                      the current prediction. It is the error δ(t) that is equivalent to
important, therefore, to look at a range of models in order to
understand the limitations of using the TD algorithm to               the dopamine reward prediction error, or learning signal, to
                                                                      create better estimates of future reward.
model the role of dopamine.
    In the present study a simulation of a ‘rat’ in a one-armed
                                                                           δ(t) = rt + γV(st + 1) - V(st)                        [Eqn 3]
maze was used to investigate the claims of Fiorillo and
colleagues, using an alternative TD model to Niv et al. The
                                                                      The Actor An extension to the TD model has been made to
maze modelled was similar to that used by McClure et al.
(2003) linking the ideas of reward prediction error and               include a role for dopamine in biasing action selection using
                                                                      the same prediction error signal, δ(t), to teach a system to
incentive salience, but contained an additional ‘satiety’ state
                                                                      take the best actions, namely those that are followed by
                                                                 2258

rewards (McClure et al., 2003; Montague et al., 1996). The                 uncertainty), or 0.75. The δ(t) values were recorded for each
way an action is selected is that the actor randomly chooses               state transition, for a single probability in each trial. Each
a possible action, and the anticipated δ(t) is calculated using            trial consisted of 1000 steps through a one-way maze with
Equation 3. The probability of taking this action is then                  eight states plus a ‘satiety’ state, with a step being a
calculated from this δ(t) value using the softmax function in              transition from one state to the next, and a run being one
Equation 4 (where m and b are parameters of the softmax                    complete journey through the maze, from start to finish. At
curve), which calculates the probability of that action                    the beginning of each trial the values of each state in the
occurring from the anticipated δ(t) value. If no action is                 maze (V) were set to zero. Movement to the next state in the
selected, time is increased by one step and another random                 maze was selected according to the effect of TD learning on
action is considered.                                                      different probabilities of receiving a reward for each run.
                                                                              In keeping with the biology of dopamine, namely the
    P (of taking action) = (1 + e –m (δ(t) - b) )-1           [Eqn 4]      asymmetry in coding of positive and negative errors, any
                                                                           negative prediction errors were scaled by a factor of one
   Actions are generated with a probability of selection                   sixth, the scaling factor used by Niv et al. (2005). The
based on the predicted values of their successor states,                   scaled δ(t) values were then averaged across fifty
preferring those actions that give a high burst of dopamine,               consecutive runs for each state, where γ = 0.98, and the
or TD error signal. There is a greater probability of                      magnitudes of the scaled values compared. This averaging
remaining at the same state and not making a move when                     corresponded to the summing of peri-stimulus-time-
the error signal is low as all states become increasingly                  histograms (PSTH) of activity over different trials and inter-
probable.                                                                  trial averaging used by Fiorillo et al. (2003).
   Learning takes place in the model according to Equation
5, where α is a learning rate parameter.                                   Reward Magnitude Individual reward magnitudes of 0.5, 1
                                                                           and 2 were compared in different trials to see the effect on
              V(si) Å V(si) + α δ(t)                [Eqn 5]                the sustained activation.
The Maze                                                                    An Example of Learning
A computer simulation was constructed of a ‘rat’ learning to
traverse a one-arm maze to receive a reward, using the TD                               1
algorithm with an ‘actor-critic’ architecture. Figure 2 shows
a maze with positions modelled as five states, starting at
State 0 (the CS) and progressing through intermediate states                          0.6
to receive a simulated reward in State 4 (the reward state).                   δ(t)
In order to model the breaks between maze runs in real rats,
it was necessary to insert a ‘satiety’ state (State 5) into the
maze, between the goal (State 4) and the start (State 0),                             0.2
where the transition between that state and State 0 remained
at zero so that no learning could take place. This had the
effect of resetting the value of start State 0 to zero, acting as                            1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
                                                                                      -0.2
a ‘resting’ state and ensuring that the ‘rat’ was always                                                         runs
surprised when starting the maze. Without this additional
                                                                                                     CS: S8-S0    S3-S4     R: S7-S8
state, the simulated rat learnt the value of the start state, and
in effect, there was no CS. Intermediate states were added
and removed, as required to make mazes of different                              Figure 3: Delta values for each state transition over first
lengths.                                                                                          thirty runs, p = 1, r = 1
    Start      State 1      State 2       State 3         Goal             With a probability of 1 and a maze of eight states plus a
    r=0         r=0          r=0           r=0            r=1
                                                                           ‘satiety’ state, complete learning took place over the first
                                                                           thirty runs (γ = 1). On the first run a large prediction error,
                                                                           δ(t), was recorded at the expected time of the reward (S7-
                            State 5                                        S8), and as runs progressed, this δ(t) was transferred back to
                            r=0
                            v=0                                            the CS (S8-S0). When full learning had taken place only the
                                                                           CS elicited a reward prediction error. This effect is
      Figure 2: Maze with five states plus ‘satiety’ state                 demonstrated in Figure 3, which shows the δ(t) at the
                                                                           expected time of reward beginning at 1 and reducing to zero
Simulations                                                                by run 9 at which point the value of the state is learnt and
Uncertainty – Degree of Probability The ranges of                          the reward fully predicted. The δ(t) at the CS begins at zero
probabilities used for trials were 0.25, 0.5 (maximum                      and increases gradually to 1, from run 10 to run 30. An
                                                                        2259

intermediate state transition S3-S4 is included which                 There would also be less of an effect on the phasic
records the δ(t) backpropagated from the reward state by run          activation seen at the CS as the effect of reward would take
5. The error increases until run 8 and then reduces to zero by        longer to reach that state. As suggested above, difficulties
run 21.                                                               arise when the backpropagating chain of reward prediction
   All the following tests with uncertainty were done post            errors is broken and runs are taken out of context of the trial
training.                                                             history.
                                                                                 1
                           Results
                                                                              0.8
                                                                                                                0.25    0.5    0.75
Uncertainty – Degree of Probability
                                                                              0.6
Eventually, by chance, actions were selected in trials for the
entire range of probabilities (p), 0.25, 0.5 or 0.75, and the          δ(t)   0.4
‘rat’ progressed along the maze towards the reward state
receiving the reward (r) of that state, r = 1. On subsequent                  0.2
runs, learning occurred as the value of the reward was
propagated backwards, updating earlier states using a                            0
proportion of the prediction error signal, δ(t).                                     S8-0   S0-1 S1-2 S2-3 S3-4 S4-5 S5-6 S6-7 S7-8
   The patterns of data obtained show that it is necessary for                -0.2    CS                                         R
the history of previous runs to be taken into consideration                                           state transitions
when analysing reward prediction errors and not just the last
trial. Accordingly, consecutive runs should be selected for
                                                                      Figure 4: Average δ(t) values, before scaling, for each state
averaging in order to preserve the backward chaining effect
                                                                      transition over 50 runs, when p = 0.25, 0.5 and 0.75, α = 0.9
of the TD algorithm. The TD algorithm uses rewards
obtained in the past to make predictions about future
expected reward, affecting the values of all the states in the                  1
maze, which are continually being updated as the rat
progresses along the maze. With uncertainty, the particular                   0.8
                                                                                                                 0.25    0.5   0.75
course a rat takes on a particular trial is novel in each trial,
as it depends on the exact order of rewarded and non-                         0.6
rewarded runs, which are delivered randomly by the
                                                                       δ(t)   0.4
computer program. The δ(t) values are then propagated
backwards, in order, from later states to earlier states, as                  0.2
time progresses.
   As the probability of obtaining a reward increased, from                     0
25% to 50% to 75%, so did the level of phasic activation at                          S8-0   S0-1 S1-2 S2-3 S3-4 S4-5 S5-6 S6-7 S7-8
the CS (S8-S0) (Figure 5), with average δ(t) values of 0.23,                  -0.2    CS                                         R
0.57 and 0.70 respectively.                                                                            state transitions
(a) The phasic activations at the expected time of reward
Without scaling the δ(t) values recorded for each state                Figure 5: Average δ(t) values, after scaling, for each state
transition to compensate for the biologically asymmetric              transition over 50 runs, when p = 0.25, 0.5 and 0.75, α = 0.9
coding of positive and negative prediction errors, no average
positive phasic activation was seen at the expected time of              In conclusion, phasic activations were seen at the
reward (Figure 4 S7-S8). However, after scaling δ(t) values           expected time of reward, in accordance with the findings of
by a factor of one sixth and averaging δ(t) values over               Fiorillo et al., when δ(t) values were scaled to compensate
consecutive trials, positive phasic activation was seen at the        for the asymmetric coding. However, unless rewarded only
expected time of reward (Figure 5).                                   trials were averaged, the phasic activation did not vary
    When comparing the average scaled δ(t) values across              monotonically with reward probability.
trials with probabilities of 0.25, 0.5 and 0.75, similar
averaged, scaled δ(t) values were recorded of 0.16, 0.16 and           (b) The sustained increase in activity Figure 4 shows that
0.14 respectively. However, if averages were taken over               no ‘ramping’ effect is seen from plotting the average δ(t)
rewarded trials only, as suggested in Figure 2A in Fiorillo et        values obtained for probabilities of 0.25, 0.5 and 0.75 for
al. (2003), δ(t) values would be positive at the expected time        each state transition. Here the symmetrical positive and
of reward as all negative values would be removed. In                 negative errors effectively cancel each other out, in
addition, there would be less non-rewarded runs to be                 accordance with the TD algorithm. However, when the δ(t)
removed at higher probabilities, resulting in the phasic              values were scaled by a factor of one sixth to compensate
activation varying monotonically with reward probability.             for the biological asymmetric coding of positive and
                                                               2260

negative errors, and averaged across consecutive runs,                 activity increases on single trials as Niv et al. (2005) did not
positive δ(t) values were seen that corresponded to the                specify what a single trial increase in delay-period activity
sustained activation and ‘ramping’ effects reported in                 should look like. In our simulations, Figure 7 is an example
Fiorillo et al. (2003) and Niv et al. (2005) respectively              of a single trial (or a single run in this simulation), and is
(Figure 5). The magnitude of the ramping effect is                     represented by recording the scaled prediction errors, δ(t),
marginally greater for maximum probability, p = 0.5 than               for each state transition, for one run through the maze. This
for the lower probabilities of p = 0.25 and p = 0.75, in               run is analogous to the activity of a single neuron in a single
accordance with the findings of Fiorillo et al. However, the           trial over time and is simply a snapshot of the δ(t) values for
difference seen between the two trials with probabilities of           each state, which may be either positive or negative with
0.25 and 0.75, which are comparable levels of uncertainty,             respect to baseline firing, depending on the history of
could be accounted for by the different reward history for             previous runs.
each. This difference should be negligible if more trials                 The single run in Figure 7 is taken from actual run 6 in
were taken into account.                                               Figure 8 and represents non-rewarded run N preceded by
                                                                       ….RNRNR, where R is a rewarded run. The preceding RNR
(c) Reward Magnitude The value of the reward was                       can be clearly identified in the δ(t) values seen for state
manipulated across different trials, with rewards given of             transitions S4-S5, S5-S6 and S6-S7 respectively, but the
0.5, 1 and 2. The size of the reward had an effect on the              results of earlier runs are harder to make out further back in
range of δ(t) values available for each state. With a larger           time, as the TD algorithm ensures rewards or non-rewards
reward comes a larger range of possible δ(t) values, and,              in the past are not worth as much as those of the present.
accordingly, larger ‘ramping’ effects (Figure 6). Therefore,              Examination of many single runs through the maze did
the sustained activation increased with increasing reward              not reveal a ramping effect. Fiorillo et al. (2005) provided
magnitude, in accordance with Fiorillo et al. (2003).                  two examples of possible sustained activation in single
                                                                       trials, but these effects could have occurred quite by chance
                                                                       due to the order of rewarded and non-rewarded trials, as
        1.4
                                                                       explained above, and not necessarily be examples of
                                                                       uncertainty. Indeed, if this within trial ramping effect were a
                                      0.5   1   2
          1                                                            regular occurrence then there would be many examples of
                                                                       single trials in support of the uncertainty hypothesis.
 δ(t)
        0.6
                                                                                  1
        0.2
                                                                                0.8
        -0.2   S8-0   S0-1 S1-2 S2-3 S3-4 S4-5 S5-6 S6-7 S7-8                   0.6
                CS              state transitions          R
                                                                         δ(t)
                                                                                0.4
Figure 6: Scaled average δ(t) values over 30 runs for reward                    0.2
           values of 0.5, 1 and 2, p = 0.5, α = 0.5
                                                                                  0
                            Discussion                                                 S8-0 S0-1 S1-2 S2-3 S3-4 S4-5 S5-6 S6-7 S7-8
A simulation of reinforcement learning, incorporating an                        -0.2    CS                                      R
                                                                                                      state transitions
‘actor-critic’ architecture of TD learning, successfully
modelled the following properties of dopamine
demonstrated by Fiorillo et al. (2003): (a) The phasic                               Figure 7: An example of a single trial:
activations at the expected time of reward; (b) the sustained                    Scaled δ(t) values for a single run, p = 0.5, r = 1
increase in activity from the onset of the conditioned
stimulus until the expected time of reward, during                        Secondly, Fiorillo and colleagues claimed that if activity
uncertainty; and (c) the sustained activation increasing with          during the delay period is due to backpropagating error
increasing reward magnitude. This supports the argument                signals that originated on previous trials, then the activity in
by Niv et al. (2005) that the ramping effect seen during               the last part of the delay period of each individual trial
uncertainty is a result of backpropagating TD errors and not           should reflect the last reward outcome. Specifically, they
a within-trial encoding of uncertainty.                                suggest that if the preceding trial was rewarded, there
   In response to the claims of Niv and colleagues, Fiorillo           should be more activity at the end of the delay period, and
et al. (2005) raised several points in support of their original       less activity if it was not rewarded, but they found no
argument, three of which are relevant to this study. Firstly,
they refer to the difficulty of determining whether or not
                                                                2261

dependence of neural activity on the outcome of preceding           evidence that ramping of the reward prediction error, δ(t), is
trials.                                                             not normally found within a trial of a single dopamine
   Our results show that it is necessary for more of the            firing, but instead arises from averaging across trials.
history of previous runs to be taken into consideration than           Our simulations add further weight to the criticisms of
just the last reward outcome, when analysing reward                 Niv et al. that the effects demonstrated by Fiorillo and
prediction errors. For example, Figure 8 shows a history of         colleagues are due to backpropagating TD errors, and not a
rewarded and non-rewarded runs RNRNRNNNNN. After                    within-trial encoding of uncertainty. We support the claims
scaling, large δ(t) values were seen for runs 1-6 because           by Niv et al. (2005) that the ramping signal is the best
alternate rewards and non-rewards were given, but runs 7-10         evidence yet for the nature of the learning mechanism of a
were not rewarded and, consequently, gradual extinction of          shift in dopamine activity from expected time of reward to
the negative prediction error occurred. This example shows          the CS.
that it is not always the case that less activity will be seen if
a trial is not rewarded (and vice versa), as runs 8-10 show an                             References
increase in firing (towards baseline) following non-                Fiorillo, C.D., Tobler, P.N., & Schultz, W. (2003). Discrete
rewarded runs.                                                        coding of reward probability and uncertainty by
           1
                                                                      dopamine neurons. Science, 299, 1989-1902.
                                                                    Fiorillo, C.D., Tobler, P.N., & Schultz, W. (2005). Evidence
         0.8                                                          that the delay-period activity of dopamine neurons
                                                                      corresponds to reward uncertainty rather than
         0.6                                                          backpropagating TD errors. Behavioral and brain
  δ(t)
                                                                      Functions, 1:7.
         0.4                                                        Hollerman, J.R., & Schultz, W. (1998). Dopamine neurons
                                                                      report an error in the temporal prediction of reward
         0.2                                                          during learning. Nature Neuroscience, 1, 304-309.
                                                                    McClure, S.M., Daw, N.D., & Montague, P.R. (2003). A
           0                                                          computational substrate for incentive salience. Trends in
                R   N   R    N    R    N    N    N    N     N         Neuroscience 26(8), 423-428.
         -0.2                                                       Montague, P.R., Dayan, P., & Sejnowski, T.J. (1996). A
                                   runs
                                                                      framework for mesencephalic dopamine systems based
                                                                      on predictive Hebbian Learning. Journal of neuroscience,
  Figure 8: Scaled average δ(t) values at expected time of            16(5), 1936-1947.
    reward (S7-S8) recorded over 10 runs, p = 0.5, r = 1            Montague, P.R., Hyman, S.E., & Cohen, J.D. (2004).
                                                                      Computational roles for dopamine in behavioral control.
                                                                      Nature 431, 760-767.
   Finally, Fiorillo et al. (2005) raise the argument that other
                                                                    Niv, Y., Duff , M.O., & Dayan, P. (2005). Dopamine,
TD models of dopamine are more biologically plausible                 uncertainty and TD Learning. Behavioral and brain
than backpropagating TD errors, for example Suri &                    Functions, 1:6 1-9.
Schultz (1999), and it is important, therefore, to look at a        Schultz, W. (1998). Predictive reward signal of dopamine
range of models in order to understand the limitations of             neurons. Journal of Neurophysiology, 80, 1-27.
using the TD algorithm to model the role of dopamine.               Schultz, W., Dayan, P., Montague, P.R. (1997). A Neural
However, our work has shown that the predictions of Niv et            substrate of prediction and reward. Science, 275: 5306,
al. (2005) are robust in the sense that they transfer to              1593-1599.
another type of model, albeit still using the same TD               Suri, R.E., & Schultz, W. (1999). A neural network model
algorithm.                                                            with dopamine-like reinforcement signal that learns a
                                                                      spatial delayed response task. Neuroscience, 91(3), 871-
                        Conclusion                                    890.
                                                                    Sutton, R.S. (1988). Learning to Predict by the Methods of
This alternate TD model to Niv et al. (2005) has effectively
                                                                      Temporal Differences. Machine Learning 3, 9-44.
simulated conditioning in a Reinforcement Learning
                                                                    Sutton, R.S., & Barto, A.G. (1998). Reinforcement learning,
paradigm and successfully modelled the effects                        MIT Press.
demonstrated in single dopamine neuron recordings,                  Waelti, P., Dickinson, A., & Schultz, W. (2001). Dopamine
suggested to be coding for uncertainty, by Fiorillo et al.            responses comply with basic assumptions of formal
(2003). In addition, we have demonstrated what a single               learning theory. Nature, 412, 43-48.
trial in TD Learning might look like and provide further
                                                                2262

