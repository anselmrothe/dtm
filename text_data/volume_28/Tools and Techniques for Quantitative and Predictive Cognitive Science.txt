UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Tools and Techniques for Quantitative and Predictive Cognitive Science
Permalink
https://escholarship.org/uc/item/1860d7qb
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Author
Stewart, Terrence C.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

           Tools and Techniques for Quantitative and Predictive Cognitive Science
                                             Terrence C. Stewart (terry@ccmlab.ca)
                      Carleton Cognitive Modelling Lab, Institute of Cognitive Science, Carleton University
                                     1125 Colonel By Drive, Ottawa, Ontario, K1S 5B6, Canada
                             Abstract                               Standard Practices
                                                                    Examining the published results of computational modelling
  A methodology is described for developing cognitive science
  theories which produce numerical predictions. This is done
                                                                    in cognitive science reveals a wide variety of approaches
  by adopting methodology from mathematical models in               and measurement standards. For example, the following
  physics, and adapting it for use with the more complex            three studies demonstrate various approaches for identifying
  computational models. Bootstrap confidence intervals and          correspondences between a complex computational theory
  equivalence testing are introduced, and parameter fitting is      and the observations that theory is meant to explain..
  shown to be an intermediate step before prediction. To ensure         (1) In (Erev & Barron, 2005), a model of forced-choice
  replication and exploration by other researchers, publication     decisions is studied by measuring the mean squared
  of the source code for the model, experimental situation, and     deviation between the choice rates for the model and human
  data analysis is required. To assist in this process, we have     subjects. Parameter fitting is used to adjust the model to
  developed a freely available tool suite, covering creation of     find the closest match to human data. After this, a further
  models, running parallel simulations, parameter exploration,      comparison is done to a separate set of human data, so as to
  data analysis, and Internet-based access to all data.             evaluate the model's ability to generalize.
                                                                        (2) In (Connell & Keane, 2006), a model of plausibility
                         Introduction                               judgments is investigated by measuring the correlation
As cognitive science theories become more complex, it is            between the model's output for 60 different situations and
increasingly difficult for the predictions of those theories to     the mean human judgment for the same situations.
be determined. The cognitive behaviour being examined is            Sensitivity analysis is used to show how this correlation
a result of an exceedingly complicated interconnection of           changes for differing parameter values of the model.
components, each of which is itself complex. This makes                 (3) In (Anderson et al., 2005), an ACT-R model of the
the interpretation of the results of cognitive science research     Tower of Hanoi task is examined. The model is first fit
somewhat unclear. Are we learning about fundamental                 based on human latency information, and is then used to
mechanisms of cognition, or are we describing their effects?        generate probability distributions for the BOLD responses
Are we finding causal links, or correlations? Are we                in three brain regions. These predictions are evaluated via a
developing frameworks, or playing “twenty questions” with           χ2 goodness-of-fit to human fMRI results, and a non-
nature? And what, exactly, is the difference between these          significant result leads to the conclusion that “the model
possibilities?                                                      does account for the systematic variance in the data”.
   One methodology that is commonly used to help frame                  There are certainly many other approaches, and a
this research is computational modelling. Here, a key               detailed analysis of the benefits and drawbacks of each
component of the research is the creation of a computer             would be a monumental task. However, no single technique
program which is a model of the actual cognitive system             will completely satisfy everyone. Researchers try to choose
under investigation. However, following this approach does          the technique which best suits their needs, but it is difficult
not, in itself, resolve the above questions. Is the computer        to foresee all questions a reader may have.
program meant to be an exact expression of the theory? Is it            This problem becomes even more apparent in conference
meant to indicate actual processes occurring inside the             proceedings, where space limitations lead to a drastic
system? Is it predictive or descriptive? How do we                  reduction in provided information. Here, it is common to
interpret the parameters of the model? Are we modelling             provide a simple side-by-side comparison of the model data
individuals or the group mean? How can we evaluate how              and the mean of the subject data, after parameter fitting on a
well our models perform?                                            measure such as the mean-squared error. Distributional
   These sorts of questions must be addressed in any                information and confidence intervals are seldom provided.
research. Since computational modelling is a relatively new             It is important to note that these observations and the
approach for science, it is especially important to be clear on     ideas in this paper should not be interpreted as a criticism of
what claims are being made, what aspects are being                  individual papers in this area. Instead, this article is
measured, and what, exactly, constitutes proof. The goal of         intended as a re-evaluation of customary research and
this paper is to present a philosophical framework for              publishing practices. It is difficult to produce concise and
understanding how computational modelling fits within               convincing measures for these varying situations, and space
science, and then from that basis to describe tools and             constraints (whether in conferences or journals) make an
techniques that help provide rigor to this scientific approach.     exhaustive approach impossible. To address this, we now
These principles and processes have been fundamental to             discuss a number of limitations of common modelling
our work in the Carleton Cognitive Modelling Lab, and we            practices, followed by our alternative methodology
believe they have wide applicability and utility elsewhere.         addressing these limitations. We then describe our software
                                                                    tools which simplify the process of following this approach.
                                                                816
                                                                821

Measurement Limitations                                                        Another aspect which must be considered is that we
The most straight-forward limitation of these standard                     should also have a way of knowing that when a particular
practices involves measurement. The general lack of                        model does fit well, that a variety of other models do not fit
confidence interval information makes it difficult to assess               as well. For example, in (Stewart, West, and Coplan, 2004),
the actual degree of match. While these intervals are not                  we show that, for a certain set of measurements, a complex
always strictly necessary for the model data (as models can                model of peer group interaction and friendship formation
be run large numbers of times, giving a large N, and thus a                fits the real-world data no better than a completely random
small confidence interval), the real-world data used for                   model. This demonstrates that studying a single model in
comparison almost always has a much lower N. This means                    isolation can lead to a false sense of the model's accuracy.
that the exact measured mean value is not a meaningful
number. A close fit to this number is merely a close fit to                Communication Limitations
the peculiarities of that particular sample of the overall                 The final limitation to be considered involves the
population. Rather, we should be more interested in the                    dissemination of information about the model.
confidence interval, which indicates that we are fairly sure                    Sharing work has been so difficult that researchers
(usually 95%) that the actual mean value (if we sampled the                    tend to build their own animat minds and worlds
entire population) is within some range.1                                      from scratch, often duplicating work that has been
    We must also be careful using confidence intervals for                     done elsewhere.... Often, the only person who ever
comparisons. It is true that if two confidence intervals do                    does experiments with an animat or agent is its
not overlap, then there is a statistically significant difference              author. In this field it has become acceptable not to
between the sets of data. This is can be correctly used to                     have direct access to many of the major systems
conclude that the model does not match with the real world                     under discussion. (Humphrys & O'Leary, 2002)
data2. However, if the confidence intervals do overlap (i.e.               Not only does this violate the basic tenant of replication in
when there is no statistical significance), then we should                 science, but it also leads to a situation where there are more
make no conclusions at all. After all, if our criteria is that a           types of models being investigated than there are current
good model is merely one which gives a confidence interval                 comparisons between architectures (Guilot and Meyer,
overlapping with the real data's confidence interval, then we              2000). Furthermore, when replication is attempted, we
are implicitly encouraging researchers to use a small sample               consistently find that vital aspects of the models are not
size, leading to larger confidence intervals. Indeed this sort             recorded in the paper describing them. Axelrod (2005)
of analysis (or the equivalent t-test) should only be used for             describes his experience in a project attempting to replicate
identifying differences, not similarities.              For further        eight standard computational models in the social sciences
discussion, see (Beaulieu-Prévost, 2006).                                  as one where “Murphy's law seemed to be operating at full
    Furthermore, it is exceedingly rare to see any statistical             strength.” Aside from standard debugging issues, he found
measurement other than the mean being used. Since most                     that there were ambiguities, gaps, and errors in the
cognitive behaviour has a high degree of variation, a good                 descriptions of models. Even when complete source code
model of that behaviour would also exhibit that same                       was available (a rare event), there were still problems
degree of variation. Indeed, the real-world data and the                   involving the readability of the code and even such issues as
model data should have indistinguishable distributions, not                the floating point accuracy of the computers the programs
just indistinguishable means.                                              were being run on. All of these issues combine in such a
                                                                           way that it is difficult to evaluate published results of such
Fitting Limitations                                                        research, and difficult to work with or expand upon the
A more important limitation concerns the process of                        models developed by others.
parameter fitting, and is best summarized by Roberts and                       This limitation also extends to the communication of the
Pashler (2000). They highlight the fact that demonstrating                 results of modelling work, as mentioned earlier, due to the
that a model can be adjusted to fit a particular set of data               constrained space available in publications.
does not in itself inform you about the validity of the theory.
In particular, it says nothing about what range of real-world                                Model-Based Science
data the model could have been adjusted to fit. Perhaps by
                                                                           To resolve these limitations and to develop a more rigorous
adjusting parameter settings it would possible to match the
                                                                           approach to quantitative modelling in cognitive science, we
model to any plausible set of data. If this is the case, then
                                                                           need to take a closer look at the scientific methodology
demonstrating a good fit merely indicates that the model is
                                                                           being applied. Some (Axelrod, 2005) have argued that
highly adjustable, not that it captures some important aspect
                                                                           using computation models and simulation is an entirely new
of the particular situation being modelled.
                                                                           way of doing science. Instead, our approach is to examine
                                                                           computational models as a generalization of standard
1
                                                                           mathematical modelling, as exemplified in physics.
  Or, more correctly, if the actual value was outside that range, then
                                                                           However, due to the increased complexity of the
we would only measure values as strange (or stranger) as what we
                                                                           computational models, many of the simplifying
did measure less than 5% of the time.
2
  Or, more correctly, if the model did exactly match the real data,        characteristics that we have come to expect in physics
then we would observe the sort of data we did observe less than            models will not apply, forcing us to find alternate ways of
5% of the time.                                                            dealing with old problems.
                                                                       817
                                                                       821

    Describing this approach requires us to be clear about                     Table 2: Statistics for the sample in Table 1
exactly what we are trying to do as scientists. For our                                      Sample Mean: 2.0
research, we do not believe that science is best described as                              Sample Median: 2
the pursuit of truth. Instead, we are adopting Giere's                       Sample Standard Deviation: 1.1726
argument in Science Without Laws:                                                            Sample Skew: 0.69775
    Rather than thinking of science as producing sets of                                  Sample Kurtosis: 3.5041
    statements that are true or false in the standard
    objectivist fashion, we should think of it as a practice             However, these statistics are measures of our sample, not
    that produces models of the world that may fit the               the actual distribution. If we had the ability to have
    world more or less well in something like the way                thousands or millions of individual measurements in our
    maps fit the world more or less well. (Giere, 1999)              sample, then the sample distribution would approach the
In other words, the goal of science is to develop set of rules       desired value. This is rarely the case in cognitive science
(or principles or theories) which allow us to take a                 research, meaning that if we build models that to match this
particular real-world situation, analyze it by measuring             particular sample's distribution, then we run the risk of over-
certain aspects, create a model from the results of that             fitting to that particular situation, and thus producing models
analysis, and then use that model to produce accurate                which do not generalize.
predictions as to other aspects of that situation (such as its           The usual method for addressing this involves basing our
behaviour into the future). For the purposes of this paper           comparisons on confidence intervals. However, standard
we will not detail Giere's conclusion that this approach to          approaches to confidence interval estimation are
the philosophy of science results in all of the features we          parametric: they make certain assumptions about the overall
want science to have (see Giere, 1988 and Giere, 1999),              distribution of the data (such as it being Gaussian). Instead
such as producing explanations as well as predictions.               of making this assumption, we use the bootstrap method
    This model-based science is an alternate way of                  (Davison and Hinkley, 1997), which is known to be non-
describing the standard scientific approach, and one which           parametric, and thus leads to more accurate confidence
leads to direct methodological solutions to the afore-               intervals for a non-normally distributed data. It should be
mentioned problems common to cognitive modelling                     noted that this technique allows for a confidence interval for
research.                                                            any measure, including such computationally intractable
                                                                     ones as the median.
Measurement Techniques
                                                                              Table 3: 95% Bootstrap Confidence Intervals
The key question for determining the appropriateness of a
                                                                                                Mean:      1.4375 – 2.5625
model in physics or in cognitive science is whether its
predictions match those of the real situation being modelled.                                Median:             1–3
This match is performed by measuring some aspect of the                         Standard Deviation:          0.696 – 1.541
real world, and measuring some aspect of the model, and                                         Skew:       -0.612 – 1.534
comparing the two. The usefulness of the model is                                           Kurtosis:        1.472 – 5.236
measured by how closely its predictions match the observed
situation. In physics, much of the time these measures are               The confidence intervals give us a more accurate
highly non-variant: repeated measurements yield results              description of what is know about the real situation we are
similar to many decimal places.                                      trying to model. We can also use them on the data produced
    However, in cognitive science, we do not have the luxury         by the model, saving us from having to run the model
of only studying phenomena of low variability. Instead, our          thousands of times before we can trust its statistics are
repeated measures of either the real data or the model data          representative.
may look more like those in Table 1.                                     However, we must be careful in applying these
                                                                     confidence intervals in this situation.             As discussed
                 Table 1: A set of measurements                      previously, if we cannot say that if the confidence intervals
                           Sample Data                               of the real-world and model data overlap, then the model is
                   1        2        2          3                    good. Instead, we will make use of the relatively unknown
                   2        1        0          5                    statistical tool called equivalence testing.
                                                                         Equivalence testing is a technique used in the evaluation
                   2        3        3          2
                                                                     of drug treatments to determine if a new, cheaper drug is as
                   1        1        1          3
                                                                     effective as some other drug, to within some pre-defined
                                                                     range. This is a modified version of the standard t-test,
    Given this set of real, measured data, we want our
                                                                     where instead of the traditional null hypothesis that the
models to produce statistically equivalent data. In other
                                                                     means of two groups are equal (μr-μm=0), the null
words, a good model should produce data with the same
                                                                     hypothesis is that the difference between the means is
statistical distribution as we find in the real world. It should
                                                                     greater than some amount (|μr-μm|>θ). The value of θ
be noted that this is exactly what mathematical models in
                                                                     defines the range of acceptable results. If we perform this
quantum physics do. To examine the distribution, we can
                                                                     statistical test, using μr as the real data set, and μm as the data
make a number of different statistical measures (Table 2).
                                                                     from a given model, then a p-value less than 0.05 allows us
                                                                 818
                                                                 821

to conclude with 95% certainty that the model and the real            blood oxygenation levels as well as reaction times and error
system do not differ by more than our threshold, θ. This              rates (Anderson et al., 2005). This broad applicability is a
approach can also be applied to ensuring that other                   key criteria in physics for accepting a model as an
statistical measures are also statistically indistinguishable.        explanation, not merely a predictive tool.
   The above description is intended for situations where                 To attain a single value which expresses the model error
we have some pre-determined threshold in mind, and we are             across multiple predictions, we should avoid tools like the
looking for models that are at least that close. This                 root mean squared error. This gives us a measurement
generally not something that is available when first                  indicating how much each measurement, on average,
developing a model for a situation. In these cases, instead           deviates from the mean. However, this can (and often does)
of setting the threshold and determining the p-value, we can          obscure situations where one measurement significantly
instead set the p-value and determine the required threshold.         differs. If we are looking for predictive models across that
This gives us a statistical measurement which has an                  set of measurements, then we should be more interested in
intuitive interpretation. If we get a value of 0.1, then we are       the worst the model does at predicting, not its average fit
95% certain that this model produces data that differs from           across the particular measures chosen. For these reason, we
the real data by no more than 0.1.                                    advocate combining equivalence test threshold measures by
   To demonstrate this alternate approach, consider the data          taking the maximum error, rather than the average error.
shown in Figure 1. Here we have two sets of data and 95%
confidence intervals for each. Under the assumption that              Fitting Techniques
the actual value for each measurement is within the                   Mathematical modelling also provides us with a mechanism
confidence interval, the maximum difference between the               for addressing the parameter fitting problem. The models
two would occur when the left-hand measurement is at the              developed for cognitive science will generally have multiple
top of its range, and the right-hand measurement is at the            parameters. However, we shall see, the same is also true for
bottom. The difference between these values is the                    mathematical models in physics, and there is a standard
threshold for which the equivalence test would give a                 methodology for working with such situations that we can
p<0.05 significance level. In other words, we can say that            adopt for use in cognitive science. To demonstrate this, we
the model and the data are statistically significantly similar        consider the mathematical formulation of Newton's Theory
to within that range.                                                 of Universal Gravitation.
                                                                                                   G⋅m 1⋅m 2
                                                                                              F=          2
                                                                                                        d
                                                                          This formula tells us how to predict what force will be
                                                                      applied to an object by gravity, given the distance between
                                                                      them (d), their masses (m1 and m2), and the universal
                                                                      gravitational constant (G). Expressing this in a model-based
                                                                      manner, we can say that this theory lets us take a particular
                                                                      situation (with known masses and distances) and create a
                                                                      predictive model for that situation.
                                                                          The important point here is that for 120 years after the
                                                                      development of this theory, the values of G, m1, and m2
                                                                      were not known. Instead, physicists would combine the
                                                                      three values into a single parameter (X). Then, they would
       Figure 1: Real and model data with 95% confidence              determine what value for this combined parameter best fit
        intervals. The equivalence test threshold is the              the particulars of a given situation.
  maximum difference that could occur between the model                   For example, if the theory was being used to predict the
     data and the real data, assuming they are both within            influence of the Sun on Jupiter, they did not need to know
           their respective 95% confidence intervals.                 the mass of either. Instead, scientists observed the path of
                                                                      Jupiter for a short period of time, determined the force
   This gives us a measurement of the correspondence                  applied by the Sun that would be required to result in such a
between the model and the real world. Unlike measures                 path, and then determined for what value of X the model
such as R2, this is a directly interpretable measure. It tells us     would give the same result. Once determined, it could then
how close we can expect this model's predictions to be.               be used in all future predictions of the gravitational
Most importantly, it takes into account sampling error. It is         influence of the Sun and Jupiter. The model parameters
this measurement that we believe should be the standard for           were fit to one situation, then applied to other situations.
modelling in cognitive science.                                           Just as the gravitational theory was useful for the first
   It is generally the case that we wish to make more than            120 years before the parameter G was known, so too can
one prediction from a model. Usually, we have multiple                computational theories which do not specify the parameters
data points from the real world, and we wish to see how               of their models. In these situations, the application of the
well our model matches for all of these points, not just one.         theory requires some process whereby the parameter can be
We also generally want to have predictions in multiple                determined for the situation in question. Once this process,
domains, such as recent work extending ACT-R to predict               which usually involves using some subset of the known
                                                                  819
                                                                  821

information and finding the best-fitting parameter setting, is      Communication Techniques
complete, the model can then be used to predict other               In mathematical modelling, a complete representation of the
aspects of the behaviour of that particular cognitive agent in      model is presented within the relevant publication. The
that particular situation. That is, we perform parameter-           models are specified using the language of mathematics.
fitting to create a model of this special case, and then can        This same language is also used to define how the models
use that model to perform predictions. Importantly, it is this      are meant to be used (i.e. how to convert a real-world
second stage which is the real test. Merely finding a               situation into a model, and how to perform the statistical
parameter setting which fits does not inform us as to the           analysis of the resulting predictions).
veracity of the theory. By taking the further step of using            However, computational models are generally
the model to predict, we avoid the problems raised above by         significantly more complicated than can be described in a
Roberts and Pashler (2000).                                         journal publication. Furthermore, we need to provide
     A more detailed theory, however, may indicate particular       source code not only for the model, but also for the
values for certain parameters. Once a value for G was               complete simulation and data analysis. To achieve parity
included within the theory of gravitation, it could be used in      with the mathematical approach, it should be possible to
new situations without the stage of first customizing the           take any published paper, get access to the complete source
model. Well-developed theories can specify a particular             code, run it, and have a complete replication of all the
value for a parameter, or can indicate a range of values,           results and graphs from the paper.
meaning that the model will be suitable no matter where in             Having such source code available simplifies the task of
that range the parameter is set (determined by a process akin       other researchers who wish to work with multiple sorts of
to that of sensitivity analysis). These established parameter       models. Furthermore, having access to all of the raw data
settings become part of the theory, rather than re-fitting the      (both from the modelling results and from the real-world
values to each individual circumstance.                             comparisons) resolves the problem of space limitations in
     It is also vital to observe that parameters need not be        publications. Within the publication, the researcher can
merely numerical values. There is no reason why a theory            present those aspects of the data which they believe best
might not treat an entire sub-module as a parameter. A              demonstrates the capabilities of a model, and any reader
particular component within a model might be implemented            interested in other aspects can do so separately.
in a number of qualitatively different ways. In this case, a
simple theory might say that we would have to fit the model
to a given situation by finding an implementation of that                          Modelling Software Suite
component which gives a match to some aspects of the real-          The limitations and techniques described in this paper are
world behaviour (just as was true for the numeric                   not new. Numerous researchers have already highlighted
parameters). If the resulting model can then be used to             the problems with standard modelling approaches, and
predict other aspects of its behaviour, then we have a useful       provided their own suggestions for solving them (e.g.
theory. However, a more developed model might specify               Ohlsson, 1988; Simon & Wallach, 1999; Humphreys &
exactly what sort of implementation, or it might specify that       O'Leary, 2002). While our approach does provide a novel
any one of a variety of implementations could be used and           justification via comparison to mathematical modelling in
still produce accurate results.                                     physics, we have extended this by developing an extensive
     It should be noted that there is no strict distinction         suite of software which supports the complete modelling
between numerical and non-numerical parameters. Indeed,             process. The goal is to reduce the effort required to perform
it is always possible to build models with parameters which         the broad analysis the we recommend, involving non-
function as if they adjust between two different                    standard measures, varying parameters and model types,
implementation systems. Furthermore, this can also occur            and communicating the complete model and results.
unexpectedly (see Sibley & Kello, 2004 for an example).                The first toolkit is an extended library of computational
This means that exploring different model implementations           models, all built to be simple to use and inter-compatible.
is as important as exploring different parameter settings. In       This software suite is designed to appeal to a broad
general, we should not be content with the current status-          audience, and is the basis of both all of our modelling
quo of working with one particular computational model.             research and a graduate cognitive science course in
Instead, we need to have a variety of models (each with a           computational modelling (Stewart, 2004). It includes
variety of parameter settings). For each of these models, we        Cellular Automata, Genetic Algorithms, Evolutionary
can perform the equivalence testing method described                Strategies, Multi-Layer Perceptrons and Back-Propagation,
previously, resulting is a numerical indication of the match        SRNNs, Kohonen Maps, ART, Q-Learning, and a re-
between each of these models and the real world.                    implementation of ACT-R (Stewart & West, 2006). This
     Generally, it is expected that we will find a large set of     tool set has been used by students with no previous
parameter settings which have highly similar equivalence            programming background to replicate foundational
test thresholds. The same will be true about qualitatively          modelling research, and is equally suitable for experienced
different models. Instead of choosing the one closest match         programmers and researchers.
(which will generally be a case of over-fitting), we should            One unique feature of this software library is that it was
report what set of parameters and implementations result in         not designed to be computationally efficient. Instead, the
equivalently predictive models. Further research can then           primary design criteria was simplicity and clarity of use.
discover what those successful models have in common.               This results in a faster development cycle, at the expense of
                                                                820
                                                                821

longer run times. We feel that the advantage of spending                              Acknowledgments
significantly less time developing is worth even an order of
                                                                  Funding was provided by Dr. Robert L. West's Carleton
magnitude increase in required computing time. For similar
                                                                  Cognitive Modelling Lab via a grant from the Natural
reasons, the library is written in the Python programming
                                                                  Sciences and Engineering Research Council of Canada.
language, which lists as one of its founding principles
“Correctness and clarity before speed”. For an example of
the use of this system, see (West et al., 2005), where we                                  References
compare mathematical, ACT-R, ANN, SRNN, and Q-                    Anderson, J. R., Albert, M. V., Fincham, J.M. (2005)
Learning models of human game playing.                              Tracing Problem Solving in Real Time: fMRI Analysis of
   The second toolkit is an Web-based system that supports          the Subject-Paced Tower of Hanoi. Journal of Cognitive
all of the aforementioned methodological steps we believe           Neuroscience, 17 1261-1274.
are required for effective modelling. The researcher writes       Axelrod, R. (2005). Advancing the Art of Simulation in the
a single program which performs a single run of their               Social Sciences. In Handbook of Research on Nature
simulation (with fixed parameter settings). Internal values         Inspired Computing for Economy and Management, Jean-
and final outputs are marked with a simple assignment               Philippe Rennard (Ed.).Hersey, PA: Idea Group.
statement, which enables the analysis software to identify        Beaulieu-Prévost, D. (2006). Statistical decision and
the relevant data. This can be done in any programming              falsification in science: going beyond the null hypothesis.
language, or using existing modelling tools, although we            In Hardy-Vallée, B., (ed), Cognitive Decision-Making:
focus on models developed using our model creating toolkit.         Empirical and Foundational Issues. Cambridge:
For such models, the software also provides trace                   Cambridge Scholars Press.
information about the changes in values over time during          Connell, L. and Keane, M. (2006). A model of plausibility.
the simulation run. We find this to be a valuable debugging         Cognitive. Science, 30 (1), 95-120.
and interpretation tool.                                          Davison, A.C. and Hinkley, D.V. (1997). Bootstrap
   Once the simulation has been run once, the source code           Methods and Their Application. Cambridge University.
is automatically stored. The system can be told to run the        Erev, I. and Barron, G. (2005). On Adaptation,
simulation multiple times, and record the results of each run       Maximization, and Reinforcement Learning Among
(for space reasons, the changes in values over time within          Cognitive Strategies. Psych. Review, 112(4), 913-931.
the model during an individual run are not recorded).             Giere, R. (1988). Explaining Science: A Cognitive
Descriptive statistics, including bootstrap confidence              Approach. Chicago: University of Chicago Press
intervals, are automatically calculated. The simulations are      Giere, R. (1999). Science without Laws. Chicago:
distributed to multiple computers running a small client            University of Chicago Press.
program. This is suitable for running on any computer with        Humphrys, M. and O'Leary, C. (2002). Constructing
Internet access, without requiring administrative privileges.       complex minds through multiple authors. From Animals
The parameters within the model are also identified, and one        To Animats 7: (SAB-02).3-12.
can set up batch simulation runs which exhaustively vary          Guillot, A. and Meyer, J.A. (2000). From SAB94 to
parameter settings across given settings.                           SAB2000 : What’s New, Animat? From Animals to
   While the results of this process can be extracted for use       Animats 6: (SAB-00). 3-12.
in existing statistical tools, there is also a facility for       Ohlsson, S. (1988) Computer simulation and its impact on
performing the similarity-based testing described in this           educational research and practice. International Journal of
paper. This includes both the determination of equivalence          Educational Research, 12, 5-34.
test threshold differences between individual parameter           Roberts, S. and Pashler, H. (2000). How persuasive is a
settings and real-world results, as well as identifying             good fit? A comment on theory testing. Psychological
parameter ranges with equivalent results. The system also           Review. 107 (2), 358-367.
produces publication-quality graphs, including contour plots      Sibley, D. E. and Kello, C. T. (2004). Computational
detailing the effects of parameter variation.                       explorations of double dissociations: Modes of processing
   Since the entire system is run through an Internet-based         instead of components of processing. Cognitive Systems
interface, the same analysis facilities provided to the             Research, 6, 61-69.
researcher are also automatically provided to the community       Simon, H. and Wallach, D. (1999). Cognitive modeling in
at large. By merely leaving the core interface program              perspective. Kognitionswissenschaft, 8, 1-4.
running (preferably on a lab server), others have complete        Stewart, T.C. (2004) Teaching Computational Modelling to
access to the research. This includes the ability to explore        Non-Computer Scientists. 6th International Conference on
the existing data set, or to download a copy of the                 Cognitive Modelling.
simulation code and to generate their own investigations of       Stewart, T.C. and West, R. L. (2006) Deconstructing ACT-
the model, exploring alternate parameter settings, model            R. 7th International Conference on Cognitive Modelling.
changes, or alternate comparative measures.                       West, R., Stewart, T.C., Lebiere, C., and Chandrasekharan,
   All of this software runs equally well on any modern             S. (2005) Stochastic Resonance in Human Cognition:
operating system, and all source code is released under the         ACT-R vs Game Theory, Associative Neural Networks,
GNU General Public License. The complete system,                    Recursive Neural Networks, Q-Learning, and Humans.
including ongoing research examples, is available at                27th Annual Meeting of the Cognitive Science Society.
<http://ccmlab.ca/ccmsuite.html>.
                                                              821
                                                              821

