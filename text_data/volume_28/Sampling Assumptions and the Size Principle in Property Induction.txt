UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Sampling Assumptions and the Size Principle in Property Induction
Permalink
https://escholarship.org/uc/item/4dp5v0jm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Author
Fernbach, Philip M.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

             Sampling Assumptions and the Size Principle in Property Induction
                                    Philip M. Fernbach (philip_fernbach@brown.edu)
                          Brown University Department of Cognitive and Linguistic Sciences, Box 1978
                                                      Providence, RI 02912 USA
                            Abstract                                 accounts of the phenomena that have been studied but their
                                                                     scope is limited.
  The ‘size principle’ emphasized in recent Bayesian models of          On the other hand are models which come from the
  inductive generalization (Kemp & Tenenbaum, 2003; Sanjana          tradition of rational analysis (Anderson, 1990). These
  & Tenenbaum, 2003; Tenenbaum & Griffiths, 2001) is tested
                                                                     models seek to identify the computational problem the
  in the domain of property induction. As predicted by the
  model, the size principle is obeyed more frequently when a         system is trying to solve and propose an optimal solution. A
  strong sampling assumption is explicit than when sampling is       common theme of the models of this type has been that they
  weak or unspecified, but it is not followed consistently. This     utilize the tools of Bayesian statistics. Heit (1998) was the
  implies that, although people are sometimes sensitive to           first to apply the Bayesian framework to property induction,
  sampling assumptions as specified by the Bayesian                  providing an account of several of the empirical phenomena.
  framework, models based on a strong sampling assumption            More recently a family of models has emerged that has
  may not provide general accounts of property induction.            advanced the project by introducing a principled likelihood
                                                                     calculation based on sampling assumptions, proposing a
                        Introduction                                 general method for generating a hypothesis space and
From the time of their introduction in the 16th century until        attempting to ground the prior distribution in domain
general acceptance in the 18th century, tomatoes were not            specific theories (Kemp & Tenenbaum, 2003; Tenenbaum
widely eaten in Mediterranean Europe. Due to their                   & Griffiths, 2001; Sanjana & Tenenbaum, 2003). I refer to
morphological resemblance to the nightshade plant, they              this family of models collectively as the Sampling Sensitive
were assumed poisonous and avoided. This is a striking —             Bayesian (SSB) model.
if unsuccessful — example of a common strategy that                     In this paper I evaluate the approaches by testing a key
human beings utilize to make sense of the world: to induce           prediction of the SSB model. The SSB model relies on a
possession of a property by one category from another.               likelihood calculation that is dependent on assumptions
Human beings perform these inductions naturally, often               about how the data are sampled. This implies that argument
from few examples and even between categories that are               strength judgments should be sensitive to variations in
prima facie dissimilar. Understanding the nature of this type        sampling procedures. Specifically, the SSB model generally
of inference is a major project of cognitive science.                assumes that human property induction is best described
  Empirical studies of property induction have been                  using strong sampling (Kemp & Tenenbaum, 2003; Sanjana
conducted since the mid 1970s using argument strength                & Tenenbaum, 2000) an assumption that examples are
ratings for premise-conclusion pairs and blank predicates.           drawn at random from the set of objects to which the
Figure 1 provides an example.                                        predicate applies. This assumption leads to a ‘size
                                                                     principle,’ a preference for smaller hypotheses over larger
                     Salmon have sesamoid bones                      ones given data that is consistent with both. If the size
                     Lions have sesamoid bones                       principle holds, then given more examples within a range,
                                                                     participants should be less willing to generalize outside of
Therefore            Polar Bears have sesamoid bones                 that range, a tendency that should manifest itself as an
                                                                     inductive non-monotonicity whereby adding similar
Figure 1: Property induction argument                                premises decreases property induction to a dissimilar
                                                                     conclusion. This phenomenon is not predicted by either
  A host of phenomena have been observed concerning how              Osherson et al’s (1990) Similarity/Coverage (SC) model or
people evaluate the strength of such arguments (reviewed by          Sloman’s (1993) Feature-Based (FB) model as neither of
Sloman & Lagnado, 2005). Several models have been                    these models is sensitive to variations in sampling
proposed to account for the phenomena. The models fall               procedures.
into two camps and as such are revealing of a deeper                    To identify the source of this divergence I first present the
methodological divergence in cognitive science. On the one           similarity and feature-based models. I then present the SSB
hand are models directly motivated by empirical work and             model and show how the size principle emerges from the
aimed at the level of algorithm (Marr, 1982). The two best           model and why it does not apply to other models. Lastly I
known were proposed in the 1990s (Osherson et al., 1990;             present an argument preference experiment designed at
Sloman, 1993) and rely on notions of similarity and feature          testing for the size principle and discuss the implications of
matching. These models tend to be good explanatory                   the results.
                                                                 1287

Similarity and Feature-Based Models                               the posterior probabilities of the hypotheses to which the
                                                                  conclusion belongs. Thus, an argument is considered strong
Similarity/Coverage The SC model is based on overall              to the extent that the conclusion is a member of hypotheses
similarity judgments and assumes static category structure.       that have high posterior probability of corresponding to the
Argument strength is assessed based on three factors. The         concept. Note that unlike the SC and FB models,
first is the maximum similarity between the set of premise        generalization is not based on comparing the premise to the
objects and the conclusion. The second is the similarity of       conclusion directly, but rather is mediated by the concept
the premise set to all of the members of the immediately          corresponding to the true set of objects to which the
super-ordinate category that contains both the premise and        predicate applies.
the conclusion, referred to as coverage. The third is a free
parameter denoting the relative weight of similarity and          Hypothesis Space and Prior The goal in specifying a
coverage for an individual subject. Thus, an argument is          hypothesis space is to identify those groupings of objects
considered strong to the extent that the premise objects are      that humans would consider candidates for correspondence
similar to the conclusion and the premise objects cover the       with the concept. This is a difficult task since objects are
super-ordinate category. The model accounts for a wide            grouped differently based on domain. Sanjana and
variety of empirical phenomena and provides a strong match        Tenenbaum (2003) propose a similarity-based hierarchical
to human data (Osherson et al, 1990).                             clustering approach where clusters and unions of clusters
                                                                  supply the hypothesis space. This is a reasonable and
Feature-Based The FB model (Sloman, 1993) is expressed            computationally tractable solution but it is imperfect since
as an artificial neural network where input units represent       overall similarity ratings may often misrepresent the way
values over a set of features and the output unit represents      objects are grouped depending on the predicate. For
the presence or absence of some property. The activation of       instance, a bat might be considered more similar to a bird
the output unit is determined by two things: the feature          overall, but biologically more similar to a whale. The source
overlap of the premise and conclusion and the number (in          of the prior distribution is also a difficult issue as prior
the binary case) or the ‘magnitude’ (more generally) of           knowledge varies across domains and individuals. Sanjana
salient features possessed by the conclusion. Thus an             and Tenenbaum (2003) posit a prior distribution motivated
argument is considered strong if there is a great deal of         by rational principles such as preference for simplicity and
overlap of the features of the premise and conclusion and         recent work has attempted to ground the prior in domain-
the conclusion has few additional features. The FB model is       specific theories (Kemp & Tenenbaum, 2003). Resolving
not based on a notion of similarity. To the extent that           these difficulties remains a major task for proponents of the
similarity is based on feature overlap the model’s feature        Bayesian framework.
matching rule may approximate similarity, but the basis for
generalization is featural, not similarity based. This is an      Calculating the Posterior Distribution Bayes’ rule
important distinction from the SC model which gives               specifies that the posterior distribution over hypotheses is
computational primacy to the similarity calculation.              proportional to the prior distribution multiplied by the
Nonetheless, the empirical predictions of the FB model are        likelihood of the data given each hypothesis
similar to the SC model. One important exception is that the
FB model never predicts non-monotonicity, that adding                                p(h d ) ∝ p(d h ) p(h)                 (1)
premises decreases argument strength, despite experimental
evidence for it.                                                  where p (h│d) is the posterior probability that a hypothesis
                                                                  (h) corresponds to the set of objects that belong to the
Sampling Sensitive Bayesian Model
                                                                  concept given some data (d), p(h) is the prior probability
The SSB model recasts the computational problem in                assigned to the hypothesis and p (d│h) is the likelihood of
statistical terms: Given data in the form of one or more          observing the data under that hypothesis.
premise objects that are examples of some concept, what is           Thus, given a hypothesis space and a prior distribution, all
the probability that the conclusion object also belongs to        that is needed to calculate the posterior distribution is the
that concept? The concept corresponds to the full set of          likelihood, the probability distribution over hypotheses of
objects to which the predicate applies (e.g. animals that         observing the data given that each hypothesis is true and
have sesamoid bones).                                             therefore corresponds to the concept. Calculation of the
   The solution is offered by Bayes’ rule which stipulates        likelihood depends on how the data are sampled. Two
how to update hypotheses in the light of data. The first step     sampling procedures are considered, weak and strong. Weak
is to identify a hypothesis space, a set of groupings of          sampling implies that an example is sampled independently
objects which could conceivably correspond to the concept.        of the concept and then given a label specifying whether or
A prior distribution is assigned to the hypotheses which          not it belongs to the correspondent set of objects. Given
denotes how likely each hypothesis is a priori. Next, using       weak sampling and data in the form of positive examples of
Bayes’ rule, the probability distribution is updated to take      the concept, the likelihood of observing the data given a
into account the data. Finally the status of the conclusion       hypothesis is 1 if the example belongs to the hypothesis and
object with respect to the concept is calculated by adding up
                                                             1288

0 otherwise and as such is a binary indication of whether the        hypothesis were true he would have had a one out of five
example is consistent with the hypothesis.                           chance of observing a lion, whereas if the larger were true,
                                                                     only a one out of twenty chance.
                           ⎧1 if d ∈ h                      (2)      Size Principle Given the strong sampling assumption the
                p ( d h) = ⎨
                           ⎩0 otherwise                              likelihood of a hypothesis decreases exponentially in
                                                                     proportion to its size as new examples are encountered. As
   To clarify why this is so, take the scenario of a scientist       posterior probability is proportional to likelihood, this
trying to identify the group of animals that have property           means that smaller hypotheses are favored over larger ones
‘X’. To simplify the example suppose that there are twenty           given data that are consistent with both (assuming that the
types of animals in the world, and one example of each               larger hypothesis is not strongly favored a priori).
type. All of the animals are vertebrates, but just five are          Intuitively, this reflects that given random sampling from
mammals. Also assume that the scientist’s hypothesis space           the concept, seeing a number of examples consistent with a
consists of just two hypotheses, that all vertebrates have           smaller hypothesis would be a coincidence if the larger
property ‘X’, and that just mammals have it. To test these           hypothesis were true and that the psychological plausibility
hypotheses, he chooses an animal from the set of twenty at           of this coincidence decreases exponentially as more
random and tests it for the property. The animal that he             examples are added that are consistent with the smaller
selects happens to be a lion and he finds that it does indeed        hypothesis.
have property ‘X’. This is an instance of weak sampling.                To see how this bears on induction we return to the strong
The data consists of an example (lion) and a label (‘has             sampling scenario in which there are twenty animals, five of
property ‘X’’) and the probability of the data is irrespective       which are mammals. If the scientist observes several
of the size of the hypothesis. If the larger hypothesis is true      examples and each is a mammal, Bayes’ rule indicates that
and all vertebrates have property ‘X’, the probability that          he should give high posterior probability to the smaller
the lion would be observed to have property ‘X’ is 1.                hypothesis relative to the larger one. The probability of
Likewise, since lions are mammals, if the smaller                    inducing a property to a conclusion is the sum of the
hypothesis is correct, the probability that the lion would be        posterior probabilities of the hypotheses of which the
observed to have property ‘X’ is also 1. So given weak               conclusion is a member. Since the non-mammals are only
sampling and data that is consistent with both hypotheses,           members of one hypothesis and it has a low posterior
the likelihood does not favor one over the other.                    probability, the scientist should be reluctant to generalize
   Strong sampling implies that the example is sampled at            the property to the non-mammals. This tendency holds for
random explicitly from the set of objects to which the               scenarios with more realistic hypothesis spaces and sets of
predicate applies. This makes the data more informative              objects and can be stated more generally as an
about the nature of that set since the probability of                unwillingness to induce a property to a conclusion outside
observing that example is tied to the size of the hypothesis.        of a range given examples within that range.
Specifically, the likelihood is the inverse of the size of the          In property induction tasks the size principle should
hypothesis if the data is consistent with the hypothesis and 0       manifest itself as a non-monotonicity whereby adding
otherwise. In the case of more than one example drawn                similar examples decreases the strength of an argument
independently, the likelihood becomes the inverse of the             whose conclusion is dissimilar. This phenomenon is not
size of the hypothesis raised to the number of examples              predicted by either the SC or FB models. The SC model
                                                                     predicts non-monotonicity only in the case where adding a
                           ⎧ 1                                       premise changes the super-ordinate category which is not
                           ⎪        if d ∈ h                (3)      the case for premises within a range and a conclusion
                p ( d h) = ⎨ h n
                           ⎪⎩ 0    otherwise                         outside of that range. The FB model never predicts non-
                                                                     monotonicity because adding premises can neither decrease
                                                                     feature overlap nor decrease the magnitude of the
where │h│ is the number of objects in the hypothesis and n           conclusion. More generally, the Bayesian model is sensitive
is the number of independently drawn examples. .                     to sampling because the sampling procedure determines
   As an example of strong sampling imagine a slightly               what can be inferred about the relationship between the
different scenario. The scientist is assessing the same two          hypothesis space and the concept. Since the FB and SC
hypotheses but this time the animals have already been               models do not rely on the notion of a hypothesis space nor
tested for the property and those possessing it placed               of a concept, phenomena that stem from variations in
together in a room. The scientist is allowed to randomly             sampling assumptions cannot be accommodated by the
observe one animal from the room at a time. The first                theories.
animal that he observes happens to be a lion. This is an
example of strong sampling. The example is sampled at                Bayesian Model Claims It is helpful to view the assertion
random from the set of objects that possess the property.            of a size principle in the SSB model as amounting to two
Unlike under weak sampling, the probability of the data is           related claims. The first claim is that judgments of argument
not the same for the two hypotheses. Rather, if the smaller          strength should be sensitive to sampling procedure. This
                                                                1289

claim is essential to the Bayesian model as it does not rest     consisted of two arguments, labeled ‘A’ and ‘B’, and
on a specific modeling assumption but on the general             participants had to choose which was stronger on a scale of
framework of the model.                                          one to seven. The arguments consisted of premises that
   The second claim is that a strong sampling assumption is      attributed some blank biological property to one, two or
appropriate for describing the property induction task at        three animals and a conclusion attributing that property to
issue. The status of this claim is controversial. Heit (2001)    raccoons. For the test items one argument contained three
maintains that a non-monotonicity associated with adding         premises which were all animals very similar to one another
premises is inconsistent with many of the findings in the        and the other contained just one of those animals.
literature; however, there is some evidence in favor of this
phenomenon. Sanjana and Tenenbaum (2003) report a non-             A.                                 B.
monotonic effect consistent with the size principle in a               Lions secrete uric acid             House Cats secrete uric
                                                                       crystals                            acid crystals
property induction experiment, but the effect is small and
their experiment departs from the standard property                                                        Lions secrete uric acid
induction paradigm in several ways. Medin et al. (2003)                                                    crystals
also predict non-monotonicity with addition of similar                                                     Tigers secrete uric acid
premises but without relying explicitly on sampling. Rather,                                               crystals
they hypothesize that participants assume that the premises
are chosen so as to be informative of the category to which            Raccoons secrete uric               Raccoons secrete uric
                                                                       acid crystals                       acid crystals
the predicate pertains. Adding premises that share a property
will increase the association between that property and the           1          2         3    4        5           6         7
predicate and weaken an argument if the conclusion does
not possess the property. This is similar to the notion of         Strongly Prefer A                              Strongly Prefer B
strong sampling in that it implies that the experimenter
purposefully chose the premises from the subset of objects        Figure 2: Example argument strength scenario
to which the predicate applies. Medin et al. do report some
                                                                     Instructions were varied across groups to reflect different
non-monotonicities from adding premises, but the
                                                                  sampling assumptions. One group was given instructions
phenomenon does not hold across all their test items.
                                                                  that implied strong sampling, one weak sampling and one
                                                                  group was given ambiguous instructions. For the ambiguous
                        Experiment                                group the instructions replicated the ones used in Osherson
To evaluate the two claims of the Bayesian model I                et al’s (1990) original study and asked participants to rate
conducted an experiment aimed at identifying if and when          which argument was stronger assuming that the statements
the size principle is manifested in property induction. I         above the lines were facts and those below the lines
asked participants to choose between one-premise and three-       conclusions that follow from those facts.
premise arguments given different cover stories which                For both strong sampling and weak sampling groups, a
implied different sampling procedures.                            cover story was used to explain how the facts were
   The first claim was tested by contrasting judgments of         generated. The story involved two students each writing a
groups given either weak sampling or strong sampling              paper about raccoons. Premises described facts the the
instructions. According to the SC and FB models, changing         students had uncovered in their research and the conclusion
sampling assumptions should not alter judgments of                was the statement about raccoons that they were going to
argument strength. The SSB model, however, is sensitive to        put into the paper. Participants were asked to rate which
sampling and predicts non-monotonicities with strong              student was more justified in putting the conclusion into
sampling but not with weak sampling. To evaluate the              their paper.
second claim, the third group was given instructions that            The difference between the instructions for the strong
were vague about the sampling procedure as in the                 sampling and weak sampling groups concerned the way the
conventional task, enabling insight into people’s default         students conducted their research. In the strong sampling
assumption.                                                       condition, the research was described as follows: “Albert
                                                                  and Bob both used the same book for the research, a book of
Method
                                                                  biological facts. Each section of the book covers some
Participants Participants were 41 Brown University                property (e.g. animals with Sesamoid bones) and each page
graduate and undergraduate student volunteers assigned            of the section contains a picture of an animal with that
randomly to three groups, 14 to the ambiguous condition, 13       property. Albert and Bob got their facts by randomly
to weak sampling, and 14 to strong sampling.                      flipping to pages in the appropriate section. So for example
                                                                  if Albert says that Lions have Sesamoid bones and Wolves
Procedure All participants were given          the same 10
                                                                  have Sesamoid bones, he knows that because he looked in
scenarios to judge. Four were test items       and six were
                                                                  the section on Sesamoid bones and then flipped at random
dummy scenarios created randomly to           eliminate any
                                                                  to a page and found a picture of a lion and then flipped at
demand characteristic. As in Figure 2,        each scenario
                                                                  random to another page and found a wolf.” In the weak
                                                             1290

sampling condition, the research was described as follows:          one-premise arguments than did the weak sampling or
“Albert and Bob got their facts by choosing an animal and           ambiguous groups and this result was significant (Figure 3).
finding out if that animal had some property (e.g. Sesamoid         Pairwise t-tests indicated a statistically significant difference
bones) and then choosing another animal and checking for            between the strong sampling group and the weak sampling
the property and so on. The number of facts in each scenario        groups (t=2.68, p<.01), and between the strong sampling
represents how many animals they looked at in that                 and ambiguous groups (t=3.18, p<.01), but no difference
scenario. So for example, if Albert says that Lions have           between the ambiguous and weak sampling groups.
Sesamoid bones and Wolves have Sesamoid bones, he
knows that because he first looked at lions and found they             7
had Sesamoid bones, and then chose to look at Wolves and
found that they too had Sesamoid bones. Please note that in
                                                                       6                             4.8                4.9
a particular scenario the animals listed are the only ones that        5          4.1
were looked at.”                                                       4
Model Predictions The SC and FB models make no                         3
assumptions about sampling so make the same predictions                2
regardless of instructions. The SC model predicts that three-
premise arguments (option A in Figure 2) should be judged
                                                                       1
                                                                                Strong              Weak           Ambiguous
stronger than the one-premise arguments (option B in Figure
                                                                              Sampling           Sampling
2) because the premises were chosen so that the three-
premise arguments had higher similarity and coverage than
the one-premise arguments. In the case of the FB model, the        Figure 3: Average preference for three-premise arguments
three-premise arguments should be judged stronger or               on a 1-7 scale. A score of 7 implies that the three-premise
approximately the same. Adding premises so close within a          argument was strongly preferred; a score of 1 implies that
range may not increase feature overlap measurably, but the         the one-premise argument was strongly preferred and a
one-premise arguments should never be favored. The SSB             score of 4 implies that the one-premise and three-premise
model predicts that one-premise arguments should be                arguments were judged equally strong.
favored in the case of strong sampling due to the size                Though the strong sampling group showed a greater
principle. For weak sampling, three-premise arguments              preference for the one-premise arguments than the weak
should be slightly favored because certain hypotheses, such        sampling or ambiguous groups, all groups showed an
as the one-premise animal alone (e.g. just lions secrete uric      overall preference for three-premise arguments. In other
acid crystals), have been eliminated increasing the posterior      words, most participants regardless of group failed to
distribution of hypotheses that include the conclusion. There      display the size principle. Participants in the strong
is no prediction inherent in the Bayesian framework for the        sampling group preferred the one-premise arguments 30%
ambiguous condition as the model can be accommodated to            of the time versus 43% for the three-premise arguments.
weak or strong sampling depending on how the instructions          Weak sampling and ambiguous groups behaved similarly to
are interpreted. Kemp & Tenenbaum (2003) and Sanjana &             one another preferring the three-premise arguments
Tenenbaum (2003) assume strong sampling, but the SSB               approximately 73% and 66% of the time respectively
model does not necessitate that assumption.                        (Figure 4).
Table 1: Prediction whether three-premise arguments or                   100%
one-premise arguments should be judged stronger for each
model across the three conditions.                                        75%         42.9%
                                                                                                       73.1%            66.1%
                  Ambiguous         Strong          Weak                  50%         26.8%
                                  Sampling        Sampling
   Similarity/     3-Premise      3-Premise       3-Premise               25%                          13.5%            23.2%
                                                                                      30.4%
   Coverage                                                                                            13.5%            10.7%
   Feature-        3-Premise      3-Premise       3-Premise                0%
   Based            or Equal       or Equal        or Equal                           Strong           Weak          Ambiguous
   Bayesian       Unspecified     1-Premise       3-Premise                         Sampling         Sampling
Results                                                                 One-Prem ise Preferred           Judged Equal
                                                                        Three-Prem ise Preferred
As predicted by the Bayesian Model, variation of sampling
assumptions yielded a statistically significant treatment          Figure 4: Percentage of scenarios in which three-premise
effect (one way ANOVA; F=5.97, p<.01). The direction of            arguments were judged stronger, one-premise arguments
the effect was also in line with the Bayesian model. The           were judged stronger and arguments were judged equally
strong sampling group displayed a greater preference for           strong across all three conditions.
                                                               1291

                         Discussion                              but for the types of stimuli used in this experiment, adding
                                                                 premises within a range did not generally weaken property
The majority of participants across all conditions displayed
                                                                 induction to a conclusion outside of that range.
a monotonic tendency given additional examples within a
range and a conclusion outside of that range. The exception                            Acknowledgments
was that participants gave a significant percentage of non-
monotonic responses in the strong sampling condition.             This work was supported by a Brown University Graduate
These results indicate that both types of models, the             Fellowship. Many thanks Tom Griffiths, Steve Sloman and
similarity and feature-based and the Bayesian, capture some       4 anonymous reviewers for valuable input.
aspect of property induction since sensitivity to sampling is                                References
predicted only by the Bayesian model, whereas a general
monotonic tendency fits better with the SC and FB models.         Anderson, J.R. (1990). The adaptive character of thought:
Both Heit (1998) and Kemp and Tenenbaum (2003) note an               Erlbaum.
empirical correspondence between the Bayesian model and           Evans, J. St. B. T. & Over, D. E. (1996). Rationality and
                                                                     reasoning. Hove: Psychology Press.
the similarity and feature-based models. They suggest that        Heit, E. (1998). A Bayesian analysis of some forms of inductive
the two types of models may be best viewed as                       reasoning. In M. Oaksford & N. Chater (Eds.), Rational models
complementary rather than competitive. The SC model, for            of cognition: Oxford University Press.
example, may be a heuristic-based approximation of the            Heit, E. (2001). What is the probability of the Bayesian model
SSB model as implemented in human beings while the SSB              given the data? Behavioral and Brain Sciences, 24, 672-673.
model provides a computational level explanation of why             (commentary on Tenenbaum & Griffiths (2001)).
the SC model should work. These results highlight an              Kemp, C., & Tenenbaum, J. (2003). Theory-based induction. Paper
empirical lack of correspondence and therefore imply a              presented at the Proceedings of the Twenty-Fifth Annual
somewhat different account.                                         Conference of the Cognitive Science Society.
                                                                  Marr, D. (1982). Vision. San Francisco: W.H. Freeman.
   There is a tension between positing a computational level      Medin, D. L., Coley, J. D., Storms, G., & Hayes, B. K. (2003). A
model and making empirical predictions. If the algorithm            relevance theory of induction. Psychonomic Bulletin and Review,
that the system uses only approximates the optimal                  10(3), 517-532.
computation then it does not follow that phenomena                Osherson, D. N., Wilkie, O., Smith, E. E., Lopez, A., & Shafir, E.
predicted by the computational level model should be                (1990). Category-based induction. Psychological Review, 97(2),
observed. Of course, the SSB model is only testable to the          185-200.
degree that it is committed to specific predictions. The SSB      Rehder, B. (2003). A causal-model theory of conceptual
model sets up the computation as a choice between                   representation and categorization. Journal of Experimental
hypotheses that is sensitive to sampling. While this                Psychology: Learning, Memory and Cognition, 29(6), 1141-
                                                                    1159.
characterization does provide new insights into the               Sanjana, N. E., & Tenenbaum, J. B. (2003). Bayesian models of
inductive process, people do not fully conform to it. Dual          inductive generalization. Paper presented at the Advance in
process theory (Evans & Over, 1996; Sloman, 1996;                   Neural Information Processes 15.
Stanovich & West, 2000) provides a possible solution. The         Shafto, P., Kemp, C., Baraff, E., Coley, J. D., & Tenenbaum, J.
size principle non-monotonicity may be attributable to an           (2005). Context sensitive induction. Paper presented at the
effortful rule-based reasoning process, whereas the more            Proceedings of the Twenty-Seventh Annual Conference of the
common monotonic response is a result of the intuitive              Cognitive Science Society.
process. According to this account, The SC and FB models,         Shepard, R. N. (1987). Toward a universal law of generalization
or some other relatively simple heuristic, represent an             for psychological science. Science, 237(4820), 1317-1323.
                                                                  Sloman, S. A. (1993). Feature-based induction. Cognitive
intuitive associative process that occurs effortlessly and          Psychology, 25, 231-280.
sometimes leads to counter-normative results. The SSB             Sloman, S. A. (1996). The empirical case for two systems of
model, in contrast, provides the computational level account        reasoning. Psychological Bulletin, 119, 3-22.
of how the rule-based system goes about integrating               Sloman, S. A., & Lagnado, D. A. (2005). The problem of
sampling information and other considerations into an                induction. In R. Morrison & K. Holyoak (Eds.), Cambridge
evaluation of candidate hypotheses. This would explain               handbook of thinking and reasoning. New York: Cambridge
both why some people were sensitive to the sampling                  University Press.
manipulation and why most people were not on the                  Stanovich, K.E. & West, R. F. (2000). Individual differences in
assumption that only a minority put in the required effort          reasoning: Implications for the rationality debate. Behavioral
                                                                    and Brain Sciences, 23, 645-726.
and had the necessary skills to solve the problem.                Tenenbaum, J. B. (2000). Rules and similarity in concept learning.
   The second claim evaluated in this paper, that strong            Paper presented at the Advances in Neural Information
sampling is appropriate for describing human property               Processing Systems 12.
induction, has little support in these data. Not only did few     Tenenbaum, J. B., & Griffiths, T. L. (2001). Generalization,
people follow the size principle given explicitly strong            similarity and Bayesian inference. Behavioral and Brain
sampling, but ambiguous instructions almost invariably              Sciences, 24, 629-640.
resulted in monotonicity. It may well be that strong
sampling is the default assumption for other inductive tasks,
                                                             1292

