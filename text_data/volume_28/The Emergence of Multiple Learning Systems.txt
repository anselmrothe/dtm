UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Emergence of Multiple Learning Systems
Permalink
https://escholarship.org/uc/item/0rv7d3hx
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Jones, Matt
Love, Bradley C.
Publication Date
2006-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                       The Emergence of Multiple Learning Systems
                                      Bradley C. Love (love@psy.utexas.edu)
                                        Matt Jones (mattj@psy.utexas.edu)
                    Consortium for Cognition and Computation, The University of Texas at Austin
                                                    Austin, TX 78712 USA
                          Abstract                               cumstances. For example, ATRIUM (Erickson & Kr-
                                                                 uschke, 1998) contains a rule and exemplar learning sys-
   Multiple learning systems models hold that separate           tem. Which system is operable is determined by a gat-
   learning systems, often organized around discrepant           ing system, allowing different classification procedures to
   principles, combine their outputs to support human cat-
   egorization. Rather than propose a complex model,             be applied to different parts of the stimulus space. For
   we adopt a complex systems’ viewpoint and propose             example, familiar items could be classified by the exem-
   that multiple learning systems emerge from a flexible         plar system whereas rules could be applied to unfamiliar
   and adaptive clustering mechanism’s interactions with         items. The power to apply qualitatively different pro-
   the environment. The model, CLUSTer Error Reduc-
   tion (CLUSTER), retains the flexibility characteristic        cedures to different stimuli is the hallmark of multiple
   of human learning by building knowledge structures as         systems models.
   needed to support a learner’s goals. Importantly, CLUS-          Proposing multiple systems begs the questions of how
   TER can apply ostensibly different procedures to dif-         many systems are present and how do they interact. Are
   ferent parts of the stimulus space, a hallmark of mul-
   tiple systems models. We describe a simulation of a           there two, three, or thirty-four systems? Do some sys-
   human learning study in which CLUSTER develops dif-           tems combine outputs whereas others shunt each other?
   ferent cluster representations for different item types.      These questions are not trivial to answer. For example,
   Rule-following items are captured by clusters that are        a two system model may suffice for one data set, but
   broadly tuned and focused on rule-relevant stimulus as-       a new manipulation could provide evidence for a third
   pects, whereas exceptions (especially those that violate
   high-frequency rules) are captured by narrowly tuned          system. As systems propagate, the complexity of the
   clusters that focus on item-specific stimulus qualities.      overall system dramatically increases. Building in this
   We end by considering the relation between CLUSTER            degree of complexity complicates model evaluation.
   and findings from the cognitive neuroscience of category         Instead of proposing a complex model of category
   learning.
                                                                 learning containing multiple systems, we advocate a
                                                                 complex systems approach to category learning model-
                      Introduction                               ing in which multiple learning systems emerge from a
                                                                 flexible and adaptive clustering mechanism’s interactions
   Proposals for category representation are diverse,
                                                                 with the environment. We evaluate the hypothesis that
ranging from exemplar- (Medin & Schaffer, 1978) to
                                                                 a relatively small set of learning principles can effectively
prototype-based (Smith & Minda, 1998) and include
                                                                 “grow” knowledge structures that satisfy the needs that
proposals between these two extremes (Love, Medin, &
                                                                 multiple systems models are intended to address.
Gureckis, 2004). Determining the best psychological
model can be difficult as one model may perform well             Past Work and Current Challenges
in one situation but be bested by a competing model in
a different situation. One possibility is that there is not      Previous work with the SUSTAIN model, which is the
a single “true” model.                                           precursor to the model that we introduce here, has par-
   In category learning, this line of reasoning has led to       tially delivered on the promise of flexibly building needed
the development of models containing multiple learning           knowledge structures. SUSTAIN is a clustering model
systems. These more complex models hold that category            that starts simple and recruits clusters in response to
learning behavior reflects the contributions of different        surprising events, such as encountering an unfamiliar
systems organized around discrepant principles that uti-         stimulus in unsupervised learning or making an error in
lize qualitatively distinct representations. The idea that       supervised learning (cf. Carpenter & Grossberg, 2003).
multiple learning systems support category learning be-          Surprising events are indicative that the existing clus-
havior enjoys widespread support in the cognitive neu-           ters do not satisfy the learner’s current goals and that
roscience of category learning (see Ashby and O’Brien,           the model should grow new knowledge structures (i.e.,
2005, for a review and Nosofsky and Zaki, 1998, for a            clusters). These clusters are modified by learning rules
dissenting opinion).                                             that adjust their position to center them amidst their
   Multiple system models of category learning detail            members. Dimension-wide attention is also adjusted to
the relative contributions of the component learning sys-        accentuate stimulus properties that are most predictive
tems. The relative contributions can depend on the cir-          across clusters.
                                                             507

   Although simple, these growth dynamics allow SUS-             justing specificity at the individual cluster level allows
TAIN to address a wide range of human learning data              for different criteria to be applied to different parts of
across various paradigms including unsupervised, infer-          the stimulus space, as in multiple systems models.
ence, and classification learning (Love et al., 2004). De-          The model that is introduced in the next section,
pending on the circumstances of the learning situation           CLUSTer Error Reduction (CLUSTER), meets these
(i.e., depending on what the task stresses and target            stated challenges. CLUSTER incorporates a formal goal
categories), SUSTAIN can evolve clusters that resemble           measure that directs cluster development. CLUSTER
prototypes, exemplars, or rules (Love, 2005). Careful be-        has sufficient flexibility to evolve conceptual structures
havioral experiments support the conclusion that SUS-            (i.e., clusters) that reflect key aspects of human knowl-
TAIN is not merely mimicking these other models, but             edge representation. After introducing the model, the
that human learners’ and SUSTAIN’s representations               formalism will be presented and a supportive simula-
are in accord (Sakamoto & Love, 2004). In summary,               tion will be discussed. The simulation illustrates how
SUSTAIN accounts for both classical studies of category          CLUSTER can evolve cluster organizations that serve
learning and the more contemporary work that suggests            the functions of multiple systems. Finally, we will con-
that conceptual organization is determined by the inter-         sider how CLUSTER is consistent with cognitive neu-
play of information structures in the environment and            roscience findings advocating multiple memory systems
task pressures or goals (Markman & Ross, 2003).                  and briefly discuss work that is being done to further
   Despite these successes, considerable challenges re-          develop and verify the model.
main. Two basic challenges are: (1) to formalize the no-
tion of a goal or task pressure and specify how such fac-                      Overview of CLUSTER
tors direct learning; (2) to endow learning models with          CLUSTER is an auto-associative model of human cat-
the flexibility to develop representations that approach         egory learning in which the “hidden” layer consists of
the range and richness of the representations that human         clusters (see Figure 1). A cluster is a bundle of related
learners build when learning from examples.                      features. A presented stimulus activates the existing
   Although SUSTAIN made strides in capturing the in-            clusters, which pass their activation to the output layer
fluence of goals, its notion of goal is underdeveloped.          via connection weights. Like other auto-associative mod-
Ideally, the notion of goal would be more encompassing           els (e.g., Kurtz, 2004), CLUSTER attempts to replicate
and continuous to capture all possible cases from pure           the input layer at the output layer and in the process
classification learning in which the only goal is to pre-        develops internal representations that seize on key regu-
dict category membership to pure unsupervised learning           larities.
in which the goal is to predict every feature (i.e., to cap-        CLUSTER differs from other auto-associative mod-
ture the correlational structure of the environment in an        els in a critical way. The error term CLUSTER min-
unbiased fashion). Importantly, the formal notion of goal        imizes does not uniformly weight reconstruction error
should directly affect the recruitment and modification          equally across features. Instead, each feature’s error
of clusters in a principled way. Learning rules should           is weighted according to its goal relevance. For ex-
update clusters to reflect the goal measure and clusters         ample, pure classification learning places all the error
should be recruited in light of how well the current clus-       term weighting on the category label features and er-
ters satisfy the current goal measure.                           ror associated with reconstructed perceptual features is
   In regards to the second basic challenge, current mod-        disregarded (as in most category learning models). At
els like SUSTAIN are too limited in terms of the range           the other extreme, pure unsupervised learning weights
of knowledge structures they can construct. For in-              the reconstruction error uniformly across features (as in
stance, SUSTAIN’s attentional mechanism accentuates              most auto-associative models). CLUSTER can capture
certain features that are predictive in the current task,        every conceivable case in between these extremes, which
but is constrained such that every cluster is focused on         is critical as the extremes are likely cartoons that do not
the same set of properties. In contrast, people stress           correspond to human learning (e.g., people incidentally
different properties in different domains. For example,          learn about feature correlations in classification learning
when shopping for clothing, color is important, but when         and place more importance on predicting certain features
shopping for a computer the type of processor is impor-          in unsupervised learning).
tant (a feature not even relevant to clothing). To evolve           The error term (with goal weights on each feature) is
these kinds of knowledge structures and to apply differ-         the learning goal, formally stated. To satisfy this goal,
ent “procedures” to different parts of the stimulus space        clusters adjust their position, attention, and weights to
as multiple systems models do, each cluster needs to be          minimize the error term through gradient descent learn-
able to accentuate the features that satisfy the learning        ing. Thus, depending on the goal weights, different
goals for the stimulus aspects it represents. A related          cluster organizations will emerge. Unlike most models,
challenge is storing information and capturing regular-          each cluster can adjust its own attention to minimize
ities at different scales ranging from very specific (e.g.,      error and attention does not sum to a fixed number.
Jim’s dog Fido) to very broad (e.g, living things). To ad-       These changes allow additional flexibility for clusters to
dress these issues, clusters need to fine tune their level       emphasize different features and to vary in specificity
of specificity to satisfy the goal measure. As in the case       (e.g., a specific dog vs. dogs in general). Although Fig-
of adjusting attention at the individual cluster level, ad-      ure 1’s grouping of features implies dimensional struc-
                                                             508

                                                                                 CLUSTER’s Formalism
                                                                    This section presents the equations that define CLUS-
                                                                    TER. First, we consider how CLUSTER generates a re-
                                                                    sponse. Then, we consider how CLUSTER learns.
                                                                    CLUSTER: Generating a prediction The distance
                                                                    between the stimulus and each cluster is calculated. The
                                                                    attention-weighted distance I j between the all the known
                                                                    features of stimulus S and cluster j is:
                                                                                            Xm
                                                                                       j
                                                                                      I =       αij (Hij − Si )2             (1)
                                                                                            i=1
                                                                    where m is the number of stimulus features, αij is cluster
                                                                    j’s attentional weighting of feature i, Hij is cluster j’s
                                                                    position along feature i (i.e., the value cluster j expects
                                                                    along feature i). Each Si is 0 (absent) or 1 (present) for
                                                                    discrete features and ranges between 0 and 1 for con-
                                                                    tinuous features. Unknown features are ignored when
                                                                    calculating distance.
                                                                       The output of cluster j is:
                                                                                                          j
                                                                                          Aj = λj · e−I                      (2)
Figure 1: CLUSTER is an auto-associative learning
model in which the hidden layer consists of clusters that           where λj is the sum of cluster j’s attentional weight-
adjust their position, attention, and association weights           ing for all known features. One subtle difference with
to minimize an error term that reflects the learner’s               most models is that the receptive field function for clus-
goals. In the illustrated example, three clusters have              ters is Gaussian instead of exponential. This functional
been recruited and the model is being asked to infer the            form allows for peak-shift responding in which stimuli
category label.                                                     outside the experienced range of examples can lead to
                                                                    responses more deterministic than for experienced stim-
                                                                    uli, consistent with rule-based generalization behavior to
ture, CLUSTER departs from the majority of models                   unfamiliar stimuli.
that utilize selective attention mechanisms (e.g., Nosof-              Activation passes from the clusters to the output units
sky, 1986) in that it does not assume a dimensional struc-          via association weights. The output of unit i is:
ture. Not assuming dimensional structure allows for ad-                                       n
ditional flexibility (e.g., the presence or absence of red                                   X
can be critical to a cluster, whereas the presence or ab-                              Oi =      wji · Aj + .5               (3)
                                                                                             j=1
sence of blue can be somewhat irrelevant).1
   CLUSTER begins with one cluster centered on the
first training example and recruits additional clusters             where wji is the association weight from cluster j to out-
when the existing clusters are not supportive of the cur-           put unit i. Outputs are truncated to lie between 0 and
rent goal. Each newly recruited cluster is centered upon            1. The default value of .5 can be conceived of as a prior
the current stimulus. Like CLUSTER’s other operations,              over features.
the algorithm for cluster recruitment is consistent across             In discrete-feature prediction tasks in which one of a
all induction tasks (there are no special cases). Despite           set of unknown features must be chosen (e.g., predicting
its consistency across situations, CLUSTER retains the              the category label in a classification task), the probabil-
flexibility to build representations that capture many of           ity of choosing unknown feature k is:
the competencies of human learners without proposing
distinct learning systems. CLUSTER is highly princi-                                                 (Ok )d
                                                                                       P r(k) = Pv            d
                                                                                                                             (4)
pled (all of its operations are tied to the goal-weighted                                            l=1 (Ol )
error term), but minimal structure is built in to the
model. Instead, CLUSTER evolves the knowledge struc-                where d is a decision parameter, and l ranges over the
tures needed to solve the current task.                             v features forming the choice set. The power response
    1
                                                                    rule is chosen over an exponential form to enable the
      Interestingly, in cases in which contrasts are consistent     aforementioned peak-shift responding behavior.
(e.g., when red is present, blue is absent, and vice versa),
CLUSTER attends equally to the contrasting features within             In recognition tasks, the recognition strength for stim-
each cluster. Thus, CLUSTER may prove to provide some               ulus S is given by the sum of all cluster activations re-
insight into how dimensional structure arises.                      sulting from the presentation of S.
                                                                509

CLUSTER: Learning and Cluster Recruitment                        findings by applying different procedures to different
After feedback, full stimulus information is known. Gra-         parts of the stimulus space and in fact provides an ac-
dient descent learning minimizes the error between the           count superior to RULEX’s.
stimulus and CLUSTER’s reconstruction of it at the out-             To test between this dual route account (i.e., rules and
put layer:                                                       exceptions) and a clustering account, Sakamoto and Love
                            m
                         1X                                      (2004) revisited the rule-plus-exception design with the
                   E=          δi · (Si − Oi )2          (5)     twist that one rule was twice as frequent as the other.
                         2 i=1
                                                                 Subjects sequentially classified stimulus items into cate-
where δi is the goal weighting for feature i subject to the      gories A and B and received corrective feedback. Each
following constraints:                                           category was defined by a rule (e.g., if large, then A; if
                                                                 small, then B). Additionally, each category contained an
                    m
                   X                                             exception (e.g., a small member of A; a large member of
                       δi = 1 and ∀i δi ≥ 0.             (6)     B). Table 1 provides the design details of Sakamoto and
                   i=1                                           Love’s variation in which one experienced category had
                                                                 twice as many rule-following items as the contrasting cat-
   Gradient descent learning rules minimize error by ad-         egory. Because subjects reason from stimulus dimensions
justing each cluster’s position, attention, and association      to categories in classification learning, the exception in
weights. These learning rules are derived by differenti-         the smaller category violates the more frequent rule in
ating the error term with respect to the adjusted quan-          Table 1 (i.e., if value 1 on the first stimulus dimension,
tity (i.e., position, attention, association weights). Each      then A). Following learning, recognition memory was as-
learning rule has an associated learning rate parameter.         sessed. In contrast to RULEX’s predictions (across all
   After receiving feedback but prior to applying the            explored parameter values), the exception violating the
learning rules, new clusters are recruited when the ex-          more frequent rule was better remembered than the ex-
isting clusters are a poor match to the current stimulus.        ception violating the less frequent rule (see Figure 2).
Specifically, a new cluster is recruited when:
                       Pn       j     j
                          j=1 A · G                              Table 1: The abstract stimulus structure for Sakamoto
                         P  n      j
                                         <τ              (7)
                            j=1 A                                and Love’s (2004) Experiment 1 is shown. Items A1 and
                                                                 B1 (indicated by the arrows) violate the imperfect rule
where τ is the recruitment threshold parameter and clus-         of the first stimulus dimension. Subjects completed 10
ter j’s goodness is:                                             training blocks where each block consisted of each item
                            Xm                                   below presented in a random order. Following learning,
                Gj = 1 −        δi · (Hij − Si )2 .      (8)     Items A1-5 and B1-B5 were paired with all combina-
                            i=1                                  tions of novel foils that matched on the first dimension
                                                                 in forced choice recognition. The actual stimuli were
   A newly recruited cluster is centered on the current
                                                                 simple geometric figures. For example, for some sub-
stimulus item. Association weights are set to zero. The
new cluster’s sum of attention for the features known at         jects the first dimension was size with a 1 indicating a
the initial stimulus presentation is set to be p (a param-       small figure and 2 indicating a large figure.
eter) above the value ξ necessary to prevent recruitment
                                                                    Learning       Dimension    Novel     Dimension
if the current stimulus was re-presented:
                                                                      Items          Values     Items       Values
                         λn+1 = ξ + p                    (9)       Category A
                                                                    → A1             21112        N1        11221
where                 Pn             Pn                                 A2           12122        N2        12112
                   τ·    j=1 Aj −       j=1 Aj · G j                    A3           11211        N3        12221
              ξ=                                     .  (10)
                                1−τ                                     A4           12211        N4        12212
Attention is allocated uniformly to all features (known                 A5           11122        N5        12222
and unknown) and is set to λn+1 divided by the number                   A6           12111        N6        21221
of known features at initial stimulus presentation.                     A7           11222        N7        22112
                                                                        A8           11212        N8        22221
               Illustrative Simulation                                  A9           12121        N9        22212
Findings from previous studies exploring rule-plus-                Category B                     N10       22222
exception learning have been problematic for exemplar               → B1             11121
models and have been used to support multiple sys-                      B2           22122
tems models, like the RULEX model of category learn-                    B3           21211
ing (Nosofsky et al., 1994). RULEX proposes that rule-                  B4           22211
following items are captured by a rule system whereas
                                                                        B5           21122
exception items reside in an exemplar store. Here, we
demonstrate that CLUSTER can accommodate such
                                                             510

                                                                here (CLUSTER does fit the pattern), the d parame-
                                                                ter was not used. In forced-choice recognition, the item
                                                                with the higher recognition strength was taken as CLUS-
                                                                TER’s choice.
                                                                   Using these parameters, CLUSTER was simulated
                                                                10,000 times adopting methods paralleling the human
                                                                study (e.g., 10 blocks of training) and the results were
                                                                averaged. Adopting the labels from Figure 2, CLUSTER
                                                                predicts Exc S=.88, Exc L=.80, Rul S=.58, and Rul
                                                                L=.59, which replicates the two major findings: excep-
                                                                tions are better remembered than rule-following items
                                                                with the exception violating the more frequent rule (i.e.,
                                                                the exception in the small category) being best recog-
                                                                nized.
                                                                   CLUSTER recruited 11.4 clusters on average (the me-
                                                                dian was 11) to represent the 14 training items. The
                                                                number of clusters recruited followed a normal distribu-
                                                                tion with solutions ranging from 4 to 23 clusters with a
                                                                standard deviation of 2.3. Every solution examined in-
                                                                volved devoting at least one cluster to encoding each ex-
Figure 2: Mean accuracies in the recognition phase of
                                                                ception with many simulations devoting multiple clusters
Sakamoto and Love’s (2004) Experiment 1 are shown               to each exception. Because CLUSTER is a distributed
along with 95% within-subjects confidence intervals (see        model and its predictions for an item depend on the re-
Loftus & Masson, 1994). Exc S is the exception of the           sponses of all clusters, an analysis of the four item types
small category, Exc L is the exception of the large cat-        was conducted that factored in all clusters.
egory, Rul S are the rule-following items of the small
                                                                   One explanation for CLUSTER’s ability to accommo-
category, and Rul L are the rule-following items of the         date the results is that it increased attention for clusters
large category.                                                 playing prominent roles in coding the exceptions, par-
                                                                ticularly for non-rule stimulus features. Encoding these
                                                                items at a different specificity than rule-following items
                                                                would help reduce confusions between these items and
   CLUSTER was applied to the data to illustrate its
                                                                rule-following items, resulting in both reduced error dur-
ability to “evolve” multiple systems. Each stimulus di-
                                                                ing training and in enhanced recognition for exceptions.
mension shown in Table 1 and category membership were
                                                                The pressure to enhance attention should be greatest for
represented by 2 features for a total of 12 features. In
                                                                the exception violating the more frequent rule as every
contrast to RULEX (which requires eight parameters to
                                                                rule-following item from the contrasting category pro-
CLUSTER’s seven for the simulation), multiple sets of
                                                                vides an impetus to enhance attention.
parameters replicated the basic pattern of results, indi-
cating that these findings follow from CLUSTER’s basic             To evaluate this explanation, following training, study
operation and that additional work is necessary to estab-       items were presented to CLUSTER and a weighted sum
lish default parameters for CLUSTER. These and other            of attention to non-rule features was calculated by mul-
model evaluation issues, such as consideration of nested        tiplying each cluster’s sum of attention for non-rule
models within CLUSTER’s formalism, are topics cur-              features by its activation. Then, these products were
rently being intensely pursued, but are set aside here in       summed and normalized by dividing by the sum of all
favor of demonstrating CLUSTER’s promise to evolve              cluster activations. The results for items of the same
multiple learning systems. In this spirit, the following        type were averaged. The mean results for the four item
parameters were selected because of the interpretability        types (averaged over 10,000 simulations) are Exc S=1.36,
of the resulting simulations: τ = .3250, p = 8.5, and the       Exc L=1.32, Rul S=1.28, and Rul L=1.29. As pre-
learning rates for attention, position, and weights were        dicted, these sums perfectly track item recognition. Ex-
.001, 10.0, and .1 respectively. The δ values were set such     ceptions (particularly the exception violating the more
that .9 of the total sum of 1 was devoted to the category       frequent rule) were stored as “hot spots” of focused ac-
label features with the remaining features weighted uni-        tivity whereas clusters coding for rule items were more
formly (i.e., the model’s primary goal was to correctly         broadly tuned and were less apt to code item specific
classify the stimulus, but some importance was given to         differences. Distinct representations emerge for the item
learning about relationships predicting other features).        types. CLUSTER provides a similar account of related
Finally, because the rule dimension (i.e., the first dimen-     data sets in which exception memory was manipulated
sion) was cued for subjects, this dimension was made            by varying the similarity between exception types and
more salient by allocating 91% of attention to the two          contrasting rule items instead of manipulating rule to-
features forming this dimension when a new cluster was          ken frequency (Sakamoto & Love, in press). SUSTAIN
recruited. Because the learning data are not discussed          cannot account for these data.
                                                            511

                       Discussion                                                   Acknowledgments
Human learners display flexibility in how they represent         This work was supported by AFOSR grant FA9550-04-
category information that outstrips the capacities of tra-       1-0226 and NSF CAREER grant 0349101 to B. C. Love
ditional single system models. In response, the field has        and NIH NRSA F32-MH068965 to M. Jones.
developed multiple system models that are themselves
not without problems. Here, we pursue a third approach
                                                                                         References
                                                                 Ashby, F., & O’Brien, J. B. (2005). Category learning and
– knowledge structures evolve as needed to satisfy the                 multiple memory systems. Trends in Cognitive Sci-
learner’s goals.                                                       ences, 9, 83-89.
   CLUSTER embodies this third position. CLUSTER
                                                                 Carpenter, G., & Grossberg, S. (2003). Adaptive resonance
has a formally defined notion of goal that spans induc-                theory. In M. Arbib (Ed.), The handbook of brain the-
tion tasks, recruits clusters when existing clusters fail to           ory and neural networks (p. 87-90). Cambridge, MA:
support the learner’s goals, and adjusts clusters’ posi-               MIT Press.
tions, attention, and association weights to reduce goal
                                                                 Erickson, M. A., & Kruschke, J. K. (1998). Rules and ex-
mismatch. These operations are sufficient to apply dif-                emplars in category learning. Journal of Experimental
ferent procedures to different parts of the stimulus space,            Psychology: General, 127, 107-140.
as multiple systems models do.
                                                                 Kruschke, J. K. (1993). Human category learning: Implica-
   How do we reconcile our position with impressive evi-               tions for backpropagation models. Connection Science,
dence from cognitive neuroscience that multiple systems                3-36.
underly human category learning performance? We do
not deny that multiple learning systems underly hu-              Kurtz, K. J. (2004). The divergent autoencoder (DIVA)
                                                                       account of human category learning. Proceedings of
man category learning. A non-exhaustive list of systems                the Cognitive Science Society, 1214-1219.
includes a dopaminergic procedural learning system,
a working memory system engaging cortical-thalamic               Love, B. C. (2005). Environment and goals jointly direct cat-
                                                                       egory acquisition. Current Directions in Psychological
loops, and a PFC-hippocampal-perirhinal learning sys-                  Science, 14, 195-199.
tem. The latter system is marked by its flexibility and is
adept at creating new conjunctive representations that           Love, B. C., & Gureckis, T. M. (under review). Models in
link features (i.e., clusters). SUSTAIN (the precursor                 search of a brain.
to CLUSTER) corresponds to this learning circuit and             Love, B. C., Medin, D. L., & Gureckis, T. (2004). SUS-
has successfully simulated populations with hippocam-                  TAIN: A network model of human category learning.
pal deficits by reducing the model’s ability to form new               Psychological Review, 111, 309-332.
clusters (Love & Gureckis, under review). CLUSTER                Markman, A. B., & Ross, B. H. (2003). Category use and
likely corresponds to the hippocampal system as well.                  category learning. Psychological Bulletin, 129, 592-613.
We believe that a fast learning hippocampal system is
                                                                 Medin, D. L., & Schaffer, M. M. (1978). Context theory of
shadowing the other learning systems. For instance,                    classification learning. Psychological Review, 85, 207-
the literature is replete (including Sakamoto and Love,                238.
2004) with cases in which learners are clearly applying
a rule stored in working memory, but are nevertheless            Nosofsky, R. M. (1986). Attention, similairty, and the
                                                                       identification-categorization relationship. Journal of
storing additional information about rule-following ex-                Experimental Psychology: General, 115, 39-57.
amples. Another way to reconcile CLUSTER with a
multiple learning systems view is to view these systems          Nosofsky, R. M., Palmeri, T. J., & McKinley, S. C. (1994).
                                                                       Rule-plus-exception model of classification learning.
emerging over an evolutionary time scale.                              Psychological Review, 101 (1), 53-79.
   Much work remains to be done. Efforts are underway
to apply CLUSTER to all the studies to which SUSTAIN             Nosofsky, R. M., & Zaki, S. F. (1998). Dissociations between
                                                                       categorization and recognition in amnesic and normal
has been applied. The results so far are promising. Ad-                individuals. Psychological Science, 9, 247-255.
ditionally, we are applying CLUSTER to studies explor-
ing how people partition knowledge and appear to apply           Sakamoto, Y., & Love, B. C. (2004). Schematic influences
different procedures depending on context (e.g., Yang &                on category learning and recognition memory. Journal
                                                                       of Experimental Psychology: General, 33, 534-553.
Lewandowsky, 2004). Finally, CLUSTER has been suc-
cessfully applied to Kruschke’s (1993) filtration and con-       Sakamoto, Y., & Love, B. C. (in press). Vancouver, toronto,
densation tasks that were intended to demonstrate the                  montreal, austin: Enhanced oddball memory through
                                                                       differentiation, not isolation. Psychonomic Bulletin &
necessity of dimensional attention (CLUSTER has clus-                  Review.
ter and feature specific attention). Although CLUSTER
does not have a built in notion of dimensional attention,        Smith, J. D., & Minda, J. P. (1998). Prototypes in the
dimensional attention emerges (i.e., there is advantage                mist: The early epochs of category learning. Jour-
                                                                       nal of Experimental Psychology: Learning, Memory, &
for aligning all clusters along the same contrasting fea-              Cognition, 24, 1411–1430.
tures) much like how what looks like multiple learning
systems emerges out of the Sakamoto and Love (2004)              Yang, L. X., & Lewandowsky, S. (2004). Context-gated
                                                                       knowledge partitioning in categorization. Journal of
simulations. While CLUSTER itself is still evolving, it                Experimental Psychology: Learning, Memory, & Cog-
appears it has the necessarily constraints built in to ac-             nition, 30, 1045-1064.
count for human learning and no more.
                                                             512

