UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Computational Model of the Metaphor Generation Process
Permalink
https://escholarship.org/uc/item/5d96219g
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Abe, Keiga
Sakamoto, Kayo
Nakagawa, Masanori
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

          A Computational Model of the Metaphor Generation Process
                                      Keiga Abe (abe@nm.hum.titech.ac.jp)
                               Kayo Sakamoto (sakamoto@nm.hum.titech.ac.jp)
                           Masanori Nakagawa (nakagawa@nm.hum.titech.ac.jp)
                   Graduate School of Decision Science & Technology, Tokyo Institute of Technology
                                  2-12-1, Ohkayama, Meguro-Ku, Tokyo, 152-8552, Japan
                          Abstract                                 metaphor generation process was constructed based on
                                                                   the results of the statistical analysis. After that, a psy-
   The purpose of this research was to construct a com-            chological experiment was conducted to examine the va-
   putational model of the metaphor generation process.
   In order to construct the model, ﬁrst, the probabilistic        lidity of the model.
   relationship between concepts and words was computed
   with a statistical analysis of language data. Secondly,           Probabilistic representation of meaning
   a computational model of the metaphor generation pro-
   cess was constructed with results of the statistical anal-      In previous studies, practical methods to compute the
   ysis of language data. The results of the simulation were       probabilistic relationship between concepts and their
   examined from a comparison with metaphors that par-             words, between words and words have been developed.
   ticipants had generated. Finally, a third-party rating of       For example, LSA(Landauer & Dumais, 1997) assumes
   the metaphors the model generated was conducted.                semantically similar words occur in common contexts.
                                                                   In LSA, text data are represented as a matrix in which
                      Introduction                                 each row stands for a unique word and each column for
                                                                   a text passage or other context. Each cell stands for the
Metaphor understanding and generation processes are
                                                                   frequency with which the word of its row appears in the
very important aspects of language study.                 How-
                                                                   passage denoted by its column. After that, LSA applies
ever, most cognitive studies of metaphor focus on
                                                                   singular value decomposition (SVD) to the matrix, as
the metaphor understanding process(Lakoﬀ & Johnson,
                                                                   follows:
1986; Glucksberg & Keysar, 1990; Kusumi, 1995), while                                                  
                                                                                          S = Uk Σk Uk .                   (1)
studies of the metaphor generation processes are rela-
tively few. The purpose of this study is to construct              Using this method, the meaning of words can be repre-
a computational model which generates a “A like B”                 sented in the coordinate of a vector space. Furthermore,
style metaphor process. In the case of “A like B” sytle            semantic similarities between words and words are rep-
metaphors, A is called the “vehicle”, and B is called the          resented by the cosine distance of vectors.
“topic”.                                                              However, LSA can not treat functional words(for ex-
   In a previous study, Kusumi(2003) showed that be-               ample, “the”, “a”, “is”). Generally, functional words oc-
lief or experience aﬀects the metaphor generating pro-             cur in various contexts with high occurrence frequency.
cess, using a metaphor generation task dealing with the            Such cooccurrence between content words and functional
concept of love. Hisano(1996) studied the relationship             words do not necessarily reﬂect semantic relation. In
between the impression of the topic and that of gener-             order to avoid this problem, LSA has to set a strong
ated metaphors, using a metaphor generation task where             weight to high occurrence frequency words. or omit low
the categories of topic and vehicle were limited. How-             occurrence frequency words, However such a weighting
ever, these studies were limited to a few concepts or cat-         method is likely to be subjective and ad-hoc.
egories. It is not clear whether the results are applicable           PLSI(Hofmann, 1999) is a probabilistic model for the
in the case of other concepts. In order to examine the             relationship between concepts and words based on the
applicability of the studies, the experimenter must con-           idea of LSA. PLSI assumes that latent semantic classes
duct a metaphor generation task with a huge number of              c’s mediate the probability of cooccurrence between doc-
concepts. It is impossible to cover large scale language           uments d’s and words w’s. In PLSI, the probability
knowledge using only a psychological experiment, be-               of cooccurrence between a document d and a word w,
cause psychological experiments require expensive time             P (d, w) is represented by the following equation:
and labor.                                                                                 
   In order to solve this problem, a statistical analysis of                    P (d, w) =     P (d|c)P (w|c)P (c),        (2)
language data was used to represent large scale human                                      c∈C
language knowledge stochastically. Applying statistical
analysis, a stochastic language knowledge structure can            where P (d|c) stands for the conditional probability of
be automatically constructed without subjective judge-             a document d, given a latent semantic class c, P (w|c)
ment. In this study, a statistical analysis of language            stands for the conditional probability of w, given c,
data was conducted and a computational model of the                and P (c) stands for the probability of c. Applying this
                                                               937

probailistic representation, PLSI does not need particu-
lar weighting to word, because the weight of a word w is             Table 1: Results of statistical analysis of language data.
included in the probability of cooccurrence P (d, w).                               “infant” latent semantic class
   There are other methods for the probabilistic repre-                      nouns       P (C|N )      adjectives     P (C|A)
sentation of meaning of words; Pereira(1993) proposed
a computational method for the probabilistic represen-                  grandchild         0.8077 young                 0.9711
tation of the relationships between nouns and verbs.                    girl               0.7184 ﬁne                    0.891
Kameya & Sato(2005) provided another statistical model                  son                0.6996 lovable               0.8701
based on PLSI to represent the relationship between                     character          0.6753 mild-mannered         0.8549
words and words in Japanese. The model assumes that                     child              0.6721 slight                0.8469
the cooccurrence probability of a word “ni ” and a word                 sister             0.6665 docile                0.7986
“aj ”, P (ni , aj ) is computed by the following formula;               baby               0.6328 small                 0.7906
                                                                       sleeping face      0.6231 slender                0.779
           P (ni , aj ) =     P (ni |ck )P (ai |ck )P (ck ), (3)        body               0.6204 innocent              0.7626
                           k                                            initial cry        0.6143 tragic                0.7596
   where P (ni |ck ) means the conditional probability of
ni , given ck which indicates a latent semantic class                                 “art” latent semantic class
assumed in this model. Parameters in the model,                              nouns       P (C|N )      adjectives     P (C|A)
P (ck ), P (ni |ck ) are estimated to be the values that max-
                                                                        harmony            0.7564   mild                 0.932
imaizes the likelihood of co-occurrence data measured
from the language corpus, with the EM algorithm.                        tune               0.7465   witty                0.931
   In this model, the meanings of words are represented                 amazement          0.7333   noble               0.9161
as a probability distribution of P (ai |ck ) or P (nj |ck ).            merody             0.7073   plain               0.9115
Furthermore, Kameya & Sato’s model can represent a                      singing voice      0.6946   heroic               0.894
semantic similarities between words and words as KL-                    lyric              0.6792   fresh               0.8933
divergence. This model was used for computational                       strain             0.6571   ﬂowing              0.8655
models of high order cognitive processes, for example,                  poetry             0.6509   massive             0.8606
the metaphor understanding process(Terai & Nakagawa,                    landscape          0.6508   elegant             0.8553
2005). This model can also be applied to the metaphor                   mid-age            0.6466   hard                 0.855
generation process using this probabilisitic representa-
tion of meaning.
   In this study, ﬁrst, a probabilistic representation
of language knowledge was constructed, by applying                   concepts, and conditional probability P (aj |ck ), P (ni |ck )
Kameya & Sato’s model to a language corpus taken from                in Kameya & Sato’s model as relationship strengths be-
the Japanese newspaper, the Mainichi-Shinbun, over a                 tween the semantic category and adjectives or nouns.
period of 10 years (1993-2002). One of the main rea-                    Based on the above assumption, a computational
sons for using this Japanese newspaper is the fact that              model that trasforms adjective-modiﬁed nouns (for ex-
it is read by a wide range of Japanese readers. The                  ample, “young, innocent, and ﬁne character”) into “A
language corpus consisted of 2783 adjectives and 14000               like B” style metaphors (for example, “the character is
nouns. The probabilistic representation consisted of 50              like a child”) was constructed. The model consists of the
latent semantic classes. Some examples of the result                 three layers below(Figure 1):
are shown in Table 1. Table 1 shows the rank order
of conditional probabilities of ci , given noun nj , or con-
                                                                     input layer: Each node in this layer corresponds to a
ditional probability of ci , given adjective nj . The rank
                                                                     word which constructs the phrases for metaphors.
order of nouns and adjectives suggests that the latent
semantic class represents a conceptual category about
“infant” or “art”. While the names of the latent seman-              hidden layer: Each node in this layer corresponds to
tic classes were applied by the authors for the practical            a latent semantic class ck in Kameya & Sato’s model
convenience, this naming has no eﬀect on the results of              assumed as a high order semantic category of human-
the simulation discussed below. The 2783 adjectives and              being’s concepts.
14000 nouns are classiﬁed by 50 latent semantic classes.
                                                                     output layer: In this layer, each node corresponds to
            Metaphor generation model                                the word for the vehicle of a metaphor.
In this study, it is assumed that the metaphor generating
process is a kind of word association between base words                In this model, weights of links between each layer
(vehicle) and target words (topic). The association pro-             are determined with conditional probability P (aj |ck ),
cess can be represented as a cooccurrence relationship               P (ni |ck ).
between words and words in Kameya & Sato’s model.
Furthermore, it is assumed that the latent semantic class               According to the model, metaphor generation is pro-
ck as a high order semantic category in human-being’s                cessed in the following steps:
                                                                 938

    +PRWV      6JGOGQHOGVCRJQTIGPGTCVKQP㧔CFLGEVKXGU PQWP㧕
                                                                                        being represented by the node as the vehicle of the
                       #HKPGUOCNNCPFKPPQEGPVEJKNF
                                                                                        metaphor.
  #PPQVCVKQP     HKPG
                                                                                        In this study, a probabilistic representation of lan-
                              UOCNN       KPPQEGPV       EJKNF
                                                                                     guage knowledge was constructed by applying Kameya
                                                                                     & Sato’s model to a language corpus. After that, a
                                                                        㪧㩿㪚㫓㪘㪀㩷
  CEVKXCVKQPQH                                                      㩷㩷㫆㫉㩷        metaphor generation model with probabilistic represen-
                                                                        㪧㩿㪚㫓㪥㪀
  NCVGPVUGOCPVKE
  ENCUUGU            %TKOG     +PHCPV         #TV       ,QD
                                                                                     tation of language knowledge was constructed.
                                                                            㪧㩿㪚㪀
                      %NCUU      %NCUU           %NCUU        %NCUU
                                                                          㪧㩿㪚㫓㪥㪀
                                                                                                            Simulation
   5GCTEJHQT
                                                                                     In order to evaluate the model, simulations were con-
   XGJKENGU      /KFCIG 2WRR[         .KQP                                ducted using three types of input phrases. Each input
                    5VQEJCUVKETGRTGUGPVCVKQPQHNCPIWCIGMPQYNGFIG                  phrase consists of a noun with three adjectives. Each
      QWVRWV                                  㩹㪫㪿㪼㩷㪺㪿㫀㫃㪻㩷㫃㫀㫂㪼㩷㪸㩷㫇㫌㫇㫇㫐㩹             word of the input phrases were selected at random from
                                                                                     top ten words according to rank order of conditional
                                                                                     probabilities P (C|N ) or P (C|A).
    Figure 1: The image of metaphor generation model
                                                                                     1 class input: This type consists of nouns and adjec-
                                                                                        tives which are strongly related to the same latent
1. When a phrase for metaphors is input to the model,                                   semantic class. For example, in the case of the in-
     the model runs a syntactic analysis of the phrase, and                             put phrase “young, innocent, and ﬁne character”, all
     decomposes the phrase to adjectives and nouns.                                     words are strongly related to the “infant” latent se-
                                                                                        mantic class.
2. Binary values are assigned to nodes in the input layer.
     The value 1 is assigned to the nodes corresponding to                           2 classes input: This type consists of adjectives
     the adjectives or nouns, while the value 0 is assigned                             strongly related to one latent semantic class and a
     to the other nodes.                                                                noun related to another latent semantic class. For ex-
                                                                                        ample, in the case of the phrase “excellent, admirable,
3. Activations of the input layer are transferred to the                                and famous son”, the adjectives are strongly related
     hidden layer. The activation value of node i in the                                to the “Job” latent semantic class, and the noun is
     hidden layer, ui is computed as follows:                                           strongly related to the “infant” class.
                                 1                                                   4 classes input: This type consists of words strongly
                               wij   = P (ci |nj ),                         (4)
                                                                                        related to separate latent semantic classes indepen-
                                             1                                          dently. For example, In the case of the phrase
                               si = Σj wij      · nj ,                      (5)
                                                                                        “small, elegant, and disconsolate nobility”, each word
                                             1                                          is strongly related to the “infant”, “art”, “emotion”
                              ui =                     .                    (6)
                                      1 + exp−si                                        and “job” classes, respectively.
                                   1                                                    In this simulation, the activation of output values con-
     In these equations, wij          is the conditional probability
     corresponding to the weight of the links between the                            cerning input phrases was computed . After that, the top
     input layer and the hidden layer. Applying a sigmoid                            20 words were considered as metaphors the model gen-
     function in equation (5), even though such a large                              erated. Results of the simulation are shown in Tables
     value is used for a speciﬁc node, the ﬁnal activation                           2,3,4.
     value does not become larger than 1.                                               According to the model, in the case of 1 class input,
                                                                                     all words of each input phrase activate a certain speciﬁc
4. In the output layer, each node receives the activa-                               class. In this case, the metaphors the model generated
     tion transferred from the hidden layer. The activation                          are concrete and easy to imagine. On the other hand,
     value of each node ol is computed with the equations                            in the case of 2 classes input, the input phrase activates
     as follows:                                                                     two latent semantic classes. The model then generates
                                  2
                                wil  = P (ci |nl ),                         (7)      intermediate words between the two classes. Therefore,
                                                                                    the metaphors the model generated are a little ambigu-
                                                   2
                               vl =         ui · wil ,                      (8)      ous compared to the case of 1 class input. In the case of
                                        i                                            4 classes input, the metaphors the model generated are
                                             1                                       less easy to visualize compared to the metaphors from 1
                              ol =                    .                     (9)      class or 2 classes input.
                                      1 + exp−vl
                                                                                        For the comparison with these models’ output, a
                                   2
     In these equations, wil          is the conditional probability                 metaphor generation task was conducted for 22 native
     corresponding to the weight of the links between the                            Japanese speakers. In this task, participants gener-
     input layer and the hidden layer. In the model, it is                           ated “A like B”style metaphors from 3 input phrases.
     assumed that the activation value of each node of the                           Those phrases presented to participants were the same
     output layer represents the probability of the word                             input phrases that were used for the model simulation
                                                                                 939

                                                          Table 4: Metaphors the model generated from the input
                                                          phrase “small, elegant, and disconsolate nobility”
Table 2: Metaphors the model generated from the input         order the nobility like a “XXX” output value
phrase “young, innocent and ﬁne character”
   order a character like a “XXX” output value                   1    mind-set                            0.5505
                                                                 2    expression                          0.5476
     1     grandchild                        0.5928              3    scream                              0.5468
     2     girl                               0.583              4    passion                             0.5454
     3     son                               0.5809              5    singing voice                       0.5421
     4     child                             0.5777              6    harmony                             0.5417
     5     sister                            0.5772              7    mentality                           0.5413
     6     baby                              0.5731              8    tune                                0.5412
     7     sleeping face                     0.5721              9    amazement                             0.54
     8     body                              0.5719             10    lost point                          0.5394
     9     baby’s ﬁrst cry                   0.5711             11    grand child                         0.5389
    10     character                         0.5685             12    melody                              0.5388
    11     physical frame                    0.5641             13    appearance                          0.5383
    12     young man                          0.561             14    lyric                               0.5381
    13     boy                               0.5592             15    manner                               0.538
    14     daughter                          0.5587             16    landscape                           0.5371
    15     old folks                         0.5585             17    girl                                0.5369
    16     infant                            0.5575             18    poetic state of mind                0.5369
    17     appearance                        0.5571             19    strain                              0.5368
    18     entrepreneurial spirit            0.5563             20    ring                                0.5367
    19     eldest-son                        0.5551
    20     second son                        0.5545
                                                          above. Participants were asked to generate as many
                                                          metaphors as possible in 5 minutes. The results of the
                                                          task are shown in Tables 5,6,7. In the metaphor gen-
                                                          eration task of the input phrases “young, innocent and
                                                          ﬁne character” and “excellent, admirable, and famous
Table 3: Metaphors the model generated from the input     son”, most participants generated the same metaphors
phrase “excellent, admirable, and famous son”             as the model did with high output value (For exam-
     order the son like a “XXX” output value              ple, “a character like a child”, “a grandchild like a aca-
                                                          demic”). Some of the metaphors the participants gen-
        1    academic                      0.5728
                                                          erated didn’t consist of the same metaphors the model
        2    surgeon                       0.5657         generated. However, participants do not always generate
        3    human resource                0.5599         good metaphors. There is a possibility that participants
        4    artist                        0.5598         generated nonsense metaphors, while the model gener-
        5    nobility                      0.5559         ated good metaphors the participants did not. There-
        6    painter                       0.5551         fore, in the next section, a third-party rating of the
        7    soldier career                 0.552         metaphors both the participants and the model gener-
        8    forerunner                    0.5502         ated was conducted.
        9    sense                         0.5501
       10    old man                       0.5485                                  Rating
       11    artist of calligraphy          0.548         In this section, a third-party rating of the metaphors
       12    military commander             0.547         both the model and the participants generated was con-
       13    ﬂower                         0.5462         ducted.
       14    student                        0.545         Method
       15    general                       0.5439         raters: In this evaluation, 13 college students partici-
       16    shrine                        0.5436         pated. All raters were native Japanese speakers.
       17    heated battle                 0.5435
       18    engineer                      0.5433         materials: Metaphors participants evaluated consist
       19    musician                      0.5423         of three groups.
       20    mis-thrown pitch              0.5415
                                                          Model’s metaphors: This           group    consists     of
                                                             metaphors the model generated, and human par-
                                                             ticipants did not. Three metaphors were chosen
                                                      940

                                                              from each input phrase, so this type consists of 9
Table 5: Metaphors participants generated from the            metaphors (3 phrases x 3 metaphors).
phrase “young, innocent and ﬁne character”(*:matched
with model output).                                        Partcipants’ metaphors: This group consists of
                                                              metaphors the human participants generated, and
          “young, innocent and ﬁne character”
                                                              those the model did not. Three metaphors were
   order the character like a “XXX” number of                 chosen from each input phrase, so this type consists
                                            answers           of 9 metaphors(3 phrases x 3 metaphors).
     1      child*                                 16
     2      puppy                                  13      Matched metaphors: This            group      consists of
                                                              metaphors both the human participants and the
     3      sun                                      7
                                                              model generated. This type consists of 6 metaphors
     4      boy, ﬂower, cat                          3        because there were no matched metaphors from
     5      infant*, girl*, glass,                   2        the input phrase “small, elegant, and disconsolate
            hamster, ﬁreworks                                 nobility” (2 phrases x 3 metaphors).
     6      moon, ball, air, sky,                    1
            strawberry, straight line,                     participants were shown these metaphors with the ma-
            wind, doll, puﬀball,                           terials used for genenerating these metaphors.
            the color of yellow, summer,
            budworm, yarn, typhoon                         procedure: Metaphors were presented without in-
                                                           forming the raters as to who generated it. Raters rated
                                                           the metaphors by 3 types of scales of 1 point to 7 point.
Table 6:       Metaphors participants generated from       adequacy: In this scale, the more adequate the expres-
the phrase “excellent, admirable, and famous                  sion of material, the higher the score is.
son”(*:matched with model output).                         ease of visualization: In this scale, the more easily vi-
         “excellent, admirable, and famous son”               sualized the metaphor is, the higher the score is.
    order        the son like a “XXX”     number of
                                           answers         amusingness: In this scale, the more amusing the
                                                              metaphor is, the higher the score is.
       1      academic*                            8
       2      sun                                  6       novelty: In this scale, the more novel the metaphor is,
       3      diamond, teacher                     4          the higher the score is.
       4      god                                  3
       5      military commander*,                 2       Results
              proﬀessor, ball,angle,                       Table 8 shows the result of the rating. In this analy-
              president, adult, king                       sis, the average scores of each type of metaphors were
       6      govemment oﬃcial, top,               1       compared, by each input phrase. For the comparison,
                                                           the average scores on each scale were computed, by each
              father, forerunner*,
                                                           type of metaphor in the input phrase.
              sample, doctor, music,                          In comparison with other cases using Bonferroni’s
              elite, witster, thinker,                     method, the metaphors of the input phrase “young,
              poet, padre, monk, star…                     innocent and ﬁne character”, the matched metaphors
                                                           gained high evaluation score on the scales of adequacy
                                                           (F (2, 24) = 37.667, p < 0.01) and ease of visualization
                                                           (F (2, 24) = 50.665p < 0.01). On the other hand, the
Table 7: Metaphors participants generated from the
                                                           model metaphors gained signiﬁcantly high evaluation
phrase “small, elegant, and disconsolate nobility”         scores on the scale of novelty compared to the human
       “small, elegant, and disconsolate nobility”         metaphors (F (2, 24) = 7.866, p < 0.01).
   order        “nobility like a XXX”      number of          The metaphors of the input phrase “excellent, ad-
                                            answers        mirable, and famous son”, the matched metaphors
     1       cat                                    5      gained higher evaluation scores on the scales of adequacy
     2       aristocrat, dame,gem,                  2      (F (2, 24) = 4.791, p < 0.01) and ease of visualization
             grandmother, inkstick,                        than the model metaphors (F (2, 24) = 5.576, p < 0.01).
                                                           On the scale of novelty, the scores of model metaphors
             rich folk,fallen leaves
                                                           were signiﬁcantly higher than the others (F (2, 24) =
     3       moon, rose, ﬂower,                     1      5.473, p < 0.01).
             diamond, chocolate, doll,                        In comparison with the model metaphors of the input
             neckrace, perl, chesil,                       phrase “small, elegant, and disconsolate nobility” with
             rainbow, angel, dead tree…                    the t-test, the human metaphors gained higher evalua-
                                                           tion scores on scales of adequacy(t(12) = −6.434, p <
                                                       941

0.01) and ease of visualization than model metaphors             possess. For 4 classes input phrases, participants could
(t(12) = −6.08, p < 0.01). On the scale of novelty, the          generate metaphors which were adequate and easyily vi-
scores of the model metaphors were signiﬁcantly higher           sualize. There is a possibility that participants gener-
than the human metaphors (t(12) = 5.505, p < 0.01).              ated abstract images, which intermediate words of input
                                                                 phrase moderately. Alternatively, participants might fo-
                                                                 cus on emergent features. In this case, emergent features
Table 8: Third party evaluation of metaphors generated           are assumed to be features which become salient only if
by the participants and model                                    speciﬁc words are well combined. In this study, partic-
           “young, innocent and ﬁne character”                   ipants might use emergent features of input phrases to
                             model human matched                 generate metaphors.
    adequacy                  3.49     2.74       5.59              The future challenges of this study are to clar-
                                                                 ify a mechanism of internal evaluation in generating
    ease of visualization     3.46     3.00       5.72
                                                                 metaphors. Metaphor generation is a kind of divergent
    amusingness               3.54     3.18       3.23           thinking. Therefore it does not necessarily have a single
    novelty                   4.08     3.51       2.79           or correct answer. However, participants have an inter-
          “excellent, admirable, and famous son”                 nal evaluation method for discriminating the metaphors
                             model human matched                 they generate. There is a possibility that this internal-
    adequacy                  2.51     3.51       3.59           evaluation mechanism ﬁlters out nonsense metaphors.
    ease of visualization     3.15     4.00       4.31
    amusingness               3.82     3.62       3.85                                  References
    novelty                   5.13     4.46       4.15           Glucksberg, S & Keysar, B. (1990). Understanding
        “small, elegant, and disconsolate nobility”                 metaphorical comparisons: Beyond similarity, Psycho-
                                                                    logical Review, 97,3-18.
                             model human
    adequacy                  1.79     3.87                      Hisano, M.(1996). Topic no Tokusei ga Hiyu Seisei ni
    ease of visualization     1.44     4.08                         Oyobosu Eikyou, Proceedings of 38th Annual Confer-
    amusingness               3.85     3.79                         ence of Japanese Association of Educational Psychol-
                                                                    ogy. 449.(in Japanese).
    novelty                   5.64     3.74
                                                                 Hofmann,T.(1999). Probabilistic latent semantic in-
                                                                    desing, Proceedings of the 22nd Annual ADM Con-
                                                                    ference on Research and Development in Information
                       Discussion                                   Retrieval (SIGIR’99), 50-57.
In this study, a statistical analysis of language data was       Kameya. Y & Sato. T.(2005). Computation of
conducted, and a computational model of the metaphor                Probabilistic Relationship between Concepts and their
generation process was constructed, based on the results            Attributes Using a Statistical Analysis of Japanese
of the statistical analysis. The central focus of this study        Corpora. Proceedings of Symposium on Large-Scale
is the application of a new statistical method, as a prob-          Knowledge Resources, 65-68.
abilistic version of LSA, to the construction of the com-
putational model.                                                Kusumi, T.(1995). Hiyu No Syorikatei To Imikouzou,
   Futhermore, the simulation of the model was con-                 Kazama Shobou.(in Japanse).
ducted, and the results were compared with metaphors             Kusumi, T.(2003). Hiyu Seisei wo Sasaeru Shinnen
human participants generated. From a comparison be-                 to Keiken: Ai no Hiyu no Haigo ni Aru Renai Ki-
tween model output and participants answers in the case             han to Keiken,Proceedings of 66th Annual Meeting of
of 1 class and 2 classes input phrases, most of the par-            Japanese Psychological Association 811.(in Japanse).
ticipants generated metaphors which had high output
                                                                 Lakoﬀ & Johnson. (1980). Metaphors We Live By. The
values in the model. Furthermore, in third-party rating
                                                                    University of Chicago Press.
of metaphors in the case of 1 class input phrases, the
metaphors the model generated were more highly eval-             Landauer, T.K. & S.T. Dumais. (1997). A solution to
uated compared to those generated by the participants.              Plato’s problem: The latent semantic analysis theory
This result suggests that the model might generate good             of acquisition, induction, and representation of knowl-
metaphors participants overlooked.                                  edge. Psychological Review, 104(2):211-240,
   However, in comparison to metaphors from 4class               Pereira, F., Tishby, N., and Lee, L.(1993). Distribu-
input phrases, the model did not match participants’                tional Clustering of English Words, The Proceedings
metaphors at all. Furthermore, in the third-party rat-              of 31st Meeting of the Association for Computational
ing of metaphors in the case of 4class input phrase, on             Linguistics, 183-190,
the scales of ease of visualization and adequacy, the
scores of metaphors the model generated were signif-             Terai, A. & Nakagawa, M.（2005） A development
icantly lower than participants’ metaphors. In other                method of a metaphorical search engine, World Con-
words, the model generates nonsense metaphors in the                ference on Educational Multimedia, Hypermedia &
case of 4class input phrases. These sharp diﬀerences may            Telecommunications, 1298-1303.
reﬂect cognitive mechanisms that the model does not
                                                             942

