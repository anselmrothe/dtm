UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Objects by Learning Models: Finding Independent Causes and Preferring Simplicity
Permalink
https://escholarship.org/uc/item/709049pm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Aslin, Richard N.
Fiser, Jozesef
Lengyel, Mate
et al.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                     Powered by the California Digital Library
                                                                       University of California

                               Learning Objects by Learning Models:
                Finding Independent Causes and Preferring Simplicity
                                          Gergő Orbán (ogergo@colbud.hu)
                                      Collegium Budapest Institute for Advanced Study
                                       2 Szentháromság utca, Budapest, 1014, Hungary
                                           József Fiser (fiser@brandeis.edu)
                     Dept of Psychology and Volen Center for Complex Systems, Brandeis University
                                                   Waltham, MA 02454, USA
                                      Richard N. Aslin (aslin@rochester.edu)
                Dept of Brain and Cognitive Sciences, Center for Visual Science, University of Rochester
                                               Rochester, New York 14627, USA
                                      Máté Lengyel (lmate@gatsby.ucl.ac.uk)
                           Gatsby Computational Neuroscience Unit, University College London
                                   17 Queen Square, London WC1N 3AR, United Kingdom
                           Abstract                               els were tailor-made to the specific psychophysical tasks
                                                                  presented in the experiments. However, in principle, in-
   Humans make optimal perceptual decisions in noisy and          ference can be performed on several levels: the genera-
   ambiguous conditions. Computations underlying such
   optimal behavior have been shown to rely on Bayesian           tive model can be used for inferring the values of hidden
   probabilistic inference. A key element of Bayesian com-        variables from observed information, and the generative
   putations is the generative model that determines the          model itself may also be inferred from previous expe-
   statistical properties of sensory experience. The goal of      rience (MacKay, 1992). Thus, it remains to be shown
   perceptual learning can thus be framed as estimating           whether more flexible, ‘open-ended’ generative models
   the generative model from available data. In previous
   studies, the generative model that subjects had to infer       could be used and learned by humans during perception.
   was relatively simple, its structure was also assumed to          We used an unsupervised visual learning task to show
   be known a priori, so that only a few model parameters         that a general class of generative models (Sigmoid Belief
   had to be estimated. We investigated whether humans            Networks) quantitatively reproduced experimental data,
   are capable of inferring more complex generative models
   from experience. In a completely unsupervised percep-          including paradoxical aspects of human behavior, when
   tual task subjects learnt subtle statistical properties of     not only the parameters of these models but also their
   visual scenes consisting of ‘objects’ that could only be       structure (ie. the number and identity of hidden vari-
   identified by their statistical contingencies not by low-      ables) was subject to learning. Crucially, the applied
   level features. We show that human performance in this         Bayesian model learning embodied the Automatic Oc-
   task can be accounted for by Bayesian learning of model
   structure and parameters within a class of models that         cam’s Razor (AOR) effect (MacKay, 1995) that preferred
   seek to explain observed variables by a minimum num-           the models that were ‘as simple as possible, but no sim-
   ber of independent hidden causes.                              pler’. This process led to the extraction of independent
                                                                  causes that efficiently and sufficiently accounted for sen-
                                                                  sory experience, without a pre-specification of the num-
                       Introduction                               ber or complexity of potential causes.
There is a growing number of studies supporting the                  All the presented experimental results were repro-
classical view of perception as probabilistic inference           duced and had identical roots in our simulations: the
(Helmholtz, 1962; Barlow, 1990). These studies demon-             model that was most probable based on the training
strated that human observers parse sensory scenes by              data developed hidden variables corresponding to the
performing optimal estimation of the parameters of the            real chunks that were originally used to generate the
objects involved (Ernst & Banks, 2002; Körding &                 training scenes. These results demonstrate that humans
Wolpert, 2004; Kersten, Mamassian, & Yuille, 2004). A             can infer complex models from experience and implicate
core element of this Bayesian probabilistic framework is          Bayesian model learning as a powerful computation un-
an internal model of the world, the generative model.             derlying such basic cognitive phenomena as the decom-
The generative model serves as a basis for inference by           position of visual scenes into meaningful chunks.1
specifying how the different sources of currently available
sensory evidence are integrated with prior expectations                        Experimental Paradigm
about the external world. Thus, in order to understand
                                                                  Human adult subjects were trained and then tested in
the computational principles of perception, it is impor-
                                                                  four different experiments using the same unsupervised
tant to characterize the forms of generative models that
                                                                  learning paradigm. Subjects saw a sequence of complex
are available for perceptual inference.
                                                                  visual scenes consisting of 6 of 12 abstract unfamiliar
   Most previous studies testing the Bayesian framework
                                                                  black shapes arranged on a 3x3 (Exp 1) or 5x5 (Exps 2-
in human psychophysical experiments used fundamen-
tally restricted generative models of perception. The                1
                                                                       A previous version of this work has been already pre-
generative models considered in these studies consisted           sented elsewhere to a rather different audience (Orbán, Fiser,
of a few observed and hidden variables, and only a lim-           Aslin, & Lengyel, 2006). The main novel aspect of the version
ited number of parameters that needed to be adjusted by           presented here is the inclusion of the Gestalt-based model,
                                                                  and a clearer explanation of human performance on embed-
experience (Ernst & Banks, 2002; Körding & Wolpert,              ded combos. This also enabled us to account for the effects
2004; Kersten et al., 2004; Weiss, Simoncelli, & Adel-            of traning length and performance on embedded triplets in
son, 2002). More importantly, these generative mod-               Exp. 4.
                                                              645

  A                                 B                                               elements of real doublets, thus a simple strategy based on
                                                 wx                  wx
                                                     1                  2
                                                                                    tracking co-occurrence frequencies of shape-pairs would
                                              x1                x2                  be sufficient to distinguish between them. The second,
                                                 w12
                                     w11               w22
                                                                     w24            frequency-balanced experiment tested whether humans
                                                            w23
                                                                                    are sensitive to higher-order statistics (at least cross-
                                     y1           y2         y3           y4        correlations, which are co-occurence frequencies normal-
                                         wy            wy       wy           wy
                                                                                    ized by respective invidual occurence frequencies).
                                            1             2        3            4
                                                                                       The structure of Experiment 1 was changed so that
Figure 1: A, Experimental design. B, Explanation of                                 while the same 6 doublet combos were used as before,
                                                                                    their appearance frequency became non-uniform intro-
graphical model parameters.
                                                                                    ducing frequent and rare combos. Frequent doublets were
                                                                                    presented twice as often as rare ones, so that certain
4) white grid (Fig. 1A). Unbeknownst to subjects, vari-                             mixture doublets consisting of shapes from frequent dou-
ous subsets of the shapes were arranged into fixed spatial                          blets appeared just as often as rare doublets. Note, that
combinations (combos) (doublets, triplets, quadruplets,                             the frequency of the constituent shapes of these mixture
depending on the experiment). Whenever a combo ap-                                  doublets was higher than that of rare doublets. The
peared on a training scene, its constituent shapes were                             training session consisted of 212 scenes, each scene be-
presented in an invariant spatial arrangement, and in no                            ing presented twice. In the test phase, the familiarity
scenes elements of a combo could appear without all the                             of both single shapes and doublet combos was tested.
other elements of the same combo also appearing. Sub-                               In the doublet trials, rare combos with low appearance
jects were presented with 100–200 training scenes, each                             frequency but high correlations between elements were
scene was presented for 2 seconds with a 1-second pause                             compared to mixture combos with higher element and
between scenes. No specific instructions were given to                              equal pair appearance frequency, but lower correlations
subjects prior to training, they were only asked to pay                             between elements.
attention to the continuous sequence of scenes.
   The test phase consisted of 2-alternative forced choice                          Experiment 3 This experiment tested whether human
(2AFC) trials, in which two arrangements of shapes were                             performance in this paradigm can be fully accounted for
shown sequentially in the same grid that was used during                            by learning cross-correlations. Here, four triplet combos
training, and subjects were asked which of the two scenes                           were formed and presented with equal occurrence fre-
was more familiar based on the training. One of the                                 quencies. 112 scenes were presented twice to subjects.
presented scenes was either a combo that was actually                               In the test phase two types of tests were performed. In
used for constructing the training set (true combo), or                             the first type, the familiarity of a true triplet and a mix-
a part of it (embedded combo) (e.g., a pair of adjacent                             ture triplet was compared, while in the second type dou-
shapes from a triplet or quadruplet combo). The other                               blets consisting of adjacent shapes embedded in a triplet
scene consisted of the same number of shapes as the first                           combo (embedded doublet) were tested against mixture
scene in an arrangement that might or might not have                                doublets.
occurred during training, but was in fact a mixture of
                                                                                    Experiment 4 This experiment compared directly
shapes from different true combos (mixture combo).
                                                                                    how humans treat embedded and independent (non-
   Here four experiments are considered that assess vari-                           embedded) combos of the same size. Here two quadru-
ous aspects of human observational learning, the full set                           plet combos and two doublet combos were defined and
of experiments are presented elsewhere (Fiser & Aslin,                              presented with equal frequency. Each training scene con-
2001, 2005). Each experiment was run with 20 naı̈ve                                 sisted of six shapes, one quadruplet and one doublet. 120
subjects.                                                                           such scenes were constructed and subjects were either
Experiment 1 Our first goal was to establish that hu-                               presented with each scene once (half training), or twice
mans are sensitive to the statistical structure of visual                           (full training). In the test phase four types of 2AFC
experience, and use this experience for judging familiar-                           trials were used: true against mixture quadruplets; em-
ity. In the baseline experiment 6 doublet combos were                               bedded against mixture doublets; true against mixture
defined, three of which were presented simultaneously                               doublets; and embedded against mixture triplets.
in any given training scene, allowing 144 possible scenes
(Fiser & Aslin, 2001). Because the doublets were not                                                Modeling framework
marked in any way, subjects saw only a group of random
                                                                                    The goal of Bayesian learning is to ‘reverse-engineer’ the
shapes arranged on a grid. The occurrence frequency of
                                                                                    generative model that could have generated the training
doublets and individual elements was equal across the
                                                                                    data. Because of inherent ambiguity and stochasticity
set of scenes, allowing no obvious bias to remember any
                                                                                    assumed by the generative model itself, the objective is
element more than others. In the test phase a true and
                                                                                    to establish a probability distribution over possible mod-
a mixture doublet were presented sequentially in each
                                                                                    els. Importantly, because models with parameter spaces
2AFC trial. The mixture combo was presented in a spa-
                                                                                    of different dimensionality are compared, the marginal
tial position that had never appeared before.
                                                                                    likelihood term will prefer the simplest model (in our
Experiment 2 In the previous experiment the elements                                case, the one with fewest parameters) that can effectively
of mixture doublets occurred together fewer times than                              account for (generate) the training data due to the AOR
                                                                                  646

effect in Bayesian model comparison (MacKay, 1995).                 where the first term is the likelihood of the model
                                                                    (Eq. 3), and the second term is the prior distribution
Sigmoid belief networks The class of generative mod-                of models. Prior distributions for the weights were:
els we consider is that of two-layer sigmoid belief net-
                                                                    P (wij ) = Exponential (4), P (wxi ) = Laplace (0, 4),
works (SBNs, Fig. 1B). The same modelling framework
has been successfully aplied to configural learning in an-          P wyj = Laplace (−2, 4). The prior over model struc-
imal classical conditioning (Courville, Daw, Gordon, &              tures preferred simple models and was such that the
Touretzky, 2004; Courville, Daw, & Touretzky, 2005).                distributions of the number of latents and of the num-
The SBN architecture assumes that the state of observed             ber of links conditioned on the number of latents were
binary variables (yj , in our case: shapes being present or         both Geometric (0.1). The effect of this preference is
absent in a training scene) depends through a sigmoidal             ‘washed out’ with increasing training length as the like-
activation function on the state of a set of hidden binary          lihood term (Eq. 3) sharpens.
variables (x), which are not directly observable:                   Testing When asked to compare the familiarity of two
                                                            !!−1 scenes (yA and yB ) in the testing phase, the optimal
                                           X                        strategy for subjects would be to compute the posterior
P (yj = 1|x, wm , m) = 1 + exp −               wij xi − wyj
                                                                    probability of both scenes based on the training data
                                            i
                                                             (1)          Z
                                                                                X
                                                                                        Z      X
                                                                                                    P yZ , x|wm , m P (wm , m|D)
                                                                                                                       
where wij describes the (real-valued) influence of hid-             P   y   |D   =         dwm
den variable xi on observed variable yj , wyj determines                            m            x
the spontaneous observed activation bias of yj , m indi-                                                                        (5)
cates the model structure, including the number of latent           and always (ie, with probability one) choose the one with
variables and identity of the observeds they can influ-             the higher probability. However, as a phenomenological
ence (the wij weights that are allowed to have non-zero             model of all kinds of possible sources of noise (sensory
value), and wm stands for all the parameters (wij , wyj ,           noise, model noise, etc) we chose a soft threshold func-
wxi within model structure m).                                      tion for computing choice probability:
   Observed variables are independent conditioned on the                                                                  !!−1
latents (i.e. any correlation between them is assumed                                                          P yA |D
                                                                       P (choose A) = 1 + exp −β log
to be due to shared causes), and latent variables are                                                           P (yB |D)
marginally independent and have Bernoulli distributions                                                                         (6)
parametrised by their biases, wx :                                  and used a single β to fit experimental data from all
                                                                    subjects (β = ∞ corresponds to the optimal strat-
                            Y
    P (y|x, wm , m) =            P (yj |x, wm , m)           (2)
                              j
                                                                    egy,   β = 1 corresponds
                                                                                                    to probability matching).
                                                                                    A           B
                            Y                    x       −1
                                                                    Here  log  P  y   |D   /P y   |D   is the log probability ratio
       P (x|wm , m) =            (1 + exp ((−1) i wxi ))            (LPR).
                              i                                        Note that when computing the probability of a test
                             (t)
Finally, training scenes y are assumed to be iid sam-               scene,   we seek the probability that exactly the given
ples from the same generative distribution, and so the              scene   was  predicted by the learned model. This means
probability of the training data (D) given a specific               that we require not only that all the shapes that are
model is:                                                           present in the test scene are predicted to be present, but
                     Y                                            also that all the shapes that are absent from the test
P (D|wm , m) =           P y(t) |wm , m                        (3) scene are predicted to be absent. A different scheme,
                       t                                            in which only the presence but not the absence of the
                     YX                                           shapes need to be matched (i.e. absent observeds are
                 =             P y(t) |x, wm , m P (x|wm , m) marginalized out just as latents are in Eq. 5) could also
                       t   x                                        be pursued, but the results of the embedding experi-
   The ‘true’ generative model that was actually used for           ments (Exp. 3 and 4, see below) discourage it.
generating training data in the experiments (Section 2)                The model posterior in Eq. 4 is analytically in-
is closely related to this model, with the combos corre-            tractable, therefore an exchange reversible-jump Markov
sponding to latent variables. The main difference is that           chain Monte Carlo sampling method (Courville et al.,
here we ignore the spatial aspects of the task, i.e. only           2004; Green, 1995; Iba, 2001) was applied, that ensured
the occurrence of a shape matters but not where it ap-              fair sampling from a model space containing subspaces of
pears on the grid. Although in general, space is certainly          differring dimensionality, and integration over this pos-
not a negligible factor in vision, human behavior in the            terior in Eq. 5 was approximated by a sum over samples.
present experiments depended on the mere presence or
absence of shapes sufficiently strongly so that this sim-                                     Results
plification did not cause major confounds in our results.           In the baseline experiment (Experiment 1) human sub-
Training Establishing the posterior probability of any              jects were trained with six equal-sized doublet combos
given model is straightforward using Bayes’ rule:                   and were shown to recognize true doublets over mix-
                                                                    ture doublets (Fig. 2A). When the same training data
         P (wm , m|D) ∝ P (D|wm , m) P (wm , m)              (4)    was used to compute the choice probability in 2AFC
                                                                  647

                     A                                                  B
                      1                 experiment
                                                                            1                            experiment
                                        simulation                                                       simulation
                     0.8                                                0.8                                                               A                                                         B
                                                     Fraction correct
                                                                                                                                                              15                                                   20
  Fraction correct
                                                                                                                                     log probability ratio
                                                                                                                                                                                              wij posterior mean
                     0.6                                                0.6
                                                                                                                                                              10                                                   16
                     0.4                                                0.4
                                                                                                                                                                 5                                                 12
                     0.2                                                0.2
                     0                                                      0                                                                                    0                                                  8
                               2                                                        2                                                                         0      80    160      240                          0      80    160      240
                    C                                                   D
                      1                 experiment
                                                                         1                            experiment, full
                                                                                                                                         C                                                          D
                                        simulation                                                    simulation, full                                           0                                                  4
                                                                                                                                       wy posterior mean                                      wx posterior mean
                                                                                                      experiment, half
                     0.8                                                0.8                           simulation, half
 Fraction correct                                    Fraction correct
                                                                                                                                                              −4                                                    0
                     0.6                                                0.6
                                                                                                                                                              −8                                                   −4
                     0.4                                                0.4
                                                                                                                                                             j                                                i
                     0.2                                                0.2                                                                                  −12                                                   −8
                                                                                                                                                                0        80    160      240                          0      80    160      240
                                                                                                                                                                      training length                                    training length
                     0                                                  0
                           3       E2                                           4   2       E2   E3
Figure 2: Comparison of human and model performance                                                                        Figure 3: Evolution of model variables with increasing
in four experiments. A–D, Results from baseline (Exp.                                                                      training length in a pilot triplet simulation performed
1), frequency-balanced (Exp. 2), triplet (Exp. 3), and                                                                     with two triplets. A, LPRs for triplet (solid line) and
quadruple experiments (Exp. 4), respectively. Bars show                                                                    embedded doublet (dashed line) 2 AFC tests. B-D, Mean
fraction of ‘correct’ responses (choosing a true or em-                                                                    (solid line) ±1s.d. (dotted lines) of posterior parameter
bedded combo over a mixture combo) for human ex-                                                                           distributions in the MAP model structure for latent-to-
periments (orange, average over subjects ±SEM), and                                                                        observed weights, wij (B); observed biases, wyj (C); and
‘correct’ choice probabilities (Eq. 7) for computer simu-                                                                  latent biases, wxi (D).
lations (brown). Labels below the bars denote the type of
test trial: 2, doublet; E2, embedded doublet; 3, triplet;                                                                  its constituent parts (the embedded doublets) loose their
E3, embedded triplet; 4, quadruplet. D, results from                                                                       significance. Our model reproduced this behavior and
experiments (yellow) and simulations (red) at half train-                                                                  provided a straightforward explanation. The main effect
ing length (half ) and full training length (full) are also                                                                of extensive training in the simulations was increasing
shown. Dotted lines show 50% correct chance level per-                                                                     certainty about the correct model structure (data not
formance.                                                                                                                  shown) and that given that model structure there was
                                                                                                                           a strong causal link between the appearence of a combo
tests with model learning, true doublets were reliably                                                                     and its constituent shapes (shift of wij weights towards
preferred over mixture doublets. Also, the model with                                                                      more positive values, Fig. 3B). Given such a confident
maximal a posteriori (MAP) probability showed that the                                                                     causal link in the learned model, whenever a combo ap-
discovered latent variables corresponded to the combos                                                                     peared it could almost only produce triplets, therefore
generating the training data (data not shown).                                                                             doublets (embedded and mixture alike) could only be
   In Experiment 2, we sought to answer the question                                                                       created by spontaneous independent activation of indi-
whether the statistical learning demonstrated in Exper-                                                                    vidual shapes. In other words, doublets were seen as
iment 1 was solely relying on co-occurrence frequencies,                                                                   mere noise that naturally produced embedded and mix-
or was using something more sophisticated, such as at                                                                      ture doublets with equal chance. An interesting predic-
least cross-correlations between shapes. Bayesian model                                                                    tion from this argument is that more training should just
learning, as well as humans, could distinguish between                                                                     further accentuate this affect, that is embedded doublets
rare doublet combos and mixtures from frequent dou-                                                                        should become even less preferred Fig. 3A).
blets (Fig. 2B) despite their balanced co-occurrence fre-                                                                     The fourth experiment tested explicitly whether em-
quencies.                                                                                                                  bedded combos and equal-sized independent true com-
   We were interested whether the performance of hu-                                                                       bos are distinguished and not only size effects pre-
mans could be fully accounted for by the learning of                                                                       vented the recognition of embedded small structures in
cross-correlations, or they demonstrated more sophis-                                                                      the previous experiment. Both human experiments and
ticated computations. In Experiment 3, training data                                                                       Bayesian model learning demonstrated that quadruple
was composed of triplet combos, and beside testing true                                                                    combos as well as stand-alone doublets were reliably rec-
triplets against mixture triplets, we also tested embed-                                                                   ognized (Fig. 2D), while embedded doublets were not.
ded doublets (pairs of shapes from the same triplet)                                                                       Moreover, longer training did not help with the recog-
against mixture doublets (pairs of shapes from different                                                                   nition of embedded doublets just as predicted before.
triplets). If learning only depends on cross-correlations,                                                                 However, the preference for embedded triplets against
we expect to see similar performance on these two types                                                                    mixture triplets was signifcantly above chance, and thus
of tests. In contrast, human performance was signifi-                                                                      above the level at which embedded doublets were pre-
cantly different for triplets (true triplets were preferred)                                                               ferred. Our simluations could also account for this ef-
and doublets (embedded and mixture doublets were not                                                                       fect. The probability of a given combo being produced
distinguished) (Fig. 2C). This may be seen as Gestalt                                                                      by the spontaneous activation of its constituent shapes
effects being at work: once the ‘whole’ triplet is learned,                                                                (noise) decreases exponentially with its size. Therefore,
                                                                                                                         648

                                 1                                    Exp 1, 2        ual shapes, further accentuated by the grid lines sep-
fraction correct (experiment)
                                                                      Exp 2, 2
                                0.8                                   Exp 2, 1
                                                                                      arating them – bias any naı̈ve observer to treat indi-
                                                                      Exp 3, 3        vidual shapes as truly independent in spite of conflict-
                                0.6                                   Exp 3, E2       ing statistical evidence. We modeled this Gestalt-based
                                                                      Exp 4, 4 F
                                                                      Exp 4, 2 F      bias phenomenologically when computing the predictive
                                0.4                                   Exp 4, E2 F     probabilities of the two test scenes in a 2AFC test trial.
                                                                      Exp 4, 4 H      The final predictive probability of a scene was a mix-
                                0.2         β=0.60, γ= 0.27           Exp 4, 2 H
                                               β=0.15                 Exp 4, E2 H     ture of the predictions of a purely statistics-based model
                                 0                                    Exp 4, E3 F     (described before), Pstats , and a so-called Gestalt-based
                                 −2     0      2    4     6   8       Exp 4, E3 H
                                 log probability ratio (simulation)                   model, PGestalt , which was constrained to have zero la-
                                                                                      tents (ie, the prior distribution on the number of latent
Figure 4: Aggregate plot of the fit of experimental data                              variables and correspondingly on the number of links was
                                                                                      δ (0)) and thus only learnt about the occurrence frequen-
with simulations. Black symbols, LPRs from statistics-
                                                                                      cies of individual shapes:
based computation; gray symbols, LPRs from Gestalt-
based computations; colored symbols, LPRs from the                                                            γ               (1−γ)
                                                                                         Pmixture yZ |D ∝ Pstats    yZ |D · PGestalt yZ |D (7)
                                                                                                                                         
mixture predictive distribution. Solid gray line, sigmoid
fit on the 12 black symbols from the data of Fig. 2 (Exp2,                           Both Pstats and PGestalt were computed based on Equa-
1 was not included in the fit). Solid black line, sigmoid                            tion 5. The same mixing coefficient γ was fitted to all
fit on the colored symbols. Symbol colors, different ex-                             experimental data (Fig. 4).
                                                                                        We used the previous 12 data points complemented
periments: (red, baseline (Exp. 1); yellow, frequency-
                                                                                     with the data on singlet recognition in Experiment 2 and
balanced (Exp. 2); green, triplet (Exp. 3); blue, quadru-                            obtained a good fit when fitting them with the noise pa-
plet (Exp. 4). Symbol shapes, different test trial types in                          rameter β and mixing coefficient γ (r = 0.76, β = 0.60
a given experiment (see Fig. 2 for an explanation of the                             and γ = 0.27; Fig. 4, colored symbols, solid black line).
legend), F and H in Exp. 4 denote full and half train-                               Although the overall fit of the model is somewhat worse
ing length, respectively. Dotted lines show that fits were                           than before, recognition of singlets in the model consid-
constrained to be unbiased, ie. to go across (0, 0.5).                               erably improved when mixing was introduced (Fig. 5A).
                                                                                     Also, Bayesian information criterion computed over all
while the appearance of an embedded doublet could be                                 data points in the purely statistical model and in the
explained away by noise as before, the appearance of                                 mixture model (−11.6 and −22.6 for the two models, re-
an embedded triplet could only be explained by the ap-                               spectively) provided evidence that the inclusion of mix-
pearance of a true quadruplet that failed to activate one                            ing in the model was justified by the data. Similar to
of its shapes. The only likely mechanism for explaining                              experiments, simulations showed a slightly greater pref-
triplets could thus produce embedded but not mixture                                 erence for singlets than for doublets (Fig. 4, orange sym-
triplets, hence the preference for the former.                                       bols). Pilot studies performed with three pairs and sim-
                                                                                     ilar statistics revealed that Gestalt effects play a signifi-
  In order to demonstrate the predictive power of our                                cant role in this result (Fig. 5B). While purely statistics-
approach we have replotted all the experimental data                                 based processing preferred both frequent singlets and
(12 data points) against our theoretical predictions from                            rare doublets, its preference for doublets was always
Figure 2 and obtained a strong quantitative match (r =                               markedly greater – simply because the true generative
0.95, Fig. 4, black symbols, solid grey line). Importantly,                          model discovered by statistical learning produced dou-
we used the same parameter set for modelling all the                                 blets and not singlets. Gestalt-based computations, con-
experiments. Only one parameter was tuned for fitting                                versely, only processed singlet frequencies and therefore
the data (β = 0.15, see Eq. 6), while no specific ef-                                strongly disfavored rare doublets and preferred frequent
fort was made to optimize the rest of the parameters;                                singlets. Thus, ameliorating the predictions of statistics-
indeed changing them in a wide range did not affect the                              based computations with that of Gestalt-based process-
qualitative outcome of our simulations.                                              ing led to a stronger preference for frequent singlets than
   In Experiment 2, although rare doublet combos were                                for rare doublets, just as seen in humans.
preferred over frequency-balanced cross-pairs, humans –
but not the model – learned about the frequencies of                                                       Discussion
their constituent shapes. Human subjects preferred con-                               We demonstrated that humans flexibly yet automatically
stituent single shapes of frequent doublets over those                                learn complex generative models in visual perception.
of rare doublets (Fig. 5A). Furthermore, experiments                                  Bayesian model learning has been implicated in several
showed a slightly greater preference for singlets than                                domains of high level human cognition, from causal rea-
for doublets. Since the approach concentrating on the                                 soning (Tenenbaum & Griffiths, 2003) to concept learn-
purely statistical aspects of perceptual learning in this                             ing (Tenenbaum, 1999). Here we showed it being at work
task failed to reproduce these findings, we hypothesized                              already at a pre-verbal stage. Thus such a probabilistic
that Gestalt cues present in training scenes affect the                               framework might be an adequate unified basis for mod-
learning process, therefore cannot be neglected. These                                eling learning processes from early sensory to complex
Gestalt cues – the clear spatial disjunctness of individ-                             cognitive levels.
                                                                                    649

                    A
                        1
                            exp
                                                           B                                                               Courville, A. C., Daw, N. D., Gordon, G. J., & Touret-
                                                                      10
                                                    log probabiliy ratio
                            sim, w/o mixing
                                                                                                   frb pair, model sel
                    0.8     sim, w mixing
                                                                                                   single, model sel         zky, D. S. (2004). Model uncertainty in classical con-
 Fraction correct
                                                                                                   frb pair, 0−latent
                                                                           5
                    0.6
                                                                                                   single, 0−latent          ditioning. In NIPS 16. Cambridge: MIT Press.
                    0.4
                                                                           0                                               Courville, A. C., Daw, N. D., & Touretzky, D. S. (2005).
                    0.2
                                                                     −5
                                                                      −8        −6   −4   −2   0
                                                                                                                             Similarity and discrimination in classical conditioning:
                                                                               wy prior mean
                        0
                                                                                 j                                           A latent variable account. In NIPS 17. Cambridge,
                                                                                                                             MA: MIT Press.
Figure 5: A, Recognition of singlets in Experiment                                                                         Ernst, M. O., & Banks, M. S. (2002). Humans integrate
2 in human experiments (red ), in simulation with                                                                            information in a statistically optimal fashion. Nature,
purely statistics-based computations (orange), and with                                                                      415, 429-33.
Gestalt-based computations (yellow ). B, Recognition of                                                                    Fiser, J., & Aslin, R. N. (2001). Unsupervised statistical
frequency balanced (red) doublets and frequent singlets                                                                      learning of higher-order spatial structures from visual
(yellow) in a series of pilot frequency-balanced simula-                                                                     scenes. Psych Sci, 12, 499–504.
tions. LPRs (Eq. 6) were calculated in simulations with                                                                    Fiser, J., & Aslin, R. N. (2002). Statistical learning of
purely statistics-based computations (solid lines) and                                                                       new visual feature combinations by infants. Proc Natl
with Gestalt-based computations (dashed lines). Calcu-                                                                       Acad Sci USA, 99, 15822–6.
lations were performed at various observed prior mean                                                                      Fiser, J., & Aslin, R. N. (2005). Encoding multi-element
values in order to test robustness.                                                                                          scenes: Statistical learning of visual feature hierar-
                                                                                                                             chies. J Exp Psychol Gen, 134, 521–37.
   An important finding of the present work was that hu-                                                                   Green, P. J. (1995). Reversible jump MCMC computa-
man data can be explained through an interplay between                                                                       tion and Bayesian model determination. Biometrika,
statistical computations and computations that are gov-                                                                      82, 711–732.
erned by Gestalt principles. Since in the experiments                                                                      Helmholtz, H. L. F. (1962). Treatise on physiological
presented here only one case (when single shapes were                                                                        optics. New York: Dover. (original published in 1867)
tested in the frequency balanced experiment) necessi-                                                                      Iba, Y. (2001). Extended ensemble Monte Carlo. Int J
tated the mixing of statistics- and Gestalt-based predic-                                                                    Mod Phys C, 12, 623–56.
tions, this result should be taken as preliminary. Never-
                                                                                                                           Kersten, D., Mamassian, P., & Yuille, A. (2004). Object
theless, in that case it seems that the strong Gestalt cues
of the experimental paradigm suggesting the indepen-                                                                         perception as Bayesian inference. Annu Rev Psychol,
dence of individual shapes substantially interfered with                                                                     55, 271–304.
statistics-based computations. In line with this account,                                                                  Körding, K. P., & Wolpert, D. M. (2004). Bayesian
infants, known to lack some of the Gestalt-based pro-                                                                        integration in sensorimotor learning. Nature, 427, 244-
cessing capabilities of adults (Kovács, 2000), did not                                                                      7.
keep track of single shape frequencies in an adapted                                                                       Kovács, I. (2000). Human development of perceptual
version of the frequency balanced experiments (Fiser &                                                                       organization. Vision Res, 40, 1301–10.
Aslin, 2002). Such interactions between statistics- and                                                                    MacKay, D. J. C. (1992). Bayesian interpolation. Neural
Gestalt-based perceptual systems are the target of future                                                                    Computation, 4, 415–447.
research.                                                                                                                  MacKay, D. J. C. (1995). Probable networks and plausi-
   Our approach is very much in the tradition that sees                                                                      ble predictions – a review of practical Bayesian meth-
the finding of independent causes behind sensory data
                                                                                                                             ods for supervised neural networks. Network: Comput
as one of the major goals of perception (Barlow, 1990).
The results demonstrate that humans can infer complex                                                                        Neural Syst, 6, 469–505.
models from experience and implicate Bayesian model                                                                        Orbán, G., Fiser, J., Aslin, R. N., & Lengyel, M. (2006).
learning as a powerful computation underlying such ba-                                                                       Bayesian model learning in human visual perception.
sic cognitive phenomena as the decomposition of visual                                                                       In NIPS 18. Cambridge, MA: MIT Press.
scenes into meaningful chunks.                                                                                             Tenenbaum, J. B. (1999). Bayesian modeling of human
                                                                                                                             concept learning. In NIPS 11. Cambridge, MA: MIT
                                  Acknowledgments                                                                            Press.
This work was supported by IST-FET-1940 program,                                                                           Tenenbaum, J. B., & Griffiths, T. L. (2003). Theory-
National Office for Research and Technology under grant                                                                      based causal inference. In NIPS 15. Cambridge, MA:
no.: NAP 2005/KCKHA005 (GO), NIH research grant                                                                              MIT Press.
HD-37082 (RNA, JF), and the Gatsby Charitable Foun-                                                                        Weiss, Y., Simoncelli, E. P., & Adelson, E. H. (2002).
dation (ML).                                                                                                                 Motion illusions as optimal percepts. Nat Neurosci, 5,
                                                                                                                             598–604.
                                              References
Barlow, H. B. (1990). Conditions for versatile learn-
 ing, Helmholtz’s unconscious inference, and the task
 of perception. Vision Res, 30, 1561-71.
                                                                                                                         650

