UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Overhypotheses
Permalink
https://escholarship.org/uc/item/2cg1z0vk
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Kemp, Charles K.
Perfors, Amy
Tenenbaum, Joshua B.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                             Learning Overhypotheses
                             Charles Kemp, Amy Perfors & Joshua B. Tenenbaum
                                                {ckemp, perfors, jbt}@mit.edu
                                           Department of Brain and Cognitive Sciences
                                              Massachusetts Institute of Technology
                            Abstract                                adults bring a lifetime of learning experience to any ex-
                                                                    periment and have already distilled overhypotheses that
   Inductive learning is impossible without overhypothe-            help them deal with most novel tasks. Infant experi-
   ses, or constraints on the hypotheses considered by the          ments are challenging for different reasons, but worth
   learner. Some of these overhypotheses must be innate,            pursuing because they can address the acquisition of
   but we suggest that hierarchical Bayesian models help
   explain how the rest can be acquired. The hierarchi-             some of the most fundamental overhypotheses. For in-
   cal approach also addresses a common question about              stance, Smith and colleagues have explored the develop-
   Bayesian models of cognition: where do the priors come           ment of the shape bias [9, 3], and Piaget and colleagues
   from? To illustrate our claims, we consider two specific         have explored how abstract kinds of knowledge (such as
   kinds of overhypotheses — overhypotheses about fea-
   ture variability (e.g. the shape bias in word learning) and      the concrete operations) can be acquired and used to
   overhypotheses about the grouping of categories into on-         support many different learning tasks.
   tological kinds like objects and substances.                        This paper argues that hierarchical Bayesian mod-
                                                                    elling [4] is a formal approach that should help to ex-
   Compared to machine-learning algorithms, humans                  plain the acquisition of overhypotheses in many differ-
are remarkable for doing so much with so little. A sin-             ent domains. Hierarchical Bayesian models (HBMs) in-
gle labelled example is enough for children to learn the            clude representations at many levels of abstraction, and
meanings of some words, and children develop grammat-               show how knowledge can be acquired at levels that are
ical constructions that are rarely found in the sentences           quite remote from the data given by experience. To illus-
they hear [2]. These inductive leaps appear even more               trate our general claim, we describe one of the simplest
impressive when we consider the many interpretations of             possible HBMs and use it to suggest how overhypothe-
the data that are logically possible but apparently never           ses about feature-variability are acquired and used to
entertained by human learners [5].                                  support categorization. One such overhypothesis is the
   Several authors have proposed that the apparent ease             shape bias, the expectation that shape is a feature that is
of human learning depends on constraints that guide in-             homogeneous within object categories. We also present
duction. This view has been applied to many cognitive               an extension of the basic model that groups categories
problems: the M-constraint and the shape bias help ex-              into ontological kinds (e.g. objects and substances) and
plain concept acquisition, universal grammar guides the             discovers the features and the patterns of feature vari-
acquisition of linguistic knowledge [2], and the develop-           ability that are characteristic of each kind.
ment of folk biology is guided by the idea that living                 The problem of overhypothesis acquisition is closely
kinds can be organized hierarchically. Constraints like             related to a problem raised by Bayesian models of cog-
these may be called framework theories or schemata, but             nition. These models usually rely on a prior distribu-
we will borrow a term of Goodman’s and refer to them                tion chosen by the modeller, and a natural response is
as overhypotheses.1                                                 to wonder where the prior comes from. HBMs address
   Some overhypotheses must be innate, but others are               this question: in the framework we adopt, learning an
probably learned. For at least two reasons the acquisi-             overhypothesis amounts to learning a prior distribution.
tion of overhypotheses has received less attention than             The two models we consider provide concrete examples
it deserves. First, the authors who have argued most                of how priors can be learned from data.
convincingly for the importance of overhypotheses often
suggest that these overhypotheses are innate [2]. Sec-                         Overhypotheses and HBMs
ond, the study of overhypothesis acquisition raises some            Goodman introduces the idea of overhypotheses using
formidable methodological challenges. Designing adult               bags of colored marbles [5]. Suppose that S is a stack
experiments to address the problem is difficult, since              containing many bags of marbles. We empty several bags
    1
                                                                    and discover that some bags contain black marbles, oth-
      Other authors distinguish between theories, schemata,         ers contain white marbles, but that each bag is uniform
scripts, and overhypotheses. There are important differences
between these varieties of abstract knowledge, but it is useful     in color. We now choose a new bag — bag n — and
to have a single term (for us, overhypothesis) that includes        draw a single black marble from the bag. This observa-
them all.                                                           tion may lead us to endorse the following hypothesis:
                                                                417

  a) Level 3: Over-overhypotheses             λ                           b)                  λ
                                                                                    1  1
      Level 2: Overhypotheses                α, β                                 α ,β                 α2 , β 2
      Level 1: Category means     θ1   θ2     θ3    θ4   ...    θn           θ1     θ2     θ3    θ4      θ5     θ6
                Data              y1   y2     y3    y4   ...    yn           y1     y2     y3    y4      y5     y6
                                                         ...
Figure 1: (a) A hierarchical Bayesian model. Each setting of (α, β) is an overhypothesis: β is the expected color
distribution for a category, and α represents the variability in color within each category. (b) A model with separate
overhypotheses for two ontological kinds meant to correspond loosely to objects and substances. α1 represents
knowledge about feature variability within the first ontological kind (object categories are homogeneous in shape but
not in material), and β 1 captures the characteristic features of the first kind (objects tend to be solid).
   H: All marbles in bag n are black.                          higher level, level 3. In Figure 1a, this knowledge is rep-
                                                               resented by λ, which captures prior knowledge about the
If asked to justify the hypothesis, we might invoke the        values of α and β. The parameter λ and the pair (α, β)
following overhypothesis:                                      are both overhypotheses, since each sets up a hypothesis
                                                               space at the next level down. We will assume that the
   O: All bags in S are uniform in color.                      level 3 knowledge is specified in advance, and show how
                                                               an overhypothesis can be learned at level 2.
   Goodman gives a precise definition of ‘overhypothesis’         Within cognitive science, linguists have provided the
but we use the term more generally to refer to any form        most familiar example of this style of model build-
of abstract knowledge that sets up a hypothesis space          ing. Language comprehension can be explained using
at a less abstract level. By this criterion, O is an over-     structural descriptions of sentences (level 1 knowledge).
hypothesis since it sets up a space of hypotheses about        Structural descriptions, in turn, can be explained with
bag n: it could be uniformly black, uniformly white,           reference to a grammar (level 2 knowledge), and the ac-
uniformly green, and so on. To give a very different           quisition of this grammar can be explained with refer-
example, Universal Grammar is an overhypothesis that           ence to Universal Grammar (level 3 knowledge). There
sets up a space of hypotheses (i.e. a space of possible        are few settings where cognitive modellers have gone be-
grammars) for language learning.                               yond three levels, but there is no principled reason to
   Hierarchical Bayesian models [4] capture this notion of     stop at level 3. Ideally, we should continue adding levels
overhypothesis by allowing hypothesis spaces at several        until the knowledge at the highest level is simple enough
levels of abstraction. We give an informal introduction        or general enough that it can be plausibly assumed to
to this modelling approach, leaving all technical details      be innate.
for the next section. Suppose that we are given a body            As the grammar-learning example suggests, it has long
of data, and we wish to account for a certain cognitive        been known that hierarchical models are capable in prin-
ability. In Goodman’s case, the data are observations          ciple of explaining the acquisition of overhypotheses.
of several bags (y i indicates the observations for bag i)     The value of hierarchical Bayesian models in particu-
and we are interested in the ability to predict the color      lar is that they explain how overhypotheses can be ac-
of next marble to be drawn from bag n. The first step          quired by rational statistical inference. Given observa-
is to identify a kind of knowledge (level 1 knowledge)         tions at the lowest level of a HBM, statistical inference
that explains the data and that supports the ability of        can be used to compute posterior distributions over en-
interest. In Goodman’s case, level 1 knowledge is knowl-       tities at the higher levels. In the model of Figure 1a,
edge about the color distribution of bags (θ i indicates       for instance, acquiring an overhypothesis is a matter of
the color distribution for the ith bag).                       acquiring knowledge at level 2. The posterior distribu-
   We then ask how the level 1 knowledge can be ac-            tion p(α, β|y) represents a normative belief about level
quired, and the answer will make reference to a more           2 knowledge — the belief, given the data y, that most
abstract body of knowledge (level 2 knowledge). For the        bags are close to uniform in color.
marbles scenario, level 2 knowledge is knowledge about            We have argued that HBMs go beyond previous hierar-
the distribution of the θ variables. As described in the       chical models proposed by cognitive scientists, but they
next section, this knowledge can be represented using          also represent an advance over the standard Bayesian
two parameters, α and β (Figure 1a). Roughly speak-            models used in cognitive science. A standard Bayesian
ing, α captures the extent to which individual bags are        model has two levels of knowledge: the elements in its
uniform in colour, and β captures the average color dis-       hypothesis space represent level 1 knowledge, and the
tribution across the entire stack of bags. If we now go        prior (generally fixed) represents knowledge at level 2.
on to ask how the level 2 knowledge might be acquired,         A common objection to Bayesian modelling is that pri-
the answer will rely on a body of knowledge at an even         ors can be chosen to approximate almost any pattern
                                                           418

                    4                                            (Dirichlet(1)) on β and an exponential distribution with
                                                                 mean λ on α (Figure 3a). For all simulations in this
     β = [0.5, 0.5] 2                                            paper we set λ = 1.
                    0
                                                                    This model is known to statisticians as a Dirichlet-
                                                                 multinomial model [4]. Using statistical notation, it can
                    4
                                                                 be written as:
     β = [0.2, 0.8] 2
                                                                                   α     ∼ Exponential(1)
                    0
                      0   0.5 1   0  0.5 1     0   0.5 1                           β     ∼ Dirichlet(1)
                        α = 0.5     α=2          α = 10                             i
                                                                                  θ      ∼ Dirichlet(α, β)
Figure 2: The 2-dimensional Dirichlet distribution serves
                                                                                  y | n ∼ Multinomial(θ i )
                                                                                    i  i
as a prior on θ, the color distribution of a bag of marbles.
Let θ2 be the proportion of black marbles within the bag.        where ni is the number of observations for bag i. As
Shown here are distributions on θ2 when the parameters           written, the model assumes we are working with a sin-
of the Dirichlet distribution (α and β) are systematically       gle dimension — for Goodman, marble color. Perhaps,
varied. When α is small, most bags are near-uniform in           however, some marbles are made from metal and others
color (θ2 is close to 0 or close to 1). When α is large, θ2      are made from glass. We deal with multiple dimensions
is expected to be close to β2 .                                  by assuming that each dimension is independently gen-
                                                                 erated according to the model, and introducing separate
of data, and that the success of a Bayesian model de-            values of α and β for each dimension. When working
pends critically on the modeller’s ability to choose the         with multiple features, we often use α to refer to the col-
right prior. HBMs disarm this objection by showing that          lection of α values along all dimensions, and β for the
knowledge at level 2 need not be specified in advance, but       set of all β vectors.
can be learned from raw data. Of course, the prior at               To fit the model to data we assume that counts y
the highest level must still be specified in advance, but        are observed for one or more bags, and use a Markov
the ultimate goal is to design models where this prior is        chain Monte Carlo (MCMC) scheme to draw a sample
simple enough to be unobjectionable.                             from p(α, β|y), the posterior distribution on (α, β). Fig-
                                                                 ures 3b and 3c show posterior distributions on α and
Computational Theory                                             β for two sets of counts. Predictions about θ new , the
We now describe one formal instantiation of the model            color distribution of a new, sparsely observed bag can
in Figure 1a. There may be other ways to formalize over-         be computed by calculating the mean prediction made
hypotheses about feature variability, but ours is perhaps        by all pairs (α, β) in the MCMC sample. Note that each
the simplest account of how these overhypotheses can             possible setting of (α, β) specifies a prior distribution on
be acquired and used to guide learning at lower levels.          θ new . By showing how knowledge about (α, β) can be
Suppose we are working with a set of k colors. Initially         acquired, we therefore show how prior distributions can
we set k = 2 and use white and black as the colors. Let          be acquired.
θ i indicate the true color distribution for the ith bag in
the stack: if 60% of the marbles in bag 7 are black, then                    Modelling behavioral data
θ 7 = [0.4, 0.6]. Let y i indicate a set of observations of      Since Goodman, psychologists have confirmed that
the marbles in bag i. If we have drawn 5 marbles from            adults [8] and children [7] have overhypotheses about
bag 7 and all but one are black, then y 7 = [1, 4].              feature variability, and use them to make inductive leaps
   We assume that y i is drawn from a multinomial distri-        given very sparse data. Nisbett et al. [8] asked subjects
bution with parameter θ i : in other words, the marbles          to imagine they were exploring an island in the South-
responsible for the observations in y i are drawn inde-          eastern Pacific. As part of the task, subjects were told
pendently at random from the ith bag, and the color of           that they had encountered a single member of the Bar-
each depends on the color distribution θ i of that bag.          ratos tribe, and that the tribesman was brown and obese.
The vectors θ i are drawn from a Dirichlet distribution          Based on this single example, subjects concluded that
parameterized by a scalar α and a vector β. Here β               most Barratos were brown, but gave a much lower es-
represents the expected distribution of colors across the        timate of the proportion of obese Barratos (Figure 3d).
stack and α captures the notion of feature variability           When asked to justify their responses, subjects often said
(Figure 2). The larger the value of α, the more likely           that tribespeople were “homogeneous with respect to
that the color distribution for any given bag will be close      color” but “heterogeneous with respect to body weight.”
to the vector β. When α is small, however, each individ-            To apply our model to this task, we replace bags of
ual bag is likely to be near-uniform in color, and β will        marbles with tribes. Suppose we have observed 20 mem-
determine the relative proportions of ‘mostly black’ and         bers from each of 20 tribes. Half the tribes are brown
‘mostly white’ bags.                                             and the other half are white, but all of the individuals in
   Each possible setting of (α, β) is an overhypothesis. In      a given tribe share the same skin color. Given these data,
order to discover values for these variables, we need prior      the model learns a posterior distribution on α indicating
distributions on β and α. We use a uniform distribution          that α is probably small, which means that skin color
                                                             419

                                                                        a)                                         0.8
   a)                        b)             c)                                            Training      T1   T2 b)
             1                  1              1                                                                   0.7              T1
                                                                          Category     11223344         1    5
 p(log(α)) 0.5                0.5            0.5                                                                   0.6              T2
                                                                          Shape        11223344         1    5
             0                  0              0                          Texture      12345678         1    9     0.2
                 −5 −2 1          −5 −2 1        −5 −2 1    log(α)        Color        12345678         1    9     0.1
             8                  8              8                          Size         12121212         1    1
                                                                                                                     0
     p(β2 )                                                                                                               e    e     r
             4                  4              4                                                                        ap xtur colo
                                                                                                                     sh    te
             0                  0              0                        Figure 4: (a) Data based on Smith et al. [9]. Each col-
                0   0.5  1        0  0.5  1      0  0.5   1   β2
                                                                        umn represents an object, and there are 10 possible col-
             8                  8              8                        ors, textures, and shapes, and 2 possible sizes. First and
  p(θ2new )  4                  4              4                        second-order generalization were tested using exemplars
             0                  0              0
                                                                        T1 and T2 . (b) Generalizations for the data in (a). Three
                0   0.5  1        0  0.5  1      0   0.5  1   θ2new     possible matches were provided for each exemplar. The
   d)                 people            model                           plot shows probabilities (normalized) that each match
               1                                 skin color             belongs to the same category as the exemplar. The
                                                 obesity                model makes exact predictions about these probabilities:
       θ2new 0.8                                                        we computed 30 MCMC estimates of these predictions,
             0.6                                                        and the error bars represent the standard error of the
                                                                        mean.
             0.4
                    1   3    20      1    3   20   examples             Novel feature values
Figure 3: (a-c) Distributions on log(α) and β given three
                                                                        Our model of the Barratos task does not address an im-
patterns of data: (a) before any data have been observed;               portant kind of reasoning that overhypotheses support:
(b) after observing 10 all-white bags and 10 all-black                  reasoning about new feature values. At least two over-
bags; (c) after observing 20 mixed bags inspired by the                 hypotheses might be induced from the data in Figure 1a:
obesity condition of the Barratos task. The final row                   the first states that all bags are uniform in color, and the
shows beliefs about the color distribution (θ new ) of a new            second states that every bag is entirely white or entirely
bag from which a single black marble has been drawn.                    black. These two overhypotheses have very different con-
After 20 bags that are either all white or all black (b),               sequences: for example, only the first can handle a case
the model realizes that most bags are near-uniform in                   where a single green marble is drawn from a new bag.
color (α is small), and that about half of these bags are                  Many real-world problems involve inferences about
black (β2 is around 0.5). These posterior distributions                 novel features. Children know, for example, that ani-
allow the model to predict that nearly all the marbles                  mals of the same species tend to make the same sound.
                                                                        Observing one horse neigh is enough to conclude that
in the new, sparsely observed bag will be black (θ2new is
                                                                        most horses neigh, even though a child may never have
close to 1). (d) Generalizations about a new tribe after                heard an animal neigh before. Similarly, children show a
seeing 1, 3, or 20 obese, brown-skinned individuals from                “shape bias:” they know that shape tends to be homoge-
that tribe. Human generalizations are replotted from                    neous within object categories. Given a single exemplar
Nisbett et al. [8].                                                     of a novel object category, children extend the category
                                                                        label to similarly shaped objects ahead of objects that
tends to be homogeneous within tribes (Figure 3b). We                   share the same texture or color as the exemplar.
can also make predictions about a sparsely observed new                    The model in Figure 1a deals naturally with infer-
tribe: having observed a single, brown-skinned member                   ences like these. We illustrate using stimuli inspired by
of a new tribe, the posterior distribution on θ new indi-               the work of Smith et al. [9]. In their second experiment,
cates that most members of the tribe are likely to be                   Smith et al. [9] trained 17-month olds on four novel cat-
brown (Figures 3b and 3d).                                              egories with two exemplars each. Category labels were
   Suppose now that obesity is a feature that varies                    provided during training. Within each category, the two
within tribes: a quarter of the 20 tribes observed have                 exemplars had the same shape but differed in size, tex-
an obesity rate of 10%, and the remaining quarters have                 ture and color (Figure 4a). After training, the authors
rates of 20%, 30%, and 40%. Obesity is represented in                   tested first-order generalization by presenting T1 , an ex-
our model as a second binary feature, and the posterior                 emplar from one of the training categories, and asking
distributions on α and β (Figure 3c) indicate that obe-                 children to choose another exemplar from the same cat-
sity varies within tribes (α is high), and that the base                egory as T1 . Three possible matches were provided, each
rate of obesity is around 30% (β2 is around 0.3). Again,                of which matched T1 in exactly one feature (shape, color
we can use these posterior distributions to make predic-                or size). Children preferred the shape match, showing
tions about a new tribe, and our model now requires                     that they were sensitive to feature distributions within a
many observations before it concludes that most mem-                    known category. Smith et al. [9] also tested second-order
bers of the new tribe are obese (Figure 3b).                            generalization by presenting children with T2 , an exem-
                                                                    420

     a) 1                          b)                           are provided for each of two bags, the model has strong
                                                                evidence about the color distribution of each bag: one
        0.4
     α                                                          is mostly black and the other is mostly white. The case
                                   c)
        0.2                                                     where two observations are provided for each of 32 bags
        0.1
                                                                represents the other extreme. Now the evidence about
                                                                the composition of any single bag is weak, but taken to-
            1   2   4 8 16 32                                   gether, these observations provide strong support for the
                   samples per bag                              idea that α is low and most bags are homogeneous.
Figure 5: (a) Mean α values after seeing 32 white mar-             The U-shaped curve in Figure 5 is a novel prediction
bles and 32 black marbles. Low values of α indicate             of our model that could be tested in developmental ex-
that bags are expected to be homogeneous in color. The          periments. More generally, the curve illustrates how a
model is most confident that bags are homogeneous when          learner might become relatively certain about an overhy-
                                                                pothesis (e.g. the value of α) even though she is uncertain
given the data in (c). (b) Data set with 32 samples per
                                                                about the individual entities described by the overhy-
bag (c) Data set with 2 samples per bag                         pothesis (e.g. the θ values for categories when only two
                                                                exemplars per category are observed). This insight ap-
plar from a novel category. Again, children preferred the       pears relevant to many learning problems: to give just
shape match, revealing knowledge that shape in general          one additional example, a hierarchical model of grammar
is a reliable indicator of category membership. Note that       induction may be able to explain how a learner becomes
this result depends critically on the training summarized       confident about some aspect of a grammar even though
by Figure 4a: children of this age do not normally reveal       most of the individual sentences that support this con-
a shape bias on tests of second-order generalization.           clusion are poorly understood.
   We supplied our model with counts y i computed from
the feature vectors in Figure 4a. The key modelling step        Discovering ontological kinds
is to allow for more values along each dimension than           The model in Figure 1a is a simple HBM that acquires
appear in the training set. We allowed for 10 shapes, 10        something like the shape bias, but to match the capaci-
colors, 10 textures and 2 sizes. For example, the count         ties of a child it is necessary to apply the shape bias se-
vector y 1 says that the observed exemplars of category 1       lectively — to object categories, for example, but not to
include 2 objects with shape value 1 and no objects with        substance categories. Selective application of the shape
shape value 10. This policy allows the model to han-            bias appears to demand knowledge that categories are
dle shapes, colors and textures it has never seen during        grouped into ontological kinds and that there are dif-
training, but assumes, of course, that the model is able        ferent patterns of feature variability within each kind.
to recognize a novel shape as a kind of shape, a novel          Before the age of three, for instance, children appear to
color as a kind of color, and so on.                            know that shape tends to be homogeneous within ob-
   Figure 4b shows the patterns of generalization pre-          ject categories but heterogeneous within substance cat-
dicted by the model. Smith et al. [9] report that shape         egories, that color tends to be homogeneous within sub-
matches are chosen 88% of the time given exemplar T1 ,          stance categories but heterogeneous within object cate-
and 70% of the time given exemplar T2 . The model               gories, and that both shape and texture tend to be ho-
reproduces this general pattern: shape matches are pre-         mogeneous within animate categories.
ferred in both cases, and are preferred more strongly              Figure 1b shows how we can give our model the ability
when the exemplar belongs to a familiar category.               to discover ontological kinds. The model assumes that
   Smith et al. [9] also measured real-world generaliza-        categories may be grouped into ontological kinds, and
tion by tracking vocabulary growth over an eight week           that each kind is associated with a different α and β.
period. They showed that experience with the eight ex-          The model, however, is not told which categories belong
emplars in Figure 4a led to a significant increase in the       to the same kind, and is not even told how many differ-
number of object names used by children. Our model              ent kinds it should look for. Instead, we give it a prior
helps to explain this striking result. Even though the          distribution on the partition of categories into kinds. In-
training set includes only four categories, the results in      tuitively, this prior should assign some probability to all
Figure 4b show that it contains enough statistical infor-       possible category partitions, but favor the simpler par-
mation to establish or reinforce the shape bias. Similarly,     titions — those that use a small number of kinds. We
our model explains why providing only two exemplars             satisfy both criteria using a prior generated by a Chinese
per category is sufficient. In fact, if the total number of     Restaurant Process [1].
exemplars is fixed, our model predicts that the best way           The new model can be written as:
to teach the shape bias is to provide just two exemplars
per category. We illustrate by returning to the marbles                           z | ncat ∼ CRP(γ)
scenario.                                                                       αk         ∼ Exponential(λ)
   Each point in Figure 5 represents a simulation where                           k
64 observations of marbles are evenly distributed over                          β          ∼ Dirichlet(1)
some number of bags. The marbles drawn from any                                 θ i
                                                                                           ∼ Dirichlet(αzi β zi )
given bag are uniform in color — black for half of the
bags and white for the others. When 32 observations                             y i | ni   ∼ Multinomial(θ i )
                                                            421

           a)                                       b)                                 c)              solid non-solid
                             Training     S   N                                S
                Category   11223344       5   6                                                                          1
                                                                               N                  1
                                                                                       category
                Shape      11223456       7   8      0.55                                         2
                Material   12345566       7   8       0.5                                         3                      0.5
                Size       12121212       1   1                                                   4
                Solidity   11112222       1   2      0.45
                                                                                                      1 2 5 3 4 6
                                                             shape      material                         category
Figure 6: (a) Data used to test the model in Figure 1b. Second-order generalization was tested using solid and
non-solid exemplars (S and N ). (b) Generalizations for the data in (a). The model chooses the shape match given
the solid exemplar and the material match given the non-solid exemplar. (c) Entry (i, j) in the matrix is the posterior
probability that categories i and j belong to the same ontological kind. The model groups categories 1, 2 and 5
(solid categories) and 3, 4 and 6 (non-solid categories).
where ncat is the number of categories, γ is the concen-           abilistic approach extends naturally to contexts where
tration parameter for the CRP, zi is the kind label for            structured representations are required: computational
category i and there is a separate αk and β k for each             linguists, for example, work with probabilistic grammars
ontological kind k.                                                that generate parse trees. It is less clear how a connec-
   Jones and Smith [6] have shown that training young              tionist approach might deal with representations more
children on a handful of suitably structured categories            complex than lists of features.
can promote the acquisition of ontological knowledge.                 Although we have argued that overhypotheses can be
We gave our model a data set of comparable size (Fig-              acquired by HBMs, we do not claim that overhypotheses
ure 6a). During training, the model saw two exemplars              can be generated out of thin air. Any HBM will assume
from each of four categories: two object categories and            that the process by which each level is generated from
two substance categories. Exemplars of each object cat-            the level above is known, and that the prior at the top-
egory were solid, matched in shape, and differed in mate-          most level is provided. Any account of induction must
rial and size. Exemplars of each substance category were           rely on some initial knowledge: the real question for a
non-solid, matched in material, and differed in shape and          learning framework is whether it allows us to build mod-
size. Second-order generalization was tested using exem-           els that require no initial assumptions beyond those we
plars from novel categories — one test exemplar (S) was            are willing to make. Whether the hierarchical Bayesian
solid and the other (N ) was not. Figure 6b shows that             approach will meet this challenge is not yet clear, but it
the model chooses a shape match for the solid exemplar             deserves to be put to the test.
and a material match for the non-solid exemplar.
                                                                   Acknowledgments Supported by the William Asb-
   Figure 6c confirms that the model correctly groups
                                                                   jornsen Albert memorial fellowship (CK), a NDSEG fel-
the stimuli into two ontological kinds: object categories
                                                                   lowship (AP) and the Paul E. Newton chair (JBT).
and substance categories. This discovery is based on the
characteristic features of ontological kinds (β) as well                                          References
as patterns of feature variability within each kind (α).           [1] Aldous, D. (1985). Exchangeability and related topics.
If the object categories are grouped into kind k, αk in-
                                                                      In École d’été de probabilités de Saint-Flour, XIII—1983,
dicates that shape is homogeneous within categories of                pages 1–198. Springer, Berlin.
that kind, and β k indicates that categories of that kind          [2] Chomsky, N. (1980). Rules and Representations. Basil
tend to be solid. The β parameter, then, is responsi-                 Blackwell, Oxford.
ble for the inference that the test exemplar S should be           [3] Colunga, E. and Smith, L. B. (2005). From the lexicon to
grouped with the two object categories, since all three               expectations about kinds: a role for associative learning.
categories contain solid objects.                                     Psychological Review, 112(2).
                                                                   [4] Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B.
                                                                      (1995). Bayesian data analysis. Chapman & Hall, New
                      Discussion                                      York.
We presented hierarchical Bayesian models that help                [5] Goodman, N. (1955). Fact, Fiction, and Forecast. Har-
explain the acquisition of the shape bias, and of over-               vard University Press, Cambridge.
hypotheses about feature variability within ontological            [6] Jones, S. S. and Smith, L. B. (2002). How children know
kinds. We know of no previous attempts to provide ratio-              the relevant properties for generalizing object names. De-
                                                                      velopmental Science, 5(2):219–232.
nal computational theories of the acquisition of the shape         [7] Macario, J. F., Shipley, E. F., and Billman, D. O. (1990).
bias, or of other overhypotheses about feature variabil-              Induction from a single instance: formation of a novel cat-
ity. Colunga and Smith [3] present a connectionist model              egory. Journal of Experimental Child Psychology, 50:179–
that acquires knowledge of this sort, but our approach                199.
is different in emphasis and explanatory effect. We pro-           [8] Nisbett, R. E., Krantz, D. H., Jepson, C., and Kunda,
                                                                      Z. (1983). The use of statistical heuristics in everyday
vided a computational theory but have not attempted to                inductive reasoning. Psychological Review, 90(4):339–363.
specify the psychological mechanisms by which it might             [9] Smith, L. B., Jones, S. S., Landau, B., Gershkoff-Stowe,
be implemented, and Colunga and Smith [3] describe                    L., and Samuelson, L. (2002). Object name learning pro-
a process model but do not provide a rational compu-                  vides on-the-job training for attention. Psychological Sci-
tational theory. A second difference is that our prob-                ence, 13(1):13–19.
                                                             422

