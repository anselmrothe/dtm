UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Knowledge-driven Gaze Control in the NIM Model
Permalink
https://escholarship.org/uc/item/61f4s3d3
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Lacroix, Joyca P.W.
Murre, Jaap M.J.
Postma, Eric O.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                            Knowledge-driven Gaze Control in the N IM Model
                                          Joyca P. W. Lacroix (j.lacroix@cs.unimaas.nl)
                                               Eric O. Postma (postma@cs.unimaas.nl)
                                    Department of Computer Science, IKAT, Universiteit Maastricht
                                          Minderbroedersberg 6a, Maastricht, The Netherlands
                                                Jaap M. J. Murre (jaap@murre.com)
                                          Department of Psychology, University of Amsterdam
                                         Roeterstraat 15, 1018 WB Amsterdam, The Netherlands
                              Abstract                                    & Ross, 2001), the initial versions of the N IM model ’fixated’
                                                                          randomly along the contours in an image. However, random
   In earlier work, we proposed a recognition memory model, the           sampling along contours can hardly be considered to agree
   Natural Input Memory (N IM) model, that operates directly on           with the active context-dependent scanning of a visual scene
   digitized natural images. When presented with a natural im-
   age, the N IM model employs a biologically-informed percep-            that is performed by humans (e.g., Rajashekar, Cormack, &
   tual preprocessing method that takes local samples (i.e., eye          Bovik, 2002). In the dynamic process of actively scanning the
   fixations) from the image and translates these into a similarity-      visual scene, eye fixations are guided by bottom-up and top-
   space representation. Recognition is based on a matching               down processes (Karn & HayHoe, 2000; Henderson, 2003;
   of incoming and previously stored representations. In this             Oliva, Torralba, Castelhano, & Henderson, 2003). Several
   paper, we investigate whether it is possible to extend the
   N IM model with a gaze control mechanism to select relevant            studies showed that bottom-up processes draw the eyes to-
   eye-fixation locations based on scene-schema knowledge and             ward salient visual features such as high edge density (see,
   episodic knowledge. We perform two experiments. In the                 e.g., Mannan, Ruddock, & Wooding, 1996) and local con-
   first experiment, we test whether the similarity-space repre-          trast (see, e.g., Parkhurst & Niebur, 2003). Based on these
   sentations can be used to infer scene-schema knowledge of a
   specific category of natural stimuli, i.e., natural face images.       findings, many models of gaze control employed a bottom-up
   In the second experiment, we examine how the model can                 approach (e.g., Braun, Koch, Lee, & Itti, 2001; Rao, Zelinsky,
   use the scene-schema knowledge in combination with stored              Hayhoe, & Ballard, 2002). Often, a so-called ‘saliency map’
   episodic knowledge to direct the gaze toward relevant spa-             is constructed that marks those image regions that are visu-
   tial locations when performing a categorization task. Our re-          ally distinct from their surround in one or more visual fea-
   sults show that the spatial structure of face images can be in-
   ferred from the N IM model’s similarity-space representations,         tures (Koch & Ullman, 1985). Gaze is directed to locations
   i.e., scene-schema knowledge can be acquired for the category          that are marked as highly salient on the saliency map. While
   of face images. Moreover, our results show that extending              visual saliency has often been used to control gaze in artifi-
   the N IM model with a gaze control mechanism that combines             cial systems, the use of top-down processes has not received
   scene-schema knowledge with stored episodic knowledge, en-
   hances performance on a categorization task.                           as much attention. Top-down processes rely on stored knowl-
                                                                          edge to select the most informative location to orient the eyes
                                                                          (see, e.g., Henderson, 2003). Several studies showed that hu-
                         Introduction                                     man gaze control relies more on top-down processes than on
                                                                          bottom-up processes when performing an active visual task
We proposed a recognition memory model called the Natural                 with meaningful stimuli (see, e.g., Oliva et al., 2003). The
I nput M emory (N IM ) model (Lacroix, Murre, Postma, & Van               top-down processes are driven by two types of knowledge.
den Herik, 2004, 2006). The N IM model differs from ex-                   One type of knowledge is episodic knowledge, which is ob-
isting memory models in that it encompasses a biologically-               tained from one or more encounters with the stimulus or vi-
informed perceptual front-end. As a consequence, the N IM                 sual scene (Henderson, 2003). A second type is knowledge
model can take realistic image-like stimuli as input. The per-            about the general spatial arrangement of a certain category of
ceptual front-end takes local samples (i.e., eye fixations) at            stimuli or scenes (i.e., scene-schema knowledge; Henderson,
selected locations in a natural image and translates these into           2003). Since natural objects and scenes within a particular
feature vectors. The extracted feature vectors contain infor-             category contain spatial regularities, the similarity-space rep-
mation on oriented edges at multiple scales and form an ef-               resentations contain information about both the object iden-
ficient basis for object recognition (see, e.g., Rao & Ballard,           tity and its spatial structure.
1995). Perceptually similar image regions result in similar
feature vectors. A representation space in which similarity                  In this paper, we investigate whether it is possible to com-
corresponds to proximity is often called a ‘similarity space’             bine episodic and scene-schema knowledge in the N IM model
(e.g., Shepard, 1957; Edelman, 1995; Edelman & Duvdevani-                 to guide the selection of relevant fixation locations. Spatial
Bar, 1997; Goldstone & Son, 2005). Recognition decisions                  knowledge is not represented explicitly in our model. How-
in the model are based on a matching of stored and incoming               ever, since natural objects and scenes within a particular cat-
similarity-space representations. By fixating different image             egory contain spatial regularities, the similarity-space repre-
regions, the N IM model has a natural way of dealing with                 sentations contain information about both the object identity
spatial (overt) attention. Since humans tend to place fixations           and its spatial structure (i.e., scene-schema knowledge).
at or near contours in a visual scene (e.g., Norman, Phillips,               For our investigations, we performed two experiments. In
                                                                     1657

the first experiment, we test whether the similarity-space rep-
resentations can be used to infer scene-schema knowledge of
a specific category of natural stimuli, i.e., natural face images.
In the second experiment, we examine how the model can
use the scene-schema knowledge in combination with stored
episodic knowledge to direct the gaze toward relevant spatial
locations in the image.
   The outline of the remainder of this paper is as follows.
In the following section, we briefly discuss how similarity
space representations are built in the N IM model. Then, in
the next section, we attempt to train a neural network to map
the similarity-space representation to the spatial representa-
tion (the coordinates of the image area) for face images. Sub-
sequently, we examine how the model can use the mapping
in combination with episodic knowledge of previously en-
countered stimuli to direct gaze toward relevant spatial lo-
cations in the image. Finally we compare gaze control in
the extended N IM model with knowledge-driven gaze con-
trol in existing systems and draw two conclusions. The first
conclusion is that spatial structure of faces can be extracted
from their similarity-space representations. The second con-
clusion is that a gaze control mechanism that combines scene-
schema knowledge with episodic knowledge of a specific set
of encountered stimuli, can enhance performance on a face-
categorization task.
                        The N IM model
The N IM model encompasses: (1) a preprocessing stage that                     Figure 1: The Natural Input Memory (N IM) model.
translates a natural image into a feature-vector representation,
and (2) a memory stage that either stores the representation or
makes a recognition decision based on a matching of an in-                  In Fig 1, the sets of white squares in each of the filtered im-
coming representation and previously stored representations.            ages correspond to the fixated image regions (centered at the
Fig. 1 shows a schematic diagram of the preprocessing and               fixation locations shown as crosses in the face image) and the
the memory stages of the N IM model. The face image is an               five bold dots in the similarity space represent the correspond-
example of a natural image. The left and right side of the di-          ing feature vectors (after P CA) that contain the contents of
agram correspond to the two stages of the N IM model: the               the filtered images within the fixated regions. A preprocessed
perceptual preprocessing stage (left) and the memory stage              image is thus represented by a number of low-dimensional
(right).                                                                feature vectors, each containing partial information about the
   Since our first experiment focuses on the N IM model’s               image.
similarity-space representations, we first discuss the extrac-              The multi-scale wavelet decomposition followed by P CA
tion of representations (i.e., the perceptual preprocessing              is a biologically plausible model of visual processing in the
stage). The memory stage will be discussed later when the                brain (cf., Palmeri & Gauthier, 2004). Moreover, several
model is tested on a categorization task.                                studies showed that the distances between representations in
   The preprocessing stage in the N IM model is based on the             the similarity space that results from preprocessing the input
processing of information in the human visual system. Mo-                with the multi-scale wavelet decomposition and P CA, agree
tivated by eye fixations in human vision, the preprocessing              well with dissimilarities as perceived by humans (see, e.g.,
stage takes local samples (i.e., fixations) at selected locations        Dailey, Cottrell, Padgett, & Adolphs, 2002; Lacroix et al.,
in a natural image. The features extracted at fixation loca-             2006).
tions in the face image consist of the responses of different
derivatives of Gaussian filters. The filter responses model the                          Extending the N IM model
responses of neurons in the visual processing area V1. By ap-            In our first experiment, we investigated whether it is possible
plying a set of filters at multiple scales and orientations, a rep-      to learn the spatial regularities of the visual features of one
resentation of the image area centered at the fixation location          category of natural stimuli, i.e., face images. In particular,
is obtained (Freeman & Adelson, 1991). After feature-vector              we assessed to what extent a mapping can be learned from
extraction, we apply a principal component analysis (P CA) to            similarity-space representation of a face image region to its
the extracted feature vectors in order to reduce their dimen-            spatial location, i.e., the similarity-to-spatial mapping. Such
sionality. We selected the first 50 principal components, since          a mapping can incorporate the scene-schema knowledge of an
it has been shown that approximately 50 components are suf-              extended N IM model on the basis of which relevant eye fix-
ficient to accurately represent faces (e.g., Hancock, Burton,            ation locations can be selected. In our experiment, we used
& Bruce, 1996).                                                          a set of 60 digitized gray-scale images of male human faces
                                                                    1658

without glasses that were used in several other studies (e.g.,
Busey & Tunnicliff, 1999; Lacroix et al., 2004, 2006). All
face images were downscaled to 156 × 198 pixels. Three ex-
amples of these faces are shown in Fig. 2. Below, we discuss
Figure 2: Three examples of faces from the set of faces used
in the experiments.                                                                                  (a)
the experimental set-up for learning the similarity-to-spatial
mapping and present the results.
Learning the similarity-to-spatial mapping
We trained different feedforward neural networks to map
similarity-space representations (i.e., fixation vectors) onto
spatial representations (i.e., image coordinates). This entails
learning a mapping between the 50-dimensional feature vec-
tors of an image area and their associated 2-dimensional spa-
tial locations. Therefore, each network was dimensioned ac-
cordingly: an input layer of 50 nodes (encoding the feature
vector), a hidden layer of h nodes, and 2 output nodes (en-
coding the x and y coordinates). Since the different faces cov-
ered about the same area within each image, the network was
trained on absolute spatial coordinates x and y. Both the x and                                      (b)
y coordinates where mapped onto the unit interval. Within
each network, the transfer functions for the hidden and out-          Figure 3: Mean error percentages for networks with h = 0, 2,
put nodes were defined as hyperbolic tangent sigmoid func-
                                                                      4, 6, 8, 10, and 12 hidden nodes after 400 training epochs, (a)
tions (range [−1, +1]) and standard sigmoid functions (range
[0, 1]), respectively.                                                for the x coordinates, and (b) for the y coordinates.
   The data set of face images was subdivided into two parts:
a training set and a test set. From these sets, fixation vec-
tors were extracted yielding training feature vectors and test        an average error of 15.44 (7.8%) pixels in the y direction.
feature vectors, respectively. Feature vectors were extracted         Adding hidden nodes slightly decreased the average spatial
randomly along the contours in the images. During training,           error to a minimum of 8.07 pixels (5.2%) in the x direction
the network was fed with 30,000 training feature vector coor-         and 11.21 pixels (5.7%) in the y direction for the networks
dinate pairs. After training the network was tested on 30,000         with h = 12 hidden nodes. The correlation between actual
test feature vectors. Each test vector was submitted to the in-       spatial locations and those predicted by the networks with 12
put layer of the trained network yielding an estimate of the          hidden units is R = 0.96 in both the x and y directions.
spatial coordinates as output. We varied the number of nodes             The results indicate that the spatial structure of face im-
in the hidden layer, h, from 0 to 12 to assess how this affected      ages can be inferred quite reliably from the N IM model’s
the accuracy of the mapping.                                          similarity-space representations. Knowledge about the spatial
                                                                      origin of visual input is implicitly present in the similarity-
Results                                                               space representations. The explanation for this is twofold.
Figs. 3(a) and 3(b) show the mean error percentages for the           First, the spatial regularity is considerable across the faces;
x and y coordinates for networks with h = 0, 2, 4, 6, 8, 10,          similar spatial locations in different images, contain similar
and 12 hidden nodes after 400 training epochs. The error              information. Second, the overlap structure in the fixation vec-
percentages are the distances between the mapped spatial x            tors ensures that the feedforward networks perform well on
and y coordinates and the actual spatial x and y coordinates          fixations that the model has not been trained on. Spatially
of the fixation vector.                                               adjacent fixations give rise to similar feature vectors. Hence,
   Overall, all networks were able to learn the similarity-to-        they provide a cue for the scene schema.
spatial mapping quite accurately. Adding hidden nodes re-                Using a simple feedforward neural network, it is possible
sulted in marginally better accuracy. The networks with h = 0         to learn a mapping between the similarity-space representa-
hidden nodes (i.e., single-layer perceptrons) showed an aver-         tion of specific visual input (from objects within one cate-
age spatial error of 10.65 pixels (6.8%) in the x direction and       gory) and its spatial location. After many encounters with
                                                                 1659

visual input from objects within a particular category, a net-        sists of the following four steps:
work can predict the spatial location of specific visual input        1. Select a central fixation location F.
quite accurately. Therefore, it can be said that the network has     2. Define the W × H pixels region centered at F as the ‘target
acquired knowledge about the spatial arrangement of objects          region’.
within the category, i.e., scene-schema knowledge (Hender-           3. Determine the feature vectors for all fixation locations
son, 2003).                                                          within the target region that lie on a contour. These feature
   In the second experiment, we examine the N IM model ex-           vectors are called ‘target feature vectors’.
tended with a gaze control mechanism based on the acquired           4. For each category c, find the target feature vector with the
scene-schema knowledge (i.e., the similarity-to-spatial map-         smallest Euclidean distance dc to a labeled feature vector of
ping) to guide the selection of relevant eye-fixation locations.     category c. The ‘belief’ in a particular category c is defined
                                                                     as 1/dc .
  Categorization with the extended N IM model                            The four steps are repeated f times, where f represents
                                                                     the number of fixations. The category with the largest belief
We extended the N IM model with the similarity-to-spatial
                                                                     value after f fixations is defined as the category associated
mapping acquired in the first experiment. Using the mapping,
                                                                     with the image to be categorized.
the extended N IM model can select novel fixation locations
by means of a control mechanism operating on the similar-            The gaze control mechanism The main goal of the gaze
ity space only. In the second experiment, the extended N IM          control mechanism is to direct gaze to spatial locations that
model was tested on a face-categorization task. In the cate-         are relevant to solve the categorization task. To direct gaze,
gorization task, the model categorized a face as one of the n        the mechanism uses two types of knowledge: (1) scene-
faces that it had previously encountered. In order to assess to      schema knowledge (i.e., the similarity-to-spatial mapping,
what extent categorization can profit from knowledge-driven          and (2) episodic knowledge (i.e., previously stored represen-
gaze control, we compared the categorization performance of          tations). The scene-schema knowledge is used to infer the
the extended N IM model with that of the N IM model without          spatial locations of the fixations that were stored during pre-
the gaze control extension.                                          vious encounters with the faces (i.e., the episodic knowledge).
   The N IM model was originally developed to model be-              From the previously stored fixations, the gaze control mech-
haviorally obtained recognition performance for individual           anism selects the most heterogeneous (spatial) cluster of fix-
items and general findings from recognition-memory studies           ations (i.e., the cluster that contains the largest variety of la-
(Lacroix et al., 2004, 2006). Therefore, in order to model           bels). For each subsequent location, the gaze control mech-
categorization rather than recognition the model needs to be         anism selects the most heterogeneous cluster of stored fixa-
adapted slightly. Below, we discuss the categorization version       tions that were not in a previously selected cluster, and so
of the (extended) N IM model and discuss the performance of          forth.
the extended N IM model with the gaze control mechanism on
the categorization task.                                             The categorization experiment
                                                                     In the categorization experiment, the N IM model and the
Adapting the N IM model for categorization                           extended N IM model were presented with n = 10 faces.
The N IM model encompasses two stages: the perceptual pre-           In both models, for each face s = 10 feature vectors were
processing stage and the memory stage. In addition, the ex-          extracted along the contours in the image and stored. Then,
tended N IM model employs a gaze control mechanism to di-            during categorization, both models produced a categorization
rect gaze to relevant image regions. The preprocessing stage         response to a test face on the basis of f fixations. In step 1
was discussed previously. Below, we discuss the two pro-             of the categorization process (described above), the fixation
cesses of the memory stage of the categorization version of          location is selected. While the original N IM model selects
the N IM model: the storage process and the categorization           fixation locations randomly along the contours in the image,
process. Subsequently, we discuss the gaze control mecha-            the extended N IM model selects fixation locations with the
nism.                                                                gaze control mechanism. For each experimental run, the n
                                                                     faces were randomly selected from the 30 faces in the dataset
The storage process In the N IM model, samples of natu-              that were not used to learn the similarity-to-spatial mapping.
ral images are retained (i.e., ‘stored’) in a similarity space.      The W and H values of the target region were set to 21 and
The storage process stores preprocessed natural images. As           27 pixels, respectively. Experiments employed f = 2 and
stated before, a preprocessed natural image is represented by         f = 3 fixations.
a number of fixations, i.e., low-dimensional feature vectors
in a similarity space, each corresponding to the pre-processed
image contents at the fixation location. For each of the n faces     Results
that the model encountered in the categorization task, s fea-
                                                                     Fig. 4 shows the mean performances of the original and the
ture vectors were selected randomly along the contours in the
                                                                     extended N IM models on the categorization task. Error bars
image and stored with the appropriate label (i.e., ‘1’ for face
                                                                     indicate the standard error of the mean. Categorization per-
1, ‘2’ for face 2, and so forth).
                                                                     formance is expressed as the percentage of correctly catego-
The categorization process The categorization process of             rized faces. The extended N IM model performed significantly
the N IM model relies on the distance between newly encoun-          better than the original one on both the experiments employ-
tered (unlabeled) and stored (labeled) feature vectors. Given        ing f = 2 and f = 3 selected fixation locations. Based on
an image to be categorized, the categorization process con-          the scene-schema knowledge incorporated in the similarity-
                                                                 1660

                                                                      cessed by the human visual system such as color, intensity,
                                                                      contrast, orientation, edge junctions, and motion (see, e.g.,
                                                                      Koch & Ullman, 1985; Itti & Koch, 2000; Parkhurst, Law, &
                                                                      Niebur, 2002). Also, in order to discover certain important
                                                                      visual dimensions for generating a saliency map, a few stud-
                                                                      ies analyzed which visual dimensions best distinguish fixated
                                                                      image regions from non-fixated regions (see, e.g., Mannan
                                                                      et al., 1996; Parkhurst & Niebur, 2003; Henderson, Brock-
                                                                      mole, Castelhano, & Mack, to appear).
                                                                         Several studies showed that, under some conditions, fixa-
                                                                      tion patterns predicted by image-driven gaze-control models
                                                                      correlate well with those observed in human subjects (see,
                                                                      e.g., Parkhurst et al., 2002). In their study, Parkhurst et al.
                                                                      (2002) recorded human scan paths when viewing a series of
                                                                      complex natural and artificial scenes. They found that human
                                                                      scan paths could be predicted quite accurately by stimulus
                                                                      saliency which was based on color, intensity, and orientation.
Figure 4: Categorization performance as a function of the             While the image-driven approach was successful in predict-
number of selected fixation locations of the original and ex-         ing human fixation patterns in some tasks, it is inaccurate pre-
                                                                      dicting fixation patterns in an active task that uses meaning-
tended NIM models. Error bars indicate the standard error of
                                                                      ful stimuli (see, e.g., Oliva et al., 2003; Turano, Geruschat,
the mean.                                                             & Baker, 2003; Henderson et al., to appear). For example,
                                                                      Turano et al. (2003) showed that a saliency model performed
to-spatial mapping, the N IM model is able to infer the spatial       as accurate as a random model in predicting the scan paths
origin of the episodic knowledge that it has stored. Using            of human subjects during a real-world activity. In contrast,
the spatial information present in the similarity-space repre-        they found that a model that used only knowledge-driven (i.e.,
sentations, it can select fixation locations in regions that are      top-down) gaze control outperformed the random model. Ob-
most likely to contain the visual information needed to solve         viously, visual saliency alone cannot account for the human
the categorization task. Our results show, that extending the         fixation patterns when performing certain tasks. Similar re-
N IM model with a gaze control mechanism that uses scene-             sults were found by Henderson et al. (to appear) who ana-
schema knowledge in combination with episodic knowledge               lyzed eye movements of subjects that viewed images of real-
enhances performance on a straightforward categorization              world scenes during an active search task. They found that
task.                                                                 a visual saliency model did not predict fixation patterns any
                                                                      better than a random model did. They concluded that visual
                          Discussion                                  saliency does not account for eye movements during active
                                                                      search and that top-down (i.e., knowledge-driven) processes
The results from our first experiment show that the spatial           play the dominant role.
structure of face images (i.e., scene-schema knowledge) can
be inferred from the N IM model’s similarity-space represen-          Knowledge-driven gaze control
tations. The results of our second experiment illustrate how
the gaze control mechanism can enhance performance on a               Knowledge-driven gaze-control models rely on stored knowl-
categorization task. As mentioned previously, two types of            edge to select the most informative location to direct gaze
gaze control models can be distinguished: (1) image-driven            (see, e.g., Henderson, 2003). The extended N IM model uses
models, and (2) knowledge-driven models. Below, we first             a knowledge-driven gaze-control mechanism that combines
discuss image-driven gaze control models. Then, we compare           acquired scene-schema knowledge with episodic knowledge.
knowledge-driven gaze control in the N IM model with other           A few other models have used a knowledge-driven ap-
gaze control models that use a knowledge-driven approach.            proach (e.g., Rybak, Gusakova, Golovan, Podladchikova, &
                                                                     Shevtsova, 1998). The main difference between knowledge-
Image-driven gaze control                                            driven gaze control in the extended N IM model and exist-
Until now, the image-driven approach has been the dominant           ing knowledge-driven gaze control models concerns the rep-
approach to model gaze control. Image-driven gaze-control            resentation of spatial knowledge. Whereas existing mod-
models generally assume that fixation locations are selected         els of knowledge-driven gaze control include separate ‘what’
in a bottom-up manner based on the image properties (e.g.,           (episodic memory) and ‘where’ (spatial and motor memory)
Itti & Koch, 2000; Rao et al., 2002). These models create            representation spaces, the N IM model relies solely on the
a saliency map that marks the saliency of each image loca-           structure of the similarity space representations. Since the
tion. Saliency is defined by the distinctiveness of a region         faces contain spatial regularities, the similarity-space repre-
from its surround on certain visual dimensions. Since loca-          sentations contain information about both the face identity
tions with a high visual saliency are assumed to be informa-         and the spatial origin. While there might be good neurobio-
tive, gaze is directed toward highly salient locations. Often,       logical evidence to support separate what and where systems,
the visual dimensions that are used to generate a saliency map       the results presented in this chapter show that the N IM model
are similar to the visual dimensions that are known to be pro-       can acquire knowledge about the spatial arrangement of ob-
                                                                 1661

jects within a particular category based on the structure of the         R. van Gompel, M. Fischer, W. Murray, & R. Hill (Eds.),
similarity-space representations.                                        Eye movements: A window on mind and brain. Elsevier.
                                                                      Itti, L., & Koch, C. (2000). A saliency-based search mecha-
                         Conclusion                                      nism for overt and covert shifts of visual attention. Vision
This paper investigated whether the N IM model could be ex-              Research, 40, 1489-1506.
tended with a knowledge-driven gaze control mechanism that           Karn, K. S., & HayHoe, M. M. (2000). Memory representa-
selects relevant eye-fixation locations based on scene-schema            tions guide targeting eye movements in a natural task. Vi-
knowledge and episodic knowledge. From our results we                    sual Cognition, 7, 673–703.
conclude that the spatial structure of natural images can be         Koch, C., & Ullman, S. (1985). Shifts in selective visual
inferred from the N IM model’s similarity-space representa-              attention: Towards the underlying neural circuitry. Human
tions, i.e., scene-schema knowledge can be acquired on the               Neurobiology, 4, 219–227.
basis of the similarity-space representations. Moreover, we          Lacroix, J. P. W., Murre, J. M. J., Postma, E. O., & Van den
showed that extending the N IM model with a gaze control                 Herik, H. J. (2004). The natural input memory model.
mechanism that combines the acquired scene-schema knowl-                 In K. Forbus, D. Gentner, & T. Regier (Eds.), Proceedings
edge with stored episodic knowledge, enhances performance                of the 26th annual meeting of the cognitive science soci-
on a categorization task. In our future work, we will use the            ety (CogSci 2004) (pp. 773–778). Mahwah, NJ: Lawrence
extended N IM model for modeling human scan paths in a va-               Erlbaum Associates.
riety of visual tasks.                                               Lacroix, J. P. W., Murre, J. M. J., Postma, E. O., & Van den
                                                                         Herik, H. J. (2006). Modeling recognition memory using
                    Acknowledgments                                      the similarity structure of natural input. Cognitive Science,
The research project is supported in the framework of the                30, 121-145.
NWO Cognition Program with financial aid from the Nether-            Mannan, S. K., Ruddock, K. H., & Wooding, D. S. (1996).
lands Organization for Scientific Research (NWO). It is part             The relationship between the locations of spatial features
of the larger project: ’Events in memory and environment:                and those of fixations made during visual examination of
modeling and experimentation in humans and robots’ (project              briefly presented images. Spatial Vision, 10, 165–188.
number: 051.02.2002).                                                Norman, J. F., Phillips, F., & Ross, H. E. (2001). Informa-
                                                                         tion concentration along the boundary contours of naturally
                          References                                     shaped solid objects. Perception, 30, 1285–1294.
Braun, J., Koch, C., Lee, D. K., & Itti, L. (2001). Perceptual       Oliva, A., Torralba, A., Castelhano, M. S., & Henderson,
   consequences of multilevel selection. In J. Braun, C. Koch,           J. M. (2003). Top-down control of visual attention in object
   & J. L. Davis (Eds.), Visual attention and cortical circuits          detection. In I EEE proceedings of the international confer-
   (pp. 215-241). Cambridge, MA: MIT Press.                              ence on image processing (Vol. 1, pp. 253–256).
Busey, T. A., & Tunnicliff, J. (1999). Accounts of blending,         Palmeri, T. J., & Gauthier, I. (2004). Visual object under-
   typicality and distinctiveness in face recognition. Journal           standing. Nature Reviews Neuroscience, 5, 291–303.
   of Experimental Psychology: Learning Memory and Cog-              Parkhurst, D. J., Law, K., & Niebur, E. (2002). Modeling the
   nition, 25, 1210–1235.                                                role of salience in the allocation of overt visual attention.
Dailey, M. N., Cottrell, G. W., Padgett, C., & Adolphs, R.               Vision Research, 42, 107–123.
   (2002). A neural network that categorizes facial expres-          Parkhurst, D. J., & Niebur, E. (2003). Scene content selected
   sions. Journal of Cognitive Neuroscience, 14, 1158–1173.              by active vision. Spatial Vision, 16, 125–154.
Edelman, S. (1995). Representation, similarity, and the cho-         Rajashekar, U., Cormack, L. K., & Bovik, A. C. (2002). Vi-
   rus of prototypes. Minds and Machines, 5, 45–68.                      sual search: Structure from noise. In Proceedings of the
Edelman, S., & Duvdevani-Bar, S. (1997). Similarity, con-                eye tracking research & applications symposium 2002 (pp.
   nectionism, and the problem of representation in vision.              119–123). New Orleans, LA.
   Neural computation, 9, 701–720.                                   Rao, R. P., Zelinsky, G. J., Hayhoe, M. M., & Ballard, D. H.
Freeman, W. T., & Adelson, E. H. (1991). The design and                  (2002). Eye movements in iconic visual search. Vision
   use of steerable filters. I EEE Trans. Pattern Analysis and           Research, 42, 1447-1463.
   Machine Intelligence, 13, 891–906.                                Rao, R. P. N., & Ballard, D. H. (1995). An active vision
Goldstone, R. L., & Son, J. Y. (2005). Similarity. In K. J.              architecture based on iconic representations. Artificial In-
   Holyoak & R. G. Morrison (Eds.), Cambridge handbook                   telligence, 78, 461–505.
   of thinking and reasoning (pp. 1–29). Cambridge, MA:              Rybak, I. A., Gusakova, V. I., Golovan, A. V., Podladchikova,
   Cambridge University Press.                                           L. N., & Shevtsova, N. A. (1998). A model of attention-
Hancock, P. J. B., Burton, A. M., & Bruce, V. (1996). Face               guided visual perception and recognition. Vision Research,
   processing: human perception and principal components                 38, 2387–2400.
   analysis. Memory and Cognition, 24, 26–40.                        Shepard, R. N. (1957). Stimulus and response generalization:
Henderson, J. M. (2003). Human gaze control during real-                 A stochastic model relating generalization to distance in
   world scene perception. Trends in Cognitive Science, 7,               psychological space. Psychometrika, 22, 325–345.
   498–504.                                                          Turano, K. A., Geruschat, D. R., & Baker, F. H. (2003). Ocu-
Henderson, J. M., Brockmole, J. R., Castelhano, M. S., &                 lomotor strategies for the direction of gaze tested with a
   Mack, M. (to appear). Visual saliency does not account                real-world activity. Vision Research, 43, 333–346.
   for eye movements during search in real-world scenes. In
                                                                 1662

