UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Semantic Packing As a Core Mechanism of Category Coherence, Fast Mapping and Basic
Level Categories

Permalink
https://escholarship.org/uc/item/2b86k80w

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)

Authors
Hidaka, Shohei
Saiki, Jun
Smith, Linda B.

Publication Date
2006-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Semantic Packing As a Core Mechanism of Category Coherence, Fast
Mapping and Basic Level Categories
Shohei Hidaka (hidaka@cog.ist.i.kyoto-u.ac.jp)
Graduate School of Informatics, Kyoto University and JSPS Research Fellow

Jun Saiki (saiki@cv.jinkan.kyoto-u.ac.jp)
Graduate School of Human and Environmental Studies, Kyoto University

Linda B. Smith (smith4@indiana.edu)
Department of Psychological and Brain Sciences, Indiana University
to solve the coherence problem. Insead, they suggested
that categorization is based on folk theories and causal
induction, or what some call “theory theory”. However,
their major criticism of similarity-based accounts, circularity, is also a problem for theory theories. To what
theory to apply to any thing – a theory about food and
the relevance of color or a theory about artifacts and the
nonerelevance of color – one has to already know what
kind of thing is at hand.

Abstract
In the present study, category coherence, a question why
intuitive groupings for natural categories exist, is considered by applying a computational theory that models
discrimination and generalization. The computational
process is what we called “semantic packing”. In the
model, category learners’ two conﬂicting constraints on
discrimination and generalization are optimized simultaneously as if it packed knowledge into memory. The
model also revealed a computational structure of eﬃcient categories, called basic categories, by mathematical proof and exempliﬁcation of the relationship between
the past proposed theory and our theory. Furthermore,
the empirical evidence of the theory from human semantic rating was shown and used for testing predictability
of novel natural categories. The results suggest that
semantic packing could reproduce the conﬁguration of
natural categories from only their generalization without any knowledge as fast mapping in which children can
generalize a novel instance without trial and error. In
summary, the semantic packing could be a core mechanism of the three essential categorization processes, category coherence, fast mapping, and basic level category.

The Outline

Introduction
Why Does “Category Coherence” Emerge?
Murphy and Medin (1985) deﬁne “category coherence”
as the intuitive and useful groupings that characterize
natural categories, and claim that this coherence is one
of most important aspects of semantic cognition. For example, color is more important for discrimination when
the item is a pea rather than when it is a ball. How do
we learn this? And how do we use the knowledge that we
have learned? How, when we see an object-a potential
pea or ball-do we know to attend to color or not?
This problem of feature selection has played a key role
in theoretical discussions of the mechanisms that underlie category learning. In past studies, some measures of
a learner’s category representation were proposed to explain the basic level category advantage. The basic category is known as the most eﬃcient level for various cognitive processes, such as picture naming, category or feature listing, and speech frequency (Rosch, Mervis, Gray,
Johnson & Boyes-Braem, 1976). This concerns the relative frequency of features or categories, thus, these measures depend on what features or categories are selected
(Murphy and Lassaline, 1997). For these and other reasons, Murphy and Medin (1985) claimed that categorization based only similarity and correlations is not enough
1500

The main goal of this study is to propose a computational theory that connects the emergence of category
coherence, basic level categories, and fast mapping (see
also the next paragraph). To this end, we propose two
metrics. The ﬁrst is what we call smoothness. This concerns the relationship between features and generalization of categories found in developmental studies and is
a measure how cohesive categories are. Categorization
with the smooth feature space makes a novel instance
generalized precisely, which is considered as fast mapping. The second metric concerns discrimination, which
yields category coherence as the result. There is an essential trade-oﬀ between generalization and discrimination. Both cannot be maximized simultaneously. The
joint optimization is obtained by what we call “semantic
packing”. This process is analogous to a task in which
various sized and shaped containers (categories) are eﬃciently packed into a larger but ﬁnite container (memory
and the attentional and retrieval processes that apply to
memory). The packing process is related to processes
such as cue validity (Rosch et al. 1976) and category utility (Corter & Gluck, 1992), which are measures for basic
level categories. We ﬁrst present the theory metaphorically and then the formal mathematical speciﬁcation
later.

Working Deﬁnition: “Semantic
Smoothness” in Natural Categories
Many developmental studies using novel word generalization tasks have shown that children systematically
attend to diﬀerent properties when generalizing diﬀerent types of entities, a process known as “fast mapping”. For example, children generalized solid artifacts
and non-solid substances based on the similarity of shape
and material, respectively (Soja, Carey & Spelke, 1991).
Therefore, children seem to solve the circularity problem,

Eﬃciency in Semantic Cognition
Past proposed measures of the basic category advantage
concentrate on the discriminability of categories. More
speciﬁcally, cue and category validity are respectively
maximized in subordinate and superordinate categories
(Murphy & Lassaline, 1997). Furthermore one variety
of category utility is equivalent to mutual information
(i.e. degree of probabilistic independence) between categories and features (Gluck & Corter, 1985). This is
maximized with dependent categories and features (e.g.,
one-to-one mapping between features and categories: an
extreme case of subordinate categories). These measures
are (positively or negatively) discriminabilities of cate1
Consistent with the mathematical term “smooth”,
“smoothness” here refers to the probabilistic degree of local
linearity of manifold (category-feature space) where generalization σi is curvature around μi .

1501

(a) Exceptional feature

(b) Efficient representation

(c) Overlap feature
prototype

property 2

knowing to attend to the right properties even though
they do not yet know the category. Importantly, neither
younger children nor late talkers show the same generalization pattern (Jones, 2003). This ﬁnding suggests
that these diﬀerential weighting patterns are learned.
An adult rating study of the similarities that characterize the ﬁrst 300 nouns learned by children showed that
their attentional biases in noun extension tasks reﬂected
the regularities in the corpus of early noun categories.
Speciﬁcally, there is a high correlation between category
generalization (i.e., shape- or material-based category organization) and property (i.e., solidity) (Samuelson &
Smith, 1999). In other words, these rating data indicated that “property” (i.e. solid or non-solid) of natural categories predicted ”generalization” (i.e. property
weighting: shape- or material- based generalization),
and vice versa. The semantic space would be in our
terms “smooth” if the correlation between property and
generalization was universal in any semantic domain, as
robust as the correlation between solidity and shapebased generalization in early acquired noun categories,
that is, the property diﬀerence between any two categories would be correlated to a diﬀerence in how those
categories are generalized (see also Equation 12 for the
mathematical deﬁnition). Furthermore, the smooth semantic space would form clusters that have a correlated
property-generalization relationship. In other words, because of the property-generalization correlation, similarly distributed categories would be grouped near each
other (i.e., domain speciﬁc property weighting: Figure
1 (b)) 1 . Thus “smoothness” of the semantic space
may be considered as a quantitative measure of category coherence. Here, in an empirical study, we investigate the smoothness of the semantic space of earlyacquired nouns. The results indicate that natural categories have “smoothness”, that some categories (e.g.,
“cat” and “tiger”) share similar properties and generalization patterns, and that other categories (e.g., “cat”
and “chair”) share dissimilar ones. Before a more detailed presentation of the theory, we consider how one
recent theory of category development has dealt with
this issue.

instance
generalization
parsimony
memory space
property 1

discriminability

(d) Category packing under two constraints

Figure 1: 20 categories (ellipses represent contour of
probabilistic distributions) in property space. (a) Category coding with exceptional feature: More property
space (dashed line) is needed for categorization. (b) Efﬁcient category coding: Less property space and more
discriminability. Categories with similar generalization
are localized. (c) Category coding with overlapped feature: Feature is useless for categorization. (d) Category
packing is considered the balanced optimal state under
two constraints: memory parsimony and discriminability
of category.

gory. Importantly, the surface diﬀerences in these measures (Corter & Gluck, 1992) could just reﬂect the differences in the selected set of categories and features
(Murphy & Lassaline, 1997).
We argue that learning eﬃciency under the trade oﬀ
between discrimination and generalization is the core
mechanism underlying category coherence. We deﬁne
“semantic memory” for the purposes of this paper as a
set of categories, which is formulated as a probabilistic
distribution in psychological feature space (Figure 1).
A reasonable question to ask is what features should
be represented for any category (e.g. A feature “with
four limbs” for both “dog” and “cat”). Categories in an
overlapping feature space need less memory. But overlapping features also mean less discriminability of the
categories (Figure 1 (c): E1 gets smaller, but G gets
bigger in Equation 7). One solution to this is to store
only exceptional features or conjunctive features such
that the conjunctions are unique for each category(e.g.,
“four-limbed, eyed, a pet, meat-eating, gather in crowds,
...” as single feature only for “dog”). The categories in
this exceptional feature space must be sparse and will
therefore need more memory (i.e., larger dashed-line enclosure). This is, by deﬁnition, all categories must be
deﬁned by unique conjunctive feature sets. (Figure 1
(a): G gets smaller, but E1 gets bigger in Equation 7).
However, this means that what one knows about one
category can not help in learning or making decisions

about another. This is precisely where the developmental evidence is most compelling: The categories children
already know help them learn new similarly structured
categories. This insight indicates that discriminability
and generalization from knowledge about one category
to another trade oﬀ. An eﬃcient semantic memory may
try to optimize both, and that may work at some midlevel between these two extremes (Figure 1 (b)).
What then would emerge in such a case? We describe
the optimal state as “category packing”, where the system packs categories of a particular shape in feature
space close together, thus taking up less feature space
overall (Figure 1 (d): both G, E1 and L get smaller
in Equation 7). Assume that one creates optimally organized categories by moving the prototypes or, alter∂L
∂L
= 0 or ∂μ
= 0 :
natively, the distributions (i.e. ∂μ
i
i
note that this is not “category learning”). This process is analogous as packing things into smaller space
(categories or things avoid probabilistic or solid “collision”, respectively). The most eﬃcient packing of diﬀerent sizes and shapes of things (or categories, we propose)
consists of packing similarly shaped things together (i.e.
emergence of semantic smoothness: Figure 1 (b) and
Equation 12). Next we brieﬂy introduce the detailed
formulation of our theory in a simple case in which categories are deﬁned by prototypical representations.

Theoretical Formulation of Packing
We prove the equivalence between semantic packing and
smoothness, under the simpliﬁcation that each category
is represented by its prototype and generalization pattern. Note that this simpliﬁcation does not assume any
predeﬁned speciﬁc “feature” or “category” in the packing process and also that the prototypical representation is not a necessary assumption but an application.
Instead of specifying categories and features, we investigated what category organization emerges as the result
of the eﬃcient categorization. Before this proof, we also
prove the approximate equivalence among our discriminability measure, cue validity, and category utility.
Assume that there are n categories c1 , c2 , ..., ci , ..., cn
in feature space θ ⊂ Ω. Assume the conditional probability of feature θ given category ci as P (θ|ci ). F deﬁned
as the equation below indicates measure of discriminability among categories, which is upper bound of minimum
error ratio under optimal decision making.
 
1
P (θ|ci ) n dθ
(1)
Fn =
Ω

i

Let the joint probability of category ci in feature θ be
category
ci when
P (ci , θ). If one evaluated that it is 

θ ⊂ Ωi , then the correct ratio is
i Ωi P (ci , θ)dθ.
In particular n = 2, the minimum error ratio of
the
optimal decision (i.e. Bayes decision) is ˆ =

min(P
(c1 , θ), P (c2 , θ))dθ, because, to maximize the
Ω
correct ratio, one must choose the category with largest
probability given θ. To estimate analytically the exact
minimum or maximum distribution is diﬃcult. Accordingly we used instead the upper bound of the minimum,
1502

called the “Bhattacheryya bound” (Duda, Hart & Stork,
2000). The inequality min(a, b) ≤ aβ b1−β is true when
a, b ≥ 0 and 0 ≤ β ≤ 1. Therefore, Using Bayse’ theorem P (ci , θ) = P (θ|ci )P (ci ), where P (θ|ci ) is conditional
probability θ given ci .
 

ˆ ≤ P (c1 )β P (c2 )1−β
P (θ|c1 )β P (θ|c2 )1−β dθ (2)
The right side in this inequality is called the “Charnoﬀ
bound”, the upper bound of the error. β giving the
minimum a Charnoﬀ bound is around β = 12 , therefore, Chernoﬀ bound with β = 12 , called Bhattacheryya bound ψb can
 be used as the second best bound.
Obviously, ψb =
P (c1 )P (c2 )F2 . When n > 2, for
short tailed probabilistic distribution such as normal
distribution, F ∼
= kF F2 = B can be approximated
with constant kF in the local of the particular nearest pair
Therefore, suppose
 of normal distributions.
1
1
ψij = P (ci )P (cj ) P (θ|ci ) 2 P (θ|cj ) 2 dθ.


1
P (ci ) n Fn ∼
Bij
(3)
= kF
i

i

j=i

The maximum correct ratio of cue validity (P (ci |θ):
Rosch et al., 1976) model considering frequency
of feature P (θ) (Reed, 1972) is deﬁned as Q̂ =
maxi P (ci |θ)P (θ)dθ = maxi P (θ, ci )dθ. Therefore,

1
P (ci )− n (1 − Q)
(4)
Fn ∼
=
i

In other words, minimum Fn indicates the maximum
cue validity Q. The category
utility of category i is de
ﬁned as U (ci ) = P (ci ) (P (θ|ci )2 − P (θ)2 )dθ(Corter &
Gluck, 1992). Total category utility U =
i U (ci ) is
a similar measure with F as follows (i.e.
the
order is

O(U ) = O(−F 2 )). Applying P (θ) =
i P (θ, ci ) and
ku = P (ci )−1 − 1.
 
 
dBij 2
ku P (θ, ci )2 dθ − 2
(
) dθ (5)
U=
dθ
i
i
j=i

Next, we prove the equivalence between semantic
packing and smoothness when the probability of feature
θ given ith category P (θ|ci ) is deﬁned as, a prototypical representation, a d-dimensional normal distribution.
1
Then P (θ|ci ) = ((2π)d |σi |)− 2 exp(− 21 (θ − μi )t σi−1 (θ −
μi )), where a mean vector (i.e., prototype) and covariance matrix (i.e., generalization or feature weighting) are
μi and σi . The superscript t refers to transposition. The
discriminability measure (Equation
n 1) can berewritten
n
as follows. Assume that A = i σi−1 , B = i σi−1 μi ，
n t −1
C = i μi σi μi , and G = log(F )

1
log |σi |)
(B t A−1 B − C − n log |A| −
2n
i
n

G=

(6)

∂G
∂G
or ∂σ
(i.e., disOptimization for only the constraint ∂μ
i
i
t
criminability in Figure 1) gives (μi −μj ) (μi −μj ) → ∞ or

|σi | → 0 , indicating an immense amount of feature space
or an instance as a category (i.e., no generalization),
respectively.Therefore, 
constraints to normal distribun
n
tions E1 = i ||μi ||2 = i μti μi and E2 = log |A−1 | are
necessary. For the cognitive process, the constraints E1
and E2 refer to maintenance of constant memory space
(i.e. parsimony in Figure 1) and generalization ranges,
respectively. The Lagrange multiplier method is used
for optimization of the constraints. The Lagrange equation with multiplier λ is L = G + λ1 E1 + λ2 E2 , which
indicates semantic packing (L) optimizes both discriminability (G) and generalization (E1 and E2 ).

0. Let ν = (ν1 , ν2 , ..., νn )t be the d-by-n-dimensional
vector having νi as its ith elements. In addition, let Σ be
the super matrix having σi as its ith diagonal elements,
and A−1 be a super matrix having n2 A−1 as its all
elements. Then,

∂L
∂
=
(G + λ1 E1 ) = −σi−1 (μi − μ̄) + λ1 μi = 0 (7)
∂μi
∂μi
n
n
where μ̄ = A−1 B = ( i σi−1 )−1 i σi−1 μi .

Survey Procedure

μi = −(λ1 σi − I)−1 μ̄

(8)

where I is the identity matrix. Therefore the relationship
between a pair of categories when L is optimized as a
function of μ is
Δμij = λ1 (λ1 σi − I)−1 Δσij (λ1 σj − I)−1 μ̄

(9)

where Δμij = μi − μj and Δσij = σi − σj . Next, in
addition to μi , L is optimized as a function of σi . As
−1
∂L
∂
∂2nG
∂σi = ∂σi (2nG + λ2 E2 ) , thus applying ∂σi = σi (μ̄ −
−1 −1 −1
−1
t −1
μi )(μ̄ − μi ) σi + nσi A σi − σi
σi

∂L
σi = (μ̄ − μi )(μ̄ − μi )t + (n + λ2 )A−1 − σi (10)
∂σi

∂L
∂L
As σi ∂σ
σi − σj ∂σ
σj = 0
i
j

Δσij =



(−1)δki (μ̂ − μk )(μ̂ − μk )t

(11)

k=i,j

(13)

Thus, Equation (7) (i = 1, ..., n) can be solved by ν as
an eigenvector of (Σ − A−1 ) in Equation (13).

Method
The ﬁrst step in the simulation study was to collect data
on the similarities of 48 nouns that are among the earliest learned by children (Fenson et al, 1993). To determine the relevant similarities across a broad range
of properties, 104 Japanese undergraduates rated each
noun category using 16 pairs of adjectives (Hidaka &
Saiki, 2004). The goal here is to place the categories in
a relatively (16 dimensions) large feature space. These
adjective pairs are the potential features. Subjects used
a 5-point scale to indicate how well the pair of adjectives
described the items in the category (e.g., large = 5, small
= 1). The 16 pairs of adjectives were selected by a pilot
survey using 41 pairs collected from prior studies. We
created questionnaires of 5 diﬀerent orderings to cancel
out the order eﬀect. Participants completed the survey
in about an hour.

Stimuli
• Adjective pairs (linguistic scales)
dynamic-static, wet-dry, light-heavy, large-small, complexsimple, slow-quick, quiet-noisy, stable-unstable, cool-warm,
natural-artiﬁcial, round-square, weak-strong, rough hewnﬁnely crafted, straight-curved, smooth-bumpy, hard-soft.

• Noun categories

where δii = 1 when i = j, otherwise δij = 0. Notice
that σi is constant in Equation 9, and Equation 11 is
Δσij ∼
= O(Δμij ). Consequently, the approximate monotonic relationship between Δμij and Δσij with a given
∂L
=0
constant α (i.e. “smoothness”) emerges, when ∂μ
i
∂L
or ∂σi = 0 (i.e. “packing”).
||μi − μj || ≈ α||σi − σj ||

ν − λ(Σ − A−1 )ν = 0

(12)

In other words, semantic smoothness, which is the correlation between feature and generalization (Equation
12), is approximately equivalent to semantic packing. A
learning system with smooth categories that optimize
the packing principle, and vice versa.
∂L
An analytic solution to ∂μ
= 0 is demonstrated as
i

n t

follows. Assume that E1 =
i νi νi where νi = μi −
−1
A B to be the constraint instead of E1 , and note that
the replacement does not lose generality. Solving the
−1
λ 
∂L
Lagrange equation L = G
2 + 2 E1 , we get ∂μi = −σi νi +
n
λ j (δij −σi−1 A−1 )νj where δii = 1 when i = j, or δij =
1503

butterﬂy, cat, ﬁsh, frog, horse, monkey, tiger, arm, eye,
hand, knee, tongue, boots, gloves, jeans, shirt, banana, egg,
ice cream, milk, pizza, salt, toast, bed, chair, door, refrigerator, table, rain, snow, stone, tree, water, camera, cup,
key, money, paper, scissors, plant, balloon, book, doll, glue,
airplane, train, car, bicycle

Analysis and Simulation
Correction of survey data The rating value was
corrected by a logistic function to make the correlation
between mean and variance zero. The original rating
showed a small positive correlation between the deviation from the median and the variance, because an extreme rating (i.e., a rating near one or ﬁve) has a smaller
variance than a rating near the median. More specifically, the parameters of the logistic function f (x) =
(1 + exp((x − b)c−1 ))−1 are estimated to have zero correlation between |x − b| and a standard deviation of rating
x, and estimated parameters are b = 3 and c = 1.2. The
corrected mean and variance is used for analysis and
simulation.

Simulation of packing category Three simulations
were run. The ﬁrst simulation involved the semantic
packing of randomly generated categories with the goal
of visualizing coherent categories. The second simulation
attempted to reproduce category organizations based on
the adjective ratings for the corpus of early learned nouns
and in doing so demonstrates how packing might explain
fast mapping. The third simulation involved a Monte
Carlo simulation investigating the relationship among
measures of discrimination.
In the semantic packing simulation, we optimized the
mean and covariance of several categories with randomly
generated initial means and covariances (i.e. the gradient method: updating the parameters based on Equation
13 and 10) . The smoothness index was measured after
updating was performed 100 times. The updated ﬁnal
state refers to optimization in terms of balanced constraints.In the simulation of the adjective rating data
for early-learned nouns, the means of categories were
reproduced by a solution of Equation 13 for a given covariance matrix of survey data. This simulation investigates the predictability of a prototype conﬁguration in
real data based on the generalization pattern. The results were evaluated based on the correlation between
the distances between all pairs of categories in the reproduced and original prototype conﬁguration. The degrees of freedom of the conﬁguration to be estimated is
752 (the number of categories without pivot of rotation
by property dimension (48 − 1) × 16). In the Monte
Carlo simulation, 20 one-dimensional normal distributions with uniform-random means (-3 to 3) and variances
(0.2 to 2) were generated
500 times. The maximum (Q),

paired minimum ( i,j=i min(P (θ|ci ), P (θ|cj ))), overlap
(F
), category utility (U ), paired Bhattacheryya bound
( i,j=i Bij ) of the generated distributions were calculated theoretically (F , U , and B) or numerically (max
and min: integral range from -10 to 10 and sample resolution 0.01), and the correlation was analyzed.

Results
Figure 2 shows the relationship between the mean norms
and the correlation norm for the adjective-rating The
correlation and contribution are .466 and .733, respectively. The correlation and contribution of the smoothness index using covariance, rather than correlation, are
1504

FKUVCPEGQHRCKTGFEQTTGNCVKQPOCVTKEGU



































FKUVCPEGQHRCKTGFOGCPXGEVQTU

Figure 2: Scatter plot of mean vector norm (x axis:
prototype dissimilarity) and correlation matrix norm (y
axis: generalization dissimilarity) in survey data



FKUVCPEGQHRCKTGFEQXCTKCPEGOCVTKEGU

Index of semantic smoothness Semantic smoothness, as predicted by Equation 12, was speciﬁcally calculated by norms of the mean vector and covariance matrix
in the model. The mean vector and covariance (or correlation) matrix represent the mean and covariance across
the 16-adjective ratings for all subjects. The correlation
and contribution of the norms of the mean vector and
the covariance were used as an index of smoothness. The
contribution of the major axis is calculated by the principal component analysis, because the norms of both the
mean and the covariance have variances. In other words,
the coeﬃcient of determination in the regression analysis underestimates this contribution because it supposes
that only the dependent variable has error.































FKUVCPEGQHRCKTGFOGCPXGEVQTU

Figure 3: Scatter plot of mean vector norm and correlation matrix norm in simulation

.357 and .688, respectively. These results suggest that
the investigated category set has smoothness. The mean
norm and covariance norm of paired categories (each
category paired with every other category) are shown
(Figure 3). The average and standard deviation of the
smoothness index for 100 simulations were .490 and .181,
respectively. The correlation and contribution between
the reproduced mean matrix in the simulation and the
mean of survey data were .430 and .715, respectively.
These results suggest that semantic packing could reproduce half of the categories from only their generalization without any knowledge of the category conﬁguration. This is the kind of result needed to explain feature
selection and fast-mapping in children.
The results of Monte Carlo simulation are shown in
Table 1. Most of the absolute correlations |R| were
greater than .8, which indicated that the ﬁve measures
were approximately equivalent as proven.

Discussion
The results of Monte Carlo simulation exploring some
discriminability measures empirically support the the-

τ \R
max
F
U
B
min

max
1
-.681
.659
-.686
-.658

F
-.867
1
-.601
.785
.620

U
.855
-.791
1
-.740
-.814

B
-.877
.938
-.903
1
.796

min
-.863
.806
-.949
.938
1

Acknowledgments
This work was supported by grants from Grants-in-Aid
for Scientiﬁc Research from JMEXT (No. 15650046),
JSPS Research Fellowships for Young Scientists, Kyoto University Foundation and the National Institutes
of Mental Health, R01 MH60200-06.

Table 1: Peason correlation coeﬃcient R (upper triangle)
and Kendall rank correlation τ (lower triangle) among
measures
oretical relationship, which is the approximate equivalence among F , cue validity and category. In the packing theory, generalization is not just negative discriminability but a limitation on category representation resulting from the whole memory capacity and a lower
bound on generalization. With the two conﬂicting constraints, the computational model predicts that smooth
category organization emerges. Consistent with this prediction, the smoothness index of the adjective-rating
data for early learned nouns suggests that propertygeneralization clusters were formed in not only speciﬁc
domains (e.g. solidity-shape in the survey of Samuelson
& Smith, 1999) but also more generally. Indeed from
these data, one can predict regions of “fast mapping” involving property-category organization correlations that
are unknown to and unexplored by researchers in early
category development. The success in a quantiﬁcation
of category coherence using smoothness index provides
empirical evidence of the role of semantic packing in human natural categories. Granted the results could be due
to the speciﬁc properties and noun categories selected.
However, the adjectives were selected by their discriminability (i.e. variance of the adjectives) to the category
set (Hidaka & Saiki, 2004), and smoothness can be observed in feature space with discriminability. Therefore,
the adjective-rating results may be taken as making predictions about the feature space as it relates to early
acquired noun categories (i.e. basic categories).
The success in reproducing the organization of early
learned nouns suggests that a category system constrained by semantic packing principle could generalize a
category to new instances without trial and error. This
implies that the system would “know” the generalization
pattern of a novel thing in a certain region of feature
space. Young children show precisely this kind of knowledge in generalizing names for novel categories. This is
typically referred to in the developmental literature as
“fast mapping.” Notice that the system has no metaknowledge , as theory-theory claims, about speciﬁc domains, but smoothness, a property of the whole system
in which categories are learned and represented, does the
work of such meta-knowledge.
In summary, the proposed theory suggests that category packing process, balanced discriminability and generalization (i.e. basic categories), leads smooth categoryfeature organization (i.e. category coherence) consistent
to human natural categories, and the smoothness help
learners’ generalization to novel categories (i.e. fast mapping).
1505

References
Corter J. E., & Gluck M. A. (1992). Explaining Basic
Categories: Feature Predictability and Information.
Psychological Bulletin, 111, 291–303.
Duda, R. O., Hart, P. E., Stork, D. G. (2000) Pattern
Classiﬁcation (2nd ed) , New York: John Wiley &
Sons.
Fenson, L., Dale, P. S., Reznick, J. S., Bates, E., Thal,
D. J., & Pethick, S. J. (1994). Variability in early
communicative development. Monographs of the Society for Research in Child Development, 59 (5, Serial
No. 242) Chicago: University of Chicago Press.
Gelman, R., & Williams, E. (1997). Enabling constraints
on cognitive development. In D. Kuhn & R. S. Siegler
(Ed&.). Cognition, perception and language. Vol. 2.
Handbook of child development. (5th ed.) (pp. 575630). (W. Damon, Ed.) New York: Wiky.
Gluck, M. A. & Corter, J. E. (1985). Information, Uncertainty, and the Utility of Categories, , Proceedings of
the Seventh Annual Conference of the Cognitive Science Society (pp. 283–287). CA: Lawrence Erlbaum
Associates.
Hidaka, S. & Saiki, J. (2004). A mechanism of ontological boundary shifting , Proceedings of the Twenty
Sixth Annual Conference of the Cognitive Science Society (pp. 565–570). Chicago, IL.
Jones, S.S. (2003). Late talkers show no shape bias in
object naming. Developmental Science, 6(5), 477–483.
Murphy, G. L., & Lassaline, M. E. (1997). Hierarchical Structure in Concepts and the Basic Level of Categorization. in Lamberts, K. & Shancks, D. (Eds),
Knowledge concepts and categories, UK: Psychology
Press.
Murphy, G.L. & Medin, D.L. (1985). The role of theories in conceptual coherence. Psychological Review,
92, 289–316.
Reed, S. K. (1972). Pattern recognition and categorization. Cognitive Psychology, 3, 382–407.
Rosch, E., Mervis, C. B., Gray,W., Johnson, D., &
Boyes-Braem, P. (1976). Basic objects in natural categories. Cognitive Psychology, 8, 382–439.
Samuelson, L. & Smith, L. (1999). Early noun vocabularies: do ontology, category structure and syntax
correspond?, Cognition, 73, 1–33.
Soja, N. N., Carey, S. & Spelke, E. S. (1991). Ontological categories guide young children’s inductions of
word meanings: object terms and substance terms.,
Cognition, 38, 179–11.

