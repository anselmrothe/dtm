UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Semantic Packing As a Core Mechanism of Category Coherence, Fast Mapping and Basic
Level Categories
Permalink
https://escholarship.org/uc/item/2b86k80w
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Hidaka, Shohei
Saiki, Jun
Smith, Linda B.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

  Semantic Packing As a Core Mechanism of Category Coherence, Fast
                                 Mapping and Basic Level Categories
                                 Shohei Hidaka (hidaka@cog.ist.i.kyoto-u.ac.jp)
                      Graduate School of Informatics, Kyoto University and JSPS Research Fellow
                                     Jun Saiki (saiki@cv.jinkan.kyoto-u.ac.jp)
                         Graduate School of Human and Environmental Studies, Kyoto University
                                       Linda B. Smith (smith4@indiana.edu)
                            Department of Psychological and Brain Sciences, Indiana University
                           Abstract                                to solve the coherence problem. Insead, they suggested
                                                                   that categorization is based on folk theories and causal
   In the present study, category coherence, a question why        induction, or what some call “theory theory”. However,
   intuitive groupings for natural categories exist, is con-       their major criticism of similarity-based accounts, circu-
   sidered by applying a computational theory that models
   discrimination and generalization. The computational            larity, is also a problem for theory theories. To what
   process is what we called “semantic packing”. In the            theory to apply to any thing – a theory about food and
   model, category learners’ two conﬂicting constraints on         the relevance of color or a theory about artifacts and the
   discrimination and generalization are optimized simul-          nonerelevance of color – one has to already know what
   taneously as if it packed knowledge into memory. The            kind of thing is at hand.
   model also revealed a computational structure of eﬃ-
   cient categories, called basic categories, by mathemati-
   cal proof and exempliﬁcation of the relationship between        The Outline
   the past proposed theory and our theory. Furthermore,           The main goal of this study is to propose a computa-
   the empirical evidence of the theory from human seman-
   tic rating was shown and used for testing predictability        tional theory that connects the emergence of category
   of novel natural categories. The results suggest that           coherence, basic level categories, and fast mapping (see
   semantic packing could reproduce the conﬁguration of            also the next paragraph). To this end, we propose two
   natural categories from only their generalization with-         metrics. The ﬁrst is what we call smoothness. This con-
   out any knowledge as fast mapping in which children can
   generalize a novel instance without trial and error. In         cerns the relationship between features and generaliza-
   summary, the semantic packing could be a core mecha-            tion of categories found in developmental studies and is
   nism of the three essential categorization processes, cat-      a measure how cohesive categories are. Categorization
   egory coherence, fast mapping, and basic level category.        with the smooth feature space makes a novel instance
                                                                   generalized precisely, which is considered as fast map-
                                                                   ping. The second metric concerns discrimination, which
                       Introduction                                yields category coherence as the result. There is an es-
Why Does “Category Coherence” Emerge?                              sential trade-oﬀ between generalization and discrimina-
Murphy and Medin (1985) deﬁne “category coherence”                 tion. Both cannot be maximized simultaneously. The
as the intuitive and useful groupings that characterize            joint optimization is obtained by what we call “semantic
natural categories, and claim that this coherence is one           packing”. This process is analogous to a task in which
of most important aspects of semantic cognition. For ex-           various sized and shaped containers (categories) are eﬃ-
ample, color is more important for discrimination when             ciently packed into a larger but ﬁnite container (memory
the item is a pea rather than when it is a ball. How do            and the attentional and retrieval processes that apply to
we learn this? And how do we use the knowledge that we             memory). The packing process is related to processes
have learned? How, when we see an object-a potential               such as cue validity (Rosch et al. 1976) and category util-
pea or ball-do we know to attend to color or not?                  ity (Corter & Gluck, 1992), which are measures for basic
   This problem of feature selection has played a key role         level categories. We ﬁrst present the theory metaphor-
in theoretical discussions of the mechanisms that under-           ically and then the formal mathematical speciﬁcation
lie category learning. In past studies, some measures of           later.
a learner’s category representation were proposed to ex-           Working Deﬁnition: “Semantic
plain the basic level category advantage. The basic cat-
egory is known as the most eﬃcient level for various cog-          Smoothness” in Natural Categories
nitive processes, such as picture naming, category or fea-         Many developmental studies using novel word general-
ture listing, and speech frequency (Rosch, Mervis, Gray,           ization tasks have shown that children systematically
Johnson & Boyes-Braem, 1976). This concerns the rela-              attend to diﬀerent properties when generalizing diﬀer-
tive frequency of features or categories, thus, these mea-         ent types of entities, a process known as “fast map-
sures depend on what features or categories are selected           ping”. For example, children generalized solid artifacts
(Murphy and Lassaline, 1997). For these and other rea-             and non-solid substances based on the similarity of shape
sons, Murphy and Medin (1985) claimed that categoriza-             and material, respectively (Soja, Carey & Spelke, 1991).
tion based only similarity and correlations is not enough          Therefore, children seem to solve the circularity problem,
                                                              1500

knowing to attend to the right properties even though
they do not yet know the category. Importantly, neither
younger children nor late talkers show the same gener-
alization pattern (Jones, 2003). This ﬁnding suggests
that these diﬀerential weighting patterns are learned.
An adult rating study of the similarities that character-
ize the ﬁrst 300 nouns learned by children showed that
their attentional biases in noun extension tasks reﬂected          (a) Exceptional feature                (b) Efficient representation   (c) Overlap feature
the regularities in the corpus of early noun categories.
Speciﬁcally, there is a high correlation between category                                                                                       prototype
                                                                         property 2
generalization (i.e., shape- or material-based category or-                                                                                     instance
ganization) and property (i.e., solidity) (Samuelson &                                                                                          generalization
Smith, 1999). In other words, these rating data indi-                                                                                           parsimony
cated that “property” (i.e. solid or non-solid) of natu-                                                                                        memory space
ral categories predicted ”generalization” (i.e. property                                                                   property 1
                                                                                                                                                discriminability
weighting: shape- or material- based generalization),                                 (d) Category packing under two constraints
and vice versa. The semantic space would be in our
terms “smooth” if the correlation between property and
generalization was universal in any semantic domain, as            Figure 1: 20 categories (ellipses represent contour of
robust as the correlation between solidity and shape-              probabilistic distributions) in property space. (a) Cat-
based generalization in early acquired noun categories,            egory coding with exceptional feature: More property
that is, the property diﬀerence between any two cate-              space (dashed line) is needed for categorization. (b) Ef-
gories would be correlated to a diﬀerence in how those             ﬁcient category coding: Less property space and more
categories are generalized (see also Equation 12 for the           discriminability. Categories with similar generalization
mathematical deﬁnition). Furthermore, the smooth se-               are localized. (c) Category coding with overlapped fea-
mantic space would form clusters that have a correlated            ture: Feature is useless for categorization. (d) Category
property-generalization relationship. In other words, be-
                                                                   packing is considered the balanced optimal state under
cause of the property-generalization correlation, simi-
larly distributed categories would be grouped near each            two constraints: memory parsimony and discriminability
other (i.e., domain speciﬁc property weighting: Figure             of category.
1 (b)) 1 . Thus “smoothness” of the semantic space
may be considered as a quantitative measure of cate-
gory coherence. Here, in an empirical study, we inves-
tigate the smoothness of the semantic space of early-              gory. Importantly, the surface diﬀerences in these mea-
acquired nouns. The results indicate that natural cat-             sures (Corter & Gluck, 1992) could just reﬂect the dif-
egories have “smoothness”, that some categories (e.g.,             ferences in the selected set of categories and features
“cat” and “tiger”) share similar properties and general-           (Murphy & Lassaline, 1997).
ization patterns, and that other categories (e.g., “cat”              We argue that learning eﬃciency under the trade oﬀ
and “chair”) share dissimilar ones. Before a more de-              between discrimination and generalization is the core
tailed presentation of the theory, we consider how one             mechanism underlying category coherence. We deﬁne
recent theory of category development has dealt with               “semantic memory” for the purposes of this paper as a
this issue.                                                        set of categories, which is formulated as a probabilistic
                                                                   distribution in psychological feature space (Figure 1).
Eﬃciency in Semantic Cognition                                     A reasonable question to ask is what features should
                                                                   be represented for any category (e.g. A feature “with
Past proposed measures of the basic category advantage             four limbs” for both “dog” and “cat”). Categories in an
concentrate on the discriminability of categories. More            overlapping feature space need less memory. But over-
speciﬁcally, cue and category validity are respectively            lapping features also mean less discriminability of the
maximized in subordinate and superordinate categories              categories (Figure 1 (c): E1 gets smaller, but G gets
(Murphy & Lassaline, 1997). Furthermore one variety                bigger in Equation 7). One solution to this is to store
of category utility is equivalent to mutual information            only exceptional features or conjunctive features such
(i.e. degree of probabilistic independence) between cat-           that the conjunctions are unique for each category(e.g.,
egories and features (Gluck & Corter, 1985). This is               “four-limbed, eyed, a pet, meat-eating, gather in crowds,
maximized with dependent categories and features (e.g.,            ...” as single feature only for “dog”). The categories in
one-to-one mapping between features and categories: an             this exceptional feature space must be sparse and will
extreme case of subordinate categories). These measures            therefore need more memory (i.e., larger dashed-line en-
are (positively or negatively) discriminabilities of cate-         closure). This is, by deﬁnition, all categories must be
    1                                                              deﬁned by unique conjunctive feature sets. (Figure 1
      Consistent with the mathematical term “smooth”,
“smoothness” here refers to the probabilistic degree of local      (a): G gets smaller, but E1 gets bigger in Equation 7).
linearity of manifold (category-feature space) where general-      However, this means that what one knows about one
ization σi is curvature around μi .                                category can not help in learning or making decisions
                                                            1501

about another. This is precisely where the developmen-                     called the “Bhattacheryya bound” (Duda, Hart & Stork,
tal evidence is most compelling: The categories children                   2000). The inequality min(a, b) ≤ aβ b1−β is true when
already know help them learn new similarly structured                      a, b ≥ 0 and 0 ≤ β ≤ 1. Therefore, Using Bayse’ theo-
categories. This insight indicates that discriminability                   rem P (ci , θ) = P (θ|ci )P (ci ), where P (θ|ci ) is conditional
and generalization from knowledge about one category                       probability θ given ci .
to another trade oﬀ. An eﬃcient semantic memory may                                                       
try to optimize both, and that may work at some mi-                          ˆ ≤ P (c1 )β P (c2 )1−β           P (θ|c1 )β P (θ|c2 )1−β dθ (2)
dlevel between these two extremes (Figure 1 (b)).
   What then would emerge in such a case? We describe
                                                                           The right side in this inequality is called the “Charnoﬀ
the optimal state as “category packing”, where the sys-
                                                                           bound”, the upper bound of the error. β giving the
tem packs categories of a particular shape in feature
                                                                           minimum a Charnoﬀ bound is around β = 12 , there-
space close together, thus taking up less feature space
overall (Figure 1 (d): both G, E1 and L get smaller                        fore, Chernoﬀ bound with β = 12 , called Bhattach-
in Equation 7). Assume that one creates optimally or-                      eryya bound ψb can     be used as the second best bound.
ganized categories by moving the prototypes or, alter-                     Obviously, ψb =           P (c1 )P (c2 )F2 . When n > 2, for
natively, the distributions (i.e. ∂μ      ∂L
                                                = 0 or ∂μ∂L
                                                               = 0 :       short tailed probabilistic distribution such as normal
                                             i
note that this is not “category learning”). This pro-
                                                           i
                                                                           distribution, F ∼    = kF F2 = B can be approximated
cess is analogous as packing things into smaller space                     with constant kF in the local of the particular near-
(categories or things avoid probabilistic or solid “colli-                 est pair
                                                                                   of normal distributions.             Therefore, suppose
                                                                                                                1            1
sion”, respectively). The most eﬃcient packing of diﬀer-                   ψij = P (ci )P (cj ) P (θ|ci ) 2 P (θ|cj ) 2 dθ.
ent sizes and shapes of things (or categories, we propose)                                                          
                                                                                               P (ci ) n Fn ∼
                                                                                                       1
consists of packing similarly shaped things together (i.e.                                                  = kF                 Bij                (3)
emergence of semantic smoothness: Figure 1 (b) and                                          i                         i   j=i
Equation 12). Next we brieﬂy introduce the detailed
formulation of our theory in a simple case in which cat-                   The maximum correct ratio of cue validity (P (ci |θ):
egories are deﬁned by prototypical representations.                        Rosch et al., 1976) model considering frequency
                                                                          of feature P (θ) (Reed, 1972) is deﬁned as Q̂ =
     Theoretical Formulation of Packing                                       maxi P (ci |θ)P (θ)dθ = maxi P (θ, ci )dθ. Therefore,
We prove the equivalence between semantic packing and                                                 
                                                                                              Fn ∼
                                                                                                                    1
smoothness, under the simpliﬁcation that each category                                            =       P (ci )− n (1 − Q)                        (4)
                                                                                                       i
is represented by its prototype and generalization pat-
tern. Note that this simpliﬁcation does not assume any                     In other words, minimum Fn indicates the maximum
predeﬁned speciﬁc “feature” or “category” in the pack-                     cue validity Q. The category          utility of category i is de-
ing process and also that the prototypical representa-                     ﬁned as U (ci ) = P (ci ) (P (θ|ci )2 − P (θ)2 )dθ(Corter &
tion is not a necessary assumption but an application.                     Gluck, 1992). Total category utility U =                        i U (ci ) is
Instead of specifying categories and features, we investi-                 a similar measure with F as follows (i.e.                   the   order is
                                                                                                                                 
gated what category organization emerges as the result                     O(U ) = O(−F 2 )). Applying P (θ) =                       i P (θ, ci ) and
of the eﬃcient categorization. Before this proof, we also                  ku = P (ci )−1 − 1.
prove the approximate equivalence among our discrim-
inability measure, cue validity, and category utility.                                                            
                                                                                                                                     dBij 2
   Assume that there are n categories c1 , c2 , ..., ci , ..., cn             U=        ku P (θ, ci )2 dθ − 2                      (       ) dθ (5)
                                                                                                                                      dθ
                                                                                                                             j=i
in feature space θ ⊂ Ω. Assume the conditional proba-                               i                                   i
bility of feature θ given category ci as P (θ|ci ). F deﬁned                  Next, we prove the equivalence between semantic
as the equation below indicates measure of discriminabil-                  packing and smoothness when the probability of feature
ity among categories, which is upper bound of minimum                      θ given ith category P (θ|ci ) is deﬁned as, a prototypi-
error ratio under optimal decision making.                                 cal representation, a d-dimensional normal distribution.
                                                                                                               1
                                                                           Then P (θ|ci ) = ((2π)d |σi |)− 2 exp(− 21 (θ − μi )t σi−1 (θ −
                                              1
                      Fn =          P (θ|ci ) n dθ                (1)      μi )), where a mean vector (i.e., prototype) and covari-
                               Ω  i                                        ance matrix (i.e., generalization or feature weighting) are
                                                                           μi and σi . The superscript t refers to transposition. The
Let the joint probability of category ci in feature θ be                   discriminability measure (Equation
P (ci , θ). If one evaluated that it is         category  ci when                                                 n 1) can berewritten
                                                                          as follows. Assume that A = i σi−1 , B = i σi−1 μi ，
                                                                                                                                            n
θ ⊂ Ωi , then the correct ratio is                                               n t −1
                                                   i Ωi P (ci , θ)dθ.      C = i μi σi μi , and G = log(F )
In particular n = 2, the minimum error ratio of
the
     optimal decision (i.e. Bayes decision) is ˆ =                                 1                                          n
  Ω
    min(P   (c1 , θ), P (c2 , θ))dθ, because, to maximize the                  G=       (B t A−1 B − C − n log |A| −                 log |σi |)     (6)
correct ratio, one must choose the category with largest                            2n                                           i
probability given θ. To estimate analytically the exact                                                                     ∂G         ∂G
minimum or maximum distribution is diﬃcult. Accord-                        Optimization for only the constraint ∂μ             i
                                                                                                                                  or ∂σ  i
                                                                                                                                            (i.e., dis-
ingly we used instead the upper bound of the minimum,                      criminability in Figure 1) gives (μi −μj ) (μi −μj ) → ∞ or
                                                                                                                               t
                                                                     1502

|σi | → 0 , indicating an immense amount of feature space              0. Let ν = (ν1 , ν2 , ..., νn )t be the d-by-n-dimensional
or an instance as a category (i.e., no generalization),                vector having νi as its ith elements. In addition, let Σ be
respectively.Therefore,      constraints to normal distribu-         the super matrix having σi as its ith diagonal elements,
tions E1 = i ||μi ||2 = i μti μi and E2 = log |A−1 | are               and A−1 be a super matrix having n2 A−1 as its all
                  n              n
necessary. For the cognitive process, the constraints E1               elements. Then,
and E2 refer to maintenance of constant memory space
(i.e. parsimony in Figure 1) and generalization ranges,                                    ν − λ(Σ − A−1 )ν = 0                    (13)
respectively. The Lagrange multiplier method is used
for optimization of the constraints. The Lagrange equa-                Thus, Equation (7) (i = 1, ..., n) can be solved by ν as
tion with multiplier λ is L = G + λ1 E1 + λ2 E2 , which                an eigenvector of (Σ − A−1 ) in Equation (13).
indicates semantic packing (L) optimizes both discrim-
inability (G) and generalization (E1 and E2 ).                                                    Method
  ∂L        ∂                                                          Survey Procedure
        =       (G + λ1 E1 ) = −σi−1 (μi − μ̄) + λ1 μi = 0 (7)
  ∂μi      ∂μi                                                         The ﬁrst step in the simulation study was to collect data
                          n             n                            on the similarities of 48 nouns that are among the ear-
where μ̄ = A−1 B = ( i σi−1 )−1 i σi−1 μi .                            liest learned by children (Fenson et al, 1993). To de-
                                                                       termine the relevant similarities across a broad range
                      μi = −(λ1 σi − I)−1 μ̄                   (8)     of properties, 104 Japanese undergraduates rated each
                                                                       noun category using 16 pairs of adjectives (Hidaka &
where I is the identity matrix. Therefore the relationship             Saiki, 2004). The goal here is to place the categories in
between a pair of categories when L is optimized as a                  a relatively (16 dimensions) large feature space. These
function of μ is                                                       adjective pairs are the potential features. Subjects used
                                                                       a 5-point scale to indicate how well the pair of adjectives
         Δμij = λ1 (λ1 σi − I)−1 Δσij (λ1 σj − I)−1 μ̄         (9)     described the items in the category (e.g., large = 5, small
                                                                       = 1). The 16 pairs of adjectives were selected by a pilot
where Δμij = μi − μj and Δσij = σi − σj . Next, in                     survey using 41 pairs collected from prior studies. We
addition to μi , L is optimized as a function of σi . As               created questionnaires of 5 diﬀerent orderings to cancel
                                                           −1
∂σi = ∂σi (2nG + λ2 E2 ) , thus applying ∂σi = σi (μ̄ −
 ∂L       ∂                                      ∂2nG
                                                                       out the order eﬀect. Participants completed the survey
               t −1        −1 −1 −1          −1
μi )(μ̄ − μi ) σi + nσi A σi − σi                                      in about an hour.
      ∂L                                                               Stimuli
  σi      σi = (μ̄ − μi )(μ̄ − μi )t + (n + λ2 )A−1 − σi (10)
      ∂σi                                                              • Adjective pairs (linguistic scales)
                                                                          dynamic-static, wet-dry, light-heavy, large-small, complex-
   As σi ∂σ∂L
             i
               σi − σj ∂σ
                        ∂L
                          j
                            σj = 0                                        simple, slow-quick, quiet-noisy, stable-unstable, cool-warm,
                                                                          natural-artiﬁcial, round-square, weak-strong, rough hewn-
                       
            Δσij =          (−1)δki (μ̂ − μk )(μ̂ − μk )t     (11)        ﬁnely crafted, straight-curved, smooth-bumpy, hard-soft.
                      k=i,j
                                                                       • Noun categories
where δii = 1 when i = j, otherwise δij = 0. Notice                       butterﬂy, cat, ﬁsh, frog, horse, monkey, tiger, arm, eye,
that σi is constant in Equation 9, and Equation 11 is                     hand, knee, tongue, boots, gloves, jeans, shirt, banana, egg,
Δσij ∼ = O(Δμij ). Consequently, the approximate mono-                    ice cream, milk, pizza, salt, toast, bed, chair, door, refrig-
tonic relationship between Δμij and Δσij with a given                     erator, table, rain, snow, stone, tree, water, camera, cup,
                                                          ∂L              key, money, paper, scissors, plant, balloon, book, doll, glue,
constant α (i.e. “smoothness”) emerges, when ∂μ             i
                                                              =0          airplane, train, car, bicycle
     ∂L
or ∂σi = 0 (i.e. “packing”).
                                                                       Analysis and Simulation
                    ||μi − μj || ≈ α||σi − σj ||              (12)
                                                                       Correction of survey data The rating value was
In other words, semantic smoothness, which is the cor-                 corrected by a logistic function to make the correlation
relation between feature and generalization (Equation                  between mean and variance zero. The original rating
12), is approximately equivalent to semantic packing. A                showed a small positive correlation between the devia-
learning system with smooth categories that optimize                   tion from the median and the variance, because an ex-
the packing principle, and vice versa.                                 treme rating (i.e., a rating near one or ﬁve) has a smaller
   An analytic solution to ∂μ      ∂L
                                        = 0 is demonstrated as         variance than a rating near the median. More specif-
                               
                                     
                                     i
                                        n t                            ically, the parameters of the logistic function f (x) =
follows. Assume that E1 =               i νi νi where νi = μi −        (1 + exp((x − b)c−1 ))−1 are estimated to have zero corre-
  −1
A B to be the constraint instead of E1 , and note that                 lation between |x − b| and a standard deviation of rating
the replacement does not lose generality. Solving the                  x, and estimated parameters are b = 3 and c = 1.2. The
                                   λ                      −1
Lagrange equation L = G       2 + 2 E1 , we get ∂μi = −σi νi +
                                                  ∂L
  n                                                                   corrected mean and variance is used for analysis and
λ j (δij −σi−1 A−1 )νj where δii = 1 when i = j, or δij =              simulation.
                                                                  1503

Index of semantic smoothness Semantic smooth-                                                                      
ness, as predicted by Equation 12, was speciﬁcally calcu-
                                                                        FKUVCPEGQHRCKTGFEQTTGNCVKQPOCVTKEGU
lated by norms of the mean vector and covariance matrix                                                           
in the model. The mean vector and covariance (or corre-
lation) matrix represent the mean and covariance across                                                            
the 16-adjective ratings for all subjects. The correlation
and contribution of the norms of the mean vector and                                                              
the covariance were used as an index of smoothness. The                                                            
contribution of the major axis is calculated by the prin-
cipal component analysis, because the norms of both the                                                           
mean and the covariance have variances. In other words,
the coeﬃcient of determination in the regression analy-                                                            
                                                                                                                                                                               
sis underestimates this contribution because it supposes                                                                    FKUVCPEGQHRCKTGFOGCPXGEVQTU
that only the dependent variable has error.
Simulation of packing category Three simulations                 Figure 2: Scatter plot of mean vector norm (x axis:
were run. The ﬁrst simulation involved the semantic              prototype dissimilarity) and correlation matrix norm (y
packing of randomly generated categories with the goal           axis: generalization dissimilarity) in survey data
of visualizing coherent categories. The second simulation
attempted to reproduce category organizations based on
the adjective ratings for the corpus of early learned nouns                                                        
and in doing so demonstrates how packing might explain
                                                                        FKUVCPEGQHRCKTGFEQXCTKCPEGOCVTKEGU
fast mapping. The third simulation involved a Monte                                                               
Carlo simulation investigating the relationship among
measures of discrimination.                                                                                        
   In the semantic packing simulation, we optimized the
                                                                                                                  
mean and covariance of several categories with randomly
generated initial means and covariances (i.e. the gradi-                                                           
ent method: updating the parameters based on Equation
13 and 10) . The smoothness index was measured after                                                              
updating was performed 100 times. The updated ﬁnal
state refers to optimization in terms of balanced con-                                                             
                                                                                                                                                                             
straints.In the simulation of the adjective rating data                                                                     FKUVCPEGQHRCKTGFOGCPXGEVQTU
for early-learned nouns, the means of categories were
reproduced by a solution of Equation 13 for a given co-          Figure 3: Scatter plot of mean vector norm and correla-
variance matrix of survey data. This simulation investi-         tion matrix norm in simulation
gates the predictability of a prototype conﬁguration in
real data based on the generalization pattern. The re-
sults were evaluated based on the correlation between
the distances between all pairs of categories in the re-         .357 and .688, respectively. These results suggest that
produced and original prototype conﬁguration. The de-            the investigated category set has smoothness. The mean
grees of freedom of the conﬁguration to be estimated is          norm and covariance norm of paired categories (each
752 (the number of categories without pivot of rotation          category paired with every other category) are shown
by property dimension (48 − 1) × 16). In the Monte               (Figure 3). The average and standard deviation of the
Carlo simulation, 20 one-dimensional normal distribu-            smoothness index for 100 simulations were .490 and .181,
tions with uniform-random means (-3 to 3) and variances          respectively. The correlation and contribution between
(0.2 to 2) were generated   500 times. The maximum (Q),          the reproduced mean matrix in the simulation and the
                                                                mean of survey data were .430 and .715, respectively.
paired minimum ( i,j=i min(P (θ|ci ), P (θ|cj ))), overlap
(F                                                               These results suggest that semantic packing could re-
 ), category utility (U ), paired Bhattacheryya bound           produce half of the categories from only their general-
( i,j=i Bij ) of the generated distributions were calcu-
                                                                 ization without any knowledge of the category conﬁgura-
lated theoretically (F , U , and B) or numerically (max          tion. This is the kind of result needed to explain feature
and min: integral range from -10 to 10 and sample res-           selection and fast-mapping in children.
olution 0.01), and the correlation was analyzed.                    The results of Monte Carlo simulation are shown in
                                                                 Table 1. Most of the absolute correlations |R| were
                        Results                                  greater than .8, which indicated that the ﬁve measures
                                                                 were approximately equivalent as proven.
Figure 2 shows the relationship between the mean norms
and the correlation norm for the adjective-rating The
correlation and contribution are .466 and .733, respec-
                                                                                                                                          Discussion
tively. The correlation and contribution of the smooth-          The results of Monte Carlo simulation exploring some
ness index using covariance, rather than correlation, are        discriminability measures empirically support the the-
                                                          1504

        τ \R    max        F      U        B     min
        max        1    -.867    .855   -.877   -.863                             Acknowledgments
          F     -.681      1    -.791    .938    .806            This work was supported by grants from Grants-in-Aid
          U      .659   -.601      1    -.903   -.949            for Scientiﬁc Research from JMEXT (No. 15650046),
          B     -.686    .785   -.740      1     .938            JSPS Research Fellowships for Young Scientists, Ky-
        min     -.658    .620   -.814    .796      1             oto University Foundation and the National Institutes
                                                                 of Mental Health, R01 MH60200-06.
Table 1: Peason correlation coeﬃcient R (upper triangle)
and Kendall rank correlation τ (lower triangle) among                                   References
measures                                                         Corter J. E., & Gluck M. A. (1992). Explaining Basic
oretical relationship, which is the approximate equiva-             Categories: Feature Predictability and Information.
lence among F , cue validity and category. In the pack-             Psychological Bulletin, 111, 291–303.
ing theory, generalization is not just negative discrim-         Duda, R. O., Hart, P. E., Stork, D. G. (2000) Pattern
inability but a limitation on category representation re-           Classiﬁcation (2nd ed) , New York: John Wiley &
sulting from the whole memory capacity and a lower                  Sons.
bound on generalization. With the two conﬂicting con-            Fenson, L., Dale, P. S., Reznick, J. S., Bates, E., Thal,
straints, the computational model predicts that smooth              D. J., & Pethick, S. J. (1994). Variability in early
category organization emerges. Consistent with this pre-            communicative development. Monographs of the So-
diction, the smoothness index of the adjective-rating               ciety for Research in Child Development, 59 (5, Serial
data for early learned nouns suggests that property-                No. 242) Chicago: University of Chicago Press.
generalization clusters were formed in not only speciﬁc
domains (e.g. solidity-shape in the survey of Samuelson          Gelman, R., & Williams, E. (1997). Enabling constraints
& Smith, 1999) but also more generally. Indeed from                 on cognitive development. In D. Kuhn & R. S. Siegler
these data, one can predict regions of “fast mapping” in-           (Ed&.). Cognition, perception and language. Vol. 2.
volving property-category organization correlations that            Handbook of child development. (5th ed.) (pp. 575-
are unknown to and unexplored by researchers in early               630). (W. Damon, Ed.) New York: Wiky.
category development. The success in a quantiﬁcation             Gluck, M. A. & Corter, J. E. (1985). Information, Uncer-
of category coherence using smoothness index provides               tainty, and the Utility of Categories, , Proceedings of
empirical evidence of the role of semantic packing in hu-           the Seventh Annual Conference of the Cognitive Sci-
man natural categories. Granted the results could be due            ence Society (pp. 283–287). CA: Lawrence Erlbaum
to the speciﬁc properties and noun categories selected.             Associates.
However, the adjectives were selected by their discrim-          Hidaka, S. & Saiki, J. (2004). A mechanism of onto-
inability (i.e. variance of the adjectives) to the category         logical boundary shifting , Proceedings of the Twenty
set (Hidaka & Saiki, 2004), and smoothness can be ob-               Sixth Annual Conference of the Cognitive Science So-
served in feature space with discriminability. Therefore,           ciety (pp. 565–570). Chicago, IL.
the adjective-rating results may be taken as making pre-
                                                                 Jones, S.S. (2003). Late talkers show no shape bias in
dictions about the feature space as it relates to early
                                                                    object naming. Developmental Science, 6(5), 477–483.
acquired noun categories (i.e. basic categories).
   The success in reproducing the organization of early          Murphy, G. L., & Lassaline, M. E. (1997). Hierarchi-
learned nouns suggests that a category system con-                  cal Structure in Concepts and the Basic Level of Cat-
strained by semantic packing principle could generalize a           egorization. in Lamberts, K. & Shancks, D. (Eds),
category to new instances without trial and error. This             Knowledge concepts and categories, UK: Psychology
implies that the system would “know” the generalization             Press.
pattern of a novel thing in a certain region of feature          Murphy, G.L. & Medin, D.L. (1985). The role of the-
space. Young children show precisely this kind of knowl-            ories in conceptual coherence. Psychological Review,
edge in generalizing names for novel categories. This is            92, 289–316.
typically referred to in the developmental literature as         Reed, S. K. (1972). Pattern recognition and categoriza-
“fast mapping.” Notice that the system has no meta-                 tion. Cognitive Psychology, 3, 382–407.
knowledge , as theory-theory claims, about speciﬁc do-
mains, but smoothness, a property of the whole system            Rosch, E., Mervis, C. B., Gray,W., Johnson, D., &
in which categories are learned and represented, does the           Boyes-Braem, P. (1976). Basic objects in natural cat-
work of such meta-knowledge.                                        egories. Cognitive Psychology, 8, 382–439.
   In summary, the proposed theory suggests that cate-           Samuelson, L. & Smith, L. (1999). Early noun vocab-
gory packing process, balanced discriminability and gen-            ularies: do ontology, category structure and syntax
eralization (i.e. basic categories), leads smooth category-         correspond?, Cognition, 73, 1–33.
feature organization (i.e. category coherence) consistent        Soja, N. N., Carey, S. & Spelke, E. S. (1991). Onto-
to human natural categories, and the smoothness help                logical categories guide young children’s inductions of
learners’ generalization to novel categories (i.e. fast map-        word meanings: object terms and substance terms.,
ping).                                                              Cognition, 38, 179–11.
                                                            1505

