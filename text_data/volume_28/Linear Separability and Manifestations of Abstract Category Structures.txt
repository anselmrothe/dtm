UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Linear Separability and Manifestations of Abstract Category Structures
Permalink
https://escholarship.org/uc/item/9vv762fz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Author
Kenpei, Shiina
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

               Linear Separability and Manifestations of Abstract Category Structures
                                                        Shiina Kenpei (shiina@waseda.jp)
                          Department of Educational Psychology, Waseda University, Tokyo, 169-8050 JAPAN
                                  Abstract                                  category, however, tends to underestimate the effect of
                                                                            structures in the environment (Malt, 1995) because people
   Experimental evidence thus far has been overwhelmingly                   do not generally use theories or themes.
   against the idea that linear separability is intrinsically                  It is fair to say that experimental evidence thus far is
   important in category learning. This paper tries to shed new
                                                                            overwhelmingly against the idea that LS is essential in
   light on this old problem and shows conditions under which
   linear separability promotes learning.                                   category learning; Medin and Schwanenflugel (1981),
                                                                            Kemler-Nelson (1984), Nakamura (1985), Wattenmaker et
After the seminal study of Medin & Schwanenflugel (1981)                    al. (1986), and Wattenmaker (1995) reported that LS had
on the role of linear separability (LS) in categorization, it               virtually no positive effect per se on category learning when
has been repeatedly reported that LS has virtually no                       within- and between- category similarities are controlled.
positive effect on category learning especially when within-                Smith, Murray, and Minda (1997), who encourage prototype
and between- category similarities are controlled. In this                  model, also failed to provide evidence. But they raised
paper, after formally defining LS and linear discriminant                   several key questions about how experimental stimuli are
function with research review, we introduce several                         generated. In particular, their demonstration of poor
properties of LS that have been unnoticed in the past studies.              differentiation of NLS categories in the universe of category
Then 2 experiments are reported that showed LS learning                     structures is quite insightful. More recently Blair and Home
advantages followed by discussion.                                          (2001) have found an LS advantage, but their experimental
                                                                            setting is different from that of the present study.
               Definition and Research History
                                                                             Some Hidden Properties of Linear Separability
Suppose there are two Categories A and B comprised of m
                                                                               In the research of categorization, there are several con-
instances each and instance i is represented by a p-
                                                                             ceptual and procedural problems that might have disturbed
dimensional row vector: x i = ( xi1 , xi 2 ,...., xip ).        A binary
                                                                             the detection of LS effects.
dimension is often called a feature in this paper. Linear
                                                                             Reconstruction of Feature Space Consider the binary,
discriminant function (LDF) f is a linear function:
                                                                             abstract category structure shown in Table 1a. Category
  f ( xi ) = ω 1 x i1+ ω 2 x i 2 + ... + ω p x ip + c , such that            structures of this type are used in numerous studies, and an
  if f ( x i ) ≥ 0 then i ∈ Category A else i ∈ Category B, where            experimenter will assign somewhat arbitrary concrete values
ω= (ω1, ω2, ...., ωp) is a weight vector and c is a constant. If             (on a nominal, ordinal, interval, or ratio scale) to the 0’s and
an LDF exists Categories A and B are linear separable.                       1’s to generate stimulus manifestations (Table 1b). A key
 The LDF approach has been a topic of intense debate                         assumption in such studies is that participants will and can
(Ashby & Maddox, 2005.) There are many studies which                         reconstruct the experimenter-defined abstract structure from
directly compared learning rates of LS and NLS (not linear                   the experienced category instances, whereas in ignorance of
separable) categories. Shepard and Chang (1963) compared                     the experimenter’s feature value assignment people should
the difficulty of supervised classification learning for 6                   assign new abstract binary values to the manifested values
category structures, and found that LS categories were                       in the reconstruction process and thus the reconstructed
easier to learn. They suggested that "the easy classifications               abstract structure may look like Table 1c. Moreover, in
tend to differ from the difficult ones in that their points can              order to control possible interactions between category
be roughly partitioned into the two subclasses simply by                     structure and feature manifestation, the concrete feature
drawing a straight line through the two-dimensional space"                   values are ordinarily assigned randomly between subjects.
(p.102). Ashby and his colleagues (Ashby & Gott 1988;                        As such, a participant might reconstruct a category structure
Ashby & Maddox 1990, 1992) found a consistent advantage                      that is a double transform of the original abstract structure.
for LS categories over NLS ones. Wattenmaker et al. (1986)                   Interpretation of Features Since we cannot numerically
and Wattenmaker (1995) also found some positive effects of                   add a circle and whiteness, the construction of an LDF
LS. Specifically, strong interactions between category                      requires inter-dimensional additivity (including subtractive-
structures and activated domain theories were found and                      ty) of the abstract values. If the binary features in Table 1c
they were attributed to the coding of stimulus properties                    are interpreted as nominal or logical variables, their addition
induced by theories or themes. They concluded that when                      is unnatural and logical operations should be used instead. If
theories are available, the compatibility between theory and                 the values are interpreted as numerical, however, arithmetic
category structure will determine the ease of learning, which                operations can be used and LDFs can be computed. Further-
gave an impetus to the theory-based category learning                        more, since we are reluctant to compare, say, body weight
models (Murphy & Medin, 1985). The idea of theory-based                      with annual income, dimensional homogeneity will promote
                                                                             LDF construction together with additivity.
                                                                        2152

 Table 1 An Abstract Category Structure (Medin and Schwanenflugel(1981) Experiment 1, LS structure)
             and a flow of typical categorization experiment
 (a) Experimenter Defined Structure                      (b) An Experimental Manifestation                       (c) Reconstructed Structure
             Dim 1 Dim 2 Dim 3 Dim 4                 A1           A2         A3         A4                                  Dim 1 Dim 2 Dim 3 Dim 4
 Category           1   0     1      1                                                                           Category         1       1      0    1
    A               1   0     1      0                                                                              A             1       1      0    0
                    1   1     0      1                                                                                            1       0      1    1
                    0   1     1      0                                                                                            0       0      0    0
 Category           1   0     0      1               B1           B2         B3          B4                      Category         1       1      1    1
   B                0   0     1      0                                                                              B             0       1      0    0
                    0   1     0      0                                                                                            0       0      1    0
                    0   0     0      1                                                                                            0       1      1    1
             Random Assignment of Concrete Values
             Dim 1 Dim 2 Dim 3 Dim 4                                                           Reconstruction of Abstract structure
             1=white 1=square 1=big      1=right
             0=black 0=circle 0=small 0=left
Ease of Weight Computation In Table 1a we can easily                              TV =         ∑ ∑ (x       ik − xk ) 2
find an LDF with weights ω1=ω2=ω3=1, ω4=0 :                                                 i∈ A ∪ B k
   If dim1+ dim2 + dim3 ≥ 2 then Category A else B, (1)                           WV =        ∑ ∑ (x
                                                                                             i∈ A k
                                                                                                         ik − x Ak ) 2 +  ∑    ∑ (x
                                                                                                                          i∈ B k
                                                                                                                                       ik  − x Bk ) 2
while in Table 1c it is not easy to find an LDF:
  If dim1 - dim2 - dim3 ≥ 0 then Category A else B. (2)                            BV =      ∑ ∑ (x
                                                                                             i∈ A k
                                                                                                         Ak − xk ) 2 +   ∑ ∑ (x
                                                                                                                         i∈ B k
                                                                                                                                       Bk  − xk ) 2
It seems unlikely that the computational cost is the same for
                                                                                 where
all LDFs: f(x)=3.7dim1-.9dim2+.2dim3+.4 should be harder
to compute than g(x) =dim1+dim2+dim3+0. In an extreme                              xk = ∑ xik / 2m , x Ak = ∑ xik / m , xBk = ∑ xik / m .
                                                                                        i∈A∪ B                      i∈A                     i∈B
situation where one dimension, say Dim2, is sufficient to
construct an LDF with null weights to irrelevant dimensions,                     TV is psychologically interpretable as the diversity of the
we have: If dim2 >c then Category A else B, which is often                       overall configuration of instances, WV as the within-
called a rule and Dim2 is ordinarily called a defining                           category variation, and BV as a measure of separation or
dimension. If an LDF happens to be: If dim1 - dim2 >0 then                        contrast between two categories. We have an interesting
Category A else B, this LDF is identical to a relational                          relation similar to (3), namely,
property “larger than”: dim1> dim2. This example shows                               TV=WV+BV                                                                (4)
that LS can capture relational properties, contrary to the                        which again shows a negative correlation between
assertion of previous studies (e.g., Medin & Schwanenflugel                       coherence and separation. With additional work relations:
                                                                                  TSD = 4mTV , WSD = 2mWV , BSD = mWV + 2mBV
(1981)). “Larger than” relational property reduces to a pair
of defining features in binary cases. In sum, any category                       can be derived and we find that :
structure that is partitioned by either a rule or a type of                      TSD=WSD+2BSD=4mTV=4mBV+4mWV.
simple relational property should be LS (but not vice versa),                    As an index of category separation, Correlation Ratio:
and this fact supports the idea that LS should play some role                     η 2 = BV / TV = (2 BSD − WSD ) / TSD                                        (5)
in categorization.                                                                is used in the sequel, which has the merit of permitting both
Within- and Between- Category Variation LS and NLS                                binary and continuous dimensions. This index ranges over
categories are different in coherence and separation                              [0, 1], with larger values indicating a better separation of
measured by within- and between- category distances or                            categories. For binary dimensions Structural Ratio (additive
similarities (e.g., Smith et al, 1997; Blair & Homa, 2001).                       version) is often used and we have the relation between the
Define Total- , Within-, and Between- category squared                            two indexes:
distances as
      ⎛ Total ⎞                                                                               similarity i∑
                                                                                         ∑ within                ∑ s + i∑
                                                                                                                      ij       ∑s   ij
                                                                                                                                          2mp − 2TV (1 − η 2 )
TSD ⎜ Squared ⎟ = ∑ ∑ dij2 = ∑ ∑ ∑ ( xik − x jk ) 2                                SR =                   =
                                                                                                             ∈A j∈A        ∈B j∈B
                                                                                                                                       =
      ⎝ Distances ⎠ i∈A∪ B j∈A∪ B                                                                                                          2mp − TV (1 + η 2 )
                                        i∈A∪ B j∈A∪ B k
                                                                                         ∑ between                2∑     ∑    sij
                                                                                              similarity
       ⎛ Within ⎞                                   ⎛ Between ⎞                                                    i∈A j∈B
WSD ⎜ Squared ⎟ = ∑ ∑ dij2 + ∑ ∑ dij2 , BSD ⎜ Squared ⎟ = ∑ ∑ dij2
       ⎝ Distances ⎠ i∈A j∈A      i∈B j∈B           ⎝Distances ⎠ i∈A j∈B          Although it is not impossible to set up LS and NLS
then it is easy to find a relation: TSD=WSD+2BSD.                        (3)      structures with equated η2 , in many cases LS categories
WSD measures category coherence, and BSD category                                have smaller WV and thus larger BV, while NLS categories
separation. Equation (3) shows that with TSD fixed, WSD                           have larger WV and smaller BV. It seems, therefore, that
and BSD are negatively correlated, as are category                                NLS categories are less easily discriminated and will have
coherence and separation. An alternative way of defining                          slower learning rates. A similar argument holds for the
category coherence and separation is through the total-                           relationship among TSD, WSD, and BSD as well.
within-, and between- category variances as in ANOVA:                               Given the properties pointed out in this section, the
Define TV, WV, and BV as follows:                                                 question of whether LS per se is advantageous to learning
                                                                             2153

under some conditions still deserves investigation. The                                    1.0
present study tries to reexamine the effects of LS by
arranging conditions in which several of the following                                     0.9
requirements are satisfied. R1) Reconstruction of the feature
                                                                     PROBABILITY CORRECT
space is a transparent process for subjects and the
                                                                                           0.8
reconstructed abstract structures are not too diverse. R2)
Interpretation of reconstructed features is unambiguous. In
particular, the features are interpreted as continuous and                                 0.7
additive (homogeneous) with constant dimensional polarity.                                                                   LS(η2=.143)
R3) LDF weights are simple, and promote relational                                         0.6                               NLS(η2=.143)
properties (e.g., “larger than”) or rules. The three                                                                         LS-NOISY(η2=.136)
requirements are all designed to promote the LS advantages.                                                                  NLS-NOISY(η2=.135)
                                                                                           0.5
Finally, R4) within- and between- category variations are                                        1-40   41-80     81-120   121-160   161-200
controlled in terms of η2 , respecting the tradition in this                                                    BLOCKS
field.                                                              Figure 1 Learning Curves of Experiment 1a
                     Experiment 1a                            Procedure To each condition 28 subjects were randomly
Using the LS structure in Table 1a and NLS one in Table 2a,   assigned. Twenty-five randomized runs were used to make a
a set of bar charts was generated as shown in Table 2b (only  total of 200 stimulus presentations. The experiment was run
the NLS set of bar charts are shown.) Relating bar lengths    individually and was subject-paced. After a subject sat in
to dimension values satisfies R1, R2, and R3, and the         front of a CRT screen, the instruction was appeared on the
category structure complies with R4.                          screen. Then a stimulus was presented one at a time and
   In the original study of Medin and Schwanenflugel          his/her task was to classify the stimulus into either Category
(1981), binary qualitative features were used and the LS and  A or Category B by pushing two response keys. An
NLS structures were equalized in terms of η2=.143. In the     immediate feedback was given and then the next stimulus
present experiment 0 referred to a short bar and 1 referred towas presented. Proceeding in this way, a total of 200
a long bar and no random assignment of concrete values        presentations were made. The labeling of categories and
were performed, that is, the subjects experienced the same    response keys were randomly assigned across subjects.
set of manifested stimuli. Evidently, if subjects construct anResults and Discussion The learning curves for the four
LDF similar to (1) the categories can be learned perfectly.   conditions are shown in Figure 1. There was an interaction
The goal of Experiment 1a is to contrast the effect of mani-  between category structure and noise. In no-noise conditions
festation of dimensions with the original experiment in       the categories were easier to master than in noisy conditions
which LS category was no easier to learn. Since the possibi-  but the LS categories were easier to learn only in the noisy
lity remains that subjects interpret the bar length as a dis- condition. Logistic regression analysis revealed that the LS
crete feature, noisy conditions were arranged where bar       effect was significant (χ2(1) =20.38, p<.01), the effect of
length was fluctuated with small random numbers ranging       noise was significant (χ2(1) =727.49, p<.01), and the
from -9 to 9 to discourage the subjects from interpreting the interaction between them were significant (χ2(1) =10.90,
dimensions as binary. Noisy conditions will also discourage   P<.01). Post hoc test by Tukey method revealed that there
memorization strategies. There were 4 conditions              was no significant difference between the LS no-noise and
combining Factors A and B (Factor A: LS vs. NLS, Factor       NLS no-noise conditions across blocks.
B: no-noise vs. noisy). The subjects were 112 students of      The values of η2 for the four conditions were virtually the
Waseda University.                                            same and thus the LS gain in the noisy condition is not
                                                                              attributable to the category variability: The
  Table 2 An Abstract Category Structure                                      results showed evidence that there are
  Medin and Schwanenflugel(1981) Experiment 1, NLS structure                  situations in which LS without favorable
  (a) Experimenter Defined Structure  (b) Noisy Experimental Manifestation    properties (large η2, relational properties, and
              Dim 1 Dim 2 Dim 3 Dim 4                                         rules) can promote category learning. One
  Category         1      0   0     0                                         common property of previous studies such as
     A             0      1   1     1
                   1      1   1     0
                                                                              Medin and Schwanenflugel (1981) that found
                   1      0   1     1                                         no positive LS effect is that the dimensions
  Category         0      1   1     0                                         were binary. Consistent with these studies,
     B             1      0   0     1                                         the binary valued dimensions in the no-noise
                   0      0   0     0
                   0      0   0     1                                         condition did not produce an LS advantage,
                                                                              whereas in the noisy conditions where the
  1 -> long bar (145 dots)                                                    random perturbation suggested to the subjects
  0 -> short bar (100 dots)
  The base is 20 dots
                                                                              that the dimensions were continuous, the LS
                                                                              structure was easier to master.
                                                             2154

  Why, then, do binary dimensions impair learning of LS                                      1.0
categories? As mentioned previously, the concept of LS in
general (and LDFs in particular) is meaningful in a                                          0.9
continuous space, because summation and multiplication are
                                                                       PROBABILITY CORRECT
valid only when the values are on an interval or on a ratio
                                                                                             0.8
scale. It follows that, for a summation strategy to be feasible,
the feature values should be interpreted as continuous even
if they can only take discrete values. In ordinary situations,                               0.7
however, binary features are interpreted as nominal values,
preventing multiplication and summation and rendering the                                    0.6
                                                                                                                             LS-NOISY(η2=.139)
concept of LS irrelevant.                                                                                                    NLS+NOISY(η2=.134)
Experiment 1b
                                                                                             0.5
     It seems that previous studies assume that there are no                                       1-40   41-80     81-120   121-160   161-200
differences in the ease of setting up LDFs. In prototype                                                          BLOCKS
models, in particular, subjects should always set up LDFs.         Figure 2 Learning Curves of Experiment 1b
Although Experiment 1a produced some evidence that LS
promotes learning, the LDFs to be derived have only
positive, unit weights (see equation (1)) and thus are easy to
compute. In Experiment 1b a variation of the original
abstract structure was used to introduce negative weights,
making the LDF computation more difficult. Even if
subjects are able to use LDFs in some situations, they might
fail to utilize negative weights and the LS effect may vanish.
Category Structure The two conditions compared were LS
(Table 1c) and NLS (Table 3) structures with noise. The LS
structure was arranged by reversing the abstract values of
dim2 and dim3 in Table 1a. The LDF thus takes the form of             Figure 3 Stimulus Arrangement in Experiment 2
equation (2). The NLS structure was arranged by first
reversing the values of dim2 and dim3 in Table 2a and
changing the order of dimensions to dim3, dim4, dim1,                                                      Experiment 2
dim2. Note further that 7 out of 8 stimuli in both structures
are common, which may control memorization effects.                In Experiment 2, using bar charts with three bars as stimuli,
Subjects and Procedure The subjects were 32 students of            we highlight the effects of one dimensional rules and
Waseda University and they were randomly assigned to the           “longer than” relational properties (RP) that are sufficient
two equal-sized conditions. None of these subjects                 conditions for LS. Specifically, we contrast the LS+RULE
participated in Experiment 1a. The general procedure was           and LS+RP category structures to two NLS structures, one
identical to that of Experiment 1a.                                with the matched-η2 and the other with a small η2.
Results and Discussion Learning curves are depicted in             Category Structure The numerical values of the experi-
Figure 2. Evidently, no difference was found between the           mental stimuli with three bars x, y, and z are summarized in
LS and NLS conditions (χ2(1)=.3572, P>.5). The results             Table 4. The position of each stimulus in the stimulus space
indicate that an LDF with negative weights did not promote         is represented in Figures 3a and 3b, where each axis corres-
category learning, and suggest that the ease of LDF                ponds to one of the bar lengths and the center (centroid) of
computation would affect LS category learning.                     each cube corresponds to the coordinates in Table 4. Two
                                                                   factors were considered. Factor A, concerned with structural
   Table 3 NLS Abstract Category Structures                        differences, includes four conditions: LS with a relational
          in Experiment 1b                                         property (LS+RP), LS with a defining dimension (LS +
          LS structure is shown in Table 1c                        RULE), NLS with a small η2 (NLS-small-η2 ), and NLS
   NLS                                                             with a large η2 that matches the η2 of LS + RP and LS +
            Dim 1 Dim 2 Dim 3 Dim 4                                RULE conditions (NLS-matched-η2). Factor B comprised
   Category     1     0     1     1                                the no-noise and noisy conditions as in Experiment 1. The
     A          0     1     0     0                                no-noise condition is explained first.
                0     0     1     0
                                                                   No-noise Conditions In this condition, bar charts corres-
                0     1     1     1
   Category     0     0     0     0                                ponding to the centroids of the cubes in Figures 3a and 3b
     B          1     1     1     1                                are presented to subjects. Category structures are defined by
                1     0     0     1                                how each cube is associated with a category label (Table 4.)
                1     1     0     1                                In the LS+RP condition (LS with a relational property ),
                                                                   cubes A, B, C, and D in Figure 3a were combined to make
                                                               2155

Category 1, and E, F, G, and H to make Category 0: The                                     1.0
category structure has a “longer than” relational property :
 if z > y then Category 1 else 0. In the LS+RULE                                           0.9
condition cubes A, B, G, and H in Figure 3a were combined
                                                                                  PROBABILITY CORRECT
to make Category 1 and C, D, E, and F Category 0, where
                                                                                           0.8
Dimension x is the defining dimension (Table 4.) In the
               2
NLS-small-η condition, A, B, E and F were combined to
make Category 1 and the others were combined to make                                       0.7
                                                          2
Category 0. Finally, in the NLS-matched-η condition, A, B,
                                                                                                                                  LS+RP (η2=.400)
G and H of Figure 3b were combined to make Category 1                                      0.6
                                                                                                                                  LS+RULE(η2=.398)
and the others were combined to make Category 0. The                                                                              NLS-small-η2(η2=0)
LS+RP, LS+RULE, and NLS-matched-η2 conditions have                                         0.5
                                                                                                                                  NLS-matched-η2(η2=.399)
the same η value, while the NLS-small-η condition have a
            2                                           2                                                1-40     41-80    81-120   121-160   161-200
much smaller value of η but have the same set of instances
                                 2                                                                                      BLOCKS
as those of the LS+RP and LS+RULE conditions.                                         Figure 4 Results of Experiment 2 no-noise conditions
Noisy Conditions While in no-noise conditions only the                                     1.0
centroids of the cubes were used, in noisy conditions                                                  LS+RP (η2=.365)
                                                                                                       LS+RULE(η2=.365)
random samples from inside the cubes were presented to the                                 0.9         NLS-small-η2(η2=0)
subjects as perturbation of controids. The side length of the
                                                                                  PROBABILITY CORRECT
                                                                                                       NLS-matched-η2(η2=.360)
cube was 60 dots. This procedure is equivalent to adding a
                                                                                           0.8
uniform noise term whose boundary is the cube surface to
the cube centroid.
Procedure The subjects were 128 undergraduate and                                          0.7
graduate students of Waseda University. No subject
participated in Experiment 1. The subjects were randomly                                   0.6
assigned to one of the eight equal-sized experimental
conditions. The general procedure was identical to that of
                                                                                           0.5
Experiment 1 and 25 randomized runs were used to make a                                                  1-40     41-80    81-120   121-160   161-200
total of 200 stimulus presentations.                                                                                    BLOCKS
Results The learning curves for 8 conditions are shown in
                                                                                      Figure 5 Results of Experiment 2 noisy conditions
Figures 4 and 5. Overall they show that the LS categories
                                                                                      than” relational property and those with a defining
were easier to learn than NLS ones even when η2 values
                                                                                      dimension are easier to learn even when η2 values are
were equated.
                                                                                      equated. Furthermore the results were consistent with the
The Analysis of No-noise Condition Logistic regression
                                                                                      idea that η2 affects performance: the two LS categories were
analysis revealed that the variable of category type (LS+RP,
                                                                                      easier to learn than the NLS-small-η2 category (LS+RP and
LS+RULE, NLS-small-η2, and NLS-matched-η2) produced
                                                                                      NLS-small-η2, q(4, ∞)=29.20, P<.01 ; LS+RULE and NLS-
different learning rates (χ2 (3)=452.42, P<.001). Multiple
comparison between the four category types revealed that                              small-η2, q(4,∞)=15.59, P<.01). Interestingly, the difference
all the differences were significant. Because the differences                         between the LS+RP and LS+RULE conditions was also
                                      2
between the NLS-matched-η and LS+RULE conditions                                      significant (q(4,∞)=16.95, P<.01) indicating that the “longer
                                                                                      than” property was more advantageous to category learning
(q(4, ∞)=5.25, p<.01) and between the NLS-matched-η and               2
                                                                                      than the defining dimension in the present experiment. The
LS+RP conditions (q(4, ∞)=21.08, p<.01) were significant,
                                                                                      difference between the LS+RULE and NLS-matched-η2
we obtained evidence that LS categories with a “longer
                                                                                                              was, however, not too compelling. In
 TABLE 4 Numerical Values of Cube Controids
                                                                                                              particular, there was no difference between
           and Category Structures                                                                            them in the third, fourth and fifth blocks
      Category Structure of Figure 3a                       Category Structure of Figure 3b
           Dimensional Values of     Category Structure          Dimensional Values of Category Structure
                                                                                                              (q(20, ∞)=1.63, P>.05 ; q(20, ∞)=1.38,
      Cube Cube Centroid                                    Cube Cube Centroid                                P>.05 ; q(20, ∞)=0, P>.05, respectively)
            x      y       z LS+RP LS+RULE NLS-small-η2           x      y       z NLS-matched-η2             The Analysis of Noisy Conditions The
      A      120     65      76    1     1       1          A       95 120 103 1                              learning curves and statistical results were
      B      101     54 112        1     1       1          B      105     80      97 1
                                                                                                              very similar overall to those of the no-noise
      C       86     69 133        1     0       0          C       86     69 133 0
      D       75 110 139           1     0       0          D       75 110 139 0                              conditions. Logistic regression analysis
      E       80 135 124           0     0       1          E       80 135 124 0                              revealed that the effect of category
      F       99 146         88    0     0       1          F       99 146         88 0                       structure on learning rates was significant
      G      114 131         67    0     1       0          G      120     65      76 1
      H      125     90      61    0     1       0          H      140     75      39 1
                                                                                                              (χ2 (3)=317.38, P<.001) and multiple
                                                                                                              comparison revealed that the differences
      Notes Relational property : if z>y then category 1 else 0 exists in LS+RP                               between the four conditions were all
             x is a defining deminsion in LS+RULE
                                                                          2156

reliable : Again the differences between the NLS-matched-             and rules will promote LS category learning. One reason
η2 and LS+RULE conditions (q(4,∞)=5.15, P<.01) and                    why previous studies failed to demonstrate positive effects
between the NLS-matched-η2 and LS+RP conditions                       of LS is that they used qualitative and/or heterogeneous di-
(q(4,∞)=13.10, P<.01) were both significant. The LS                   mensions; under such situations, LS categories would not be
categories with a “longer than” property and with a defining          able to enjoy the benefits of relational properties and rules.
dimension were easier to learn than the NLS-small-η2                      The generality of the present results should be examined
category (LS+RP and NLS-small-η2, q(4, ∞)=29.05, P<.01 ;              further, paying close attention to the scale type of
LS+RULE and NLS-small-η2, q(4, ∞)=16.60, P<.01 ).                     dimensions, because the use of homogeneous dimensions
Finally, the difference between the LS+RP and LS+RULE                 would have promoted the evaluation of relative lengths of
                                                                      the bars. As in the numerous previous studies, if we had
conditions was again significant ( q(4, ∞)=8.09, P<.01).
                                                                      used heterogeneous and qualitative dimensions instead, the
Joint analysis of noisy and no-noise conditions. Logistic
                                                                      results would have been greatly different. Because there is
regression analysis showed that the effect of category type
                                                                      no logical reason that stimulus dimensions should be
(χ2(3)=627.66, p<.001) and the effect of noise
                                                                      homogeneous, it is safe to say for the present that LS is
( χ2(1)=158.39, p<.001) were both significant.                        advantageous to category learning only when dimensions
Summary and Discussion Essentially, the results showed                are homogeneous and/or directly comparable inducing
that the differences between LS+RP and LS+RULE,                       relational properties and rules. Conversely, because there is
LS+RULE and NLS-matched-η2, and NLS-matched-η2 and                    no necessity that stimulus should be comprised of binary
NLS-small-η2 were all reliable and clearly the LS+RP                  features and in view of the plain fact that many dimensions
condition was the easiest to learn. Therefore we can state the        of natural categories are continuous, further studies
following: (1) We obtained evidence that “longer than”                contrasting binary and continuous dimensions will be
relational properties and defining dimensions, which can be           needed.
derived from LS but never from NLS, both promote
category learning. This will be so even when contrast NLS                                         References
category structures have almost the same η2. (2) At the               Ashby, F.G., & Gott, R.E. (1988). Decision rules in the perception
same time, the results suggested that relational properties            and categorization of multidimensional stimuli. Journal of Experi-
could be more effective than defining dimensions. (3) The              mental Psychology: Learning, Memory, and Cognition, 14, 33–53.
difference between the LS+RULE and NLS-matched-η2                    Ashby, F.G., & Maddox, W.T. (1992). Complex decision rules in
conditions without noise would not be too compelling.                   categorization: contrasting novice and experienced performance.
  These results clearly indicate that some types of LS                  Journal of Experimental Psychology: Human Perception and
category are easier to learn when the four requirements                 Performance, 18, 50–71.
                                                                     Ashby, F.G., & Maddox, W.T. (2005). Human Category Learn-
described at the end of introduction are satisfied. A next
                                                                         ing. Annual Review of Psychology, 56, 149-78.
question is whether this is also the case in the absence of          Blair, M., & Homa, D. (2001). Expanding the search for a linear
such beneficial properties. The results of Experiment 1b and           separability constraint on category learning. Memory & Cognition,
numerous existent studies strongly suggest that the LS                 29 (8), 1153-1164.
advantage will vanish.                                               Kemler-Nelson, D.G. (1984). The effect of intention on what
   In this experiment the “longer than” relational property             concepts are acquired. Journal of Verbal Learning and Verbal
was more effective than the defining dimension, which may               Behavior, 23,734-759.
show that people are more sensitive to emerging relational           Malt, B.C. (1995). Category coherence in cross-cultural pers-
properties than they are to parent dimensions at least under           pective. Cognitive Psychology, 29, 85-148.
                                                                     Medin, D.L., & Schwanenflugel ,P.J. (1981). Linear separability in
some conditions. The apparent fact that bar charts are used
                                                                         classification learning. Journal of Experimental Psychology:
to compare values supports the idea that the bar chart                   Learning, Memory, and Cognition, 8, 37-50.
stimuli would have promoted dimensional comparisons and              Murphy, G.L., & Medin, D. (1985). The role of theories in con-
helped subjects to find relational properties. Another                 ceptual coherence. Psychological Review, 92, 289-316.
interpretation is, because the separation of values on               Nakamura, G.V. (1985). Knowledge-based classification of ill-
Dimension x in the LS+RULE condition was small (there                  defined categories. Memory & Cognition, 13, 377-382.
was only 2 dot difference); the defining dimension did not            Shepard, R.N., & Chang, J.J. (1963). Stimulus generalization in the
have perfect validity especially in the noisy condition in             learning of classifications. Journal of Experimental Psychology,
which random noise on the original dimensional values                  65, 94-102.
                                                                      Smith, J. D., Murray, M. J., & Minda, J. P. (1997). Straight talk
made the difference quite imperceptible.
                                                                       about linear separability. Journal of Experimental Psychology:
                                                                       Learning, Memory, and Cognition, 23, 659-680.
                   General Discussion                                 Wattenmaker, W.D., Dewey, G.I., Murphy, T.D., & Medin, D.L.
   This paper does not claim that LS should be beneficial in           (1986). Linear separability and concept learning : Context, rela-
all situations and thus contributes little to the revival of pro-      tional properties, and concept naturalness. Cognitive Psychology,
totype theory. This paper does deny the assertion, however,            18, 158-194.
that LS has nothing to do with categorization. It is highly            Wattenmaker, W.D. (1995). Knowledge structures and linear
                                                                       separability: integrating information in object and social
plausible that high structural coherence, relational properties,
                                                                       categories. Cognitive Psychology, 28, 274-328.
                                                                 2157

