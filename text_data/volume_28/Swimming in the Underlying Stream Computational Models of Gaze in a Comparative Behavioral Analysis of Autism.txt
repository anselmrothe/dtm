UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Swimming in the Underlying Stream: Computational Models of Gaze in a Comparative
Behavioral Analysis of Autism
Permalink
https://escholarship.org/uc/item/18p5f60j
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Jones, Warren
Klin, Ami
Shic, Frederick
et al.
Publication Date
2006-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

          Swimming in the Underlying Stream: Computational Models of Gaze in a
                                    Comparative Behavioral Analysis of Autism
       Frederick Shic (frederick.shic@yale.edu)                                 Warren Jones (warren.jones@yale.edu)
      Department of Computer Science, 51 Prospect St                          Yale Child Study Center, 230 South Frontage Road
                   New Haven, CT 06511 USA                                               New Haven, CT 06520 USA
               Ami Klin (ami.klin@yale.edu)                                       Brian Scassellati (scaz@cs.yale.edu)
     Yale Child Study Center, 230 South Frontage Road                          Department of Computer Science, 51 Prospect St
                   New Haven, CT 06520 USA                                               New Haven, CT 06511 USA
                               Abstract                                to describe advertisement exploration (Wedel & Pieters,
                                                                       2000). Naturally, highly specialized visual activities, such
   Focal visual attention is a scarce resource. In order to best       as reading, may exhibit very specific patterns of transitional
   utilize this resource, the brain allocates movements of the eye     gaze activity that can be explicitly coded (Rayner, 1998).
   to focus upon locations in the spatiotemporal visual scene that
                                                                       While these various models are capable of describing the
   are maximally informative. How informative a location in the
   visual field is, however, depends on the dynamic internal
                                                                       behaviors observed in specific situations, in natural settings,
   goals and intrinsic preferences of the observer himself. This       or free-viewing tasks, such models typically have much less
   interplay suggests that, by tracking, recording, and modeling       utility- a model for reading, for example, would be useless
   the movements of subjects watching some visual scene, we            in describing the search patterns of an individual who is
   can tap into the underlying stream of human motivation. In          viewing a face.
   this paper we present a framework for the computational                The goal of this paper is to examine computational
   modeling of human gaze and, by instantiating this framework,        models of gaze in natural situations where a goal or task is
   demonstrate how the visual strategies of human subjects can         not given. This is not to say that these are situations in
   be quantified and compared. This comparison is formulated
                                                                       which internal representations do not come into play; on the
   in terms of the subsequent implications of shared and
   unshared strategies in a population of adolescents with autism
                                                                       contrary, the difficulty of this domain is that the individual
   and with matched controls.                                          motivations governing eye fixation transitions exist but are
                                                                       not known a priori. To this end we begin this paper by
                           Introduction                                formulating a general framework for computational models
                                                                       of visual attention. We instantiate this model with one
The eye can only fixate upon one point in the                          particular implementation and use it to analyze the gaze
spatiotemporal scene at a time. Consequently, the dynamics             patterns of individuals with autism as compared to matched
of foveal fixation represent the allocation of a scarce                controls. In contrast to previous work that examined this
resource that reflects the changing internal processes, goals,         same dataset at the highly semantic level of social
and motivations of the human observer (Luck, Hillyard,                 interaction and associated face processing (Klin, Jones,
Mouloua & Hawkins, 1996).                      Though the exact        Schultz, Volkmar & Cohen, 2002), our aim is to examine
mechanisms, purpose, and utility of eye movements is a                 eye patterns from the level of elementary features. We
subject of some debate, that eye movements are in some                 show that such analysis leads to results compatible with
way affected by internal mental processes is not (e.g.                 high-level behavioral interpretations, suggesting that
Hayhoe & Ballard, 2005). Furthermore, though it is                     differences between typical and atypical gaze patterns can
possible to allocate visual attention covertly without an              be captured along a continuum reflecting common
overt shift in gaze fixation, when an overt shift in gaze does         underlying behavioral currents.
occur, it seems to simultaneously demand attention (Deubel
& Schneider, 1995). Thus, the movement of the eyes can
                                                                          A Framework for Computational Models of
serve as a window into the internal motivations of the mind.
    Where a task is given explicitly, models of eye dynamics                                Visual Attention
can be formulated. For instance, models used to describe               Given a representation of the visual scene, computational
saccadic actions in visual search paradigms can be                     models of visual attention determine a point in that scene to
computationally framed in terms of bottom-up features (Itti,           which focal attention is directed. One common framework
Koch & Niebur, 1998; Wolfe, 1994; Wolfe & Gancarz,                     for these models is shown in Figure 1. Models within this
1996). Models that seek to describe higher level tasks, such           framework begin with a representation of the spatiotemporal
as those tasks that require synchronization of physical motor          scene I(s,t) as a function of some spatial coordinate s and
skills, can be framed in terms of reinforcement learning and           temporal index t. This representation is then decomposed,
uncertainty propagation (Sprague & Ballard, 2003). In                  by feature extraction, into a set of features F(s,t) that maps
fields such as marketing, where the goal is maximization of            in many-to-one fashion onto the original spatiotemporal
brand recall or user retention, Markov models may be used              coordinate system. Operating over these features, an
                                                                   780

attentional system converts these features into a saliency         representing that point’s visual prominence. While there
map, S(s,t). Finally, a gaze policy is applied to the saliency     exists some evidence for a saliency map in the brain (Li,
map in order to extract a point, g(t), that corresponds to a       2000), computational models of visual attention typically
location that will actually be fixated upon.            Many       employ saliency maps for computational and organizational
computational models of visual attention (Itti, Niebur, &          reasons and do not assume a direct biological correlate.
Koch, 1998; Brezeal & Scassellati, 1999; Wolfe & Gancarz,             Currently, many different strategies are available for the
1996) obey this formulation.                                       computation of saliency. Most strategies rely upon the
                                                                   feature integration theory of Treisman and Gelade (1980)
         scene            feature          features                which views saliency as the integration of multiple input
                                                                   modality maps, often by linearly weighted summation or
          I(s,t)         extraction          F(s,t)
                                                                   nonlinear transfer of linearly weighted summation
                                                                   (Balkenius, Astrom & Eriksson, 2004; Brezeal &
           gaze           saliency         attention               Scassellati, 1999; Itti et al, 1998; Wolfe & Gancarz, 1996).
          policy            S(s,t)          model                  Others view salience in more theoretical terms. For
                                                                   instance, Itti & Baldi (2006) view the salience of
                                  gaze computation                 spatiotemporal locations in terms of Bayesian “surprise”,
                                                                   and Torralba (2003) characterizes global contextual factors
                      gaze point: g(t)
                                                                   affecting salience in information-theoretic terms. Later in
  Figure 1: A generic framework for computational models           this paper we will present another perspective on saliency
                      of visual attention.                         maps by framing salience as a classification problem on
                                                                   points attended-to by observers and points that are not
Feature Extraction                                                 attended-to.
In computational models of attention, feature extraction is
the process of extracting from the input stream abstract           Gaze Policy
representations or key characteristics relevant to the final       A gaze policy takes the saliency map as input and from it
attentional decision. What exactly comprises the best set of       derives the location where attention should be next directed.
features for guiding visual attention is an open question,         Formally, if the salience at each point in the saliency map is
though much progress has been made, especially in areas            real-valued, we can simply define this point as:
pertaining to visual search (Wolfe & Horowitz, 2004).
Most feature extraction modules, however, choose their                                  g(t) = arg max s (S(s,t))
attributes based on a combination of biological inspiration
and practical concerns. For example, the model of Itti et al.      As with the other steps in our framework, the actual
(1998) uses separate channels for image intensity, edge            implementation of a gaze policy can be more involved,
orientation, and color, where each channel is in turn              incorporating higher order interactions such as inhibition of
composed of even more elementary channels, such as the             return (as in Itti et al., 1998). Furthermore, the actual action
“redness” or “brightness” of points.                               of fixating the eye can involve a change in visual input as
   Note that though the chosen features are processed early        the high-resolution fovea rotates to sample the area at a
in the visual pathway, their computational formulation or          chosen point non-linearly (as in Wolfe & Gancarz, 1996).
characterization can be arbitrarily simple or complex. For         Thus there may exist some level of interaction between the
example, by considering an augmented set of features that          gaze policy and the scene input to the system, completing a
depend upon previously computed internal variables, we can         circuit describing this framework for visual attention.
account for models of selective attention, such as the
selective tuning model of Tsotsos et al. (1995), which                            Comparing Gaze Patterns
incorporates bidirectional excitation and inhibition between
                                                                   One natural metric for judging how well a model performs
the feature extraction module and the attention model. This
                                                                   is to compare it against human subjects (Ouerhani, von
is an important feature, as strictly bottom-up models of
                                                                   Wartburg, Hugli & Muri, 2004; Parkhurst, Law & Niebur,
visual attention adequately represent neither the true
                                                                   2002; Shic & Scassellati, 2006). To do this, the free
neurophysiological underpinnings of visual attention
                                                                   parameters of a model can be tuned so that the model best
(Desimone and Duncan, 1995; Posner & Petersen, 1990) nor
                                                                   describes the gaze allocation behavior of, say, one particular
its computational capabilities and limitations (Tsotsos.
                                                                   individual.
1988).
                                                                      How should the similarity between simulated gaze
                                                                   patterns generated by a computational model and the actual
The Attention Model
                                                                   gaze actions of subjects be measured? A simple measure
The role of the attention model is to convert the elementary       would be to consider the time-varying Euclidean distance
features into a saliency map, an intermediate representation       between two gaze trajectories. Figure 2 demonstrates one of
first proposed by Koch and Ullman (1985) that associates           the problems with this naïve approach.
with each point in the spatiotemporal scene a specific value
                                                               781

                                                                          Both the pure probabilistic formulation using Bayesian
                       A            B           C                      inference and the dimensionality reduction strategy
                                                                       employing Fisher’s linear discriminant are natural methods
                                                                       for tuning computational models of visual attention to the
                                                                       gaze patterns of an individual. Once the model is tuned, the
                                                                       corresponding maps of salience at every point in time and
                                                                       space for that individual are easily generated. We can
  Figure 2: The problem with using Euclidean distance as a             obtain a measure of how well the model fits by examining
 measure for gaze similarity. An individual who focuses on             the salience at locations where the individual actually looks
 B is most likely not using the same scanning strategy as the          in comparison to the salience of the locations that the
person who is focusing on A. In contrast, an individual who            individual does not look.
  focuses on C might be using a very similar gaze strategy.               Once we have a tuned model, however, we are not limited
                                                                       to model-individual comparisons. We can also take this
   A better solution is to phrase the distance between gaze            same model and apply it to other individuals. That is, we
patterns in terms of the similarity of the features underlying         can evaluate how well a particular model, tuned to one
the points of fixation. To do this we require some way of              particular individual, explains the gaze patterns of other
comparing features. This is easily accomplished by viewing             individuals. Furthermore, the results of model cross-
the generation of saliency within the attention model as a             application can be aggregated in order to investigate
classification problem that separates attended-to locations in         population specific trends.
the visual stream from those locations that are not-attended-
to. A Bayesian formulation is:                                                                 Experiment
                                                                       As a test of our framework and comparative techniques, we
                                    p ( f | c i ) p (c i )             apply our methods to the analysis of a population of
                   p (c i | f ) =                                      individuals with autism and matched controls. We know
                                           p( f )
                                                                       that differences in gaze patterns exist between these two
                                                                       groups both qualitatively (Figure 3) and as a result of the
Where f is the underlying set of features associated with a            high-level analysis conducted by Klin et al. (2002) which
particular location, c0 is the attended-to class and c1 = ¬c0 is       showed that individuals with autism, in comparison to
the not-attended-to class.              By transforming this to a      controls, focused more on mouths and objects than on eyes.
classifier that would choose class c0 if p(c0 | f ) > Ө p(c1 | f )     In this work, we are primarily interested in the implications
for some threshold Ө, and would choose class c1 otherwise,             of cross-population and inter-population statistics upon the
we can define the saliency associated with features f to be:           developmental and cognitive deficits inherent in autism.
                                     p( f | c0 )                       Subjects and Data
                        S( f ) =
                                    p ( f | ¬c 0 )                     The data and subjects used in this study were drawn from a
                                                                       subset of the data obtained in Klin et al. (2002). In this
   If we further relax the strict probabilistic interpretation of      experiment, adolescents and young adults diagnosed with
salience, however, we can access a much larger set of                  autism (N=10) were matched with a control group (N=10)
dimensionality reduction techniques. For instance, by                  on the basis of age and verbal-IQ. These individuals
maximizing the Fisher criterion function we can find a                 watched two different one-minute clips of the 1966 black-
projection w that in some sense represents an optimal 1D               and-white movie “Who’s Afraid of Virginia Woolf?” in a
projection for discriminating between attended-to and not-             controlled environment while their eye movements were
attended-to locations. That is we can obtain the solution:             tracked via a head mounted eye-tracker. The movie
                                                                       occupied a width of approximately 34º in the visual field,
                        w = SW −1 (m1 − m2 )                           and the eye tracker was accurate to ±0.3º over a horizontal
                                                                       and vertical range of ±20º. For further information
                           1
           where mi =
                           ci
                               ∑ x , SW        = S1 + S 2 , and        regarding the parameters of data acquisition, subject
                                                                       statistics, and diagnostic criteria, see Klin et al (2002).
                               x∈Ci
                                                                          As a control against computational bias, several synthetic
         Si =  ∑ ( x − mi )( x − mi ) t = ( c i − 1)∑ i = ki ∑ i       gaze trajectories were also incorporated into the experiment.
              x∈Ci
                                                                       These gaze trajectories were uncorrelated with the visual
                                                                       scene and included (i) random filters (random weight
(for reference see Duda and Hart, 2001). In this manner we             matrices in the Fisher’s linear discriminant formulation) (ii)
avoid having to estimate p( f | c ), a task that can be quite          random saccades (a sequence of fast jumps across the screen
difficult in high dimensions, and can require more                     triggered probabilistically) and (iii) random walks (small
complicated approximation techniques even in lower                     movements across the screen every frame).
dimensions.
                                                                   782

                                                                       with the covariance matrix was taken to be equal for all
                                                                       classes. Training of models occurred over odd frames of
                                                                       one particular clip, allowing for testing over the highly-
                                                                       correlated even frames of the same clip, as well as an
                                                                       independent comparison on a completely different clip.
                                                                       Comparative Method
                                                                       Our computational framework provides a method for
                                                                       determining, for some particular individual, the saliency of
                                                                       every spatiotemporal point in the visual scene. If we thus
                                                                       generate a model for an individual A, we can see how well
                                                                       our techniques work by examining the reported saliencies at
 Figure 3: Eye scanning paths of controls (solid lines from            the locations of A’s gaze (Figure 4). If our techniques are
circles) as compared to individuals with autism (dotted lines          good, the average saliency at the locations where A fixates
   from squares) on a scene from the 1966 movie “Who’s                 should be high. Furthermore, we can take A’s model and
     Afraid of Virginia Woolf?” (Klin et al., 2002). The               look at the locations where another individual, B, looks.
instantaneous fixation point is the circle or square and each          This gives us a measure of how well the model of A
path stretches 250 ms into the future. The gaze locations of           describes the gaze trajectories of B, leading to a natural
     controls are clustered on the left-most face; the gaze            measure for the distance between the two individuals.
      locations of individuals with autism are scattered.
                                                                           gaze saliency rank percentile
                                                                                                            1
Computational Model
                                                                                                           0.8
Feature Extraction – The features used in this experiment
consisted of a linearization of raw patch features drawn                                                   0.6
from points in history. That is, points of eye fixation
corresponding to attended-to locations (and 15 randomly                                                    0.4
selected points at least 2.9º distant from the actual gaze
point for not-attended-to locations) were considered the                                                   0.2
                                                                                                              20   22   24          26   28   30
center of a square area which was further subdivided                                                                         time (s)
spatially into a uniform grid of subblocks. Each subblock
within the grid was taken to be representative of the
underlying spatial content by averaging (i.e. the subblock
represented the corresponding region by a single average
intensity), and the set of all subblocks associated with
selected points in time prior to the fixation constituted the
features associated with an attended-to location. The entire
grid spanned approximately 9.3º and was divided into 11x11
subblocks, sampled at 100ms and 300ms in the past.
Temporal sampling was necessary to allow for motion                     Figure 4: Time-varying salience record representing how
encoding, as the scene was time-varying. Though this                    well a particular model, tuned to one individual, describes
feature set was not completely physiological, being coarser            the gaze behavior of another individual. In this example the
in sampling and larger in extent than the fovea, its simple            yellow crosses correspond to locations actually fixated upon
expression struck a useful tradeoff between spatiotemporal                 by an individual. These locations are associated with
extent and computational expedience. Several other feature                particular salience values. When the actual fixation is
sets were also tested, including both a multiscale                     adequately represented by the model, salience is high (close
representation as well as the more complex biologically-                    to 1.0); when not, salience is close to chance (0.5).
inspired model of Itti et al. (1998). Neither the use of these
other feature sets, nor the variation of their associated                 In order to maintain consistency and comparability across
parameters within a wide range, impacted the nature of our             all frames in the movies and all individuals we first
final results.      Further details on varying feature                 normalized the saliency values in each frame to a rank
representations can be found in Shic & Scassellati (2006).             percentile. That is, if a particular spatial location in some
                                                                       given frame was the 95th highest percentile (most salient)
Attention Model – Saliency maps were generated by using                location, it was normalized to a value of 0.95 regardless of
the method of dimensionality reduction via projection of               what its actual projected value was. This reflected the fact
features upon Fisher’s linear discriminant. To compensate              that fixation is a relative and not an absolute decision
for the much larger sampling of non-attended-to locations              process. Next, the gaze patterns of a particular individual
versus attended-to locations, the coefficient k associated             were indexed into the salience map generated by another
                                                                 783

individual. From this we were able to obtain time-varying                                       trajectory lead to performance much better than those
salience records (Figure 4). Finally, in order to obtain an                                     obtained by random chance, as developed by synthetic gaze
overall score representing how well a model matched an                                          trajectories (Figure 5 & 6; p<0.01). This suggests that both
individual, the median salience value from the time-varying                                     individuals with autism and control individuals rely on some
salience record was taken as representative. This provided a                                    common scanning approach, implying the existence some
robust measure of average model effectiveness.                                                  core human strategy. Furthermore, this result suggests that
                                                                                                it is unlikely that a methodological bias exists in either the
Results                                                                                         learning technique or the feature representation.
By applying models tuned for each trajectory (both human                                           Second, the extremely high matched-application (control
and synthetic operating over two movie clips) to every other                                    on self and autism on self groupings) within-movie scores
trajectory in our data set, we were able to obtain a large                                      (Figure 5) suggest that each subject relies upon some
number of cross-trajectory comparisons. By aggregating the                                      specific individual strategy.         This specific individual
data into groups we obtained the statistics of Figure 5 & 6.                                    strategy does not seem to transfer across scenes, as
                                                                                                demonstrated by matched comparison score drops as we
                                                   93%       91%       83%      78%
                                            100                                                 move from within-movie comparisons to across-movie
                    Median Saliency Rank%
                                                                                                comparisons, suggesting that top-down or contextual
                                             90
                                                                                                influences on gaze strategy are significant.
                                             80                                                    Third, as highlighted by Figure 6, control individuals,
                                                                                                who are taken to be socially more typical than individuals
                                             70
                                                                                                with autism, exhibit much greater coherence (p<0.01) in
                                             60                                                 terms of attraction to underlying features than cross-
                                                                                                application cases that involve individuals with autism. This
                                             50
                                                                                                suggests that the strategies of controls transfer well to other
        Model of                                  Control   Autism    Control   Autism
                                                                                                controls, but that the strategies of individuals with autism do
        Gaze of                                     Self     Self      Self      Self
                                                                                                not transfer to the same degree to either normal individuals
          Movie                                    Same     Same       Diff      Diff
                                                                                                or even other individuals with autism.
  Figure 5: Self-tuning comparisons across movies. Results
 are aggregated (N=10 in each condition) for models trained
                                                                                                                        Discussion
on one individual (control or autism) and tested on the gaze
  patterns of that same individual (watching either the same                                    The original Klin et al. (2002) study found that individuals
  movie or a different movie). When a model is trained on                                       with autism spent more time focusing on mouths, bodies,
  one movie and applied to another movie, we get a drop in                                      and objects, whereas controls spent significantly more time
performance. However, in all cases, human models describe                                       looking at eyes. In terms of elementary features, eyes vary
    the gaze of other humans much better than random as                                         the least; objects vary the most. Thus our results in this
  determined theoretically (50%) and empirically (52±13%,                                       paper could derive specifically from this disparity. If eyes
      N=600). Error bars span two standard deviations.                                          vary the least, and controls focus on eyes much more often
                                                                                                than individuals with autism (the difference between eye
                                                   82%       76%       77%       75%
                                                                                                fixation time fractions between the two populations exceeds
                                            100                                                 40%), we would expect a higher correspondence among
          Median Saliency Rank%
                                                                                                control individuals. Similarly, if features associated with
                                             90
                                                                                                bodies and objects vary most, we would expect individuals
                                             80                                                 with autism to exhibit fine tuned strategies specific to
                                                                                                particular objects or image characteristics not generally
                                             70                                                 found elsewhere. If these strategies are extremely fine
                                             60
                                                                                                tuned, they cannot transfer to other individuals.
                                                                                                   The disadvantage of a featural level analysis, compared to
                                             50                                                 higher-level considerations, is that much of the internal
       Model of                                   Control   Control   Autism     Autism         circuitry of low-level models is impenetrable. For instance,
       Gaze of
                                                   Other
                                                            Autism    Control
                                                                                 Other          we can frame the results obtained in our results in terms of
                                                  Control                        Autism
                 N                                  90       100        100        90
                                                                                                semantic labels associated with subject fixation. However,
                                                                                                the converse, predicting high level implications from low
 Figure 6: Cross-tuning comparisons within the same movie
                                                                                                level aggregate effects, could prove very difficult. On the
 clip. Models for the gaze of controls describe the gaze of
                                                                                                other hand, since we do have as many time-varying salience
     other controls better than the any cross-population
                                                                                                records as we have comparisons, it is possible that by
 comparison that involves autism, including autism models
                                                                                                pinpointing locations of mutually high salience we could
    applied to the gaze of other individuals with autism.
                                                                                                discover classes of highly correlated specific gaze behavior.
                                                                                                The use of our comparative techniques as an exploratory
  The application of our framework leads to several results.
                                                                                                tool in this manner remains to be investigated.
First, all applications of a human’s model to a human’s gaze
                                                                                          784

   The advantage of featural level analysis is that preexisting     Itti L., Koch C., and Niebur E. (1998). A model of saliency-
labels with associated semantic implications are not                   based visual attention for rapid scene analysis. IEEE
assumed.        If the underlying featural representation              Trans. Pattern Anal. and Mach. Intel., 20(11), 1254-1259.
associated with a particular computational model of visual          Itti, L. & Baldi, P. (2006). Bayesian Surprise Attracts
attention is sufficient to represent some common underlying            Human Attention. In Adv. Neural Information Processing
strategy within a population, our techniques should uncover            Systems (pp. 1-8). Cambridge, MA: MIT Press.
this fact. In this investigation we have uncovered two tiers        Klin, A., Jones, W., Schultz, R., Volkmar, F. & Cohen, D.
of shared strategies. The first tier represents the underlying         (2002). Visual fixation patterns during viewing of
gaze patterns associated with the scanning behavior of all             naturalistic social situations as predictors of social
humans, mechanisms likely hardwired into the early visual              competence in individuals with autism. Archives of
                                                                       General Psychiatry, 59(9), 809-816.
system. The second tier is found between controls, likely
                                                                    Klin, A., Jones, W., Schultz, R. & Volkmar, F. (2003). The
representing typical development versus early derailment as
                                                                       enactive mind, or from actions to cognition: lessons from
predicted by enactive mind theory (Klin, Jones, Schultz, and           autism. Phil. Trans. of the Royal Society of London,
Volkmar, 2003). Finally, the ability for models to match               Series B, Biological Sciences, 358(1430), 345–360.
specific individual preferences suggests that order does exist      Koch, C., & Ullman, S. (1985). Shifts in selective visual
in the gaze patterns of individuals with autism, suggesting            attention: towards the underlying neural circuitry. Human
that when early derailment of social skill development                 Neurobiology, 4, 219–227.
occurs it is replaced by some other set of visual behavior          Li, Z. (2002). A salience map in primary visual cortex.
that likely reflects a unique cascading specialization.                Trends Cogn. Sci., 6, 9–16.
                                                                    Ouerhani, N., von Wartburg, R., Hughli, H., Muri, R.
                         Conclusions                                   (2004). Empirical Validation of the Saliency-based model
We have presented a general framework for visual attention,            of Visual Attention. Electronic Letters on Computer
realized one particular implementation of this framework,              Vision and Image Analysis, 3(1), 13-24.
and applied the resulting model to analyze the eye scanning         Parkhurst, D., Law, K. & Niebur, E. (2002). Modeling the
trajectories of individuals with autism and matched controls.          role of salience in the allocation of overt visual attention.
We show that such feature level analysis offers a wealth of            Vision Research, 42(1), 107-123.
insight into the fundamental behaviors and preferences of           Posner, M. I. & Petersen, S.E. (1990). The attention system
subject populations, and that these uncovered insights are             of the human brain., Ann. Rev Neurosci. 13, 25-42.
consistent with higher-level analysis. Future avenues for           Rayner, K. (1998). Eye movements in reading and
investigation include augmented sets of features such as               information processing: 20 years of research.
scanpath memory via Markov model, application of various               Psychological Bulletin, 124(3), 372-422.
dimensionality reduction techniques, and the use of our             Shic, F. & Scassellati, B. (2006). A behavioral analysis of
comparative method as an exploratory tool for decomposing              robotic models of visual attention. Under review.
the global scanning behavior into component elements.               Sprague, N. and Ballard, D. (2003) Eye movements for
                                                                       reward maximization. In Advances in Neural Information
                                                                       Processing Systems, Vol. 16, MIT Press.
                          References                                Torralba, A. (2003) Modeling global scene factors in
Balkenius, C., Eriksson, A. P. & Astrom, K. (2004).                    attention. J. Opt. Soc. Am. A Opt. Image Sci. Vis. 20,
   Learning in Visual Attention. In Proceedings of LAVS                1407–1418.
   ’04. St Catharine’s College, Cambridge, UK.                      Treisman, A. M. & Gelade, G. (1980). A feature-integration
Breazeal, C. and Scassellati, B. (1999). A Context-                    theory of attention. Cognitive Psychology, 12(1), 97-136.
   Dependent Attention System for a Social Robot. In T.             Tsotsos, J. K. (1988). A ‘complexity level’ analysis of
   Dean, (Ed.), Proc. 16th Intl. Conf. Artificial Intel., 1146-        immediate vision. Intl. Journal Comp. Vis., 1(4):303–320.
   1153. Morgan Kaufmann Publishers, San Francisco, CA.             Tsotsos, J. K., Culhane, S. M., Wai, W. Y. K., Lai, Y.,
Desimone, R. & Duncan, J. (1995). Neural mechanisms of                 Davis, N., Nuflo, F. (1995). Modeling visual attention via
   selective visual attention. Ann. Rev. Neurosci, 18,193–222          selective tuning. Artificial Intelligence, 78, 507-545.
Deubel, H. & Schneider, W.X. (1996). Saccade target                 Wedel, M., Rik, P. (2000), Eye fixations on
   selection and object recognition: evidence for a common             advertisementsand memory for brands: A model and
   attentional mechanism. Vis. Research, 36(12),1827–1837.             findings. Marketing Science, 19(4), 297–312.
Duda, R., Hart, P., & Stork, D. (2001). Pattern                     Wolfe, J. M. (1994). Guided Search 2.0: A revised model of
   Classification. New York: Wiley.                                    visual search. Psychonomic Bull. and Rev., 1(2), 202-238.
Hayhoe, M. & Ballard, D. (2005). Eye Movements in                   Wolfe, J. M., & Gancarz, G. (1996). Guided Search 3.0: A
   natural behavior. Trends in Cog. Sci. 9(4), 188-194.                model of visual search catches up with Jay Enoch 40
Luck, S. J., Hillyard, S. A., Mouloua, M., & Hawkins, H. L.            years later. In V. Lakshminarayanan (Ed.) Basic and
   (1996). Mechanisms of visual-spatial attention: Resource            Clinical Applications of Vision Science, Dordrecht,
   allocation or uncertainty reduction? Journal Exp. Psych.:           Netherlands: Kluwer Academic.
   Human Perception and Performance, 22, 725-737.                   Wolfe, J.M. & Horowitz, T.S. (2004). Opinion: What
                                                                       attributes guide the deployment of visual attention and
                                                                       how do they do it? Nature rev., Neuroscience, 5(6), 495.
                                                                785

