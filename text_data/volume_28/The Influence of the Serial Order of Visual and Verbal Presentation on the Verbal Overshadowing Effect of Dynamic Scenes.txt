UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Influence of the Serial Order of Visual and Verbal Presentation on the Verbal
Overshadowing Effect of Dynamic Scenes

Permalink
https://escholarship.org/uc/item/5jx309rc

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)

Authors
Garsoffky, Barbel
Huff, Markus
Schwan, Stephan

Publication Date
2006-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

The Influence of the Serial Order of Visual and Verbal Presentation on
the Verbal Overshadowing Effect of Dynamic Scenes
Markus Huff (m.huff@iwm-kmrc.de)
Bärbel Garsoffky (b.garsoffky@iwm-kmrc.de)
Stephan Schwan (s.schwan@iwm-kmrc.de)
Cybermedia Research Unit, Knowledge Media Research Center
Konrad-Adenauer-Strasse 40, 72072 Tübingen, Germany

Abstract
This article investigates the influence of verbalization
processes on the visual recognition performance of dynamic scenes. In the recognition phase different distractor items were used: event model incompatible
and event model compatible. It was hypothesized that
source confusion, which is said to be responsible for inducing the verbal overshadowing effect, is reduced with
event model incompatibility. In two experiments people
viewed a dynamic scene and read a verbal summary related with the scene. The first experiment showed a verbal overshadowing effect when the verbal summary was
presented after the film. In the second experiment – the
verbal summary was shown before the film – recognition
performance improved with event model incompatibility. Source confusion could be eliminated by changing
the order of visual and verbal presentation.

Verbal Overshadowing
The question whether verbally describing a visual stimulus fosters or hinders its subsequent recognition has got
a long tradition in cognitive psychology. Read (1979)
showed that face recognition performance was improved
using verbalization. More recently verbal overshadowing has come in the focus of research. Schooler and
Engstler-Schooler (1990) examined the role of verbalization processes. In the verbal overshadowing paradigm,
subjects first see a visual stimulus (e.g. a face) and then
have to describe this stimulus verbally. Typically, in a
subsequent recognition test, their recognition accuracy
is lower (Schooler, 2002). In a meta analysis Meissner
and Brigham (2001) analyzed 29 studies examining the
verbal overshadowing phenomenon and found a negative
effect of verbalization processes on the recognition performance. While active verbalization induces a transfer
inappropriate processing shift, that dampens individuals’ ability to apply certain non-verbal operations, Dodson, Johnson, and Schooler (1997) evinced that a verbal
overshadowing effect (VOE) even appears if participants
read a verbal description passively. As recognition performance did not show a VOE when participants were
asked to ignore their verbal representation, source confusion about the validity of which mental representation
should be the basis for the recognition was made accountable to this kind of VOE.
Although verbal overshadowing is a robust effect it
could be shown that it disappeared under certain circumstances. Kitagami, Sato, and Yoshikawa (2002) found a
1539

VOE when distractor items were designed highly similar to the target items, while no verbal overshadowing effect appeared at low similarity. In a former study
Bartlett, Till, and Levy (1980) found, using realistic photos as stimulus material, that verbalization led to higher
recognition performance, when it enabled the participants to distinguish between target and distractor items.
These findings are in accordance with the assumption
that viewers develop a kind of model which specifies the
characteristic and relevant features of the visual stimulus. This model enables the viewers to distinguish between target and distractor items in a recognition paradigm. On the one hand, model incompatible distractor
items are easier to identify and on the other hand, model
compatible distractor items are less likely identified. Additionally, it can be assumed that models directly derived
from the visual stimulus contain more detailed information than models derived from the verbal description of
the visual stimulus.

Research questions
The considerations above suggest that VOE is not a
general effect, but appears only under certain conditions. In order to specify these conditions more precisely,
two experiments were conducted, which were based on
the following research questions. So far, VOE research
has addressed the verbalization of static entities like
the appearance of a face. In everyday life such entities
are seldom verbally described and therefore little practiced. Unlike dynamic scenes, in sporting reports it is
quite common to describe spatial relationships verbally.
Hence, the first question focused on the domain: Is there
a VOE within the domain of dynamic scenes, which are
commonly accompanied by verbalizations (e.g. sports
events)? Second, does the VOE influence recognition
performance not only in quantitative, but also in qualitative terms? Regarding the spatial properties, it was
shown that recognition of dynamic scenes is viewpoint
dependent (Garsoffky, Schwan, & Hesse, 2002). A verbal
summary which contains no spatial information (e.g. left
or right) could lead to a less viewpoint dependent visual
recognition performance, thereby indicating a qualitative influence of VOE. Third, it was assumed that event
model compatibility had an impact on the VOE. While
distractor items being designed in line with the verbal
description (event model compatibility) should be difficult to identify, distractor items which violate the verbal

Figure 1: Stimulus material. a) is showing balls moving away from the observer, b) is showing balls moving
towards the observer

Figure 2: Stimulus material with cameras indicating
the viewpoints from which the recognition items were
recorded.

summary (event model incompatibility) should be easily
identified. Event model incompatibility should reduce
source confusion and consequently the VOE. The VOE
after reading the description of a previously seen visual
stimulus can be explained by source confusion which is
induced by two competing representations. First, a very
detailed one, derived from the visual input, second a less
detailed one, derived via visualization processes (Intraub
& Hoffman, 1992) from the verbal description. Therefore as a fourth research question, it was supposed that
the serial order of visual and verbal presentation had
an impact to the VOE. If the serial order of the stimulus presentation is switched (first the verbal and then
the visual presentation) the participants will first visualize a raw sketch which can be enriched with details
while watching the visual presentation. A VOE was expected in the classical condition (visual before verbal)
and it was expected to disappear if the serial order was
inverted (verbal before visual).

Experiment 1

The first independent variable was the verbalization,
realized as a blocked within-design. In the film only condition the participants received no additional verbal information. In the film + verbal condition, the participants received a short verbal summary after watching
the film. And in the verbal only condition they received
solely the verbal summary. The verbal summary described the behavior of one ball, containing its starting
position, its passings (in relation to the other balls) and
its finishing position, for example: ”The red ball starts
behind all other balls, passes every other ball and finally
wins.” The verbal summary was supposed to establish an
event model. It was supposed that the event model derived from the verbal summary contained very abstract
information about the dynamic scene.
The second independent variable was the event model
compatibility. It was realized by using the distractor
items in the recognition test. Two types of distractor
items were created: event model compatible items and
event model incompatible items. The event model incompatible items violated the event model. In the example above, a possible event model incompatible distractor
item could be created in moving the blue ball toward the
finishing line, in that case it became impossible for the
red ball to win this race. In contrast, the event model
compatible items did not violate the event model. In the
example, the yellow ball was moved. This modification
is in line with the event model, that the red ball starts
in the rearest position, passes all the other balls and finally wins the race. As third independent variable the
deviation between the viewpoint in the learning and the
testing phase of the video stills in the recognition phase
was varied. The deviation was 0◦ , 45 ◦ , 90◦ and 135◦
(see figure 2). And finally as fourth independent variable the recognition items were recorded at 4 different
points of time during the scene (3.0, 4.5, 6.0 and 7.5 s.).
This variable was introduced to measure several points
over the length of time of the dynamic scene on the one
hand, and also to collect enough datapoints for analysis.
Procedure All participants were tested individually
and received the instruction via computer monitor. They
got a description of the kind of dynamic scenes used,
the verbalization conditions and the subsequent recognition test. After that they passed a training phase in
which every condition was presented. These data were
excluded from the analysis. The following experimental

The first experiment, in which the order of the presentation was the same like in the classical VOE studies, was
conducted to examine the research questions 1, 2 and 3.

Method
Participants Subjects were 18 students (11 female, 7
male) of the University of Tübingen, Germany. Average
age was 23 years. They were paid for their participation.
Apparatus The experimental procedures were controlled by a Microsoft computer and programmed using
MediaLab and directRT. Video clips and video stills were
presented on a black background in the middle of a 17”
CRT-Monitor.
Stimulus material and design As stimulus material a kind of race1 consisting of four balls moving on
parallel laps toward a white line were created using 3ds
max 6. Every ball had a different starting position and a
different movement characteristic (constant velocity, accelerating or decelerating). Each race lasted 8 seconds
and was rendered either in that way, that the balls were
moving away from the observer or that the balls were
moving toward the observer (see figure 1).
1
Examples of the stimulus materials are available at
http://www.iwm-kmrc.de/cybermedia/cs06-voe/

1540

phase consisted of three blocks: one block for every verbalization condition (film only vs. verbal only vs. film +
verbal). These blocks were presented in a balanced manner. Between the blocks a rest period of 5 minutes was
established to avoid exhaustion and possible carry-over
effects.
In the film only condition the participants were shown
a dynamic scene twice. After the film the participants
were shown a progress bar for the duration of 10 seconds. In the film + verbal condition the participants
were shown a film twice before they read a verbal summary of the dynamic scene, which appeared for 10 seconds on the monitor. Instead of the film, in the verbal
only condition the participants viewed a progress bar
for 17 seconds. Afterwards a verbal summary of the dynamic scene appeared for 10 seconds.
In all conditions, before the recognition test, a video
still, depicting the previously seen scene was shown to
indicate the beginning of the recognition test, which included 48 video stills: 16 target items showing the original scene from 4 different viewpoints at 4 different points
in time. 16 event model incompatible distractor items,
showing a distractor scene (also 4 different viewpoints
and 4 different points in time) and 16 event model compatible distractor items (4 different viewpoint deviations
and 4 different points in time as well).
Taken together a 3 (verbalization) x 2 (event model
compatibility) x 4 (viewpoint deviation) x 4 (point of
time) design was realized.

Results
To compute the sensitivity measure A0 (Pollack & Norman, 1964), the mean hit rate (yes-answers to target
items) and the mean false-alarm rate (yes-answers to distractor items) for the event model incompatible and the
event model compatible distractor items for every condition were calculated.
Across all conditions and participants a mean of .697
for A0 was calculated. An ANOVA with repeated measurement was calculated including the independent variables verbalization (film vs. verbal vs. film + verbal),
event model (incompatible vs. compatible), viewpoint
deviation (0◦ vs. 45◦ vs. 90◦ vs. 135◦ ) and point of
time (3.0 vs. 4.5 vs. 6.0 vs. 7.5 s. after the beginning of the scene). A significant main effect for verbalization was found (F (2, 34) = 13.588, M SE = 0.313,
p < .001, ηp2 = .4442 ). In the film only condition an A0
measure of .782 was observed. In the film + verbal condition A0 was .700 and in the verbal only condition it was
.601. Single comparisons according to Scheffé revealed
significant differences between all of the three conditions
(p < .01). A significant main effect for event model was
found (F (1, 17) = 28.973, M SE = 0.403, p < .001,
ηp2 = .630). Event model incompatibility led to higher
recognition performance (A0 = .779) than event model
compatibility (A0 = .615). There was no main effect for
2

In the reported experiments the partial η 2 (ηp2 ) as effect size measure is reported, because it is more appropriate
to the the design with more than one independent variable
(Tabachnick & Fidell, 1989).

Figure 3: The interaction of verbalization and event
model. Error bars indicating the standard error of the
mean.
viewpoint deviation(F < 1). The main effect for point
of time was significant (A0 for 1: .636, 2: .678, 3: .733,
4: .743; F (3, 51) = 14.506, M SE = 0.07412, p < .001,
ηp2 = .460). For this effect there was a significant linear trend indicating a recency effect (F (1, 17) = 27.975,
M SE = 0.109, p < .001, ηp2 = .622).
The interaction between verbalization and event model
compatibility was significant (F (2, 34) = 10.366, M SE =
0.272, p < .001, ηp2 = .379). Single comparisons according to Scheffé revealed the following significant differences (p < .01). At event model incompatibility there
was a significant difference between the film only (.816)
and the film + verbal condition (.750). For event model
compatible distractor items significant differences were
found between all conditions. Single comparisons regarding the event model compatibility within the verbalization conditions revealed, that in every verbalization condition, event model incompatibility led to higher
recognition rates (p < .01). See figure 3. A planned comparison of the predicted difference between the film only
and the film + verbal condition regarding event model
compatibility revealed no difference; there was no significant interaction between verbalization (film only vs. film
+ verbal) and event model compatibility (F < 1).
The interaction between event model compatibility and
viewpoint deviation was significant (F (3, 51) = 3.086,
M SE = 0.01584, p < .05, ηp2 = .154). Post hoc
analysis (Scheffé) revealed that there were no differences
between the different viewpoint deviations both within
event model incompatibility and event model compatibility, p < .01 (see table 1).
The interaction between event model compatibility
and point of time was significant (F (3, 51) = 4.714,
M SE = 0.032522, p < .01, ηp2 = .217). Post hoc analysis
showed the following result pattern (see table 2). Event
model incompatibility led to lower recognition performance at the first point of time than at the other points
of time. There were no differences between point of time

1541

holds for dynamic scenes. Recognition performance was
lower in the film + verbal condition than in the film only
condition. In the control condition, in which the participants only read a verbal summary of the film, recognition performance was lowest. This finding suggests,
that verbalization shows the same impacts on dynamic
scenes, describing spatial relationships as on static material. Although former are more often accompanied by
verbal descriptions.
Secondly, it was assumed, that a so called event model
describing the similarity of the verbal summary and the
distractor items, could help the participants to distinguish between target and distractor items (research questions 1 and 3). As expected, there was a significant interaction between the verbalization condition and the
event model (research question 3). But the results do
not show the expected absence of the VOE for event
model incompatibility, although the event model enabled
the participants in the verbal only condition to perform
above chance level. A possible explanation could be that
source confusion (Dodson et al., 1997) was not completely reduced by this manipulation and the participants still were not able to apply the appropriate representation during the recognition test. Unlike Kitagami
et al. (2002), the similarity of distractor items with target items seemed not to be important for this kind of
VOE.
There is no indication that VOE led to a change of
the qualitative features of the recognition performance in
this experiment (research question 2). Neither in the film
only condition nor in the conditions containing the verbal summary a viewpoint deviation effect was observed.
Therefore this effect is not part of the discussion in this
paper.
The assumption that visualization processes occur after reading the verbal summary is supported by the fact,
that participants are able to perform above chance level
in a visual recognition test in the verbal only condition
(Intraub & Hoffman, 1992, reported similar effects).
In sum, results from the film only condition indicate,
that participants did not need the verbal summary to
develop an event model of a dynamic scene – in the
film only condition, event model incompatible distractor items were more often identified than event model
compatible items. The VOE in the film + verbal condition can be explained as follows: there were two competing representations, a detailed one, derived from the
visual input and a second one, which was more abstract,
derived from the verbal input. Source confusion now occurred because the latter could not be integrated into the
former one. That is, because participants had already
formed an event model from the visual input, which is
not necessary congruent with the event model induced
by the verbal summary. Therefore two representations
existed, which led to source confusion and consequently
to the observed VOE.

Table 1: Interaction between viewpoint deviation and
event model compatibility (sensitivity measure A0 ).
event model
incompatible compatible
viewpoint 0◦
.795
.611
deviation 45◦
.772
.601
90◦
.770
.636
135◦ .781
.613

2, 3 and 4. Event model compatibility led to a significant
difference between the first two and the last two points
of time (p < .01).
There was also a significant interaction between verbalization condition and point of time (F (6, 102) =
4.681, M SE = 0.06415, p < .001, ηp2 = .216, see table 2). While there were no differences in the film +
verbal condition over the different points of time, there
were several differences in the verbal only and the film
only conditions: In the film only condition there were
significant differences between point of time 1 and 3 and
4 as well as between point of time 2 and 3. In the verbal only condition there were significant differences between points of time 1 and 3 and 4 and between 2 and 4
(p < .01).
There was a significant interaction between verbalization condition, event model compatibility and point of
time (F (6, 102) = 11.104, M SE = 0.04167, p < .001,
ηp2 = .395). Single comparisons according to Scheffé
revealed the following result pattern. Event model incompatibility led to no significant differences across the
different points of time solely in the film + verbal and the
film only condition. In the verbal only condition there
was lower recognition performance at point of time 1
than at all other points of time. Event model compatibility again led to no differences across the different points
of time in the film + verbal condition. In the film only
condition lower recognition performance was observed at
point of time 1 than at point of time 3. In the verbal only
condition there was lower performance at point of time
2 than on the fourth one (p < .01, see table 2).
Table 2: Interaction between verbalization condition,
event model and point of time (sensitivity measure A0 ).
verbalization condition
film only
verbal only film + verbal
event model compatibility
yes
no
yes
no
yes
no
point 1 .760 .614 .607 .474 .760 .599
of
2 .792 .748 .781 .347 .784 .615
time
3 .872 .869 .837 .416 .739 .663
4 .840 .758 .863 .553 .717 .725

Discussion

Experiment 2

One main focus of this experiment was the question
whether the verbal overshadowing phenomenon also

In contrast to the first experiment, in experiment 2 the
order of the visual presentation and the verbal summary

1542

was changed. The verbal summary was presented before the visual presentation. When the verbal summary
was presented before the visual presentation it was expected that no source confusion occurred, because only
one mental representation of the event was constructed
by the participants: they first visualize a raw sketch including a kind of event model. This representation, containing no detailed information about the scene, can be
enriched with more details while watching the film. It
was hypothesized, that a verbal summary, presented before the video clip increased recognition performance,
when it contained information which was relevant to
identify event model incompatible distractor items. This
is because the event model derived from the verbal summary should direct the attention of the participants to
relevant parts of the scenes.

Figure 4: The interaction of verbalization and event
model. Error bars indicating the standard error of the
mean.

Method
Participants Subjects were 18 students (15 female, 3
male) of the University of Tübingen, Germany. Average
age was 23 years. They were paid for their participation.
Apparatus, stimulus material and design The
same setting, stimuli and design as in the first experiment were used.
Procedure The procedure was almost the same as in
the first experiment, apart from the order of the video
clip and verbal summary. The film only condition was
adapted in that way, that after watching the film twice
(17 seconds) the participants were asked to start the
recognition test by pressing the space bar. In the verbal
only condition the verbal summary was presented. After
reading the text the participants had to press the space
bar, then a progress bar appeared for 17 seconds. Again
the recognition test started after pressing the space bar.
In the verbal + film condition the participants first read
a verbal summary of the film they were shown after
pressing the space bar. Right after the film clips (which
lasted 17 seconds) the participants were asked to start
the recognition test by pressing the space bar.

compatibility lead to higher sensitivity measures (.792)
than event model compatibility (.668). There was no
main effect for viewpoint deviation (F < 1). The
main effect for point of time was significant (A0 for 1:
.683, 2: .710, 3: .739, 4: .789; F (3, 51) = 19.969,
M SE = 0.05335, p < .001, ηp2 = .495). For this effect
there was a significant linear trend indicating a recency
effect (F (1, 17) = 35.172, M SE = 0.07695, p < .001,
ηp2 = .665).
The interaction between verbalization and event model
compatibility was significant (F (2, 34) = 5.673, M SE =
0.133, p < .01, ηp2 = .250). Single comparisons according to Scheffé showed following significant differences
(p < .01). Event model incompatibility led to higher
sensitivity measures in the verbal + film condition than
in both the film only and verbal only conditions. However, no difference between the film and verbal + film
conditions was observed during event model compatibility, just the verbal only condition there was lower performance than in the other two conditions (see figure
4).

Results
As in experiment 1, A0 as dependent variable was calculated. Across all participants and conditions a mean
sensitivity rate of A0 = .730 was measured. An ANOVA
with repeated measurement was conducted using the independent variables verbalization (film vs. verbal vs.
verbal + film), event model (incompatible vs. compatible), viewpoint deviation (0◦ vs. 45◦ vs. 90◦ vs.
135◦ ) and point of time (3.0 vs. 4.5 vs. 6.0 vs. 7.5
sec. after the beginning of the scene). There was a significant main effect for verbalization (F (2, 34) = 7.605,
M SE = 0.214, p < .05, ηp2 = .309). Signal comparisons with the Scheffé procedure revealed significant differences between all conditions. In the verbal only condition the sensitivity measure was .671, in the film only
condition .744 and in the verbal + film condition .775.
As in experiment 1 there was also a significant main effect for event model compatibility (F (2, 34) = 75.260,
M SE = 0.08879, p < .001, ηp2 = .816). Event model in-

Discussion
The goal of this experiment was to show that the same
verbal summary, impairing visual recognition performance in experiment 1, will be able to improve recognition performance, if it is presented before the video
clip. It was assumed that reading a verbal summary
leads to a visualization process resulting in a raw sketch
including a model of the event which is enriched with
details while looking to the film. That is, only one mental representation of the event was constructed. Indeed,
no VOE was observed. The significant interaction between the event model and the verbalization condition
qualifies this main effect. It is important to stress, that
the improvement in the verbal + film condition only occurred at event model incompatibility. That is, those
distractor items violating the verbal summary were easier to identify than those distractor items which were in
line with the verbal summary. In the film only condi-

1543

tion there was no difference regarding the event model
compatibility. The verbal only condition as control condition indicated that the improvement of the recognition
performance in the verbal + film condition can not be reduced to the verbal summary, because the performance
in the verbal only condition is lower than in the verbal +
film condition.

General Discussion
There were five major points regarding the experiments
reported in this paper. First, verbal overshadowing appears during the recognition process of dynamic scenes.
Second, source confusion seems to be important for the
classical verbal overshadowing effect (visual before verbal presentation) and cannot be reduced by the introduction of different distractor items (unlike Kitagami
et al., 2002). It seems that watching a film leads to
the establishment of an event model. If the verbal summary, also containing an event model, is presented after
the film, source confusion (Dodson et al., 1997) will occur because the two competing event models cannot be
integrated into one representation by the participants.
Third, source confusion is no general effect of presenting visual and verbal information. It could be shown
that VOE can be reduced by changing the serial order of
verbal and visual presentation. It was assumed that visualization processes while reading the verbal summary
(Intraub & Hoffman, 1992) are responsible for this effect.
When participants first viewed a detailed dynamic scene
and then read an abstract verbal summary, they visualized it and got confused. But when the abstract verbal summary was presented first, they visualized a raw
sketch with the corresponding event model at first. This
model now directed the attention of the participants to
the relevant details of the dynamic scene during the visual presentation, which led to an enrichment of the raw
sketch. Only one event model was generated by the participants, no source confusion appeared, hence no VOE
was observed. Contrariwise, results showed that presenting a verbal summary prior to the film led to higher
recognition performance when event model incompatible distractor items were used. Fourth, the different
time lags between stimulus presentation in the learning
phase and visual recognition in experiment 1 and 2 could
be responsible for the following effects. (a) In experiment 1 recognition perfomance in the film only condition at event model incompatibility was higher than at
event model compatibility; this effect did not occur in
experiment 2. This could be because in experiment 1
the participants in the film only condition had enough
time while watching the progress bar to develop an event
model autonomous, unlike in experiment 2, where visual recognition followed the film immediately. This
event model was made responsible for higher recognition performance in experiment 1. (b) In experiment 1
recognition performance in the verbal only condition at
event model compatibility was at chance level (A0 = .5)
(t(17) = −1.111, p = .282) but not in experiment 2
(t(17) = 3.921, p < .01). One possible explanation
for this finding could be that rehearsal processes while
1544

looking at the progress bar took place in experiment
2 which finally led to higher recognition performance.
Fifth, recognition performance indicated no qualitative
change regarding the viewpoint dependency.
Future research is needed to develop a framework in
which the mentioned processes can be integrated. Therefore a next step should address the question whether
the verbal overshadowing effect for dynamic scenes also
holds for active verbalization processes.

Acknowledgments
This work was supported by grant from the DFG
(Deutsche Forschungsgemeinschaft).

References
Bartlett, J. C., Till, R. E., & Levy, J. C. (1980). Retrieval characteristics of complex pictures: Effects
of verbal encoding. Jounal of Verbal Learning and
Verbal Behavior, 19 (4), 430–449.
Dodson, C. S., Johnson, M. K., & Schooler, J. W. (1997).
The verbal overshadowing effect: why descriptions
impair face recognition. Memory & Cognition,
25 (2), 129–139.
Garsoffky, B., Schwan, S., & Hesse, F. W. (2002).
Viewpoint dependency in the recognition of dynamic scenes. Journal of Experimental Psychology:
Learning, Memory, & Cognition, 28 (6), 1035–
1050.
Intraub, H., & Hoffman, J. (1992). Reading and visual memory: remembering scenes that were never
seen. American Journal of Psychology, 105 (1),
101–114.
Kitagami, S., Sato, W., & Yoshikawa, S. (2002). The
influence of test-set similarity in verbal overshadowing. Applied Cognitive Psychology, 16 (8), 963–
972.
Meissner, C. A., & Brigham, J. C. (2001). A metaanalysis of the verbal overshadowing effect in face
identification. Applied Cognitive Psychology, 15,
603–616.
Pollack, L., & Norman, D. A. (1964). Non-parametric
analysis of recognition experiments. Psychonomic
Science, 1, 125–126.
Read, J. D. (1979). Rehearsal and recognition of human
faces. American Journal of Psychology, 92 (1), 71–
85.
Schooler, J. W. (2002). Verbalization produces a transfer
inappropriate processing shift. Applied Cognitive
Psychology, 16, 989–997.
Schooler, J. W., & Engstler-Schooler, T. Y. (1990). Verbal overshadowing of visual memories: some things
are better left unsaid. Cognitive Psychology, 22 (1),
36–71.
Tabachnick, B. G., & Fidell, L. S. (1989). Using multivariant statistics. Harper Collins: New York.

