UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Influence of the Serial Order of Visual and Verbal Presentation on the Verbal
Overshadowing Effect of Dynamic Scenes
Permalink
https://escholarship.org/uc/item/5jx309rc
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Garsoffky, Barbel
Huff, Markus
Schwan, Stephan
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                     University of California

The Influence of the Serial Order of Visual and Verbal Presentation on
                 the Verbal Overshadowing Effect of Dynamic Scenes
                                        Markus Huff (m.huff@iwm-kmrc.de)
                                  Bärbel Garsoffky (b.garsoffky@iwm-kmrc.de)
                                    Stephan Schwan (s.schwan@iwm-kmrc.de)
                              Cybermedia Research Unit, Knowledge Media Research Center
                                  Konrad-Adenauer-Strasse 40, 72072 Tübingen, Germany
                           Abstract                               VOE when distractor items were designed highly sim-
                                                                  ilar to the target items, while no verbal overshadow-
   This article investigates the influence of verbalization       ing effect appeared at low similarity. In a former study
   processes on the visual recognition performance of dy-         Bartlett, Till, and Levy (1980) found, using realistic pho-
   namic scenes. In the recognition phase different dis-          tos as stimulus material, that verbalization led to higher
   tractor items were used: event model incompatible
   and event model compatible. It was hypothesized that           recognition performance, when it enabled the partici-
   source confusion, which is said to be responsible for in-      pants to distinguish between target and distractor items.
   ducing the verbal overshadowing effect, is reduced with        These findings are in accordance with the assumption
   event model incompatibility. In two experiments people         that viewers develop a kind of model which specifies the
   viewed a dynamic scene and read a verbal summary re-
   lated with the scene. The first experiment showed a ver-       characteristic and relevant features of the visual stimu-
   bal overshadowing effect when the verbal summary was           lus. This model enables the viewers to distinguish be-
   presented after the film. In the second experiment – the       tween target and distractor items in a recognition para-
   verbal summary was shown before the film – recognition         digm. On the one hand, model incompatible distractor
   performance improved with event model incompatibil-            items are easier to identify and on the other hand, model
   ity. Source confusion could be eliminated by changing
   the order of visual and verbal presentation.                   compatible distractor items are less likely identified. Ad-
                                                                  ditionally, it can be assumed that models directly derived
                                                                  from the visual stimulus contain more detailed informa-
               Verbal Overshadowing                               tion than models derived from the verbal description of
                                                                  the visual stimulus.
The question whether verbally describing a visual stimu-
lus fosters or hinders its subsequent recognition has got
a long tradition in cognitive psychology. Read (1979)
                                                                                   Research questions
showed that face recognition performance was improved             The considerations above suggest that VOE is not a
using verbalization. More recently verbal overshadow-             general effect, but appears only under certain condi-
ing has come in the focus of research. Schooler and               tions. In order to specify these conditions more precisely,
Engstler-Schooler (1990) examined the role of verbaliza-          two experiments were conducted, which were based on
tion processes. In the verbal overshadowing paradigm,             the following research questions. So far, VOE research
subjects first see a visual stimulus (e.g. a face) and then       has addressed the verbalization of static entities like
have to describe this stimulus verbally. Typically, in a          the appearance of a face. In everyday life such entities
subsequent recognition test, their recognition accuracy           are seldom verbally described and therefore little prac-
is lower (Schooler, 2002). In a meta analysis Meissner            ticed. Unlike dynamic scenes, in sporting reports it is
and Brigham (2001) analyzed 29 studies examining the              quite common to describe spatial relationships verbally.
verbal overshadowing phenomenon and found a negative              Hence, the first question focused on the domain: Is there
effect of verbalization processes on the recognition per-         a VOE within the domain of dynamic scenes, which are
formance. While active verbalization induces a transfer           commonly accompanied by verbalizations (e.g. sports
inappropriate processing shift, that dampens individu-            events)? Second, does the VOE influence recognition
als’ ability to apply certain non-verbal operations, Dod-         performance not only in quantitative, but also in qual-
son, Johnson, and Schooler (1997) evinced that a verbal           itative terms? Regarding the spatial properties, it was
overshadowing effect (VOE) even appears if participants           shown that recognition of dynamic scenes is viewpoint
read a verbal description passively. As recognition per-          dependent (Garsoffky, Schwan, & Hesse, 2002). A verbal
formance did not show a VOE when participants were                summary which contains no spatial information (e.g. left
asked to ignore their verbal representation, source con-          or right) could lead to a less viewpoint dependent visual
fusion about the validity of which mental representation          recognition performance, thereby indicating a qualita-
should be the basis for the recognition was made ac-              tive influence of VOE. Third, it was assumed that event
countable to this kind of VOE.                                    model compatibility had an impact on the VOE. While
   Although verbal overshadowing is a robust effect it            distractor items being designed in line with the verbal
could be shown that it disappeared under certain circum-          description (event model compatibility) should be diffi-
stances. Kitagami, Sato, and Yoshikawa (2002) found a             cult to identify, distractor items which violate the verbal
                                                             1539

Figure 1: Stimulus material. a) is showing balls mov-           Figure 2: Stimulus material with cameras indicating
ing away from the observer, b) is showing balls moving          the viewpoints from which the recognition items were
towards the observer                                            recorded.
summary (event model incompatibility) should be easily
identified. Event model incompatibility should reduce              The first independent variable was the verbalization,
source confusion and consequently the VOE. The VOE              realized as a blocked within-design. In the film only con-
after reading the description of a previously seen visual       dition the participants received no additional verbal in-
stimulus can be explained by source confusion which is          formation. In the film + verbal condition, the partic-
induced by two competing representations. First, a very         ipants received a short verbal summary after watching
detailed one, derived from the visual input, second a less      the film. And in the verbal only condition they received
detailed one, derived via visualization processes (Intraub      solely the verbal summary. The verbal summary de-
& Hoffman, 1992) from the verbal description. There-            scribed the behavior of one ball, containing its starting
fore as a fourth research question, it was supposed that        position, its passings (in relation to the other balls) and
the serial order of visual and verbal presentation had          its finishing position, for example: ”The red ball starts
an impact to the VOE. If the serial order of the stim-          behind all other balls, passes every other ball and finally
ulus presentation is switched (first the verbal and then        wins.” The verbal summary was supposed to establish an
the visual presentation) the participants will first visu-      event model. It was supposed that the event model de-
alize a raw sketch which can be enriched with details           rived from the verbal summary contained very abstract
while watching the visual presentation. A VOE was ex-           information about the dynamic scene.
pected in the classical condition (visual before verbal)           The second independent variable was the event model
and it was expected to disappear if the serial order was        compatibility. It was realized by using the distractor
inverted (verbal before visual).                                items in the recognition test. Two types of distractor
                                                                items were created: event model compatible items and
                    Experiment 1                                event model incompatible items. The event model in-
The first experiment, in which the order of the presenta-       compatible items violated the event model. In the exam-
tion was the same like in the classical VOE studies, was        ple above, a possible event model incompatible distractor
conducted to examine the research questions 1, 2 and 3.         item could be created in moving the blue ball toward the
                                                                finishing line, in that case it became impossible for the
Method                                                          red ball to win this race. In contrast, the event model
                                                                compatible items did not violate the event model. In the
Participants Subjects were 18 students (11 female, 7
                                                                example, the yellow ball was moved. This modification
male) of the University of Tübingen, Germany. Average
                                                                is in line with the event model, that the red ball starts
age was 23 years. They were paid for their participation.
                                                                in the rearest position, passes all the other balls and fi-
Apparatus The experimental procedures were con-                 nally wins the race. As third independent variable the
trolled by a Microsoft computer and programmed using            deviation between the viewpoint in the learning and the
MediaLab and directRT. Video clips and video stills were        testing phase of the video stills in the recognition phase
presented on a black background in the middle of a 17”          was varied. The deviation was 0◦ , 45 ◦ , 90◦ and 135◦
CRT-Monitor.                                                    (see figure 2). And finally as fourth independent vari-
                                                                able the recognition items were recorded at 4 different
Stimulus material and design As stimulus mate-
                                                                points of time during the scene (3.0, 4.5, 6.0 and 7.5 s.).
rial a kind of race1 consisting of four balls moving on
                                                                This variable was introduced to measure several points
parallel laps toward a white line were created using 3ds
                                                                over the length of time of the dynamic scene on the one
max 6. Every ball had a different starting position and a
                                                                hand, and also to collect enough datapoints for analysis.
different movement characteristic (constant velocity, ac-
celerating or decelerating). Each race lasted 8 seconds         Procedure All participants were tested individually
and was rendered either in that way, that the balls were        and received the instruction via computer monitor. They
moving away from the observer or that the balls were            got a description of the kind of dynamic scenes used,
moving toward the observer (see figure 1).                      the verbalization conditions and the subsequent recog-
                                                                nition test. After that they passed a training phase in
   1
     Examples of the stimulus materials are available at        which every condition was presented. These data were
http://www.iwm-kmrc.de/cybermedia/cs06-voe/                     excluded from the analysis. The following experimental
                                                           1540

phase consisted of three blocks: one block for every ver-
balization condition (film only vs. verbal only vs. film +
verbal). These blocks were presented in a balanced man-
ner. Between the blocks a rest period of 5 minutes was
established to avoid exhaustion and possible carry-over
effects.
   In the film only condition the participants were shown
a dynamic scene twice. After the film the participants
were shown a progress bar for the duration of 10 sec-
onds. In the film + verbal condition the participants
were shown a film twice before they read a verbal sum-
mary of the dynamic scene, which appeared for 10 sec-
onds on the monitor. Instead of the film, in the verbal
only condition the participants viewed a progress bar
for 17 seconds. Afterwards a verbal summary of the dy-
namic scene appeared for 10 seconds.
   In all conditions, before the recognition test, a video
still, depicting the previously seen scene was shown to             Figure 3: The interaction of verbalization and event
indicate the beginning of the recognition test, which in-           model. Error bars indicating the standard error of the
cluded 48 video stills: 16 target items showing the origi-          mean.
nal scene from 4 different viewpoints at 4 different points
in time. 16 event model incompatible distractor items,
showing a distractor scene (also 4 different viewpoints             viewpoint deviation(F < 1). The main effect for point
and 4 different points in time) and 16 event model com-             of time was significant (A0 for 1: .636, 2: .678, 3: .733,
patible distractor items (4 different viewpoint deviations          4: .743; F (3, 51) = 14.506, M SE = 0.07412, p < .001,
and 4 different points in time as well).                            ηp2 = .460). For this effect there was a significant lin-
   Taken together a 3 (verbalization) x 2 (event model             ear trend indicating a recency effect (F (1, 17) = 27.975,
compatibility) x 4 (viewpoint deviation) x 4 (point of              M SE = 0.109, p < .001, ηp2 = .622).
time) design was realized.                                             The interaction between verbalization and event model
                                                                    compatibility was significant (F (2, 34) = 10.366, M SE =
Results                                                             0.272, p < .001, ηp2 = .379). Single comparisons accord-
To compute the sensitivity measure A0 (Pollack & Nor-              ing to Scheffé revealed the following significant differ-
man, 1964), the mean hit rate (yes-answers to target               ences (p < .01). At event model incompatibility there
items) and the mean false-alarm rate (yes-answers to dis-          was a significant difference between the film only (.816)
tractor items) for the event model incompatible and the            and the film + verbal condition (.750). For event model
event model compatible distractor items for every con-             compatible distractor items significant differences were
dition were calculated.                                            found between all conditions. Single comparisons re-
   Across all conditions and participants a mean of .697           garding the event model compatibility within the ver-
for A0 was calculated. An ANOVA with repeated mea-                 balization conditions revealed, that in every verbaliza-
surement was calculated including the independent vari-            tion condition, event model incompatibility led to higher
ables verbalization (film vs. verbal vs. film + verbal),           recognition rates (p < .01). See figure 3. A planned com-
event model (incompatible vs. compatible), viewpoint               parison of the predicted difference between the film only
deviation (0◦ vs. 45◦ vs. 90◦ vs. 135◦ ) and point of              and the film + verbal condition regarding event model
time (3.0 vs. 4.5 vs. 6.0 vs. 7.5 s. after the begin-              compatibility revealed no difference; there was no signifi-
ning of the scene). A significant main effect for verbal-          cant interaction between verbalization (film only vs. film
ization was found (F (2, 34) = 13.588, M SE = 0.313,               + verbal) and event model compatibility (F < 1).
p < .001, ηp2 = .4442 ). In the film only condition an A0              The interaction between event model compatibility and
measure of .782 was observed. In the film + verbal con-             viewpoint deviation was significant (F (3, 51) = 3.086,
dition A0 was .700 and in the verbal only condition it was          M SE = 0.01584, p < .05, ηp2 = .154). Post hoc
.601. Single comparisons according to Scheffé revealed             analysis (Scheffé) revealed that there were no differences
significant differences between all of the three conditions         between the different viewpoint deviations both within
(p < .01). A significant main effect for event model was            event model incompatibility and event model compati-
found (F (1, 17) = 28.973, M SE = 0.403, p < .001,                  bility, p < .01 (see table 1).
ηp2 = .630). Event model incompatibility led to higher                 The interaction between event model compatibility
recognition performance (A0 = .779) than event model                and point of time was significant (F (3, 51) = 4.714,
compatibility (A0 = .615). There was no main effect for             M SE = 0.032522, p < .01, ηp2 = .217). Post hoc analysis
    2                                                               showed the following result pattern (see table 2). Event
      In the reported experiments the partial η 2 (ηp2 ) as ef-
fect size measure is reported, because it is more appropriate       model incompatibility led to lower recognition perfor-
to the the design with more than one independent variable           mance at the first point of time than at the other points
(Tabachnick & Fidell, 1989).                                        of time. There were no differences between point of time
                                                               1541

                                                                  holds for dynamic scenes. Recognition performance was
Table 1: Interaction between viewpoint deviation and              lower in the film + verbal condition than in the film only
event model compatibility (sensitivity measure A0 ).              condition. In the control condition, in which the partic-
                                   event model                    ipants only read a verbal summary of the film, recog-
                          incompatible compatible                 nition performance was lowest. This finding suggests,
      viewpoint 0◦        .795              .611                  that verbalization shows the same impacts on dynamic
      deviation 45◦       .772              .601                  scenes, describing spatial relationships as on static ma-
                   90◦    .770              .636                  terial. Although former are more often accompanied by
                   135◦ .781                .613                  verbal descriptions.
                                                                     Secondly, it was assumed, that a so called event model
                                                                  describing the similarity of the verbal summary and the
2, 3 and 4. Event model compatibility led to a significant        distractor items, could help the participants to distin-
difference between the first two and the last two points          guish between target and distractor items (research ques-
of time (p < .01).                                                tions 1 and 3). As expected, there was a significant in-
    There was also a significant interaction between ver-         teraction between the verbalization condition and the
balization condition and point of time (F (6, 102) =              event model (research question 3). But the results do
4.681, M SE = 0.06415, p < .001, ηp2 = .216, see ta-              not show the expected absence of the VOE for event
                                                                  model incompatibility, although the event model enabled
ble 2). While there were no differences in the film +
                                                                  the participants in the verbal only condition to perform
verbal condition over the different points of time, there
                                                                  above chance level. A possible explanation could be that
were several differences in the verbal only and the film
                                                                  source confusion (Dodson et al., 1997) was not com-
only conditions: In the film only condition there were
                                                                  pletely reduced by this manipulation and the partici-
significant differences between point of time 1 and 3 and
                                                                  pants still were not able to apply the appropriate repre-
4 as well as between point of time 2 and 3. In the ver-
                                                                  sentation during the recognition test. Unlike Kitagami
bal only condition there were significant differences be-
                                                                  et al. (2002), the similarity of distractor items with tar-
tween points of time 1 and 3 and 4 and between 2 and 4
                                                                  get items seemed not to be important for this kind of
(p < .01).
                                                                  VOE.
    There was a significant interaction between verbaliza-
tion condition, event model compatibility and point of               There is no indication that VOE led to a change of
time (F (6, 102) = 11.104, M SE = 0.04167, p < .001,              the qualitative features of the recognition performance in
ηp2 = .395). Single comparisons according to Scheffé             this experiment (research question 2). Neither in the film
revealed the following result pattern. Event model in-            only condition nor in the conditions containing the ver-
compatibility led to no significant differences across the        bal summary a viewpoint deviation effect was observed.
different points of time solely in the film + verbal and the      Therefore this effect is not part of the discussion in this
film only condition. In the verbal only condition there           paper.
was lower recognition performance at point of time 1                 The assumption that visualization processes occur af-
than at all other points of time. Event model compatibil-         ter reading the verbal summary is supported by the fact,
ity again led to no differences across the different points       that participants are able to perform above chance level
of time in the film + verbal condition. In the film only          in a visual recognition test in the verbal only condition
condition lower recognition performance was observed at           (Intraub & Hoffman, 1992, reported similar effects).
point of time 1 than at point of time 3. In the verbal only          In sum, results from the film only condition indicate,
condition there was lower performance at point of time            that participants did not need the verbal summary to
2 than on the fourth one (p < .01, see table 2).                  develop an event model of a dynamic scene – in the
                                                                  film only condition, event model incompatible distrac-
                                                                  tor items were more often identified than event model
Table 2: Interaction between verbalization condition,             compatible items. The VOE in the film + verbal con-
event model and point of time (sensitivity measure A0 ).          dition can be explained as follows: there were two com-
                    verbalization condition                       peting representations, a detailed one, derived from the
                film only     verbal only film + verbal           visual input and a second one, which was more abstract,
                                                                  derived from the verbal input. Source confusion now oc-
                       event model compatibility
                                                                  curred because the latter could not be integrated into the
               yes     no     yes    no       yes   no            former one. That is, because participants had already
   point 1 .760 .614 .607 .474 .760 .599                          formed an event model from the visual input, which is
   of      2 .792 .748 .781 .347 .784 .615                        not necessary congruent with the event model induced
   time    3 .872 .869 .837 .416 .739 .663                        by the verbal summary. Therefore two representations
           4 .840 .758 .863 .553 .717 .725                        existed, which led to source confusion and consequently
                                                                  to the observed VOE.
                       Discussion                                                     Experiment 2
One main focus of this experiment was the question                In contrast to the first experiment, in experiment 2 the
whether the verbal overshadowing phenomenon also                  order of the visual presentation and the verbal summary
                                                             1542

was changed. The verbal summary was presented be-
fore the visual presentation. When the verbal summary
was presented before the visual presentation it was ex-
pected that no source confusion occurred, because only
one mental representation of the event was constructed
by the participants: they first visualize a raw sketch in-
cluding a kind of event model. This representation, con-
taining no detailed information about the scene, can be
enriched with more details while watching the film. It
was hypothesized, that a verbal summary, presented be-
fore the video clip increased recognition performance,
when it contained information which was relevant to
identify event model incompatible distractor items. This
is because the event model derived from the verbal sum-
mary should direct the attention of the participants to
relevant parts of the scenes.                                    Figure 4: The interaction of verbalization and event
                                                                 model. Error bars indicating the standard error of the
Method
                                                                 mean.
Participants Subjects were 18 students (15 female, 3
male) of the University of Tübingen, Germany. Average
age was 23 years. They were paid for their participation.        compatibility lead to higher sensitivity measures (.792)
                                                                 than event model compatibility (.668). There was no
Apparatus, stimulus material and design The                      main effect for viewpoint deviation (F < 1). The
same setting, stimuli and design as in the first experi-         main effect for point of time was significant (A0 for 1:
ment were used.                                                  .683, 2: .710, 3: .739, 4: .789; F (3, 51) = 19.969,
Procedure The procedure was almost the same as in                M SE = 0.05335, p < .001, ηp2 = .495). For this effect
the first experiment, apart from the order of the video          there was a significant linear trend indicating a recency
clip and verbal summary. The film only condition was             effect (F (1, 17) = 35.172, M SE = 0.07695, p < .001,
adapted in that way, that after watching the film twice          ηp2 = .665).
(17 seconds) the participants were asked to start the               The interaction between verbalization and event model
recognition test by pressing the space bar. In the verbal        compatibility was significant (F (2, 34) = 5.673, M SE =
only condition the verbal summary was presented. After           0.133, p < .01, ηp2 = .250). Single comparisons accord-
reading the text the participants had to press the space        ing to Scheffé showed following significant differences
bar, then a progress bar appeared for 17 seconds. Again         (p < .01). Event model incompatibility led to higher
the recognition test started after pressing the space bar.      sensitivity measures in the verbal + film condition than
In the verbal + film condition the participants first read      in both the film only and verbal only conditions. How-
a verbal summary of the film they were shown after              ever, no difference between the film and verbal + film
pressing the space bar. Right after the film clips (which       conditions was observed during event model compatibil-
lasted 17 seconds) the participants were asked to start         ity, just the verbal only condition there was lower per-
the recognition test by pressing the space bar.                 formance than in the other two conditions (see figure
                                                                4).
Results
As in experiment 1, A0 as dependent variable was cal-                                  Discussion
culated. Across all participants and conditions a mean          The goal of this experiment was to show that the same
sensitivity rate of A0 = .730 was measured. An ANOVA            verbal summary, impairing visual recognition perfor-
with repeated measurement was conducted using the in-           mance in experiment 1, will be able to improve recog-
dependent variables verbalization (film vs. verbal vs.          nition performance, if it is presented before the video
verbal + film), event model (incompatible vs. com-              clip. It was assumed that reading a verbal summary
patible), viewpoint deviation (0◦ vs. 45◦ vs. 90◦ vs.           leads to a visualization process resulting in a raw sketch
135◦ ) and point of time (3.0 vs. 4.5 vs. 6.0 vs. 7.5           including a model of the event which is enriched with
sec. after the beginning of the scene). There was a sig-        details while looking to the film. That is, only one men-
nificant main effect for verbalization (F (2, 34) = 7.605,      tal representation of the event was constructed. Indeed,
M SE = 0.214, p < .05, ηp2 = .309). Signal compar-              no VOE was observed. The significant interaction be-
isons with the Scheffé procedure revealed significant dif-     tween the event model and the verbalization condition
ferences between all conditions. In the verbal only con-        qualifies this main effect. It is important to stress, that
dition the sensitivity measure was .671, in the film only       the improvement in the verbal + film condition only oc-
condition .744 and in the verbal + film condition .775.         curred at event model incompatibility. That is, those
As in experiment 1 there was also a significant main ef-        distractor items violating the verbal summary were eas-
fect for event model compatibility (F (2, 34) = 75.260,         ier to identify than those distractor items which were in
M SE = 0.08879, p < .001, ηp2 = .816). Event model in-          line with the verbal summary. In the film only condi-
                                                            1543

tion there was no difference regarding the event model         looking at the progress bar took place in experiment
compatibility. The verbal only condition as control con-       2 which finally led to higher recognition performance.
dition indicated that the improvement of the recognition       Fifth, recognition performance indicated no qualitative
performance in the verbal + film condition can not be re-      change regarding the viewpoint dependency.
duced to the verbal summary, because the performance             Future research is needed to develop a framework in
in the verbal only condition is lower than in the verbal +     which the mentioned processes can be integrated. There-
film condition.                                                fore a next step should address the question whether
                                                               the verbal overshadowing effect for dynamic scenes also
                General Discussion                             holds for active verbalization processes.
There were five major points regarding the experiments                          Acknowledgments
reported in this paper. First, verbal overshadowing ap-        This work was supported by grant from the DFG
pears during the recognition process of dynamic scenes.        (Deutsche Forschungsgemeinschaft).
Second, source confusion seems to be important for the
classical verbal overshadowing effect (visual before ver-                             References
bal presentation) and cannot be reduced by the intro-
duction of different distractor items (unlike Kitagami         Bartlett, J. C., Till, R. E., & Levy, J. C. (1980). Re-
et al., 2002). It seems that watching a film leads to                trieval characteristics of complex pictures: Effects
the establishment of an event model. If the verbal sum-              of verbal encoding. Jounal of Verbal Learning and
mary, also containing an event model, is presented after             Verbal Behavior, 19 (4), 430–449.
the film, source confusion (Dodson et al., 1997) will oc-      Dodson, C. S., Johnson, M. K., & Schooler, J. W. (1997).
cur because the two competing event models cannot be                 The verbal overshadowing effect: why descriptions
integrated into one representation by the participants.              impair face recognition. Memory & Cognition,
Third, source confusion is no general effect of present-             25 (2), 129–139.
ing visual and verbal information. It could be shown           Garsoffky, B., Schwan, S., & Hesse, F. W. (2002).
that VOE can be reduced by changing the serial order of
                                                                     Viewpoint dependency in the recognition of dy-
verbal and visual presentation. It was assumed that vi-
sualization processes while reading the verbal summary               namic scenes. Journal of Experimental Psychology:
(Intraub & Hoffman, 1992) are responsible for this effect.           Learning, Memory, & Cognition, 28 (6), 1035–
When participants first viewed a detailed dynamic scene              1050.
and then read an abstract verbal summary, they visu-           Intraub, H., & Hoffman, J. (1992). Reading and vi-
alized it and got confused. But when the abstract ver-               sual memory: remembering scenes that were never
bal summary was presented first, they visualized a raw               seen. American Journal of Psychology, 105 (1),
sketch with the corresponding event model at first. This             101–114.
model now directed the attention of the participants to        Kitagami, S., Sato, W., & Yoshikawa, S. (2002). The
the relevant details of the dynamic scene during the vi-             influence of test-set similarity in verbal overshad-
sual presentation, which led to an enrichment of the raw             owing. Applied Cognitive Psychology, 16 (8), 963–
sketch. Only one event model was generated by the par-
                                                                     972.
ticipants, no source confusion appeared, hence no VOE
was observed. Contrariwise, results showed that pre-           Meissner, C. A., & Brigham, J. C. (2001). A meta-
senting a verbal summary prior to the film led to higher             analysis of the verbal overshadowing effect in face
recognition performance when event model incompati-                  identification. Applied Cognitive Psychology, 15,
ble distractor items were used. Fourth, the different                603–616.
time lags between stimulus presentation in the learning        Pollack, L., & Norman, D. A. (1964). Non-parametric
phase and visual recognition in experiment 1 and 2 could             analysis of recognition experiments. Psychonomic
be responsible for the following effects. (a) In experi-             Science, 1, 125–126.
ment 1 recognition perfomance in the film only condi-          Read, J. D. (1979). Rehearsal and recognition of human
tion at event model incompatibility was higher than at               faces. American Journal of Psychology, 92 (1), 71–
event model compatibility; this effect did not occur in              85.
experiment 2. This could be because in experiment 1
                                                               Schooler, J. W. (2002). Verbalization produces a transfer
the participants in the film only condition had enough
time while watching the progress bar to develop an event             inappropriate processing shift. Applied Cognitive
model autonomous, unlike in experiment 2, where vi-                  Psychology, 16, 989–997.
sual recognition followed the film immediately. This           Schooler, J. W., & Engstler-Schooler, T. Y. (1990). Ver-
event model was made responsible for higher recogni-                 bal overshadowing of visual memories: some things
tion performance in experiment 1. (b) In experiment 1                are better left unsaid. Cognitive Psychology, 22 (1),
recognition performance in the verbal only condition at              36–71.
event model compatibility was at chance level (A0 = .5)        Tabachnick, B. G., & Fidell, L. S. (1989). Using multi-
(t(17) = −1.111, p = .282) but not in experiment 2                   variant statistics. Harper Collins: New York.
(t(17) = 3.921, p < .01). One possible explanation
for this finding could be that rehearsal processes while
                                                          1544

