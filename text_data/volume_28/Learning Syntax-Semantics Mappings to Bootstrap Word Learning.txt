UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Syntax-Semantics Mappings to Bootstrap Word Learning
Permalink
https://escholarship.org/uc/item/5kp245p7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Author
Yu, Chen
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                     University of California

              Learning Syntax-Semantics Mappings to Bootstrap Word Learning
                                                     Chen Yu (chenyu@indiana.edu)
                         Department of Psychological and Brain Sciences, and Program in Cognitive Science
                                              Indiana University, Bloomington, 47405, IN USA
                               Abstract                                  to deduce the word argument structures. In contrast, Gleit-
   This paper addresses possible interactive effects between word        man (1990) proposed an alternative account called syntactic
   learning and syntax learning at an early stage of develop-            bootstrapping. She argued that children use syntactic knowl-
   ment. We present a computational model that simulates how             edge they have developed to learn what words mean. More
   the results from a syntax learning process and a word learn-          specifically, the semantically relevant syntactic structures sur-
   ing process can be integrated to build syntax-semantics map-
   pings, and how the emergence of links between syntax and              rounding a verb, such as the subcategorization frames around
   word learning could facilitate subsequent word learning. The          a verb, provide contextual cues for its meaning. These two
   central idea of our statistical model is to categorize words into     hypotheses focus on different aspects of potential interactions
   groups based on their syntactic roles and then estimate seman-        between syntax and semantic learning, and both of them have
   tic meanings of those syntactic categories using lexical knowl-       been supported by empirical studies.
   edge acquired from a concurrent word learning process. Once
   built, those syntax-semantics mappings can be further utilized           The present paper proposes a computational model of how
   as a syntactic constraint in statistical word learning. We ap-        syntax-semantics mappings can be learned and emerged from
   plied the model to realistic data collected from child-mother         two language learning processes – syntax learning and word
   picture book reading interaction. A comparative study be-             learning, and how these mappings can then facilitate word
   tween a statistical model and the model based on both statisti-
   cal and syntactic information shows that syntactic cues can be        learning. Our study is quite different from previous work
   seamlessly integrated in statistical learning and significantly       in several important ways. First, we propose and implement
   improve word learning performance.                                    a general statistical-learning mechanism in which syntactic
                                                                         cues can be seamlessly integrated with already learned se-
                           Introduction                                  mantic knowledge to help the learning of new words. We
                                                                         suggest that syntax can act as a linguistic spotlight that fa-
One of the most complex learning tasks young children are                cilities word learning by selecting, grouping and highlighting
faced with is to learn their native language. Language ac-               those words that are likely to have the same type of refer-
quisition, of course, consists of several distinct tasks, such as        ents. Using the proposed learning mechanism, we demon-
speech perception, speech segmentation, word learning and                strate how syntactic learning could help object name learning
syntax learning. Among others, word learning involves how                and how the development of grammatical abilities continues
to map a phonological form to a conceptual representation,               to be highly linked to lexical development. Second, both the
such as associating the sound “dog” to the concept of dog.               proposed learning mechanism of syntax-semantics mappings
Thus, the crucial issue in word learning is to build word-               and the mechanism of utilizing the mapping knowledge in
to-world mappings from language and extralinguistic con-                 word learning are general that can be applied not only to a
texts. Syntax learning, on the other hand, is mainly about               specific syntactic category (verb, etc.) but also to other cat-
how to categorize words into grammatical categories (e.g.                egories. Thus, we suggest that the acquisition of syntax and
noun, verb, etc.) which are basic building blocks of gram-               the integration of syntactic cues in word learning might par-
mar, and then how to acquire the hierarchical and context-               tially account for the explosive expansion of vocabulary as
sensitive structures that are represented by those syntactic cat-        primary syntactic structures are gradually acquired. Third, we
egories. Therefore, syntax learning uses sequential symbolic             apply the model to raw data collected from everyday parent-
data (sentences in a language, etc.) to construct a grammar.             children interaction but not to some artificial or synthesized
   Although acquisition of the lexicon and acquisition of the            data, and show a dynamic picture of how the learning mech-
grammar seem to address totally different issues, these two              anism works with realistic input.
learning processes might be closely related due to universal                                    Related Work
correspondences between syntax and semantics. For instance,              There are a number of existing models that account for dif-
Bloom (1994) pointed out bidirectional mappings between                  ferent aspects of word learning. Plunkett, Sinha, Miller,
syntax and semantics, such as count nouns to kinds of in-                and Strandsby (1992) built a connectionist model of word
dividuals, mass nouns to kinds of portions, and Noun Phrases             learning in which a process termed autoassociation mapped
(NPs) to individuals. Such mappings suggest a possible boot-             preprocessed images with linguistic labels. The linguis-
strapping procedure between these two learning processes –               tic behavior of the network exhibited non-linear vocabu-
the progresses in one learning process could facilitate the              lary growth (vocabulary spurt) that was similar to the pat-
other learning process. In fact, two compelling hypotheses               tern observed in young children. Colunga and Smith (2005)
have been proposed by theorists. The semantic bootstrapping              presented a connectionist model showing that regularities
hypothesis (Pinker, 1989) argued that word meanings can be               among object and substances categories were learnable and
antecedently acquired from the observation of events and then            generalizable enabling the system to become, after train-
used to determine the syntactic category of each word and                ing, a more rapid learner of new object and substance
                                                                     924

names. Siskind (1996) developed a mathematical model                   3 objects co-occur in a single learning moment without any
based on cross-situational learning and the principle of con-          information about which word goes to which object from a
trast, which learned word-meaning associations when pre-               trial itself. Only 3.13%(132/4223) of co-occurring pairs are
sented with paired sequences of pre-segmented tokens and               correct, which shows the difficulty of the word-learning task.
semantic representations. Regier (2005) suggested that atten-          As shown in Table 1, every word in a learning situation can be
tion to relevant aspects of form and meaning could account             potentially associated with any co-occurring object. Thus, the
for developmental changes without a change in associative              learning task for both young children and simulated learners
mechanism. Tenenbaum and Xu (2000) developed a compu-                  is to find a very few correct lexical items from a huge amount
tational model based on Bayesian inference which could infer           of irrelevant co-occurring words and objects.
meanings from one or a few examples without encoding the                                Table 2: Statistics of training data
constraint of mutual exclusion. Li, Farkas, and MacWhin-                  # of words       # of unique words        words per situation
ney (2004) proposed a SOM-based developmental model that                      3571                 581                      >8
learned topographically organized representations for linguis-            # of objects # of unique objects objects per situation
tic categories over time. However, the role of syntax in word                 1230                 113                      >3
learning has not been systematically studied in cognitive de-              # of pairs       # of unique pairs        # of correct pairs
velopment (but also see Siskind, 1992 using syntactic con-
straints to help the acquisition of semantics based on a logic-              12173                4223                      132
inference mechanism). In addition, artificial or synthesized                                      The Model
data are used to demonstrate a model’s performance and il-             Our model consists of three components: statistical word
lustrate the key ideas in a model. In contrast, this work ap-          learning without syntax, syntax learning and the integration
plies the data collected from natural parent-child interaction         of syntactic knowledge in word learning. The central idea
to our proposed model and the results show what a learning             is that a syntax learning process can learn structural infor-
mechanism could achieve from realistic data.                           mation in unsupervised mode based on statistical regulari-
                              Data                                     ties in speech. Meanwhile, a word learning process uses co-
Six 18-month-old children and their parents participated in            occurrence regularities between language and extralinguistic
data collection. Each parent was asked to narrate one picture          contexts to build word-referent mappings. Importantly, the
book. In total, six books for 1-3 year old children were used.         results in these two learning processes can be merged to gen-
Parents were also instructed to act naturally without any con-         erate new kinds of statistical regularities. More specifically,
straint about what they had to say or what they had to do.             the syntax learning process categorizes words into several
Picture book narration is one of common parent-child activi-           groups based on their linguistic roles. Semantic meanings
ties in everyday life from which children learn the names of           of the words in a syntactic category can be acquired through
objects shown in the picture books. Therefore, the data col-           the word learning process and jointly determine the seman-
lected from this setting is realistic and representative of every-     tic meaning of that syntactic category. Thus, the integration
day word learning. The data used in this simulation study              of the results in these two learning processes generates the
were our descriptions of video clips. More specifically, our           mappings between syntax and semantics, which in turn can
description of the audio input – what we feed into the sta-            facilitate lexical acquisition by considering the syntactic role
tistical simulated learner – is the entire list of spoken words.       of a new word in word learning. The following subsections
Our description of the video stream, again what we feed into           will describe this learning mechanism in detail.
the statistical learner, is the list of all the (basic-level) ob-      Statistical Word Mapping Without Syntactic Cues
jects in picture books that a narrator was attending to from           In early word learning without syntactic cues, children have
moment to moment when spoken utterances were produced.                 to start by pairing spoken words with co-occurring contexts,
Table 1 shows several examples wherein each row represents             collecting multiple such pairs, and then figuring out the com-
one learning situation (defined by speech silence) consisting          mon elements. Although no one doubts this process, there has
of multiple words and multiple objects.                                been few modeling studies (but also see Siskind, 1996). Yu,
                 Table 1: Examples of training data                    Ballard, and Aslin (2005) introduce a formal model of statis-
   speech                                  visual context              tical word learning which provides a probabilistic framework
                                                                       for encoding multiple sources of information. Given multiple
                                                                       scenes paired with spoken words collected from natural in-
   is that a little baby                   boy, flowers, bird
                                                                       teractions between caregivers and children, the model is able
   and what is the little baby holding     boy, flowers, bird          to compute the association probabilities of all the possible
   that is right flowers                   boy, flowers, bird          word-meaning pairs.
   ......                                  ......                         The general setting is as follows: suppose we have a
   that is a pumpkin and look              boy, pumpkin, leave         word set X = {w1 , w2 , ..., wN } and a meaning set Y =
   what is this back there                 boy, pumpkin, leave         {m1 , m2 , ..., mM }, where N is the number of words and M
   ......                                  ......                      is the number of meanings (basic-level objects, etc.). Let S
   look what he is doing now               boy, hat, bird, wall        be the number of spoken utterances. All word data are in a
                                                                                       (s)   (s)
   ......                                  ......                      set χ = {(Sw , Sm ), 1 ≤ s ≤ S}, where each spoken ut-
                                                                                   (s)
    The statistics of the data set (the sum over six subjects)         terance Sw consists of r words wu(1) , wu(2) , ..., wu(r) , and
are described in Table 2. The learning environment is rather           u(i) can be selected from 1 to N . Similarly, the correspond-
                                                                                                         (s)
highly ambiguous wherein on average more than 8 words and              ing contextual information Sm include l possible meanings
                                                                   925

mv(1) , mv(2) , ..., mv(l) and the value of v(j) is from 1 to M .          perspective, this result might be quite in line with the learn-
Assume that every word wn can be associated with a mean-                   ing performance of young children who also learn syntactic
ing mm . Given a data set χ, We use the machine translation                knowledge in an unsupervised manner and would not learn
method proposed by Brown, Pietra, Pietra, and Mercer (1994)                a grammar overnight but instead gradually acquire and accu-
to maximize the likelihood of generating the meaning strings               mulate syntactic knowledge. In this way, equivalence classes
given English descriptions:                                                can be treated as temporary results toward grammatical cate-
              P (Sm  (1)    (2)
                         , Sm           (S) (1)
                                , ..., Sm            (2)
                                              |Sw , Sw           (S)
                                                         , ..., Sw   )     gories. The next section will show how this partial linguistic
                                                                           knowledge can be utilized in word learning.
               YS X
                                                                                 Table 3: Examples of the results of syntax learning
                              (s)         (s)
          =               p(Sm     , a|Sw     )
                                                                              E582 { and at below hope so in by maybe except... }
              s=1 a
                                                                              E584 { big little }
               YS
                          ǫ      Y l X  r
          =                                 p(mv(j) |wu(i) )                  E586 { apples here there these they flowers }
              s=1
                    (r + 1)l j=1 i=0                                          E588 { rabbit rooster bear bunny cat mom dog duck }
                                                                              E590 { front case }
where the alignment a indicates which word is aligned with
which meaning. p(mv(j) |wu(i) ) is the association probability                P583 [ a E584 ]
for a word-meaning pair:                                                      P585 [ E586 are ]
  p(mv(j) |wu(i) ) = p(mv(j) |wu(i) , gu(i) )p(gu(i) |wu(i) )                 P587 [ E582 the E588 ]
where p(gu(i) |wu(i) ) is the probability that a word has a refer-            P589 [ in E590 of ]
ent and p(mv(j) |wu(i) , gu(i) ) is the association probability of         Learning Syntax-Semantics Mappings
a word-referent pair given that the word has a referent. With-             The general idea is like this: the syntax learning process cat-
out any linguistic knowledge and starting from scratch, the                egorizes words into groups based on their syntactic roles.
model assumes that every word could have a referent. There-
                                                                           Meanwhile, the word learning process builds the association
fore, p(gn |wn ) is set to be 1 for every word. The estimate               probabilities between words and objects. Building syntax-
of p(mm |wn , gn ) can be found in Yu et al. (2005). The up-               semantics mappings can then be accomplished by integrating
per right figure in Figure 1 shows the results of word-referent
                                                                           the results of these two learning processes. The present model
association probabilities p(mv(j) |wu(i) ).                                explores how two specific mappings could emerge from the
Early Syntax Learning                                                      integration: (1) those words with high association probabili-
We used to the learning algorithm developed by Solan, Horn,                ties with objects (thus, with similar semantic properties) are
Ruppin, and Edelman (2005) to extract linguistic structures.               also likely to be in the same syntactic groups built by the syn-
The method represents sentences as paths on a graph and                    tax learning process, and (2) the words that are less likely to
words as vertices on the paths. It aligns and identifies those             refer to objects are also grouped together based on their syn-
sentences that share some words and extracts both common                   tactic roles. In this way, all the words in a syntactic class can
and variant words in those sentences. The approach then pro-               jointly define the semantic role of that class. Next this link
gressively infers linguistic structures from the accrued statis-           between syntax and semantics can be used to guide subse-
tical knowledge. For instance, given two simple sentences                  quent word learning. For example, if most words in a syntac-
“this is a cat” and “here is a dog”, the method can extract the            tic class are associated with some object kinds and therefore
pattern “x is a y” while x can be replaced by {this, here} and             they are likely to be object names, other words in the same
y stands for {cat, dog}. Technical details can be founded in               class should also be likely to associate with object kinds be-
Solan et al. (2005).                                                       cause of the inherent relationship between count nouns and
   Table 3 shows examples of applying the syntax learning                  object names. Thus, when the model considers whether a new
method on our data. Induced syntactic structures are repre-                word refers to an object, it is based on not only the associa-
sented in two forms: patterns and equivalence classes. A pat-              tion probability between these two (co-occurrence statistical
tern represents a set of full (or part of ) sentences or phrases           regularities between word-to-world mappings) but also what
that share common symbols. Those symbols can be either a                   syntactic class this word is in. By doing so, syntax-semantics
word or a group of words termed an equivalence class that                  mappings may improve the performance of statistical word
can be replaced in the pattern to form different sentences or              learning.
phrases. For instance, the pattern P587 can represent dif-                    Without any linguistic cues, the previous model assumes
ferent phrases by selecting different members in the equiv-                that every word (even function words) could be associated
alence classes E582 and E588, such as “and the rabbit”, “and               with an object referent and therefore estimates the association
the rooster”, “at the rabbit”, “below the rooster” and so on.              probabilities between any co-occurring word-object pairs.
Thus, the members in an equivalence class of a pattern play                Now association probabilities are estimated by two parts:
the same syntactic role and are replaceable in context of that                     p(m|w) = p(g|w)p(m|w, g)
syntactic pattern. We notice that most equivalence classes                                         X
are not necessarily identical to grammatical categories. Al-                                  =        p(C|w)p(g|C) × p(m|w, g)
though all the members in E588 are object names, E586 con-                                          C
tains not only object names (apples, flowers,etc.) but also                where a new variable C represents the syntactic class of a
pronouns (they,these,etc.). This is because the unsupervised               word. The model uses both the syntactic class of a word
method here can induce only partial and not-precise knowl-                 p(C|w) and the semantic property of the syntactic class
edge from a limited amount of data. From a developmental                   p(g|C) to estimate the probability that this word refers to
                                                                       926

Figure 1: Learning syntax-semantics mappings. Upper left: the syntax learning process categorizes words into several syntactic groups.
Upper right: the word-learning process estimates the association probabilities of any co-occurring word-object pairs represented by cells in
the figure. White color means high association probabilities and dark color means low association probabilities. Bottom: the integration of
semantic and syntactic knowledge results in p(g|C) – the semantic property of each syntactic class. p(g|C) can then be used to improve the
estimates of word-object association probabilities by considering semantic properties of other words in the same groups.
an object kind. In practice, a word could appear in more                the above section. The other approach uses syntax-semantics
than one equivalence (syntactic) classes, each of which is as-          mappings to facilitate word learning. In both approaches, a
sociated with a linguistic pattern. Therefore, p(C|w) is the            lexical item L(mj ⇔ wi ) is discovered based on both asso-
probability that a word w belongs to a syntactic class C and
P                                                                       ciation probabilities of a word-object pair and the number of
    p(C|w) = 1. The probability that a word is in a specific            their co-occurring times:
class can be estimated based on normalizing the occurrences                    L(mj ⇔ wi ) = p(mj |wi ) × (# < mi , wj >)
of the word in that pattern:       # < Ci , wj >                           Both models can then set up a threshold to select a set of
                 p(Ci |wj ) =                                           word-object pairs from all the co-occurring ones in the asso-
                                        # wj
   Meanwhile, p(g|C) is the probability that the words in a             ciation matrix. Two metrics are used to evaluate the word-
syntactic class C refer to object-kind categories, which is             learning performances for these two approaches: (1) word-
jointly determined by the association probabilities of all the          learning accuracy measures the proportion of selected pairs
words in this class:                                                    that are actually correct and (2) word-learning completeness
                         1 X                                            measures the proportion of correct pairs in the data that a
       p(gi |Ci ) =                     max        p(m|w, g)            model successfully selects. The choice of different thresh-
                       |Ci |       m∈Y ;m6=N ON
                             w∈Ci                                       olds leads to different values in accuracy and completeness.
   Figure 1 illustrates the syntax-semantics mapping mecha-             To compare these two approaches, we make one metric con-
nism with examples. If a word is in a syntactic class wherein           stant and measure the difference in the other metric. In one
other words have high association probabilities to objects, the         measure as shown on the left of Figure 2, the completeness
probability that this word also associates with an object kind          percentage is fixed and we show that encoding syntactic cues
would increase. Similarly, if a word is syntactically grouped           improve accuracy. Similarly, the completeness in the statisti-
with other words without semantic mappings (such as func-               cal and syntactic model is better than that of the purely statis-
tion words), it is less likely that the word refers to an object        tical model when the accuracy is fixed.
kind. Overall, p(m|w) is determined not only by p(m|w, g)                  Table 4 shows the top 25 word-object pairs selected by two
but also the semantic properties of all the syntactic classes           models as a comparison. 15 word-object pairs are selected
that this word belongs to.                                              by both models and all of them are correct except one pair
                   Experimental Results                                 “climbs”–boy. For the pairs (marked by *) selected by the
We applied the same data set to two learning approaches.                statistical model only, 2 out 10 are correct. One crucial rea-
One is purely based on statistical learning as described in             son that most of those pairs are incorrect is that some function
                                                                    927

     1                                     1                                   Table 4: The top 25 lexical items learned in these two meth-
                                                                               ods. The marked items are word-object pairs that are selected
   0.8                                   0.8                                   by one model but not the other. The bold words are incorrect.
                                                                                       statistical                    statistical+ syntactic
   0.6                                   0.6
                                                                                       “tree” – tree                  “dog” – dog
   0.4
                                                                                       “bear” – bear             ◦ “pig” – pig
                                         0.4
                                                                                       “dog” – dog                    “tree” – tree
   0.2                                   0.2                                           “bird” – bird             ◦ “sheep” – sheep
                                                                                   * “make” – boy                     “bear” – bear
     0                                     0                                           “flower” – flower         ◦ “book” – book
           statisitcal     statisitcal +     statistical     statistical +
                             syntactic                         syntactic               “bowl” – bowl                  “flower” – flower
               (a) completeness = 0.6           (b) accuracy = 0.6                     “horse” – horse                “bed” – bed
Figure 2: A comparison of statistical learning and statistical                         “climbs” – boy            ◦ “crib” – crib
learning with syntactic cues. Left: With a fixed value of complete-
ness, the accuracy of our proposed model is much better than that of               * “pond” – pond                    “bowl” – bowl
the statistical model. Right: With a fixed accuracy, the completeness                  “rooster” – rooster       ◦ “hat” – hat
of our model is significantly better than that of the statistical model.           * “finally” – bed                  “climbs” – boy
words happen to frequently co-occur with some specific ob-                             “sun” – sun                    “rooster” – rooster
jects (but not other objects). Therefore, both their association                       “snake” – snake                “horse” – horse
probabilities and their numbers of co-occurrences are rela-                        * “getting” – book            ◦ “chicken” – chicken
tively high. In contrast, the statistical and syntactic approach                   * “umbrella” – umbrella            “bird” – bird
improved the performance by removing those pairs from the                              “girl” – girl             ◦ “rattle” – rattle
top items in its list based on the syntactic roles of those func-                      “dad” – dad                    “snake” – snake
tion words. As shown in Table 4, 8 out 10 pairs (marked by
                                                                                   * “all” – cat                 ◦ “picnic” – basket
◦) selected by our model are correct. Overall, these two lists
provide a concrete example of the differences between these                        * “at” – crib                 ◦ “duck” – duck
two approaches.                                                                        “bed” – bed                    “sun” – sun
                                                                                   * “these” – hen                    “boat” – boat
                         General Discussion                                        * “going” – duck                   “girl” – girl
Three statistical learning mechanisms are introduced and im-                           “boat” – boat                  “dad” – dad
plemented in the model: statistical word learning to build                         * “see” – boy                 ◦ “blanket” – grass
word-to-world mappings, statistical syntax learning to ac-
quire linguistic patterns, and word learning with syntax-
                                                                               two grammars in an artificial language, 12-month-olds could
semantics mappings. This section discusses relevant exper-
                                                                               discriminate new strings from the two grammars, suggest-
imental studies and findings that support the cognitive plausi-
                                                                               ing that statistical learning might play a role also in ac-
bility of those learning mechanisms.
                                                                               quiring rudimentary syntax (the ordering of words, etc.).
Statistical word learning One of the most important find-                      Meanwhile, recent computational studies show that structural
ings in language acquisition is that humans are sensitive to                   knowledge can be acquired through relatively simple com-
statistical regularities in language and are able to acquire                   putational mechanisms (Redington, Chater, & Finch, 1998;
linguistic knowledge based on statistical learning. Saffran,                   Mintz, Newport, & Bever, 2002; Solan et al., 2005). For
Aslin, and Newport (1996) demonstrated that 8-month-old                        instance, Mintz et al. (2002) showed that grammatical cate-
infants are able to find word boundaries in an artificial lan-                 gories of nouns and verbs can be acquired through calculating
guage based only on statistical regularities. Can statistical                  distributions over words. Specifically, a distributional analy-
learning also account for word acquisition? The kind of sta-                   sis was developed in which nouns and verbs were success-
tistical learning requested in word-to-world mappings would                    fully categorized based on their co-occurrence patterns with
be quite different from statistical speech segmentation – not                  surrounding words. The syntax learning mechanism we ap-
simply count the frequency or condition probabilities of word                  plied and integrated in our model is another example of how
or syllables in a speech stream, but compute co-occurring sta-                 grammatical structures can be deduced in unsupervised mode
tistical regularities across language and extralinguistic con-                 (Solan et al., 2005). Putting together, statistical syntax learn-
texts. Nonetheless, our recent findings (Yu & Smith, sub-                      ing is also likely to be a fundamental mechanism in language
mitted) show that both adult and children are sensitive to                     acquisition.
statistical regularities. When presented with multiple trials,                 Interaction between word and syntax learning processes
each containing multiple pictures and names with no infor-                     Recent empirical studies have suggested that syntactic cues
mation about which picture is paired with which name, both                     could play a crucial role in the course of lexical develop-
adults and even 12-month old babies are able to build correct                  ment. Gleitman (1990) demonstrated that learners use evi-
picture-name pairings. The computational model of statistical                  dence from the syntactic structure in which verb occurs to
word learning in this work demonstrates how such learning                      help verb learning. The role of syntactic cues is particularly
mechanism works.                                                               useful when an extralinguistic scene is insufficient for dis-
Statistical syntax learning Gomez and Gerken (1999)                            covering the meaning of a verb. For instance, there are paired
have shown that after less than 2-min exposure to one of                       verbs that most often share the same extralinguistic context,
                                                                           928

such as give and receive, the situation that statistical regu-       any significant changes of underlying statistical machinery.
larities in cross-situational observation cannot help at all to                               References
disentangle the meanings of these two verbs. More recently,          Bloom, P. (1994). Possible names: The role of syntax-
Snedeker and Gleitman (2004) showed that the knowledge                 semantics mappings in the acquisition of nominals. In
of known nouns co-occurred in construction with a verb can             L. Gleitman & B. Landau (Eds.), The acquisition of the
also significantly improve verb learning. To sum up, behav-            lexicon (p. 297-329). Cambridge, MA: MIT Press.
ioral studies have shown that syntactic cues can facilitate lex-     Brown, P. F., Pietra, S., Pietra, V., & Mercer, R. L.
ical learning. The open questions, however, are how the cor-           (1994). The mathematics of statistical machine trans-
respondences between syntax and semantics can be learned               lation:parameter estimation. Computational Linguistics,
and what kind of learning mechanisms can support the use of            19(2), 263-311.
syntax-semantics mappings to bootstrap word learning.                Colunga, E., & Smith, L. (2005). A connectionist account
   We suggest that since statistical word learning and syntax          of the object-substance distinction. Psychological Review,
learning may function simultaneously during human devel-               112, 347-382.
opment, the integration of the results from these two concur-        Gleitman, L. (1990). The structural sources of verb meanings.
rent learning processes may lead to the emergence of syntax-           Language Acquisition, 1(1), 1-55.
                                                                     Gomez, R. L., & Gerken, L. (1999). Artificial grammar learn-
semantics mappings, which in turn may generate new sta-
                                                                       ing by 1-year-olds leads to specific and abstract knowledge.
tistical regularities. A statistical mechanism acquiring new
                                                                       Cognition, 70, 109-135.
knowledge from the new regularities can then apply this              Li, P., Farkas, I., & MacWhinney, B. (2004). Early lexical
new knowledge to bootstrap both syntax and lexical learn-              development in a self-organizing neural networks. Neural
ing processes. The model we implemented demonstrates                   Networks, 17, 1345-1362.
such learning system. If word learners could discover both           Mintz, T. H., Newport, E. L., & Bever, T. G. (2002). The
the grammatical categories of words through a syntax learn-            distributional structure of grammatical categories in pseech
ing process and the semantic categories from a word learn-             to young children. Cognitive Science, 26, 393-424.
ing process, they might then be able to build mappings be-           Pinker, S. (1989). Learnability and cognition. Cambridge,
tween syntactic and semantic categories. The mappings can              MA: MIT Press.
then be directly utilized to facilitate word learning because        Plunkett, K., Sinha, C., Miller, M., & Strandsby, O. (1992).
the model is able to estimate semantic properties of a word            Symbol grounding or the emergence of symbols? vocabu-
based on both its syntactic and semantic roles. Our simula-            lary growth in children and a connectionist net. Connection
tion results support this hypothesis by showing that syntac-           Science, 4, 293-312.
tic cues can significantly improve word learning. But is the         Redington, M., Chater, N., & Finch, S. (1998). Distribu-
proposed learning mechanism at all cognitively plausible? A            tional information: A powerful cue for acquiring syntactic
new finding by Saffran and Wilson (2003) demonstrated that             categories. Cognitive Science, 22(4), 425-469.
12-month-old infants were able to perform two kinds of sta-          Regier, T. (2005). The emergence of words: Attentional
tistical computations in the same sound sequence simultane-            learning in form and meaning. cognitive science, 29(6),
ously when they were exposed to multiword utterances: one              819-865.
is to segment the speech into individual words and the other         Saffran, J., Aslin, R., & Newport, E. (1996). Statistical learn-
is to find the orderings of those words. The finding suggests          ing by 8-month old infants. Science, 274, 1926-1928.
that different learning processes may work concurrently and          Saffran, J., & Wilson, D. P. (2003). From syllables to syntax:
therefore there may be interactive effects between those learn-        Multilevel statistical learning by 12-month-old infants. In-
ing processes. The present study demonstrates how two key              fancy, 4(2), 273-284.
                                                                     Siskind, J. M. (1992). Naive physics, event perception, lexical
learning processes in language acquisition – syntax learning
                                                                       semantics and language acquisition. Unpublished doctoral
and word learning – may bootstrap each other through uni-
                                                                       dissertation, MIT.
versal syntax-semantics mappings.                                    Siskind, J. M. (1996). A computational study of cross-
                          Conclusion                                   situational techniques for learning word-to-meaning map-
This paper presents a computational model to simulate the              pings. Cognition, 61, 39-61.
emergence and acquisition of syntax-semantics mappings and           Snedeker, J., & Gleitman, L. (2004). Why it is hard to label
the effects of the mappings on word learning. Using realistic          our concepts. In Hall & S. Waxman (Eds.), Weaving a
data collected from natural child-parent interaction (picture          lexicon (p. 257-294). Cambridge,MA: MIT Press.
                                                                     Solan, Z., Horn, D., Ruppin, E., & Edelman, S. (2005). Un-
book reading, etc.), we demonstrate that syntactic cues can be         supervised learning of natural languages. in Proc. Natl.
seamlessly integrated in and bootstrap statistical word learn-         Acad. Sci, 102(33), 11629-11634.
ing. As first steps towards understanding the learning mech-         Tenenbaum, J., & Xu, F. (2000). Word learning as bayesian
anisms that support syntax-semantics interfaces, the present           inference. In L. Gleitman & A. Joshi (Eds.), Proceeding
study focuses on learning a specific kind of words – ob-               22nd annual conference of cognitive science society (p.
ject names. Nonetheless, the proposed learning mechanism               517-522). Mahwah,NJ: ErlBaum.
doesn’t take advantage of the properties of specific grammat-        Yu, C., Ballard, D. H., & Aslin, R. N. (2005). The role of
ical or semantic categories, but is purely based on statistical        embodied intention in early lexical acquisition. Cognitive
regularities within language, across language and extralin-            Science, 29(6), 961-1005.
guistic contexts, and interactive effects of these two. There-       Yu, C., & Smith, L. B. (submitted). Statistical cross-
fore, this general learning mechanism has a potential to ex-           situational learning to build word-to-world mapping.
tend to more grammatical and semantic categories without
                                                                 929

