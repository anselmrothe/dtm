UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Compositional Neural-network Solution to Prime-number Testing
Permalink
https://escholarship.org/uc/item/5sg7n4ww
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Egri, Laszlo
Shultz, Thomas R.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

               A Compositional Neural-network Solution to Prime-number Testing
                                             László Egri (laszlo.egri@mail.mcgill.ca)
                               School of Computer Science, McGill University, 3480 University Street
                                                    Montreal, QC H3A 2B4 Canada
                                          Thomas R. Shultz (thomas.shultz@mcgill.ca)
              Department of Psychology and School of Computer Science, McGill University, 1205 Penfield Avenue
                                                    Montreal, QC H3A 1B1 Canada
                              Abstract                               relations between elements rather than mere attributes of the
                                                                     elements being compared (Gentner & Markman, 1993).
   A long-standing difficulty for connectionism has been to             A symbolic expression exhibits what is called
   implement compositionality, the idea of building a knowledge      concatenative compositionality, which means that the
   representation out of components such that the meaning arises     expression incorporates its constituents without changing
   from the meanings of the individual components and how            them, something that is supposed to be impossible for neural
   they are combined. Here we show how a neural-learning             networks (Fodor & Pylyshyn, 1988). In response to this
   algorithm, knowledge-based cascade-correlation (KBCC),            challenge, it has been argued that current neural networks
   creates a compositional representation of the prime-number
                                                                     exhibit a unique functional form of compositionality that
   concept and uses this representation to decide whether its
   input n is a prime number or not. KBCC conformed to a basic
                                                                     may be able to model the compositional character of
   prime-number testing algorithm by recruiting source networks      cognition even if the constituents are altered when
   representing division by prime numbers in order from              composed into a complex expression (van Gelder, 1990).
   smallest to largest prime divisor up to √n. KBCC learned how      Perhaps the original constituents could be retrieved from a
   to test prime numbers faster and generalized better to            complex expression by natural connectionist means. For
   untrained numbers than did similar knowledge-free neural          example, an encoder network can learn to encode simple
   learners. The results demonstrate that neural networks can        syntactic trees on distributed hidden unit representations and
   learn to perform in a compositional manner and underscore         then decode them back into the same syntactic trees at the
   the importance of basing learning on existing knowledge.          outputs (Pollack, 1990). We argue later in this paper that a
                                                                     newer connectionist algorithm implementing knowledge-
   Keywords: knowledge-based learning; compositionality;
   KBCC; prime-number testing.
                                                                     based learning can exhibit concatenative compositionality.
                                                                      Prime numbers
                          Introduction
                                                                      One area in which to study the issue of compositionality is
                                                                      that of testing for prime numbers. Is an integer n a prime
Compositionality                                                      number or not? A prime number is a natural number that has
Compositionality is the idea of building a problem                    exactly two different divisors, 1 and itself. A number that
representation out of components such that the meaning                has more than two divisors is composite. The number 1 is
comes from the meanings of the components and the way                 neither composite nor prime.
they are combined (Fodor & Pylyshyn, 1988). In a                        At first glance, the most intuitive way to test whether n is
compositional representation, there is a distinction between          a prime number is by checking whether n is divisible by any
structurally atomic and molecular representations.                   numbers between 2 and n – 1. However, this test can be
Structurally molecular representations have syntactic                much more efficient (Nagell, 1951). First, because n is
constituents that are themselves either structurally molecular       divisible by a number in the interval [2, n - 1] if and only if
or are structurally atomic, and the semantic content of a            it is divisible by a number in the interval [2, √n]1, it is
representation is a function of the semantic contents of its          sufficient to check whether n is divisible by any number in
syntactic parts, together with its constituent structure. For         that interval [2, √n]. Second, if n is divisible by a composite
example, consider the sentence The princess kissed the frog.         number in the interval [2, √n], then n is also divisible by all
The meaning of this sentence can be computed from the                prime factors of that composite number. Therefore, it is
meanings (semantic content) of the units the, princess,              sufficient to check whether n is divisible by any primes in
kissed and frog, and their positions in the sentence                 the interval [2, √n]. For example, 105 is a composite number
(constituent structure). Changing the word order changes the         because it is divisible by 3, a prime number in the interval
meaning, so that the sentence The frog kissed the princess           [2, 10].
conveys a different idea.                                               In short, to test whether a number n is a prime number,
   Compositionality exists in a variety of psychological             one could divide n by all the primes in increasing order, up
functions, not only in language (Bregman, 1977). For
example, related arguments were made about similarity                 1
                                                                        Throughout this paper, √n denotes the integer part of a real
comparisons sometimes requiring a focus on structural                number, e.g., 2.3 is interpreted as 2.
                                                                 1263

to the integer part of the square root of n, stopping when a           Net input x to a unit j is computed as a sum of products of
successful divisor is found. Thus, the statement that n is          sending-unit activations yi and connection weights wij:
prime can be represented in a compositional fashion by the
Boolean expression: ¬(n is divisible by 2) ∧ ¬(n is divisible
                                                                        xj = ∑ i
                                                                                 wij y i                          Equation 2
by 3) ∧ … ∧ ¬(n is divisible by √n). In this representation,
                                                                       CC learning begins with reducing network error by
propositions of the form n is divisible by x correspond to
                                                                    training weights feeding the output layer. Training these
syntactic units. These syntactic units have their own
semantic content (i.e., a number n is divisible by number x).       output weights is the so-called output phase of CC. Any
                                                                    single-layer network learning algorithm could be used to
The semantic content of the overall expression is a function
                                                                    train output weights but the usual choice is the Quickprop
of the syntactic units, together with the constituent structure.
The constituent structure is expressed by the Boolean               algorithm (Fahlman, 1988). Quickprop works like ordinary
                                                                    back-propagation learning except that it employs the second
operators not and and.
                                                                    (curvature), as well as the first (slope), derivative of the
   To demonstrate that constituent structure plays an
essential role in the semantic content of the overall               error surface to adjust connection weights, thus enabling it
                                                                    to reduce error more aggressively (Shultz, 2003). In
expression, consider a different Boolean expression: (n is
                                                                    Quickprop, weight change is proportional to the negative of
divisible by 2) ∧ ¬(n is divisible by 3) ∧ (n is divisible by 5)
∧ ¬(n is divisible by 7) …. The syntactic parts are exactly         slope (the rate at which error changes with a change in
                                                                    weight) divided by curvature (the rate at which slope
the same as before but the constituent structure is different.
                                                                    changes with a change in weight).
Accordingly, the meaning of the overall expression has
changed. Here the expression means that n is divisible by              When output training ceases to reduce error or if error is
                                                                    still too high after a specified number of training cycles
prime numbers in odd positions, but is not divisible by
                                                                    (epochs), training shifts to input phase. In input phase, a
prime numbers in even positions in the list of prime
numbers.                                                            single new hidden unit is trained and added to the network.
                                                                    After the new unit is added, its input weights are frozen and
   So far the only psychological evidence on how ordinary
                                                                    a new output phase begins. This two-phase cycle continues
people test prime numbers comes from continued
educational use of the ancient sieve of Eratosthenes (circa         until error is reduced to a satisfactory level, meaning that all
                                                                    output activations are within a specified range of their target
200 BC). In number theory, a sieve is a process of
                                                                    values, known as the score-threshold.
successively eliminating members of a list according to a set
of rules such that only some members remain. With                      In input phase, candidate hidden units receiving incoming
                                                                    connections from input units and from pre-existing hidden
Eratosthenes’ method, all integers from 1 to n are inscribed
                                                                    units are trained. In this phase, candidate output is not
on paper. Because 1 is not a prime, it is crossed out. The
smallest remaining number, 2 is a prime number. All other           connected to any other unit. The incoming weights of the
                                                                    candidate units are initialized with small random weights.
multiples of 2 are crossed out because they are composites.
                                                                    Next, the set of training examples is cycled through (a single
The next smallest remaining number 3 is the next prime
number. All other multiples of 3 are similarly crossed out          epoch) and the weights feeding the candidates are adjusted
                                                                    with Quickprop in order to increase S, the magnitude of a
because they are composite numbers. This process is
                                                                    modified correlation between a candidate unit’s activation
repeated until the next divisor is greater than √n, by which
time all the composites have been crossed out. Numbers not          and network error observed at the output units. Input phase
                                                                    ends when S values cease to improve. At this point, the
crossed out comprise all the primes from 2 to n. An
                                                                    candidate unit with the highest absolute correlation is
interesting feature of this method is that the smaller the
prime number, the more composite numbers it eliminates.             connected to all the output units with small random weights
                                                                    and a new output phase begins. Usually eight candidates are
                                                                    trained in parallel, the best one is installed, and the rest of
Cascade-correlation
                                                                    the candidates are discarded.
Cascade-correlation (CC) is a neural-network learning                  The original version of CC is the one just described and it
algorithm that constructs a feed-forward network as it learns       is sometimes called deep CC because each new hidden unit
(Fahlman & Lebiere, 1990). A CC network initially has no            is installed on its own separate layer, creating a fairly deep
hidden units; there is only a randomly-determined direct            network topology. However, the version used here is
connection from each input to each output unit. Throughout          sibling-descendant CC (SDCC). In SDCC, two types of
this paper, whenever a connection weight is randomized, it          candidate units are trained. Four of the eight candidate units
is set to a random value in the range [-0.5, 0.5] in a uniform      are trained as descendant units like in deep CC (Baluja &
distribution.                                                       Fahlman, 1994). The other four candidate units are trained
   All the networks in this paper are built out of sigmoid          as sibling units without receiving any connections from the
units with an offset of -0.5 to center activation around 0:         previous layer. Thus, when a sibling unit is installed,
             1                                                      network depth does not increase.
    yj =      −x
                  − 0.5                       Equation 1               Sibling and descendant candidate hidden units compete
         1+ e j                                                     with each other for being recruited. The S values of
where xj is net input to unit j, e is the base of the natural       descendant candidates are typically multiplied by 0.8 to
logarithm, and yj is resulting activation value of unit j.          avoid overly-deep networks resulting from a natural bias to
                                                                    recruit the more complex descendant candidates, having
                                                                    extra weights from the current highest layer of hidden units.
                                                                1264

This 0.8 multiplier has been observed to reduce network            the shape, KBCC recruited source networks representing a
depth without harming network generalization (Baluja &             vertical rectangle and a horizontal rectangle to learn a target
Fahlman, 1994).                                                    cross shape (Shultz & Rivest, 2001). Also KBCC recruited
   A resulting SDCC network then has multiple layers with          source knowledge of small checkerboard shapes to learn
one or more units in each hidden layer. SDCC networks              target patterns of larger checkerboard shapes and source
have been found to perform similarly to CC networks in             knowledge of small parity problems to learn target problems
psychology simulations but with fewer layers and more              of larger parity problems (Rivest & Shultz, 2004).
interesting varieties of network topology (Shultz, 2006).          Knowledge-based learning was considerably faster than
                                                                   learning without knowledge, and faster with relevant than
Knowledge-based cascade-correlation                                with irrelevant knowledge.
Knowledge-based cascade-correlation (KBCC) is a natural
extension of CC in which the target network can recruit not                                  Method
only single hidden units, but also previously-learned
networks with possibly multiple inputs and outputs (Shultz         Primality testing
& Rivest, 2001). We refer to previously-learned networks in        Given an input number in the range [21, 360], a target
the candidate pool as source networks and to networks              network had to indicate whether the input was prime or
created by KBCC as target networks.                                composite. There were nine input units because the largest
   Source networks can be installed indirectly or directly.        number 360 requires nine digits in binary format. There was
When they are installed indirectly, each input of the source       one binary output unit. A target output value of 0.5
network has an incoming connection from all inputs of the          indicated that the input was prime, and -0.5 indicated that
target network, from all outputs of previously installed           the input was composite. The score-threshold was 0.4.
unit(s) and network(s), and from the bias unit. If a hidden        Thirty-four randomly-chosen problem instances not used in
unit or network was installed as a sibling, there is no            training (10% of the total) were used to test generalization
connection from the current highest layer of hidden units.         ability of the networks.
   When source networks are installed directly, weights
connecting the corresponding inputs of the target network
                                                                   Source networks
and the source network are initialized with weights of 1.0
and the other connection weights are initialized with              Nineteen SDCC source networks were trained in the
weights of 0.0. This direct connection scheme implies that         following way. The input to each source network was a
the number of inputs of the target and source networks must        number encoded in binary in the range [2, 360]. The binary
be the same. Direct connection enables the use of relevant         digit 1 was encoded as 0.5, and the binary digit 0 as -0.5.
knowledge without much additional training.                        Nine input units were needed because the largest number
   In general, all networks have a bias unit sending a             360 has nine digits in binary format. Each source network
trainable connection to every other unit and recruited             was trained to divide its input by a specific number. There
network except for the input units. The bias unit has a            was a network to divide by 2, a network to divide by 3, and
constant activation value of 1.0. All connection weights are       so on up to 20. Each source network had one output unit.
initialized with random values.                                    The target output was 0.5 if the input was divisible by the
   An example KBCC network from the present project is             number the given network was responsible for and
portrayed in Figure 1. This network has a bias unit, nine          otherwise the target output was -0.5. These source networks
input units, and one output unit. It recruited six source          were trained with a relatively low score-threshold of 0.01 to
networks, installing all of them as siblings on the same           ensure that they provided a clear output. We refer to a
layer. Further details about KBCC can be found elsewhere           source network that was trained to divide by number x as a
(Shultz & Rivest, 2001).
                                                                  division-by-x network.
   A major advantage of KBCC over previous knowledge-
based learning algorithms is that, with indirect connections,
KBCC can recruit any function that predicts network error,
                                                                   Conditions
without regard to matching the number and function of input        There was an experimental and two control conditions. In
units between the source and target networks.                      the experimental condition, 20 sibling-descendant KBCC
   As noted, a long-standing difficulty for connectionism has      (SD-KBCC) target networks were trained on primality
been to implement compositionality. A goal of this paper is        testing. The set of candidate units and networks SD-KBCC
to demonstrate that KBCC creates a compositional                   could recruit from is called the candidate pool. The
representation of the prime-number concept and uses this           candidate pool here contained the 19 divisor (source)
representation to decide whether its input is a prime number       networks described earlier. Divisor networks could be
or not. We hypothesized that learning a compositional              connected only directly, either as a sibling or descendant of
representation is faster, results in better generalization to      the current highest layer. The candidate pool also contained
input numbers not used in training, and requires less              four sigmoid units that likewise could be installed as a
recruitment relative to knowledge-free SDCC learning.              sibling and four that could be installed as a descendant.
   Some evidence already exists for compositionality in               There were also 32 randomized networks in the candidate
KBCC solutions. In a task requiring a network to distinguish       pool that were created in the following way. Four trained
points inside a two-dimensional shape from points outside          divisor networks were chosen randomly. The weights in the
                                                              1265

networks were reset to random values, keeping only the             Knowledge-representation analysis
structures of the divisor networks. In effect, we erased the
                                                                   KBCC networks were subjected to further analysis to
memory in the source networks, but left all the structure
                                                                   discover how they managed their successful performance.
intact. For each randomized network, four copies were
created and put in the candidate pool. The first copy was          Almost all (17 / 20) KBCC networks had the same structure
directly connected and the remaining copies were indirectly        and recruited their sources in the same order. Divisor source
connected. Each copy could be installed as a sibling or a          networks were recruited in the following order: division-by-
descendant. The idea of including these randomized                 3, division-by-5, … , division-by-17 (all primes from 3 to
networks in the candidate pool was to control for                  17). There were three exceptions and in those networks,
complexity of at least some of the recruits. It might be that      only the last two recruitments differed from this pattern.
KBCC would recruit the most complex source, regardless of             Weights feeding the recruited source networks were
whether it contained relevant knowledge. Recruiting trained        extracted from the 17 typical KBCC networks. For each
sources over randomized sources would rule out a mere              source network, weights connecting the input of the target
preference for complexity and demonstrate a preference for         network to the corresponding input of the source network
relevant knowledge.                                                had values close to 1, and all other weights feeding the
   In one control condition, 20 SDCC networks were trained         inputs of the source network had values close to 0.
on the primality-testing problem. The candidate pool                  Weights going to the output units (output weights) of the
contained four sigmoid units that could be installed as a          target network were also extracted from the 17 typical
sibling and four sigmoid units that could be installed as a        KBCC networks. There was one weight from the bias unit,
descendant. Comparison with KBCC networks provided an              nine weights from the input units, and six weights from the
assessment of the impact of prior knowledge, in this case          recruited sources. Because weight patterns of the networks
knowledge of divisibility with divisors between 2 and 20.          were very similar, output weights were averaged over the 17
   In another control condition, randomized KBCC, 20               typical networks. The mean output weights from the
KBCC networks were trained on the primality-testing                recruited source networks, bias unit, and last input unit are
problem. Their candidate pool was exactly the same as that         shown in Figure 1.
in the KBCC condition except that connection weights in                  -20.3                  9.0
the 19 divisor networks were reset to random values.
Comparison with ordinary KBCC provided another                                                           Output
assessment of the impact of prior knowledge but with an
additional control for complexity of the recruits. In this
condition, there were networks in the candidate pool that
                                                                        -8.2 -7.7 -7.0 -7.0 -6.7 -1.9    Weights from recruits
were just as complex as the candidates in the KBCC
condition, but they contained no knowledge of divisibility
because their connection weights were randomized.                           3 5 7 11 13 17               Recruits (divide by)
                          Results
Performance comparisons                                                  Bias 1 2 3 4 5 6 7 8 9          Inputs
Numbers of epochs to learn, recruits, and layers, as well as
percent success on generalization were each subjected to a           Figure 1: KBCC network with mean output weights from
factorial ANOVA with condition as the single factor.                      recruited source networks, bias unit, and input 9.
Results are presented in Table 1 in terms of F ratios and
comparison of means using Tukey’s HSD test at p < .05.                We found that KBCC networks could also be trained and
KBCC networks learned in fewer epochs than did                    tested on the primality of the same input numbers used in
randomized KBCC networks, which in turn learned faster            their source networks. If the KBCC networks were trained
than knowledge-free SDCC networks. The same pattern               on numbers in the interval [2, 360], they took longer to learn
held for number of recruits during learning. KBCC and             (M = 437 epochs) and recruited more divisor networks (M =
randomized KBCC created shallower networks than did               8.05), but they learned their training patterns perfectly and
SDCC. Generalization to untrained numbers was better for          generalized correctly to 98.2% of their test patterns. The
KBCC than for the other two network types.                        order of their divisor-network recruits was a bit more
                                                                  variable, but still correlated highly with size of divisor (M =
                                                                  .86). All but 3 of these 20 correlations were significantly
        Table 1: Comparison of performance means.
                                                                  positive at p < .05.
                          Condition means          F(2, 57)
  Variable          KBCC Random           SDCC
                                                                                            Discussion
  Epochs to learn      335 < 1015 < 1122                347
  Recruits             6.00 < 8.60 < 9.95              43.5        Results showed that KBCC networks learned faster and with
  Network layers       1.00 ≈ 1.25 < 2.35              26.4        fewer recruits than did either randomized KBCC networks
  Generalization       98.7 > 75.1 ≈ 73.1               112        or knowledge-free SDCC networks. This confirms previous
                                                              1266

research showing the superior learning speed of knowledge-           positive value (because the product of two negatives is
based networks (Shultz & Rivest, 2001), and shows that the           positive). In addition, if the input is a prime number then its
key advantage of KBCC lies with its knowledge and not the            last digit is 1, coded here as 0.5. Because the weight from
mere complexity of its topology. Randomized KBCC                     this last input to the output averaged 9.0, this further
networks of equal complexity as KBCC networks learned                increases the output value. More precisely, the mean net
faster and with fewer recruits than did SDCC networks,               input to the output unit (excluding the smallish direct input-
showing that complexity alone has some beneficial effect.            output connections from inputs 1-8) for a prime-number
   Generalization, often considered to be an essential               input can be calculated according to Equation 2 as (1 x -
characteristic of successful learning, was nearly perfect in         20.3) + (0.5 x 9.0) + (-0.5 x -8.2) + (-0.5 x -7.7) + (-0.5 x -
KBCC networks and considerably better than in either                 7.0) + (-0.5 x -7.0) + (-0.5 x -6.7) + (-0.5 x -1.9) = 3.45.
                                                                     This net-input value is fed into the sigmoid output unit,
randomized KBCC or SDCC networks. This indicates that
                                                                     according to Equation 1, yielding a value close to 0.5, thus,
knowledge-based learning in KBCC differs not only in
                                                                     indicating that the input is a prime number.
speed and efficiency but also in quality of learning.                   Notice that if any of the recruited networks indicate that
   At first glance, it may seem that randomized-KBCC and             the input is divisible by some number (meaning that the
SDCC networks generalized rather well on their test                  input number is composite), then at least one of the -0.5
problems at 75% and 73% success, respectively. However,              activation values in this computation becomes 0.5. Because
because only 19% of the integers in the range [21, 360] are          all the weights from the source networks to the output of the
prime numbers, a system can do well (81% success) by                 target network are close to -7, the overall net input decreases
guessing that every integer in that range is composite. Thus,        by about 3.5 and becomes negative, making the output of
learning to test for prime numbers appears to be sufficiently        the network negative, indicating that the input is a
challenging that knowledge-based learning is required for            composite number.
correct generalization. This is the first task we have studied          Precise activation and weight values do not matter much
in which CC or SDCC networks could not eventually catch              for this exposition because the weights were averaged over
up to the learning quality of KBCC networks. In that sense,          all networks and the small, direct input-output weights from
the present results underscore the importance of knowledge-          inputs 1-8 were ignored. For present purposes, we are
based learning. It may be that some tasks can only be                interested only in the average trend of network performance.
learned by building on existing knowledge.                              There is an exception in the size of the recruit output
   It is interesting that KBCC networks recruited only those        weights: the weight connecting the output of the division-
six source networks that were trained to divide by the prime        by-17 network to the output of the target network is -1.9,
numbers from 3 to 17 (3, 5, 7, 11, 13, and 17) and recruited        which is not a value close to -7. This affects only one
them in that precise order from smallest to largest. This is in     composite input number, 17 x 17 = 289. It is likely that
accord with the basic primality-testing method discussed in         KBCC networks handled this particular input of 289 by
the introduction. KBCC target networks avoided recruiting            using the direct input-output weights that we excluded from
division-by-composites networks, any networks with                   the foregoing computation.
divisors greater than √n, even if the divisor was a prime               This interpretation of the network structure is in accord
number, i.e., 19, and randomized source networks.                    with the idea that the internal representation of prime
   The last network recruited was the division-by-17                 numbers in KBCC networks is computationally equivalent
network. The reason for stopping is that composite numbers           to the Boolean expression: ¬(n is divisible by 2) ∧ ¬(n is
less than or equal to 360 always have a prime factor less            divisible by 3) ∧ … ∧ ¬(n is divisible by √n). Therefore
than or equal to 17. The lowest number that has no factor            KBCC represents the prime-number concept in a
less than or equal to 17 is 361 = 19 x 19. In other words, the       compositional way.
primality-testing problem could be solved without                       Astute readers may have noticed one glaring omission
considering any divisors larger than 17.                             from this analysis: KBCC did not ever recruit a division-by-
   The knowledge-representation analysis shows how KBCC              2 source network, even though 2 is a prime number that
combined the six syntactic components (i.e., the source              rules out more composites than any of the larger prime
networks). First recall that the input to the target network         numbers. This does not imply that KBCC networks ignored
was directly delivered to the source networks with weights           divisibility of the input number by 2. Instead, KBCC
of about 1 between corresponding units. With that in mind,           networks noticed that if the last digit of a binary number is 0
let’s attempt to interpret the output weights, i.e., weights         (coded here as -0.5), the number is an even number divisible
entering the output unit.                                            by 2 and thus a composite number. Notice that unlike the
   The bias unit has a large negative influence on the output.       small weights from inputs 1-8 to the output unit, this last
Assume that the input is a prime number. In that case, all           input 9 has a relatively large weight of about 9 to the output
recruited source networks output a value close to -0.5               unit. The computation of 9 x -0.5 = -4.5 would drive the net
because the input number is not divisible by any prime               input to the output unit to be negative, thus allowing a
numbers between 3 and 17. Given these negative activation            conclusion that the input number is a composite.
values on the outputs of the six recruited networks, because            This provides a simple and ingenious solution of the sort
weights from these outputs to the output of the target
                                                                     often noted in humans who don’t need to explicitly divide
network have large negative values, the overall effect of the
                                                                     by 2 to determine whether a decimal-coded number is odd
recruited networks on the target network’s output is a large
                                                                1267

or even – they only need to check whether the last digit is                                Acknowledgments
odd or even. As another example, a number in decimal form             This research was supported by a grant from the Natural
is divisible by 5 if and only if the last decimal digit is either     Sciences and Engineering Research Council of Canada to
0 or 5. In this way, humans can easily tell whether a number          the second author. We are grateful to Yoshio Takane, J-P.
is divisible by 5 without actually dividing that number by 5.         Thivierge, and Frédéric Dandurand for helpful comments.
The results show that KBCC networks achieved an effective
compositional solution to prime-number testing.
   Even more convincing for compositionality would be a
                                                                                               References
demonstration that KBCC networks could learn to do any of             Baluja, S., & Fahlman, S. (1994). Reducing network depth
several tasks with the same component source networks.                  in the cascade-correlation learning architecture (Tech.
This would show that KBCC can build different                           Rep. No. CMU-CS-94-209). Pittsburgh, PA: Carnegie
compositions with the same components to solve different                Mellon University, School of Computer Science.
target tasks. We are currently investigating this possibility.        Bregman, A. S. (1977). Perception and behavior as
   It is important to consider whether the compositional                compositions of ideals. Cognitive Psychology, 9, 250-292.
solution achieved by the present KBCC networks is                     Fahlman, S. E. (1988). Faster-learning variations on back-
concatenative in the sense that the components of the                   propagation. Proceedings of the 1988 Connectionist
composition are preserved. Or, is this compositional                    Models Summer School. Morgan Kaufmann.
solution merely functional as in van Gelder’s (1990)                  Fahlman, S. E., & Lebiere, C. (1990). The cascade-
characterization of Pollack’s (1990) recursive networks for             correlation learning architecture. In D. S. Touretzky (Ed.),
encoding and decoding syntactic trees? Based on how the                 Advances in Neural Information Processing Systems 2.
KBCC algorithm works, we would argue that KBCC’s                        Los Altos, CA: Morgan Kaufmann.
solution does indeed implement a genuine concatenative                Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionsism
compositionality because, when source networks are
                                                                        and cognitive architecture: A critical analysis. Cognition
recruited, their internal structure and content are preserved
                                                                        28, 3-71.
intact. Only weights from the inputs of the target-network to
the inputs of the recruited network and weights from the              Gentner, D., & Markman, A. B. (1993). Analogy –
outputs of recruited networks are trained.                              Watershed or Waterloo? Structural alignment and the
   Besides demonstrating that KBCC networks can learn a                 development of connectionist models of cognition. In S. J.
compositional procedure for prime-number testing, we                    Hanson, J. D. Cowan, & C. L. Giles (Eds.), Advances in
wonder whether KBCC could serve to model the                            neural information processing systems, 5. San Mateo,
psychology of ordinary human performance on this                        CA: Kaufmann.
problem. Apart from pedagogical recommendations to use                Nagell, T. (1951). Introduction to number theory. New
the ancient sieve of Eratosthenes in teaching about prime               York: Wiley.
numbers, there is currently little or no psychological                Pinker, S. (1997). How the mind works. New York: Norton.
analysis to rely on. However, it is interesting that                  Pollack, J. (1990). Recursive distributed representations.
Eratosthenes’ method does bear some interesting similarities            Artificial Intelligence, 46, 77-105.
to the solution learned by KBCC. Both methods order                   Rivest, F., & Shultz, T. R. (2004). Compositionality in a
divisors from small to large and use only prime divisors                knowledge-based constructive learner. Papers from the
below √n. We are currently studying whether people test                 2004 AAAI Fall Symposium, Technical Report FS-04-03,
prime numbers with these constraints.                                   pp. 54-58. AAAI Press: Menlo Park, CA.
   In conclusion, the KBCC learning algorithm creates a               Shultz, T. R. (2003). Computational developmental
compositional representation of the prime number concept                psychology. Cambridge, MA: MIT Press.
and the resulting network uses this representation to decide          Shultz, T. R. (2006). Constructive learning in the modeling
whether the input is a prime or composite number. Further,              of psychological development. In Y. Munakata & M. H.
KBCC’s compositional representation results in faster                   Johnson (Eds.), Processes of change in brain and
learning and better generalization, using less hidden units
                                                                        cognitive development: Attention and performance XXI.
than control networks without knowledge. The claim that
                                                                        Oxford: Oxford University Press.
neural networks cannot handle compositionality appears to
be incorrect, at least when such networks are allowed to              Shultz, T. R., & Rivest, F. (2001). Knowledge-based
recruit previously-learned knowledge. Have we shown that                cascade-correlation: Using knowledge to speed learning.
neural networks can handle all of the compositional tasks               Connection Science, 13, 43-72.
that humans are capable of? Absolutely not, but success on            van Gelder, T. (1990). Compositionality: A connectionist
even one compositional task shows the claim of                          variation on a classical theme. Cognitive Science, 14, 355-
impossibility to be incorrect and suggests that neural                  364.
networks may be able to achieve other kinds of
compositionality.
                                                                 1268

