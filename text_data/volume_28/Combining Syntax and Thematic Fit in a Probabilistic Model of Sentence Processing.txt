UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Combining Syntax and Thematic Fit in a Probabilistic Model of Sentence Processing
Permalink
https://escholarship.org/uc/item/44q4z5g8
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Croker, Matthew
Keller, Frank
Padó, Ulrike
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

         Combining Syntax and Thematic Fit in a Probabilistic Model of Sentence
                                                               Processing
                                                 Ulrike Padó (ulrike@coli.uni-sb.de)
                                                         Computational Linguistics
                                                  Saarland University, 66041 Saarbrücken
                                                 Frank Keller (keller@inf.ed.ac.uk)
                                              School of Informatics, University of Edinburgh
                                               2 Buccleuch Place, Edinburgh EH8 9LW, UK
                                            Matthew Crocker (crocker@coli.uni-sb.de)
                                                         Computational Linguistics
                                                  Saarland University, 66041 Saarbrücken
                              Abstract                                  most plausible structure. If syntactic and semantic prefer-
                                                                        ences conflict, we predict processing difficulty, as reanalysis
   We present a model of human sentence processing that extends         is required. Both the syntactic and the semantic component of
   a standard probabilistic grammar model with a semantic mod-
   ule which computes the thematic fit of verbs and arguments in        our model are automatically trained on annotated corpus data
   a cognitively plausible way. Our model differs from existing         and require no hand-tuning of constraints. We successfully
   probabilistic accounts (e.g., Jurafsky, 1996) by capturing both      model the influence of thematic fit on processing the Main
   syntactic and semantic influences in human sentence process-         Clause/Reduced Relative (MC/RR) ambiguity (which cannot
   ing. It also overcomes limitations of constraint-based mod-          be modelled by standard probabilistic grammar models) and
   els (Spivey-Knowlton, 1996; Narayanan and Jurafsky, 2002),
   as its parameters can be acquired automatically from corpus          the NP/Sentence Complement (NP/S) ambiguity.
   data, and no hand-coding of constraints is required. We evalu-          We begin by introducing probabilistic grammar models
   ate our semantic module against human ratings of thematic fit,       and demonstrate how they fail to capture semantic processing
   and also test the complete model’s performance for two well-         effects in the MC/RR ambiguity. We then review two alterna-
   studied ambiguities from the sentence processing literature.
                                                                        tive constraint-based models that account successfully for this
                                                                        ambiguity by integrating syntactic and semantic information
                          Introduction                                  from different sources. However, methodological and practi-
In the investigation of human sentence processing, the cen-             cal problems exist with these approaches. We then introduce
tral importance of frequency is a recurring theme. A wide               our own model in detail, focusing on our main innovation, the
range of frequencies seem to be used by the human sen-                  semantic module, and evaluate its cognitive plausibility. Fi-
tence processor, including verb frame frequencies (e.g., Gar-           nally, we demonstrate that our model correctly captures pro-
nsey et al., 1997), frequencies of morphological forms (e.g.,           cessing data for the MC/RR and the NP/S ambiguity.
Trueswell, 1996), lexical category frequencies (e.g., Crocker
and Corley, 2002) and structural frequencies (e.g., Brysbaert                                  Previous Models
and Mitchell, 1996). These findings have led to the formu-
lation of a range of probabilistic models that account for fre-         Probabilistic Parsers
quency effects in human sentence processing.                            A standard account of frequency effects in language process-
   In this paper, we review two approaches that have been put           ing is provided by models based on probabilistic grammars
forward in the area of sentence processing: Models based on             (Jurafsky, 1996; Crocker and Brants, 2000). Typically, these
probabilistic grammars (Jurafsky, 1996; Crocker and Brants,             models use a Probabilistic Context-Free Grammar (PCFG) to
2000) and models which integrate a number of constraints                compute the probability of each possible structure given the
into an activation-based model (Spivey-Knowlton, 1996) or a             input sentence. A PCFG consists of a set of context-free rules
Bayesian belief net (Narayanan and Jurafsky, 2002).                     which define the daughter nodes licensed by a mother node in
   Both approaches have drawbacks. Models based on prob-                a phrase structure tree. Each rule is annotated with a probabil-
abilistic grammars are not designed to take into account the            ity which represents the likelihood of expanding the mother
influence of semantic processing. Constraint-based models,              category into the daughter categories. The probability of a
in contrast, require both manual specification of a different           syntactic structure (parse tree) T is defined as the product of
set of constraints for each phenomenon and the compilation              the probabilities of all rules applied in generating T . An ex-
of parameters from many, often heterogeneous, sources.                  ample PCFG is given in Fig. 1. This grammar can be used
   In this paper, we introduce a new model of sentence pro-             to generate the parse tree shown underneath, with probability
cessing which extends probabilistic grammar-based models.               P(T ) = .105. Algorithms exist to efficiently compute PCFG
We add a semantic component which evaluates the plausibil-              parse probabilities on a word-by-word basis (Stolcke, 1995).
ity of each structure the parser generates, on the basis of the            In order to incrementally predict processing difficulty,
thematic fit between a verb and its arguments. The model                complexity measures can be defined based on the probabil-
identifies one syntactically most likely and one semantically           ities of all parses licensed by a PCFG. The Surprisal account
                                                                    657

                          S       →        NP VP               1.0                           Input                   Structure    Flip
                        NP        →        DT NN               1.0
                        VP        →        V NP                 .6
                        VP        →        V                    .4                           The                        MC         no
                         V        →        fired                .7                           employer/employee          MC         no
                         V        →        left                 .3
                        DT        →        the                 1.0                           fired                      MC         no
                       NN         →        employer             .5
                       NN         →        employee             .5                           by                          RR       yes
                                 S                                               Figure 2: Preferred structure as predicted incrementally by a
                                                                                 PCFG-based model for an item from McRae et al. (1998).
                 NP                             VP
             DT        NN              V                 NP
                                                                                 (1) The employer fired by the owner was jobless
             The    employer         fired        DT            NN               At the verb fired, both a main clause continuation (e.g. as
                                                   the       employee            the employer fired the employee) and the ultimately correct
                                                                                 reduced relative continuation are possible.
              P(T ) = 1.0 · 1.0 · 1.0 · .5 · .6 · .7 · 1.0 · 1.0 · .5 = .105
                                                                                    Fig. 2 shows the structural predictions made for this sen-
   Figure 1: Example of a PCFG and one tree it generates                         tence by an incremental PCFG-based parser (Roark, 2001).
(Hale, 2001) monitors the incremental changes in the prob-                       The parser’s (unlexicalised) grammar and lexicon are de-
ability distribution over all parses to predict cognitive load,                  rived from sections 2-21+24 of the Wall Street Journal cor-
assuming fully parallel processing and making no predictions                     pus (Marcus et al., 1994). The parser predicts the main clause
about preferred structures. Alternatively, PCFG probabilities                    (MC) structure at fired, and then switches to the reduced rela-
can be used to rank the parses (Ranking approach), assuming                      tive (RR) structure at by. This “flip” in preferred structures
that the most likely structure is the one preferred by humans.                   predicts processing difficulty at by. The parser makes the
Processing difficulty is linked specifically to the processing                   same structural predictions for both employer and employee.
effort made when a previously preferred analysis suddenly                           However, the results of reading time studies by McRae
becomes dispreferred (Jurafsky, 1996; Crocker and Brants,                        et al. (1998) and Trueswell et al. (1994) demonstrate that
2000). Human memory limitations are modelled by a search                         readers use the thematic fit of the first NP and verb in pro-
beam containing only the most likely analyses, since the num-                    cessing this ambiguity. When the first NP is a plausible agent
ber of possible structures rises with the size of the grammar.                   of firing, like the employer, readers prefer the main clause
   PCFG-based models account elegantly for frequency ef-                         interpretation and show difficulty during the disambiguating
fects in lexical category and morphological form ambiguities                     by-phrase, as predicted by the parser. However, when the first
through probabilistic lexicon entries, while structural prefer-                  NP is a bad agent, but a good patient of the verb, like the em-
ences are covered by probabilistic grammar rules.                                ployee, readers reanalyse towards the reduced relative reading
   These models also account for processing failure in diffi-                    right away because it allows them to interpret the first NP as
cult garden path sentences. In the famous example, the horse                     a patient of the verb. In this case, they show difficulty in the
raced past the barn fell, the ultimately correct reduced rela-                   verb region, and not during the by-phrase.
tive analysis corresponding to the horse that was raced past                        In sum, unlike human readers, the PCFG-based model
the barn is assigned only a small probability because it is in-                  makes the wrong prediction for the good patient (employee)
frequent overall and raced is biased towards the intransitive,                   case because it does not take the semantics of the first NP into
active interpretation. In the Ranking approach, the analysis                     account. This general problem is common to all PCFG-based
drops out of the beam of accessible parses and cannot be                         models, be they Ranking or Surprisal approaches.
retrieved any more when fell is encountered, which causes
                                                                                 Constraint Integration Models
parsing to fail for this sentence. Alternatively, the Surprisal
approach uses the fact that the very likely main clause parse                    To account for both syntactic and semantic influences in
becomes impossible at fell to correctly predict difficulty.                      sentence processing, McRae et al. (1998) use a constraint-
   The most important difference among PCFG-based mod-                           based model, the Competition-Integration model (Spivey-
els is the way the grammar is induced. The Jurafsky (1996)                       Knowlton, 1996). In contrast to the PCFG-based models,
model uses a set of hand-selected rules, with probabilities ex-                  this model does not create the structural alternatives itself,
tracted from a corpus of structurally annotated sentences. The                   but only decides between them. It uses weighted constraints
Crocker and Brants (2000) model, on the other hand, induced                      which provide support for the analyses by activating them to
both grammar rules and probabilities from a corpus. This                         a greater or lesser degree. The activation of each analysis is
gives their model broad coverage of syntactic constructions                      computed iteratively, and an analysis is chosen when its acti-
and allows it to correctly analyse unseen text, which is an                      vation exceeds a threshold. If all constraints point towards the
important characteristic of human sentence processing.                           same analysis, the model needs few iterations to settle than if
                                                                                 constraints conflict. The number of iterations until settling
Restriction to Syntax                                                            can be used to predict processing difficulty.
While PCFG-based models have been shown to account for a                            To model the MC/RR ambiguity, McRae et al. (1998) used
variety of syntactic phenomena in sentence processing, they                      the following constraints in the verb and by-region that were
suffer from a major restriction: They are unable to take se-                     estimated from a variety of different sources: Thematic fit of
mantic information into account. Consider sentence (1):                          first NP and verb (from a rating study), tense/voice prefer-
                                                                                 ences of the first verb (from a corpus study), a bias for the
                                                                             658

reduced relative interpretation when reading by and a gen-           matic fit is influenced (at least) by the verb (or, more specifi-
eral bias for the main clause analysis over the reduced rela-        cally, its current sense), the argument head, the thematic role,
tive (from a corpus study). After disambiguation, two more           and the grammatical function of the argument. We equate the
constraints supported the relative clause interpretation. The        plausibility of a verb-role-argument triple with its probabil-
weights for the constraints were set by fitting to off-line com-     ity, which we compute as the joint probability of the verb’s
pletion data. The resulting model successfully predicts hu-          sense vs 1 , the role r, the argument head a, and the grammati-
man processing data for the MC/RR ambiguity.                         cal function g f of a:
   A second constraint-integrating model that accounts for
the data is described by Narayanan and Jurafsky (2002). It                              Plausibilityv,r,a = P(vs , r, a, g f )
extends Jurafsky’s original model by proposing a combi-
nation of Bayesian belief nets (a formalism for reasoning            This joint probability cannot be reliably estimated from co-
about events based on partial probabilistic information). The        occurrence counts due to lack of data. But we can decom-
proposed architecture can incrementally integrate a parsing          pose this term into a number of subterms that approximate
model with any number of constraints from other sources in           intuitively important information such as syntactic subcate-
a mathematically clean and consistent way. The parser is cast        gorisation (P(g f |vs )), the syntactic realisation of a semantic
as a belief net which computes the syntactic probability of          role (P(r|vs , g f )) and selectional preferences (P(a|vs , g f , r)):
each parse, while a second belief net integrates thematic fit
and lexical (verb tense/voice and valence) probabilities. The                 Plausibilityv,r,a = P(vs , r, a, g f ) =
predictions of the nets are combined into a single probability                    P(vs ) · P(g f |vs ) · P(r|vs , g f ) · P(a|vs , g f , r)
value for each structure. Again, the most probable analysis is
taken to be the preferred one, and flips predict difficulty.         This formulation allows us to estimate each of the subterms
   However, there are two main drawbacks to both types of            from training data with semantic role annotation. However,
constraint-integrating model. First, a specific set of con-          we still need to smooth our estimates, especially as the counts
straints has to be chosen for each ambiguity (e.g., Tanenhaus        needed for estimating the P(a|vs , g f , r) term remain sparse.
et al., 2000). Consequently, the models are unlikely to gener-          We use two complementary approaches to smoothing
alise to new constructions without further changes.                  sparse training data. One, Good-Turing smoothing, ap-
   A second problem is that the constraint weights often are         proaches the problem of unseen data points by assigning them
estimated from a diverse set of sources, for example various         a small probability. This method relies on re-estimating the
corpus studies as well as rating and completion studies (e.g.,       probability of seen and unseen events based on knowledge
Narayanan and Jurafsky, 2002). This is at least inelegant and        about more frequent events. We apply it to all estimates that
can be costly if rating studies have to be run. It may even be       are 0 or 1 to avoid not being able to make predictions at all.
problematic if sources (e.g., corpora) with different or even           The other method, class-based smoothing, attempts to ar-
conflicting biases are used (see Roland and Jurafsky, 1998).         rive at semantic generalisations for words. These serve to
                                                                     identify equivalent verb-argument pairs that furnish addi-
        A PCFG-Based Model with Semantics                            tional counts for the estimation of P(a|vs , g f , r). For example,
                                                                     if {boss, employer, chief} forms a synonym group of nouns,
We introduce a probabilistic processing model based on the           class-based smoothing allows us to share counts for boss-fire-
Ranking approach that overcomes the limitations of PCFG-             agent and employer-fire-agent. While little is known about
based models by integrating a semantic module. At the same           the cognitive plausibility of smoothing, the kind of generali-
time, our model does not require the stipulation of arbitrary        sations we use seem intuitively not far removed from human
semantic constraints, and its parameters need not be set by          reasoning. We employ noun classes as well as verb classes.
hand, but are learnt automatically from corpus data. Learning        Our noun classes are the lowest class level from WordNet
from corpora also gives our model broad coverage both of             (Miller et al., 1990), the synonym sets. Our verb classes are
structures that are processed effortlessly as well as those that     induced from the training data by unsupervised soft clustering
cause interesting disruption. This allows the model to cover         methods (see Padó et al., 2006). Soft clustering allows differ-
different phenomena without requiring modifications.                 ent verb senses to be distinguished by a verb’s membership in
   Our model computes the plausibility of the verb-argument          different clusters.
relations in each structure that the parser constructs and uses
the plausibility score to complement the syntactic probability       Evaluation of the Semantic Module
computed by the parser. When the role assignment that the            We first establish that our semantic module reliably captures
semantic module prefers is incompatible with the preferred           human intuitions. The model’s task is to correctly predict hu-
syntactic interpretation, we predict difficulty due to the pro-      man thematic fit judgements for verb-role-argument triples.
cessing effort made to solve the conflict.                           We take a significant positive correlation of the predictions to
   We first introduce the semantic module in detail. We then         the human judgements to indicate reliable performance.
test the cognitive plausibility of the semantic module’s pre-
dictions, and finally review the complete model’s handling of        Training and Test Data To train our model, we need lan-
the MC/RR and NP/S ambiguities.                                      guage data with thematic role annotation. To date, there are
                                                                     two main efforts to semantically annotate corpora: PropBank
Semantic Module                                                      (PB, Palmer et al., 2005) and FrameNet (FN, Baker et al.,
The task of our semantic module is to compute the plausibil-             1 Since the correct verb sense is unknown, we compute plausibil-
ity of a verb-argument relation in terms of thematic fit. The-       ity for all senses and choose the most plausible one.
                                                                 659

  fire.01 [The employer Arg0 ] fired [the employee Arg1 ]                  Test          Train       Coverage     Correlation (ρ)
  Firing [The employer Employer ] fired [the employee Employee ]                          PB          88.0%       0.130, ns
                                                                         McRae
                                                                                          FN          56.0%       0.368, **
Figure 3: Example annotation:           PropBank (above) and
                                                                                    Upper Bound       100%             0.68
FrameNet (below).
                                                                                          PB          100%        0.272, ***
             Verb      Noun         Role     Rating                       Own             FN          98.6%       0.532, ***
                                                                                       FN Seen        97.7%       0.593, ***
              fire   employer       agent      6.1
                                                                                      FN Unseen        99%        0.428, ***
              fire   employer      patient     2.4
              fire   employee       agent      1.9                   Table 2: Coverage and correlation strength for PB and FN
              fire   employee      patient     6.4                   data on McRae and own test sets. ns: not significant, **: p <
                                                                     0.01, ***: p < 0.001
Table 1: Test items: Verb-noun pairs with ratings for the agent
and patient role using a 7 point scale (McRae et al., 1998).         ing for each verb-argument pair the roles that the verb typi-
                                                                     cally assigns to its subject and object. Our semantic module
2003). The PB corpus (c. 120,000 propositions, c. 3,000              has more information about the items in this set, but its task
verbs) adds semantic annotation to the Wall Street Journal           remains non-trivial, since half of the verb-argument pairs still
corpus, the same data our parser is trained on. Arguments            have not been seen together in each training set.
and adjuncts are annotated for every verbal proposition in the          We collected ratings on the World Wide Web (using the
corpus. A common set of argument labels Arg0 to Arg5 and             WebExp package, www.webexp.info). To avoid participants
ArgM (for adjuncts) is used, and interpreted in a verb-specific      rating the same item in both the agent and patient interpre-
way. Some consistency in mapping has been achieved, so that          tation and due to the large number of items, we presented
agents are generally Arg0 and patients/themes Arg1.                  four separate lists of items that were assigned randomly to
   The FN corpus groups verbs with similar meanings to-              participants. Participation in the experiment was voluntary,
gether into frames (i.e., descriptions of situations), and as-       but restricted to native speakers of English. The raters were
sumes a set of frame-specific roles for the participants (e.g.,      recruited through postings to mailing lists and Usenet.
an Employer and Employee in the Firing frame). Fig. 3 gives             106 raters completed the experiment. We excluded five
an example of PB and FN style annotation. The FN resource            participants because they did not supply a valid email address
is about half as large as PB at 57,000 propositions (c. 2,000        (which we took as a sign of participation in earnest) and one
verbs). Since corpus annotation is frame-driven, only some           non-native speaker. From the remaining 100 (25 participants
senses of a verb may be present and word frequencies in the          per sub-experiment), we excluded one more participant who
FN corpus may not be representative of English. The PB               had rated only one item. We further excluded ratings that
approach annotates running text, which makes it more reli-           were more than 2 points from the item median. The average
able in this respect. However, both the definition of frames         number of ratings per item was 21.
as semantic verb classes and the semantic characterisation of
frame-specific roles introduce information to FN annotation          Results We trained our model on both the PB and FN cor-
that is not present in PB. We train on both corpora and com-         pora and created predictions for both test sets, which we
pare the results below.                                              then correlated with the human judgements. Since the data
   Our test data consists of two sets of verb-argument pairs         are not normally distributed, we used Spearman’s ρ, a non-
with plausibility ratings for two roles each. For both sets,         parametric rank-order test.
raters answered questions like How common is it for an em-              Table 2 gives an overview of the results. Due to the sparse-
ployer to fire someone? with a rating from 1 (very uncom-            ness of verbs from the McRae et al. items in our training set,
mon) to 7 (very common), as for the example item in Table 1.         the coverage of the FN model is relatively low. Nonetheless,
One test set comprises 100 out of the 160 verb-argument pairs        only its predictions are correlated significantly with the hu-
from McRae et al. (1998) (the remaining 60 were used as a            man data, despite the better coverage of the PB model. As
development set for parameter setting). Not all of the verbs         intended, FN coverage rises when we use our own test set,
and nouns used in this study were seen in our training cor-          and both models’ predictions are significantly correlated to
pora. For example, for only 64 test set items the verb had           human judgements on the α = 0.001 level. The FN model’s
been seen in FN, while PB covers the verbs from 92 items.            ρ value is however still much higher than the PB model’s.
Nouns are even sparser. This is largely due to vocabulary dif-          To obtain an upper bound for model performance we com-
ferences between our training corpora and the items.                 puted inter-rater agreement, i.e., the degree of consensus
   We gathered a second, larger test set ourselves with the          about how a role should be rated. This can be estimated by
goal of obtaining human judgement data that is more similar          correlating the ratings of a single participant with the aver-
in vocabulary to the training data for a fairer evaluation. To       age ratings of all remaining participants, and repeating for all
ensure that all the verbs in the new test set are covered, we        participants. The resulting upper bound of ρ = 0.68 shows
used 18 verbs that appear in both FN and PB. We extracted            that our model performs reasonably well by achieving a max-
the three most frequent arguments seen as subjects and ob-           imum of ρ = 0.532. (The inter-rater agreement for the McRae
jects in each corpus, so that for each verb, there were usually      et al. items is presumably similar.)
six arguments from each corpus (some overlap could not be               These results, however, raise the question why perfor-
avoided). We constructed 414 verb-role-argument triples us-          mance is so much better on our data than on the McRae et al.
                                                                 660

                Input         Syn    Sem     Conflict    Correct      Input           Syn     Sem      Conflict    Correct
                The           MC       –        no          yes       The             MC        –        no           yes
                employer      MC       –        no          yes       employee        MC        –        no           yes
                fired         MC     MC         no          yes       fired           MC       RR        yes          yes
                by            RR     MC        yes          yes       by              RR       RR        no           yes
Figure 4: MC/RR ambiguity: Preferred structure as predicted incrementally by our combined model consisting of a PCFG-
based parser (Syn) and a semantic module (Sem) with % of conflict over all items. Good Agent first NP (left) vs Good Patient
first NP (right).
items. A closer look at the seen and unseen verb-argument             The NP/S Ambiguity
pairs in our own test set reveals that the model’s predictions        We now turn to a second phenomenon, the so-called NP/S
are better when more is known about the verb-argument pair,           complement ambiguity. Note that, unlike the constraint-
while the model still makes reliable predictions for unseen           integration models, our model requires no changes to account
combinations. This explains the performance gap between               for new phenomena. Consider sentence (2):
our data and the McRae et al. items: Virtually all of the verb-
argument pairs in the literature items are unseen, leading to         (2) The man realised his goals were out of reach.
predictions that are reliable, but worse than for our data.
   In sum, using the FN corpus, we are able to correctly model        At goals, it is unclear whether the NP is a direct object of
items from the literature, as well as data from our own study.        realise or the subject of a sentence complement (as were
Our semantic module can therefore be used as a model of               later indicates). Pickering et al. (2000) investigated this phe-
human semantic intuitions about verb-argument-role triples.           nomenon using nouns which are plausible and implausible
                                                                      objects of the first verb realise. Despite a verb preference for
The MC/RR Ambiguity Revisited                                         the sentence complement, they found evidence that readers
                                                                      initially prefer the NP interpretation: Their results show ro-
We now consider our combined model and its predictions for            bust effects of difficulty in the noun region (his goals) when
the MC/RR ambiguity. The syntactic predictions are made               the noun is an implausible object of the verb. This indicates
by the same parser as in the Restriction to Syntax section            that readers initially construct the object reading of the noun
above (see Fig. 2), and on their own would fail to account            phrase and reanalyse if this interpretation is implausible. Re-
for both sentences. However, our semantic module is able              analysis is restricted to the noun region. At the disambiguat-
to counterbalance the syntactic preferences: We assume that           ing verb (were), there are indications of difficulty for sen-
processing difficulty occurs if the semantic module prefers a         tences with plausible object nouns only. While this effect is
different syntactic structure than the parser. We define the se-      weaker, Pickering et al. conclude that readers now reanalyse
mantic module’s preferred structure as the one that is consis-        their initially plausible object interpretation of the NP.
tent with the semantic module’s preferred role. For example,             Fig. 5 shows our model’s predictions for the example item
if the model prefers an agentive role for employer given the          from Pickering et al. (2000). To see what a pure PCFG-based
employer fired, it thereby prefers the main clause reading.           model would do, consider the parser’s predictions in column
   As shown in Fig. 4, the semantic module provides predic-           Syn only. At realised, the predicted structure is the same for
tions as soon as the first verb-argument pair is seen (at fired).     both interpretations. At his and goals/shoes, the parser pre-
For the good agents (employer-fired), it prefers the role that is     dicts the object interpretation (NP), which is correct accord-
consistent with the main clause reading. The parser and the           ing to Pickering et al.’s results. 2 The parser fails to predict
semantic module agree, so no difficulty is predicted. Seeing          difficulty at shoes for implausible direct object nouns. In-
by makes the main clause reading of fired unlikely because            stead, it always predicts difficulty at the disambiguating verb
no direct object was seen (and none can follow now), and the          by switching to the sentence complement (S) interpretation.
parser switches to the reduced relative. However, the seman-             We now turn to the predictions of the combined model.
tic module continues to prefer the role which indicates the           For this ambiguity, the semantic model either prefers the di-
main clause reading. This conflict between the syntactic and          rect object interpretation of goals/shoes by assigning a role
semantic modules predicts difficulty. This example was pre-           licensed by admit, or it prefers to assume that a role will be
viously correctly accounted for by the parser alone, and we           assigned to the NP by an upcoming, unseen verb in the em-
make the right predictions again using the semantic module.           bedded sentence reading.
   For the good patients, the semantic module disagrees with             For the plausible object case (Fig. 5, left), goals is assigned
the parser’s preference already at fire by preferring the pa-         a role: The semantic module agrees with the parser in prefer-
tient role for employee-fired, which is consistent only with          ring the object interpretation (Sem column). At were, it con-
the reduced relative structure. This conflict correctly predicts      tinues to do so, conflicting with the syntactic parser and thus
processing difficulty at the verb. As the sentence unfolds,           correctly predicting difficulty. Later in the sentence, the main
the parser changes its preferred interpretation so that it agrees         2 Note that this preference for the object interpretation is due to
with the semantic module’s, and no more difficulty is pre-
                                                                      the small tree bias inherent in PCFG-based models, where fewer rule
dicted at by. Using the semantic module in combination with           applications mean higher tree probabilities. Following Crocker and
the PCFG-based model allows us to make a correct prediction           Brants (2000), we propose to interpret this bias as implementing a
that the parser alone could not make.                                 preference for simple structures.
                                                                  661

                  Input       Syn    Sem     Conflict    Correct     Input        Syn      Sem     Conflict    Correct
                  realised     –      –        no          yes       realised       –        –        no         yes
                  his         NP      –        no          yes       his           NP        –        no         yes
                  goals       NP     NP        no          yes       shoes         NP        S        yes        yes
                  were         S     NP        yes         yes       were           S        S        no         yes
Figure 5: NP/S ambiguity: Preferred structure as predicted incrementally by our combined model consisting of a PCFG-based
parser (Syn) and a semantic module (Sem). Plausible Object NP (left) vs Implausible Object NP (right).
verb of the embedded clause will be available to assign a role       Crocker, M. and Brants, T. (2000). Wide-coverage probabilis-
to goals, which can reverse the structural preference.                  tic sentence processing. Journal of Psycholinguistic Research,
                                                                        29(6):647–669.
   For the implausible object case (Fig. 5, right), the semantic     Crocker, M. and Corley, S. (2002). Modular architectures and sta-
module immediately prefers not to interpret shoes as a di-              tistical mechanisms: The case from lexical category disambigua-
rect object of realised. This causes a conflict with the parser,        tion. In Merlo, P. and Stevenson, S., editors, The lexical basis of
                                                                        sentence processing. John Benjamins.
which correctly predicts difficulty at the noun. This conflict       Garnsey, S., Pearlmutter, N., Myers, E., and Lotocky, M. (1997).
is overcome when the parser at the next word switches to                The contributions of verb bias and plausibility to the comprehen-
preferring the embedded sentence interpretation, too. Our               sion of temporarily ambiguous sentences. Journal of Memory and
model thus accounts correctly for human preferences that a              Language, 37:58–93.
                                                                     Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic
pure PCFG-based approach cannot mode.                                   model. In Proceedings of NAACL.
                                                                     Jurafsky, D. (1996). A probabilistic model of lexical and syntactic
                                                                        access and disambiguation. Cognitive Science, 20:137–194.
                         Conclusions                                 Marcus, M., Santorini, B., and Marcinkiewicz, M. A. (1994). Build-
                                                                        ing a large annotated corpus of English: The Penn Treebank.
We reviewed two approaches to modelling human sentence                  Computational Linguistics, 19(2):313–330.
processing. The first approach is based on probabilistic gram-       McRae, K., Spivey-Knowlton, M., and Tanenhaus, M. (1998). Mod-
mars, and has the advantage of being automatically trainable            eling the influence of thematic fit (and other constraints) in on-
on a single data source. However, it does not incorporate se-           line sentence comprehension. Journal of Memory and Language,
                                                                        38:283–312.
mantic information, which means the influence of thematic            Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D., and Miller,
information in human sentence processing cannot be cap-                 K. J. (1990). Introduction to WordNet: An on-line lexical
tured. The second approach integrates a wide variety of con-            database. International Journal of Lexicography, 3(4):235–244.
                                                                     Narayanan, S. and Jurafsky, D. (2002). A Bayesian model predicts
straints (Competition-Integration, Bayes Nets) and is able to           human parse preference and reading time in sentence processing.
correctly account also for thematic effects in the human data.          In Dietterich, T. G., Becker, S., and Ghahramani, Z., editors, Ad-
However, the constraints have to be manually specified for              vances in Neural Information Processing Systems 14. MIT Press.
each construction, and constraints have to be derived from a         Padó, U., Crocker, M., and Keller, F. (2006). Modelling semantic
                                                                        role plausibility in human sentence processing. In Proceedings of
range of diverse data sources.                                          EACL.
   We presented an alternative model which builds on prob-           Palmer, M., Gildea, D., and Kingsbury, P. (2005). The Proposition
abilistic grammar, but integrates a semantic module that as-            Bank: An annotated corpus of semantic roles. Computational
                                                                        Linguistics, 31(1):71–105.
signs thematic roles for the structures generated by the parser.     Pickering, M., Traxler, M., and Crocker, M. (2000). Ambiguity reso-
The parameters of the complete model can be automatically               lution in sentence processing: Evidence against frequency-based
acquired from corpus data, which affords broad coverage and             accounts. Journal of Memory and Language, 43:447–475.
                                                                     Roark, B. (2001). Robust Probabilistic Predictive Syntactic Process-
allows the model to cover different phenomena without re-               ing: Motivations, Models, and Applications. PhD thesis, Brown
quiring hand-tuning. We verified the cognitive plausibility of          University.
our semantic module by successfully correlating its predic-          Roland, D. and Jurafsky, D. (1998). How verb subcategorization fre-
                                                                        quencies are affected by corpus choice. In Proceedings of COL-
tions to human thematic fit ratings. We also demonstrated               ING/ACL.
that the complete model correctly predicts processing diffi-         Spivey-Knowlton, M. (1996). Integration of visual and linguistic
culty for two classic ambiguities in the psycholinguistic liter-        information: Human data and model simulations. PhD thesis,
ature: the MC/RR and the NP/S ambiguity.                                University of Rochester.
                                                                     Stolcke, A. (1995). An efficient probabilistic context-free parsing
   In future work, we want to derive a quantitative measure             algorithm that computes prefix probabilities. Computational Lin-
of processing difficulty, as evidenced, e.g., by reading times.         guistics, 21(2):165–201.
We will explore two different strategies: One is to continue         Tanenhaus, M., Spivey-Knowlton, M., and Hanna, J. (2000). Mod-
                                                                        eling thematic and discourse context effects with a multiple con-
exploiting the disagreement between the modules by quanti-              straints approach: Implications for the architecture of the lan-
fying the strength of disagreement. The other is to quantify            guage comprehension system. In Crocker, M., Pickering, M., and
the preference for the best analysis given a combined syntac-           Clifton, C., editors, Architectures and Mechanisms for Language
                                                                        Processing. Cambridge University Press.
tic and semantic score.                                              Trueswell, J., Tanenhaus, M., and Garnsey, S. (1994). Semantic
                                                                        influences on parsing: Use of thematic role information in syn-
                         References                                     tactic ambiguity resolution. Journal of Memory and Language,
                                                                        33:285–318.
Baker, C., Fillmore, C., and Cronin, B. (2003). The structure of     Trueswell, J. C. (1996). The role of lexical frequency in syntactic
   the Framenet database. International Journal of Lexicography,        ambiguity resolution. Journal of Memory and Language, 35:566–
   16(3):281–269.                                                       585.
Brysbaert, M. and Mitchell, D. C. (1996). Modifier attachment in
   sentence parsing: Evidence from Dutch. Quarterly Journal of
   Experimental Psychology, 49A(3):664–695.
                                                                 662

