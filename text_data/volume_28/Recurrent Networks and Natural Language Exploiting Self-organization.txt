UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Recurrent Networks and Natural Language: Exploiting Self-organization
Permalink
https://escholarship.org/uc/item/2h67s62d
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Crocker, Matthew W.
Farkas, Igor
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                  Recurrent Networks and Natural Language: Exploiting
                                                   Self-organization
                                             Igor Farkas (ifarkas@coli.uni-sb.de)
                                      Matthew W. Crocker (crocker@coli.uni-sb.de)
                                     Department of Computational Linguistics and Phonetics
                                      Saarland University, Saarbr•ucken, D-66041, Germany
                           Abstract                                 but has also been argued to have a greater biological
                                                                    plausibility. There have been a number of unsupervised
   Prediction is believed to be an important cognitive com-         methods proposed during the last decade (see overview
   ponent in natural language processing. Within connec-            in Barreto et al., 2003; Hammer et al., 2004a). Here we
   tionist approaches, Elman’s simple recurrent network
   has been used for this task with considerable success,           focus on two models, namely Recursive Self-Organizing
   especially on small scale problems. However, it has been         Map (RecSOM; Voegtlin, 2002) and feedforward Sard-
   appreciated for some time that supervised gradient-              Net (James and Miikkulainen, 1995) that represent, in
   based learning models have difficulties with scaling up,         a sense, complementary approaches to representation
   because their learning becomes very time-consuming for           of the temporal context. RecSOM has been shown to
   larger data sets. In this paper, we explore an alter-
   native neural network architecture that exploits self-           demonstrate a rich repertoire of dynamic behavior when
   organization. The prediction task is effectively split into      trained on a complex symbolic sequence such as natural
   separate stages of self-organized context representation         language text (Tiňo and Farkaš, 2005). Similarly, it has
   and subsequent association with the next-word target             been shown that SardNet, when added as a parallel input
   distribution. We compare various prediction models and
   show, in the task of learning a language generated by            preprocessing module to a supervised recurrent network,
   stochastic context-free grammar, that self-organization          enhances the processing capacity of a neural network in
   can lead to higher accuracy, faster training, greater ro-        a shift-reduce parsing task (Mayberry and Miikkulainen,
   bustness and more transparent internal representations,          1999).
   when compared to Elman’s network.
                                                                       Once the context representations are optimized with
                                                                    a chosen self-organizing module, we associate them with
                       Introduction                                 desired predictions using a supervised learning module.
                                                                    We tested two such modules. One is a simple counting
Recurrent neural networks have been traditionally used              method that builds independent prediction distributions
in various tasks that involve time-dependent data. The              for all units in the map. The other is a single-layer per-
best known architecture is the Simple Recurrent Net-                ceptron trained by the error delta rule.
work (SRN; Elman, 1990) that has been employed in
a variety of tasks, including language learning by pre-                              Simulation methods
diction (e.g. Elman, 1991; Servan-Schreiber et al., 1991;
Rohde and Plaut, 1997; Christiansen and Chater, 1999).              Self-organization of temporal context
Supervised learning of temporal dependencies by predic-             For temporal context learning, we explored two basic
tion typically involves error gradient learning algorithms          self-organizing modules – RecSOM and SardNet – as well
of which various forms have been proposed (see Pearl-               as a combination of the two, which we called RecSOM-
mutter, 1995, for overview). Despite their considerable             sard. We describe them all in more detail below.
success, the supervised learning approaches are difficult
to scale up to realistic tasks due to learning complexity.          Recursive Self-Organizing Map The architecture
One common aspect of these methods is that via error                of the RecSOM model is shown in Figure 1 (without the
back-propagation they optimize the internal states of a             top layer). Each map neuron i ∈ {1, 2, ..., N } has two
recurrent network to a particular task. In the prediction           weight vectors associated with it: wi ∈ Rn linked with
task this implies that both internal states and predic-             an n-dimensional input s(t), and ci ∈ RN linked with
tions are optimized using the same learning mechanism.              the context y(t − 1) = (y1 (t − 1), y2(t − 1), ..., yN (t − 1)),
   Here we explore an alternative avenue along which                containing map activations yi (t − 1) from the previous
we split the whole task into two subtasks and treat                 time step. The output of a neuron i at time t is computed
them independently. Hence, we first optimize internal               as yi (t) = exp(−di (t)), where
states, and then we associate these with desired pre-
dictions. Optimizing internal states consists in build-                    di (t) = α · ks(t) − wi k2 + β · ky(t − 1) − ci k2
ing temporal context representations and since it is not
driven by supervision, it can potentially benefit from self-        with k · k denoting the Euclidean norm. Parameters α >
organization. Self-organized temporal context learning              0 and β > 0 respectively influence the effect of the input
is expected not only to facilitate the learning process             and the context upon a neuron’s profile. Both weight
                                                               1275

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              allows each unit to represent different inputs depending
                                                                                                                                                                                                                                                                                                                                                                      y’(t)                                                                                                                                                                                                                                                                                                                   on the context, which leads to an efficient representation
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              of sequences, and also generalizes well to new sequences.
                                                    ##$           # $            # $          # $            # $## "               ! "!!"                     ! "                ! "            ! %"!! &                                                                                 ''&% (     ' (   ' (         ' (        ' (      ' (''
                                                      $
                                                       $# #         $# #           $# #         $# #           $# $$# !              "! ""! !                    "! !               "! !           "! %        " %&       %% &         %% &
                                                                                                                                                                                                                                                     &
                                                                                                                                                                                                                                                                   %% &
                                                                                                                                                                                                                                                                     &
                                                                                                                                                                                                                                                                                  %% &
                                                                                                                                                                                                                                                                                    &
                                                                                                                                                                                                                                                                                              %% (
                                                                                                                                                                                                                                                                                                &      (' &%% '   (' '  (' '        (' '       (' '     ' (('
                                                                                                                                                                                                                                                                                                                                                               ((
                       
                                          $           $
                                                                            $
                                                                                           $    $
                                                                                                        
                                                                                                         
                                                                                                         
                                                                                                                       
                                                                                                                        
                                                                                                                         
                                                                                                                         
                                                                                                                                    "
                                                                                                                                     
                                                                                                                                                         "
                                                                                                                                                                   
                                                                                                                                                                              
                                                                                                                                                                                
                                                                                                                                                                                     "         "
                                                                                                                                                                                                                 %"! 
                                                                                                                                                                                                                              &        &
                                                                                                                                                                                                                                                    % 
                                                                                                                                                                                                                                                         &         % 
                                                                                                                                                                                                                                                                         &
                                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                     % 
                                                                                                                                                                                                                                                                                        &
                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                   (
                                                                                                                                                                                                                                                                                                 % 
                                                                                                                                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                                                (
                                                                                                                                                                                                                                                                                                              & 
                                                                                                                                                                                                                                                                                                                 
                                                                                                                                                                                                                                                                                                                       (
                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                    (
                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                (                                                                                                                                                                                                                                                                                                                                          RecSOMsard This combined model has the RecSOM
                         
                           .
                                  
                                      .
                                              
                                                  .
                                                              
                                                                  .- 0
                                                                              
                                                                                  0
                                                                                                      0             0
                                                                                                                                      0
                                                                                                                                                      *)0/    *          *              
                                                                                                                                                                                                         *         
                                                                                                                                                                                                                        *      
                                                                                                                                                                                                                                   *       
                                                                                                                                                                                                                                               +*)   
                                                                                                                                                                                                                                                           ,+              
                                                                                                                                                                                                                                                                           ,+        ,+      ,+        ,+  ,+          y(t)
          -- .-
            .               -           -               -                /             /                     /               /             /                             )                   )             )         )          )                                                                                                                                                                                                                                                                                                                                                                                                                                                    architecture and processing, but adds on a SardNet-like
               . -- .        -- .       -- .           -- .-- 0        // 0         // 0                 // 0           // 0         // *)0// *                                                                               )) +                    + ++ ,++ ,++ ,++ ,++
             .-.  ..                     ..                      . 0                 00                                  00                       *)0 *
                                                                                                                                                                            )) *               )) *
                                                                                                                                                                                                  **
                                                                                                                                                                                                                )) *       )) *
                                                                                                                                                                                                                              **                +*)*) ,    ,+ ,            ,,,,,
                                                                                                                                                                                                                                                                                                                                                                                    ci
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              output preprocessing to be fed to a prediction module
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            y(t−1)                                                            (see Figure 1). RecSOM outputs are replaced during
                                                                                                                                                 wi                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                                                                          
                                                                                                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   processing by spatially distributed representations of the
                                                                                                                                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                                                                                                                  
                                                                                                                                                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                                                                                                                                                                                 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
                                                                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           context. In each iteration, the winner’s activation yk in
                                                                                                                                   s(t)                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                 
                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    RecSOM is transformed to a focused Gaussian profile
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              yi0 = exp{−d(i, k)2 /σy2 } centered arround the winner k,
Figure 1: RecSOMsard architecture. The bottom part                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            and previous activations in the top layer are decayed
(without the top layer) represents Recursive SOM. Solid                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       via y 0 ← λy 0 (as in SardNet). At boundaries between se-
lines represent trainable connections, dashed line repre-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     quences, all activations y 0 are reset to zero. This SardNet
sents one-to-one copy of the activity vector y. In Rec-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       feature establishes that the activation vector y(t) with
SOMsard, y is transformed to y’ in the top layer by a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         mostly unimodal shape is transformed to a distributed
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              activation vector y0 (t) whose number of peaks equals the
mechanism described in the text.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              position of a current word in a sentence (see Figure 2). In
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              this manner, the context in RecSOMsard becomes rep-
vectors can be updated using the same form of learning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        resented both spatially (due to SardNet) and temporally
rule (Voegtlin, 2002):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (due to RecSOM). 1
                                              ∆wi                                                                                                                             = γ · hik · (s(t) − wi ),
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  the       girls    who        see     dogs    walk     .
                                                             ∆ci                                                                                                              = γ · hik · (y(t − 1) − ci ),
where k is an index of the best matching unit (‘winner’)
at time t, (i.e. the unit with the highest activation yk (t))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Figure 2: RecSOMsard activation patterns during a sen-
and 0 < γ < 1 is the learning rate. Neighborhood func-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       tence processing.
tion hik is a Gaussian on the distance d(i, k) of units
i and k in the map: hik = exp{−d(i, k)2 /σ 2 (t)}. Pa-
rameter σ linearly decreases in time to allow for forming                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Next-word prediction modules
topographic representation of input sequences.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We employed two next-word supervised prediction mod-
   As a result of self-organization, RecSOM units learn                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ules built on top of the outputs of the trained context-
to topographically represent temporal contexts (sub-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         representation module.
sequences). Specifically, it has been shown that the
context-based input representations typically become or-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     W-module The Winner-based prediction module uses
ganized in a Markovian manner: Subsequences sharing                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          the matrix U (of size N ×n) of counters, which are selec-
a common suffix are mapped close to each other (Tiňo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         tively updated during a single epoch through the training
et al., 2005). At the same time, a more complex input se-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    data set, after the training has been completed. For each
quence can lead to more complex, non-Markovian behav-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        input st , the winner i is found and its counter U (i, st+1 )
ior (Tiňo and Farkaš, 2005). Learning in recursive self-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   corresponding to the next word st+1 is incremented by
organizing networks (such as RecSOM) has been shown                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          one. At the end of the epoch, the rows of U are normal-
to approximate stochastic gradient descent driven by in-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ized using L1 -norm to be interpretable as unit’s predic-
put data (Hammer et al., 2004b). Due to recurrency,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           tion probability vectors. Hence, in W-module the pre-
the process of weight optimization can be thought of as                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      diction in each step is retrieved locally, from the winner’s
temporally enhanced vector quantization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      probability distribution vector.
SardNet The alternative model, SardNet (Sequential                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            P-module The Perceptron-based prediction module
Activation Retention and Decay Network; James and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            uses n softmax outputs, which allows for the outputs
Miikkulainen, 1995), has an architecture and mechanism                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        to be interpreted as prediction probabilities. In soft-
very similar to the standard SOM (Kohonen, 1990). Un-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         max, the output ofP      the perceptron unit j is computed
like RecSOM, it does not have a recursive architecture,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       as zj = exp(netj )/ P      l exp(netl ), where the total input
so it learns to unambiguously represent the sequences as                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      of unit j is netj =           i vji yi . Perceptron weights are
distributed activation patterns over the map. For each                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        updated via the delta rule as
input, the winner is assigned the activation of 1.0 and the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ∆vji = η(t)yi (dj − zj )
activations of all other units representing previous inputs
in the current sequence are decayed via yi ← κyi using                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        with targets dj being one-hot encoded, and learning rate
a preset decay factor 0  κ < 1. Once the unit is acti-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       η(t) linearly decreasing to zero. Unlike W-module, P-
vated, it is removed from competition and cannot repre-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     the latter being the case, because each winner in Rec-
sent later input in the current sequence. Forcing other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      SOM best matches the current input in a particular temporal
(neighboring) units to participate in the representation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     context
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1276

module involves a true learning algorithm, so it requires
multiple epochs of training, rather than one.                  Table 1: The stochastic context-free grammar used to
                                                               generate training corpora.
Models                                                            S → Declar (.75) | Interrog (.2) | Imper (.05);
Using a complete cross-design of the above context-               Declar → SP VP (.85) | NP-Adj (.1) | That-NP (.05);
learning and next-word prediction modules, we end up                 SP → NP | NP RC;
with six possible models exploiting the self-organization            RC → who VI | who VT SP | who SP VT;
in the context learning stage. Not all of these models               NP-Adj → NP is/are/were Adj;
can be expected to work well, such as SardNet com-                   That-NP → that/those is/are/were NP;
bined with W-module (SardNet-W), in which tempo-                  Interrog → Wh-Qn (.65) | Aux-Qn (.35);
ral information is lost. All models were evaluated for
                                                                     Wh-Qn → where/who is/are/were NP | where/who/what
comparison and were compared with Elman’s SRN as                                do/does NP VP;
well as statistical bigram and trigram models (Stolcke,              Aux-Qn → do/does NP VP | do/does NP wanna VP |
2002). The SRN was matched in architecture with Rec-                             is/are/were NP Adj;
SOM and hence, was a two-layer network using N hidden             Imper → VP;
units with sigmoidal activation function but like the P-          VP → VI | VT NP;             NP → ART ADJ N;
module, it had n softmax output units. The SRN was                ART → ”” | a | the;          ADJ → ”” (.5) | Adj;
trained using online stochastic gradient descent to min-
imize cross-entropy between outputs and corresponding
targets. Error was not back propagated through time,            obtain the best performance. Each input to the networks
only through the current time step.                             contained a localist representation of a word. Hence, all
                                                               neural network models had n = 72 input units. Rec-
Training data                                                   SOM(sard) and SardNet contained N = 18 × 18 neurons
We used SLG (Rohde, 2002) to generate sentences con-           and their weights were randomly initialized within the
structed by a moderately complex stochastic context-           interval [0.4; 0.6]. Other parameters for models with self-
free grammar specified in Table 1. The choice of the           organization were as follows: α = 3, β = 0.6, γ = 0.1, σ :
grammar was motivated by the use of child-directed sen-        8 → 0.5, σy = 1, κ = 0.9 and λ = 0.8. Weights of the
tences (following Christiansen and Dale, 2001) enriched        P-module were trained for 8 epochs with η : 0.3 → 0.03.
with recursive structures, as used in earlier works on         RecSOM output activations and SRN context-layer acti-
SRN in next-word prediction tasks (Elman, 1993; Ro-            vations were not reset between sentences, since sentence
hde and Plaut, 1997). Our grammar included three pri-          boundaries were clearly detectable by end-of-sentence
mary sentence types: declarative, interrogative, and im-       marker. The weights in the SRN were intialized within
perative. Each type consisted of a variety of common           the interval [-0.1; 0.1], it had 324 hidden units, a learn-
utterances. Declarative sentences most frequently ap-           ing rate set to 0.05, and no momentum. Higher learning
peared as transitive or intransitive verb constructions,        rate for the SRN (to speed up convergence) had a desta-
but also included predication using the copula. Inter-          bilizing effect and mostly resulted in a somewhat higher
rogative sentences were composed of wh-questions and            testing error. On the other hand, lower learning rates did
questions formed by using auxiliary verbs. Imperatives          not lead to further decrease of the testing error (during
were the simplest class of sentences, appearing as intran-      100 epochs of training). Varying the size of the hidden-
sitive or transitive verb phrases. We controlled subject-       layer did not lead to significant improvement either. All
verb agreement, as well as appropriate determiners ac-          models were trained for maximum 10 epochs, except for
companying nouns. The sentences obeyed a number of              the SRN that needed at least 30 epochs to sufficiently
semantic constraints, similar to those used in Rohde and        converge. 2
Plaut (1997). Regarding recursive sentences, we did not
follow the ‘starting small’ scenario, hypothesized by El-                                  Results
man (1993), as we believe it would not be beneficial to         First, we evaluated each trained model M in terms of
the networks (Rohde and Plaut, 1997). Sentences longer          its normalized negative log-likelihood (NNL) using the
than 16 words were discarded in generating the corpus,          testing data. It is computed as
but these were so rare (< 0.1%) that their loss should
have negligible effects. The lexicon contained 72 words                                          LX
                                                                                                  tst −1
                                                                                           −1
including the end-of-sentence marker.                                 NNLM (Stst ) =                     logn PM (st+1 |Ct )
                                                                                        Ltst − 1 t=1
Parameters
Using the above grammar, we generated 10,000 sen-               where Ct = s1 s2 ...st is the context at time t, and Ltst
tences. Each model was run 10 times, using a differ-            is the length of the testing sequence. The higher the
ent training set comprising a randomly chosen subset            predictive probabilities assigned to the actual next sym-
of 50% of total sentences (the complementary subset             bols are, the smaller the NNL is. The NNL measure
was used for testing). On average, the training set con-        can be viewed as a quantification of a statistical average
tained 30867 words and testing set 23148 words (we re-              2
                                                                      In terms of actual implementation, one cycle for training
moved duplicate sentences from the testing sets). In each      SRN took somewhat longer than that of any self-organizing
model, we experimented with a number of parameters to          module coupled with a prediction module.
                                                           1277

             1                                                                               0.9
                                                   RecSOM−W
                                                   RecSOM−P
           0.95                                    SardNet−P
                                                   RecSOMsard−P                              0.8
            0.9                                    Srn
                                                                                             0.7
           0.85
                                                                               Mean cosine
            0.8
                                                                                             0.6
Mean NNL
           0.75
                                                                                             0.5
            0.7
           0.65                                                                              0.4
            0.6                                                                                                                         RecSOM−W
                                                                                             0.3                                        RecSOM−P
                                                                                                                                        SardNet−P
           0.55                                                                                                                         RecSOMsard−P
                                                                                                                                        Srn
            0.5                                                                              0.2
               0   1   2   3   4    5      6   7   8     9        10                            0    1    2    3    4    5      6   7   8     9        10
                                   Epoch                                                                                Epoch
Figure 3: Mean NNLs for various prediction models.                        Figure 4: Mean cosines between actual and optimal pre-
SRN was actually trained for 30 epochs, but is aligned                    diction distribution vectors for various prediction mod-
with other curves to facilitate comparison. Error bars                    els. The SRN was actually trained for 30 epochs, but is
around means (not shown) were below 0.01.                                 aligned with other curves to facilitate comparison. Error
                                                                          bars around means (not shown) were below 0.06.
of ‘surprise’ experienced by the model upon seeing the
sequence.                                                                     ter than RecSOM-W (p < 10−8 ). This suggests that,
   The NNL is a standard measure used in prediction,                          for the purpose of prediction, the spatially distributed
but due to the ambiguity in grammar, more candidates                          representation of the context in SardNet may be more
should be predicted (which is not captured by the NNL).                       important than localized temporal representations in
For this reason, we also used an alternative measure that                     RecSOM-W. However, respective comparison of Sardnet
assesses the entire prediction vectors. One such measure                      and RecSOMsard (linked with either prediction mod-
is the mean cosine between the actual and the optimal                         ule) suggests that also temporal information encoded lo-
prediction vectors (calculated from the grammar).                             cally in RecSOM units is exploited in the complete com-
   Figure 3 shows the mean NNL for selected models, as                        bined model, hence leading to its best performance (both
a function of training epochs. It is evident that all mod-                    p < 10−9 ).
els learn, but to different degrees of accuracy. Several
observations can be made: First, RecSOMsard-P model
achieves the best performance, which may be both due                          Table 2: Mean NNLs and cosines (in parentheses) on
to its spatio-temporal representation of the context and                      testing data for models with self-organization.
due to the P-module (the means difference of the paired                                               RecSOM           SardNet      RecSOMsard
t-test was significant at p < 10−9 compared to RecSOM-                                       W      0.544 (0.821)   0.560 (0.783)   0.543 (0.820)
W). This claim is supported by the second observation,                                       P      0.568 (0.797)   0.552 (0.845)   0.510 (0.880)
that the NNL of RecSOMsard-P further decreases dur-
ing the last 3 epochs of training which occurs when the
neighborhood radius σ drops sufficiently (below 1) to                        The second observation resulting from cosines is that
allow for fine-tuning of RecSOM units. This is also ob-                   the SRN makes the least accurate prediction vectors (cos
served in RecSOM-P whose map units have recurrent                         = 0.785) which makes it worse than most models using
connections, but not in case of feedforward SardNet-                      self-organization (cf. Table 2). We acknowledge that in
P. Third, the SRN has comparable performance to that                      principle, the SRN may be able to achieve better perfor-
of RecSOM-W and SardNet-P (mutual mean differences                        mance by varying different parameters: e.g. using fewer
n.s.), while RecSOM-P has significantly the poorest ac-                   hidden nodes in combination with BPTT learning algo-
curacy (e.g. p < 10−6 compared to SardNet-P).                             rithm (Werbos, 1990), or using more hidden layers of
   The corresponding Figure 4 displays the mean cosines                   units could be beneficial (as e.g. in Elman, 1993; Rohde
for these models and is quite consistent with the previ-                  and Plaut, 1997).
ous figure. However, the cosines lead to at least two new                    It is also interesting to compare a self-organizing mod-
observations: The first relates to the individual contri-                 ule with the SRN in terms of the structure of internal
butions of both processing stages in the models that can                  (state-space) representations. The hidden-layer of the
lead to higher accuracy. According to the NNL, both                       SRN has been shown to form distributed representations
RecSOM-W and SardNet-P have similar performance,                          with nice structural properties (as shown originally by
but in terms of cosines SardNet-P is significantly bet-                   Elman) that mostly lead to Markovian behaviour: in-
                                                                       1278

put sequences sharing common suffixes are mapped to-                            1.4
                                                                                       RecSOM−W
gether in the hidden-layer and hence are likely to lead to                             RecSOM−P
                                                                                1.3    SardNet−P
similar predictions. The state-space vizualization of the                              RecSOMsard−P
                                                                                       Srn
SRN is typically achieved either by clustering techniques                       1.2
(showing a dendrogram), or by examining a few dimen-
sions via PCA. In contrast, the state-space representa-                         1.1
tion in SardNet and RecSOMsard is very transparent
                                                                     Mean NNL
                                                                                 1
and hence directly comprehensible in high-dimensional
map space (see Figure 2). Markovian behavior applies                            0.9
here as well, because sequences sharing common suffixes
have a high overlap, and are hence spatially close (mod-                        0.8
ulated by proper setting of σy in RecSOMsard).
                                                                                0.7
                                                                                0.6
Table 3: Mean correlation coefficients between predic-
tions of three selected models and optimal predictions                          0.5
                                                                                   0         10           20            30        40   50
given by the grammar. Error bars around means were                                                    Percentage node lesioning
below 0.014.
            RecSOMsard-P SRN 3-gram                              Figure 5: Mean NNLs of various models after lesioning.
                 0.881       0.822   0.785                       Error bars around means (not shown) were below 0.05
                                                                 for maps, below 0.25 for the SRN.
   In terms of the NNLs, the bigram and trigram mod-
els performed surprisingly very well (0.561 and 0.519,           internal representations. The benefits of combined ar-
respectively). However, the accuracy of n-grams drops            chitectures is not new in neural network research. Ear-
when we look at the similarity between the predictions           lier experiments with feedforward architectures showed,
and the optimal predictions, 3 computed as correlation           that, for example, radial-basis-function networks whose
coefficients, see Table 3. Trigram has significantly the         first layer is unsupervised, typically need shorter train-
lowest accuracy among the three models (all p0 s < 10−6 ).       ing, albeit they may require more hidden units for the
   While we leave a systematic analysis of errors types          same accuracy, compared to fully supervised two-layer
for the different models for future research, we did ob-         perceptron (Tarassenko and Roberts, 1994).
serve difficulty for all models with the prediction of long-        We can distinguish three approaches in total, regard-
distance dependencies. This is not suprising, given that         ing the optimization of context representations. The first
all models were observed to be driven by Markovian dy-           is the above mentioned supervised approach (as in the
namics.                                                          SRN), where the adaptation of the recurrent weights is
Lesioning test To test the robustness of the models,             driven by output targets. The unsupervised approach,
we randomly lesioned (deactivated) a subset of the mid-          explored in this paper, optimizes the recurrent weights
dle layer units (i.e. map units, or hidden-layer units in        independently from the predictions. As a third option,
the SRN). In each run, we randomly lesioned the net-             there exist recent models with no temporal context learn-
work only once. Figure 5 suggests, showing the mean              ing, such as the so called ‘liquid state’ machines (Maass
NNLs for the selected models, that sparse representa-            et al., 2002), ‘echo state’ networks (Jaeger, 2001), or pre-
tion in the maps lends itself to higher robustness than          diction fractal machines (Parfitt et al., 2000). In these
fully distributed representation in the hidden layer of          models, the recurrent part is not trained, but with suit-
the SRN. Among the self-organizing models, prediction            ably preset parameters (weights) and due to the so called
models with P-modules always yield higher robustness             architectural bias (Tiňo et al., 2004) it is able to gener-
compared to their counterpart with W-modules (To pre-            ate contextual representations with nice structural prop-
serve clarity, the other models with W-modules are not           erties. However, the exploration of these models is at
included, but their NNL increase was quite similar to            an early stage, and in addition, we have shown (Tiňo
that of RecSOM-W.) Consistently with previous figures,           et al., 2006) that learning the recurrent weights in Rec-
RecSOMsard-P model is the best which is due to the               SOM leads to temporal representations with significantly
sparseness of representation over a number of participat-        deeper contexts, when compared to an untrained recur-
ing units (and their nearest neighbors) within a sentence.       rent model based on affine contractions (as used in PFM;
                                                                 Parfitt et al., 2000).
                      Discussion                                    Our preliminary results in this paper shed light on the
In this paper we illustrated the benefits of a combined          benefits of self-organization in learning data with tempo-
architecture in terms of better prediction accuracy, faster      ral structure, as exemplified on a word-prediction task.
training, greater robustness and better transparency of          Nevertheless, exploiting self-organization is not limited
                                                                 to with word prediction; in principle it is also applicable
   3
     We did not compute cosines for n-grams, because SRILM       to other language tasks, such as the case-role assignment.
package does not have that feature.                              As a matter of fact, even more complicated architec-
                                                              1279

tures than ours have recently been shown to profit from           the AAAI Symposium on Compositional Connection-
self-organization in incremental nonmonotonic parsing             ism in Cognitive Science, Washington, DC: Erlbaum.
task (Mayberry and Miikkulainen, 2003; Mayberry and            Mayberry, M. & Miikkulainen, R. (1999). Using a se-
Crocker, 2004).                                                   quential SOM to parse long-term dependencies. In
                                                                  Proc. of the 21st Annual Conf. of the Cognitive Sci-
                 Acknowledgments                                  ence Society, Hillsdale, NJ. Erlbaum.
Igor Farkaš was supported by the Alexander von Hum-            Mayberry, M. & Miikkulainen, R. (2003).            Incre-
boldt Foundation and by Slovak Grant Agency for Sci-              mental nonmonotonic parsing through semantic self-
ence. He was on leave from the Department of Applied              organization. In Proc. of the 25th Annual Conf. of the
Informatics, Comenius University in Bratislava, and In-           Cognitive Science Society, Mahwah, NJ. Erlbaum.
stitute of Measurement Science, Slovak Academy of Sci-         Parfitt, S., Tiňo, P., & Dorffner, G. (2000). Graded
ences. Matthew Crocker gratefully acknowledges the                grammaticality in prediction fractal machines. In Ad-
financial support of the German Research Foundation               vances of NIPS, 12. MIT Press.
(SFB-378, project “Alpha”). Both authors are thank-
                                                               Pearlmutter, B. (1995). Gradient calculations for dy-
ful to Marty Mayberry for fruitful discussions, and three
                                                                  namic recurrent neural networks: A survey. IEEE
anonymous reviewers for constructive comments.
                                                                  Trans. on Neural Networks, 6(5), 1212-1228.
                       References                              Rohde, D. (2002). The simple language generator:
                                                                  Encoding complex languages with simple grammars.
Barreto, G., Araújo, A., & Kremer, S. (2003). A tax-             http://tedlab.mit.edu/˜dr/SLG.
   anomy of spatiotemporal connectionist networks re-
   visited: The unsupervised case. Neural Computation,         Rohde, D. & Plaut, D. (1997). Simple recurrent net-
   15, 1255-1320.                                                 works and natural language: How important is start-
                                                                  ing small? In Proc. of the 19th Annual Conf. of the
Christiansen, M. & Chater, N. (1999). Toward a connec-            Cognitive Science Society (pp. 656-661). Hillsdale, NJ:
   tionist model of recursion in human linguistic perfor-         Erlbaum.
   mance. Cognitive Science, 23(2), 157-205.
                                                               Servan-Schreiber, D., Cleeremans, A., & McClelland, J.
Christiansen, M. & Dale, R. (2001). Integrating distribu-         (1991). Graded state machines: The representation of
   tional, prosodic and phonological information in a con-        temporal contingencies in simple recurrent networks.
   nectionist model of language acquisition. In Proc. of          Machine Learning, 7(2-3), 161-193.
   the 23rd Annual Conf. of the Cognitive Science Society
   (pp. 220-225). Mahwah, NJ: Lawrence Erlbaum.                 Stolcke, A. (2002). SRILM – an extensible language
                                                                  modeling toolkit. In Proc. International Conf. on Spo-
Elman, J. (1990). Finding structure in time. Cognitive            ken Language Processing (pp. 901-904). Denver, CO.
   Science, 14,179-211.
                                                               Tarassenko, L. & Roberts, S. (1994). Supervised and
Elman, J. (1991). Distributed representations, simple             unsupervised learning in radial basis function classi-
   recurrent networks, and grammatical structure. Ma-             fiers. IEE Proceedings – Visual Image Signal Process-
   chine Learning, 7, 195-225.                                    ing, 141(4), 210-216.
Elman, J. (1993). Learning and development in neural           Tiňo, P. & Farkaš, I. (2005). On non-markovian topo-
   networks: The importance of starting small. Cogni-             graphic organization of receptive fields in Recursive
   tion, 48(1), 71-99.                                            Self-Organizing Map. In Advances in Natural Compu-
Hammer, B., Micheli, A., Sperduti, A., & Strickert, M.            tation (pp. 676-685). Lecture Notes in Computer Sci-
   (2004a). Recursive self-organizing network models.             ence, Springer.
   Neural Networks, 17(8-9), 1061-1085.                        Tiňo, P., Farkaš, I., & van Mourik, J. (2005). Recursive
Hammer, B., Micheli, A., Strickert, M., & Sperduti, A.            Self-Organizing Map as a contractive iterative func-
   (2004b). A general framework for unsupervised pro-             tion system. In Intelligent Data Engineering and Auto-
   cessing of structured data. Neurocomputing, 57, 3-35.          mated Learning (pp. 327-334). Lecture Notes in Com-
Jaeger, H. (2001). Short term memory in echo state                puter Science, Springer.
   networks. (Tech. Rep. GMD Report 152). German               Tiňo, P., Farkaš, I., & van Mourik, J. (2006). Dy-
   National Research Center for Information Technology.           namics and topographic organization in recursive self-
James, D. & Miikkulainen, R. (1995). SardNet: a self-             organizing map. Accepted to Neural Computation.
   organizing feature map for sequences. In Advances in        Tiňo, P., Čerňanský, M., & Beňušková, Ľ. (2004).
   NIPS, 7 (pp. 577-584). MIT Press.                              Markovian architectural bias of recurrent neural net-
Kohonen, T. (1990). The self-organizing map. Proceed-             works. IEEE Transactions on Neural Networks, 15,
   ings of the IEEE, 78(9), 1464-1480.                            6-15.
Maass, W., Natschläger, T., & Markram, H. (2002).             Voegtlin, T. (2002). Recursive self-organizing maps.
   Real-time computing without stable states: A new               Neural Networks, 15(8-9), 979-992.
   framework for neural computation based on pertur-           Werbos, P. (1990). Backpropagation through time: what
   bations. Neural Computation, 14(11), 2531-2560.                it does and how to do it. Proceedings of the IEEE, 78,
Mayberry, M. & Crocker, M. (2004). Generating seman-              1550-1560.
   tic graphs through self-organization. In Proceedings of
                                                           1280

