UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Using Ideal Observers in Higher-order Human Category Learning
Permalink
https://escholarship.org/uc/item/2hm1829m
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Holyoak, Keith J.
Hummel, John E.
Kittur, Aniket
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                 Using Ideal Observers in Higher-order Human Category Learning
                                                 Aniket Kittur (nkittur@ucla.edu)
                                          Keith J. Holyoak (holyoak@lifesci.ucla.edu)
                            Department of Psychology, University of California, Los Angeles CA 90095
                                     John E. Hummel (jehummel@cyrus.psych.uiuc.edu)
                     Department of Psychology, University of Illinois at Urbana-Champagne, Urbana IL 61820
                              Abstract                                 assume that differences in performance capture theoreti-
   Ideal observer models have proven useful in investigating as-
                                                                       cally-central differences between conditions. However,
   sumptions about human information processing in a variety of        there may be differences between conditions that are not
   perceptual tasks. However, these models have not been ap-           relevant to the variable being measured (e.g., noise). Ideal
   plied in the area of higher-order category learning. We de-         observers can provide a theoretical upper bound on human
   scribe a simple Bayesian ideal observer and apply it to em-         performance (given a set of assumptions), and can be used
   pirical data on category learning. We describe an experiment        to control for some of these extraneous variables.
   in which we found that acquisition of family resemblance               In this paper we describe a simple method for creating an
   categories was drastically impaired if the categories were de-      ideal observer model that takes as input features, and rela-
   fined by relations between features rather than by the features     tions between features, of the sort commonly used in cate-
   themselves. An ideal observer was used to test whether this
   effect could be accounted for by inherent information differ-
                                                                       gory learning studies with artificial stimuli. The ideal ob-
   ences between the conditions. A comparison of participants’         server assumes that the experimenter (but not necessarily
   performance to the model found a significant difference in ef-      the learner) has full knowledge of the generating model used
   ficiency of learning even after accounting for information dif-     to construct stimuli based on these features and relations.
   ferences between conditions. This analysis illustrates how          This assumption is typically met in experimental category
   ideal observer methods can provide useful tools for analyzing       learning paradigms in which artificial stimuli are used. We
   higher-order category learning.                                     will first describe the model and then apply it by simulating
   Keywords: categorization, category learning, relations, fea-        performance in an actual category learning experiment.
   tures, ideal observer
                                                                                                  The Model
                          Introduction
   An ideal observer model makes optimal use of a set of                  The model uses a Bayesian framework to assign stimuli to
given information in performing a task. Such models have               categories and to learn from labeled feedback. We use a
traditionally proven useful in investigating human informa-            version of a naïve Bayesian classifier, one of the simplest
tion use in various perceptual tasks by providing an upper             probabilistic classifiers, which is optimal when all input
bound or benchmark by which to measure performance. If a               features are independent (and can even be optimal in certain
human can perform at the same level as (or better than) the            less restricted circumstances; see Domingos & Pazzani,
ideal model, then we know that the human is making use of              1997). The naïve Bayesian classifier makes the assumption
all of the available information in the situation (or, in the          that all features of a given category are generated independ-
case of humans outperforming the ideal, more information               ently, that is:
than was available to the model). If humans underperform                  p (C , F1 ,..., Fn ) = p (C ) p ( F1 | C )... p ( Fn | C ) (1)
the ideal model, the difference can often highlight specific           for class variable C (which represents all possible categories)
constraints that limit human information processing. The               and feature variables 1 through n. Applying Bayes rule re-
degree to which human performance approaches that of an                sults in the following equation:
ideal observer can provide a measure of processing effi-                                                      n
ciency.                                                                                             p (C )∏ p ( Fi | C )
   Ideal observers have most commonly been applied to un-
                                                                        p (C | F1 ,..., Fn ) =              i =1
                                                                                                                                     (2)
derstanding human low-level visual tasks involving detec-                                                          n
tion and discrimination (see Geisler, 2003), though they                                        ∑C [ p(C )∏ p( Fi | C )]
have also been applied to tasks such as reading (Legge,                                                          i =1
Klitz, & Tjan, 1997), object recognition (Liu, Knill, & Ker-           The denominator in Equation 2 is a normalization constant
sten, 1995), and reaching (Trommershäuser, Gepshtein, Ma-              that is identical for all categories and thus often ignored for
loney, Landy, & Banks, 2004). However, few studies have                simplicity (though implemented in the model). With two
applied ideal observer methods to higher-order cognitive               equally-probable categories (as is most common in category
tasks, at least in part because of the difficulty of specifying        learning paradigms) p(C) is also constant (.5), and thus the
exactly what is ideal. Instead, most studies of human cate-            main determinant of classification is p(Fi|C). This probabil-
gory learning compare conditions against each other and                ity is calculated in the following manner:
                                                                   435

                                n Fi |C + α Fi                           category learning paradigms focus on differences in learning
                p( Fi | C ) =                                  (3)       rates, with a common metric being the number of trials
                                 nC + α C                                needed to reach a certain performance criterion. Viewed in
where n Fi |C is the number of items with feature Fi in cate-            terms of statistical sampling (e.g., the number of samples
                                                                         needed to learn a certain distribution), this metric provides a
gory C, n c is the total number of items in category C, and              natural comparison of human and model learning. Specifi-
                                                                         cally, we can define sampling efficiency as the ratio of the
αF   i
       and α C are   uniform priors1. Equation 3 can be inter-           number of trials the model needed to learn to criterion to the
preted as updating a uniform prior with new information,                 number of trials a human needed (see Scholkopf & Smola,
with the prior eventually overwhelmed as more features are               2002; Stankiewitz, 2003):
observed.                                                                                       ttc mod
   The classifier can be extended to reflect underlying de-                                                     (6)
pendencies between features that are not independently gen-                                     ttc par
erated. This refinement can often be useful in categoriza-               where ttcmod is the trials to criterion needed for the model
tion studies when one feature constrains the values of other             and ttcpar the trials needed for the participant. The closer
features. When these dependencies are known, they can be                 human performance comes to the ideal, the higher the effi-
incorporated into the model by retaining the relevant condi-             ciency.
tional probabilities. For example, Equation 4 is a toy model                In many ways the ideal observer described here is similar
with two features in which feature 2 is dependent on feature             to Anderson’s (1991) “rational” theory of categorization.
1 (the normalization constant is left out for simplicity):               However, Anderson focuses on determining the optimal
                                                                         categorization given a general environment in pursuit of a
  p (C | F1 , F2 ) = p (C ) p ( F1 | C ) p ( F2 | C , F1 ) (4)           descriptive theory of human categorization. In contrast, our
                                                                         ideal observer simply aims to be normative in a specific
                                                                         environment for which the structure and generating model is
   Equation 3 can also be extended for features that are de-
                                                                         known, and to provide a benchmark or upper bound on hu-
pendent on other features, becoming:
                                                                         man performance. Thus many of the goals and assumptions
                                                                         of Anderson’s model are very different from the ideal ob-
                                 n Fi |C , F j + α Fi                    server described here. For example, since we know and
             p( Fi | C , F j ) =                              (5)
                                                                         capture the dependencies between features, we do not make
                                      nC + α C
                                                                         the simplifying assumption that all features are conditionally
where Fi is dependent on feature F j .                                   independent. Dropping this assumption is necessary in or-
   The algorithm for category learning operates as follows.              der to maintain optimality for the types of generating mod-
First, a new example is presented to the model without cate-             els commonly used in higher-order category learning, where
gory label information. The probability of its being in each             features are often constrained by the values of other rela-
category is calculated based on previously-observed labeled              tions or features. Also, instead of predicting an unseen fea-
examples, and the resulting probabilities are used to assign a           ture (such as the category label) through chained inference,
predicted category to the example2. The typical way to clas-             we focus on the simpler task of predicting a category class
sify a new example is to choose the category with the maxi-              given a set of features. This simpler goal allows us to avoid
mum probability of generating the example (Geisler, 2003).               using weighted category averages and only requires compu-
Once category assignment is complete the example is placed               tation of the maximum likelihood category. Finally, we
into the observed set along with its category label. This step           avoid the need for an empirically-determined variable gov-
simulates the effect of feedback, with the new example now               erning the probability of creating a new category (Ander-
affecting future classification judgments. Order of presenta-            son’s “coupling probability”).
tion is important: like the participants, the model’s predic-               We will now apply this ideal observer in order to model
tions can only be based on previously seen exemplars.                    learning in a categorization experiment in which the differ-
   There are many ways to compare the model’s perform-                   ent conditions may have different types and amounts of in-
ance with humans. One possibility is to use a metric based               formation associated with them.
on the number of correct and incorrect trials, which is per-
haps the closest analog to how ideal observers have been                                       The Experiment
used in recent studies (Geisler, 2003). However, many                          A fundamental shift in the understanding of categoriza-
                                                                         tion resulted from the “family resemblance” view of catego-
                                                                         ries, which argued that many categories have a graded struc-
1
  More specifically, the αFis describe the parameters of a Dirichlet     ture based on shared features (Medin & Schaffer, 1978;
prior in which all values are set to 1, with their sum being αc.         Rosch, 1976; Rosch & Mervis, 1975; Wittgenstein, 1953).
2
  One issue with the algorithm is how to get it started. Although
                                                                         The family resemblance view has had great success ac-
there are a number of justifiable methods, here we start the model
with the smallest number of examples for which there is one ex-
                                                                         counting for peoples’ learning and generalization of catego-
ample for each category.                                                 ries that can be represented as simple lists of features. Such
                                                                     436

categories can be learned implicitly and automatically, with           identical family resemblance structure. Whereas learning
feature-category associations not necessarily available to             family resemblance categories based on simple features may
conscious verbalization (Ashby, Maddox, & Bohil, 2002;                 be done implicitly through tracking and averaging the fea-
Ashby & Waldron, 1999).                                                tures of the exemplars of each category, learning relational
   However, much of human conceptual knowledge is com-                 categories will be much more difficult because the same
posed of categories that cannot be represented as simple               feature(s) may be associated with multiple categories, de-
features (Barsalou, 1983; Keil, 1989; Murphy & Medin,                  pending on the relations involved. To test this hypothesis
1985; Rips, 1989; Ross & Spalding, 1994). Rather, many                 we used a 2x2 between-subjects design, in which categories
concepts are based on the relationships between things                 either had a single dimension perfectly predictive of cate-
rather than the literal features of the things themselves. For         gory membership (deterministic) or had a family resem-
example, a barrier is a relational concept that can be as con-         blance structure in which three out of four dimensions were
crete as a wall or moat or as abstract as a lack of money or           characteristic of the category but no single dimension was
the color of one’s skin. Relational concepts abound in eve-            perfectly predictive. Dimensions were defined either by
ryday life, with examples including social understanding (a            individual feature values or by the relations between fea-
love triangle), law (breach of contract), religion (atonement          tures. We predicted an interaction between category struc-
for sins), science (conservation of energy), as well as basic          ture and type, such that the relational family resemblance
perception (recognizing arrangements of objects as scenes)             condition would be much more difficult to learn than any of
(e.g., Gentner & Kurtz, 2005, Holyoak & Thagard, 1995).                the other three conditions.
   Although relational concepts are fundamental to human
intelligence, our understanding of how we learn them is                                           Method
poor compared to our understanding of feature-based cate-
gories. A reasonable and parsimonious hypothesis is that               Subjects. 96 University of California, Los Angeles under-
relational categories act just like feature-based categories           graduates participated for partial fulfillment of course re-
with the features replaced by relations—that is, concept               quirements.
learning may be a single unified process that can take either
features or relations as input. This view predicts that rela-
tional categories should show the same kind of family re-
semblance structure evidenced by feature-based categories,
thus generalizing what we have learned about category
learning from feature-based to relational categories.
   However, there is evidence that relations and features
may be psychologically distinct. For example, Medin,
Goldstone, and Gentner (1990, 1993) demonstrated strong
empirical differences between relational and feature-based
similarity, suggesting that relations and features may rely on
separate, competing processes for assessing similarity.
Consistent with these findings, some researchers have ar-
gued that feature lists are fundamentally inadequate to rep-
resent relational concepts, and that such concepts must in-
stead be mentally represented as relational structures such as
“schemas” or “theories” (Gentner, 1983; Holland, Holyoak,
Nibett, & Thagard, 1986; Hummel & Holyoak, 2003; Keil,
1989; Murphy & Medin, 1985). In such accounts, learning
a relational category is more akin to inducing a schema than
to learning a list of diagnostic features. Most accounts of
schema induction assume that a shared, deterministic cohe-
sive element is necessary to create the schema in the first
place (Hummel & Holyoak, 2003; Kuehne et al., 2000).
   We conducted an experiment to test whether relational
and feature-based categories were learned in similar ways3.               Figure 1. Examples of family resemblance categories. De-
Specifically, we hypothesized that relational categories in               terministic relational categories were formed by removing
which no single defining element existed—as is the case in                one exemplar from each category. The table depicts the di-
family resemblance categories—would prove drastically                     mensions categories were defined on, as well as the value of
more difficult to learn than feature-based categories with an             each exemplar on the dimensions (filled = value 1, empty =
                                                                          value 2). For example, in relational category exemplar i, the
3
  The experiment described here is based on pilot data reported in        octagon (O) is bigger, darker, above, and behind the square
Kittur, Hummel, and Holyoak (2004), which describes in more               (S). Shown are only a small subset of all instantiations of
detail the methods used and additional measures collected.                the four exemplar types for a category.
                                                                   437

Stimuli and Procedure. All stimuli were composed of an
                                                                              Efficiency (Model/Human) Trials to Criterion (Model) Trials to Criterion (Human)
octagon and a square set in a fixed background resembling a                                                                                                      400              Det   FR                (a)
                                                                                                                                                                 350
computer chip. Either the relations between the two shapes
                                                                                                                                                                 300
(relational condition) or the individual features of each                                                                                                        250
shape (feature-based condition) determined category mem-                                                                                                         200
bership of each exemplar. Relational categories were de-                                                                                                         150
fined by whether the octagon was 1) larger, 2) darker, 3)                                                                                                        100
                                                                                                                                                                  50
vertically above, and 4) in front of the square (see Figure 1).                                                                                                    0
Feature-based categories were defined by individual abso-                                                                                                              Featural              Relational
lute feature values: 1) size of the octagon, 2) color of the                                                                                                     25                                       (b)
octagon, 3) size of the square, and 4) color of the square.
   Crossed with the feature-based and relational conditions                                                                                                      20
was the structure of each category. In the family resem-                                                                                                         15
blance condition, each category member had three out of                                                                                                          10
four dimensions consistent with its category and one incon-
                                                                                                                                                                  5
sistent dimension. In the deterministic condition one di-
mension was perfectly diagnostic across all exemplars. This                                                                                                       0
design yielded four conditions to which participants were                                                                                                              Featural              Relational
randomly assigned: relational family resemblance or deter-                                                                                                       0.5
ministic (R-FR or R-D) and feature-based family resem-                                                                                                                                                    (c)
                                                                                                                                                                 0.4
blance or deterministic (F-FR or F-D).
   On each trial of the acquisition phase, a participant                                                                                                         0.3
viewed one exemplar, categorized it as a “math” or “graph-                                                                                                       0.2
ics” chip, and received accuracy feedback. Acquisition con-                                                                                                      0.1
tinued until the participant reached criterion (>88% correct
for two consecutive blocks4).                                                                                                                                     0
                                                                                                                                                                       Featural              Relational
                    Behavioral Results                                     Figure 2. (a) Mean trials to criterion taken by participants to
   The relational family resemblance condition proved much                 learn the categories. Det=Deterministic, FR=Family Re-
more difficult to learn than the other three conditions: 22%               semblance. (b) Mean trials to criterion for the model to
of participants in the relational family resemblance condi-                learn the categories given the same stimuli in the same order
tion did not learn to criterion within 600 trials (no partici-             as each individual participant. (c) Efficiency of human per-
pants in any other conditions failed to learn). All results                formance compared to model performance measured by the
make the extremely conservative assumption that partici-                   ratio of the number of trials needed by the model to learn to
pants who failed to learn would have succeeded on trial 601.               criterion to the number of trials needed by human learners.
   The mean number of trials to criterion for each condition               (Note that efficiency is calculated on a per-subject basis,
is shown in Figure 2a. There were main effects of both                     and so cannot be determined from panels (a) and (b) alone.)
category type (relations vs. features, F(1, 95) = 4.71, p
= .032, and category structure (family resemblance vs. de-                 blance condition is instead due to a difference in the amount
terministic, F(1, 95) = 9.83, p = .002; importantly, there was             of available diagnostic information. In other words, are
also a significant interaction of category type and structure,             people worse only because some conditions are inherently
F(1,95) = 6.14, p = .015, due to extremely impaired acquisi-               more difficult to learn due to lack of information?
tion when the category was defined by relations and had a                    To answer this question we adapted the ideal observer
family resemblance structure.                                              model described earlier to the current experimental task.
                                                                           The features and relations available to participants were
                Ideal Observer Analysis                                    coded as discrete values on separate dimensions and used as
                                                                           inputs to the model. For example, the model received as
  One explanation of these results is that relations and fea-              separate inputs the size of the square, the size of the octagon,
tures are represented and processed differently in the brain,              and the relation of which was bigger. The same information
and that relational categories may not have access to the                  was available to participants, who could use information
machinery that is used to learn feature-based family resem-                about either the features or the relations on each trial. How-
blance categories. However, another explanation could be                   ever, the relational and featural information on a dimension
that the selective impairment of the relational family resem-              were not independent: in the example above, if the relation
                                                                           was “octagon bigger than square”, then knowing the size of
4
                                                                           the octagon provides information about the size of the
 This criterion was chosen so that simple feature-tracking strate-         square (which must be smaller; see Figure 3). To account
gies (e.g., memorizing the associations of single features with            for this dependency, relations were modeled as independent
categories) would lead to sub-criterion performance.
                                                                     438

                                                                           ries with deterministic structure were learned as quickly as
                                                                           deterministic feature-based categories, suggesting that the
                                                                           effect is not merely due to the relational nature of the task.
                                                                           This interaction is inconsistent with the hypothesis that rela-
                                                                           tional categories are learned in the same way as feature-
Figure 3. Relations and features involved in category gen-                 based categories.
eration. Arrows depict dependencies (i.e., constraints) be-                   An ideal observer analysis was used to determine whether
tween relations and features.                                              this impaired learning might be due to inherent informa-
inputs, whereas feature inputs were conditional on their re-               tional differences between conditions. By comparing the
spective relation.5                                                        efficiency of human performance to that of the ideal model,
   For the ideal observer described here to be truly optimal,              we were able to show that objective differences in difficulty
the generating model must meet certain assumptions. First,                 between conditions did not account for the experimental
the distribution of category members must be sampled from                  data. Rather, it appears that relations and features are repre-
independent multinomial distributions with Dirichlet-                      sented or processed differently in human category learning.
distributed parameters. This assumption holds true: cate-                     Identifying exactly how relations and features differ is an
gory members were generated by sampling from independ-                     important subject for future research. One potentially useful
ent multinomial distributions with an equal likelihood of                  approach is to determine what changes to the ideal observer
each member appearing. Second, all dependencies that arise                 could make it more closely match human data. For example,
in generating feature values for each category member must                 what happens when the model does not have perfect mem-
be captured in the conditional probabilities (i.e., relations              ory, or cannot perfectly update its prior? Or when its
constraining features). This assumption is also valid: the                 “working memory” is impaired so it cannot attend to all
dependencies shown in Figure 3 reflect how the exemplars                   relations and features at once? Observing how the model
were generated.                                                            degrades as additional constraints are added could provide
   The ideal observer model was run on each participant’s                  valuable insights into human information processing.
data, and the number of trials necessary to learn to criterion                Alternatively, it is possible that no processing-related
was measured 6 (see Figure 2b). We then computed each                      changes in the ideal observer will capture the dissociation in
participant’s efficiency according to Equation 6. These effi-              the human data. Instead, it may be necessary to take into
ciencies are depicted in Figure 2c. Human performance as                   account the representational difference between simple fea-
measured by statistical efficiency was much worse in the                   tures and relational predicates. It remains an open question
relational family resemblance condition than in any other                  how to incorporate structured predicates into a Bayesian
condition. An ANOVA was performed on the efficiency                        framework; extant analyses of categorization using Bayes-
measure following a log transformation to normalize the                    ian inference treat relations as correlations or unstructured
variances. The results demonstrated that the critical interac-             features rather than as structured predicates (e.g., Kemp et
tion was significant, F(1, 95) = 3.93, p < .05. Since effi-                al., 2004). Indeed, one possible explanation of our results
ciency takes into account differences in model as well as                  may be that the likelihood updating mechanism at work in
human performance, finding an interaction on this measure                  featural categorization may not be used for relational cate-
indicates that the human learning rates for these conditions               gorization, resulting in impairment of relational family re-
were more different than would expected given the inherent                 semblance learning.
difficulty of the conditions. That is, inherent informational                 At first glance the present results appear counterintuitive:
differences between the conditions were insufficient to ac-                relational category learning is severely impaired if no ele-
count for the disparities in human performance.                            ments are constant across all exemplars, yet people seem
                                                                           able to conceptualize family resemblance relational catego-
                                                                           ries, such as Wittgenstein’s (1953) classic example of
                            Discussion                                     “game”. This paradox highlights the need for additional
   The behavioral results revealed a clear impairment in ac-
                                                                           empirical studies. One approach to exploring this seeming
quisition for relational categories defined by a family re-
                                                                           inconsistency may be to examine prior knowledge and ex-
semblance structure, as compared to categories based on
                                                                           perience. While a single constant element may be necessary
features, which are learned quickly whether they had family
                                                                           to learn novel relational categories, when prior knowledge
resemblance or deterministic structure. Relational catego-
                                                                           and experience are brought into play this critical need may
5
                                                                           be reduced. It is possible that a coherent theory that ex-
  A natural question is: should the features be defining in the featu-     plains a relational family resemblance structure might make
ral conditions, rather than the relations? No change is needed in          learning easier (Rehder & Hastie, 2004; Rehder & Ross,
the model because in the featural conditions the dimensions on
which the features were considered dependent (relative size and
                                                                           2001). In addition, repeated experience with the relevant
shade) had the same relational value for both categories. Thus the         relations may lead to low-level chunking of a stimulus, as in
features become effectively independent.                                   chess experts’ memory for board positions. Thus both
6
  A more statistically accurate phrasing of this would read: “the          higher-order causal explanations and lower-order experience
number of samples needed to learn the distribution to a certain            may facilitate relational learning.
degree of accuracy.”
                                                                       439

   In summary, the dissociation between feature-based cate-        Keil, F. C. (1989). Concepts, kinds, and cognitive develop-
gory learning, which is robust to family-resemblance struc-          ment. Cambridge: The MIT Press.
ture, and relation-based category learning, which is not,          Kemp, C., Griffiths, T. L., & Tenenbaum, J. B. (2004). Dis-
suggests that current feature-based models of category               covering latent classes in relational data. MIT AI Memo
learning may have limited applicability to relational catego-        2004-019.
ries. The difficulty for such models is not only that feature      Kuehne, S., Forbus, K. D., Gentner, D., & Quinn, B. (2000).
lists are inadequate to represent relations, but that the two        SEQL: Category learning as progressive abstraction using
kinds of categories are processed differently as well.               structure mapping. Proceedings of the Twenty-second An-
   The present study also demonstrates how ideal observer            nual Conference of the Cognitive Science Society.
methods can be applied in higher-order category learning.          Legge, G. E., Klitz, T. S., & Tjan, B. S. (1997). Mr. Chips:
Here we used an ideal observer to provide an objective               An ideal-observer model of reading. Psychological Re-
measure of the ease of learning in each condition. The               view, 104, 524-553.
model is easy to implement in category learning studies            Liu, Z., Knill, D. C., & Kersten, D. (1995). Object classifi-
with discrete stimuli for which the generating model is              cation for human and ideal observers. Vision Research, 35,
known. We believe that the ideal-observer approach can               549-568.
have general applicability for studies of category learning in     Medin, D. L., Goldstone, R. L., & Gentner, D. (1990). Simi-
which different learning conditions may have different in-           larity involving attributes and relations: Judgments of
formational content.                                                 similarity and difference are not inverses. Psychological
                                                                     Science, 1, 64-69.
                    Acknowledgments                                Medin, D. L., Goldstone, R. L., & Gentner, D. (1993). Re-
                                                                     spects for similarity. Psychological Review, 100, 254-278.
Preparation of this paper was supported by NSF grant SES-
                                                                   Medin, D. L., & Schaffer, M. M. (1978). Context theory of
0350920 to KH. Special thanks to Hongjing Lu, Zili Liu,
                                                                     classification learning. Psychological Review, 85, 207-
and Barbara Spellman for helpful comments and discussion.
                                                                     238.
                                                                   Murphy, G. L. (2002). The big book of concepts. Cambridge:
                         References                                  The MIT Press
Anderson, J. R. (1991). The adaptive nature of human cate-         Murphy, G. L., & Medin, D. L. (1985). The role of theories
   gorization. Psychological Review, 98, 409-429.                    in conceptual coherence. Psychological Review, 92, 289-
Ashby, F. G., Maddox, W. T., & Bohil, C. J. (2002). Obser-           316.
   vational versus feedback training in rule-based and infor-      Rehder, B., & Hastie, R. (2004). Category coherence and
   mation-integration category learning. Memory & Cogni-             category-based property induction. Cognition, 91, 113-
   tion, 30, 666-677.                                                153.
Ashby, F. G., & Waldron, E. M. (1999). On the nature of            Rehder, B., & Ross, B. H. (2001). Abstract coherent catego-
   implicit categorization. Psychonomic Bulletin & Review,           ries. Journal of Experimental Psychology: Learning,
   6, 363-378.                                                       Memory, & Cognition, 27, 1261-1275.
Barsalou, L. W. (1983). Ad hoc categories. Memory & Cog-           Rips, L. J. (1989). Similarity, typicality, and categorization.
   nition, 11, 211-227.                                              In S. O. Vosniadou, Andrew (Ed.), Similarity and ana-
Domingos, P., & Pazzani, M. J. (1997). On the optimality of          logical reasoning. New York: Cambridge University
   the simple Bayesian classifier under zero-one loss. Ma-           Press.
   chine Learning, 29, 103-130.                                    Rosch, E., & Mervis, C. B. (1975). Family resemblances:
Geisler, W. S. (2003). Ideal Observer analysis. In L. Cha-           Studies in the internal structure of categories. Cognitive
   lupa, and J. Werner (Eds.), The visual neurosciences (pp.         Psychology, 7, 573-605.
   825-837). Boston: MIT Press.                                    Rosch, E., Simpson, C., & Miller, R. S. (1976). Structural
Gentner, D., & Kurtz, K. J. (2005). Learning and using rela-         bases of typicality effects. Journal of Experimental Psy-
   tional categories. In W. K. Ahn, Goldstone, R.L., Love,           chology: Human Perception & Performance, 2, 491-502.
   B.C., Markman, A.B., & Wolff, P.W. (Ed.), Categoriza-           Ross, B. H., & Spalding, T. L. (1994). Concepts and catego-
   tion inside and outside the lab. Washington, D.C.: APA.           ries. In R. J. Sternberg (Ed.), Thinking and problem solv-
Gentner, D. (1983). Structure-mapping: A theoretical                 ing. Handbook of perception and cognition (2nd ed.) (pp.
   framework for analogy. Cognitive Science, 7, 155-170.             119-148). San Diego, CA: Academic Press, Inc.
Holland, J. H., Holyoak, K. J., Nisbett, R. E., & Thagard, P.      Scholkopf, B., & Smola, A. J. (2002). Learning with Ker-
   R. (1986). Induction: Processes of Inference, Learning,           nels: MIT Press.
   and Discovery. Cambridge, MA, US: The MIT Press.                Stankiewicz, B.J., Legge, G.E., Mansfield, J.S., & Schlicht,
Holyoak, K. J., & Thagard, P. (1995). Mental leaps: Anal-            E.J. (in press). Lost in virtual space: Studies in human and
   ogy in creative thought. Cambridge, MA: The MIT Press.            ideal spatial navigation. Journal of Experimental Psy-
Hummel, J. E., & Holyoak, K. J. (2003). A symbolic-                  chology: Human Perception and Performance.
   connectionist theory of relational inference and generali-      Wittgenstein, L. (1953). Philosophical investigations. New
   zation. Psychological Review, 110, 220-264.                       York: Macmillan.
                                                               440

