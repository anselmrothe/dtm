UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Is Recurrence Redundant? Revisiting Allen and Christiansen
Permalink
https://escholarship.org/uc/item/3cm8x77x
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Author
Rytting, C. Anton
Publication Date
2006-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

             Is Recurrence Redundant? Revisiting Allen and Christiansen (1996)
                                          C. Anton Rytting (rytting@ling.ohio-state.edu)
                                                          The Ohio State University
                                                          Department of Linguistics
                                                                222 Oxley Hall
                                                                1712 Neil Ave.
                                                          Columbus, OH 43210 USA
                               Abstract                                  turn). As the child attends to the speech input to which it is
                                                                         exposed, it notices patterns in the signal (Saffran et al., 1996).
   Several influential connectionist models of the word segmenta-        Christiansen et al. (1998) conceive of a second, immediate
   tion task (e.g. Cairns, Shillcock, Chater, & Levy, 1997; Chris-
   tiansen, Allen, & Seidenberg, 1998) follow Elman (1990) in            task in language acquisition: namely, to continually update a
   using simple recurrent networks (SRNs). The use of SRNs in            statistical representation of these salient patterns.
   this context appears to be traditional rather than independently          This process is exemplified in a simple model presented in
   motivated.                                                            Allen and Christiansen (1996). In this scenario, a network
   This paper investigates whether alternatives to SRNs can              receives input one phone at a time. Its immediate task is learn
   achieve similar performance. Specifically, it reports a repli-        to predict the identity of the next phone.1 The immediate task
   cation of part of Allen and Christiansen (1996), and a compar-
   ison with multi-layer perceptrons (MLPs). The major results           is “self-supervised” in that the next input provides immediate
   are confirmed; however, the SRN topology is shown not to be           feedback as to what the preceding output should have been.
   necessary for the task addressed. MLPs perform equally well,              To model the contribution of distributional regularities at
   even when they have access to less context.                           the phone level to the discovery of word boundaries, a neural
                                                                         network with an input unit and an output unit for each phone
                           Introduction                                  may be constructed, and the likelihood of a word boundary
Word segmentation is the task of recovering word boundaries              approximated by measuring points at which the error in pre-
from an initially unsegmented stream of linguistic input. En-            dicting the next phone is high (see Cairns et al., 1997, for a
glish writing marks these boundaries, making the task trivial,           model based on this idea). In order to model the interaction of
but Chinese writing does not, neither does any variety of nat-           utterance-boundary info and distributional regularity, an ad-
ural speech.                                                             ditional unit, symbolizing an utterance boundary or pause in
   If a language learner is to make any headway in acquir-               speech, is added to the input layer, and corresponding unit to
ing the words in a language, that learner must learn how to              the output layer. The idea of having multiple output nodes
pull these words out of running speech and hear them as sep-             learning multiple problems simultaneously is referred to as
arate words. By learning the indirect cues that signal word              “hints” or “catalyst” nodes (see Suddarth & Kergosien, 1991).
boundaries for a given language, the word learner solves the             The intuition (following Aslin et al., 1996) is that since utter-
word segmentation task. This begins early in the language                ance boundaries are a subset of word boundaries with more
acquisition process: studies show that babies begin to learn             or less representative properties, the utterance boundary out-
word segmentation strategies around 7-9 months after birth               put unit should be activated not only at utterance boundaries,
(see e.g. Jusczyk, Houston, & Newsome, 1999, for a review).              but (to a lesser degree) at all word boundaries. Hence, above-
   Many of the initial experimental studies of word segmen-              average activation of the utterance boundary output unit is
tation focused on individual cues: prosody (Jusczyk, Cut-                used to measure the network’s performance on the derived
ler, & Redanz, 1993), allophonic variations (Jusczyk, Hohne,             task: identifying word boundaries. The other output units,
& Bauman, 1999), distributional regularities (Saffran, Aslin,            corresponding to phones, can be thought of as catalyst units
& Newport, 1996), and phonotactics (Friederici & Wessels,                to the utterance boundary unit.
1993). Similarly, early computational simulations of the                     A major point of Allen and Christiansen (1996) is that word
word segmentation task focused on single cues. Brent and                 boundaries can be learned in this way only for languages with
Cartwright (1996) used distributional regularities as a cue,             particular statistical properties. It is straightforward to con-
combined with minimum description length as a language-                  struct artificial languages that lack these properties. Specifi-
independent heuristic. Cairns et al. (1997) also modeled the             cally, for a language where each syllable is equally likely to
use of distributional regularities, but as a precursor to a later        follow any other (or to end a word or an utterance), the net-
stage using stress (which was not explicitly modeled). Aslin             work can learn at best only syllable boundaries. But if certain
et al. (1996) modeled the generalization of segmental cues at            segments or groups of segments (such as syllables) are more
the ends of utterances to the ends of words.                                  1 The assumption that the identity of the phone is unambiguously
   Christiansen, Allen, and Seidenberg (1998) explicitly ad-             observable is itself an idealization. In real life, the intended segment
dresses the interaction among several cues: distributional reg-          is not observable with perfect accuracy, particularly for infants who
ularities, stress, and utterance-boundary information. Their             may be presumed to be still learning the language’s inventory of
model reflects a particular conception of the word segmen-               sounds. The automatic speech recognition community rightly treats
                                                                         phone identity as part of the hidden structure, not the observable.
tation task within the larger context of language acquisition.           Fully addressing this problem would unduly complicate the model
The infant’s primary task is the comprehension of the speech             under discussion, although Christiansen and Allen (1997) addresses
around it (as well as producing comprehensible speech in                 it in part.
                                                                     2065

likely to appear at the ends of words than others (as seems to        challenging, these results show that the additional tasks ac-
be the case with most human languages), and a sufficiently            tually help the net learn the derived task of interest. Hence,
large sample of the language is available to the learner, then        it provides a plausible way of combining multiple cues in a
the network can learn.                                                computational model.
                                                                         On the other hand, the specific type of network used (SRN
     A summary of the Allen and Christiansen                          versus time-windowed MLP) seems to be inherited from ear-
                                                                      lier studies such as Elman (1990). Elman (1990) developed
                    (1996) simulations
                                                                      his SRN topology to deal with sequential problems such as
Method                                                                language, and SRNs have been used for modeling a num-
                                                                      ber of language tasks where memory of indefinite length is
To show this point, two artificial “mini-languages” were con-         needed. However, the usefulness of SRNs for modeling mem-
structed to train the net: a “variable transition probability”        ory of longer time sequences has been shown to be some-
(vtp) language, and a language with “flat” transitional proba-        what limited, a weakness sometimes known as the “latch-
bilities between syllables. Each of these languages used the          ing” problem (Hochreiter, Bengio, Frasconi, & Schmidhuber,
same twelve syllable types: ‘b’, ‘d’, ‘p’, or ‘t’ followed by ‘a’,    2001; Tino, Cernansky, & Benuskova, 2004). Some have at-
‘i’, or ‘u’. Each language consisted only of three-syllable,          tempted to develop alternative ways of modeling memory, to
six-segment (CVCVCV) words. The “flat” language con-                  overcome some of these limitations (Hochreiter & Schmid-
tained 12 such words constructed to maintain a word-internal          huber, 1997). But for some language processing tasks, par-
transitional probability of 0.667 from one syllable to another;       ticularly those dealing with speech processing (rather than
the “vtp” language used 15 words following different restric-         longer-range or syntactically complex tasks), arguably sim-
tions: for example, no word begins in ‘b’ or ends in ‘u’.             pler and easier-to-train models such as time-windowed multi-
   Both languages were trained using a simple recurrent net           layer perceptrons (MLPs) are still used (e.g. Zhu, Chen, Mor-
(SRN) with 8 units each in the input and output layers and            gan, & Stolcke, 2004; Morgan, Chen, Zhu, & Stolcke, 2004).
30 each in the hidden and context layers. Word boundaries                MLPs have also been used to model word segmenta-
were not explicitly marked, but utterance boundaries (corre-          tion. Aslin et al. (1996) decided to use a time-windowed
sponding to the last input unit) were placed at intervals rang-       MLPs rather than SRNs, explicitly stating their preference
ing from 2 to 6 words long. Training was done for seven it-           on grounds of simplicity. An additional complexity result-
erations over a corpus with 120 instances of each word in the         ing from this choice is the need to explicitly vary the length
mini-language; testing was done on a corpus without marked            of the time window, but this too had an upside, in that one
utterance boundaries.                                                 knows explicitly what contribution each time step in the prior
                                                                      history is making.
Results                                                                  Allen and Christiansen (1996) do not explicitly defend
For all experiments, the dependent measure was activation of          SRNs as an essential design choice, although they seem to
the utterance boundary output unit. On the vtp language, the          prefer it implicitly to feed-forward networks (given that they
network predicts a significantly higher activation for word-          incorporate the cue from Aslin et al. (1996) within their re-
boundary positions (0.204 on average) than for word in-               current framework). Whether their use of SRNs rather than
ternal positions (0.04 on average), including other syllable-         MLPs is crucial to their model is an open question. While
boundary positions. The network trained and tested on the             this question is perhaps secondary to other considerations, it
flat language showed higher activation at syllable- boundary          is nevertheless worth considering for two reasons:
positions, but these did not differ significantly between word-          First, the empirical clarity of a computational model de-
boundary and word-internal syllables (although the graph              pends in part on knowing and stating as precisely as possible
suggests that the activation after the first syllable of a word       which aspects of the model are essential to its success in sim-
may be very slightly less). These results are seen to validate        ulating human behavior and which are accidental. A second
the points discussed above.                                           and more practical reason is simple efficiency: if MLPs make
                                                                      just as good predictions, and are easier to work with and faster
               Issues of network structure                            to train, then this allows for a greater number of explorations
                                                                      with less effort.
While the findings in Allen and Christiansen (1996) may
seem of minor importance, their significance may be found                    Revisiting Allen and Christiansen (1996)
in the basis of this same architecture in a number of stud-
ies involving actual language (Christiansen & Allen, 1997;            Simulation 1: Simple Recurrent Networks
Christiansen et al., 1998; Christiansen, Conway, & Curtin,            First, a replication of part of Allen and Christiansen (1996)
2005; Curtin, Mintz, & Christiansen, 2005). Before building           was performed using the SRN package available within the
on these works, it is useful to re-examine the initial assump-        conx neural network toolkit (Blank, Kumar, Meeden, &
tions made and methods used to distinguish the essential char-        Yanco, 2003).2 The training parameters were matched to that
acteristics from the accidental.                                      work as nearly as possible. The network consists of an input
   For example, the notion of hints is essential to Allen and         layer and an output layer of eight units each (one for each
Christiansen’s (1996) argument, and is an innovation over             symbol in the mini-language, including the utterance bound-
earlier studies such as Elman (1990); Cairns et al. (1997);
Aslin et al. (1996). Where one might suppose that a net-                  2 available as part of the Python Robotics toolkit at pyro-
work would find dealing with multiple prediction tasks more           robotics.org
                                                                  2066

ary marker), along with thirty units each in the hidden and            A useful summary statistic for an ROC is the area under
context layers. “One-hot” encoding was used: each sym-              the curve (AUC), which corresponds to the probability a ran-
bol was associated with a single input, for which the acti-         domly chosen pair of one positive item and one negative item
vation was 1 if the input was that symbol, and 0 otherwise.         will be correctly ranked (Hanley & McNeil, 1982). In this do-
As in the original study, a learning rate of 0.1 and momen-         main, this provides the probability, given a word-final symbol
tum of 0.95 was used, and weights were initialized with ran-        and a non-word-final symbol (both randomly selected), that
dom values from a flat distribution ranging between -0.25 and       the activation of the utterance-boundary unit at the word-final
0.25. The activation function used was a logistic sigmoid           symbol will be higher than the activation at the the non-word-
σ(x) = 1/(1 + e−x). Standard backpropagation was used; er-          final symbol.
ror was measured with a squared error loss function.                   An AUC of 0.5 is no better than chance; perfect discrimi-
   The artificial languages used for the training and testing       nation would have an AUC of 1. For each of the conditions
data were not available and had to be recreated according           here, an ROC curve was calculated using only the syllable
to the specifications in the text. These recreated languages        boundary positions, rather than for every phone (since the
are available from the author’s web-site.3 Separate training        syllable-internal positions are trivial to learn). As shown in
and test corpora are used: the training corpus for each lan-        Figure 1, the SRN in the vtp condition achieves a mean AUC
guage is marked with utterance boundaries; the test corpus is       of 0.85 (min 0.81, max 0.90), meaning that it performs bet-
not. The level of activation of the utterance-boundary output       ter than chance at distinguishing word boundaries from other
node is predicted to be higher at word boundaries than at non-      syllable boundaries. In the flat condition the SRN output has
boundary positions. Since a certain amount of variation is to       a mean AUC of 0.50 (min 0.43, max 0.54), showing it does
be expected from the random initial weights, the results of         no better than a syllable-boundary detector.
sixteen iterations, each with different starting weights, were
averaged together for each condition tested.                        Simulation 2: Multi-layer Perceptrons
   The results from this replication were qualitatively similar
to those reported in (Allen & Christiansen, 1996), although         In the second experiment, the same simulation was replicated,
the difference in activation levels in the vtp condition was not    substituting an MLP for the SRN with a context of 1, 2, and
so large. As predicted, activation levels in the vtp condition      3 preceding phones. For the MLP with 1 phone context, the
are higher at word boundaries (mean value of 0.136 across the       network topology was identical (8 input, 30 hidden, and 8
sixteen runs, min 0.103, max 0.179) than at non-word bound-         output units) except for the removal of the 30 context units.
aries (mean 0.018, min 0.014, max 0.023). There is also a dif-      The 2- and 3-phone context MLPs had 16 and 24 input units.
ference between syllable boundaries and syllable-internal po-       All were fully connected with the hidden layer. The training
sitions (0.076 vs. 0.0018). Crucially, word-internal syllable-      procedure was the same.
boundaries show an average activation of 0.046—less than            1-phone context MLPs The average activations from the
half that of the word boundaries. This shows that the network       1-phone condition were very similar to those reported above
is learning word boundaries, not just syllable boundaries.          for the SRNs. Activation levels in the vtp condition are
   For the flat condition, activation is also higher at word        once again higher at word boundaries (0.12) than at non-
boundaries (mean 0.069) than at non-word boundaries                 word boundaries (0.024), syllable boundaries have higher ac-
(0.025). However, this difference is fully accounted for in dif-    tivations than syllable-internal positions (0.079 vs. 0.00036),
ferent activation levels between syllable boundary positions        and activations at word-internal syllable-boundaries are again
(0.069) and syllable-internal positions (0.0024). There is          about half as strong actual word boundaries (0.060 vs. 0.12).
no appreciable difference between word boundary positions           This suggests that the MLP is more sensitive to word bound-
and word-internal syllable-boundary positions (both 0.069).         aries than to other syllable boundaries, even with only one
Hence, the flat condition is only able to learn syllable bound-     phone of prior context. The discriminability of this network
aries, not word boundaries, just as Allen and Christiansen          is not quite as good as the SRN: the area under the corre-
(1996) observed.                                                    sponding ROC curve (not shown) is 0.81. Since the MLP
   What is more important than the raw differences in aver-         with only one phone of context converged to the same solu-
age activation, however, is the degree of discriminability be-      tion in all sixteen trials, the standard error is practically zero.
tween true word boundaries and non-boundary positions from          This is worse than the SRN (F(1,30) = 22.58, p <0.001), but
this activation level. This depends not only on the activa-         clearly better than chance.
tion difference, but also on the variations between activation         For the flat condition, in contrast, the average activation
levels, which are not directly reported. A better measure of        is equal for each of the syllable boundaries (0.0644) Just as
the overall discriminability is a receiver operating character-     in the SRN case, the network essentially fails to learn any-
istic (ROC) curve. This measure plots the true positive rate        thing other than syllable boundaries (mean AUC = 0.515, min
(the probability of correctly detecting a word boundary) over       0.501, max 0.517).
the false positive rate (the probability of incorrectly positing
a word boundary). Since any point along the curve corre-            2-phone context MLPs The results for the 2-phone con-
sponds to the performance at a given threshold of activation,       text MLP in the vtp condition are (unsurprisingly) better than
the curve as a whole summarizes the performance of a binary         the 1-phone context: The average activation for word bound-
decision procedure at any relevant threshold.                       aries (0.133) is once again higher than for word-internal sylla-
                                                                    ble boundaries (0.659) and for syllable boundaries generally
    3 http://www.ling.ohio-state.edu/                               (0.71) The mean area under the ROC curve for the sixteen
∼ rytting/cogsci2006/AC96MiniLangs.txt                              trials is 0.86 (min 0.845, max 0.869)—overlapping with the
                                                                2067

                                           ROC curve for "flat" transitional probability dataset                                         ROC curve for variable transitional probability dataset
                                  1                                                                                                 1
                                 0.9                                                                                               0.9
                                 0.8                                                                                               0.8
                                 0.7                                                                                               0.7
            true positive rate                                                                                true positive rate
                                 0.6                                                                                               0.6
                                 0.5                                                                                               0.5
                                 0.4                                                                                               0.4
                                 0.3                                                                                               0.3
                                 0.2                            mean AUC = 0.497244                                                0.2                         mean AUC = 0.853969
                                 0.1                        std. dev. AUC = 0.0300716                                              0.1                     std. dev. AUC = 0.0336545
                                  0                                                                                                 0
                                       0           0.2         0.4         0.6      0.8            1                                     0        0.2         0.4         0.6      0.8         1
                                                             false positive rate                                                                            false positive rate
Figure 1: Receiver operating characteristic (ROC) curves for 16 runs of the SRN in Simulation 1. Area under the curve (AUC)
shows the SRN’s discrimination between word boundaries and other syllable boundaries. Syllable-internal positions are not
included.
range of scores seen for the SRN, though with less variation                                              seen in the ROC curve, which is no longer merely at chance
between trials. The difference between the 2-phone MLP and                                                (AUC = 0.596, min 0.553, max 0.646). Although there are
the SRN is not significantly significant (F(1, 30) = 0.69, p =                                            no cues in the syllabic transitional probabilities, by looking
0.41).                                                                                                    at more than one syllable of context, the net observes longer-
   For the flat condition, syllable boundaries are once again                                             range regularities that arise in the data. This effect may need
higher than non-syllable boundaries (0.065 vs. 0.0008).                                                   further study to be completely understood, however.
However, word boundaries are no higher than other syllable
boundaries, just as in the 1-phone case. The mean AUC is                                                                                                       Discussion
0.472, slightly below chance (min 0.471, max 0.476).                                                      We may see in the results above that MLPs also account as
3-phone context MLPs The average activations in the vtp                                                   well as SRNs for the effects of multiple-cue integration in a
condition follow the same pattern as the other MLPs for this                                              restricted domain. Generally speaking, the more context, the
condition, but with somewhat greater differences between                                                  more successful the net is at differentiating word boundaries
the word boundary activations (mean 0.148, min 0.121, max                                                 from other syllable boundaries, though even a single phone of
0.170) and the word internal averages (0.015). As with the                                                context, without recurrence of any sort, is sufficient for better-
other conditions, there is a large, consistent difference be-                                             than-chance performance. This is due largely to the design of
tween word boundaries and syllable boundaries generally                                                   the artificial language used. One of the most obvious cues to
(mean 0.073, min 0.064, max 0.083). However, the differ-                                                  an upcoming word boundary in the vtp language: ‘a’ or ‘i’
ence between the 3-phone MLP and the other nets is most                                                   vowel (as opposed an ‘u’) was of course just as visible to the
clearly seen in the areas under the ROC curves. As shown in                                               1-phone MLP as to any other net. With two phones of context,
Figure 2, the AUC for distinguishing word boundaries from                                                 it was possible to pick up also on a second major cue in the
other syllable boundaries is 0.92 (min 0.893, max 0.932). The                                             language: ‘b’ (not being allowed at the beginning of a word)
performance as measured by the AUC is significantly better                                                is slightly more likely to start a word-final syllable than the
than that on the SRN condition (F(1,30) = 48.21, p <0.0001).                                              other three consonants. This may explain why the two-phone
                                                                                                          MLP did just as well as the SRN: because the most salient,
   For the flat condition, syllable boundaries are once again
                                                                                                          relevant cues in the language occurred just two phones from
higher than non-syllable boundaries (0.067 vs. 0.0004).
                                                                                                          the end of the word.4
However, this time word boundaries are higher than other
syllable boundaries (0.074 vs. 0.062). This is not because                                                    4 Tino et al. (2004) and others mention another potential differ-
word boundaries themselves are being learned directly, but                                                ence between SRNs and MLPs: namely, an architectural bias that
rather because the net is learning that the first syllable bound-                                         may result from the net’s structure combined with its initial random
                                                                                                          weights before training. To test for this bias, pretests were run on
ary cannot be a word boundary: the average activation for                                                 the testing data before any training took place. No evidence of such
the end of the first syllable is 0.049, markedly lower than the                                           a bias, or any appreciable difference between the MLP and the SRN
second (0.077) or the third (0.074). This effect also may be                                              prior to training, was found for this task as described above.
                                                                                                       2068

                                           ROC curve for "flat" transitional probability dataset                                         ROC curve for variable transitional probability dataset
                                  1                                                                                                 1
                                 0.9                                                                                               0.9
                                 0.8                                                                                               0.8
                                 0.7                                                                                               0.7
            true positive rate                                                                                true positive rate
                                 0.6                                                                                               0.6
                                 0.5                                                                                               0.5
                                 0.4                                                                                               0.4
                                 0.3                                                                                               0.3
                                 0.2                            mean AUC = 0.596243                                                0.2                         mean AUC = 0.916215
                                 0.1                        std. dev. AUC = 0.025865                                               0.1                     std. dev. AUC = 0.0123728
                                  0                                                                                                 0
                                       0           0.2         0.4         0.6      0.8            1                                     0        0.2         0.4         0.6      0.8         1
                                                             false positive rate                                                                            false positive rate
Figure 2: Receiver operating characteristic (ROC) curves for 16 runs of the MLP with three phones of context in Simulation
2. Area under the curve (AUC) shows the MLP’s discrimination between word boundaries and other syllable boundaries.
Syllable-internal positions are not included.
   While the toy language may in hindsight look like a ridicu-                                            itively attractive topology for modeling the prediction of se-
lously easy problem for the net, it is worth noting that such                                             quential data (including unsupervised discovery of sequential
patterns are not unheard of in human languages. Many lan-                                                 structures), particularly when the exact extent of the relevant
guages (e.g., Spanish, Italian, Modern Greek) have a re-                                                  context is unknown, there may be situations when other mod-
stricted set of phonemes word finally, compared with other                                                els are preferable: e.g., allowing for the use of more efficient
positions in the word (even the final position of word-internal                                           training techniques and faster simulation of larger-size prob-
syllables), and a great many languages have certain phonemes                                              lems.
(particularly those commonly found in word-final inflections,                                                Finally, keeping in mind what parts of the model are essen-
like English /-s/) that are extremely frequent at the end of                                              tial, and which are incidental, may be helpful in relating these
words compared with other positions. Thus, word segmen-                                                   and future models to issues of biological plausibility. Natu-
tation may not be a problem for which recurrence is particu-                                              rally, all ANNs are to some degree biologically implausible,
larly necessary, although more context does of course help. In                                            and there is much that is still unknown about the design of
contrast, no such claim is made about problems of language                                                the neural system actually used in language processing. If it
acquisition involving higher-level structures, such as syntax,                                            should someday be shown that recurrent models of the Elman
where unbounded dependencies pose problems that connec-                                                   type are incompatible with the actual mechanism for speech
tionist models quite likely need some recourse to recurrence                                              processing, the multiple cues model need not be rejected au-
in order to solve.                                                                                        tomatically, inasmuch as it has been shown not to depend cru-
                                                                                                          cially on recurrence. By abstracting away from non-essential
                                                  Conclusion                                              elements such as recurrence, the model not only gains some
                                                                                                          flexibility, but may better highlight its essential element: the
The notion of hints and multiple tasks within neural-net
frameworks is a useful paradigm for modeling and investi-                                                 hints themselves.
gating the exploration of multiple cue interaction in problems
of language acquisition, such as the word segmentation task.                                                                                             Future directions
However, the use of hints is not dependent on any partic-                                                  This work is a preliminary step toward extending connec-
ular topology of neural network; these results suggest that                                                tionist models of the word segmentation task to other types
the procedure works equally well for two different network                                                 of input, including automatically pre-processed (noisy) au-
topologies, SRNs and time-windowed MLPs, at least for a                                                    dio data, and input from languages besides English. Future
constrained task on artificial data. Furthermore, the exact size                                           steps include the replication of larger-scale studies using tran-
of the time-window is not always crucial.                                                                  scriptions of both adult- and child-directed English (Chris-
   This finding is useful in that it frees the researcher to con-                                          tiansen & Allen, 1997; Christiansen et al., 1998). Later steps
sider the best design for the problem at hand, independent of                                              include moving away from human transcriptions to acoustic
the cue interaction issues. While SRNs are certainly an intu-                                              input from speech recordings and finally to exploring the in-
                                                                                                       2069

tegration of automatic, concurrent, unsupervised acquisition       Elman, J. L. (1990). Finding structure in time. Cognitive
of a phonemic inventory with the word segmentation task.             Science, 14(2), 179-211.
                                                                   Friederici, A., & Wessels, J. (1993). Phonotactic knowledge
                         References                                  and its use in infant speech perception. Perception and Psy-
Allen, J., & Christiansen, M. H. (1996). Integrating multiple        chophysics, 54, 287–295.
  cues in word segmentation: A connectionist model using           Hanley, J., & McNeil, B. (1982). The meaning and use of the
  hints. In Proceedings of the eighteenth annual cognitive           area under a receiver operating characteristic (ROC) curve.
  science society conference (pp. 370–375). Mahwah, NJ:              Radiology, 143(1), 29-36.
  Lawrence Erlbaum Associates.                                     Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J.
Aslin, R. N., Woodward, J. Z., LaMendola, N. P., & Bever,            (2001). Gradient flow in recurrent nets: the difficulty of
  T. G. (1996). Models of word segmentation in fluent ma-            learning long-term dependencies. In S. Kremer & J. Kolen
  ternal speech to infants. In J. L. Morgan & K. Demuth              (Eds.), A field guide to dynamical recurrent neural net-
  (Eds.), (pp. 117–134). Mahwah, NJ: Lawrence Erlbaum                works. IEEE Press.
  Associates.                                                      Hochreiter, S., & Schmidhuber, J. (1997). Long short-term
Blank, D., Kumar, D., Meeden, L., & Yanco, H. (2003).                memory. Neural Computation, 9, 1735–1780.
  Pyro: A Python-based versatile programming environment           Jusczyk, P. W., Cutler, A., & Redanz, N. (1993, June). Infants
  for teaching robotics. Journal of Educational Resources in         preference for the predominant stress patterns of English
  Computing (JERIC), 3(4), 1–15.                                     words. Child Development, 64(3), 675–687.
Brent, M. R., & Cartwright, T. A. (1996). Distributional reg-      Jusczyk, P. W., Hohne, E. A., & Bauman, A. (1999). In-
  ularity and phonotactic constraints are useful for segmen-         fants’ sensitivity to allophonic cues for word segmentation.
  tation. Cognition, 61, 93–125.                                     Perception and Psychophysics, 61, 1465–1476.
Cairns, P., Shillcock, R., Chater, N., & Levy, J. (1997). Boot-    Jusczyk, P. W., Houston, D., & Newsome, M. (1999). The be-
  strapping word boundaries: A bottom-up corpus based ap-            ginnings of word segmentation in English-learning infants.
  proach to speech segmentation. Cognitive Psychology, 33,           Cognitive Psychology, 39, 159–207.
  111–153.                                                          Morgan, N., Chen, B., Zhu, Q., & Stolcke, A. (2004,
Christiansen, M. H., & Allen, J. (1997). Coping with varia-          May 17–21). TRAPping conversational speech: Extend-
  tion in speech segmentation. In A. Sorace, C. Heycock, &           ing TRAP/Tandem approaches to conversational telephone
  R. Shillcock (Eds.), Proceedings of gala.                          speech recognition. In Proceedings icassp-2004. Montreal:
Christiansen, M. H., Allen, J., & Seidenberg, M. (1998).             IEEE.
  Learning to segment speech using multiple cues: A con-           Saffran, J. R., Aslin, R. N., & Newport, E. L. (1996). Statis-
  nectionist model. Language and Cognitive Processes, 13             tical learning by 8-month-old infants. Science, 274, 1926–
  (2/3), 221–268.                                                    1928.
Christiansen, M. H., Conway, C. M., & Curtin, S. (2005).           Suddarth, S., & Kergosien, Y. (1991). Rule-injection hints
  Multiple-cue integration in language acquisition: A con-           as a means of improving network performance and learn-
  nectionist model of speech segmentation and rule-like be-          ing time. In L. Almeida & C. Wellekens (Eds.), Neural
  havior. In J. W. Minett & W. S.-Y. Wang (Eds.), Language           networks/eurasip workshop 1990 (pp. 120–129). Berlin.
  acquisition, change and emergence: Essay in evolutionary         Tino, P., Cernansky, M., & Benuskova, L. (2004, January).
  linguistics. Hong Kong: City University of Hong Kong               Markovian architectural bias of recurrent neural networks.
  Press.                                                             IEEE Trans. Neural Netw., 15(1), 6–15.
Curtin, S., Mintz, T. H., & Christiansen, M. H. (2005).            Zhu, Q., Chen, B., Morgan, N., & Stolcke, A. (2004). On
  Stress changes the representational landscape: Evidence            using mlp features in lvcsr. In Interspeech 2004 - icslp,
  from word segmentation. Cognition, 96, 233-262.                    8th international conference on spoken language process-
                                                                     ing (pp. 921–924). Jeju Island, Korea: ISCA.
                                                               2070

