get stimuli and it supports possibility of their common            ready learned words meanings and given input stimulus
mechanism.                                                         with the noun. We call the second ability “ad-hoc mean-
   Linda Smith and colleagues led LBA studies (Smith,              ing substitution (AMS).” Both have no speciﬁc mecha-
1995; Samuelson & Smith, 1999; Smith, Jones, Lan-                  nism to generate biased behavior. But when exposed to
dau, Gershkoﬀ-Stowe, & Samuelson, 2002; Samuelson,                 the statistically biased vocabulary of toddlers, learner
2002; Colunga & Smith, 2005). They explained that                  with the two abilities shows shape and material biases.
when experiencing word learning, a “nonlinear atten-               This triad of the word meaning induction, AMS, and the
tional system (NAS)” organized attention to particular             early biased vocabulary is essential for the emergence of
dimensions (e.g. shape dimensions), and the selective              biases. But this triad can cause stable biases from very
attention resulted in word learning bias (Smith, 1995).            early stages of development, while young children actu-
Recently, they advanced their hypothesis as “higher-               ally show ineﬃcient word learning and no bias (1.1.1a;
order abstraction (HOA)” (Smith et al., 2002). It ex-              1.1.2a). The delay of bias emergence is caused not by
plains that from learned noun meanings, children ab-               the triad but by a secondary factor: maturity in learn-
stract higher-order knowledge which represents mean-               ing word meanings.
ing nouns generally have; children use it as a template
to estimate meanings of novel nouns, which accelerates                            Proposed Hypothesis
vocabulary acquisition; when children with this mecha-             Input and Word Representation
nism experience early vocabulary that is dominated by
nouns organized by similarity in shape (Samuelson &                Following Samuelson (2002), input consisted of three at-
Smith, 1999), learned higher-order knowledge becomes               tributes: SOLIDITY, SYNTAX, and FEATURE. Their
shape-based, which subsequently encourages acquisition             attributes are represented as 3, 3, and 30 dimensions,
of shape-based meanings.                                           respectively. SOLIDITY, which denotes solidness of ob-
   Their hypothesis is meaningful because it intended              jects/substances presented to learners, has three discrete
to illustrate mechanism that had been compressed into              attributes, SOLID, NONSOLID, and AMBIGUOUS2 .
NAS. But their hypothesis contains two points of debate:           The attributes were represented as vectors of (0.95, 0,
the age at which cognitive functions become available              0), (0, 0, 0.95), and (0, 0.95, 0), respectively. SYNTAX,
and the concreteness of their hypothesis. First, Smith             which represents a contextual attribute in a sentence
(1989) demonstrated that children begin to pay selective           given in parallel with the named object3 , has three dis-
attention to particular sensory dimensions in nonnaming            crete attributes: COUNT, MASS, and AMBIGUOUS4 .
categorization tasks after ﬁve years old. So we consider it        They are represented as vectors of (0.95, 0, 0), (0, 0,
diﬃcult to explain shape and material bias at 24 months            0.95), and (0, 0.95, 0), respectively. FEATURE is an
by NAS, although experiencing novel nouns may encour-              attribute that denotes other information and consists of
age the organization of selective attention (Smith, 1995).         three attributes: SHAPE, MATERIAL, and OTHER5 .
And they also didn’t provide evidence that HOA is possi-           They are represented as separate 10-dimentional vectors,
ble at 24 months1 . The second issue about concreteness            respectively.
means that their hypothesis still retains some unclear               We assumed children constructed category knowledge
points. Therefore though they veriﬁed NAS or HOA by                for nouns. Based on the deﬁnition of FEATURE, we
simulations with neural networks (Smith, 1995; Samuel-             deﬁned three categories: SHAPE-BASED, MATERIAL-
son, 2002; Colunga & Smith, 2005), it remains unclear              BASED, and OTHER-BASED, organized on the basis of
whether those studies could have veriﬁed their hypothe-            similarity in SHAPE, MATERIAL, and OTHER, respec-
sis because they didn’t describe its essence, that is, how         tively. These categories can overlap for each noun. In-
to acquire the attention or abstraction.                           puts belonging to a SHAPE-BASED category had arbi-
   So, in this study we manifest requirements for an alter-        trary but constant values of SHAPE dimensions and ran-
native hypothesis of shape and material biases as follows:         dom values of MATERIAL and OTHER dimensions. To
(1.3a) concrete description of computational process of            the ﬁxed SHAPE dimensions, random noises between [-
the biases; (1.3b) explanation for why their emergence             0.05, 0.05] was added. Inputs belonging to MATERIAL-
depends on children’s vocabulary size; (1.3c) explanation          and OTHER-BASED categories were made in the same
for the existence of interference between them; (1.3d)             way as SHAPE-BASED category, respectively.
clariﬁcation of essential factors that enable shape bias           Word Distributional Prototype (WDP)
to appear at 24 months. Based on them, we hypothesize
about these biases as below. Shape and material biases             We introduced explicit representation of word meaning,
arise from simple learning and consist of two common               which enable us to describe process of AMS concretely.
primitive abilities available from infancy: (1) ability to         We assume that a word meaning is deﬁned as an ex-
learn the meanings of words by induction, and (2) ability          tended prototype (Rosch & Mervis, 1975) that consists of
to instantly estimate novel noun meaning based on al-              distribution formed by inputs co-occurring with the word
                                                                      2
    1
      Children begin to use words of superordinate-level cat-           AMBIGUOUS denotes borderline cases in which SOLID-
egories at about four years.       If the knowledge of a           ITY is neutral or both SOLID and NONSOLID are possible.
                                                                      3
superordinate-level category word is abstracted from the                Broad sense of syntax such as articles and determiners.
                                                                      4
knowledge of basic-level category words, the abstraction pro-           SYNTAX AMBIGUOUS also denotes borderline cases.
                                                                      5
cess should resemble HOA and both available ages can hardly             OTHER includes miscellaneous information other than
be expected to be so diﬀerent.                                     SHAPE and MATERIAL: color, taste, temperature, fun, etc.
                                                              1641

 (Kurosaki & Omori, 2005a,b). For example, banana is                                  p0 (x) p c (x)         Copying Variance
 generally used for crescent-shaped yellow fruits, but not                                                       Matrix. (NNH)
 for red or globular things. It’s so sensitive to shape and                                                              Novel WDP
                                                                        Known WDPs
 usage information that even slight discrepancy in them
                                                                        ( = Meanings)
                                                                                             ...         ...
 is unacceptable. Meanwhile, such information as emo-                                       0          c
                                                                                                                           Novel Noun
 tion isn’t crucial for the recognition of banana, though                   Nouns      Dog Cloud             Salt
                                                                                                                             "Yith"
 it’s also presented simultaneously with the name. Such
                                                                                      Copying Input Vector.
 characteristics of word meaning can be expressed by two
 factors: mean value in each input dimension and allow-                                     SHAPE         MATERIAL OTHER
                                                                                                 ...           ...              ...
 able deviation from the mean value. Children must learn
 them based on experience.                                              SOLIDITY SYNTAX                     FEATURE
    So we used a multidimensional normal distribution as
 one of the simplest possible representations for a word’s                Figure 1: Ad-hoc meaning substitution (AMS).
 meaning to fulﬁll the above requirements. We called it
 word distributional prototype (WDP). Its mean vector
 denoted the word’s standard appearance in input space.
                                                                                               ∂εc (~x)           µc i − xi
 Its variance matrix denoted the allowable range of ﬂuctu-                       ∆µc i = −α               = −α                        (3)
 ation from the mean vector. We used diagonal variance                                           ∂µc i              σc i 2
 matrices assuming that input dimensions had no corre-                                               (                          )
                                                                                    ∂εc (~x)               1      (µc i − xi )2
 lation each other. Though the simpliﬁcation might have                  ∆σc i = −β           = −β            −                     . (4)
 some problems, we thought WDP was suﬃcient for word                                  ∂σc i              σc i         σc i 3
 meaning representation of young children.
    WDP described each word meaning as below. ~x ∈ <M ,              Ad-hoc Meaning Substitution (AMS)
~x = (x1 x2 · · · xM )T is an input vector in M -di-                 When a child is given a novel objects/substances and a
 mensional input space, j is an ID number for WDPj ,                 corresponding noun, the single experience may indicate
 ~ j ∈ <M , µ
 µ             ~ j = (µj 1 µj 2 · · · µj M )T is a mean vector       the mean vector of the noun, but not the standard devi-
 of WDPj , µj i ∈ < is a mean value of input unit i of               ation. Nevertheless, the existence of word learning bias
 WDPj , Σj ∈ <M × <M is a diagonal variance matrix of                demonstrates that children do complement the missing
 WDPj , and σj i ∈ < is a standard deviation of input unit           information of standard deviations in some way. Chil-
 i of WDPj . Then, likelihood of WDPj to an input ~x is              dren must do it by use of mechanisms nonspeciﬁc to vo-
 calculated as:                                                      cabulary learning because as discussed in introduction,
                                                                     the biases seems to be learned.
                   1           ( 1                        )             We explain a process of novel noun generalization as
   pj (~x) =       M     1  exp − (~x −~ µj )T Σ−1 x −~
                                                j (~  µj ) . (1)     below. Children determine the noun’s meaning based
             (2 π) |Σj |
                   2     2         2
                                                                     on meaning of a known noun: The variance matrix of
                                                                     a known noun WDP, which output the highest likeli-
    WDPs were trained by following algorithm: (2a) Ini-              hood to the input information (Eq. (5)), is copied as
 tially, each word is paired with an individual WDP; (2b)            the variance matrix of the novel noun WDP (Eq. (6)).
 Given an input vector ~x and a corresponding word, all              We call the copying process “nearest neighbor hypothe-
 WDPs calculate likelihood pj (~x) for the input, and the            sis (NNH)” (Fig. 1), which denotes that WDP which has
 winning WDPc that outputs the highest likelihood is                 the nearest Mahalanobis distance to the input vector was
 chosen; (2c) If the WDPc is the correct paired one with             chosen. Meanwhile, the input vector presented with the
 the given word, then it learns to increase its likelihood for       novel noun was copied as the mean vector of the novel
 the input (Eq. (3) and (4)), and the others don’t; (2c’)            noun WDP (Eq. (7)). Though they may not work al-
 If WDPc is incorrect WDP for the given word, then it                ways correctly, algorithm which always complements the
 learns to decrease its likelihood pj (~x) for the input. Its        missing information correctly doesn’t exist. We suggest
 update rules correspond to those of the opposite direc-             that one of probable strategies for the complements is to
 tion of (2c). The correct WDP, which is paired with                 substitute the nearest known word and input vector for
 a given word and should give the highest likelihood, si-            the information. We call the ad-hoc construction of novel
 multaneously learns by the update rule (2c); (2d) Re-               noun meaning ad-hoc meaning substitution (AMS).
 peat (2b), (2c), and (2c’) depending on word inputs.
 The learning corresponds to extension of “learning vec-                                  c = arg max pj (~x)                         (5)
 tor quantization (LVQ)” (Kohonen, 1995). To maintain                                                  j
 stable learning, we set lower limit of σj i to 0.1 and range
 of µj i to [−1, 1]. The initial value of every σj i is set to                                 Σnew = Σc                              (6)
 a suﬃciently large value so that all WDPs don’t output                                         µ
                                                                                                ~ new = ~x                            (7)
 higher likelihood to particular inputs in the initial state.
 Loss function εc (~x) and update rules for each parameter
 are deﬁned as:
                      εc (~x) = − log (pc (~x))              (2)
                                                                1642

Table 1: Structural ratio in early vocabulary evaluated in this study. Upper table shows ratio of each attribute value
in SOLIDITY, SYNTAX, and FEATURE. Lower tables show conditional ratio of each attribute value.
                                SOLIDITY                              SYNTAX                                 FEATURE
                      SOLID       NON-      AMBIG-        COUNT MASS              AMBIG-         SHAPE         MATE-        OTHER
                                  SOLID     UOUS                                  UOUS                         RIAL
                      .63         .04       .32           .74          .10        .16            .48           .16          .39
          SOLID                                           .88          .03        .09            .71           .09          .19
     NONSOLID                                             .07          .43        .50            .07           .86          .14
  AMBIGUOUS                                               .56          .19        .25            .24           .36          .45
                       Simulation 1                                OTHER values, and no SYNTAX7 value. Random
                                                                   noises between [−0.05, 0.05] were added to the SHAPE
Inductive Learning Phase                                           values. The material-match stimuli were made in the
We divided the word learning process into two phases.              same way. Both sets were initially prepared and shared
In “inductive learning phase,” learners received sets of           by all learners in all groups.
words and corresponding input and constructed WDPs                   In our model, shape choice probabilities for each group
with Eq. (3) and (4). Then in “generalizing phase,” they           were calculated as below. Given a novel target stimulus
were conducted novel noun generalization tasks on with             and a novel noun, Learnerg i of Groupg made a novel
AMS and the learned WDPs. We prepared six groups                   noun WDP applying AMS. Then we gave Learnerg i
whose learners were given 18, 50, 102, 213, 281, and 312          20 pairs of shape-match and material-match stimuli
words, respectively (see Samuelson & Smith, 1999). We             and compared which evoked higher likelihood for each
called them groups 1, 2, 3, 4, 5, and 6, respectively. Each       pair. Shape choice probability to Target stimulusj by
learner in a group learned diﬀerent words each other.             Learnerg i was calculated by p(g, i, j) = (winning number
   Vocabulary used in this phase should contain words             of shape-match stimuli) / (total number of pairs). Shape
generally heard and produced by young children. Hence,            choice probability to all target
                                                                                               (∑ stimuli)by Learnerg i was
we used MCDI (Fenson et al., 1994), which include a               calculated by pL (g, i) =        j p(g, i, j) / (total number
typical vocabulary for 16- to 30-month-olds. Based on             of target stimuli). Mean probability
previous studies (Samuelson & Smith, 1999; Samuelson,                                                      (∑of shape )choice in
                                                                  Groupg was calculated by p(g) =               i pL (g, i) / (total
2002), we used 312 words from nine categories6 of MCDI.
We evaluated the words ourselves in terms of SOLID-               number of learners).
ITY, SYNTAX, and FEATURE (Table 1) so that the                       For the solid set, t-test conﬁrmed that the shape choice
evaluated structural ratio in the vocabulary was consis-          probability for each group was signiﬁcantly larger than
tent with previous study (Samuelson & Smith, 1999) and            chance: They showed shape bias; t(20)=12.95, p≺.001;
made the 36-dimensional input data.                               t(20) =23.02, p≺.001; t(20) =26.77, p≺.001; t(20)=
   We set learning parameters α and β to 0.001, the ini-          27.98, p≺.001; t(20)=27.89, p≺.001; and t(20)=32.45,
tial values of µj i to [0.4999, 0.501] and σj i to 1.0. We        p≺.001, respectively. For the nonsolid set, the proba-
prepared 50 instances for a word, and a corresponding             bilities of shape choice for groups 1, 2, and 3 were sig-
WDP learned the instances. In an epoch , a learner expe-          niﬁcantly larger than chance even for the nonsolid set:
rienced all instances of all words prepared for the learner.      They showed overgeneralized shape bias; t(20)=2.71, p≺
The learning iterated the epoch 30 times. We conﬁrmed             .05; t(20)=3.27, p≺.01; and t(20)=4.68, p≺.001, respec-
that the learning of all groups almost converged at 30th          tively. But those for groups 4, 5, and 6 were signiﬁcantly
epoch, and that the learned parameters in each WDP                smaller than chance: They showed material bias; t(20)
were correctly estimated to form distribution of corre-           =−3.54, p≺.01; t(20)=−3.57, p≺.01; and t(20)=−4.82,
sponding noun.                                                    p≺.001, respectively (Fig. 2).
                                                                       Results of the stable shape bias during somewhat
Generalizing Phase                                                larger vocabulary (1.1.1b), the overgeneralized shape
                                                                  bias during small vocabulary (1.1.2b), and the material
We prepared solid and nonsolid sets and conducted novel           bias during large vocabulary (1.1.2c) is explained as fol-
noun generalization tasks separately with them. Each              lows. WDPs having same SOLIDITY as a target stim-
set had 21 stimulus sets, and a stimulus set consisted            ulus, which were SOLID WDPs for solid set and NON-
of a target stimulus and 20 pairs of shape-match and              SOLID WDPs for nonsolid set, were likely to be chosen
material-match stimuli. Shape-match stimuli were made             as the nearest WDP by NNH because they output higher
as input vectors that had same SOLIDITY and SHAPE                 likelihood. Since SOLID WDPs tended to be SHAPE-
values as the target stimulus, random MATERIAL and
    6                                                                 7
      animals, vehicles, toys, food and drink, clothing, body           SYNTAX information was withheld because we intended
parts, small household items, furniture and rooms, outside         to compare our results to previous experimental results with-
things                                                             out syntax (see Samuelson & Smith, 1999; Samuelson, 2002).
                                                              1643

                               1                                                                                          1
                                          ***     ***              ***        *** ***                                                                        ***        *** ***
                                    ***
Probability of Shape Choice                                                                Probability of Shape Choice
                              0.8                                                                                        0.8
                                                                                                                                             ***
                              0.6
                                     * **         ***                                                                    0.6                 ***
                                                                   **         ** ***                                           ***     ***                              *** ***
                              0.4                                                                                        0.4   ***                             **
                              0.2            Solid Set                                                                   0.2            Solid Set
                                          Nonsolid Set                                                                               Nonsolid Set
                               0                                                                                          0
                                     18    50     102                 213        281 312                                        18   50     102                 213        281 312
                                             Number of Known Words in Each Group                                                       Number of Known Words in Each Group
Figure 2: Probabilities of shape choices in generalizing phase of simulation 1 (left) and 2 (right). Vertical lines depict
standard errors. Horizontal broken line depicts chance level (=.5). *p ≺ .05. **p ≺ .01. ***p ≺ .001.
BASED (see Table 1), novel WDP was inclined to copy                                                             for the solid set, demonstrating material bias: t(20)=
the SHAPE-BASED variance matrix by NNH and it led                                                               −20.39, p≺.001. But in group 2, there was no signiﬁcant
to shape bias. Meanwhile, NONSOLID WDPs couldn’t                                                                diﬀerences to chance, that is, it showed no bias: t(20)=
be the nearest to nonsolid set during small vocabulary.                                                         0.081, pÂ.05. After that, the probabilities for groups 3,
NONSOLID WDPs were far less than SOLID WDPs and                                                                 4, 5, and 6 were signiﬁcantly higher than chance, showing
the NONSOLID ﬁeld was too sparse (see Table 1) for                                                              shape bias: t(20)=15.01, p≺.001; t(20)=33.59, p≺.001;
them to be chosen as the nearest. Therefore SOLID                                                               t(20)=26.63, p≺.001; and t(20)=37.49, p≺.001, respec-
WDPs could be chosen alternatively, though they had                                                             tively. For the nonsolid set, the shape choice probabili-
a disadvantage of having diﬀerent SOLIDITY from the                                                             ties for groups 1, 2, 4, 5, and 6 were signiﬁcantly smaller
target stimulus. Then as with the case of solid set, it                                                         than chance, showing material bias; t(20)=−24.54, p≺
led to overgeneralized shape bias. When the sparseness                                                          .001; t(20)=−5.98, p≺.001; t(20)=−3.77, p≺.01; t(20)=
disappeared, NONSOLID WDPs could be the nearest to                                                              −3.86, p≺.001; and t(20)=−4.01, p≺.001, respectively.
nonsolid stimuli and it caused material bias.                                                                   But in group 3, we observed a signiﬁcantly larger shape
                                                                                                                choice than chance, demonstrating shape bias even for
                                                Simulation 2                                                    the nonsolid set; t(20)=5.48, p≺.001 (Fig. 2).
                                                                                                                   Only the results during small vocabulary were dif-
Inductive Learning Phase                                                                                        ferent from simulation 1. So, we discuss the disap-
We introduced maturity to produce a situation in which                                                          pearance of robust shape bias during small vocabulary
convergence of word meaning learning was more insuﬃ-                                                            (1.1.1a, 1.1.2a) and the alternative appearance of ma-
cient for learners who had small vocabularies than large                                                        terial bias. WDPs in the very early groups were closer
vocabularies. We realized the situation by reducing the                                                         to hyperspheres because learning of their variance ma-
number of instances for each word depending on the vo-                                                          trices hadn’t progressed at all due to the eﬀect of ma-
cabulary size of each group: 5, 10, 20, 40, 50, and 55,                                                         turity. Novel WDPs that copied the variance matrix
respectively, in ascending order of group number. Other                                                         output low likelihood to every input equally. It led to no
conditions were identical to simulation 1.                                                                      shape and material bias. Actually the robust shape bias
  Learning hadn’t converged even at 30th epochs in                                                              during small vocabulary in simulation 1 disappeared and
groups 1 and 2. Their parameters hadn’t been esti-                                                              p(g = 1) and p(g = 2) became almost close to chance.
mated correctly, compared with simulation 1. Though                                                             But statistically, early groups showed signiﬁcant mate-
the learning in group 3 apparently converged better than                                                        rial choice. The problem seemed to be due to our simpli-
groups 1 and 2, their parameters hadn’t been estimated                                                          ﬁed setting in each group. Exactly the same setting of
as well as simulation 1. After group 4, their learning                                                          the maturity and the number of words for learners in a
had almost converged and their parameters had been es-                                                          group caused almost same pL (g, i) and their small stan-
timated well enough. From these results, the expected                                                           dard deviation. In that case, just slight diﬀerence than
eﬀects by introducing the maturity were realized.                                                               chance lead to shape/material bias. But the problem
                                                                                                                could be resolved by modifying the current simpliﬁca-
Generalizing Phase                                                                                              tion. In sum, we could replicate some previous ﬁndings
The generalizing phase procedure was identical to simu-                                                         in simulation 1 (1.1.1b; 1.1.2b; 1.1.2c) and the others in
lation 1. For the solid set, the probability of shape choice                                                    simulation 2 (1.1.1a; 1.1.2a) and also explain the process
for group 1 was signiﬁcantly lower than chance even                                                             computationally.
                                                                                       1644

                  Position of AMS                              Colunga, E., & Smith, L. B. (2005). From the lexicon
                                                                 to expectations about kinds: A role for associative
HOA and AMS were proposed as LBA instead of ac-                  learning. Psychological Review , 112 (2), 347–382.
counts based on innateness of the biases. It’s well known
that children reason about animals and plants by anal-         Dickinson, D. K. (1988). Learning names for materials:
ogy with humans familiar to them (Inagaki & Hatano,              Factors constraining and limiting hypotheses about
1987). Since AMS means that children construct knowl-            word meaning. Cognitive Development, 3 (1), 15–35.
edge of unfamiliar nouns based on nouns familiar to            Fenson, L., Dale, P. S., Reznick, J. S., Bates, E., Thal, D.
them, AMS resembles in it. Therefore AMS is natural for          J., & Pethick, S. J. (1994). Variability in early com-
them as a method to estimate novel noun meaning. And             municative development. Monographs of the Society
AMS has simpler process, but it can illustrate wider phe-        for Research in Child Development, 5 , 242.
nomena of shape bias, material bias, and overgeneralized      Imai, M., & Gentner, D. (1997). A cross-linguistic study
shape bias than other accounts. Besides, since applica-          of early word meaning: universal ontology and linguis-
bility of HOA to them all is unclear and it has the age          tic inﬂuence. Cognition, 62 (2), 169–200.
problem, we argue that AMS is more appropriate expla-         Inagaki, K., & Hatano, G. (1987). Young children’s
nation for the biases. But we shouldn’t exclude HOA              spontaneous personiﬁcation as analogy. Child Devel-
because it can apparently explain acquisition of words           opment, 58 , 1013–1020.
belonging to superordinate categories or more abstract        Kohonen, T. (1995). Self-organizing maps. New York:
concepts well. And it is quite plausible that HOA and            Springer-Verlag.
AMS cooperatively engage in bias emergence with other
                                                              Kurosaki, K., & Omori, T. (2005a). Computational mod-
functions (e.g. theory of mind ). We consider AMS to be
                                                                 eling of word learning biases by using known word
the simplest function within them, and thus it takes an
                                                                 meanings. Proceedings of the 9th IASTED Interna-
important role as the foundation of bias emergence from
                                                                 tional Conference on Artiﬁcial Intelligence & Soft
the early stage of development.
                                                                 Computing (pp. 201–206), Calgary: ACTA Press.
                      Conclusion                              Kurosaki, K., & Omori, T. (2005b). Children construct
                                                                 novel word meaning ad-hoc based on known words:
In this paper, we presented an integrated explanation of         Computational model of shape and material biases.
ad-hoc meaning substitution (AMS) for behaviors that             Manuscript submitted for publication.
had been described separately as shape and material bi-       Landau, B., Smith, L. B., & Jones, S. S. (1988). The
ases and veriﬁed it by computer simulations. Besides, to         importance of shape in early lexical learning. Cognitive
describe AMS’s processing, we introduced word distri-            Development, 3 (3), 299–321.
butional prototype (WDP) with the inductive learning          Quine, W. V. (1960). Word and object. Cambridge: MIT
function. We consider the explicit meaning representa-           Press.
tion is valid methodology for illustrating the computa-
tional mechanism of word learning bias, which is deeply       Rosch, E., & Mervis, C. B. (1975). Family resemblances:
committed to word meaning.                                       Studies in the internal structure of categories. Cogni-
  Simulation 1 revealed that when learners that possess          tive Psychology, 7 , 573–605.
(1) WDP and (2) AMS were exposed to (3) early biased          Samuelson, L. K. (2002). Statistical regularities in vo-
vocabulary, they showed shape, material, and overgen-            cabulary guide language acquisition in connection-
eralized shape bias. This result suggested that the triad        ist models and 15-20-month-olds. Developmental Psy-
is essential for emergence of the biases. Simulation 2           chology, 38 (6), 1016–1037.
revealed that when maturity was introduced, learners          Samuelson, L. K., & Smith, L. B. (1999). Early noun vo-
showed neither shape nor material bias during the early          cabularies: do ontology, category structure and syntax
small vocabulary. This result indicated that the period          correspond? Cognition, 35 (3), 1–33.
of bias emergence is decided not by the triad but by          Smith, L. B. (1989). A model of perceptual classiﬁcation
maturity. So we can reply to the requirements in in-             in children and adults. Psychological Review , 98 , 125–
troduction: (1.3a) AMS (especially, NNH); (1.3b) bias            144.
emergence is inﬂuenced by maturity and sparseness of          Smith, L. B. (1995). Self-organizing processes in learning
SOLID and NONSOLID WDPs; (1.3c) shape and mate-                  to learn words: Development is not induction. In C.
rial biases derive from common mechanisms; (1.3d) the            A. Nelson (Ed.), The Minnesota Symposia on Child
triad and maturity. Our results suggest that phenomena           Psychology, Vol. 28 . Mahwah, NJ: Erlbaum.
concerning shape and material biases, which have been
                                                              Smith, L. B., Jones, S. S., Landau, B., Gershkoﬀ-Stowe,
explained by meta learning (HOA) or built-in language-
                                                                 L., & Samuelson, L. K. (2002). Object name learning
speciﬁc mechanism, are explicable with a simple ad-hoc
                                                                 provides on-the-Job training for attention. Psycholog-
learning mechanism.
                                                                 ical Science, 13 (1), 13–19.
                                                              Soja, N. N., Carey, S., & Spelke, E. S. (1991). Onto-
                      References                                 logical categories guide young children’s inductions of
Carey, S., & Bartlett, E. (1978). Acquiring a single new         word meaning: Object terms and substance terms.
   word. Papers and Reports on Child Language Devel-             Cognition, 38 (2), 179–211.
   opment, 15 , 17–29.
                                                          1645

