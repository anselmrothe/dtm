UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Human Speechome Project
Permalink
https://escholarship.org/uc/item/2g24h4mr
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
DeCamp, Philip
Fleischman, Michael
Gorniak, Peter
et al.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                    The Human Speechome Project
           Deb Roy, Rupal Patel∗ , Philip DeCamp, Rony Kubat, Michael Fleischman,
               Brandon Roy, Nikolaos Mavridis, Stefanie Tellex, Alexia Salata,
                      Jethran Guinness, Michael Levit, Peter Gorniak
                                  Cognitive Machines Group, MIT Media Laboratory
                      ∗
                          Communication Analysis and Design Laboratory, Northeastern University
                           Abstract
  The Human Speechome Project is an effort to observe
  and computationally model the longitudinal course of                                            3INGLE FAMILY HOME OF CHILD  CAREGIVERS
  language development of a single child at an unprece-                                           INSTRUMENTED WITH CAMERA  MICROPHONES
  dented scale. The idea is this: Instrument a child’s                                            PROVIDING TOTAL VISUAL AND AUDIO COVERAGE
  home so that nearly everything the child hears and sees                                         OF ALL ROOMS IN HOME
  from birth to three is recorded. Develop a computa-
  tional model of language learning that takes the child’s
  audio-visual experiential record as input. Evaluate the                           #ONTINUOUS MULTI TRACK AUDIO
                                                                                    VISUAL RECORDING FOR THREE YEARS
                                                                                                                            3PEECH  VIDEO
  model’s performance in matching the child’s linguistic
  abilities as a means of assessing possible learning strate-
  gies used by children in natural contexts. First steps                                          2AW SENSOR
                                                                                                  DATA                      META DATA
  of a pilot effort along these lines are described including
  issues of privacy management and methods for overcom-                                                         !DAPTIVE                     ,ANGUAGE
  ing limitations of fully-automated machine perception.                                                        -ACHINE                       ,EARNING
                                                                                                               0ERCEPTION                      -ODEL
   Stepping into the Shoes of Children                                                                          (UMAN
                                                                                                                !NALYSTS
To date, the primary means of studying language acqui-
sition has been through observational recordings made in
laboratory settings or made at periodic intervals in chil-          Figure 1: The goal of HSP is to create computational
dren’s homes. While laboratory studies provide many                 models of word learning evaluated on longitudinal in vivo
useful insights, it has often been argued that the ideal            audio-visual recordings.
way to observe early child development is in the home
where the routines and context of everyday life are min-
imally disturbed. Bruner’s comment is representative:
                                                                    surge in availability of digital sensing and recording tech-
  I had decided that you could only study language                  nologies enables ultra-dense observation: the capacity
  acquisition at home, in vivo, not in the lab, in vitro.           to record virtually everything a child sees and hears in
  The issues of context sensitivity and the format of               his/her home, 24 hours per day for several years of con-
  the mother-child interaction had already led me to                tinuous observation. We have designed an ultra-dense
  desert the handsomely equipped but contrived video                observational system based on a digital network of video
  laboratory...in favor of the clutter of life at home.             cameras, microphones, and data capture hardware. The
  We went to the children rather than them come to                  system has been carefully designed to respect infant and
  us. [Bruner, 1983]                                                caregiver privacy and to avoid participant involvement
                                                                    in the recording process in order to minimize observer
   Unfortunately, the quality and quantity of home ob-
                                                                    effects.
servation data available is surprisingly poor. Observa-
tions made in homes are sparse (typically 1-2 hours per               The recording system has been deployed and at the
week), and often introduce strong observer effects due              time of this writing (January 2006), the data capture
to the physical presence of researchers in the home. The            phase is six months into operation. Two of the au-
fine-grained effects of experience on language acquisition          thors (DR, RP) and their first-born child (male, now six
are poorly understood in large part due to this lack of             months of age, raised with English as the primary lan-
dense longitudinal data [Tomasello and Stahl, 2004].                guage) are the participants. Their home has been instru-
   The Human Speechome Project (HSP) attempts to                    mented with video cameras and microphones. To date,
address these shortcomings by creating the most com-                we have collected 24,000 hours of video and 33,000 hours
prehensive record of a single child’s development to date,          of audio recordings representing approximately 85% of
coupled with novel data mining and modeling tools to                the child’s waking experience. Over the course of the
make sense of the resulting massive corpus. The recent              three-year study this corpus will grow six-fold.
                                                                2059

   Our ultimate goal is to build computational models of        merous theories of word learning have been pro-
language acquisition that can “step into the shoes” of a        posed, some positing language-specific learning con-
child and learn directly from the child’s experience (Fig-      straints (e.g., [Clark, 1987, Markman, 1989]), domain-
ure 1). The design and implementation details of any            general learning mechanisms (e.g., [Merriman, 1999,
computational model will of course differ dramatically          Smith, 2000]), or the importance of social inference (e.g.,
from the mental architecture and processes of a child.          [Baldwin et al., 1997, Bloom, 2000]). There is no ob-
Yet, the success of a model in learning from the same           vious way to resolve disputes that arise between these
input as a child provides evidence that the child may           views without a better understanding of the exact na-
employ similar learning strategies.                             ture of children’s input as it pertains to each theoretical
   A critical question underlying any model of learning         position. For example, domain-general learning theories
concerns the balance between nature and nurture. HSP            may explain results in controlled word learning experi-
brings a new perspective to this age-old debate. Given a        ments, but we cannot know which aspects of real-world
near-complete, contextually-rich record of a child’s first      word learning can be explained by these theories unless
three years, what are the set of ontological constraints        they can be tested on more representative data. The
that must be built into a model for it to successfully          same argument holds for the other positions.
learn aspects of language? If a machine can be shown               Historically, the scope of observational studies has
to acquire some capability or structure X without cor-          broadened with advances in technology. In the earliest
responding innate preconditions, this provides evidence         studies, parent-investigators relied on diaries to record
that the child’s environment provides X – and thus need         observations in the home. Diarists can of course only
not be innate. While definitive conclusions cannot be           record a small subset of complete activity – typically first
drawn from a single subject study, the methodology de-          usages and salient errors. Detailed records of input to
veloped herein enables a new line of investigation that         the child (especially visual and social context) and com-
has until now been unexplored.                                  plete histories of the child’s everyday behavior cannot be
   This paper is structured as follows. Some further            captured. The introduction of analog and then digital
background motivation is provided for collecting ultra-         audio recording technology has enabled more detailed
dense, in vivo observations. The data collection process        and unbiased observation. Video recordings, however,
and a set of initial data visualization and mining tools        are more difficult to make, are slower to analyze, and
for semi-automatic annotation are then described. We            thus remain rare in language acquisition studies. Yet,
conclude by sketching preliminary directions of a mod-          visual context is vital for understanding language devel-
eling effort aimed at harnessing the HSP corpus to ex-          opment. For example, to study how a child learns the
amine the role of physical and social interaction in word       meaning of thank you, investigators need to know the
learning.                                                       non-linguistic contexts in which the phrase was heard
                                                                and used to understand how the child generalizes from
       Longitudinal In Vivo Observation                         particular instances to new contexts.
                                                                   In general, many hypotheses regarding the fine-
Bruner’s comment is echoed in the work of many re-              grained interactions between what a child observes and
searchers who recognize the importance of observing lan-        what the child learns to say cannot be investigated due
guage development as it actually unfolds in the home            to a lack of data. How are a child’s first words related to
(e.g., [Bloom, 1973, Nelson, 1974]). The value of longi-        the order and frequency of words that the child heard?
tudinal small-subject studies for examining detailed dy-        How does the specific context (who was present, where
namics of language acquisition is also well established         was the language used, what was the child doing at the
(e.g., [Brown, 1973]). The density of data samples is           time, etc.) affect acquisition dynamics? What specific
critical. With observations spaced weeks or months              sequence of grammatical constructions did a child hear
apart, apparently sudden changes of linguistic abilities        that led her to revise her internal model of verb inflec-
(e.g., new vocabulary, new grammatical constructions)           tion? These questions are impossible to answer without
cannot be carefully investigated. Most researchers rely         far denser data recordings than those currently available.
on speech recordings that cover less than 1.5% of a
child’s complete linguistic experience (e.g., most cor-
pora in the CHILDES archive [MacWhinney, 2000]) –
                                                                Ultra-Dense Observation for Three Years
and far less, if any, visual context. As a result, theo-        Eleven omni-directional mega-pixel resolution color dig-
ries of language acquisition hinge on remarkably incom-         ital video cameras have been embedded in the ceil-
plete observations and are subject to large random ef-          ings of each room of the participants’ house (kitchen,
fects due to sampling errors. While researchers widely          dining room, living room, playroom, entrance, exercise
recognize the need for increased observational data to          room, three bedrooms, hallway, and bathroom). Video
advance empirical and theoretical rigor in the field (e.g.,     is recorded continuously from all cameras since the child
[Tomasello and Stahl, 2004]), few efforts have been un-         may be in any of the 11 locations at any given time.
dertaken to acquire ultra-dense longitudinal recordings         In post processing, only the relevant video channel will
of language acquisition within the home environment.            be analyzed for modeling purposes. A sample video
   Dense longitudinal observations, coupled with appro-         frame from the living room camera under evening light-
priate data mining and modeling tools, have the po-             ing is shown in Figure 1(left image). The image on the
tential to make significant theoretical advances. Nu-           right shows an enlargement of a region of the left image
                                                           2060

demonstrating the camera’s spatial resolution. Video is
captured at 14 images per second whenever motion is de-
tected, and one image per second in the absence of mo-
tion. The result is continuous and complete full-motion
video coverage of all activity throughout the house.
   While omnidirectional cameras provide situational
awareness of essentially everything in the house (other
than objects occluded due to furniture and people), de-
tails such as facial expressions are lost. Although eye
gaze and other subtle behaviors are important for lan-
guage learning, current technology is unable to provide
both comprehensive spatial coverage and high resolution.
Given our interests in observing and modeling social dy-
namics expressed in the movements and spatial relations       Figure 2: Top left: Camera and microphone embedded
of caregivers and infants throughout the house (see be-       in ceiling with camera shutter open (the microphone is
low), we have opted for wide coverage at the expense of       the small dark disk to the right). Bottom left: shutter
resolution.
                                                              closed. Right: Wall mounted touch display for recording
   Boundary layer microphones (BLM) are used to record
                                                              control (see section on privacy management for details).
the home’s acoustic environment. These microphones
use the extended surface in which they are embed-
ded as sound pickup surfaces. BLMs produce high                  Over the first six months of operation, the participants
quality speech recordings in which background noise is        have settled into a stable pattern of use with the privacy
greatly attenuated. We have embedded 14 microphones           controls. On most days, audio recording is turned off
throughout the ceilings of the house placed for opti-         throughout the house at night after the child is asleep
mal coverage of speech in all rooms. Audio is sampled         (typically just after 10pm), and is turned back on in
from all 14 channels at greater than CD-quality (16-bit,      the morning when the child awakes (8am). Audio is
48KHz). When there is no competing noise source, even         paused on average for one hour per day, mainly around
whispered speech is clearly captured.                         adult dinner time, for a total of about 13 hours of au-
   Concealed wires (above the ceiling) deliver power and      dio recordings per day. Video is paused on average 2
control signals to the cameras and microphones, and           hours per day, mainly during nursing, resulting in ap-
transmit analog audio and networked digital video data        proximately 12 hours of video per day. The oops button
to a cluster of 10 computers and audio samplers located       has been used 109 times (63 video deletions, 46 audio
in the basement of the house. The computers perform           deletions) in the first six months of usage. On average,
real-time video compression and generate time-stamped         video (audio) segments deleted using this feature have
digital audio and video files on a local 5-terabyte disk      been 7.6 (8.4) minutes long.
array. With video compression, approximately 300 giga-           Various other privacy management precautions are
bytes of raw data are accumulated each day. A petabyte        being implemented including blackout of video related
(i.e., 1 million gigabyte) disk array is under construc-      to bathroom and change table interactions, procedures
tion at MIT to house the complete three-year data set         for retroactive data retrieval from archives upon partic-
and derivative metadata. Data is transferred periodi-         ipants request, and the design of secure data servers to
cally from the house to MIT using portable disk drives.       limit access to data.
               Privacy Management                                    Handling 338,000 Hours of Data
Audio and video recordings can be controlled by the           The network of cameras and microphones are generating
participants in the house using miniature wall-mounted        an immense flow of data: an average of 300 gigabytes of
touch displays (Figure 2, right). Cameras are clustered       data per day representing about 132 hours of motion-
into eight visual zones (cameras that view overlapping        compressed video per day (12 hours x 11 cameras) and
physical spaces are grouped into zones). Eight touch          182 hours of audio (13 hours x 14 microphones). In
displays are installed next to light switches around the      just the first six months we have collected approximately
house, each enabling on/off control over video recording      24,000 hours of video and 33,000 hours of audio. At this
in each zone by touching the camera icon. Audio record-       rate, the data set is projected to grow to 142,000 hours of
ing can also be turned on and off by touching the micro-      video and 196,000 hours of audio by the end of the three
phone icon. To provide physical feedback on the status        year period. Clearly, new data mining tools must be
of video recording, motorized shutters rotate to conceal      designed to aid in analysis of such an extensive corpus.
cameras when they are not recording. The “oops” but-             Figure 3 shows a screen shot of a multichannel
ton at the bottom of the display (marked with an excla-       data browser and annotation system developed for this
mation mark) opens a dialog box that allows the user          project. Spectrograms of selected audio channels are dis-
to specify any number of minutes of audio and/or video        played in the main window. A fish-eye timeline is used to
to retroactively and permanently delete from the disk         display large spans of data. The annotator can quickly
array.                                                        get the gist of activity throughout the house by view-
                                                         2061

           Figure 3: Graphical environment for multi-channel data visualization, playback, and annotation.
ing all channels simultaneously, and then focus in on          channel to transcribe given that sound sources usually
regions of interest. To annotate audio, the user selects       register on multiple microphones. To address these is-
segments by mouse and uses the right panel to add cat-         sues, we have developed a transcription system which
egorical labels (e.g., speech versus machine noise; iden-      automatically finds speech within long recordings using
tity of speaker, etc.) and text transcriptions. A second       a decision tree algorithm that operates on spectral fea-
window (not shown) displays video from all 11 cameras          tures extracted from the audio recordings. The speech
simultaneously and plays synchronized audio recordings.        detection algorithm has been trained using labeled ex-
Future iterations of the browser will include video visu-      amples created using the annotation system (Figure 3).
alization and annotation tools.                                Regions of speech activity are chunked at pause bound-
                                                               aries from the audio channel with the highest intensity
Machine-Assisted Data Analysis                                 and integrated with a “listen-and-type” interface that
                                                               automatically paces speech playback to keep up with
Speech analysis consists of two parallel paths. The first      transcription rate. In initial evaluations, we have found
path is to transcribe speech using a state-of-the-art au-      that one minute of conversational speech takes approxi-
tomatic speech recognizer (ASR) designed for speech in         mately 2.5 minutes to transcribe, signifying a 2- to 3-fold
noise [Prasad et al., 2002]. Even the best ASR systems         increase in speed over standard transcription tools used
will produce high error rates when transcribing uncon-         in the field.
strained speech. Thus the second path introduces hu-
man annotation. A large corpus of speech which occurs             To automatically generate contextual meta-data for
in the vicinity of the child will be manually transcribed      speech transcription, we are experimenting with algo-
in order to obtain a relatively error-free complete tran-      rithms for speaker identification, prosodic feature anal-
script of all speech heard and produced by the child. The      ysis, and audio event classification.
practicality of this latter goal will depend on available         Our long term plan is to adapt and apply computer
human resources for transcription as well as the devel-        vision techniques to the video corpus in order to detect,
opment of tools to speed transcription. Transcripts will       identify, and track people and salient objects. Since the
also provide immediate benefits to the development of          visual environment is cluttered and undergoes constant
the ASR, which requires approximately 50 hours of tran-        lighting changes (from direct sunlight to dimmed lamps),
scribed speech to adapt acoustic and language models for       automatic methods are inherently unreliable. Thus, sim-
optimal performance in the HSP recording environment.          ilar to our approach with speech transcription, we plan
   Current transcription tools are unsatisfactory for          to design semi-automatic tools with which humans can
working with large, multi-channel recordings. Significant      efficiently perform error correction on automatically gen-
time is spent on finding speech embedded within long           erated meta-data. The combination of automatic motion
stretches of non-speech audio, and in selecting which          tracking with human-generated identity labels will yield
                                                          2062

complete spatiotemporal trajectories of each person over                                                AM
the entire three year observation period. The relative lo-                              
                                                                                        
cations, orientations, and movements of people provide a                                
                                                                                        
basis for analyzing the social dynamics of caregiver-child                              
                                                                                        
                                                                                        
interactions.                                                                           
                                                                                        
                                                                                       
                                                                                                        AM
       Modeling In Vivo Word Learning                                                   
                                                                                        
                                                                                        
As data collection and analysis proceeds, the HSP cor-                                  
                                                                                        
pus may be used to study numerous aspects of language                                   
                                                                                        
including the development of grammatical constructions,                   
                                                                               
                                                                                        
                                                                                        
prosody, speech acts, and so forth. In this section, we                           
                                                                                       
                                                                                                         (OURS
describe first steps of one effort underway to model word                           
                                                                                        
learning.                                                                         
                                                                                        
                                                                                        
   In previous work, we developed a model of word learn-                        
                                                                                        
                                                                                        
ing called CELL (Cross-Channel Early Lexical Learning)                                  
                                                                                        
which learned to segment and associate spoken words                                     
                                                                                       
with acquired visual shape categories based on untran-
scribed speech and video input. This model demon-               Figure 4: Sample camera image from the kitchen camera
strated that a single mechanism could be used to re-            (top left), 10 regions of interest (bottom left), and visual-
solve three problems of word learning: spoken unit dis-         ization of activity of 1 minute, 22 minutes, and 24 hours
covery, visual category formation, and cross-situational
                                                                (right). For each of the displayed periods, the level of
mappings from speech units to visual categories. The
model operated under cognitively plausible constraints          movement in each region is indicated by the brightness
on working memory, and provided a means for analyzing           of the corresponding horizontal band, with time running
regularities in infant-directed observational recordings.       from left to right. In the lowest display of a full 24 hour
   CELL was evaluated on speech recordings of six moth-         period, three meals are revealed as clusters of activity
ers as they played with their pre-verbal infants using          most clearly indicated in region 10.
toys. Recordings were made in an infant lab using a
wall-mounted video camera and a wireless microphone
worn by the mothers. The speech recordings were paired          verbs (since it did not model actions), nor could it learn
with video of the same objects recorded by a robot,             social terms such as hi and thank you.
providing multisensory “first-person” audio-visual input           The HSP corpus overcomes the limitations inherent in
for the model (the video of caregiver-infant interactions      collecting small corpora within laboratory settings. To
was only used to keep track of which toy was in play).         address the issue of semantic grounding in terms of phys-
CELL successfully acquired a vocabulary of visually-           ical and social action, we have recently developed com-
grounded words using a learning strategy that combined         putational models of perceived affordances for language
within-modality co-occurrence and recurrence analysis          understanding [Gorniak, 2005] and intention recognition
with cross-modal mutual information clustering. The            for word learning [Fleischman and Roy, 2005]. In these
model enabled quantitative analysis of the effects of vi-      models, stochastic grammars are used to model the hi-
sual context on speech segmentation, and the effects of        erarchical and ambiguous nature of intentional actions.
short term memory size on word learning performance            In [Fleischman and Roy, 2005], sequences of observed
(see [Roy and Pentland, 2002] for details).                    movements are parsed by behavior grammars yielding
   Three simplifications made in CELL may be con-              lattices of inferred higher level intentions. Verb and noun
trasted with our new modeling effort using the HSP cor-        learning is modeled as acquiring cross-situational map-
pus. First, CELL was evaluated on a relatively small           pings from constituents of utterances to constituents of
set of observations. Caregiver-infant pairs were only ob-      intention lattices. We plan to use a similar approach
served for two one-hour play sessions, held about a week       with the HSP data, but with a semi-automatic procedure
apart. The data was thus a snapshot in time and could          for learning behavior grammars from video data. Words
not be used to study developmental trajectories. Sec-          related to routines (baths, meals, etc.) and names of lo-
ond, observations were conducted in an infant lab leading      cations (crib, highchair, etc.) might be modeled on this
to behaviors that may not be representative of natural         basis.
caregiver-infant interactions in the home. It is unclear           The first stage in learning behavior grammars is
whether CELL’s learning strategy would work with a             to identify stable, hierarchically organized patterns of
more realistic distribution of input. Third, visual in-        movements that yield a “behavior lexicon”. We exploit
put was oversimplified and social context was ignored.         the fact that we have static cameras in order to divide
The only context available to CELL was video of single         each room into human assigned regions of interest (in-
objects placed against controlled backdrops. As a con-         dicated as numbered regions on the lower left of Figure
sequence, the model of conceptual grounding in CELL            4). These regions correspond to locations or objects in
was limited to visual categories of shapes and colors un-      the room that do not move (e.g., the refrigerator) and
derlying words such as ball and red. It could not learn        have some relevance to various actions (e.g., is used in
                                                           2063

cooking events). Computing the amount of movement                 Infants’ reliance on a social criterion for estab-
in each region provides a multi-variate representation of         lishing word-object relations. Child Development,
movement patterns within a room. This representation              67(6):3135–53.
is useful for visualizing behavioral patterns at multiple
time scales (see right side of Figure 4).                       [Bloom, 1973] Bloom, L. (1973). One word at a time.
                                                                  Mouton, The Hague.
   We can recast the problem of learning behavior gram-
mars into one of discovering reliable patterns of move-         [Bloom, 2000] Bloom, P. (2000). How Children Learn
ment in these multi-variate data streams. In an initial           the Meanings of Words. MIT Press, Cambridge, MA.
experiment, we used a set of temporal relations such as
before and during as the basis for learning these pat-          [Brown, 1973] Brown, R. (1973). A First Language: The
terns. We designed an algorithm that identifies move-             Early Stages. Harvard University Press.
ments in different regions of interest that reliably oc-        [Bruner, 1983] Bruner, J. (1983). Child’s Talk: Learning
cur in those temporal relations (e.g. counter-movement            to Use Language. Norton.
before sink-movement). When a relation between such
primitive movements becomes significantly reliable, it is       [Clark, 1987] Clark, E. (1987). The principle of contrast:
treated as a complex movement which itself can partic-            A constraint on language acquisition. In Mechanisms
ipate in temporal relations with other movements. The             of language acquisition. Erlbaum, Hillsdale, NJ.
algorithm proceeds through the data in an online fash-          [Fleischman and Roy, 2005] Fleischman, M. and Roy,
ion, generating hierarchical patterns of movement until           D. (2005). Why are verbs harder to learn than nouns?
all the data is examined. We have found that these pat-           Initial insights from a computational model of situ-
terns can be used to recognize high level events such as          ated word learning. In Proceedings of the 27th Annual
making coffee and putting away the dishes (details forth-         Meeting of the Cognitive Science Society.
coming).
   Extensions of this work will focus on developing a           [Gorniak, 2005] Gorniak, P. (2005). The Affordance-
video parser that uses grammars constructed from ac-              Based Concept. PhD thesis, Massachusetts Institute
quired behavior patterns to infer latent structure un-            of Technology.
derlying movement patterns. Cross-situational learning          [MacWhinney, 2000] MacWhinney, B. (2000).             The
algorithms will be developed to learn mappings from spo-          CHILDES project: Tools for analyzing talk. Lawrence
ken words and phrases to these latent structures.                 Erlbaum Associates, Mahwah, NJ.
                      Conclusions                               [Markman, 1989] Markman, E. M. (1989). Categoriza-
                                                                  tion and naming in children. MIT Press, Cambridge,
The Human Speechome Project provides a natural, con-              MA.
textually rich, longitudinal corpus that serves as a basis
for understanding language acquisition. An embedded             [Merriman, 1999] Merriman, W. (1999). Competition,
sensor network and data capture system have been de-              attention, and young children’s lexical processing. In
signed, implemented, and deployed to gather an ultra-             MacWhinney, B., editor, The Emergence of Language,
dense corpus of a child’s audio-visual experiences from           pages 331–358. Lawrence Erlbaum Associates.
birth to age three. We have described preliminary stages
                                                                [Nelson, 1974] Nelson, K. (1974). Concept, word, and
of data mining and modeling tools that have been devel-
                                                                  sentence: Interrelations in acquisition and develop-
oped to make sense of over 300,000 hours of observations.
                                                                  ment. Psychological Review, 81:267–285.
These efforts make significant progress towards the ul-
timate goal of modeling and evaluating computationally          [Prasad et al., 2002] Prasad, R., Nguyen, L., Schwartz,
precise learning strategies that children may use to ac-          R., and Makhoul, J. (2002). Automatic transcription
quire language.                                                   of courtroom speech. In Proc. IEEE Int. Conf. Acous-
                                                                  tics, Speech, and Signal Processing, pages 1745–1748,
                 Acknowledgments                                  Denver, CO.
We thank Walter Bender and the Media Lab indus-                 [Roy and Pentland, 2002] Roy, D. and Pentland, A.
trial consortia for project support, Seagate Corpora-             (2002). Learning words from sights and sounds: A
tion and Zetera Corporation for donation of disk storage          computational model. Cognitive Science, 26(1):113–
hardware, Steve Pinker, Brian MacWhinney, and Linda               146.
Smith for helpful discussions, and Steve Pinker for nam-
ing the project. This paper is based upon work sup-             [Smith, 2000] Smith, L. (2000). Learning how to learn
ported under a National Science Foundation Graduate               words: An associative crane. In Golinkoff, R. and
Research Fellowship, a National Defense Science and En-           Hirsch-Pasek, K., editors, Becoming a word learner.
gineering Fellowship, and NSF Award SGER-0554772.                 Oxford Univeristy Press.
                                                                [Tomasello and Stahl, 2004] Tomasello, M. and Stahl,
                       References                                 D. (2004). Sampling children’s spontaneous speech:
[Baldwin et al., 1997] Baldwin, D., Markman, E., Bill,            How much is enough? Journal of Child Language,
   B., Desjardins, R., Irwin, J., and Tidball, G. (1997).         31:101–121.
                                                           2064

