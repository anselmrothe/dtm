UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Cognitive Models of the Effect of Audio Cueing on Attentional Shifts in a Complex
Multimodal, Dual-Display Dual Task
Permalink
https://escholarship.org/uc/item/70d1x38m
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Brock, Derek
Halverson, Tim
Hornof, Anthony J.
et al.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                      University of California

                    Cognitive Models of the Effect of Audio Cueing on Attentional
                         Shifts in a Complex Multimodal, Dual-Display Dual Task
             Derek Brock (brock@itd.nrl.navy.mil) and Brian McClimens (mcclimen@itd.nrl.navy.mil)
                                           Naval Research Laboratory, 4555 Overlook Ave., S.W.
                                                          Washington, DC 20375 USA
             Anthony Hornof (hornof@cs.uoregon.edu) and Tim Halverson (thalvers@cs.uoregon.edu)
                             Department of Computer and Information Science, 1202 University of Oregon
                                                         Eugene, OR 97403-1202 USA
                              Abstract                                    convey a variety of task-related information, including the
                                                                          onset, location, and identity of critical events.
   A comparative cognitive model of two manipulations of a                    Brock et al. (2004) demonstrated that the use of 3D sound
   complex dual task in which 3D audio cueing was used to                 can significantly improve dual-task performance. The
   improve operator performance is presented. The model is
                                                                          research presented here describes recent cognitive modeling
   implemented within the EPIC cognitive architecture and
   describes extensions that were necessary to simulate gaze shifts       work that has been done to explain the effects of audio cueing
   and the allocation of attention between separated task displays.       observed by Brock et al., and the effect of audio cueing on the
   A simulation of meta-cognitive decision-making to explain              allocation of attention between tasks. The models are based
   unprompted, volitional shifts of attention and the effect of audio     on the human data observed by Brock et al. in (a) the “no
   cueing on performance and the frequency of attention shifts are        sound” condition and (b) one particular sound condition (the
   explored.                                                              “screen-centric” condition).
                                                                              Cognitive modeling is a research practice that endeavors to
Keywords: cognitive modeling; EPIC; dual task; 3D                         build computer programs that behave in some way like
auditory cueing; separated task displays; gaze shifts;                    human beings. The models presented here are implemented
volitional shifts of attention; simulated meta-cognitive                  within the EPIC (Executive Process-Interactive Control)
decision-making; gamma distribution; sense of timing                      cognitive architecture (Kieras and Meyer, 1997), which is a
                                                                          computational framework for building models of human
                                                                          performance based on the constraints of human perceptual,
              Introduction and Background                                 cognitive, and motor processing.
System designers take numerous approaches to reduce the                       The cognitive modeling presented in this paper specifically
number of workstation operators necessary to accomplish                   explores (a) a simulation of meta-cognitive decision-making
complex decision-making tasks in Navy command-and-                        to explain the volitional shifts of attention, (b) performance
control centers. Approaches include (a) the automation of                 aspects of task-related audio cueing, a somewhat new domain
multiple tasks and (b) the adoption of supervisory rather than            for cognitive modeling, and (c) extensions to the perceptual-
direct control. As workstation operators are asked to manage              motor components of EPIC that were necessary to simulate a
an increasing number of tasks, reliable techniques are needed             complex dual-display task.
to manage operator attention. The research presented here
demonstrates how cognitive modeling can explain how                                   The Attention Management Study
operators manage conflicting attention demands and also how
cognitive modeling can inform the design of human-machine                  The Dual Task
interfaces that facilitate efficient and accurate multi-display,
multi-task execution.                                                      Figure 1 shows the physical layout of the dual task modeled
   The Navy has developed a prototype decision support                     in this paper. The task is from Brock et al. (2004).
workstation that features three flat-panel monitors centered in            Participants used a joystick to continuously track an evasive
a 135° arc in front of the user (Osga, 2000). With this                    target on the right and, at the same time, used the keyboard to
configuration, the operator can access much data but loses                 periodically assess and classify “blips” moving down the
peripheral access to all three monitors when his or her gaze is            radar screen on the left. The right task is “tracking” and the
turned to look at either the right or left screen. This loss can           left task is “tactical.” The task displays were separated by 90°
reduce the speed and accuracy of critical decisions (Brock et              of arc, such that the unattended display could not be seen with
al., 2002; Brock et al., 2004).                                            peripheral vision. The task was originally developed by
   The Naval Research Laboratory (NRL) is developing                       Ballas, Heitmeyer & Perez (1992) and is analogous in many
techniques for directing attention in complex operational                  ways to the level of multitask activity that future Navy
settings using three-dimensional (3D) or “spatialized” sound               workstation operators will be subjected to.
(Begault, 1994). Properly designed 3D sounds can be used to
                                                                      1044

                                                                   conditions. Response time is the total time to classify a blip
                                                                   (with two keystrokes) after it changes color.
                                                                                                   Number of Gaze Shifts                              Tactical Assessment Response Times
                                                                                            475                                                3400
                                                                                            450
                                                                                                                                               3200
                                                                                            425
                                                                       ave. (total count)
                                                                                            400                                                3000
                                                                                                                                   ave. (ms)
                                                                                            375
                                                                                                                                               2800
                                                                                            350
                                                                                            325                                                2600
                                                                                            300
                                                                                                                                               2400
                                                                                            275
                                                                                            250                                                2200
                                                                                                   No-Sound                Sound                            No-Sound              Sound
                                                                        Figure 2: The total number of gaze shifts (based on head
                                                                         turns) and the response times observed in the no-sound
                                                                       (baseline) and sound conditions. (Note the nonzero y-axis.)
         Figure 1: The physical layout of the dual task.              As seen in Figure 2, audio cueing reduced the number of
                                                                   head turns that were needed, and at the same time improved
   Whereas the tracking demands continuous attention, the          the tactical assessment response times (with no difference in
tactical task can be accomplished with brief intermittent          response accuracy). Both differences are significant (p <
glances and bursts of activity. The procedure for tactical         .001). These results demonstrate that screen-centric audio
assessment and classification is somewhat complex. The             cues can improve complex dual-task performance.
tactical radar deals in three shapes of blips. All initially          Table 1 shows counts of the attentional shifts made by
appear as black, numbered icons, and move at varying slow          participants, and provides a more detailed picture of how
speeds, and in different patterns, from the top to the bottom of   participants allocated their attention in the no-sound and
the screen. A few seconds after each blip first appears, its       sound conditions. The no-sound condition, for example,
color is changed from black to red or blue or yellow. This         required 44% more shifts from the right (tracking) display to
color-coding lasts for about ten seconds, and during this time,    left (tactical) display (174 compared to 121). The data also
a two-keystroke combination must be entered—the blip’s             indicate that, even in the sound condition, participants did not
number and its classification. Red and blue blips are              rely solely on audio cues to monitor the status of blips on the
classified as, respectively, hostile and neutral. Yellow blips     tactical radar screen. Even though only 65 blips were
must be classified based on their onscreen behavior and rules      presented in each manipulation, participants in the sound
learned in advance. After assessment (or ten seconds), the         condition averaged 121 looks from right to left; this means
blip disappears.                                                   that participants made, on average, 56 additional self-
                                                                   motivated shifts to the tactical task. These 56 inspections are
Auditory Cueing In some conditions (from Brock et al.,             perhaps analogous to the 174 self-motivated right-to-left
2004), the tasks were augmented with 3D auditory cues. This        shifts in the no-sound condition. In each condition, this is an
paper discusses models of the baseline “no sound” condition        estimate of the total number of meta-cognitively-prompted
and one of the “sound” conditions (the screen-centric              volitional shifts from right to left.
condition). A unique sound loop was played for each of the
three different blip shapes on the tactical radar screen. Audio        Table 1: Counts of attentional shifts (based on head turns)
cues were sounded when blips were color-coded. Only one               observed in the no-sound and sound conditions. Location key:
audio cue was played at a time. Thus, if a new blip became                          right = tracking and left = tactical.
color-coded while another blip’s auditory cue was already
playing, the new one had to wait until the previous blip was                                          No-Sound - Mean Count of Attentional Shifts
classified and/or disappeared. The apparent 3D source of the                                      Shift from      Right    Left      Keybd
                                                                                              Right to                0    174             7                                     180
sounds was located forward and 45° to the left of the                                         Left to              170        0          27                                      197
orientation of participant’s head. Headphones were used, but                                  Keybd to               10      23            0                                      34
not head tracking.                                                                                                 180     197           34                                      411
Empirical Measures of Performance The performance                                                       Sound - Mean Count of Attentional Shifts
                                                                                                  Shift from      Right     Left      Keybd
measures discussed here (originally presented in Brock et al.,                                Right to                0     121            6                                     127
2004) are derived from (a) tactical-response timing data                                      Left to              116         0          23                                     139
logged by the dual-task software, and (b) counts of participant                               Keybd to               11       19           0                                      29
gaze shifts between the left and right displays and the                                                            127      139           29                                     295
keyboard, logged by an experimenter on a Palm Pilot. The
actual observed shifts were head turns, which are assumed to
correspond to gaze shifts and attentional shifts based on the
large visual angle among the devices.
   Figure 2 shows the mean number of gaze shifts and tactical
assessment response times in the sound and no-sound
                                                               1045

   The role and frequency of these meta-cognitive decisions to       during which that subtask was completed “automatically” by
shift from tracking to tactical assessment is explored in the        the computer (Ballas, et al., 1999).
model through parametric manipulations of a gamma                        Similarly, the modeling work presented here examines a
distribution. The distribution is used to represent the time         different set of issues than did the modeling work of Kieras et
that elapses before the participant motivates an internal            al. The prior modeling effort focused on audio-visual
decision to switch back to the tactical task in the absence of       localization performance and on the problem of explaining
any external motivator to do so.                                     observed patterns of response time sequences corresponding
                                                                     to the negative effects of the automation deficit. Also, in the
                                                                     prior work, switches of attention from tracking to tactical
          Modeling Dual-Task Performance                             assessment in the model were motivated by the appearance of
                                                                     radar blips in peripheral vision. In the new version of the task
The Modeling framework                                               presented here, the two visual displays are far apart, and so
The EPIC cognitive architecture (Executive Process-                  switches of attention are motivated either by auditory cues or
Interactive Control; Kieras and Meyer 1997) was used in this         volitional decisions to shift attention.
modeling effort. EPIC is a unified theory of human
perceptual, cognitive, and motor processing that provides a           Architectural Extensions Modeling the complexity of
computational framework for modeling human information                the current dual task required two extensions to EPIC.
processing in simulated task environments. Based on                      The first extension to EPIC was to add a new version of
fundamental human processing capabilities and limitations,            ocular motor movement that corresponds to both an eye and
the architecture is designed to accurately capture the details of     head movement. This was needed to model gaze shifts
human performance in predictive cognitive models.                     between the two task displays, which are separated by 90° of
   Cognitive processing in EPIC is implemented as a                   arc. Longer eye movements are generally accompanied by
production rule system that allows multiple rules to fire in          head movements (Corneil & Munoz, 1996), though the time
parallel, and whose working memory maintains a declarative            course of these longer movements can be described by the
representation of the world. Perceptual-motor processing is           same time course of smaller eye movements, and is linear for
implemented as a set of simulated sensory and motor                   amplitudes from 5° to at least 90° (Becker, 1991). The time
processing peripherals, running in parallel with cognition,           course of the newly-programmed eye-and-head movement
whose organization, performance, and constraints are derived          corresponds to the linear relation given by Carpenter (1988).
from the human performance literature.                                   The second modification to EPIC was to introduce, in
   Models in EPIC are composed of three principal                     effect, a “sense of timing.” The architecture needed a way to
components: a reproduction of the interactive task                    maintain an internal sense of timing and priority of a subtask,
environment; a set of perceptual features associated with the         which in this case was the timing associated with meta-
stimuli in the task environment; and a task performance               cognitively prompted volitional shifts of attention to the
strategy, implemented as a set of production rules. Models            tactical task in the absence of any new perceptual stimuli.
are run in the context of a task simulation, with the task            (Recall that 56 self-motivated shifts were observed in the
strategy guiding various motor activities as well as the focus        sound condition, and 174 in the no-sound condition.) This
of perceptual attention (via eye movements), which in turn            sense of timing was not needed when modeling earlier
informs cognition, which further acts on the task simulation          versions of the task because the two displays were adjacent
through the execution of motor processing.                            and blips could be perceived peripherally.
                                                                         The sense of timing was introduced via a mechanism in the
Earlier, Related Modeling Work The EPIC framework was                 production rule system. The timing command generates
chosen a variety of reasons. It was used to model a nearly            unprompted shifts of attention between tasks based on a
identical, earlier version of the dual task that was displayed        generalized form a cumulative gamma distribution, which is
on a single screen (Kieras, Ballas, & Meyer, 2001), and this          characterized by McGill and Gibbon (1965) as useful for
presented an opportunity to elaborate on a body of existing           modeling multistage processes that are measured as a single
work. However, EPIC was also chosen because its auditory              reaction time. The distribution is characterized by two free
processing is more complete that of other cognitive                   parameters that specify its shape and scale. Manipulations of
architectures, and because, in the core architecture, it              these parameters in the context of the model's comparative
accomplishes visual tasks by moving its simulated eyes,               performance with and without audio cues are evaluated below
which corresponds tightly with our empirical observations of          in the discussion of the model's performance.
how people executed the dual task in the dual-screen
configuration.                                                        The Model
   In the prior version of the dual task, audio cueing was not        An EPIC model was constructed to simulate and predict how
used to assist in real-time attention management between              people perform the dual-display dual task. The model’s
subtasks, but to reduce an “automation deficit” that occurred         organization largely follows the same hierarchical scheme
when participants resumed the tactical task after a period            developed by Kieras et al. (2001). A top-level executive
                                                                 1046

process controls the execution of three sub-processes, two of           A detailed description of the tactical subtask is beyond the
which carry out the subtasks of tracking and tactical               scope of this paper. The task is very complex, though it is
assessment.      The third sub-process performs a global            modeled with great detail and fidelity. Once the eyes arrive
monitoring role and updates working memory based the state          on the tactical radar display, a great many decisions are made
of the radar display. The functions of these three sub-             and continue to be made throughout the subtask. Decisions
processes in the baseline model are described next, including       pertain to which blips to put the eyes on, when and whether to
how they have evolved from original model.                          classify blips as neutral or hostile, when to move to another
                                                                    blip based on the color of the currently-fixated blip and other
The Tracking Task The new model’s tracking                          blips, when to move the eyes based on changes in blip status
implementation remains an intentionally simple process that         during the task, the manual motor process of entering blip
continuously follows the target with the eyes and manually          classifications, and even eye movements to the keypad.
pushes the cursor toward it. Consistent with participant                The sub-processes include selection, classification, and
behavior discussed by Ballas et al. (1999), the model               response. Consistent with the architecture’s constraints,
suspends tracking when it turns its attention to the tactical       though, much of the cognitive and motor processing for these
assessment task.                                                    sub-processes can overlap. The response sub-process first
                                                                    waits for the monitoring process to classify the selected blip
The Monitoring Subtask The monitoring subtask updates               as hostile or neutral, and then uses this information in
working memory with status changes in the environment.              working memory to select the appropriate key and carry out
Most of the responsibilities are carried over from the previous     the keystroke. The response sub-process contains a simple
model, although its role in the allocation of attention between     probabilistic rule that causes the model to occasionally move
the tasks has changed. Formerly, this process ran in parallel       its gaze to the keyboard while executing the keystroke. This
with the tracking process and notified the dual-task executive      contributes to both the gaze shifts and increased response
of changes in the radar display to prompt a task switch. In the     times that are observed in the no-sound condition.
current model, however, with no peripheral access to the
radar display, the monitoring process is used to trigger timing     How the Model Responds to Audio Cues The new model
commands every time the tracking task resumes. These                performs the dual task on dual screens both with and without
commands start EPIC’s new “sense of timing” clock which,            sound. To respond to audio cues, a rule was added to the top-
based on a gamma distribution, stochastically determines an         level executive process to listen for audio cues. When a cue
appropriate time in the near future to notify the dual-task         is detected, the tracking task is suspended and the gaze moves
executive that it is time to switch to the tactical task.           to tactical display. It was not necessary to add rules to the
   As in the original model, the monitoring process also            blip selection sub-process to associate the cues with their
monitors the visual status of blips and notifies the dual-task      corresponding blip shapes. However, the rules in the sub-
executive as these events occur. In the new model, the              process that try to classify a black blip before returning to the
monitoring process also classifies blips as hostile or neutral.     tracking task led to unrealistically fast performance in the
An analog of this classification function was present in the        sound condition. Accordingly, this part of the selection
single-screen model, but its parameters were different.             strategy was removed for the sound condition.                The
Independently established free parameters for the time needed       implications are interesting, and discussed in the next section
to inspect the behavior each of the three blip types were used      on modeling results.
in lieu of modeling eye movements for which there was no
empirical data. An analysis of the participant response time                              Modeling results
data in Brock et al. (2004), however, suggests that, counter-
                                                                     Once the task analysis was implemented and other aspects of
intuitively, response times for red and blue blips were roughly
                                                                     the model’s structure were settled, its free parameters were
equivalent to those for yellow blips. Thus, in the new model,
                                                                     derived and its performance strategy was adjusted for each
all colors of blips are classified by the monitoring process
                                                                     condition. The time required for blips to be classified as
using a single time parameter.
                                                                     either hostile or neutral was calculated by running a version
                                                                     of the model in which only audio cues prompted looks to
The Tactical Assessment Task The tactical assessment task
                                                                     events on the tactical assessment display, and the timing of
is very complex and operates at the same level as the tracking
                                                                     the classification procedure was set to zero. The resulting
and monitoring sub-processes.             Unlike tracking and
                                                                     response times represented the performance overhead
monitoring, though, tactical assessment is hierarchically
                                                                     associated with selecting and responding to blips. The times
organized as an executive sub-process. It controls the
                                                                     were subtracted from the empirical mean for the screen-
execution the three sub-sub-processes that select, classify, and
                                                                     centric sound condition. The difference (670 ms) was used as
respond to blips. Although the new model implements a
                                                                     the time required to classify all blips.
number of changes here, only the blip selection sub-process
                                                                        The model’s response time performance in the sound
differs substantially. It now follows a more straightforward
                                                                     condition with this fitted classification time parameter,
search logic that is based on a careful reanalysis of the visual
                                                                     though, was unrealistically fast. In addition, it was spending
selection task.
                                                                     too much time on the left screen classifying black blips. This
                                                                     consequence of the modeling suggests that, in the sound
                                                                1047

condition, participants did not spend time trying to classify                                                      Table 2 shows mean counts of the model’s gaze shifts
black blips as they did in the no-sound condition. This aspect                                                  among the two displays and the keyboard, along with the
of the tactical sub-strategy was thus removed for the sound                                                     mean empirical counts from Table 1. The most important of
condition.                                                                                                      these shifts are the counts of right-to-left looks, which capture
   The shape and scale parameters of the gamma distribution                                                     the model’s allocation of attention to the tactical assessment
used to simulate self-prompted shifts of attention in the                                                       task. As can be seen, this measure of the model’s
baseline condition were determined manually. A gamma                                                            performance is quite close to the empirical data.
probability density curve was fit to the frequency histogram
of observed right-to-left gaze transition latencies, which were                                                       Table 2: Predicted/observed counts of attentional shifts
taken to represent the duration of dwell times on the tracking                                                      (based on head turns) in the no-sound and sound conditions.
task. The values of the shape and scale parameters of this                                                               Location key: right = tracking and left = tactical.
fitted distribution were respectively 2.5 and 0.95. The tail of
this distribution was noticeably steeper than the tail of the                                                                  No-Sound - Mean Count of Attentional Shifts
                                                                                                                      Shift from      Right         Left        Keybd
empirical data. An explanation for this discrepancy might be
                                                                                                                      Right to            0     174/174            0/7 174/180
that there was a greater degree of variability in the                                                                 Left to      168/170            0          25/27 193/197
experimenter's recording process for long latencies. At this                                                          Keybd to         7/10       18/23              0     25/34
point, the fitted model for the no-sound condition was                                                                             175/180      192/197          25/34 392/411
considered complete.
                                                                                                                                Sound - Mean Count of Attentional Shifts
   Appropriate gamma distribution parameters were
                                                                                                                      Shift from     Right          Left       Keybd
determined to motivate the 56 additional self-motivated shifts                                                        Right to           0     120/121             0/6 120/127
of attention in the sound condition. The gamma function’s                                                             Left to       93/116            0         28/23 121/139
shape parameter is commonly interpreted as the nth                                                                    Keybd to       27/11         0/19              0   27/29
occurrence of some event. Taking this to be descriptive of an                                                                      120/127     120/139          28/29 268/295
internal process that determines when a self-prompted look to
the tactical decision task should be carried out, it can be
reasoned that the same process is likely to apply in both the                                                                              Discussion
no-sound and sound conditions, only at different rates.                                                         Two particularly interesting aspects of this model include (a)
Therefore, the shape parameter should be held constant, and                                                     the difference blip assessment strategies necessary between
only the scale parameter varied across conditions. Using this                                                   the no-sound and sound conditions and (b) the model’s
reasoning to make the final fit, it was quickly determined that                                                 emergent behavior of looks away from the keypad.
widening the scale parameter to 2.5 in a run of the model in                                                       There are several reasons why participants might assess
the sound condition resulted in an average of 120 looks to the                                                  black blips much less frequently in the sound condition.
tactical assessment task. This difference in the scale                                                          Looks in the sound condition are in part driven by prompts in
parameter effectively measures the increase in meta-cognitive                                                   the task environment and, as a result, are generally more
volitional processing necessary when sound is removed from                                                      efficient. If a participant turns away from a blip that is about
the task environment.                                                                                           to change color in this condition, he or she is alerted to this
   Figure 3 compares the fitted model’s performance in the                                                      fact. In the no sound condition, however, the cost of turning
no-sound and sound conditions. Each of the performance                                                          back to the tracking task just before a blip changes color is
measures shown for the model is the mean of six randomly                                                        much greater because the response time for that blip is more
seeded runs, each of which was driven by a different tactical                                                   likely to be poor. As a result, participants have incentive to
task scenario. The model’s close fit with the mean number of                                                    dwell on the left screen longer when they feel a blip is getting
gaze shifts in each condition is a direct consequence of the                                                    close to changing color.
stochastic approach used to simulate self-prompted looks.                                                          The model’s emergent pattern of looks away from the
                                                                                                                keyboard is quite interesting. Looks to the keyboard were
                               Number of Gaze Sh                          Tactical Assessment Response T
                                                                                                                modeled for fidelity. However, the implementation led to an
                        475                                        3400                                         unforeseen result: The model reveals how looks away from
                        450
                        425
                                                                   3200                            emprica
                                                                                                                the keyboard interact with the blip selection sub-process.
   ave. (total count)
                                                                                                   model
                        400                                        3000
                                                                                                                When color-coded blips remain on the left screen, the model
                                                       ave. (ms)
                        375
                                                                   2800
                        350
                        325                                        2600
                                                                                                                always returns to the tactical assessment task from the
                        300
                        275
                                                                   2400                                         keyboard. In all other circumstances, the model returns to the
                        250
                              No-Sound         Sound
                                                                   2200
                                                                              No-Sound          Sound
                                                                                                                tracking task. No attempt was made to motivate these moves.
                                                                                                                It is particularly interesting that in the no-sound condition, the
                                                                                                                proportion of gaze shifts away from the keyboard in either
 Figure 3: The observed and predicted response times and
                                                                                                                direction matches observed data. This strengthens the
total number of gaze shifts (based on head turns) for the no-
                                                                                                                likelihood that the blip selection sub-process used in this
          sound (baseline) and sound conditions.
                                                                                                                condition is close to what participants actually used. It also
                                                                                                                follows that the corresponding disparity in the sound
                                                                                                             1048

condition suggests that the blip selection sub-process is more                                References
subtle.
   Eye-tracking data will enable us to directly examine aspects
                                                                     Ballas, J., Heitmeyer, C., and Perez, M. (1992). Evaluating
of the sub-strategies that participants use in the two
                                                                       two aspects of direct manipulation in advanced cockpits. In
conditions. For instance, it will show how often, and in
                                                                       CHI'92 Conference Proceedings: ACM Conference on
which conditions, participants spend time looking at black
                                                                       Human Factors in Computing Systems. Monterey, CA.
blips; whether time spent on a black blip directly benefits its
                                                                     Ballas, J., Kieras, D., Meyer, D., Brock, D., and Stroup, J.
corresponding response time; whether or not assessments that
                                                                       (1999). Cueing of display objects by 3-D audio to reduce
are interrupted effect response times; whether subjects spend
                                                                       automation deficit. In Proceedings of the 4th Annual
time on the left screen after a gaze shift from the keyboard;
                                                                       Symposium and Exhibition on Situational Awareness in the
and whether the left screen is only a brief stop for the eyes on
                                                                       Tactical Air Environment. Patuxent River, MD, 1999.
their way back to the tracking task.
                                                                     Becker, W. (1991). Saccades. In (Carpenter, R. H. S., ed.)
                                                                       Eye movements. Boca Raton, FL: CRC Press.
                         Conclusion                                  Begault, D. (1994). 3-D sound for virtual reality and
The long-term motivation for the modeling effort presented in          multimedia. Chestnut Hill, MA: AP Professional.
this paper is to analyze and predict the costs and benefits of       Brock, D., Ballas, J., Stroup, J., and McClimens, B. (2004).
using 3D audio in the information displays of operational              The design of mixed-use, virtual auditory displays: Recent
settings such as those the Navy expects to deploy in the next          findings with a dual-task paradigm. Proceedings of the
ten to fifteen years. Although many aspects of the model’s             10th International Conference on Auditory Display.
implementation, its performance strategies, and the process of         Sydney, Australia.
deriving appropriate values for its free parameters may not          Brock, D., Stroup, J. and Ballas, J. (2002a). Effects of 3D
appear to comment directly on this goal, several important             auditory cueing on dual task performance in a simulated
aspects of generally overlooked issues in the simulation of            multiscreen watchstation environment. In Proceedings of
human performance are addressed here. A computational                  the Human Factors and Ergonomics Society 46th annual
model of the performance benefits associated with an                   Meeting. Baltimore, MD.
uncluttered auditory information design is presented. The            Brock, D., Stroup, J. and Ballas, J. (2002b). Using an
model addresses the problem of usefully characterizing the             auditory display to manage attention in a dual task,
parameters of self-regulated allocation of attention. The              multiscreen environment. In Proceedings of the 8th
model predicts the effect of system level strategies for               International Conference on Auditory Display, Kyoto,
ameliorating effort when concurrent demands are involved.              Japan.
   Multi-task operational settings can be notoriously more           Carpenter, R. (1988). Movements of the eyes. London: Pion.
complex than the dual task modeled here, but designers of           Corneil, B., and Munoz, D. (1996). The influence of
supervisory control systems absolutely need to know the                auditory and visual distractors on human orienting gaze
baseline requirements for the allocation of attention before           shifts. Journal of Neuroscience. 16(24), 8193-8207.
they can design and implement effective attention                   Kieras, D., Ballas, J., and Meyer, D. (2001). Computational
management solutions. In particular, the ground truth for              models for the effects of localized sound cuing in a complex
modeling inter-task performance depends on knowing the                 dual task. TR-01/ONR-EPIC-13, University of Michigan,
demands of process combinations of unassisted access rates             Ann Arbor, MI.
to information that must be acted upon, acceptable levels of        Kieras, D. and Meyer, D. (1997). An overview of the EPIC
error, and requirements for initiative and physical effort that        architecture for cognition and performance with application
can be quantified.                                                     to human-computer interaction.           Human Computer
                                                                       Interaction, 12, 391-438.
                    Acknowledgments                                 Osga, G. (2000). 21st Century Workstations: Active partners
This work was supported by the Office of Naval Research                in accomplishing task goals. In Proceedings of the Human
under work request N0001406WX20042.                                    Factors and Ergonomics Society 44th Annual Meeting, San
                                                                       Diego, CA.
                                                                1049

