UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Effects of Negative Premises on Inductive Reasoning: A Psychological Experiment and
Computational Modeling Study
Permalink
https://escholarship.org/uc/item/2bp9v902
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Nakagawa, Masanori
Sakamoto, Kayo
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                      The Effects of Negative Premises on Inductive Reasoning:
                 A Psychological Experiment and Computational Modeling Study
                                  Kayo Sakamoto (SAKAMOTO@Nm.Hum.Titech.Ac.Jp)
                                         Tokyo Institute of Technology, 2-21-1 O-okayama,
                                                Meguro-ku, Tokyo, 152-8552 JAPAN
                             Masanori Nakagawa (NAKAGAWA@Nm.Hum.Titech.Ac.Jp)
                                         Tokyo Institute of Technology, 2-21-1 O-okayama,
                                                Meguro-ku, Tokyo, 152-8552 JAPAN
                            Abstract                                  studies, such as causal learning (e.g., Buehner and Cheng,
   Various learning theories stress the importance of negative        2005). However, the effects of negative premises have
   learning (e.g., Bruner, 1959; Hanson, 1956). However, the          rarely been discussed in any detail in the context of
   effects of negative premises have rarely been discussed in         inductive reasoning studies concerned with the evaluation
   any detail within theories of inductive reasoning (with the        of arguments.
   exception of Osherson et al., 1990). Although Sakamoto et al.          Investigating the effects of negative premises can
   (2005) have proposed some computational models that can            undoubtedly contribute to our understanding of inductive
   cope with negative premises and verified their psychological       reasoning. For instance, cases where the entities in both
   validity, they did not consider cases where category-based         negative and positive premises belong to the same category
   induction theory is ineffective, such as when the entities in
   both negative and positive premises belong to the same             are clearly problematic for the category-based induction
   category. The present study was conducted to test the              theory (Osherson et al., 1990) because it is impossible to
   hypothesis that, even when negative and positive premises          distinguish between negative premises and positive
   involve same-category entities, people can estimate the            premises from the categorical viewpoint. In contrast to the
   likeliness of an argument conclusion by comparing feature          similarity and coverage model based on category-based
   similarities. Based on this hypothesis, two computational          induction theory, Sloman (1993) has proposed a feature-
   models are proposed to simulate this cognitive mechanism.          based model based on a simple perceptron. According to
   While both these models were able to simulate the results          Sloman, knowledge of category structure is not required for
   obtained from the psychological experiment, a perceptron           the evaluation of arguments. Rather, he assumes that
   model could not. Finally, we argue that the mathematical
                                                                      argument evaluation is based on a simple computation of
   equivalence (from Support Vector Machines perspective) of
   these two models suggests that they represent a promising          feature similarities between the entities of the premises and
   approach to modeling the effects of negative premises, and,        the conclusion. Thus, Sloman’s feature-based model may be
   thus, to fully handling the complexities of feature-based          more effective at coping with negative premises than
   induction on neural networks.                                      category-based       induction    theory.     However,    the
                                                                      psychological validity of Sloman’s model has yet to be
                          Introduction                                tested in terms of processing negative premises.
    This study is concerned with evaluating “arguments”,              Accordingly, this study examines the validity of feature-
such as:                                                              based induction theory to handle these cases that are so
                   Collies produce phagocytes.                        problematic for category-based induction theory.
                                                                          In terms of model construction, what kind of model can
                   Horses produce phagocytes.
                                                                      adequately represent the cognitive process of feature-based
                 Shepherds produce phagocytes.                        induction, including negative premises? In addition to
                                                                      Sloman’s (1993) model, Sakamoto, Terai and Nakagawa
The propositions above the line are referred to as                    (2005) have also proposed a feature-based model. While
“premises” while the statement below is the “conclusion”.             structurally similar, their model extends the learning
The evaluation of an argument involves estimating the                 algorithm in order to cope with negative premises.
likelihood of the conclusion based on the premises.                   Moreover, their model utilizes corpus-analysis results to
Osherson, Smith, Wilkie, Lopez, and Shafir (1990) refer to            compute feature similarities, rather than the results of
this kind of argument as a “categorical” argument, because            psychological evaluations used in Sloman’s model. This
the predicate (e. g., “produce phagocytes”) in the premises           means that Sakamoto et al’s model is capable of simulating
and conclusion is attributed to one or more entities (e. g.,          a far greater variety of entities than Sloman’s model (over
“Collies”, “Shepherds”).                                              20,000 compared to just 46), because the corpus analysis
     The premises can also be negative in form (e. g.,                provides information for an enormous quantity of words.
“Penguins do not produce phagocytes ”). Since classic                 However, when induction of the appropriate category is
studies, such as discrimination learning (e. g., Hanson,              difficult, then the level of computation involved in the
1956) and concept learning (e.g., Bruner,1959), the                   feature similarity comparisons will far exceed the
importance of negative examples has been widely                       computational capacity of simple perceptrons. This study
recognized, and has been demonstrated in more recent                  therefore proposes a modified version of the Sakamoto et al
                                                                 2081

model, referred to as the “multi-layer feature-based neural           algorithm (See Kameya & Sato, 2005). In this study, term
network model”, which is compared with the previous                   “Ni ” represented a noun, and term “Aj ” represents a feature
perceptron model.                                                     word, such as a predicate. The number of latent classes was
     A further alternative model for coping with negative             fixed at 200.
premises would also seem to be possible. Osherson, Stern,                 For the actual estimation, the word co-occurrence
Wilkie, Stob, and Smith (1991) have also proposed a model             frequencies used were extracted from Japanese newspaper
to handle negative premises based on feature similarity,              articles, covering a ten-year span (1993-2002) of the
involving more complex computation than possible with                 Mainichi Shimbun. This co-occurrence frequency data
perceptrons. However, the Osherson et al.‘s feature-based             comprises the combinations of 21,205 nouns and 83,176
model requires knowledge of relevant taxonomical                      predicates in modification relations. CaboCha (Kudo &
categories, and also utilizes psychological evaluations like          Matsumoto, 2002), a Japanese analysis tool for modification
Sloman’s mo del. As the restricted number of available                relations, was used for extraction.
entities (46, similar to Sloman’s model) makes it difficult to            In order to test the assumption that the latent classes
apply that model, this study also modifies the Osherson et            correspond to categories, it is important to identify the
al feature-based model in order to handle greater numbers             meanings of the classes. In this case, it is possible to
of entities and to eliminate the need for categorical                 identify the meaning of a class from the conditional
knowledge.                                                            probability of the latent class Ck given a predicate Aj
     The outline of this study is as follows: First, the corpus       (P(Ck|Aj )) and the probability of the latent class Ck given a
analysis is described. The results of the corpus analysis             noun Ni (P(Ck|Ni )), which can be computed from the
were utilized in creating clear category definitions and in           estimated parameters P(Ck), P(Ni |Ck), and P(Aj |Ck ), applying
constructing the models. Second, a psychological                      the Bayesian theorem.
experiment is described which was conducted to examine
                                                                                              Class of Foods(c1)
the effects of negative premises, that cannot be accounted
for by the category-based induction theory. Third, two                                    P(c1|ni)                 P(c1|aj)
models —the multilayer feature-based model and a                             1       steak         0.876    boil down       0.987
modified version of Osherson et al’s feature-based                           2     set meal        0.867         eat        0.978
model—are proposed and tested in terms of their
                                                                             3                               fill one’s
psychological validity. Finally, this study argues that the                      grain foods       0.811    mouth with 0.937
mathematical equivalence (from a support vector machine                      4   vegetable soup 0.817       want to eat 0.927
perspective) of these two models suggests that they
                                                                             5       meat          0.739        stew        0.920
represent a promising approach to modeling the
complexities of feature-based induction on neural networks.                  6       curry         0.734     don’t eat      0.918
                                                                             7     Chinese         0.720      can eat       0.913
                 A Corpus Analysis for Category                                     noodle
                                                                             8       pizza         0.716         boil       0.894
            Definitions and Model Construction
                                                                             9   barleycorn                     clear
Categories used in this study are defined as latent semantic                                       0.594    one's plate     0.894
classes estimated from an analysis of the words in a                        10    rice cake        0.555      let’s eat     0.892
Japanese corpus. The estimations were based on a soft-
clustering of words according to modifying frequencies in
the corpus. The soft-clustering results are represented as                             Class of Valuable assets(c2)
conditional probabilities of words given the latent classes .                             P(c2|ni)                 P(c2|aj)
From these probabilities, the conditional probabilities of                   1       stock         0.929       issue        0.929
feature word/phrases, given particular nouns, are also                       2   government        0.862         list       0.916
computed. The models in this study applied these                                     bonds
conditional probabilities of feature words as the strengths of               3       place         0.791     increase       0.899
the relationships between nouns (entities) and features.                     4   building estate 0.780        release       0.884
     The method of soft-clustering was based on a method of                  5        real
                                                                                     estate        0.757     vend out       0.877
similar structure to Pereira’s method or PLSI (Hofmann
1999; Kameya & Sato 2005; Pereira, Tishby, and Lee 1993).                    6      cruiser        0.662         sell       0.852
This method assumes that the co-occurrence probability of a                  7    farmland         0.657    borrow on       0.841
term “Ni ” and a term “A j ”, P(Ni ,A j ), can be represented as
                                                                             8      foreign        0.628      not sell      0.802
formula (1):                                                                         bonds
 P ( N i , A j ) = ∑ P ( Ni | Ck ) P ( A j | Ck ) P (Ck ) ,
                    k
                                                            (1)              9       house         0.594
                                                                                                                buy
                                                                                                             and add        0.802
where P(Ni |Ck) is the conditional probability of term Ni ,                 10    currency         0.555        keep        0.781
given the latent semantic class Ck. Each of the probabilistic
parameters in the model, P(Ck), P(Ni |Ck), and P(Aj |Ck) are                                       Table 1.
estimated as values that maximize the likelihood of co-                           Examples of estimated classes and
occurrence data measured from a corpus using the EM                                   their representative members
                                                                 2082

                                                                                                   7
    Now you know the following premises:                                                                    withing category condition
                                                                              Conclusion ratings
                  Mr.H likes “physics”.                                                            6        between category condition
               Mr. H likes “astronomy”.
                                                                                                         ˎˎ
                                                                                                   5
              Mr. H doesn’t like “French” .
                                                                                                   4
    Please estimate how likely the following conclusion is true
                                                                                                   3
    given the above premises:
                                                                                                   2
                   Mr. H likes “chemistry” .                                                       1
   strongly            relatively             relatively           strongly                        0
    likely    likely     likely     neutral    unlikely unlikely   unlikely
                                                                                                         Low-rating group in         High-rating group in
                                                                                                         the within category         the within category
                                    …                                                                         condition                   condition
      Figure 1. Example of inductive reasoning tasks                                                   Figure 2. Conclusion likelihood ratings as a function of the
                                                                                                            within -category and between-category condition,
     This probability denotes the class membership of each                                                          and two rating groups (**: p<0.01)
word. Based on the estimations, most of the latent classes
were identified as meaningful categories, as shown in Table
1.
     From the estimated parameters P(Ck), P(Ni |Ck), and                                               Table 2. Example of the inductive reasoning task sets
P(Aj |Ck), it is also possible to compute the conditional                                           (inductive reasoning about the category of learning subject)
probabilities of feature words given particular nouns, as                                                                 within                   between
follows:                                                                                             positive                        "physics"
                ∑ P( Aj | C k ) P( N i | C k )P(C k ) . (2)
 P( Aj | N i ) = k
                                                                                                     premise                       "astronomy"
                   ∑k P( N i | C k )P(C k )                                                         negative
                                                                                                                        "French"
                                                                                                                                                 "shopping"
                                                                                                                                            (from the category
     In this study, this conditional probability P(Aj |Ni ) is                                       premise
                                                                                                                                                  of leisure)
assumed as the strengths of the relationships between
                                                                                                                                                chosen from
features and entities. When a certain feature word has a high                                                         chosen from
conditional probability given a particular noun, it is natural                                                                                 the category of
                                                                                                   conclusion        the category of
that the entity denoted by the noun has the feature indicated                                                                                 learning subjects
                                                                                                                    learning subjects
by the feature word. This conditional probability was                                                                                            and leisure
therefore applied in the models.
     Problematic Experimental Data for the
       Category-based Induction Theory                                                                           Table 3. Examp le of t wo rating groups
This study hypothesizes that people are able to estimate the                                              based on the ratings in the within -category condition.
likeliness of an argument conclusion, even when the entities                                                                    ratings   ratings      P(‘learning
of both negative and positive premises belong to the same                                                                        within   between      subject’|n)
category, by comparing feature similarities. This is
something which the usual category-based induction theory                                                     "mathematics"       6.000        5.864          0.727
cannot fully explain. In order to test this hypothesis, the                                        High-       "arithmetic"       5.745        5.661          0.711
following psychological experiment was conducted.                                                  rating
                                                                                                   group       "chemistry"        5.236        5.492          0.677
METHOD                                                                                                         "pharmacy"         4.655        4.712          0.701
Participants: Undergraduate students (N = 114) were                                                             "Japanese
randomly assigned to one of two inductive reasoning tasks                                                       literature"       3.836        3.729          0.717
presented in a questionnaire format; 59 students completed                                         Low-
                                                                                                                 "English"        3.036        4.136          0.699
one task list, while 55 students completed the other task list.                                    rating
                                                                                                   group        "Chinese"         2.964        3.441          0.801
Materials and Procedure: The questionnaire task lists
                                                                                                                "Hangeul"         2.800        3.407          0.701
required inductive reasoning. Each list consisted of three
task sets of inductive reasoning arguments. Each set
contained two positive premises where the entities belong to                                           premise belongs to the same category as the positive
a particular category, a negative premise, and thirty                                                  premises. In contrast, in the between-category condition, the
conclusions that share the same premise statements (See                                                negative premise belongs to a different category from the
Figure 1). In the within -category condition, the negative                                             positive premises (See Table 2).
                                                                                    2083

The premise and conclusion statements all consisted of a                       W ⋅ I (N i ) ,
                                                                    O( N i ) =
combination of a predicate (Mr. H likes ‘~’) and an entity                      I(Ni)
                                                                                         2
(curry), such as “Mr. H likes curry.” In the case of negative
premises, the predicate involved a negative verbal form,                                         N i ∈ N i+ , Ni− , Nic , (6)
such as “Mr. H doesn’t like sports.” As shown in Figure 1,
the participants were asked to rate the likelihood of the                       +                                             −
                                                                    where N i is the ith positive premise entity, N i is the ith
conclusions on a 7-point scale, given a set of three premises                                                   c
                                                                    negative premise entity, and N i is the ith conclusion entity.
presented above the conclusions. For each task list, the two                +               −                 c
                                                                    W( N i ), W( N i ) and W( N i ) indicate the weights when
conditions were counterbalanced among the three task sets.           N i+ , N i− and N ic are encoded as the premises and the
                                                                    conclusion, respectively. Ti denotes the target value for the
RESULTS                                                             ith conclusion. This value is obtained from psychological
The results for the two conditions were compared in terms           experiment. W represents the current weight when entity
of conclusion ratings. These conclusions were divided                N i is input. I(Ni ) is the feature vector of Ni , and the values
equally into a high-rating group and a low-rating group             of P(Aj |Ni ) are used for this vector. O(Ni ) is the activation
based on the rating scores in the within-category condition.        value of the output node as the response to I(Ni ). In the
Table 3 shows the members of each group. As shown in                actual simulation, the number of the vector elements was 20.
Figure 2, only the low-rating group of the within category          The feature words that are strongly related to the categories,
condition was significantly different from that group of the        including both positive and negative premise words, are
between category condition. On the other hand, the high-            selected. This selection is based on the assumption that only
rating group of the within category condition is not strongly       properties relevant to the context are used for induction (e.g.,
different from the group of the between category condition.         Shafto, Kemp, Baraff, Coley, and Tenenbaum, 2005).
For example, when the negative premise entity is “French”
that belongs to the category of learning subject, the rating        Multilayer Neural Network Model: It is well known that
of conclusion entity “English” is significantly lower than          this type of perceptron model cannot solve complex
when the negative premise is “shopping”. However, the               problems, such as linearly inseparable problems . The
ratings of conclusion entity “chemistry” in the cases of            similarity-based induction processing that is indicated from
negative premise “French” and negative premise                      the experimental data would appear to be beyond the
“shopping,” are not radically different. This would suggest         computational capacity of a perceptron-based model.
that “English” was judged as being similar to “French”, and         Accordingly, this study modifies the previous perceptron
hence its rating with the negative premise “French” was             model to create a multi-layer model. The structure of the
lower than that with the negative premise “shopping”. As            multilayer model is shown in Figure 3, and involves the
shown in Table 3, membership to the relevant category               following formula:
(P(C |N)) does not in itself yield a simple explanation of the
similarity between negative premises and conclusions,
which is problematic for the category-based induction                ok = σ    (∑ W f ), (7)
                                                                                          i    i
                                                                                                 k
                                                                                (∑ w x ), (8)
                                                                                     i
theory. In the next section, we will explore a solution to
such complex similarity judgments within inductive                   fik = σ           j     ij
                                                                                                   k
                                                                                                   j
reasoning by constructing some models based on the
feature-based induction theory.                                     where o k denotes the activation value of the output node
                                                                    when the pattern of the kth conclusion Nkc is input, Wi
                  Construction of the Models                        indicates the weights between the ith middle layer node and
Previous Perceptron Model: This study proposes two                  the output node, fi k represents the activation value of the ith
types of models. However, because their psychological               middle layer node, and xj k denotes the jth element of the kth
validity is compared with the validity of the feature-based         input pattern corresponding to P(Aj |Nkc). The activation
perceptron model proposed by Sakamoto et al. (2005), it is          strength of the output node o k represents the likelihood of
appropriate to start with a brief description of the feature-       the conclusion. An ordinary sigmoid function was adopted
based perceptron model.                                             as the activate function, σ , while the usual back
    The feature-based perceptron model is an extended
                                                                    propagation method was employed as the learning rule.
version of Sloman’s model (Sloman, 1993), which consists
of an input layer and one output node, where the weights            The premises and the conclusions were used for the learning
between the input nodes and the output node are estimated           process. In the learning process, the weight parameters are
by the usual delta method using the features strengths of           tuned so that the activation value of the output node o k
positive and negative premises, which are computed as the           equals 1 in the case of positive premises, equals 0 in the
conditional probability P(Aj |Ni ), as previously detailed,         case of negative premises, and equals each value obtained
according to the following formulas:                                from the conclusion ratings of psychological experiment in
                                                                    the case of conclusions. The number of input nodes is 20,
W ( N i+ ) = W ( Ni+−1 ) + [1 − O( Ni+ )]O( Ni+ ) , (3)             which is the same for the prior perceptron model. The
                                                                    number of middle layer nodes is set at 2, to keep the model
W ( N i− ) = W (N i−−1 ) + [0 − O( Ni− )]O( Ni− ) , (4)
                                                                    as simple as possible.
W ( N ic ) = W ( N i−−1 ) + [Ti − O( Ni− )]O( Ni− ) , (5)
                                                               2084

                        o     1
                               k         likelihood
                                     of the conclusion
                                                                                 similarities between the conclusion entity         N ic and the
                                                                                 positive premise entities, while SIM -(      N ic ) denotes the
                                                                                 similarities between    N ic and the negative premise entities.
                   f   k
                       1         f   2
                                      k
                                                                                 ß is the only parameter in these functions. d ij + and dij - are
                                                                                 also the original functions for word distance based on the
                                                                                 feature words (denoted as a k). Here, the number of feature
                                                                                 words m is fixed to 20, matching the other models in this
         x x x x x …
                                                                                 study. Although another similarity function was used in
             k          k            k           k          k
             1         2            3            4                               Osherson et al’s model, as it required knowledge about
                                                           5                     some taxonomical categories and about the feature strengths
                 the kth feature patterns :                                      of entities based on human ratings, that function would not
         P( a1 | nkc ), P( a2 | nck ), P (a3 | nck ), P (a4 | nkc )....          allow the extended model to handle vast quantities of
    Figure 3. Structure of Multilayer Neural Network Model                       features that change dynamically according to context.
Regression Model Based on Similarity Distance: This
study also proposes another model of complex induction                           Evaluating the Model Simulations according to
based on feature similarities, which is an extension of the                                        the Experimental Data
Osherson et al. (1991) similarity regression model. In that
                                                                                 Simulations for all three models were executed. Table 4
model, the likelihood of a certain conclusion is computed                        shows correlation coefficients between the simulation
using the linear summation of two kinds of similarities: the                     results and the results from the psychological experiment of
similarities that exist between the conclusion entity and the                    the within category condition, and F ratio (the fitness
positive premises and the similarities that exist between the                    indices for the models). On the other hand, all correlation
conclusion entity and the negative premises. These                               coefficients in the cases of the between category condition
similarities are based on the features. The regression model                     were larger than 0.7 and significant at p < 0.01, and all F
proposed in this study has greater flexibility than the                          ratio were also significant at p < 0.05. Considering these
previous perceptron model, and is, therefore , also capable of                   results, it is clear that the two models proposed in this study
simulating human performance for the complex task of                             correlate well with both conditions, while the previous
induction based on feature similarities.                                         model only correlates with the between-category condition.
In that model, the likelihood of a conclusion including                          These results indicate that the previous model is not able to
entity ci , denoted as v(ci ), is represented as follows:                        simulate the experimental results obtained when the entities
  ( )                      ( )
 v N ic = a SIM + N ic + b SIM − N ic + const         ( )            , (9)
                                                                                 in positive and negative premises both belong to the same
                                                                                 category.
                                  n+
where            ( )
         SIM + N ic = ∑ e
                                         − βdij+
                                                  , (10)                                       Table 4. Correlation coefficients
                                                                                               of the within category condition
                                   j
                        n−                                                                                       set1       set2         set3
 SIM − N  ( )= ∑e
               c
               i
                                 −βd ij−
                                         , (11)
                                                                              Regression Model
                                                                                   correlation coefficient **0.939         **0.841      **0.936
                          j                                                                          F ratio **30.06        **9.71      **28.58
             ((                 ) (                    ))
          m
                                                                              Multilayer Model
 d ij = ∑ P Ak | N i − P Ak | N j
    +                         c                      +   2
                                                            , (12)                 correlation coefficient **0.968         **0.816      **0.899
          k                                                                                          F ratio **135.94 **17.93           **37.93
             ((               ) (                    ))
         m
                                                                              Perceptron Model
 d ij = ∑ P Ak | N i − P Ak | N j
    −                       c                      −   2
                                                         . (13)                    correlation coefficient 0.185n.s. -0.09n.s.          0.36n.s.
          k                                                                                          F ratio 0.32n.s.      0.08n.s.     1.43n.s.
where a, b, and const are parameters estimated from the
likelihood of the positive premises (defined as value 7), the
likelihood of the negative premises (defined as value 1), and
the likelihood of each conclusion (value obtained from the
                                                                                                            Discussion
experiment).     N +j is the entity of the positive premise, and                 The experiment results reported in this study are consistent
 N −j is the entity of the negative premise. SIM +( N ic ) and                   with the hypothesis that people can estimate the likelihood
                                                                                 of a conclusion, even when the entities in both positive and
SIM -(   N ic ) are the original functions for the feature                       the negative premises belong to the same category, based on
                                                                                 comparisons of the similarities between entities in positive
similarities in this model. SIM +(                    N ic ) represents the      premises and conclusions, and between those in negative
                                                                            2085

premises and conclusions. Thus, these results provide                  positive premises and the conclusion, and between the
verification of this hypothesis. The previous perceptron               entities in negative premises and the conclusion.
model, proposed by Sakamoto et al. (2005), was not able to
simulate this experimental result.                                                         Acknowledgments
From the comparisons of the simulation and experimental                This study was supported by the Tokyo Institute of
results, it is clear that the multilayer neural network model          Technology       21COE       Program,      “Framework       for
and the regression model based on similarity distance both             Systematization and Application of Large-scale Knowledge
correlated well with the results from the experiments, and             Resources”.
that the performance of these models was better than the               The authors would like to thank Dr. T. Joyce, a post-doc
perceptron model. These results indicate the computational             with the 21COE-LKR, for his critical reading of
capacity of a perceptron model is not sufficient to handle             manuscripts and valuable comments on an earlier draft.
cases where the induction of the appropriate category is
difficult. On the other hand, the fact that the two proposed                                    References
models both correlated well with the experimental data
                                                                       Bruner, J. (1959). Inhelder and Piaget's The growth of
would seem to imply that two quite different approaches
                                                                         logical thinking. British Journal of Psychology, 50, 363-
can both provide equally adequate accounts of the cognitive              370.
mechanisms underlying inductive reasoning. Despite their               Buehner, M. J., & Cheng, P. W. (2005) Causal Learning. In.
different theoretical underpinnings, however, the two                    Holyoak, K. j., & Morrison, R. G. (Eds.) The Cambridge
proposed models would be represented in essentially                      Handbook of Thinking and Reasoning. New York NY:
identical ways in terms of support vector machines (SVMs)                Cambridge university press
(Vapnik, 1995). SVMs are a kind of multilayer neural                   Kameya, Y., & Sato, T. (2005). Computation of
networks that provide solutions to the types of problems                 probabilistic relationship between concepts and their
associated with multilayer neural networks, such as                      attributes using a statistical analysis of Japanese corpora.
determining the number of multilayer-nodes and local                     Proceedings of Symposium on Large-scale Knowledge
minimum convergence. In order to avoid such problems,                    Resources: LKR2005
SVMs map feature patterns onto another dimensional space,              Kudoh, T., & Matsumoto, Y. (2002). Japanese Dependency
where they become linearly partitioned. However, the                     Analysis using Cascaded Chunking. Proceedings of the
computation of such complex mapping can also be achieved                 6th Conference on Natural Language Learning: CoNLL
by a nonlinear-function computation, known as the kernel                 2002. 63-39.
function. The kernel function is unconstrained except in               Hanson, H. M., (1956). Effects of discrimination training on
instances where the function satisfies the mathematical                  stimulus generalization. Journal of Experimental
condition of ‘positive definiteness’. Returning to consider              Psychology, 58, 321-334.
the regression model based on similarity distance proposed             Hofmann, T. (1999). Probabilistic latent semantic indexing.
in this study, formulas (10) and (11) would correspond to                Proceedings of the 22nd International Conference on
the nonlinear mapping of the feature patterns       (
                                                  P Ak | N ic   )        Research and Development in Information
                                                                         Retrieval :SIGIR ’99. 50-57.
and    (           )
      P Ak | N j . Moreover, as d ij is a symmetric function           Osherson, D. N., Smith, E. E., Wilkie, O. Lopez, A., and
                                                                         Shafir, E. (1990). Category-Based Induction.
of the feature pattern,     (         )       (          )
                           P Ak | N ic and P Ak | N j , then             Psychological Review, 97, 2, 185-200.
                                                                       Osherson, D. N., Stern, J., Wilkie , O., Stob, M., and Smith,
   − βd ij                                            − βd ij            E. E. (1991) Default Probability. Cognitive Science, 15,
 e         is also symmetric, that means that      e          has        251-269.
‘positive definiteness’ and thus satisfies the condition of the        Pereira, F., Tishby, N., and Lee, L. (1993). Distributional
kernel function. This instance indicates that the regression             clustering of English words. Proceedings of the 31st
model can be represented as a SVM, that is, a multi-layer                Meeting of the Association for Computational Linguistics.
neural network. Consequently, the two models proposed in                 183-190.
this study both have properties that are mathematically                Sakamoto, K., Terai, A., and Nakagawa, M. (2005)
equivalent to the extent that after mapping feature patterns,            Computational Models of Inductive Reasoning and Their
which are nonlinear, linear partitions, as expressed in                  Psychological Examination: Towards an Induction-Based
formula (7) and (8), can be achieved in the case of the                  Search-Engine. Proceedings of the Twenty-Seventh
regression model. Thus, despite their surface differences,               Annual Conference of the Cognitive Science Society.
the two models proposed in this study would both appear to             Shafto, P., Kemp, C., Baraff, L., Coley, J., and Tenenbaum,
be tapping into the basic cognitive mechanisms underlying                J. B. (2005). Context -sensitive induction. Proceedings of
the complex nature of inductive reasoning involving feature              the Twenty-Seventh Annual Conference of the Cognitive
comparisons. In conclusion, in certain circumstances,                    Science Society.
people are able to estimate the likelihood of an argument’s            Sloman, A. T. (1993). Feature-Based Induction. Cognitive
conclusion through complex processing involving                          Psychology, 25, 231-280.
comparisons of feature similarities between the entities in            Vapnik, V. (1995). The Nature of Statistical Learning
                                                                         Theory. Springer.
                                                                  2086

