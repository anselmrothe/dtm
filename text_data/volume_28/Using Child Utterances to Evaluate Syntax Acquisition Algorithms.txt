UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Using Child Utterances to Evaluate Syntax Acquisition Algorithms
Permalink
https://escholarship.org/uc/item/3fh4h2t4
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Chang, Franklin
Lieven, Elena
Tomasello, Michael
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

               Using Child Utterances to Evaluate Syntax Acquisition Algorithms
                                      Franklin Chang (chang.franklin@gmail.com)
                                            NTT Communication Sciences Laboratories
                                        2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan
                  Elena Lieven (lieven@eva.mpg.de) and Michael Tomasello (tomas@eva.mpg.de)
      Department of Developmental and Comparative Psychology, Max Planck Institute for Evolutionary Anthropology
                                            Deutscher Platz 6, 04103 Leipzig, Germany
                           Abstract                                  set of words from the utterance that we are trying to
                                                                     predict.     This approach fits better with human
  Several algorithms for learning syntactic categories from          performance and allows us to use corpora from multiple
  distributional information were tested against utterances          languages to test ideas from theories of syntax acquisition
  from adults and children in twelve typologically different         and adult sentence production.
  languages. The evaluation measure that was developed
  allows one to examine word order constraints over a whole
                                                                        Our version of this evaluation measure will be called
  corpus and developmentally. By comparing several                   Word Order Prediction Accuracy (WOPA) and it is based
  different algorithms of varying abstraction against actual         on models of sentence production. In production, one has
  corpora of children’s speech, the evaluation measure               a message that one wants to convey. To model the effect
  determined that lexically specific knowledge is more               of the message in a rough way, we start with an unordered
  advantageous than more broad-based category knowledge              bag of words made up of words from the utterance that
  in predicting word order.                                          we want to predict, which we call the candidate set.
                                                                     Given this candidate set, the system has to try to predict
                       Introduction                                  the order of words in the sentence in a word-by-word
There is a growing interest in unsupervised computational            fashion. After a word has been produced, it is removed
approaches to syntax acquisition (e.g., Mintz, 2003;                 from the candidate set, and the system tries to predict the
Redington, Chater, & Finch, 1998). These systems                     next word in the sequence. After a sequence has been
collect statistical information from corpora and use that            produced in this manner, we compare the sequence
information to extract syntactic categories and constraints.         against the actual utterance in the corpus, and record
These systems are evaluated by comparing their internal              whether it was correctly predicted or not. The Word
representations with syntactic representations that have             Order Prediction Accuracy (WOPA) score is the number
been labeled or created by humans (e.g. tagged corpora).             of correctly predicted utterances out of all the utterances
To compare these systems to human children, we would                 of two words or more (one word utterances only have one
need to label child utterances with a set of syntactic               ordering).
representations. Since there is little agreement about the              In this article, we will use WOPA to compare six
nature of syntactic representations in human children at             syntax acquisition algorithms based on ideas from work in
each point in development (contrast Pinker, 1984, with               computational linguistics and child language. For input
Tomasello, 2003), it is difficult to use child utterances            and testing, we will use utterances from child and parent
with computational approaches that are evaluated against             interactions from twelve typologically-diverse languages.
human labeled representations.                                       Our goal is to be able to determine which of these
  One way to evaluate computational models that does                 algorithms best matches the knowledge that may have
not depend on tagged categories is suggested by                      yielded these corpora. More generally, we hope that this
connectionist approaches to syntax (Elman, 1990).                    demonstration will suggest that WOPA can be useful as a
Connectionist syntax acquisition models learn internal               general way of comparing computational approaches that
abstractions by making predictions over words. In these              use different internal representations.
systems, the accuracy at predicting the order of words in a
sentence is a measure of the system’s language                                                Corpora
knowledge. One difficulty with scaling these systems up              Computational learners should be tested against multiple
to real corpora is the use of neural assumptions such as             typologically-different languages to avoid biases towards
gradual weight-based learning. In language models (e.g.,             particular languages (e.g., English) or particular language
n-grams), similar evaluation measures have been used,                typologies (e.g., languages without rich morphology).
but since there are not enough constraints to predict the            This is not usually done, because tagged corpora are not
next word from the whole lexicon, researchers have                   always available for different languages. WOPA does not
typically looked at performance of these models with a               evaluate against human-labeled syntactic tags and can be
subset of the data (e.g., frequent words). Here, we used             evaluated against raw word-separated corpora. Our
prediction as the evaluation measure, but restrict it to the         corpora were twelve typologically diverse corpora
                                                                154

(Cantonese, Croatian, English, Estonian, French, German,          and hence it set up a competition between all of the words
Hebrew, Hungarian, Japanese, Sesotho, Tamil, Welsh)               that were activated by the message. So in this model, the
from CHILDES (Aldridge, Borsley, Clack, Creunant, &               sequencing system and meaning systems are
Jones, 1998; Berman, 1990; Demuth, 1992; Kovacevic,               independently trying to activate words, and the word with
2003; Lee, Wong, Leung, Man, Cheung, Szeto, & Wong,               the best combined activation is selected as the next word
1996; Miller, 1976; Miyata, 2000; Narasimhan, 1981;               in the utterance.
Réger, 1986; Suppes, Smith, & Leveillé, 1973;                        The Dual-path architecture was the basis for the
Theakston, Lieven, Pine, & Rowland, 2001; Vihman &                statistics that were collected in our non-connectionist
Vija, in press). In addition, two larger English and              category-based statistical learners. To represent the
German dense corpora from the Max Planck Institute for            sequencing pathway in the Dual-path model, a statistic
Evolutionary Anthropology were also used (Abbot-Smith             was collected called the context statistic. This statistic
& Behrens, in press; Lieven, Behrens, Speares, &                  represented how often the category of a word directly
Tomasello, 2003). These languages differ syntactically in         followed another word (akin to bigram statistics in
important ways. German, Japanese, Croatian, Hungarian,            computational linguistics). The other statistic represented
and Tamil have more freedom in the placement of noun              the message pathway in the Dual-path model and it was
phrases (although the order is influenced by discourse            called the access statistic. Unlike the context statistic,
factors) than English, French, and Cantonese. Several             which only encodes the relationship between adjacent
allow arguments to be omitted (e.g., Japanese,                    words, the access statistic encodes how often a category
Cantonese). Several have rich morphological processes             precedes other words in the sentence separated by any
that lead to complex word forms (e.g. Croatian,                   number of words (these statistics can encode long
Hungarian). Four common word orders are represented               distance dependencies). The most important difference in
(SVO, SOV, VSO, no default order). Eleven genera are              the context and access statistics will be in how they are
represented Chinese, Germanic, Finnic, Romance,                   used in production, which we will describe after we
Semitic, Ugric, Japanese, Slavic, Bantoid, Southern               describe the categorization algorithms and present an
Dravidian, Celtic). All the corpora involved interactions         example of the statistics that they collect.
between a target child and at least one adult that were
collected from multiple recordings over several months or         Lexstat Learner: A simple categorization algorithm for
years. For each corpus, the child utterances were the             words is one that simply treats all words as separate
target child utterances for that corpus, and the adult            categories (each category has one member). The Lexstat
utterances were all other utterances.                             learner exemplifies this strategy by collecting statistics
                                                                  using the words themselves as the category. For example,
 Syntax Learners: Category-based algorithms                       the word “ate” would be a member of the ate category.
In this section, we will compare how well several
different algorithms for learning distributional categories       Prevword Learner: Many computational linguistic
work in predicting the utterances in our fourteen corpora.        approaches make use of the preceding word as context for
WOPA evaluation depends on statistics collected from              classifying the next word (Redington, Chater, & Finch,
corpora. We will first describe the motivations for the           1998). In our version of this learner, each word is
kinds of statistics that will be collected. Second, we will       categorized based on the most frequent previous word. If
describe the different algorithms for creating the                the most frequent previous word is “you”, then “ate”
categories that the statistics are collected on (Lexstat,         would be classified as a member of the YOU category.
Prevword, Freqframe, Token/Type, Type/Token). In
addition, we will present a Chance learner that gives us a        Freqframe Learner: Mintz (2003) proposed that a frame
baseline level of performance. Then we will give an               made up of one word before and one word after was a
example of the statistics that are collected for a particular     better way to classify words into categories like nouns
sentence, and also an example of how these statistics are         and verbs. For our version of this learner, each word was
used in production of a test sentence.                            classified by the most frequent frame that surrounded it.
   Our algorithms use statistics that are based on a dual-
pathway model of sentence production (Chang, 2002;                Token/Type Learner and Type/Token Learner: The
Chang, Dell, & Bock, 2006). In this Dual-path model,              slots in the Freqframe algorithm differ in their lexical
there are two pathways to guide word sequences. One               diversity. Some slots seem to have a wide range of
pathway was called the sequencing system (using a                 members, and some have a relatively small set. One way
simple recurrent network architecture, Elman, 1990), and          to measure lexical diversity is with the ratio of unique
this system learned how previous words constrained the            word types to the number of token words. So it might be
next word in the sequences. The second pathway was                useful to pick frames based on their lexical diversity of
called the meaning system, and it had a representation of         their slots. However, it is not yet clear whether we should
the message that was to be produced. The meaning                  prefer slots with a large lexical diversity or a small lexical
system was not able to encode sequencing information,             diversity. A large lexical diversity may be evidence of a
                                                              155

general categorizer, or it might simply be a frame which         frequency elements equivalent to the ordering statistics
accepts many different categories. Likewise, a frame with        for high frequency elements.
a small lexical diversity might be a selective categorizer,         Before describing how these statistics are used, we
but it might also be a frame whose behavior is dominated         should first introduce the last learner, which is the Chance
by a few idiosyncratic members. To test both of these            learner. The Chance learner just estimates the probability
possibilities, two learners were created, one which              of getting the sentence right by randomly generating an
categorizes words with the frame that has the highest            order. For example, a two word utterances has only two
lexical diversity (Type/Token Learner) and one which             orders and therefore a 50% chance of getting the order
uses the frame with the lowest diversity (Token/Type             right by guessing. A three word utterances has six orders
Learner).                                                        and 16.7% chance of getting the right order. Chance
  Now that the five categorization algorithms have been          sentence accuracy is then simply a function of the length
described, we can examine how the context and access             of the utterance (% correct = 100/n! ). If a word occurs
statistics would be collected for these different learners.      more than once in an utterance, it is give a unique label
To make this concrete, let us work through how the               (e.g., “the-1 boy saw the girl”), but either “the” and “the-
statistics for “ate” would be calculated in the sentence         1” are consider correct when “the” is expected. Hence the
“We ate the cake” (Table 1).                                     Chance learner is a bit lower than the actual chance level.
                                                                    The creation of the categories and the collection of the
Table 1: Example of statistics incremented for word “ate”.       statistics encompass the creation of the learner. Next, we
Capitalized Words are categories.                                need to test these learners and evaluate the results using
Learner         Context Statistic       Access Statistics        WOPA. To give an example of this, we will work
Lexstat         we -> ate               ate > the                through the utterance “Do you want to throw something
                                        ate > cake               in the rubbish?” (Brian 3;5), which was correctly
Prevword        we -> YOU               YOU > the                predicted by the Lexstat learner and which has never been
                                        YOU > cake               produced by the adults in the corpus. Initially, the
Freqframe       we -> YOU_IT            YOU _IT > the            algorithm starts with the candidate set (“do”, ”in”,
                                        YOU _IT > cake           ”rubbish”, ”something”, “the”, “throw”, “to”, “want”,
Token/Type we -> SHE_IT                 SHE_IT > the             “you”) and uses the punctuation as the first previous word
                                        SHE_IT > cake            (question mark in this case). Since sentences tend to start
Type/Token we -> YOU_IT                 YOU _IT > the            with words that are related to their purpose (e.g., English
                                        YOU _IT > cake           questions tend to start with question words or verbs, while
                                                                 statements tend to start with pronouns or determiners),
In the Lexstat Learner, the “we” to “ate” context statistic      using the punctuation as the first previous word allows us
and the “ate” before “the” and the “ate” before “cake”           to captures this relationship. Each of the candidate words
access statistic would be incremented. In the Prevword           has a choice value, and the word with the highest choice
approach, if the most frequent word preceding “ate” in the       value is selected at each point in a sentence. The choice
corpus is “you”, then “ate” would be in the YOU category         function for each word in the candidate set is incremented
and the same statistics would be collected except that the       by the context statistic from the previous word. For
YOU category would replace the “ate” category. In the            example, English input to children has many questions
Freqframe approach, if the most frequent frame that “ate”        that start with “do”, so the context statistic from “?” to
occurs in is “you ate it”, then “ate” would be classified as     “do” will be strong, and that will increase the choice
an YOU_IT category, and corresponding statistics would           value for “do” at the beginning of this specific novel
be collected. The categories in the Type/Token and               utterance. The choice function for each word in the
Token/Type Learners would depend on the number of                candidate set is also augmented by the access statistics
members in each frame. Lets say that “ate” occurs also           between this word and all the other candidate words. For
between “she” and “it”, and the SHE_IT category has a            example, “want” and “throw” both appear after pronouns
frequency of 4 and 2 unique members and the YOU_IT               like “you” in the input to the child. But when an
has a frequency of 9 and 9 unique members. The                   utterance has both “want” and “throw”, “want” tends to
Type/Token Learner would classify “ate” with YOU_IT              precede “throw” (as in the mother’s utterance “I didn’t
(9/9 > 2/4), while the Token/Type Learner would classify         want you to throw the string.”). If “want” precedes “to”,
with SHE_IT (4/2 > 9/9).                                         “throw”, “something”, “in”, “the”, and “rubbish” more
   Since context and access statistics are just counts of        than “throw” precedes “to”, “want”, “something”, “in”,
how often the word and a category appear together in a           “the”, and “rubbish”, then the access statistics will
particular order, the counts can vary greatly due to the         increase the value of the choice function for “want”.
frequency of two elements involved. To equate for this,          Since the choice value of a particular word is changed by
we divide the context and access statistics by a count of        access statistics from all the words in the candidate set,
how often two elements both occur in the same utterance.         the context statistic from the previous word is multiplied
This helps to make the ordering statistics for low
                                                             156

by the length of the candidate set before being added to          typologically-different languages as our population,
the choice value.                                                 differences between our learners will generalize to other
   Our goal here is to see which of these proposed theories       languages that come from that population.
of category formation are best able to learn constraints             Fig 1 shows that our learners were able to predict the
that are implicit in the utterances in our corpora. The first     order of words in these typologically-different languages.
test will be a situation which we called self-prediction          Using a learner that categorizes words using the previous
where the input and the output for the algorithm was the          word (Prevword) yields a 28% improvement over what
same. This allows us to see to what extent our particular         would expected by chance (t(13) = 11.37, p < 0.0001).
learners can account for the data under ideal input               Restricting the categories with the following word
conditions. Self-prediction gives us a view of how                (Freqframe) increases the prediction accuracy by 10%
consistent a corpus is with itself. For example, if a corpus      (t(13) = 7.38, p < 0.0001). Both of these algorithms pick
has only two sentences: “it is here” and “here it is”, then       the most frequent categorizers, but if we divide by the
the statistics in these two sentences are not going to be         number of unique words in that frame and pick the frame
able to predict whether “here” goes before or after “it is”.      with the best ratio (Token/Type), we get a further 8%
Hence, self-prediction with this corpus will be at 50%            improvement (t(13) = 12,19, p < 0.0001).                  The
(one of these two sentences will be incorrectly predicted).       Token/Type Learner prefers frames which have few
Notice that this is higher than chance (100/3! =16.7%),           members, but are highly frequent. But an even higher
since the order for the words "it" and "is" is consistent         improvement (5%) is reached if we pick frames with a
with this corpus. If the orders in the corpora are                high Type/Token ratio (t(13) = 8.0, p < 0.0001). Finally,
consistent for each set of words and predictable with our         a learner that just uses lexical-lexical statistics has the
learners, then we should expect that the self-prediction          highest accuracy (71%) over all the category-based
accuracy would be higher than the Chance learner.                 learners (7% higher than the Type/Token learner, t(13) =
   To test whether the models differ from each other, t-          12.24, p < 0.0001).
tests were performed treating our fourteen corpora as the            Another way to evaluate these algorithms is by
population. These t-tests tell us how likely we would see         examining how well the adult input can be used to predict
a difference between these learners if we selected a              the child’s output (Fig. 2). This is a stricter test, because
random corpus from the same population. By using                  there are words and utterances in the child’s speech which
                                                              157

are not present in the adult speech, and hence this tests the     child from this age range and from a language with
systems’ ability to generalize to novel sentences.                features that are similar to our typologically-diverse
   The average adult-child results suggest that rank order        sample.
of learners is the same, even when tested on child                   The Prevword and Frequent-frame learners had the
utterances. But some of the adult differences between             same slope (t(2344) = 0.03, p = .97). And although the
learners are no longer significant.           Prevword and        intercept for the Token/Type is higher than the Frequent-
Freqframe Learners are the same (t(13) = 1.31, p = 0.21).         frame, the slope is the same (t(2344) = 0.80, p = 0.43).
Token/Type Learner is still better than the Freqframe             The Type/Token Learner has a more positive slope than
Learner (t(13) = 3.65, p = 0.003). The Type/Token                 the Token/Type Learner (t(2344) = 2.0, p = 0.045) and
Learner is no different than the Token/Type Learner               was not different from the Lexstat Learner (t(2344) =
(t(13) = 1.72, p = 0.11). And the Lexstat Learner remains         0.82, p = 0.41). This suggests that the Type/Token and
the better than the Type/Token Learner (t(13) = 3.03, p =         Lexstat learners are best able to account for the more
0.01). So it would seem that taking lexically-specific            complex utterances later in development, where the
information into account, either in the ratio or in the           accuracy results for five algorithms diverge.
statistics, is what yields improvement in the learners.              So in general, what do these results says about the
   Another way to compare learners to see which learner           match between these algorithms and child data? It is clear
best matches the child’s syntactic development. To                that category and statistics in the Prevword and Freqframe
examine this, we calculated the prediction accuracy of all        learners are useful in characterizing the orders in child
five learners for each day in each corpus (Fig. 3). Each of       speech, but at the same time, the tendency of these
the corpora is collected at different frequencies (daily,         algorithms to discover broad categories (e.g., like nouns
monthly) and at different periods in each child’s life            and verbs) makes it hard to order members of the same
between 1 and 5 years. For each learner, we estimate a            category relative to one another. The Token/Type and
linear regression that attempts to predict the accuracy           Frequent Frame algorithms both depend on high token
level given the age of the child in days (computed with           frequency, but by dividing by the number of unique types,
the age in days approximation: years * 365 + months * 31          the Token/Type learner is able to yield more specific
+ days). The slope of the regression tells us how                 categories, which seems to increase the match with what
consistent the algorithm is at predicting the utterances          children are producing. The Type/Token is a quite
over development. Since the utterances that children              different algorithm from the Token/Type, as it prefers
produce are becoming more syntactically complex over              frames with a high lexical diversity in the slot. This
time, the slope is typically negative, since the systems can      preference actually makes the Type/Token learner more
usually predict simpler utterances better than longer and         like the Lexstat learner. This is because frames with a
more complex utterances. The slope is independent from            single word member (like lexical items in the Lexstat
overall accuracy, because it is possible to have an               Learner) have a high type/token ratio and therefore are
algorithm which has a high overall accuracy and a low             sometimes selected by the Type/Token learner. Given the
slope, and vice versa. To compare slopes, we use a t-test         results here, we can say that we are better able to
with accuracy on each day over all the corpora as the             characterize the order of words in child and adult speech
sample. This analysis tells us whether we would see a             using more specific categories like words rather than with
difference in these slopes if we were to sample another           broad categories. So although none of the learners has
                                                              158

learned standard linguistic syntactic categories (e.g.,          Chang, F., Dell, G. S., & Bock, J. K. (2006). Becoming
nouns, verbs, adjectives, determiners), the ones that were         syntactic. Psychological Review, 113(2), 234-272.
closest to having these broad categories (e.g., Prevword,        Demuth, K. (1992). Acquisition of Sesotho. In D. Slobin
Freqframe) were less good at producing word order than             (Ed.), The Cross-Linguistic Study of Language
those with non-standard categories (e.g., Lexstat,                 Acquisition (Vol. 3, pp. 557-638). Hillsdale, N.J.:
Type/Token). It maybe the case that combinations of                Lawrence Erlbaum Associates.
broad and specific categories might work better, but more        Elman, J. L. (1990). Finding structure in time. Cognitive
work is needed to specify how this is done.                        Science, 14(2), 179-211.
                                                                 Kovacevic, M. (2003). Acquisition of Croatian in
                       Conclusion                                  crosslinguistic perspective. Zagreb.
Learning to order words is a crucial behavior in language        Lee, T. H. T., Wong, C. H., Leung, S., Man, P., Cheung,
acquisition and constrained word order is one important            A., Szeto, K., et al. (1996). The development of
indicator of internal syntactic constraints. But instead of        grammatical competence in Cantonese-speaking
using word order for evaluation, most computational                children. Hong Kong: Dept. of English, Chinese
systems used abstract categories and structures for
                                                                   University of Hong Kong. (Report of a project funded
evaluation and these measures depend on theoretical
                                                                   by RGC earmarked grant ,1991-1994).
considerations for their validity. Our approach takes
                                                                 Lieven, E., Behrens, H., Speares, J., & Tomasello, M.
advantage of the syntactic constraints on word order and
therefore does not require human-labeled categories or             (2003). Early syntactic creativity: A usage-based
structures for evaluation.       In this work, we have             approach. Journal of Child Language, 30(2), 333-367.
demonstrated that WOPA evaluation measures can be                Miller, M. (1976). Zur Logik der frühkindlichen
used to compare six different learners with child and adult        Sprachentwicklung: Empirische Untersuchungen und
utterances in twelve typologically different languages.            Theoriediskussion. Stuttgart: Klett.
This evaluation measure provides several advantages.             Mintz, T. H. (2003). Frequent frames as a cue for
Instead of optimizing computational linguistics systems            grammatical categories in child directed speech.
for the limited set of languages that are typically studied,       Cognition, 90(1), 91-117.
we can use WOPA to compare systems against                       Miyata, S. (2000). The TAI corpus: Longitudinal speech
typologically different languages, allowing us to work             data of a Japanese boy aged 1;5.20 - 3;1.1. Bulletin of
towards a universal account of syntax acquisition. In              Shukutoku Junior College, 39, 77-85.
addition, because WOPA is compatible between                     Narasimhan, R. (1981). Modeling language behavior.
connectionist and non-connectionist approaches to                  Berlin: Springer.
language, it provides a way to combine and integrate             Pinker, S. (1984). Language learnability and language
these computational approaches. Finally, WOPA works                development. Cambridge, MA: Harvard University
on child utterances, which are rarely tested by                    Press.
computational systems, because they are difficult to tag         Redington, M., Chater, N., & Finch, S. (1998).
with syntactic representations. Since children are the only        Distributional information: A powerful cue for
known systems that can learn the syntax of any human               acquiring syntactic categories. Cognitive Science, 22(4),
language, it seems wise to use their utterances to help            425-469.
evaluate syntax acquisition algorithms.
                                                                 Réger, Z. (1986). The functions of imitation in child
                                                                   language. Applied Psycholinguistics. , 7(4), 323-352.
                        References                               Suppes, P., Smith, R., & Leveillé, M. (1973). The French
Abbot-Smith, K., & Behrens, H. (in press). How known               syntax of a child’s noun phrases. Archives de
  constructions influence the acquisition of new                   Psychologie, 42, 207–269.
  constructions: The german periphrastic passive and             Theakston, A., Lieven, E., Pine, J., & Rowland, C.
  future constructions. Cognitive Science.                         (2001). The role of performance limitations in the
Aldridge, M., Borsley, R. D., Clack, S., Creunant, G., &           acquisition of verb-argument structure: An alternative
  Jones, B. M. (1998). The acquisition of noun phrases in          account. Journal of Child Language, 28(1), 127-152.
  Welsh. In        Language      acquisition: Knowledge          Tomasello, M. (2003). Constructing a language: A usage-
  representation and processing. Proceedings of GALA               based theory of language acquisition. Cambridge, MA:
  '97. Edinburgh: University of Edinburgh Press.                   Harvard University Press.
Berman, R. A. (1990). Acquiring an (S)VO language:               Vihman, M. M., & Vija, M. (in press). The acquisition of
  Subjectless sentences in children's Hebrew. Linguistics,         verbal inflection in Estonian: Two case studies. In N.
  28, 1135-1166.                                                   Gagarina & I. Gluzow (Eds.), Verb Grammar in the
Chang, F. (2002). Symbolically speaking: A connectionist           Early Stages of Language Acquisition (Vol. 1-22).
  model of sentence production. Cognitive Science, 26(5),          Dordrecht: Kluwer.
  609-651.
                                                             159

