UC Merced
Proceedings of the Annual Meeting of the Cognitive Science Society
Title
Does Writing Improve as a Function of Number of Reviewers?
Permalink
https://escholarship.org/uc/item/82m8t5bs
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Cho, Kwangsu
Schunn, Christian D.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                      Powered by the California Digital Library
                                                                        University of California

                                                                                                                                        1/
                                 Does Writing Improve as a Function of Number of Reviewers?
                      Kwangsu Cho (KWANGSU@Pitt.Edu) Christian Schunn (SCHUNN@Pitt.Edu)
                                School of Information Science            Learning Research & Development Center
                            University of Missouri, Columbia             University of Pittsburgh
                            Abstract                                    resources (Cho & Schunn, in press). Large numbers of
                                                                        reviewers provide more information about writers’ problems
   Evaluation management systems, especially reciprocal peer            (Wittenbaum & Stasser, 1996). Also, participants generate
   evaluation systems, often have an assumption that more               as well as receive evaluations, which may help participants
   reviewers will produce better results. This tendency is labeled
                                                                        actively reflect upon their own performance as well as that
   as the maxima strategy. This study examines the maxima
   strategy from both agreement and performance perspectives            of others (Schriver, 1990). They develop crucial evaluation
   with the intent of examining the role of information in              skills applied to their professions (Oldfield & MacAlphine,
   reliability and performance through an optimal number of             1995) and in the process dispel negative connotations about
   peer reviewers per evaluate in writing. It was found that the        evaluation. In addition, their participation motivates them to
   maxima strategy works consistently with agreement                    engage more fully with their tasks (Michaelson & Black,
   perspectives, whilst the relationship between the maxima             1984).
   strategy and performance improvement follows an inverted              Despite advantages, three major concerns discourage using
   U-shaped function. Accordingly, we recommend that the                RPE systems in practice: reliability, outliers, and
   number of reviewers needs to be decided on the optimal
                                                                        performance. The reliability and outlier concerns are
   balance between reliability and performance, which
   maximizes writers’ performances without sacrificing                  addressed from the assessment perspective, while the
   evaluation reliability.                                              performance concern is addressed from the learning and
                                                                        performance perspective. Interestingly, all three concerns
Consistent with a recent movement of integrating evaluation             are typically addressed by using the maxima strategy in
assessment and training (Dochy, Segers, & Sluijsmans,                   RPE systems.
1999) with evaluation management systems, reciprocal peer                                  Reliability Perspective
evaluation (RPE) and its distinction of relying upon multiple             Reliability is often measured as consistency which
reviewers has gained popularity throughout education and                concerns the degree to which different peer reviewers
training (Magin, 2001) and in numerous organizations                    generate consistent evaluations on the same tasks. Various
(Harris & Schubroeck, 1988; Illgen, 1999; Katzenbach &                  studies reported medium or low reliabilities among peer
Smith, 1993). According to the American Society for                     reviewers (e.g. Mowl & Pain, 1995), while some studies
Training and Development (ASTD), for instance, about                    report high reliability (e.g. Hughes & Large, 1993). What
33% of firms utilized RPE systems in 1999, an increase                  these studies claim to measure as reliability is actually mean
from only 10% in 1997 (ASTD, 1999). Unlike typical                      reliability, defined as expected reliability of an individual
expert-based evaluation management systems where                        reviewer (Rosenthal & Rosnow, 1991). For example, when
participants receive evaluations only from experts,                     the mean reliability between two reviewers is .4, it indicates
participants in RPE systems maximize resources by playing               the expected reliability of either single reviewer, not that of
dual roles: reviewer and writer. As a reviewer, each                    combined reviewers. Therefore, what this study needs to
participant provides peer feedback. As a writer, each                   know is the aggregate reliability of the total reviewers,
receives feedback from peers. Thus, RPE systems allow                   known as effective reliability (Rosenthal & Rosnow, 1991).
participants to construct as well as receive evaluations.               Effective reliability provides the measure of composite
  In this study, we examine from assessment perspectives                internal consistency of combined reviewers. To compute
and also from performance perspectives the optimal number               this reliability, we use the following formula adapted from
of peer reviewers for effective evaluation management in                the Spearman-Brown formula (Rosenthal & Rosnow, 1991):
RPE systems. Considering that a primary advantage of RPE                                                  nr
systems is providing multiple peer reviewers and hence                                           R=
                                                                                                     1 + (n − 1)r
more feedback, deducing the optimal number of reviewers
                                                                        where R is the effective reliability coefficient, n is the
warrants examination. However, few empirical studies have
                                                                        number of reviewers, and r is the mean reliability among
systematically examined this issue. Therefore, the question
                                                                        reviewers. Therefore, effective reliability will demonstrate
of optimal number of reviewers is still open to examination.
                                                                        an increasing asymptote function as the number of
  In this situation, prevalently accepted is what has come to
                                                                        reviewers increases (see Figure 1a).
be known as the maxima strategy, meaning the implicit
                                                                          Another issue discouraging RPE system use is outliers
assumption that more reviewers will produce better results.
                                                                        due to evaluation biases, particularly when participants’
The maxima strategy provides RPE systems with various
                                                                        identities are disclosed. Biases cause unfair peer evaluations
advantages not afforded by traditional expert-based
                                                                        (Bence & Oppenheim, 2004; Michaelson & Black, 1994) to
evaluation management systems. For example, RPE system
                                                                        which writers are generally sensitized (Michaelson & Black,
participants receive rich feedback without sacrificing expert
                                                                   1109

                                                                                                                              2/6
1994). Peer evaluations are biased by factors such as              being the more helpful (Finn, 1997), many organizations
writers’ gender (Falchikov & Magin, 1997), personal                have increased the number of reviewers and, hence, the
knowledge (Cooper, 1981), and appearances (Oppler,                 volume of feedback provided to employees (Bratton &
Campbell, Pulakos, & Borman, 1992), as well as their               Gold, 2003).
relationship with the reviewer (Kingstrom & Mainstone,                Concerning the impact of the maxima strategy on
1985). These concerns can be addressed mainly by utilizing         performance, different predictions are made by different
anonymity among participants; but the most effective               theories. Herein we discuss predictions based on theories of
remedy to bias is instituting the maxima strategy (Rosenthal       detection, reinforcement, threshold, and cognitive overload.
& Rosnow, 1991).                                                   According to detection theory, more reviewers tend to find
   A related problem on the boundary between agreement             more problems (Borman, 1974). Assuming that writers
and performance involves false outliers. Evaluation in many        improve performance by fixing problems in their task, they
tasks has a component that is legitimately subjective: the         need to detect and fix as many problems as possible. Hence,
object being evaluated triggers different problems for             employing the maxima strategy should function most
different audiences. In any case, some degree of variability       effectively by identifying a greater number of problems to
in evaluations will occur even among generally accurate            be resolved. However, due to the potentially limited number
reviewers. Among a small number of reviewers, there is a           of problems to be found, a linear relationship between the
reasonable chance that one of the reasonable evaluations           number of reviewers and the number of detected problems is
will appear (but falsely) as an outlier, which will cause the      not expected. Thus, the number of detected problems will
writer to ignore (but erroneously) the evaluation. Consider        increase only up to a certain number of reviewers, after
the case in which a writer receives five evaluations rated as      which diminishing returns will indicate an expiration of
follows: 4, 4, 5, 6, and 7. Here there are no outliers since the   newfound problems. Therefore, this theory predicts a curve
central tendency of 5.2 is well supported by all points and        increasing to an asymptote (see Figure 1c).
their general variability. Suppose, however, the writer only          Reinforcement theory (Annett, 1969; Deterline, 1964)
received three of those five evaluations: 4, 6, and 7. The         focuses on the redundancy of problems detected among
mean of 5.7 is still very close to the original mean of 5.2,       reviewers. By contrast to the detection theory, this theory
but the writer may now consider the 4 point as an outlier          emphasizes that writers improve performance by focusing
and thus hasten to a falsely positive interpretation of the        efforts only on problems recurrently mentioned by
feedback. However, occurrence of false outliers is bound to        reviewers, thus in part by filtering out incorrect or
decrease with an increase of reviewers (see Figure 1b).            inappropriate idiosyncratic feedback and in part by shifting
                 Performance Perspective                           attention to the most problematic feedback (Annett, 1969;
   Advocating peer evaluation as a means to improve peer           Anderson, 1982). Therefore, it is expected that the number
performance constitutes the mainstay of the current                of problems found in multiple evaluations grows as a
evaluation system movement to integrate assessment with            function of the number of reviewers and that at a certain
training. Research indicates that peer evaluation (compared        point, the rate of change in the number of recurrent
to expert evaluation) may equivalently or superiorly               problems could be diminishing or asymptotic (see Figure
influence peer performance (Cho & Schunn, in press; Hinds,         1d).
Patterson, & Pfeffer, 2001; Hughes & Large, 1993). For                Threshold theory (e.g. Bernardin & Beatty, 1984)
example, when Hinds et al. (2001) asked domain experts             assumes that more reviewers impose more difficult
and novices to instruct novices (junior and senior                 evaluations for writers to satisfy. Thus, writers may improve
humanities majors) on electronic-circuit activity, the novice-     performance to the degree that they satisfy concerns set by
instructed students showed fewer errors than did the expert-       all reviewers. In this sense, the maxima strategy plays a role
instructed students. It seems that peer writers benefit from       of validity. For example, tasks examined by a greater
common ground or mutual knowledge (Clark & Brennan,                number of reviewers are considered of higher quality than
1991) constructed at a similar knowledge level between peer        those examined by a lesser number of reviewers. Yet it is
reviewers and writers (Damon & Phelps, 1989; Rogoff,               simply harder to satisfy all of the reviewers, and there might
1998).                                                             be a limit to the number of reviewers that could be
   It is commonly assumed that the maxima strategy as a            successfully satisfied. Hence, it is predicted that the maxima
proxy of diversity control could augment this favorable            strategy will show a decreasing function of performance
effect. Consistently, it was found that peer writers benefit       across reviewers (see Figure 1e).
from more feedback (Cho & Schunn, in press) that includes             Finally, cognitive overload theory (Sweller, 1998)
frequent exposure to common ideas across evaluations,              suggests that performance follows an inverted-U shape as a
different perspectives across evaluations (Brinko, 1993),          function of the number of reviewers (see Figure 1f).
and more cognitive conflicts across evaluations (Cohen,            According to the theory, writers should process given
1994). In organization contexts, having learned of the             evaluations in working memory. But because working
positive correlation between amount of feedback and                memory is limited in capacity and duration (Baddley, 2002),
employee performance (MacDonald, Mullin, & Wilder,                 only successfully processed evaluation feedback will be
2003) and of peer writers’ perception of more feedback             retained in writers’ long-term memory, which is unlimited
                                                              1110

                                                                                                                          3/6
                               (a) Reliability      (b) Bias
                 Agreement
                               (c) Detection        (d) Reinforcement    (e) Threshold        (f) Overload
                 Performance
                               Number of reviewers (+)
                  Figure 1. Theoretical predictions
in capacity and very organized using schemata. Faced with         As a reviewer, each participant evaluated six documents in
excessive information, novices or those who did not yet           both their first and final versions. A total of 2,490
develop schemata will experience controlled processing of         evaluations by 248 students on 496 documents were
information in the working memory (Schneider & Schiffrin,         analyzed. Because we controlled only the number of writers
1977; Schiffrin & Schneider, 1977). Thus when working             per reviewer but the number of reviewers per writer was
memory load is kept minimal, optimal learning and                 randomly assigned, participants received evaluations from
performance may occur because unused working memory               different numbers of peer reviewers. Among the 248
capacity can support the learning process in long-term            participants, three participants received peer evaluations
memory. By contrast, when mental workload exceeds                 from two peers, 25 from three, 90 from four, 51 from five,
working memory capacity, learning and performance can be          27 from six, 29 from seven, and 23 from eight. Reviewer-
undermined because evaluation information is not properly         writer pairings were assigned randomly and blindly. All
processed (Baddeley, 2002; Mayer & Moreno, 2003).                 participants used the SWoRD system, a web-based
Therefore, writers’ performance will improve only when            reciprocal peer evaluation system (Cho & Schunn, in press),
assessed by a limited number of reviewers that once               described in the Interface and Procedure section.
exceeded, risks cognitive overload and impairs learning and       Document task. The task assigned to participants was to
performance.                                                      write a document within a content area of cognitive
   In sum, there are inconsistent predictions about how the       psychology. Participants received various writing topics,
number of reviewers will influence writer performance. The        which they tailored to their content areas. Average
goal of this study is to determine the optimal number of peer     document size was 5.89 pages (SD = 1.6), 18.5 paragraphs
reviewers to address reliability and performance concerns in      (SD = 12.3), and 1,446.8 words (SD = 406.9). No
RPE systems. As shown in Figure 1, reliability theories           differences were found across content areas.
support the maxima strategy. Thus, it is expected that more       The participants evaluated each draft along three
reviewers improve reliability and outlier concerns. By            dimensions: flow, logic, and insight. Consequently, writing
contrast, performance theories predict different patterns of      quality was defined as the average of the three dimensions.
performance. The detection and reinforcement theories             For guidance, participants received instruction on important
support maxima strategy use, whereas the threshold and the        features of effective evaluation in each of the dimensions.
cognitive overload theories caution against maxima strategy       The flow dimension, the most basic level, considered the
abuse. Therefore, this study examines the impact of the           extent to which a document involved a lack of faults or
number of peer reviewers on agreement and performance             problems in prose flow. The logic dimension examined the
concerns to find an optimal number of peer reviewers in           extent to which a document was structurally coherent in
RPE systems. It is important to note that the optimal number      terms of the organization of facts and arguments. The
may vary with setting. This study focuses on the case of          insight dimension accounted for the extent to which a
peers with relatively low domain knowledge and medium             document contributed new knowledge to the content area.
task ability because that core is very common in training         Reviewers assessed each document and both qualitatively
situations.                                                       and quantitatively in each of the three dimensions. They
                                                                  generated both written comments and numeric ratings on a
                           Method                                 seven-point scale from disastrous (1) to excellent (7).
Participants. Participants included 248 undergraduate             Interface and Procedure. In the evaluation process, all
students from three cognitive psychology courses for non-         participants used the SWoRD system (Cho & Schunn, in
majors at the University of Pittsburgh. They participated in      press). Writers electronically submitted their documents to
RPE activities for their course credits. Each participant         SWoRD, which distributed the documents to randomly
played the dual role of reviewer and writer. As an writer,        selected sets of peer reviewers. Reviewers reviewed the
each participant wrote a document, received evaluation            documents, generated written comments and numeric
feedback, and revised the document based on the feedback.         ratings, and submitted the results to the system. Having
                                                             1111

                                                                                                                               4/6
received the results from the system, writers revised their       expected, Figure 3 shows that the occurrences of outliers
documents accordingly, submitted revisions, and in turn           decrease as the number of reviewers increases.
back-evaluated the reviewers’ feedback in terms of the
feedback’s effectiveness according to a 5-point scale from               2
not helpful at all (1) to very helpful (5). The process
involved a second round: reviewers reviewing the revisions
                                                                       1.5
and submitting another set of comments and ratings, writers
receiving and back-evaluating the second round of
feedback. All proceedings were conducted anonymously.                    1
                             Results
                                                                       0.5
Reliability                                                                      y = 1.97e
                                                                                    2
                                                                                           -0.39x
                                                                                   R = 0.90
   In order to determine the optimal number of reviewers in
terms of reliability, a mean reliability among individual                0
                                                                             3           4        5             6  7   8
reviewers was first computed, which registered as .26, p. <                                       No of Evaluators
.05. Using the Spearman-Brown formula, effective
                                                                         Figure 3. Outliers
reliabilities as a function of the number of reviewers were
estimated as shown in Figure 2, supporting the prediction.
                                                                  Results indicate that peer evaluations locate within a certain
        1.00                                                      range of deviations regardless of numbers of reviewers. In
        0.90                                                      other words, outliers among a small number of evaluations
        0.80                                                      would not register as such in a larger context. For example,
        0.70
                                                                  with three reviewers, the differences among evaluations
        0.60
        0.50
                                                                  would be highly contrasted; while with eight reviewers the
        0.40
                                                                  differences among reviewers would blur. To examine this
        0.30                                                      interpretation, a one-way ANOVA with numbers of
        0.20                                                      reviewers as a between-subject variable was performed on
        0.10                                                      standard deviations as a dependent variable: 3 reviewers (M
        0.00
              2   3   4    5   6      7      8 9 10 11 12
                                                                  = 2.24, SD = 1.00), 4 reviewers (M=2.65, SD = 1.17), 5
                              No of Evaluators                    reviewers (M = 2.67, SD = 1.25), 6 reviewers (M = 2.42, SD
                                                                  = 1.13), 7 reviewers (M = 2.55, SD = .88), and 8 reviewers
         Figure 2. Effective reliabilities                        (M = 2.59, SD = .96). No significant difference was found
                                                                  on the size of standard deviations, F (5, 295) = .84, p = .52.
  The rate of true outlier reviewers, defined as outlier found    Therefore, this result supports the hypothesis in that
in the maxima data case (n=8 reviewers), is relatively low        perceived outliers in a smaller number of peer reviewers
(M = .25, SD = .50) or approximately 3% of reviewers.             may not be perceived as outliers in larger numbers of peer
Although it is statistically challenging to detect among          reviewers because they tend to appear within a certain range
smaller numbers of evaluations, the frequency of true             of legal variations.
outliers (t) will be a linear positive function of the number
of reviewers (n), t = .3n . Yet the number of false outliers is
expected to decrease with the number of reviewers.                Performance
   We conducted the Grubbs test (Grubbs & Beck, 1972) on          Concerning the performance predictions, task quality
the data to see how many outliers were found as a function        improvement as a function of the number of reviewers was
of reviewers (see Figure 3). Because the number of detected       computed. The task quality improvement was defined as the
outliers is smaller except for n=7 and 8, we can assume the       difference between the quality of first documents and that of
majority of theses cases are false outliers. The Grubbs test      final documents. Figure 4 shows that the performance
calculates the standardized difference between each               improvement is an inverted U-shape. As the number of
evaluation and the mean of all evaluations. Thus, it is           reviewers increases, performance likewise increases, peaks
computed based on the distance between each rating and all        around six reviewers, and then decreases. Because the trend
ratings divided by their standard deviation. Figure 3 shows       appears consistent with the cognitive overload theory, a
the average number of outliers according to each number of        polynomial of 2nd degree in the number of reviewers, n, was
reviewers. An estimated exponent function computed was            estimated as follows:
 O = 1.97e −.39 n where O is the number of outliers and n is                              I = −.16n 2 +1.84n − 3.43 ------ (1)
the number of reviewers. The function shows an excellent          where I is the performance improvement and n is the
fit, R2 = .90. The differentiation of the function proves a       number of reviewers. Thus, there is either a negative or an
decreasing asymptotic function, ΔΔOn = −.77e −.39 n . As          absence of impact on performance when the amount of
                                                                  feedback is too little or too much. The estimated polynomial
                                                             1112

     3
   2.5
     2                                                                                                                            5/6
   1.5
     1
                                                                         perspectives are jointly considered. As predicted, the
                                                   Improvement
                                                   Poly. (Improvement)   maxima strategy works consistently with agreement
   0.5                                                                   theories. Thus, more reviewers improve reliability and
                                                                         evaluation biases. By contrast, performance improvement
     0
                                                                         follows an inverted U-shaped trajectory as a function of the
  -0.5
                                                                         number of reviewers, as is consistent with the cognitive
         2        3       4          5
                              No of Evaluators
                                               6 7            8
                                                                         overload theory (Sweller, 1998). Unlike a common
                                                                         assumption that more evaluations augment a favorable peer
       Figure 4. Performance improvement                                 evaluation effect, we found that too much feedback may
function shows a good fit, R2 = .83. To find the maximum                 hamper writers’ performance. Consistently, Goodman and
improvement point, (1) was differentiated to calculate the               Wood (2004) found that increasing the amount of specific
rate of changes:                                                         feedback may hurt learning.
                    ΔI = −.32n + 1.84 ------ (2)                           The results of this study have an implication for
                    Δx                                                   evaluation management and training. The maxima strategy
Thus the rate of performance improvement increases until                 using multiple reviewers is considered to generate more
the number of reviewers is 5.8 reviewers (1.84/.32). With                accurate or acceptable evaluations (Latham & Wexley,
more than 5.8 reviewers, the rate of performance                         1982), an assumption grounded in assessment theory. At the
improvement decreases. In addition, the function explains                same time, it is regarded as critical to provide performance
that with 11 or more reviewers, evalautees’ performance                  feedback. Therefore, it seems the maxima strategy is
would suffer rather than improve. Therefore, the function                implicitly accepted as the means of increasing both the
supports the cognitive overload hypothesis.                              reliability of evaluations and the performance of receivers.
    However, the performance decrement after 5.8 reviewers               In addition, the advances of available information
in the inverted-U shaped performance could be the result of              technology greatly enhance RPE efficiency by providing
writers receiving less feedback even with more reviewers                 writers with rich feedback on their performances but also by
(e.g., if reviewers compensated for being in a setting with              facilitating the involvement of greater numbers of
more reviews by giving less feedback per review). Thus,                  reviewers. The results of this study, however, caution about
unlike the assumption that larger numbers of reviewers                   this very abuse of the maxima strategy promoted by
provide more information, the actual amount of evaluation                reliability and performance theories as well as technology:
information could be low (Marwell & Oliver, 1993). To test               too many reviewers may simply overload writers with
this undersupply possibility, we computed the amount of                  information (Jones, Ravidi, & Rafaeli, 2004).
feedback that the participants accepted and processed,                     Of course, there remains much to be improved. First, this
which was measured by the number of characters in the                    study did not focus on the difficulty or complexity of tasks.
written comments that the writers rated as helpful (3 or                 We agree that task characteristics may define various
higher) for revising their documents. Excluded was                       aspects of evaluation and its effectiveness. However, how
feedback rated less helpful (2 or lower). Accepted feedback              specific tasks influence evaluation effectiveness is not yet
(F) increases as a function of number of reviewers (n),                  well understood (Kluger & DeNisi, 1996) and is
 F = 1769.3n − 1072.5 , R2 = .98, p < .001. Consequently, the            recommend for further study.
hypothesis involving decreasing amount of feedback after                   In reporting empirical findings that six reviewers
5.8 reviewers is rejected.                                               constitute an optimal number in a RPE system, this study
  Evaluation Time on Task. The amount of evaluation time                 contributes critical knowledge to research practice on
spent by each reviewer contextualized our argument.                      evaluation feedback. Although many studies show that
Although task evaluation time is not a focus in this paper, it           evaluation feedback improves task performance, a
should be noted that the number of reviewers is frequently               considerable amount of research reports that evaluation
based on the estimated time on task each participant needs               feedback does not improve performance and in fact
or can invest. In our study, the reviewers were asked to                 deteriorates it (see Kluger & DeNisi, 1996). Given mixed
report how much time they spent to do the evaluations. It                results concerning the impact of evaluation feedback, the
was found that reviewers spent averaged 34.0 minutes (SD=                number of reviewers or the amount of feedback could play
17.8) reading each document and 29.6 minutes (SD= 19.0)                  the role of an independent or mediating control variable,
generating each evaluation. Therefore, evaluation time                   making it possible to refine existing theories and develop
averaged about one hour per document and about 12 hours                  new ones.
total evaluating first and final peer documents.
                                                                                                 References
                          Discussion                                     Annett, J. (1969). Feedback and human behavior.
The maxima strategy as a key characteristic in RPE was                     Hammondsworth, England: Penguin.
considered from the reliability and also from the                        Anderson, J .R. (1982). Acquisition of cognitive skill.
performance perspective. This study shows that there are                   Psychological Review, 89, 369-406.
trade-offs between reliability and performance when
                                                                    1113

                                                                                                                         6/6
Baddeley, A. D. (2002). Is working memory still working?          Hughes, I. E., & Large, B. J. (1993). Staff and peer-group
  European Psychologist, 7, 85-97.                                  assessment of oral communication skills. Studies in
Bence, V. & Oppenheim, C. (2004). The influence of peer             Higher Education, 18, 379–385.
  review on the research assessment exercise. Journal of          Jones, Q., Ravidi, G., & Rafaeli, S. (2004). Information
  Information Science, 30 (4), 347-368.                             overload and the message dynamics of online interaction
Bratton, J. & Gold, J. (2003). Human resource management,           spaces: A theoretical model and empirical exploration.
  3rd ed. Palgrave Macmillan Pub.                                   Information Systems Research, 15(2), 194-210.
Brinko, K. T. (1993). The practice of giving feedback to          Katzenbach, J. R. & Smith, D. K. (1993). The wisdom of
  improve teaching: What is effective? Journal of Higher            teams. Cambridge, MA: Harvard Business School Press.
  Education, 64 (5).                                              Kluger & DeNisi, 1996
MacDonald, J. E., Mullin, J., & Wilder, D. A. (2003).             Kingstrom, P. O., & Mainstone, L. E. (1985). An
  Weekly feedback vs. daily feedback: An application in             investigation of the rater-ratee acquaintance and rater
  retail. Journal of Organizational Behavior Management,            bias. Academy of Management Journal, 28(3), 641-653.
  23 (2/3), 21-43.                                                Latham, G. P. & Wexley, K. N. (1982). Increasing
Bernardin, J. H., & Beatty, R. W. (1984). Performance               productivity through performance appraisal. Reading,
  appraisal: Assessing human behavior at work. Boston,              MA: Addison-Wesley.
  MA: Kent-Wadsworth Pub.                                         Magin, D. (2001). Reciprocity as a source of bias in
Borman, W. C. (1974). The rating of indivisuals in                  multiple peer assessment of group work. Studies in
  organizations: An alternative approach. Organizational            Higher Education, 26(1), 53-63.
  Behavior and Human Performance, 12, 105-124.                    Marwell, G., & Oliver, P. (1993). The critical mass in
Cho, K., & Schunn, C. D. (in press). Scaffolded writing and         collective action. New York: Cambridge University Press.
  rewriting in the discipline. Computers & Education.             Mayer, R. E., & Moreno, R. (2003). Nine ways to reduce
Clark, H. H., & Brennan, S. E. (1991). Grounding in                 cognitive load in multimedia learning. Educational
  communication. In L. B. Resnick, J. M. Levine, & S. D.            Psychologist, 38, 43-52.
  Teasley (Eds.), Perspectives on socially shared cognition.      Michaelson, L. K. & Black, R.H. (1994). The key to
  Washington, DC: American Psychological Association.               harnessing the power of small groups I: Higher education
Cohen, E. (1994). Designing groupwork: Strategies for the           building learning teams. Growth Partners p. 14.
  heterogeneous classroom. NY: Teachers Collage Press.            Mowl, G., & Pain, R. (1995). Using self and peer
Cooper, W. H. (1981). Ubiquitous halo. Psychological                assessment to improve students’ essay writing: A case-
  Bulletin, 90, 218-244.                                            study from geography. Innovations in Education and
Damon, W., & Phelps, E. (1989). Critical distinctions               Training International, 32, 324-335.
  among three approaches to peer education. International         Rogoff, B. (1998). Cognition as a collaborative process. In
  Journal of Educational Research, 13, 9-20.                        W. Damon (Series Ed.) & D. Kuhn & R. S. Siegler (Vol.
Deterline, W.A. (1962). An Introduction to Programmed               Eds.), Handbook of child psychology: Vol. 2: Cognition,
  Instruction. New York: Prentice-Hall.                             perception & language. New York: John Wiley & Sons.
Dochy, F., Segers, M., & Sluijsmans, D. (1999). The use of        Rosenthal, R. & Rosnow, R. L. (1991). Essentials of
  self-, peer and co-assessment in higher education: A              behavioral research, 2nd. New York: McGraw Hill.
  review. Studies in Higher Education, 24(3), 331-350.            Schriver, K. A (1990). Evaluating text quality: The
Falchikov, N. & Magin, D. (1997). Detecting gender bias in          continuum from text-focused to reader-focused methods.
  peer marking of students’ group process work.                     Technical Report No. 41. National Center for the Study of
  Assessment and Evaluation in Higher Education, 22,                Writing and Literacy.
  385–396.                                                        Oppler, S. H., Campbell, J. P., Pulakos, E. D., & Borman,
Finn, D. M. (1997). Perceptions of performance feedback             W. C. (1992). Three approaches to the investigation of
  received by female and male managers: A field study.              subgroup bias in performance measurement: Review,
  Dissertation Abstracts International Section A:                   results, and conclusions. Journal of Applied Psychology,
  Humanities & Social Sciences, 57(7-A), Jan 1997, 3119.            77(2), 201-217.
  US: Univ Microfilms International.                              Sweller, J. (1988). Cognitive load during problem solving:
Goodman, J. S., & Wood, R. E. (2004). Feedback                      Effects on learning. Cognitive Science, 12, 257-285
  specificity, learning opportunities, and learning. Journal      Wittenbaum, G. M., & Stasser, G. (1996). Management of
  of Applied Psychology, 89(5), 809-821.                            information in small groups. In J. L. Nye, A. M. Brower
Harris, M. M. & Schubroeck, J. (1988). A meta-analysis of           (eds), What’s social about social cognition? Research on
  self-supervisor, self-peer, and peer-supervisor ratings.          socially shared cognition in small groups. CA: Sage.
  Personnel Psychology, 41, 43-62.
Hinds, P. J., Patterson, M., & Pfeffer, J. (2001). Bothered by
  abstraction: The effect of expertise on knowledge transfer
  and subsequent novice performance. Journal of Applied
  Psychology, 86 (6), 1232-1243.
                                                             1114

