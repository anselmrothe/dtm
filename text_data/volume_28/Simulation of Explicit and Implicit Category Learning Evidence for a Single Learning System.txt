UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Simulation of Explicit and Implicit Category Learning: Evidence for a Single Learning System
Permalink
https://escholarship.org/uc/item/6tq092x4
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Author
Dandurand, Frederic
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                      University of California

                        Simulation of Explicit and Implicit Category Learning:
                                      Evidence for a Single Learning System
                                    Frédéric Dandurand (fdandu@ego.psych.mcgill.ca)
                                            Department of Psychology, McGill University,
                                  1205 Dr.Penfield Avenue, Montreal, Quebec, H3A 1B1, Canada
                            Abstract                                  1. A significant main effect of condition (i.e., processing
                                                                      load) – “… showing that the concurrent task group needed
  Previous experiments (Waldron & Ashby, 2001) showed that            more training to learn the category structures than did the
  category learning was differentially impaired by a concurrent       control group.” (p. 171)
  Stroop task, depending on the type of rule used to categorize
                                                                      2. A significant main effect of rule type – “…showing that,
  items. Learning was more impaired for simple explicit rules
  than for complex implicit rules. The present simulation             over all other conditions, explicit rules required less training
  suggests that the multiple learning systems hypothesized by         than did implicit rules.” (p. 171)
  Waldron and Ashby are not necessary to explain their results        3. A significant interaction between rule type and
  because a single learning system provides a parsimonious            condition – “…showing that the concurrent task produced
  account of the data. In this model, the harder of two               greater interference with explicit rules than with implicit
  concurrent tasks determines learning time. Therefore,               rules.” (p. 171)
  combined task complexity explains why the concurrent                   In addition, Waldron and Ashby found a significant
  Stroop task impairs learning in the explicit group more than in     improvement in performance by the explicit concurrent task
  the implicit group.
                                                                      condition as the experiment progressed. The probable cause
                                                                      is a reduction of concurrent task interference, because the
                        Introduction                                  Stroop effect is known to diminish with training (Stroop,
Categorization is an important cognitive task (e.g., Harnad,          1935; MacLeod, 1991). As a result, the critical Rule Type
2005). It is currently controversial whether categorization is        by Condition interaction found “Early in Session”
a single process, or if there are multiple systems involved           disappears by the end of the experiment (“Late in Session”).
for different kinds of categorization tasks.                          No other differences between Figure 1A and 1B are
  Waldron and Ashby (2001) performed an experiment with               statistically significant.
human participants that seemed to support the hypothesis of              Waldron and Ashby (2001) claimed that the observed rule
multiple learning systems. Participants had to categorize             type by condition interaction supports the existence of
four-dimensional items. Two variables were manipulated.               multiple learning systems because the concurrent task
First, the type of rule determined the difficulty of the              interferes with explicit learning, but not with implicit
categorization task. In the explicit rule condition, items            learning. They further claim that the differential
could be categorized according to a single input dimension.           improvement in explicit learning during the experiment also
In contrast, under the implicit rule condition, three of the          supports multiple systems. Finally, Waldron and Ashby
four input dimensions had to be integrated in order to                argued that, because the secondary (Stroop) task is
determine category membership. They used term explicit                commonly thought to reduce processing resources available
because participants can typically verbalize simple, one-             to the primary (categorization) task, category learning
dimensional rules. By contrast, participants generally cannot         should be more difficult when the Stroop task is
verbalize complex, multi-dimensional implicit rules, even             concurrently performed. Furthermore, complex, implicit
when they perform well on categorization tasks using those            rules require more processing resources than simple, explicit
rules.                                                                rules. Thus, they conclude that in a single learning system,
  Second, Waldron and Ashby manipulated processing                    processing of implicit rules should always be impaired at
load. In a concurrent condition, participants performed a             least as much as that of simple rules when performed
numerical Stroop task while learning to categorize. In a              concurrently with the secondary task. Finally, they say that
control condition, participants only had to learn to                  this prediction is contrary to what was observed: the
categorize.                                                           learning of explicit rules was more impaired under
  Participants were randomly assigned to either the control           concurrent task that the learning of implicit rules.
condition or to the concurrent condition. Each participant               These kinds of arguments based on interactions are
was presented with four categories to learn (two explicit and         commonly taken as evidence for multiple learning systems.
two implicit) in random order. The set of features relevant           An important example of such interactions is double
for determining category membership was selected                       dissociations. However, some simulations suggest that
randomly, and changed for each of the four categories to be            multiple learning systems are not necessary to account for
learned.                                                               them. In fact, Kello, Sibley and Plaut (2005) found that a
  Results (Waldron & Ashby, 2001) are reproduced in                    single connectionist system could model double dissociation
Figure 1. Their statistical analyses of those results showed:          phenomena.
                                                                  1168

  Similarly, this paper challenges Waldron and Ashby’s                 Note. From “The effects of concurrent task interference
claim that the interaction they found is evidence for multiple     on category learning: Evidence for multiple category
learning systems. A simulation of their experiment was             learning systems” by E. M. Waldron and F. G. Ashby, 2001,
performed using Cascade-correlation neural networks                Psychonomic Bulletin & Review, 8 (1), p. 172. Copyright
(Fahlman & Lebiere, 1990). Results suggest that multiple            2001 by The Psychonomic Society. Reprinted with
category learning systems might not be necessary to account         permission.
for their data.
                                                                                               Method
                                                                   In the model, the categorization task has 4 inputs (binary
                                                                   features) and 1 output for a binary category decision (as in
                                                                   Waldron & Ashby, 2001). Like Waldron and Ashby (2001),
                                                                   two kinds of rules determined category membership:
                                                                   explicit rules where a single input feature determined the
                                                                   output (e.g., output is 1 if the second feature is 1), and
                                                                   implicit rules where 3 out of 4 features need to be integrated
                                                                   (output is 1 if any 2 of those 3 features are 1). The choice of
                                                                   input features used for categorization was random and
                                                                   different for each category. An example of implicit
                                                                   categorization task is presented in Table 2.
                                                                       The concurrent task was a Stroop task similar to the one
                                                                   used by Waldron and Ashby (2001). There were four binary
                                                                   input units representing the numerical (N) and physical (P)
                                                                   size of each of two digits (D1 and D2), each coded as 0 for
                                                                   small and 1 for large. A fifth binary input unit coded an
                                                                    instruction (Instr.) to identify the larger digit based on its
                                                                    numerical value (0) or physical size (1). The output was
                                                                    binary coded to identify which digit was larger (0 for digit
                                                                    1, or 1 for digit 2).
                                                                                 Table 1 – Stroop task patterns (N=16)
                                                                                            Inputs                       Output
                                                                       D1-N      D2-N       D1-P      D2-P       Instr.
                                                                         0         1          0         1          1        1
                                                                         1         0          0         1          1        1
                                                                         1         0          1         0          1        0
                                                                         0         1          1         0          1        0
                                                                         0         1          0         1          1        1
                                                                         1         0          0         1          1        1
                                                                         1         0          1         0          1        0
                                                                         0         1          1         0          1        0
                                                                         0         1          0         1          1        1
                                                                         1         0          0         1          1        1
                                                                         1         0          1         0          1        0
                                                                         0         1          1         0          1        0
                                                                         0         1          0         1          1        1
                                                                         1         0          0         1          1        1
                                                                         1         0          1         0          0        0
                                                                         0         1          1         0          0        1
                                                                       In the training set, 14 of the 16 patterns were questions
                                                                   about physical size, while 2 were about numerical size.
                                                                   Networks were presented with patterns in which the
                                                                   physical and numerical sizes were different, representing 4
  Figure 1 – Results from Waldron and Ashby (2001). Note           different combinations: (Small and Large) x (Digit 1 and
that 1D rules were explicit, whereas 3D rules were implicit.
                                                                   Digit 2), therefore some replications of patterns were
                                                               1169

necessary to match the 16 patterns of the categorization            Cascade-Correlation Algorithm
task.                                                               As mentioned, this simulation used the Cascade-correlation
                                                                    (Cascor) algorithm (Fahlman & Lebiere, 1990). Cascor is a
Table 2 – Example of patterns for an implicit task (N=16).          general purpose neural network algorithm that successfully
In this example, the 4th input feature is not used to determine     simulated a range of cognitive tasks including the balance
category membership. Output is 1 if at least two out of the
                                                                    scale task, acquisition of pronouns and learning of distance,
other three features are 1.
                                                                    time, and velocity concepts (Shultz, 2003). As opposed to
                        Inputs                    Output
                                                                    standard      backpropagation      algorithms       in     which
          I1         I2        I3         I4
                                                                    experimenters need to set network structure prior to
          0          0          0          0         0              training, Cascor is a constructive technique where network
          0          0          0          1         0              size expands as needed to solve a task.
          0          0          1          0         0                 Cascor networks begin with input and output units but no
          0          0          1          1         0              hidden unit. Training starts in the output phase during which
          0          1          0          0         0              Cascor minimizes the sum of squared error using some
          0          1          0          1         0              standard learning algorithm like QuickProp (Fahlman,
          0          1          1          0         1              1988):
          0          1          1          1         1
          1          0          0          0         0                  E = ∑∑ (Vo , p − To , p ) 2                           (1)
          1          0          0          1         0                         o  p
          1          0          1          0         1                 where V is the activation of output o for pattern p, and T
          1          0          1          1         1              is the corresponding target value that the network is trying
          1          1          0          0         1              to learn.
          1          1          0          1         1                 If error reduction stagnates before the task is successfully
          1          1          1          0         1              learned, Cascor enters the input phase. In input phase, a set
          1          1          1          1         1              of candidate units compete for recruitment. Those units are
                                                                    typically sigmoids, and each one starts with different
   For the simulations of the concurrent condition, the             random input connection weights. By adjusting those
categorization and concurrent tasks were learned in parallel.       weights, Cascor maximizes the covariance of each candidate
As a straightforward way to model this, inputs and outputs          units’ outputs with the residual network error:
of each task were concatenated, for a total of 9 and 2
respectively. 32 networks were trained in each condition for            S = ∑ ∑ (V p − V )( E p ,o − E o )                    (2)
a total of 128 networks. Networks varied in the values of                     o    p
their initial weights, which were randomly selected.                   where E p ,o is the error at output unit o for pattern p,
Networks trained in the early in session condition were
reused to simulate the late in session condition, after              Eo is the average error at output unit o, V p is the output unit
connection weights were modified to partially reset the
Stroop effect (see Initializing Networks section for details).     activation for pattern p, and V is the average output unit
The goal was to mimic as much as possible the methods              activation.
used in Waldron and Ashby (2001).                                      When covariance maximization stagnates, the unit with
   In the simulations, the Stroop task was construed as a          the highest covariance is selected and connected to the
short-term “on the fly” learning to suppress default               output units, and thus becomes a new hidden unit1. Other
automatic response and to generate the unusual, but correct        units are discarded. Cascor then proceeds with another
answer (evaluate differences in physical size). Because            output phase with the newly recruited unit. Training
connection weights were initialized so that the network            alternates between output and input phases until target level
automatically generates the automatic response (evaluate           of error is reached or training times out.
differences in numerical size), the system had to unlearn this         A major advantage of Cascade-Correlation over standard
automatic response, and instead generate the correct               backpropagation is that determining network topology
response (evaluate differences in physical size). Stronger         becomes part of the learning process, and is therefore
Stroop effects would manifest themselves in longer training        automated. Although not necessarily optimal, network
time.                                                              structures generated using Cascor tend to be relatively small
   Similarly, the categorization rules learned were also short     (Fahlman & Lebiere, 1990).
lived. The system had to constantly learn novel associations
because new combinations of features determining category
membership were randomly selected for each of the four
categories.
                                                                    1
                                                                       Note that Fahlman and Lebiere (1990) found that using
                                                                    covariance (S) worked better than using true correlation in most
                                                                    situations.
                                                               1170

Initializing Networks                                                                                                                          Category Learning Early in Session
As noted, the Stroop effect diminishes with training (Stroop,
                                                                                                                                                                Control   Concurrent
1935; MacLeod, 1991). To simulate this effect, a different
initialization scheme for networks trained to simulate the                                                                                20
early and late conditions was used.
   Networks simulating the early phase were initialized so
                                                                                                                                          15
that numerical size outputs were given large connection
                                                                                                                    Epochs to Criterion
weights between the numerical inputs and the output (called
“numerical-bias weights”). Training thus involved inhibiting                                                                              10
this automatic response (i.e., reducing the numerical-bias
weights) and increasing weights between physical size
                                                                                                                                          5
inputs and the output.
   By contrast, to simulate the late in session condition,
network weights were reset to a value equal to the mean of                                                                                0
early training weights and numerical-bias weights, as                                                                                           Explicit (1D)                          Implicit (3D)
illustrated in figure 2. This reflected the fact that early                                                                                                        Rule Type
training reduces the Stroop effect, but only partially.
To sum up, the experiment had two independent factors: (1)                                                        Figure 3A – Category Learning Early in Session. Error bars
Condition (i.e., processing load) with two levels: Control                                                        represent standard error.
(Categorization only) and Concurrent, and (2) Rule Type
with two levels: Explicit and Implicit. Finally, there was one                                                                                 Category Learning Late in Session
repeated factor, session with two levels: Early and Late.
                                                                                                                                                                Control   Concurrent
                                                                                                                                          20
                                            Weight to numerical output   Weight to Physical output
                              4.5
                                                                                                                                          15
                                                                                                                    Epochs to Criterion
                               4
                              3.5
  Size of connection weight
                                                                                                                                          10
                               3
                              2.5
                                                                                                                                          5
                               2
                              1.5
                               1                                                                                                          0
                                                                                                                                                Explicit (1D)                          Implicit (3D)
                              0.5
                                                                                                                                                                   Rule Type
                               0
                                    Early - before
                                      training
                                                       Early - after
                                                        training
                                                                         Late - before
                                                                           training
                                                                                            Late - after
                                                                                             training
                                                                                                                  Figure 3B – Category Learning Late in Session. Error bars
                                                                                                                  represent standard error.
Figure 2 – Example of network initialization for the Stroop                                                    Statistical Analyses
task. The first set of weights represents initialization of the
network before training. A large weight to the numerical                                                      A mixed ANOVA was performed with Condition and Rule
output (numerical-bias weight) yields the automatic (but                                                      type as independent factors, and Session as a repeated
incorrect) response. After training (second set of weights),                                                  factor. The following statistically significant effects were
the network learned to generate the correct answer, as                                                        found:
shown by a large connection weight to physical output. In                                                     1. Main effect of Condition (processing load): F(1,124) =
the third set, network weights are reset to the average of the                                                     129.8, p < 0.001. Learning was faster in the control
previous two sets of weights (before and after training) to                                                        group than in the concurrent group. A main effect of
partially reset the automatic response. Finally the last set of                                                    Condition was also observed in human data.
weights (similar to the second one) indicates that the                                                        2. Main effect of Rule Type: F(1,234) = 437.8, p < 0.001.
network is generating the correct response again after                                                             Learning was faster for explicit rules than for implicit
training late in session.                                                                                          rules because explicit rules are simpler than implicit
                                                                                                                   ones.
                                                          Results                                             3. Condition by Rule Type interaction: F(1,124) = 77.3,
Simulation results are presented in Figure 3.                                                                      p < 0.001. The model produced the same critical
                                                                                                                   interaction that Waldron and Ashby (2001) found.
                                                                                                           1171

4.   Main effect of Session: F(1,124) = 67.4, p < 0.001.              Note that, from a computational modeling perspective, all
     Training was faster late in session than early, a logical     these tasks are easy to learn using the Cascade Correlation
     consequence of reduction in the Stroop effect due to          learning algorithm. A single output phase was sufficient to
     learning. This trend was present in Waldron and               learn the tasks, and thus no hidden units were recruited,
     Ashby’s work, although it was not significant.                indicating that these tasks are linearly separable. Because
5. Session by Condition interaction: F(1,124) = 47.0,              learning is so fast, differences of a few epochs can be
     p < 0.001. Again, as a result of the reduction of the         relatively important. Consequently, results are sensitive to
     Stroop effect, the concurrent group improved more than        changes in parameter values such as inputs values, learning
     the control group during the late session.                    rates, and score thresholds.
6. Three-way interaction – Condition by Rule Type by                  In Cascade-Correlation networks, interference naturally
     Session: F(1,124) = 10.8, p < 0.001. The group that           occurs in recruited hidden units because all network inputs
     improved the most was the concurrent explicit                 and previously installed units contribute to the weighted
     condition, consistent with Ashby and Waldron (2001).          sum of input used to determine the level of activation of a
   In the simulation, the only effect that was not significant     given unit. As discussed above, although Cascade-
was session by rule type interaction F(1,124) = 2.9,               Correlation can build complex network structures as it
p > 0.09. Thus all significant effects in Waldron and              learns, no hidden units were necessary in this simulation to
Ashby’s data were captured in the model, and it also               succeed at the tasks presented. As a result, the topology for
produces two additional effects: a main effect of Session,         this task is identical to a fully-connected backpropagation
and a Session by Condition interaction.                            network with 4 inputs and 1 output.
   In short, when we compare Figures 1 and 3, we see that             I am currently working on a new model that captures
the pattern of simulation results is very similar to the one in    interference effects using hidden unit recruitment. However,
Waldron and Ashby (2001) except that the F values were             the current model does capture the limited capacity aspect of
generally larger. This is a typical difference between human       task concurrence by virtue of being built out of only 9 input
experiments and simulations because simulations have less          and 2 output units.
error variance.                                                       In short, this single learning system accounts for the
                                                                   pattern of results in Waldron and Ashby (2001) because the
                          Discussion                               harder task determines learning time. Combined task
The model was designed so that the categorization and              complexity (categorization + concurrency) explains why the
concurrent tasks were learned in parallel. As a result, the        Stroop task impairs the explicit task more than the implicit,
number of epochs to criterion (success) was determined by          and why the explicit concurrent group improves the most
the more complex task of the two.                                  with training.
   Early in the session, the implicit and the Stroop task have
about the same complexity. This explains why three of the          Concerns about Waldron and Ashby’s study (2001)
four groups cluster around 15 to 17 epochs, while the              Waldron and Ashby (2001) chose the numerical Stroop task
system learns the easier explicit control task in about 7          because “Recent neuroimaging studies have shown that the
epochs.                                                            anterior cingulate and dorsolateral prefrontal cortex are
   When the networks were retrained to simulate the late-in-       strongly activated in the Stroop (1935) task (Bench et al.,
session situation, categorization tasks remained as difficult      1993)” (p. 170). The same brain regions are active when an
as before because the content of the categories kept               explicit rule is learned, but not when an implicit rule is
changing. However, the Stroop task was easier to learn             learned.
because the weight initialization included a portion of the           Although this is a compelling reason for this choice,
weights previously trained. Actually, by varying the               Waldron and Ashby (2001) did not control for task
proportion of trained vs. numerical-bias weights, the              concurrence. Their hypothesis was that the Stroop task
difficulty of the task can be varied from about zero (by           would interfere with the explicit, verbally-driven learning
taking 100% of trained weights, and 0% of numerical-bias           system causing more performance impairment to the
weights) to as hard as at the beginning (by taking 0% of           explicit rule learning. However, perhaps concurrence by
trained weights, and 100% of numerical-bias weights).              itself is sufficient to account for the pattern of data. Perhaps
Empirically, a 50% weighing resulted in a suitable level of        similar experiments should be performed involving other
difficulty.                                                        concurrent tasks varying in complexity and difficulty
   In the simulation, changes in the difficulty of the Stroop      including some known not to activate the anterior cingulate
task only affected the explicit concurrent condition. In fact,     and the dorsolateral prefrontal cortex.
learning in the implicit concurrent condition was unchanged           Furthermore, Waldron and Ashby (2001) did not test the
because the implicit task remained at the same difficulty          verbalizability of their rules. More specifically, after
level. This generally explains how the model captured              participants reached success criterion they should be asked
differential performance impairment in rule learning in            what rule they are using to classify elements to verify the
presence of the concurrent Stroop task, and the various             explicit/implicit nature of the rules.
statistical interactions observed.
                                                               1172

   Under a multiple learning system model, learning using                              Acknowledgments
the explicit learning system would be impaired with the            This work was supported by the Lloyd Carr-Harris McGill
concurrent task. Because learning of one-dimensional rules         Major Fellowship.
still occurs under Stroop task concurrence, perhaps the
implicit learning system is responsible for such learning.         I would also like to thank Thomas R. Shultz and Kris Onishi
Actually, the fact that performance level is very similar          for their input, guidance and support.
under impaired (concurrent) one-dimensional learning and
three-dimensional rule learning is compatible with this                                    References
claim. Under the multiple learning system model,
participants should not be able to verbalize one-dimensional       Ashby, F. G., & Ell, S. W. (2002). Single versus multiple
rules learned using the implicit system because the implicit         systems of category learning: Reply to Nosofsky and
                                                                     Kruschke (2002), Psychonomic Bulletin & Review, 9 (1),
system is not connected to language processing modules of
                                                                     175-180.
the brain.
                                                                   Fahlman, S. E. (1988). Faster-learning variations on back-
   By contrast, an alternative explanation supporting a single       Propagation: An empirical study. Proceedings of the 1988
category learning system is that differences in ability to           Connectionist Models Summer School, Morgan
verbalize rules are due to limitations in the system which is        Kaufmann.
interpreting, extracting, or decoding what the single learning     Fahlman, S. E., & Lebiere, C. (1990). The cascade
system has learned. It might be that, while an interpretive          correlation learning architecture. In D. S. Touretzky (Ed.),
system is capable of extracting verbally-encoded rules for           Advances in Neural Information Processing Systems 2
simple tasks such as one-dimensional rules, it can not do so         (pp. 524-532). Los Altos, CA: Morgan Kaufmann.
for more complex tasks like the three-dimensional rule. This       Harnad, S. (2005). To cognize is to categorize: Cognition is
model therefore predicts that participants would be able to          categorization. In C. Lefebvre & H. Cohen (Eds.),
verbalize one-dimensional rules learned in a concurrent              Handbook on Categorization, Elsevier.
condition.                                                         Kello, C. T., Sibley, D. E., & Plaut, D. C. (2005).
   Other models of these data have been proposed, including          Dissociations in performance on novel versus irregular
COVIS (Waldron & Ashby, 2001) and Alcove (Nosofsky &                 items: Single-route demonstrations with input gain in
Kruschke, 2002, see also Ashby & Ell, 2002 for a reply).             localist and distributed models. Cognitive Science, 29,
COVIS posits different learning systems for explicit and             627-654.
implicit rules. ALCOVE requires setting four free                 MacLeod, C. M. (1991). Half a century of research on the
parameters.                                                          Stroop effect: An integrative review. Psychological
   In short, the current model suggests that Waldron and             Bulletin, 109, 163-203.
Ashby’s multiple learning systems are not necessary to            Nosofsky, R. M., & Kruschke, J. K. (2002). Single-system
cover the critical Rule Type by Condition interaction, that          models and interference in category learning:
is, the fact that the concurrent Stroop task interferes more         Commentary on Waldron and Ashby (2001),
                                                                     Psychonomic Bulletin & Review, 9 (1), 169-174.
with the learning of explicit rules than implicit rules. A
                                                                  Shultz, T. R. (2003). Computational Developmental
Cascade-Correlation model provides a simple and
                                                                     Psychology. Cambridge, MA: MIT Press.
parsimonious account in a single learning system. This and        Stroop, J. R. (1935). Studies of interference in serial verbal
other simulations (e.g., Kello et al., 2005; Nosofsky &              reactions. Journal of Experimental Psychology, 28, 643-
Kruschke, 2002) suggest that we need to be careful about             662.
using interaction evidence to draw conclusions about              Waldron, E. M., & Ashby, F. G. (2001). The effects of
complex cognitive systems.                                           concurrent task interference on category learning:
                                                                     Evidence for multiple category learning systems.
                                                                     Psychonomic Bulletin & Review, 8 (1), 168-176.
                                                              1173

