        Table 1: Ten basic situations in the microworld.             Table 2: Rewrite rules of the microlanguage. The probabili-
                                                                     ties of possible productions are indicated if they are unequal.
          situation   meaning                                        S = sentence; NP = noun phrase; VP = verb phrase; APP = ad-
          B OUT       Bob is outside                                 verbial/prepositional phrase. Variable n denotes verb number
          J OUT       Jilly is outside                               (singular or plural).
          SOCCER      Bob and Jilly play soccer
          HIDE        Bob and Jilly play hide-and-seek                  Head                Production
                                                                        S              →    Splay | Swin
          B COMP      Bob plays a computer game
                                                                        Splay          →    NPsing VPplay,sing (.7) |
          J COMP      Jilly plays a computer game
                                                                                            NPplu VPplay,plu (.3)
          B DOG       Bob plays with the dog
                                                                        Swin           →    NPsing VPwin,intrans | NPsing VPwin,trans
          J DOG       Jilly plays with the dog
          B WIN       Bob wins                                          NPsing         →    someone (.2) | bob (.3) | jilly (.3) |
          J WIN       Jilly wins                                                            bob or jilly (.1) | jilly or bob (.1)
                                                                        NPplu          →    bob and jilly | jilly and bob
                                                                        VPplay,n       →    Vplay,n (.36) | Vplay,n Place (.24) |
                                                                                            Vplay,n Activities (.24) |
            Microworld and Microlanguage                                                    Vplay,n Activities Place (.16)
Since we are dealing with semantic (as opposed to syntac-               VPwin,intrans  →    Vwin,intrans (.3) | Vwin,intrans APP (.7)
tic) systematicity, arising from situational representations, it        VPwin,trans    →    Vwin,trans NPsing (.3) |
is necessary to define a world in which situations occur. Ob-                               Vwin,trans NPsing APP(.7)
viously, the real world (or even a substantial subset thereof)          APP            →    Place (.3) | at Games (.3) |
is far too complex to implement in a computational model.                                   Place at Games (.2) |
For this reason, we make use of the microworld developed by                                 at Games Place (.2)
Frank, Koppen, Noordman, and Vonk (2003) for their Dis-                 Vplay,sing     →    plays
tributed Situation Space (DSS) model of knowledge-based
                                                                        Vplay,plu      →    play
inference in story comprehension. The microworld has only
two characters, called Bob and Jilly, who can perform differ-           Vwin,intrans   →    wins | loses
ent actions and be in several places and states. Anything that          Vwin,trans     →    beats | loses to
happens to them can be expressed as a boolean combination               Place          →    inside | outside
of a small number of so-called ‘basic situations’. Only ten of
these, listed in Table 1, are relevant here.                            Activities     →    game (.15) | Activity (.45) |
                                                                                            Activityi or Activity j (i 6= j) (.4)
    Of course, not just anything is possible in the microworld.
For instance, soccer and hide-and-seek are always played by             Activity       →    Game (.75) | with dog (.25)
Bob and Jilly together. This is why there are no basic sit-             Games          →    game (.15) | Game (.45) |
uations in which only Bob or only Jilly plays one of these                                  Gamei or Game j (i 6= j) (.4)
games. Also, someone can win only when SOCCER or HIDE                   Game           →    soccer | hide-and-seek | computer-game
is the case, or when both Bob and Jilly play a computer game.
Furthermore, soccer can only be played when Bob and Jilly
are outside, while computer games are only played inside.
Naturally, Bob and Jilly cannot win simultaneously.                     Note that the microlanguage (as any natural language) can
    Any situation in the microworld can be described by a            describe situations that are impossible to occur, such as Jilly
sentence from a microlanguage. Sentence comprehension is             losing to herself or playing soccer inside. To prevent the
taken to be the reconstruction of the situation from the sen-        occurrence of a very large number of such sentences, plu-
tence that describes it. In the Frank (2005) model, there were       ral noun phrases and the phrase with dog are not allowed in
only 15 different words, which could be combined to form             sentences describing winning or losing. As a result, there are
328 different sentences. Moreover, word order was not rel-           no sentences stating that both Bob and Jilly win or lose, nor
evant to these sentences. Here, a more complex microlan-             any sentence stating that someone wins or loses at playing
guage is used: It has 20 words that can form 3558 different          with the dog. Both these situations are impossible in the mi-
sentences, some of which contain a transitive verb that makes        croworld.
word order relevant to the sentence’s meaning.
    The language’s lexicon contains two proper names (bob,                                    The Model
jilly), one pronoun (someone), five nouns (dog, soccer, hide-
and-seek, computer-game, game), five verbs (wins, loses,             Representing microworld situations
beats, plays, play), two adverbs (inside, outside), three prepo-
sitions (with, to, at), and two connectives (and, or). These         According to Hadley (2004, p. 150), for a sentence-
words form sentences according to the grammar in Table 2.            comprehension model to display semantic systematicity, it is
A few examples of sentences, and the way the corresponding           necessary that ‘the resultant meaning representation for the
microworld situations are constructed from basic situations,         entire sentence possesses properties which would enable us to
are shown in Table 3.                                                justly claim that the entire representation could, in principle,
                                                                 227

                                                                            be estimated by:
Table 3: Examples of microlanguage sentences and the con-
struction of the corresponding situations from the basic situ-                                          τ(p ∧ X) ∑i µi (p)xi
ations in Table 1.                                                                         τ(p|X) =               =             .            (3)
                                                                                                          τ(X)          ∑i xi
      Sentence                         Situation
      jilly plays                      SOCCER ∨ HIDE       ∨ J COMP         These τ-values are called belief values because they indicate
                                              ∨ J DOG                       the extent to which the DSS model may ‘believe’ particular
      bob plays game                   SOCCER       ∨ HIDE ∨ B COMP         (basic) situations to be the case given a situation vector. As
      bob and jilly play with dog      B DOG ∧ J DOG                        shown empirically by Frank et al. (2003), belief values are
      someone wins                     B WIN ∨ J WIN                        accurate estimates of (un)conditional probabilities in the mi-
      bob or jilly wins at             (J WIN ∧ J COMP )                    croworld. This proves that relations among microworld situ-
             computer-game                    ∨ (B WIN ∧ B COMP )           ations are indeed (implicitly) encoded in the organization of
      bob loses                        J WIN                                situation space.
      jilly loses to jilly inside      B WIN ∧ J WIN ∧¬J OUT
                                                                            The Network
                                                                            A new approach to dynamical computation by neural net-
constrain the set of situations which could render the sentence             works was recently developed independently by Maass,
true.’ The representations used by the DSS model (which are                 Natschläger, and Markram (2002) and by Jaeger (2003).
also used here) have exactly these properties. Any situation                Their so-called Liquid State Machines (LSM; Maass et al.)
that can occur in the microworld is represented distributively              and Echo State Networks (ESN; Jaeger)2 are both based on
by a vector in a high-dimensional ‘situation space’. As for-                the insight that training all connections of a recurrent net-
malized below, relations among these ‘situation vectors’ cor-               work is not needed. Instead, the weights in the recurrent part
respond to probabilistic relations among the represented mi-                of the network may remain fixed, greatly increasing train-
croworld situations.                                                        ing efficiency. These recurrent connections contribute to the
   Situation vectors are developed by training a Self-                      network’s computations by forming a ‘dynamical reservoir’
Organizing Map (SOM; Kohonen, 1995) on descriptions of                      (DR) that stores the input sequence in an unstructured manner
situations occurring in the microworld (see Frank et al., 2003,             (in much the same way that the pattern of waves in a bucket
for details). These descriptions take the form of binary vec-               of water contains information about what has recently fallen
tors containing a 1 for each basic situation that occurs at a cer-          in). A separate ‘readout’ network is trained to convert the
tain moment in time, and a 0 for each basic situation that does             activation patterns in the DR into target outputs.
not. As a result of training, a membership value µi (p) ∈ [0, 1]               The network used here (drawn schematically in Figure 1) is
is associated to each SOM-cell i and basic situation p. This                in fact an extension of the ESN in that the readout network is a
value indicates the extent to which cell i forms part of the                two-layer feedforward network, that is, it has a hidden layer.
representation of p. The SOM has 150 cells, so the repre-                   Frank (in press) reports that this additional layer improves
sentation of p can also be viewed as a 150-element situation                generalization in a sentence-processing task. The complete
vector of membership values µ(p) = (µ1 (p), . . . , µ150 (p)).              network consists of four layers:
   Representations of negations, conjunctions, and disjunc-
tions are constructed as is common in fuzzy logic:                          • The input layer, with one unit for each of the 20 words in
                                                                               the microlanguage.
                  µi (¬p) = 1 − µi (p)
               µi (p ∧ q) = µi (p)µi (q)                            (1)     • The dynamical reservoir, serving as a short-term memory
                                                                               for retaining the input sequence. The number of DR-units
               µi (p ∨ q) = µi (p) + µi (q) − µi (p)µi (q),                    was varied from 80 to 250.
where p and q can themselves be combinations of (basic) sit-                • The hidden layer, varied in size from 10 to 40 units.
uations.
   Let vector (x1 , . . . , x150 ) represent some microworld situa-         • The output layer, with one unit for each of 150 dimensions
tion X.1 The a priori probability that this situation occurs can               of situation space.
be estimated by
                                       1                                       Words enter the network one at a time by activating only
                                     150 ∑
                             τ(X) =            xi .                 (2)     the corresponding unit of the input layer. This activation is
                                            i
                                                                            sent to the DR, which, like the recurrent layer of a SRN, also
   The content of situation X can be extracted by compar-                   receives its own activation that resulted from processing the
ing its representation (x1 , . . . , x150 ) to several known situation      previous word. The main difference with a SRN is that the
vectors µ(p). From Equations 1 and 2 it follows that the con-
ditional probability that some p is the case in situation X, can                2 Although LSMs and ESNs are conceptually similar, there are
                                                                            some differences in their technical descriptions and in the way they
    1 This can be any vector in situation space, that is, it does not       are commonly applied. The network we use here is most like an
need to have been constructed using Equation 1.                             ESN, so we shall refer to it as such.
                                                                        228

                     150 output units                                         The network is trained on situations in which Bob loses or
                                                                              Jilly wins. However, these situations are only described using
                        Wout                                                  the intransitive verbs wins and loses, not the transitive beats
                                                     readout                  and loses to.
                                                     network                     The microworld situation referred to by each training sen-
                       hidden units
                                                                              tence was the target output during training on that sen-
                        Whid                                                  tence. After each word of each training sentence, connec-
                                                                              tion weights Whid and Wout were updated using the standard
                 Dynamical Reservoir               Wdr                        backpropagation algorithm.
                                                                                 After training, the network was tested on three sets of test
                                                                              sentences:
                         Win
                                                                              • The ‘win’-set, consisting of all 88 sentences that describe a
                      20 input units                                             possible situation in the microworld, do not specify a place,
                                                                                 and have either bob as object of beats or jilly as object of
Figure 1: Extended Echo State Network. Solid arrows in-                          loses to.
dicate connections with trainable weight, dashed arrows are
                                                                              • The ‘game/place’-set, consisting of all 14 sentences that
connections with fixed weight. Connection weight matrices
                                                                                 describe a possible situation in the microworld, and end
are denoted W.
                                                                                 in either play(s) with dog inside or play(s) hide-and-seek
                                                                                 outside.
ESN’s input and DR-weight matrices Win and Wdr remain                         • The ‘random’-set, consisting of 100 random sentences.
fixed at their initial values.3 If the two weight matrices of
the readout network, Whid and Wout , are trained correctly, the                  The sentences in the ‘random’-set may or may not
network’s output activation vector represents the microworld                  have been training sentences. In contrast, the ‘win’ and
situation referred to by the input sentence.                                  ‘game/place’-sets contain only novel sentences. Moreover,
   Although Wdr , the matrix of DR connection weights, has                    the situations described by the ‘game/place’-sentences were
random values, not just any matrix will do. First, it is im-                  not presented to the network during training. To successfully
portant that only a small fraction of possible DR-connections                 process these sentences, the network must have learned how
is present. In the network used here, 85% of DR-connection                    novel combinations of known words refer to novel combina-
weights are set to zero. Second, the optimal overall scaling                  tions of known situations. That is, it must display semantic
of weights depends on the network’s task. This scaling is                     systematicity.
expressed by the spectral radius of Wdr ,4 which was varied
                                                                              Rating performance
from .4 to .95. The larger this value, the longer information
remains in the DR.                                                            Comprehension is not an all or none type of phenomenon.
                                                                              Millis, King, and Kim (2000), for instance, showed experi-
Training and Testing                                                          mentally that the extent to which readers develop a situational
The network was trained on a set of 10 000 sentence tokens,                   representation of a text varies between subjects and experi-
randomly generated by the production rules of Table 2. The                    mental tasks. The model, too, may only construct the correct
following sentences were excluded from the training set:                      situation vector to a degree. To ascertain the network’s perfor-
                                                                              mance, the comprehension measure defined by Frank (2005)
• Those with bob, bob or jilly, or jilly or bob as object of                  is used. It is based on the belief values from Equations 2 and
   beats.                                                                     3.
                                                                                 Suppose the network processes a sentence that states p, and
• Those with jilly, bob or jilly, or jilly or bob as object of
                                                                              the resulting output vector represents some situation X. The
   loses to.
                                                                              associated comprehension score is then defined as
• Those containing both dog and inside.
                                                                                                        τ(p|X) − τ(p)
                                                                                                                       .
• Those containing both hide-and-seek and outside.                                                      τ(p|p) − τ(p)
   This means that the network does not learn to construct any                   If the probability of p in situation X is larger than its a pri-
situation in which Bob and/or Jilly plays with the dog inside,                ori probability (i.e., if τ(p|X) > τ(p)), processing the sen-
nor any situation in which they play hide-and-seek outside.                   tence has increased belief in p. This means that the sentence
    3 Furthermore, in our network, DR-units are linear and have no
                                                                              was understood to some extent, as indicated by a positive
bias weights, while all units of a SRN usually have sigmoidal acti-           comprehension score. A perfect network will result in X = p,
vation functions and bias weights.                                            so τ(p|X) = τ(p|p), corresponding to a comprehension score
    4 The spectral radius of a matrix is its largest absolute eigenvalue.     of 1. In contrast, if processing the sentence decreases belief
                                                                          229

in p (i.e., if τ(p|X) < τ(p)), the sentence was misunderstood          bob or bob beats jilly, yet it could correctly construct rep-
and the network made an error. The resulting comprehension             resentations of these situations. Furthermore, it was never
score is negative.                                                     trained to represent situations in which Bob and/or Jilly play
   Most sentences describe more than one basic situation. For          with the dog inside, nor situations in which hide-and-seek is
instance, jilly wins at hide-and-seek outside describes a game         played outside. Nevertheless, it did comprehend sentences
(Jilly plays hide-and-seek), a place (Jilly is outside) and an         referring to such situations.
outcome (Jilly wins). To investigate whether the network un-              These results are quite robust in the sense that they did not
derstood all of these basic situations, comprehension scores           crucially depend on a delicate setting of parameters, nor re-
are computed not only for the complete situation HIDE ∧                quired an extensive and complex training procedure. Ten net-
J OUT ∧ J WIN, but also separately for the three basic situa-          works were trained, starting with different weight settings,
tions. In general, each sentence may give rise to up to four           and each of these managed to comprehend the test sentences.
separate comprehension scores: for the game, the place, the            Also, we demonstrated that many novel sentences were com-
outcome, and the conjunction of the three.                             prehended correctly, again indicating that the network’s abil-
                                                                       ities are not just accidental.
                            Results                                       As stated in the Introduction, two more criteria are of-
To investigate the robustness of the training process and the          ten applied to evaluate systematicity in connectionist models:
model’s results, three parameters were varied: the number of           scalability and usability in inference. It is only fair to ad-
DR-units (80 – 250), the number of hidden units (10 – 40),             mit that the size and complexity of the simulations need to
and the spectral radius of Wdr (.4 – .95).                             be expanded considerably in order to be psychologically re-
   Apart from the smallest networks, all combinations of pa-           alistic. Although the microlanguage was 10 times the size of
rameter settings resulted in positive comprehension scores for         Frank’s (2005b), other models of language processing have
all test sets (averaged over test sentences). In general, the          used more extensive languages. However, it should be kept
larger the network, the better its performance. For the largest        in mind that our focus on microworld situations requires the
network, the optimal spectral radius was around .6.                    time-consuming and effortful construction of a microworld
                                                                       and corresponding microlanguage, making large-scale simu-
   Applying this optimal parameter setting, ten networks were
                                                                       lations laborious to set up. As for the criterion of usability
trained, differing only in their initial random weights. The re-
                                                                       in inference, be reminded that the situational representations
sulting comprehension scores, averaged over the ten networks
                                                                       were developed for the DSS model of inferencing in story
and over test sentences, are listed in Table 4. Clearly, all
                                                                       comprehension (Frank et al., 2003). Correct predictions of
scores are significantly positive, indicating that the network
                                                                       experimental data by the DSS model indicate that these rep-
understood each part of the sentences (as well as complete
                                                                       resentations not only allow for simulations of inferencing, but
sentences) of each test set. The table also shows the percent-
                                                                       do so in a realistic manner.
age of errors. Most of these are very low, meaning that only
a few sentences were misunderstood.                                                         Acknowledgments
                         Conclusions                                   We would like to thank Leo Noordman and Michael Klein for
                                                                       their useful remarks. The research presented here was sup-
The sentence comprehension model clearly behaves system-               ported by grant 451-04-043 of the Netherlands Organization
atically: It was not trained on sentences stating that jilly beats     for Scientific Research (NWO).
                                                                                                References
Table 4: Average comprehension scores with 95% confidence              Aizawa, K. (2003). The systematicity arguments. Dordrecht,
intervals, and percentage of errors for all three test sets.              The Netherlands: Kluwer Academic Publishers.
     set            part         comprehension      % errors           Bodén, M., & Niklasson, L. (2000). Semantic systematicity
     win            game             .80 ± .02          0.0               and context in connectionist networks. Connection Science,
                    outcome          .30 ± .01          0.0               12, 111-142.
                    complete         .28 ± .01          0.0            Butler, K. (1993). Connectionism, classical cognitivism and
     game/place     game             .29 ± .03          7.1               the relation between cognitive and implementational levels
                    place            .76 ± .03          0.0               of analysis. Philosophical Psychology, 6, 321–333.
                    complete         .27 ± .02          0.0
                                                                       Chalmers, D. J. (1990). Syntactic transformations on dis-
     random         game             .77 ± .02          2.9               tributed representations. Connection Science, 2, 53–62.
                    outcome          .49 ± .03          0.4
                    place            .70 ± .04          7.4            Christiansen, M. H., & Chater, N. (1994). Generalization
                    complete         .60 ± .02          2.2               and connectionist language learning. Mind & Language, 9,
                                                                          273–287.
                                                                   230

Dennett, D. C. (1991). Mother nature versus the walking             Hadley, R. F., & Cardei, V. C. (1999). Language acquisi-
  encyclopedia: a western drama. In W. Ramsey, S. Stich, &            tion from sparse input without error feedback. Neural Net-
  D. Rumelhart (Eds.), Philosophy and connectionist theory.           works, 12, 217–235.
  Hillsdale, NJ: Erlbaum.
                                                                    Hadley, R. F., Rotaru-Varga, A., Arnold, D. V., & Cardei,
Elman, J. L. (1990). Finding structure in time. Cognitive             V. C. (2001). Syntactic systematicity arising from semantic
  Science, 14, 179–211.                                               predictions in a Hebbian-competitive network. Connection
                                                                      Science, 13(1), 73–94.
Fodor, J. A., & McLaughlin, B. (1990). Connectionism and
  the problem of systematicity: Why Smolensky’s solution            Haselager, W. F. G., & Van Rappard, J. F. H. (1998). Con-
  does not work. Cognition, 35, 183–204.                              nectionism, systematicity, and the frame problem. Minds
                                                                      and Machines, 8, 161–179.
Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and
  cognitive architecture: a critical analysis. Cognition, 28,       Jaeger, H. (2003). Adaptive nonlinear system identifica-
  3–71.                                                               tion with echo state networks. In S. Becker, S. Thrun, &
                                                                      K. Obermayer (Eds.), Advances in neural information pro-
Frank, S. L. (2005). Sentence comprehension as the construc-          cessing systems (Vol. 15). Cambridge, MA: MIT Press.
  tion of a situational representation: a connectionist model.
  In A. Russell, T. Honkela, K. Lagus, & M. Pöllä (Eds.),         Kohonen, T. (1995). Self-organizing maps. Berlin: Springer.
  Proceedings of AMKLC’05. Espoo, Finland: Helsinki Uni-
  versity of Technology.                                            Maass, W., Natschläger, T., & Markram, H. (2002). Real-
                                                                      time computing without stable states: a new framework for
Frank, S. L. (in press). Learn more by training less: system-         neural computation based on perturbations. Neural Com-
  aticity in sentence processing by recurrent networks. Con-          putation, 14, 2531–2560.
  nection Science.
                                                                    Millis, K. K., King, A., & Kim, H. J. (2000). Updating situa-
Frank, S. L., Koppen, M., Noordman, L. G. M., & Vonk,                 tion models from descriptive texts: a test of the situational
  W. (2003). Modeling knowledge-based inferences in story             operator model. Discourse Processes, 30, 201–236.
  comprehension. Cognitive Science, 27, 875–910.
                                                                    Niklasson, L. F., & Van Gelder, T. (1994). On being system-
Hadley, R. F. (1994a). Systematicity in connectionist lan-            atically connectionist. Mind & Language, 9, 288–302.
  guage learning. Mind & Language, 9(3), 247–272.
                                                                    Pollack, J. B. (1990). Recursive distributed representations.
Hadley, R. F. (1994b). Systematicity revisited: reply to Chris-       Artificial Intelligence, 46, 77–105.
  tiansen and Chater and Niklasson and van Gelder. Mind &
  Language, 9, 431–444.                                             Wilks, Y. (1990). Some comments on Smolensky. In D. Pa-
                                                                      tridge & Y. Wilks (Eds.), The foundations of AI. Cam-
Hadley, R. F. (2004). On the proper treatment of semantic             bridge: Cambridge University Press.
  systematicity. Minds and Machines, 14, 145–172.
                                                                    Zwaan, R. A., & Radvansky, G. A. (1998). Situation model
                                                                      in language comprehension and memory. Psychological
                                                                      Bulletin, 123, 162–185.
                                                                231

