UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Model of Syntactic Parsing Based on Domain-General Cognitive Mechanisms
Permalink
https://escholarship.org/uc/item/0d00v4tf
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Cassimatis, Nicholas
Murugesan, Arthi
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

     A Model of Syntactic Parsing Based on Domain-General Cognitive Mechanisms
                                               Arthi Murugesan (muruga@rpi.edu)
                                               Nicholas Cassimatis (cassin@rpi.edu)
                                                    Rensselaer Polytechnic Institute
                                             Department of Cognitive Science, 110 8th Street
                                                        Troy, NY 12180 USA
                              Abstract                                seeing. Therefore, the mechanisms used to understand a
                                                                      seemingly simple sentence include language, vision, reason-
   The relationship between linguistic and nonlinguistic cogni-       ing and semantic knowledge. The modular theory doesn’t
   tion is an important area of study including the questions of      address the question raised by this phenomenon: if the
   language modularity and learnability. We believe that insights
                                                                      mechanisms of language and other kinds of inferences are
   to this relationship can be obtained by implementing a precise
   computational model of sentence understanding within a gen-        so different, how do they interact so tightly and so well?
   eral cognitive architecture. In this paper we have represented               On the other hand, the non-modular approach
   a wide-coverage grammar (Head-driven Phrase Structure              claims that language is a part of and interacts freely with the
   Grammar) using non-linguistic representation. We have              greater cognitive system (Newell & Simon 1972; Marslen-
   shown how grammar-specific representations like specifier,         Wilson and Tyler, 1989). The striking problem here, per-
   complement, modifier and gap map to domain-general event           haps most visible in syntax, is that concepts involved in lan-
   representations such as subgoals and temporal constraints.         guage (e.g., argument structure, head projection or agree-
   The paper thus demonstrates that domain-general cognitive          ment) and concepts involved in physical reasoning (e.g.,
   mechanisms are sufficient and adequate for syntactic parsing.
                                                                      gravity, time or place) seem to be very dissimilar and unre-
   Keywords: Syntactic Parsing; domain-general mechanisms;            lated. How could we use, for example, the same mechanism
   HPSG; Polyscheme; modularity of language.                          to solve an equation, plan a shortest route to a restaurant and
                                                                      determine the binding properties of a given pronoun? The
                          Introduction                                lack of a detailed answer to these questions makes the
                                                                      modularity hypothesis more plausible.
The relationship between linguistic and nonlinguistic cogni-                    Cassimatis (2004) addressed this disparity by
tion is an important question in cognitive science. One way           showing that aspects of language syntax such as categories,
of studying this topic is to ask if language is an independent        word order and constituency can be mapped onto domain-
module or integrated with the rest of cognition. Both ap-             general cognitive structures. Although this work demon-
proaches have problems explaining the phenomenon.                     strated that broad linguistic concepts have duals in domain-
            The modularity of language has been argued for by         general cognition, it did not demonstrate that these high-
Fodor (1983) and Chomsky (1988). Both in computational                level dualities were sufficient to map a sophisticated gram-
linguistics and in cognitive modeling of sentence process-            matical theory onto domain-general cognitive structures.
ing, modular approaches have trouble explaining the inter-
action of people’s syntax-specific processing mechanisms
                                                                        Addressing Modularity with Cognitive Models
and general inference mechanisms about the world. A
number of sentences that we parse in daily life require vis-                        of Sentence Understanding
ual information, social awareness, higher reasoning and                We believe that designing a sentence parsing system within
other complementary cognitive processes to obtain the sub-             a cognitive architecture will aid us to get a better insight of
tle meanings, for example:                                             language integration with cognition. Our approach is driven
                                                                       by the hypotheses described in this section.
            I saw a star with a telescope.                                      Language processing uses the same mechanisms
            I saw an astronomer with a telescope.                      as the other forms of cognition. We accept the evidence
                                                                       that language is highly integrated with the rest of cognition.
In these sentences, the telescope could be the instrument              In addition to the commonalities between syntactic and
used to view the object (star or astronomer) or the telescope          physical structure outlined in Cassimatis (2004), there are
could be seen near (with) the object. However, typically               several reasons for this hypothesis in other parts of lan-
people infer the star to be viewed through the telescope and           guage. For example, Bloom (2000) and Tomasello, Kruger,
the astronomer to be near the telescope. Disambiguation can            and Ratner (1993) provide experimental results that show
be attributed to both the visual perception of the telescope           the general theory of mind learning also aids in learning
and the reasoning that an astronomer uses the instrument               words. Jackendoff (1990) demonstrates how concepts from
frequently while watching an astronomer through the tele-              physical reasoning can be used to formalize semantics of
scope is improbable. This reasoning also requires knowl-               verbs in many fields. Clark (1996) analyzes many aspects
edge about astronomers, their instruments and the action of            of language use as a species of social interaction. All this
                                                                  1850

work makes it at least plausible that the syntactic structure     tional mechanisms. Because the Polyscheme cognitive
of human languages can be accounted for using domain-             framework is designed to model the integration of multiple
general cognitive notions.                                        reasoning and learning mechanisms, we developed our
           Wide-coverage of linguistic mechanisms is re-          model within it.
quired. We hope to show how the various aspects of sen-
tence processing relate to and rely on general cognitive           Polyscheme
processes. More specifically in modeling terms, the chal-          Polyscheme is a cognitive framework which has been used
lenge is to show how a cognitive architecture with domain-         to model a wide range of phenomena from fundamental
general mechanisms can be used to model syntactic parsing.         physics laws to theory of mind based intention reading
The design of such a model can be approached either by (1)         (Cassimatis 2005). The driving principle behind Poly-
detailed modeling of the components of the process (e.g.           scheme is that different high-level tasks use the same under-
part of speech disambiguation, past tense retrieval (Taatgen       lying set of common functions and that these common func-
& Anderson 2002)) followed by integration or (2) through           tions can be implemented using multiple computational ap-
comprehensive modeling of the entire process from the be-          proaches. Each module implementing a common function is
ginning. Though careful accurate component modeling can            called a specialist and a specialist shares opinions with the
give us tight fits to human data, these highly focused models      other specialists through altering the truth values of a com-
have fewer constraints leading to higher risk of overfitting       mon substrate of propositions. As the specialists are inte-
exclusively to the data set studied. On the other hand in a        grated through a common substrate, the internal working of
comprehensive model the need for the various subparts to           each specialist is abstracted, enabling the specialist to use
interact with each other naturally, provides a stronger and        any computational method required. Some of the mecha-
more restrictive list of constraints. Though we realize that       nisms used for implementing the parser include rules,
even a comprehensive model is just a possible explanation,         spreading of activation and constraint satisfaction.
the chances of overfitting are greatly reduced with the in-                  The various specialists used for the parser include
crease in the complexity of the task being modeled. Fur-           difference specialist, temporal specialist and rule specialist.
thermore, limiting the study to a specific component may           The difference specialist checks for uniqueness and identity
also lead to situations where the hardest part of the problem      of objects, while the temporal specialist handles sequencing
lies outside the focus of study. These convictions lead us to      of events. The rule specialist by itself enables a simple and
build a model of the entire sentence parsing system instead        powerful production system. The rules stored in the rule
of focused components.                                             specialist consist of a set of antecedent propositions, the set
           Base model on wide-coverage grammatical the-            of resulting propositions and a strength of the rule in terms
ory. To be confident that our model can potentially account        of confidence level. For the sake of familiarity an example
for the wide array of syntactic phenomena, we decided to           of a proposition is
base our model on a wide-coverage theory of grammar.
Among the various candidate grammatical theories, Head-              PropositionName object1 object2 Time World.
driven Phrase Structure Grammar (HPSG) has wide accep-
tance and several computational implementations. The                         A proposition can take any number of objects. Note
flexibility and broad coverage offered by few general rules        that all propositions have the time, for which they hold true,
make HPSG a powerful tool. Though we are primarily con-            associated with them. A proposition can exist with varying
cerned with the syntactic aspect of sentences, HPSG can            truth values in different worlds and this world concept can
also be used as a tool to bridge syntax with semantics             be thought of as namespaces within the common substrate.
(Dorna & Emele, 1996). These features of HPSG and the
general trend in the field towards HPSG have led us to base        A Model of Syntactic Parsing
our model on HPSG as proposed by Sag, Wasow and
                                                                   The major insight our model is based on is that representa-
Bender (2003).
                                                                   tions normally used to capture nonlinguistic relations can be
           Parsing integrates multiple cognitive structures        used to represent linguistic relations. Once we show how to
and processes. For example, a common trend in HPSG and             represent these nonlinguistic relations in Polyscheme, then
other parsing and grammatical literature has been to use sta-      its existing domain-general reasoning mechanisms will infer
tistics to fine tune the bare HPSG Parser (Torisawa, Ni-           the structure of a sentence with no special inference ma-
shida, Miyao, & Tsujii, 2000). The various aspects in which        chinery just for syntax.
statistics are used include skewing the success of a phrase in               The HPSG grammar is based on words and phrases
the lexicalization step, handling word sense disambiguation,       modeled as feature structures. The feature structures interact
prioritizing the selection of possible parse tree path and as-     with each other through the constraints of the Grammar
signing weight to HPSG rules themselves. Certain tech-             rules to form larger phrases and eventually a sentence.
niques like learning rules based on induction are best cap-        Hence the essential components of modeling a HPSG parser
tured by connectionist models. Thus to be able to implement        are word and phrase representations, feature structures,
a complex system like a parser, we require a framework that        grammar rules and lexical rules. Here we show how these
provides a seamless way of integrating various computa-            basic HPSG entities can be represented in terms of domain
                                                                   general concepts.
                                                              1851

Words and phrases as events                                         ment features take a list instead of one object. In Poly-
The utterance of a word is treated in our model like the oc-       scheme this is captured by creating a list-object which in
currence of any other event that can be perceived. In this         turn is associated with all the objects that it contains.
case the perceived event is the Utterance event. Some of
the properties required to distinctly identify and describe an     Tags In HPSG besides features, tags are used widely to rep-
Utterance event (for example the occurrence of word                resent relations between feature structures. Tags with the
“dog” in the sentence “The dog barked”) are the start time,        same label indicate that the feature structures are identical
end time and phonology of the word.                                with essentially the same reference and not just copies hold-
                                                                   ing similar values. For e.g. phrase1 and phrase3 are con-
ISA dogphr UtteranceEvent E R.                                     strained to be the same by tag ||1||.
StartTime dogphr t2 E R.                                                                                                 ⎡ phrase3           ⎤
EndTime dogphr t3 E R.                                                    ⎡ phrase1          ⎤ ⎡ phrase2         ⎤       ⎢                   ⎥
                                                                    ||1|| ⎢                  ⎥ ⎢                 ⎥ ||1|| ⎢ Head [ POS Noun ] ⎥
Phonology dogphr ‘dog’ E R.                                               ⎣ Head [ POS Noun ]⎦ ⎣ Head [ POS Verb]⎦       ⎢⎣ PHONOLOGY 'bat'⎥⎦
ISA theDogPhr UtteranceEvent E R.                                                This is captured in Polyscheme by ensuring that the
StartTime theDogPhr t1 E R.                                         two objects are one and the same instance using the Same
EndTime the DogPhr t3 E R.                                         proposition as in Same phrase1 phrase3 E R.
Phonology theDogPhr ‘the dog’ E R.
                                                                    Grammar Rules
A phrase which is formed by combining neighboring words             An important part of HPSG is a small number of very gen-
is also considered a Utterance event as the phrase shares
                                                                    eral grammar rules. With just Head-Specifier, Head-
the same properties of unique start time, end time and pho-
                                                                    Complement, Head-Modifier and Head-Filler Rule, an ef-
nology. For example, the noun phrase “the dog” which is
generated by combining thephr of time t1 to t2 and                  fective parsing system can be built.
dogphr of time t2 to t3 has the structure shown above. ISA
is the Polyscheme proposition denoting object category.             Head-Specifier Rule: The Head Specifier Rule states that a
                                                                    phrase selecting a preceding or “specifier” phrase, when
Feature Structure as Event Structure                                preceded by the required specifier combine to form a larger
                                                                    phrase. A typical example of Head Specifier rule instance is
Feature Structures of words and phrases form the basic
                                                                    a verb requiring a subject as the specifier phrase. For exam-
building blocks of parse trees and sentences in HPSG. The
                                                                    ple, in the sentence “Dogs barked”, “barked” is the verb re-
typical features of a phrase or word are Head, Part of
                                                                    quiring the subject “dogs” as the specifier. Another com-
Speech, Agreement, Number, Person, Specifier,
                                                                    mon example of Head Specifier phrase is a noun requiring a
Complement, Phonology and Gap. In Polyscheme, the
                                                                    determiner, as in “dog” requiring “a” or “the”, to precede it.
utterance of a word is an event and the features of this event
                                                                                 Though the constraints of Head Specifier rule seem
are considered as objects. For example, the Head feature of
                                                                    to be specific to words and phrases, we can see its core idea
the event dogphrase is an object, say dogphrHead. A
                                                                    as an instance of a general rule, relating events with other
characteristic property of all these features is that each fea-
                                                                    events that must precede them. This concept of an event re-
ture can take only one object as its value. In Polyscheme an
                                                                    quiring another event to come before and qualify the event
object taking a single value is termed as an attribute. Declar-
                                                                    is common in many domains like planning where the pre-
ing the features as attributes, lets us leverage on a number of
                                                                    conditions (Fikes & Nilsson, 1971) of an event are explicitly
built in Polyscheme functionalities.
                                                                    stated. We can also find analogies of preceding events in
                  ⎡      ⎡ POS Noun            ⎤⎤
                  ⎢      ⎢                     ⎥⎥                   common physical reasoning scenarios. For example, in the
                  ⎢ Head ⎢ AGR [ NUM Singular ]⎥ ⎥                  action of opening a locked door, the event of unlocking and
                  ⎢      ⎢⎣COUNT Countable ⎥⎦ ⎥
                  ⎢                              ⎥                  opening the door is the main or head event. However, the
                  ⎢⎣ PHONOLOGY ' dog '           ⎥⎦                 unlocking-door-event is incomplete without the re-
Polyscheme representation of the above feature structure is        trieval-of-keys event preceding it. Hence the unlock-
                                                                   ing-door-event corresponds to an example of a Head
UtteranceEvent dogphr E R.                                          Specifier rule where the head event is unlocking and open-
    Head dogphr dogphrHead E R.                                     ing the door and the specifier event is taking out the keys.
      POS dogphrHead Noun E R.                                      Examples of physical world preceding events are:
      AGR dogphrHead dogphrAgr E R.
           NUM dogphrAgr Singular E R.                                           (Take out the keys) (Opening a locked door)
      COUNT dogphrHead Countable E R.                                            (Loading a gun) (Firing the shot)
    PHONOLOGY dogphr ‘dog’ E R.
                                                                    Thus the specifier as defined in HPSG can be thought of as
Note that the nested features are written as propositions re-
                                                                   a preceding event in domain-general terms. Hence the
lating the internal contained object to the subsuming object
                                                                   specifier attribute of a HPSG feature structure is captured
as shown with Agreement and Num. The Gap and Comple-
                                                                   through the Preceding proposition in Polyscheme.
                                                               1852

          The HPSG head specifier rule is defined as follows       the likelihood of Same (precedingevt, precededby)
in terms of feature structures and tags.                           being conserved by the difference specialist. This process of
                               ⎡ phrase       ⎤                    rejection by difference specialist is the essential constraint
 ⎡ phrase      ⎤               ⎢              ⎥                    that filters invalid phrases like “the go” from valid head
 ⎢             ⎥ → 1        H ⎢SPR 1          ⎥                    specifier phrases like “the dog”.
 ⎣SPR          ⎦               ⎢              ⎥                               Let us consider the formation of the phrase “the
                               ⎣⎢COMPS        ⎦⎥                   dog”. The table below compares the feature structures of
                                                                   ‘the’ and ‘dog’ phrases.
There are three constraints posed by the Head Specifier rule.
The first constraint is that the Head phrase must immedi-                       The phrase                       Dog phrase
ately follow the Specifier (SPR) phrase in time. Secondly,
the Complement (COMPS) feature of the Head phrase must                   Start Time t0                   Start Time t1
already be satisfied and made empty. The final constraint                End Time t1                     End Time t2
ensures the identity of the event that comes before the                  POS determiner                  POS Noun
phrase and the specifier of the phrase through the tag ||1||.            Preceding empty                 Preceding detPhrase
          The domain-general Polyscheme rule that captures               Following empty                 Following empty
the Head Specifier rule is in terms of preceding event and
following event corresponding to the specifier and comple-          The matching rule instance of the pre-head specifier rule is
ment attributes of HPSG.
                                                                    Meets ThePhr DogPhr E ?w +
Meets ?precedingevt ?headevent E ?w +                               Preceding DogPhr detPhrase E ?w +
Preceding ?headevent ?precededby E ?w +                             Following DogPhr empty E ?w
Following ?headevent empty E ?w +                                             ~~> , Same ThePhr detPhrase E ?w
Same ?precedingevt ?precededby E ?w
=> ,                                                                The difference specialist gives opinion on Same (ThePhr,
Preceding ?Phrase- empty E ?w +
                                                                    detPhrase) by comparing the attributes of the two
PercolatePrinciples ?Phrase- ?headevent E ?w
                                                                    phrases. The attributes of ThePhr are listed above and the
          The immediate sequencing of the specifier and             feature structure of a generic detPhrase in HPSG is given as:
Head phrase is an essential constraint to be modeled. In                                ⎡ Head [ POS determiner ]⎤
                                                                                        ⎢                        ⎥
Polyscheme, the Temporal Specialist tracks the se-                                      ⎢SPR empty               ⎥
quencing of events and the Meets proposition specifically                               ⎢                        ⎥
                                                                                        ⎢ COMPS    empty         ⎥
indicates the ending time of the first event coinciding with                            ⎢ MOD empty              ⎥
the beginning time of the second event. The next constraint                             ⎢                        ⎥
                                                                                        ⎢⎣GAP empty              ⎥⎦
enforced by the Head Specifier rule is on the Following-
event attribute of the Head Phrase to be empty. The final          As the attributes defined for ThePhr - POS, Preceding and
constraint of the Head Specifier rule, imposed by the tag ||1||    Following are determiner, empty and empty showing con-
is captured though the Same proposition.                           sistency with the feature structure of detPhrase, the differ-
          Though Same(precedingevt,precededby) is                  ence specialist allows the claim Same(precEvt,
easy to capture, Polyscheme does not initially assume that         precBy) to exist in likely confidence.
precedingEvt and precededBy phrases could be the                              However, considering the phrase “the go”
same as each word occurs independently of the other words.                     The phrase                       Go phrase
The new object created for Preceding event attribute of
                                                                         Start Time t0                 Start Time t1
the head phrase is hence independent of the phrase, pre-
                                                                         End Time t1                   End Time t2
cedingevt. To capture the identity between preced-
                                                                         POS determiner                POS Verb
ingevt and precededBy, a pre-head Specifier rule is in-
                                                                         Preceding empty               Preceding nounPhrase
troduced in Polyscheme. The pre-head Specifier states that
                                                                         Following empty               Following empty
if all other constraints of a Head Specifier Rule are satisfied
then the phrases precedingevt and precededby are
likely to be the same.                                             Matching rule instance:
                                                                    Meets ThePhr GoPhr E ?w +
Meets ?precedingevt ?headevent E ?w +
                                                                    Preceding GoPhr nounPhrase E ?w +
PrecedingEvent ?headevent ?precededby E ?w+
                                                                    Following GoPhr empty E ?w
FollowingEvent ?headevent empty E ?w
                                                                              ~~> , Same ThePhr nounPhrase E ?w
       ~~> , Same ?precedingevt ?precdedby E ?w
The difference specialist verifies that no attributes in the
                                                                    The HPSG feature structure of a generic noun phrase is
two phrases contradict and automatically falsifies the likeli-
                                                                    similar to the detPhrase except that the value of POS is
hood of Same proposition in the case of a contradiction.
                                                                   Noun. The difference specialist falsifies Same(ThePhr,
Hence the domain general Head-Specifier rule proceeds on
                                                                    nounPhrase) as the POS attribute of ThePhr and noun-
                                                               1853

Phrase are different. This falsification prevents the Head          Hence the domain-general following event can effec-
Specifier rule of ‘the go’ phrase from firing.                     tively capture the Complement attribute of HPSG features.
          In the cases where the difference specialist pre-                   The HPSG rule in terms of feature structures is
serves the likelihood of the same proposition, all the condi-       ⎡ phrase        ⎤→     ⎡ phrase                   ⎤
tions required for the domain general Head Specifier rule           ⎢               ⎥     H⎢                          ⎥ ||1||,.., ||n||
are met. The result of Head Specifier rule is the complete          ⎣COMPS          ⎦      ⎣COMPS ||1||, .. , || n || ⎦
feature structure of the combined event. The Percolation           The domain general Polyscheme rule enforcing the head
principles aid in the creation of the new phrase.                  complement rule is
Percolation Principles:                                             Following ?headEvent ?actionList E ?w +
When feature structures interact through Grammar Rules,             NOT Same ?actionList empty E ?w +
only the specific feature corresponding to the rule applied is      First ?actionList ?firstComplement E ?w+
                                                                    Rest ?actionList ?otherComplements E ?w+
defined for the new phrase. For example a phrase formed by          Meets ?headEvent ?followingEvent E ?w +
the Head Specifier Rule has only the preceding attribute de-        Same ?firstComplement ?followingEvent E ?w
fined (here empty). Hence in HPSG there are principles that           ==> ,
fill in the other attribute values of a new phrase given that       COMPS ?Phrase- ?otherComplements E ?w +
the feature structure of the composing phrases including the        PercolatePrinciples ?Phrase- ?headEvent E ?w
Head Phrase are known. The Gap Percolation rule concate-
nates the list of all the composing phrases’ Gap objects in                   In Polyscheme, the Head-Complement rule is im-
the order of occurrence. While the Head Feature Principle           plemented as a recursive rule that creates phrases by com-
and Valence Principle state that the Head Feature and the           bining the Head with the first complement phrase. The Fol-
Valence Feature of the new created phrase are the same as          lowing attribute of the new phrase created has all but the
that of the Head Phrase. The applicability of Percolation          first element of the Head event’s Following, while all the
rules in a domain general scenario is also intuitive. In the       other attributes of the new phrase are filled in through the
example of a gun firing event, the Head event is firing the        percolation principles.
shot. The features of this head event would include the type
and features of the gun say Remington Model 68, 6.22 mm            Head-Modifier Rule: Head modifier rule states that a
shotgun. These features are essential in the bigger gun firing     phrase can modify a head even though it is not specifically
event created by combining preceding and following terms           selected by that head. For example, “The dog in the park
to the head shot fired event.                                      barked” is a sentence in which the phrase “the dog” is fur-
                                                                   ther described using the modifier “in the park”. Note that
Head-Complement Rule1: The HPSG Head Complement                    the phrase “The dog barked” would still make a valid sen-
Rule is similar to Head-Specifier Rule in that a head phrase       tence even without the modifier “in the park”. Hence the es-
needs another qualifying phrase to build a new and com-            sence of the head modifier relation is that the head can exist
plete phrase. However, the Head-Complement rule differs            independently of the modifying event. However, the modi-
on the two accounts of the qualifier phrase following the          fying phrase supplements the effect of the main event.
head and the head phrase taking more than one qualifying                      As with previous rules, we can generalize head-
phrase.                                                            modifier rule to beyond language. This depends on seeing
          A typical example of a head complement rule is a         modifiers as events that contribute to alter another event
verb taking objects. A strictly transitive verb like ‘devour’      without being required by it.
takes a single complement after it like in the sentence ‘he                   The Head Modifier rule given by HPSG and the
devoured the food’. On the other hand, a ditransitive verb         corresponding domain general Polyscheme rule are shown
like ‘handed’ takes a two object complement list as in ‘he         below.
handed (me) (the pen)’. Prepositions, like in and on, also                                                ⎡ COMPS ⎤
take complements typically noun phrases to create complete          [ phrase] → H ||1|| ⎡⎣COMPS ⎤⎦ ⎢                      ⎥
phrases like ‘on the roof’ and ‘in the room’.                                                             ⎣⎢ MOD ||1|| ⎦⎥
          Even in real world events like dining in a restaurant
need complement actions to complete the event. In this ex-          Meets ?headEvent ?ModifyingEvent E ?w +
ample, the process of dining would be the head event and            Following ?headEvent empty E ?w +
paying the bill event which follows the head would be the           Modifies ?ModifyingEvent ?BossEvent E ?w +
complement. Considering an event of starting a car, the             Following ?ModifyingEvent empty E ?w +
                                                                    Same ?headEvent ?BossEvent E ?w +
complements would be essentially the events that immedi-
                                                                             ==> ,
ately follow it like engine starting and making a sound.            PercolatePrinciples ?Phrase- ?headEvent E ?w
1
                                                                    Head-Filler Rule: Head Filler Rule is HPSG’s rule for
  Our treatment is specific to SVO but that we are confident        dealing with long distance dependencies. An example of a
we can generalize to other word orders.                             sentence with long distance dependency is “The table that
                                                               1854

Jim uses is old”. In this sentence, although intuitively we            This has several implications for issues surrounding re-
know the object of the verb ‘uses’ to be ‘the table’, ‘the ta-         search into language and thought. First, the implausibility
ble’ is not spoken after the verb ‘uses’. Hence the table              of non-modular approaches due to the superficial conceptual
though stated only once is essentially used in two places as           differences between linguistic and nonlinguistic cognition
in ‘Jim uses the table’ and ‘the table is old’. Such hidden            are reduced. Second, it suggests a possible explanation for
reference to a previously used term as in the phrase ‘Jim              children’s ability to learn language within a short time
uses’ alluding to the object ‘the table’, is termed as a long          frame (Chomsky, 1980) and with relatively small exposure
distance dependency.                                                   to linguistic input (Pullum, 1996). Many of the processes
               The concept of long distance dependencies can be        and structures required for language may have developed in
thought of as a subordinate event modifying a super event.             children before they begin to understand and use language.
In this example “the table” is the super event while the               Third, if the mind represents syntactic and non-syntactic in-
phrase ‘Jim uses the table’ is the subordinate event or sub-           formation using the same mechanisms, the puzzle of how
goal adding more information to the super event or super               the two forms of information can interact so seamlessly in
goal. Note that ‘the table’ or the super goal is itself a part of      language use is reduced.
the subgoal ‘Jim uses the table’. Real world analogies to an                      In the future, we plan to expand on this ability of
event with a subgoal include the scenario of launching a ball          reasoning, social awareness, and meta-information to aid in
to break a window. The main event or super goal in this                the disambiguation of both word senses and parse trees. In-
scenario is launching the ball. However, a possible subgoal            tegrating the semantics HPSG offers with the syntactical
is to swing a golf club for launching the ball. The swing-             parse structures is another area to be pursued.
golf-club event provides more information to the launch-
ball event while in itself taking the launch-ball event as a re-                                 References
sulting action.                                                        Cassimatis, N. (2005). Integrating cognitive models based
                                 ⎡SubEvent                   ⎤            on different computational methods. Proceedings of the
       ⎡SuperEvent            ⎤  ⎢(e.g. SwingGolfClub) ⎥
 ||1|| ⎢                      ⎥  ⎢                           ⎥            10th Conference of the Cognitive Science Society.
       ⎣ ( e. g . Launchball) ⎦
                                 ⎣⎢[Result ||1|| Launchball]⎦⎥        Cassimatis, N. L. (2004). Grammatical Processing Using the
Long distance dependencies are common in language usage                   Mechanisms of Physical Inferences. The Twentieth-Sixth
and form a difficult problem in language understanding. In                Annual Conference of the Cognitive Science Society.
HPSG the unspecified preceding or following event in                  Chomsky, N. (1980). Rules and Representations. Oxford:
the subordinate phrase is indicated through the GAP feature.              Basil Blackwell.
In the example sentence the object of the verb ‘uses’ would           Clark, H. H. (1996). Using Language. Cambridge Univer-
be a gap feature. The Head Filler rule describes how a new                sity Press, Cambridge.
phrase can be formed when the super goal precedes a sub-               Dorna, M. & Emele, M. (1996) Semantic-based Transfer,
goal event with a gap in it. The corresponding Polyscheme                 Proc. 16th Inernational Conference on Computational
rule is also shown below.                                                 Linguistics (pp. 316—321). København, Denmark.
                                                                       Fodor (1983). The Modularity of Mind, Cambridge, MA
[ phrase] → || 1|| ⎡ GAP ⎤ H ⎡ COMPS ⎤
                         ⎣           ⎦     ⎢             ⎥             Fikes, R., & Nilsson, N. (1971). STRIPS: A new approach
                                           ⎣⎢ GAP ||1|| ⎦⎥                to the application of theorem proving to problem solving.
              Meets ?superEvent ?subEvent E ?w +                          Artificial Intelligence, 2:189-208.
              Following ?subEvent empty E ?w +                         Jackendoff, R. (1990). Semantic Structures.Cambridge,MA.
              SuperGoal ?subEvent ?missing E ?w +
              Same ?superEvent ?missing E ?w
                                                                       Marslen-Wilson, W., & Tyler, L. (1989). Modularity in
==>SuperGoal ?Phrase- empty E ?w +                                        Knowledge Representation and Natural-Language Un-
     PercolatePrinciples ?Phrase ?subEvent E ?w                           derstanding. Cambridge, MA: MIT Press. 37-62.
                                                                       Newell, A., & Simon, H. A. (1972). Human problem solv-
Domain-General Representations                                            ing. Englewood Cliffs, NJ: Prentice-Hall.
                                                                       Pullum, G. (1996). Learnability, hyperlearning, and the
                     HPSG Type                Category
                                                                          poverty of the stimulus. 22nd Annual Meeting of the
                     SPR               Preceding Event
                                                                          Berkeley Linguistic Society. Berkeley, California.
                     COMPS             Following Event
                     MOD               Modifies Event                  Sag, I., Wasow, T., & Bender, E. (2003). Syntactic Theory:
                     GAP               Super Goal                         A Formal Introduction. (CSLI Lecture Notes No. 152).
The HPSG features are mapped to the above representations              Taatgen, N.A., & Anderson, J.R. (2002). Why do children
which are automatically handled by Polyscheme’s common                    learn to say "broke"? A model of learning the past tense
functions with no additional language specific mechanisms.                without feedback. Cognition 86(2).
                                                                       Tomasello, M., Kruger, A., and Ratner, H. (1993). Cultural
                                Conclusions                               Learning. Behavioral and Brain Sciences, 16:495-552.
                                                                       Torisawa, Nishida, Miyao, & Tsujii (2000). An HPSG
               Our model demonstrates that domain-general cog-            parser with CFG filtering. Jounal of Natural Language
nitive mechanisms are sufficient to model syntactic parsing.              Engineering 6(1):63–80
                                                                  1855

