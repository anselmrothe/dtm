UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Not Learning a Complex but Learnable Category
Permalink
https://escholarship.org/uc/item/2dx1p30k
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Author
Danks, David
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                            (Not) Learning a Complex (but Learnable) Category
                                                 David Danks (ddanks@cmu.edu)
                               Department of Philosophy, Carnegie Mellon University, 135 Baker Hall
                                                     Pittsburgh, PA 15213 USA; and
                                     Institute for Human & Machine Cognition, 40 S. Alcaniz St.
                                                        Pensacola, FL 32502 USA
                              Abstract
                                                                                       Theoretical Background
   Recent theoretical research has argued that multiple
   psychological theories of categorization are mathematically          This section outlines the mathematical/theoretical results
   identical to inference in probabilistic graphical models (a          that motivate the experiment reported later in the paper. The
   framework developed in statistics and computer science).             central claim of this section is that most psychological
   These results imply that the major extant psychological              models of categorization can be represented as inference in
   theories can all be represented mathematically as special cases      various types of probabilistic graphical models. Due to
   of inference in (subclasses of) chain graphs, a particular type      space constraints, this exposition is necessarily at a high
   of probabilistic graphical models. These formal results              level. The precise formulations of the frameworks, theories,
   suggest that people should be capable of learning significantly      and equivalencies can all be found in the cited works.
   more complicated category structures than can be expressed
   in the standard psychological theories. In this paper, we            Probabilistic Graphical Models
   present an experiment in which people apparently failed to
   learn the complex category, though a significant group of
                                                                        At a high level, probabilistic graphical models use a graph
   participants seemed to have learned something about the              to encode independencies in a probability distribution. This
   contrast category. Although inferences to cognitive failure are      compact encoding of the independence relations can then be
   notoriously problematic, these results suggest that the hyper-       used to dramatically speed up inference, learning, and
   general theory useful for the mathematical equivalencies does        prediction. The two most common types of graphical
   not accurately describe human categorization.                        models are Bayesian networks and Markov random fields.
                                                                           Bayes nets (e.g., Pearl, 1988, 2000; Spirtes, Glymour, &
                          Introduction                                  Scheines, 1993) represent a probability distribution using a
                                                                        directed acyclic graph (where the variables are nodes in the
Much of the experimental research on category learning has              graph). For example, A Æ B Å C encodes the independence
aimed to distinguish between various theories by presenting             pattern in which A and C are unconditionally independent,
people with categories that can be learned according to one             but no other pairs are independent (conditionally or
theory, but not another. After some learning period with the            unconditionally). Bayes nets are widely used to model
categories, experimental participants are tested to determine           causal relationships, and have more recently been used to
how closely their category representations match the true               model people’s psychological representations of causal
category structure. In contrast, there has been relatively little       structure (Gopnik, Glymour, Sobel, Schulz, Kushnir, &
experimental research investigating the limits of category              Danks, 2004; Griffiths & Tenenbaum, 2005; Lagnado &
learning: specifically, whether there are categories that can           Sloman, 2004; Tenenbaum & Griffiths, 2001; Waldmann &
be learned (in principle) by some statistical procedure, but            Martignon, 1998).
not by people. Most research on the limits of category                     Markov random fields (Lauritzen, 1996) represent the
learning has focused on constraints from other cognitive                independence structure of a probability distribution using an
systems (e.g., memory bounds). In contrast, this paper is a             undirected graph. For example, A ⎯ B ⎯ C implies that “A
preliminary attempt to ask whether some category structures             independent of C given B” is the only independence in the
are sufficiently complex (in a statistical sense) that people           probability distribution. Markov random fields have
are unable to learn them, even though they are sufficiently             frequently been used to model spatially correlated data (e.g.,
structured that they are (in principle) learnable.                      image data). They have not been widely used to model
   We first describe a set of theoretical and mathematical              cognitive phenomena, in part because the edges do not have
results that connect psychological theories of categorization           an obvious interpretation (in contrast with Bayes nets).
with inference to model structure in the computational                     In general, we focus on probability distributions that can
framework of probabilistic graphical models. Those formal               be perfectly represented by some graphical model: that is,
results suggest that people might be able to learn categories           cases in which the graphical model (whether Bayes net or
with a (previously unstudied) complex statistical structure.            Markov random field) predicts all and only the
We thus conducted a category learning experiment using                  independencies that are found in the probability distribution.
this statistical structure, and found suggestive evidence that          The set of probability distributions that can be perfectly
people were in fact unable to learn this category.                      represented by a Bayes net only overlaps with the set of
                                                                        those that can be perfectly represented by a Markov random
                                                                        field. That is, there are probability distributions that can
                                                                   1186

only be perfectly represented in one of the two formalisms          equal to the probability of the category’s causal structure
(but also some that can be perfectly represented by both).          generating a case with the given features.
   Finally, chain graphs (Lauritzen & Richardson, 2002;
Lauritzen & Wermuth, 1989) provide a unifying framework              Categorization Theories and Graphical Models
for Bayes nets and Markov random fields, which are the two           Consider the problem of “categorization” from the graphical
most common types of graphical models. Specifically, chain           models point of view. In particular, suppose that we have
graphs can contain both directed and undirected edges in the         some set of categories that are described by probability
same graph, and so can perfectly represent a richer set of           distributions, where each can be perfectly represented by a
probability distributions.                                           suitable probabilistic graphical model. Given these models
                                                                     and some novel case, we can straightforwardly determine
Standard Accounts of Categorization                                  (using Bayesian updating) the probability that the novel case
Four significant (classes of) psychological theories of              was drawn from each of the probability distributions. That
categorization are: exemplar-based, decision bound,                  is, for each graphical model G under consideration, we can
prototype-based, and casual model theories. Exemplar-                compute P(G | X) for the instance X. At least superficially,
based models (Kruschke, 1992; Medin & Schaffer, 1978;                these conditional probabilities resemble the predictions of
Nosofsky, 1984, 1986) represent a category by a set of               the various categorization theories (which all have the form
exemplar instances, where each exemplar must previously              ‘P(Say “A” | X)’).
have been observed to be a member of the category. To                   Danks (2004) proves that the resemblance is more than
categorize some novel instance, one first computes the               superficial. The psychological theories of categorization are
“similarity” of the instance to each possible category. This         each mathematically equivalent to computing P(G | X),
similarity is the weighted average distance in “similarity          where G is restricted to be a particular type of graphical
space” between the novel instance and each exemplar in the          model. Differences among (the graphical model versions of)
category. The similarities for each category are then               the different psychological theories arise because the
integrated into a probabilistic response using the Shepard-         possibility space is restricted in different ways. The
Luce rule (Luce, 1963; Shepard, 1957).                              relationship between categories in psychological theories
   Decision bound models (Ashby & Townsend, 1986)                   and graphical model classes can be summarized as:
represent categories as regions of “feature space,” and             • Exemplar-based categories are equivalent to (a subclass
categorization decisions are made by determining the region                of) Bayes nets with the structure shown in Figure 1;
in feature space in which some instance most likely resides         • Prototype-based categories are equivalent to (a subclass
(given some model of perceptual noise). From a                             of) Markov random fields; and
mathematical point-of-view, decision bound models are               • Causal model categories are equivalent to Bayes nets
closely connected to exemplar-based models. Ashby and                      with arbitrary structure.
Maddox (1993) extensively explored the formal connections
between exemplar-based categorization models and decision                                                        F1
bound models in general recognition theory. In general, they
found that there are strong equivalencies between these
model-types, and so for space reasons, we do not explore                                                         F2
                                                                                              U
decision bound models more closely in the remainder of this
paper.
   Prototype-based models (Minda & Smith, 2001, 2002)
                                                                                         (Unobserved)
                                                                                                                É
represent a category by a prototypical instance (i.e., a                                                         Fm
specific point in the relevant feature space). Standard
prototype models are formally similar to exemplar-based                   Figure 1: Bayes net structure for exemplar categories
models with only one exemplar, though the prototypical
instance does not have to be observed. Probabilistic                 Danks (in press) explores some methodological and
categorization responses are then generated using the                theoretical implications of these mathematical equivalencies
Shepard-Luce rule. These models typically contain only               (e.g., enabling connections with causal learning research, or
first-order (observed) features, and so have no interaction          explaining recent experimental results). Our focus here is on
terms. In order to capture the intuition that the prototype can      one particular possibility raised in that chapter. Since
encode (in some sense) a “summary” of the observed data,             categories in the psychological theories correspond to
prototype models can also contain second-order features:             special cases of chain graphs, perhaps all psychological
variables whose value is entirely determined by two first-           categorization is simply Bayesian categorization on various
order features (as in Rehder, 2003a; Rehder, 2003b).                 chain graphs. That is, the extant psychological theories
   Finally, causal model theory (Rehder, 2003a, 2003b;               might simply be particularly salient special cases of
Rehder & Hastie, 2004) holds that some categories are                categorization that arise because Bayes nets and Markov
defined by causal structure: individuals are members of the          random fields are the simplest types of chain graphs.
same category just when their observable features are                   If this suggestion is correct, then people should be capable
generated by the same underlying causal structure.                   of learning categories that are perfectly representable only
Formally, causal structures are represented using Bayes              by a full-fledged chain graph (but not a Bayes net or
nets, and the similarity (of a novel instance to a category) is
                                                                1187

Markov random field). These probability distributions are          task was probabilistic, perfect performance was not
quite complex, and so learning such a category should be           possible. Optimal performance⎯i.e., correctly choosing the
challenging.                                                       more likely category for each case⎯results in 66.67%
                                                                   correct classification (on average).
                         Experiment
                                                                               Table 1: Experimental case distribution
The experiment reported here had a very simple central
question: Can people learn a category whose distribution
(over features) can be represented by a chain graph, but not             F1      F2     F3      F4      Target      Contrast
a Bayes net or Markov random field?                                        0      0      0       0          4            1
                                                                           0      0      0       1          2            1
Experimental Design                                                        0      0      1       0          2            1
Consider the chain graph: F1 Æ F3 ⎯ F4 Å F2. This chain                    0     0       1       1       0 [[1]]      0 [[1]]
graph implies: F1 and F2 are unconditionally independent;                  0      1      0       0          2            2
F1 and F4 are independent conditional on {F3, F2}; and F2                  0      1      0       1          4            2
and F3 are independent conditional on {F4, F1}. Probability                0      1      1       0          1            2
distributions with these independencies cannot be perfectly                0     1       1       1       0 [[2]]      0 [[2]]
represented by any Bayes net or Markov random field. That                  1      0      0       0          2            2
is, any graphical model with only directed edges, or with                  1      0      0       1          1            2
only undirected edges, will necessarily imply strictly more                1      0      1       0          4            2
or strictly fewer independencies than actually obtain in this              1     0       1       1       0 [[2]]      0 [[2]]
probability distribution. This chain graph is the simplest one             1      1      0       0          1            4
that cannot be perfectly represented by a Bayes net or a
                                                                           1      1      0       1          2            4
Markov random field.
   The formal equivalencies between the psychological                      1      1      1       0          2            4
theories of categorization and inference in probabilistic                  1     1       1       1       0 [[4]]      0 [[4]]
graphical models enable us to conclude immediately that
any probability distribution perfectly representable by this        Participants and Materials
chain graph cannot be perfectly represented by a prototype          Eighty-eight Carnegie Mellon students were compensated
or causal model learner. The distribution could be learned          $10 for participation in a group of experiments containing
by exemplar-based category learning, but only by encoding           this one. The full set of experiments took approximately 45
all of the exemplars (twelve in this experiment). A category        minutes to complete.
based on this probability distribution thus provides a critical       The experiment was done on computers. Participants were
test case: none of the current psychological theories of            placed in the role of biologists classifying two novel species
categorization predict that people will easily learn this           of insects. To help them learn how to categorize, they were
category, and only the exemplar theories predict that               presented with a sequence of “already classified” insects.
anything at all could be learned. At the same time, since the       For each case in the learning sequence, participants were
distribution can be perfectly represented with the chain            presented with an image of the four-featured insect and
graph, it is (in principle) learnable.                              asked to classify it as a “Marbock” or “Wermer.” After
   The distribution in the Target column of Table 1 can only        categorizing the insect, participants were given feedback
be perfectly represented by this chain graph, and so serves         about the actual insect name, as well as whether their
as the target category for our learning experiment. For our         answer was correct.
contrast class, we use a multiplicative prototype category in         Participants were told that the learning phase would last
which the central prototype is F1 = F2 = 1, and F3, F4 are          until they were “sufficiently good at classifying these bugs.”
irrelevant. F1 and F2 have equal weights in the category,           Due to the difficulty of the learning task, however, we did
and so cases with only one of the two features occurs half as       not actually use a specific performance criterion. Instead,
frequently as the prototypical cases; cases without either          we simply presented every participant with two complete
feature are one-fourth as frequent. The case distribution for       sets (108 total cases). Within each block of 54 cases, the
the contrast category is also provided in Table 1.                  cases were presented in a randomized order.
   Four of the cases listed in Table 1⎯those with F3 = F4 =           After completing the learning phase, participants were
1⎯were not shown to participants, and thus provide an               presented with all sixteen possible insects (randomized
instrument for measuring category generalization. For               order) and asked “how likely is it that this bug is a
completeness, we have provided the implied counts for               [TARGET]?” where ‘TARGET’ was replaced by the chain
those four cases using numbers in double brackets.                  graph category name. Participants provided a 0-100 rating
   In deterministic learning scenarios, high performance can        using a slider movable only in increments of five (i.e., the
result simply by memorization of salient or common                  rating was functionally on a 0-20 scale). The lower and
exemplars, and not from any understanding of the category           upper ends of the rating scale were respectively labeled
structure. Therefore, we deliberately used a probabilistic          “Cannot be a [TARGET]” and “Definitely a [TARGET].”
categorization task because we wanted to know whether                 We have found in other experiments that participants can
people could learn the underlying distribution. Because the         successfully learn other categories using the same interface
                                                               1188

and functionally similar images (Zhu & Danks, in prep).
Thus, there is no a priori reason to think that the interface        1111
impedes category learning (to any significant degree).               1110
Results and Discussion                                               1101
One measure of successful category learning is performance           1100
in the learning phase prediction task. Since we randomized
                                                                     1011
the presentation order within each 54-case block, we cannot
directly compare performance in smaller intervals (since the         1010
optimal performance might differ across individuals). A
                                                                     1001
histogram of percent correct responses in the second half of
the learning phase is given in Figure 2.                             1000
                                                                     0111
 20
 18
                                                                     0110
 16                                                                  0101
 14
 12                                                                  0100
 10
  8
                                                                     0011
  6                                                                  0010
  4
  2                                                                  0001
  0
   0.
  0.  3
     32           0.
                 0.  4
                    42           0.
                                0.  5
                                   52           0.
                                               0.  6
                                                  62
                                                                     0000
  0.
  0. 34
     36          0.
                 0. 44
                    46          0.
                                0. 54
                                   56          0.
                                               0. 64
  0. 38          0. 48          0. 58             66
                                                                            0          20        40          60      80       100
      Figure 2: Histogram of second half learning rates                                         All (N=88)   Truth
This performance distribution is not significantly different
from normal (p > .70; Shapiro-Wilk test), and the mean is                       Figure 3: Mean ratings for all participants
not significantly different from 0.5 (p > .70; one-sample t-
test). Thus, we have prima facie evidence that participants       At a more fine-grained level, however, the data present a
did not (in general) learn the categories: their performance      subtler picture. If the performance distribution in the
in the second half of the learning phase is not statistically     learning phase is due primarily to chance, then we would
distinguishable from what one would expect from chance            expect to find statistically similar ratings from participants
performance in a population of this size.                         with (i) above-chance performance, and (ii) below-chance
   The sixteen likelihood ratings also provide information        performance. In the same spirit, if any individuals actually
about learning performance: specifically, did any of the          did learn something about the category structures, then we
participants learn (something like) the correct category          would expect that this knowledge would translate into
structures? The mean ratings (error bars indicate 95%             above-chance performance. Thus, if any significant learning
confidence intervals), as well as the correct likelihood for      occurred, then the above-chance performers should exhibit a
observed cases, are shown in Figure 3.                            better understanding of the category structures.
   There are 120 pairwise comparisons of ratings, and so we          To test for increased understanding, we split the
applied the Benjamini & Yekutieli (2001) false discovery          participant population into two groups: (i) those with
rate correction (henceforth, BY-FDR) to the two-sample            second-half learning phase performance ≥ 0.5 (N = 45); and
paired t-test p-values. After applying this correction, there     (ii) those with performance < 0.5 (N = 43). This split was
was only one significant difference in ratings: 1010 vs. 1111     based on learning phase performance, and so we cannot
(p < .05). This analysis of the ratings for the full population   perform any meaningful analyses on differences in
thus supports the previous analysis: people do not seem to        performance.
have learned the categories in this experiment.                      The mean group ratings are shown in Figure 4. On the
                                                                  surface, these groups are not particularly different: None of
                                                                  the likelihood ratings are significantly different (two-sample
                                                                  t-tests with BY-FDR correction).
                                                              1189

                                                                        equally likely to be in either the target or contrast category.
   1111                                                                 However, if one learned only the contrast prototype, then
   1110                                                                 1111 should receive a low rating. Thus, we argue that these
                                                                        ratings are better explained by the hypothesis that
   1101                                                                 participants understood only some of the contrast category
   1100                                                                 structure, than by the hypothesis that they correctly learned
                                                                        (some of) both category structures.
   1011
   1010                                                                                          Conclusions
   1001                                                                 It is notoriously difficult to determine cognitive limits, since
   1000                                                                 there are typically many different reasons why participants
                                                                        might have failed at some task. We thus must be careful
   0111                                                                 about drawing any particularly strong conclusions from this
   0110                                                                 one experiment. Nevertheless, by a variety of measures, it
                                                                        seems that participants did not learn much about the target
   0101
                                                                        (chain graph) category. Learning performance was not
   0100                                                                 statistically distinguishable from chance responses, and the
   0011                                                                 only significant differences in test phase ratings (for only
                                                                        one sub-group) seem to be due to an understanding of the
   0010                                                                 contrast category structure, not the target category structure.
   0001                                                                    There are at least three obvious alternative explanations
                                                                        for the apparent failure of participants to learn the target
   0000                                                                 category. First, participants might not have had sufficient
         0            20           40           60           80
                                                                        experience with the categories. That is, perhaps performance
                                                                        would improve substantially given more observations. Pilot
                   High-performing    Low-performing                    experimental results do not suggest a substantial increase in
                                                                        performance over time, but more investigation is needed.
                                                                           Second, the two categories used in this experiment might
             Figure 4: Mean ratings for subgroups                       be overly similar. If there were more contrast between the
                                                                        categories (i.e., if they were more separable), then people’s
However, although the ratings in the two groups are similar,            performance and understanding might significantly increase.
there is an important difference between them: the ratings of           This concern is particularly salient given the significant
the low-performing group are much more tightly clustered                dependence of target category learning on the structure of
around 50 than the high-performing group’s ratings. The                 the contrast category (Goldstone, 1996).
standard deviation for the low-performing group’s mean                     Third, the measures used here might not have accurately
ratings is only 3.74, and none of the likelihood ratings are            revealed people’s category learning. The “high performers”
different from one another (after BY-FDR).                              were identified using mean correct responses over the last
   In contrast, the high-performers seemed to draw important            54 cases, so people who learned quite late in the sequence
(and accurate) distinctions among some of the cases. The                could have been excluded. Use of a fixed presentation order
standard deviation for the high-performers’ mean ratings is             could enable us to determine more accurately the subset of
9.63, and there are 21 pairwise significant differences in the          individuals who actually learned the target (and contrast)
high-performers’ ratings (after BY-FDR).1                               categories. Alternately, in the test phase rating collection,
   Interestingly, all of the significant differences involve a          we could ask about only crucial cases, rather than all cases.
11** case (principally 1111 and 1101) being judged much                    Given these alternatives, the most definitive evidence that
less likely than a non-11** case to be in the target category.          people are unable to learn categories with these complex
That is, it seems that the high-performers were not learning            statistical structures would be a series of experiments using
the structure of the target category, but rather something              categories that are progressively more difficult to learn (in
about the contrast category structure: namely, that 11**                theory). In particular, the categories would vary along the
cases (i.e., the prototypical cases for the contrast category)          dimension of: complexity of a graphical model that
were highly likely to be in the contrast category, and so               perfectly represents the underlying probability distribution.
received much lower likelihood ratings for the target                   By tracking the changes in participant performance, we
category. The ratings for the 1111 case are particularly                could potentially determine something about the learnability
interesting, since it never appeared in the learning phase.             of various classes of probability distributions. There are, of
Moreover, according to the “true” distributions, 1111 was               course, many studies testing progressively harder
                                                                        categories⎯perhaps most famously the canonical study of
1
  Ordered by BY-FDR corrected p-value, the significantly different      Shepard, Hovland, & Jenkins (1961)⎯but none of those
pairs were: p < .01: {0000, 0011, 0110} vs. 1111, 0000 vs. 1101; p      studies vary the category complexity along the dimension
< .02: {1010, 1011, 1000, 0001, 0010} vs. 1111, {0011, 1000} vs.        proposed here. We are currently developing an appropriate
1101; p < .05: 0111 vs. 1111, {0001, 1001, 1011, 1010, 0010,            series of categories (from the graphical models perspective),
0101, 0110, 0111} vs. 1101, 1000 vs. 1100.                              and we hope that experiments using those categories will
                                                                   1190

help us to understand better the nature of the limits on the       Medin, D. L., & Schaffer, M. M. (1978). Context theory of
category structures that can be learned.                              classification learning. Psychological Review, 85, 207-
                                                                      238.
                   Acknowledgments                                 Minda, J. P., & Smith, J. D. (2001). Prototypes in category
D. Danks was partially supported by grants from the Office            learning: The effects of category size, category structure,
of Naval Research, and the National Aeronautics and Space             and stimulus complexity. Journal of Experimental
Administration.                                                       Psychology: Learning, Memory, & Cognition, 27, 775-
                                                                      799.
                         References                                Minda, J. P., & Smith, J. D. (2002). Comparing prototype-
Ashby, F. G., & Maddox, W. T. (1993). Relations between               based and exemplar-based accounts of category learning
  prototype, exemplar, and decision bound models of                   and attentional allocation. Journal of Experimental
  categorization. Journal of Mathematical Psychology, 37,             Psychology: Learning, Memory, & Cognition, 28, 275-
  372-400.                                                            292.
Ashby, F. G., & Townsend, J. T. (1986). Varieties of               Nosofsky, R. M. (1984). Choice, similarity, and the context
  perceptual independence. Psychological Review, 93, 154-             theory of classification. Journal of Experimental
  179.                                                                Psychology: Learning, Memory, & Cognition, 10, 104-
                                                                      114.
Benjamini, Y., & Yekutieli, D. (2001). The control of the
  false discovery rate in multiple testing under dependency.       Nosofsky, R. M. (1986). Attention, similarity, and the
  Annals of Statistics, 29, 1165-1188.                                identification-categorization relationship. Journal of
                                                                      Experimental Psychology: General, 115, 39-57.
Danks, D. (2004). Psychological theories of categorization
  as probabilistic models (Technical Report No. CMU-               Pearl, J. (1988). Probabilistic reasoning in intelligent
  PHIL-157).                                                          systems: Networks of plausible inference. San Francisco:
                                                                      Morgan Kaufmann Publishers.
Danks, D. (in press). Theory unification and graphical
  models in human categorization. In A. Gopnik & L. E.              Pearl, J. (2000). Causality: Models, reasoning, and
  Schulz (Eds.), Causal learning: Psychology, philosophy,             inference. Cambridge: Cambridge University Press.
  and computation. Oxford: Oxford University Press.                 Rehder, B. (2003a). A causal-model theory of conceptual
Goldstone, R. L. (1996). Isolated and interrelated concepts.          representation     and     categorization.   Journal      of
  Memory & Cognition, 24, 608-628.                                    Experimental Psychology: Learning, Memory, and
                                                                      Cognition, 29, 1141-1159.
Gopnik, A., Glymour, C., Sobel, D. M., Schulz, L. E.,
  Kushnir, T., & Danks, D. (2004). A theory of causal               Rehder, B. (2003b). Categorization as causal reasoning.
  learning in children: Causal maps and Bayes nets.                   Cognitive Science, 27, 709-748.
  Psychological Review, 111, 3-32.                                 Rehder, B., & Hastie, R. (2004). Category coherence and
Griffiths, T. L., & Tenenbaum, J. B. (2005). Structure and            category-based property induction. Cognition, 91, 113-
  strength in causal induction. Cognitive Psychology, 51,             153.
  334-384.                                                         Shepard, R. N. (1957). Stimulus and response
Kruschke, J. K. (1992). Alcove: An exemplar-based                     generalization: A stochastic model relating generalization
  connectionist model of category learning. Psychological             to distance in psychological space. Psychometrika, 22,
  Review, 99, 22-44.                                                  325-345.
Lagnado, D. A., & Sloman, S. A. (2004). The advantage of           Shepard, R. N., Hovland, C. I., & Jenkins, H. M. (1961).
  timely intervention. Journal of Experimental Psychology:            Learning      and    memorization      of   classifications.
  Learning, Memory, & Cognition, 30, 856-876.                         Psychological Monographs, 75, 1-42.
Lauritzen, S. L. (1996). Graphical models. Oxford:                 Spirtes, P., Glymour, C., & Scheines, R. (1993). Causation,
  Clarendon Press.                                                    prediction, and search. Berlin: Springer-Verlag.
Lauritzen, S. L., & Richardson, T. S. (2002). Chain graph          Tenenbaum, J. B., & Griffiths, T. L. (2001). Structure
  models and their causal interpretation. Journal of the              learning in human causal induction. In T. Leen, T.
  Royal Statistical Society, Series B, 64, 321-361.                   Deitterich & V. Tresp (Eds.), Advances in neural
                                                                      information processing systems 13 (pp. 59-65).
Lauritzen, S. L., & Wermuth, N. (1989). Graphical models
                                                                      Cambridge, MA: The MIT Press.
  for association between variables, some of which are
  qualitative and some quantitative. Annals of Statistics, 17,     Waldmann, M. R., & Martignon, L. (1998). A Bayesian
  31-57.                                                              network model of causal learning. In M. A. Gernsbacher
                                                                      & S. J. Derry (Eds.), Proceedings of the 20th annual
Luce, R. D. (1963). Detection and recognition. In R. D.
                                                                      conference of the cognitive science society. Mahwah, NJ:
  Luce, R. R. Bush & E. Galanter (Eds.), Handbook of
                                                                      Lawrence Erlbaum.
  mathematical psychology. New York: Wiley.
                                                                   Zhu, H., & Danks, D. (in prep). Effects of presentation and
                                                                      goal in category learning: Carnegie Mellon University.
                                                               1191

