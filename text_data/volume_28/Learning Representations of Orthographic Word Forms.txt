UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning Representations of Orthographic Word Forms
Permalink
https://escholarship.org/uc/item/9m71q84k
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Kello, Christopher T.
Sibley, Daragh E.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                          Learning Representations of Orthographic Word Forms
                                            Daragh E. Sibley (dsibley@gmu.edu)
                                        Department of Psychology, George Mason University
                                                     Fairfax, VA 22030-4444 USA
                                           Christopher T. Kello (ckello@gmu.edu)
                                        Department of Psychology, George Mason University
                                                     Fairfax, VA 22030-4444 USA
                             Abstract                                the positions in one word relate to the positions in another.
   Presented is an extension of the simple recurrent network         A good alignment scheme should capture information
   (SRN), termed the sequence encoder, which learns fixed-           about the similarities between a pair of word forms. To
   width representations of variable-length sequences. This          illustrate the problem, if the two orthographically similar
   architecture was used to learn orthographic representations       sequences JUMP and JUMPS are aligned by their first
   for nearly 75,000 English words, of which nearly 69,000           letter then they share four of the same letters in the same
   were multisyllabic. Analyses showed that sequence encoder
   representations are shaped by the dependencies among
                                                                     positions; thus for these two sequences, alignment by the
   letters in English word forms that reflect orthographic           first letter suggests they are very similar. However, when
   structure. The model was used to predict participant ratings      aligned by their first letter the two orthographically similar
   of the orthographic legality of pseudowords, and results          sequences BACK and ABACK share 0 letters in the same
   showed that the model accounted for a substantial amount          position. If BACK and ABACK are instead aligned by
   of variance in the ratings.                                       their last letter, they share four letters in the same position.
                                                                     However, when aligned by their last letter, JUMP and
                                                                     JUMPS share no letters. Essentially, aligning word forms
                          Introduction                               by either their first or last letter, or even around vowels
Orthographic structure in English word forms is reflected            (Daugherty & Seidenberg, 1992), fails to represent the
in complex dependencies among letters and positions                  similarities in various word forms.
within each word. The probability of a given letter being               The desire to represent multisyllabic word forms
present in a word may depend upon the presence of other              motivated our efforts to develop a mechanism for learning
letters, resulting in common clusters like GH. The                   representations of sequences that is based on the well-
probability of finding a given letter may vary by position;          known Serial Recurrent Network (SRN) architecture
for instance, the letter Z is rarely found at the end of             (Elman, 1990; Jordan, 1986). The sequence encoder
English words. A representational scheme intended to                 (Kello et al., 2004) learns such representations in the
capture the structure of English word forms must be                  service of learning to encode and decode sequences of
sensitive to these as well as other kinds of dependencies.           variable-length. By virtue of being based on the SRN
Ideally, a representational scheme should be sufficiently            architecture, the sequence encoder representations are
sensitive to the structure in its corpus to differentiate word       shaped by dependencies (e.g., conditional probabilities)
forms by their legality, where legality is determined by a           that exist in its training corpus of sequences. Thus the
word form’s conformity to standard English dependencies.             sequence encoder may be used to learn representations of
   Representational schemes have been used to build                  both mono- and multisyllabic word forms.
models that capture the structure of English word forms,                The sequence encoder, shown in Figure 1, is created by
notably those used in Plaut, McClelland, Seidenberg, and             conjoining two SRNs. The first, called the encoding SRN,
Patterson (PMSP) and the dual route cascaded (DRC)                   receives a sequence of input letters and encodes them into
models of lexical processing (Plaut et al., 1996; Coltheart          a single distributed representation, called the bridge
et al., 2001). From the representational schemes used in             representation. The second, called the decoding SRN,
these two models it was possible to extract a substantial            decodes the bridge representation into a sequence of output
amount of information about the structure in English word            letters.
forms. However, these representational schemes have                     By conjoining these two SRNs, the sequence encoder
restricted models to processing only monosyllabic word               can be trained to code all the letters of a word in order. By
forms. This is due to difficulties inherent in representing          contrast, a standard SRN that is presented with letters one
the position of letters in multisyllabic word forms, arising         at a time and trained to simply predict each subsequent
from the increased variability in length of multisyllabic            letter will learn to code only the information necessary to
word forms.                                                          minimize the prediction error. For many kinds of lexical
   The difficulty with variable-length word forms can be             corpora, it is either not necessary or not useful for the SRN
described as a problem of alignment. Choosing how to                 to code information about all, or even many, of the letters
align word forms of variable-length involves defining how            in order to minimize the prediction error. The consequence
                                                                2158

is that the learned representations will not be shaped to                second bank consisted of two units which denoted whether
code all the letters and their positions.                                the current letter was a consonant or vowel (the vowel
                                                                         letters were A, E, I, O, U and Y).
                 Output Sequence                                         Model architecture. The sequence encoder, as shown in
  Decoding SRN
                                                                         Figure 1, was built from 7 layers of connectionist units.
                                                                         The first layer received an input sequence on a set of 29
                     Hidden                                              localist nodes. Activation for each letter in the sequence
                                                                         was propagated to hidden and context layers, producing
                                           Context                       distributed representations along each layers’ 500 nodes.
                     Bridge                                              Activation flowed from these representations to the Bridge
                              i                                          layer, where they were accumulated to generate a single
  Encoding SRN
                                                                         500 dimension distributed representation for the entire
                     Hidden                                              input sequence. Activation was then propagated thorough
                                                                         a set of hidden and context layers each having 500 nodes,
                                           Context                       to produce a sequence of localist representations along the
                 Input Sequence                                          29 localist nodes of the output layer.
                                                                            The representational layers of the sequence encoder were
                                                                         connected as shown in Figure 1. Any two layers linked by
        Figure 1: The sequence encoder architecture                      a solid arrow were fully connected; an independent
   Most generally, the sequence encoder can be trained to                connection weight extended from each unit in the sending
learn bridge representations for any variable-length                     layer to each unit in the receiving layer. Before training,
input/output mapping. In other words, input sequences do                 these weights were initialized to small random values. The
not have to match output sequences in length or content.                 dashed arrows represent a copy function; the activation
However, the task of learning word forms is most directly                pattern on the sending layer is copied onto the receiving
implemented by training the sequence encoder to                          layer at the end of each time step.
reproduce verbatim each input sequence as an output                         The sequence encoder processed each sequence as a
sequence. This “auto-encoding” task forces the bridge                    number of discrete time steps, first encoding then decoding
representations to code each letter in its position. It has              a sequence. On each encoding time step, the input units
also been shown that connectionist auto-encoder models                   representing a single letter were activated and the encoding
exploit the statistical structure of their inputs (Bishop,               SRN run. Upon completion of the last encoding time step,
1995).                                                                   a final bridge representation was generated. Then, for
   The remainder of this paper examines the sequence                     each decoding time step the bridge representation was run
encoder’s ability to learn representations that are sensitive            through the decoding SRN, producing a pattern of
to the structure of both mono- and multisyllabic word                    activation on the output layer. On each decoding time step,
forms. To this end, a sequence encoder was trained on                    the most active letter unit and consonant/vowel unit was
75,265 English orthographic word forms, varying in length                taken as the model’s response. For a sequence to be
from 1 to 18 letters. The model’s sensitivity to the                     correct, every letter and consonant/vowel value had to be
dependencies in English word forms was assessed by                       produced in the proper order.
testing its ability to account for participants’ ratings of the             The net input to each hidden and output unit was
orthographic legality of pseudowords.                                    calculated as the dot product of the incoming weight and
                                                                         activation vectors. The activation of each hidden unit was
                     Modeling Method                                     computed as the hyperbolic tangent of its net input. This
Training corpus. Representations were learned for                        function is sigmoidal with asymptotes at -1 and 1. The
74,265 English words, 68,945 of which were multisyllabic.                activation of each output unit was computed as the
These words were chosen by intersecting the CMU                          exponential of its net input, normalized across the bank of
pronunciation         dictionary         (available      at              output units. This normalized exponential function is
http://www.speech.cs.cmu.edu/cgi-bin/cmudict) and the                    appropriate because letters were coded as localist
Wall Street Journal Corpus (Marcus et al., 1993), then                   representations and so could be interpreted as the
discarding homographs (for purposes outside the scope of                 probability of the output of each letter (Rumelhart, 1995).
the work discussed here) and words with more than 18                        Weight updates were made after every batch of 50
letters.                                                                 training examples chosen at random from the corpus. For
Input and output representations. Each letter in a                       each element of each sequence, error derivatives were
sequence was coded by activating a single unit in two                    calculated and back-propagated up to the bridge
different banks of nodes. The first bank consisted of 26                 representation. This error signal was used to train the
units, each representing a single letter (A through Z) with              decoding SRN, with a learning rate of 0.00005. Next, the
one additional unit representing the end of a sequence. The              error signal accumulated on the bridge layer, for the entire
                                                                  2159

length of the sequence, was back-propagated through time                                  Model Results
while the sequence was reset onto the input layer. Weight            The sequence encoder correctly processed 89% of the
updates were then made on the encoding SRN with a                    words in its training corpus, including 88% of the 68,945
learning rate of 0.00000001. A smaller learning rate was             multisyllabic words. This performance demonstrated the
needed for the encoding SRN because of the accumulation              sequence encoder’s ability to generate representations in
of error derivatives on the bridge units. Learning appeared          the service of auto-encoding both monosyllabic and
to reach asymptote after 250,000 epochs of training, and             multisyllabic word forms. The sequence encoder correctly
was halted.                                                          processed 76% of the legal nonwords, including 75% of
                                                                     the multisyllabic legal nonwords. Finally, the model
Training and testing the sequence encoder. A coarse                  correctly auto-encoded only 24% of the illegal nonwords.
test of the sequence encoder’s sensitivity to structure was          The poor performance of the model on illegal nonwords
achieved by examining the model’s ability to auto-encode             relative to legal nonwords is clear evidence that the auto-
3 different types of sequences. These three types of                 encoding task drove the model to discover and exploit
sequences embody 3 different levels of legality, so a model          dependencies among letters that reflect the structure of
sensitive to the structure of English word forms should              English word forms.
process these different sequences with different degrees of            The results of this simulation show that the sequence
success.                                                             encoder provides a viable means of learning
   First, sensitivity to the structure of English should result      representations for large numbers of monosyllabic and
in correct auto-encoding of the trained words. Second, to            multisyllabic word forms. Representations were shaped by
demonstrate that the model discovered general structure              the structure of English word forms, as evidenced by the
instead of memorizing the specific sequences in the                  selective generalization to legal but not illegal nonwords.
training corpus, the model was assessed for its ability to           This shaping occurred through the learning of
correctly auto-encode “Legal Nonwords”.               A legal        dependencies among letters and positions within the
nonword is a sequence of letters that conform to the                 sequences. This sensitivity to dependencies enabled the
structure of English word forms, but are not in the training         model to differentiate between words based on their
corpus (i.e., pseudowords). Correct performance of these             legality, despite never having been explicitly trained to
items required the model to generalize information about             perform this task.
the dependencies in the training corpus. Third, to test
whether the model learned about the structure of English
word forms and not how to auto-encode arbitrary
                                                                                 Pseudoword Legality Ratings
sequences, we assessed its performance on sequences of               The previous analysis suggests that the sequence encoder
letters that did not conform to the structure of English, here       was able to learn some amount of structure in both
called “illegal nonwords”. To achieve this analysis, we              monosyllabic and multisyllabic word forms. However, the
needed to generate legal nonwords and illegal nonwords.              analysis provided little information about the extent to
   To create legal nonwords, we took each trained word               which the bridge representations became sensitive to
form and replaced one of its letters. These replacements             different sorts of dependencies. Further, it did not show
yielded new sequences which were not in the training                 whether the dependencies captured by the sequence
corpus.      To make the new sequences legal, each                   encoder corresponded to those to which real language users
replacement letter was chosen from a word that shared the            are sensitive.     These questions were addressed by
letters surrounding the replaced letter. To illustrate, the          evaluating the model’s sensitivity to orthographic structure
word SPORTY could be changed into SPARTY by                          relative to behavioral data. In particular, we tested whether
replacing the sequence SPORT with SPART, where                       the model’s success at processing novel word forms
SPART occurs in the word SPARTAN. In this example, a                 predicted participants’ judgments about the legality of a
letter is replaced by a new legal letter, determined by              sequence of letters. We then examined the dependencies to
matching the 4 flanking letters.            For each of the          determine which might give rise to the sequence encoder’s
replacements used to create a legal nonword, a single letter         predictive power.
was chosen at random and replaced with another letter of               The sequence encoder’s sensitivity to the structure of
the same class, vowel or consonant. Three letters to either          English word forms can be assessed by the model’s ability
side were matched unless the beginning or end of the                 to auto-encode novel sequences. Successful auto-encoding
sequence was encountered.                                            of a novel word form requires the model to generalize
   To create illegal nonwords, the letters in each of the            information about the structure of the learned word forms.
legal nonwords were scrambled. An inspection of the                  A failure to auto-encode a novel sequence reflects the
resulting illegal nonwords revealed that this method                 model’s inability to represent the novel sequence using the
generally produced sequences that violated the structure of          information it acquired about the structure of English. As
English word forms.                                                  such, the model’s success in processing each novel
                                                                     sequence should reflect the legality of the sequence. The
                                                                     following analysis tested the extent to which the sequence
                                                                2160

encoder’s performance predicted participants’ judgments               To formulate a more sensitive measure of the model’s
of orthographic legality.                                          generalization it was necessary to measure the model’s
   The comparison of the model and behavioral data was             performance beyond a binary (correct or incorrect)
performed with word forms within a restricted range of             distinction. Instead, a continuous measure of the sequence
legality; none of the sequences in this analysis were either       encoder’s performance was generated for each of the 600
typical words (like FOOTBALL) or completely illegal                words. This was possible because the model tended to
nonwords (like TLLBAOOF). Because of the restricted                activate target nodes less than the maximum allowed.
range of legality, the model needed to differentiate                  One conceptual interpretation of a connectionist model’s
between subtle differences in legality, in order to account        output is that the activation of each output node reflects the
for any variance in behavioral responses. This produced a          probability (nodes assume a value between 0 and 1) of
more rigorous test of the sequence encoder’s abilities.            producing the represented feature. So the probability of
                                                                   producing a fully correct sequence could be calculated as
                           Method                                  the product of the activation on each of the target nodes.
                                                                   However, because many of the model’s outputs were
The sequence encoder’s ability to generalize was compared
to participants’ judgments about the legality of 600               asymptotically correct, a logarithmic transform was used to
nonwords. The nonwords for this analysis were developed            provide a more sensitive measure. A continuous measure
                                                                   of the model’s performance was created
using the previously described method for generating legal
nonwords. To generate word forms distinct from real                                   P = − log(1 − ( Πai )) ,
English, the procedure previously discussed was run                where P represents the model’s performance on a given
iteratively, 5 times on each word. This resulted in 600            sequence and ai is the activity on target node i in the
relatively legal nonwords, each of which corresponded to a         sequence (the activation on a perfectly produced target
real word with up to 5 of its letters replaced.                    node was 1). For the nonwords used in this analysis, P
   As would be expected, the nonwords created by                   assumed a value between 0 and 4, (for all but 3 discarded
replacing up to 5 letters in each word tended to be less           outliers) with 0 being the worst produced word form and 4
legal than those produced by replacing a single letter. This       being the best produced.
resulted in decreased performance on these new legal                  Nine participants judged each of the 600 legal nonwords
nonwords relative to those previously discussed - 71%              for their conformity to the structure of English word forms.
instead of 76% correct, respectively. Also, the shorter            Participants were instructed to consider the likelihood that
word forms created by this procedure tended to be more             each sequence was an English word they were unfamiliar
legal than the longer word forms. The final 600 sequences          with. Legality judgments for each of the 600 word forms
were chosen by randomly selecting 150 of the new legal             were made on a 5 point scale.
nonwords from the 4 most frequent lengths; 5, 6, 7, and 8
letters. Length was restricted to this range to reduce any                                    Results
confounding length effects introduced by the procedure
used to make nonwords. This was necessary because                  Table 1 shows the 10 word forms for which P was closest
iteratively replacing 5 letters would have a different effect      to each of the integers; 0, 1, 2, 3, and 4. Inspection of this
                                                                   table yields two observations, which are corroborated by
on a word form composed of 3 letters than one composed
of 13 letters. Of the final 600 new legal nonwords, 571            later analysis. First, the model generated a reasonable
were multisyllabic. A representative sample of the word            estimation of the legality of nonwords. That is, word
                                                                   forms in the left most column were less regular than those
forms used in this analysis is shown in Table 1.
                                                                   in the right most column. Second, the length of a sequence
  Worst performance                         Best performance
      0             1              2            3        4         interacted with the model’s proxy for legality - sequence
  sriteanf       roadham      wingins        broins    roins       length accounted for 17.2% of the variance in P. However,
  voruranc      baclater        ronte       shiriner   teres
                                                                   sequence length also accounted for 5.1% of the variance in
                                                                   the participants’ average responses. Because the model
   aterkan       dertlee        prare         macee    lanter
                                                                   and participants responded differentially to items of
   mislerl      naintira       battly         barss     cears      different length, we can partially attribute the length by
   crultie       crerley      tonkies        bardey    stors       legality interaction to an anomaly in the procedure used to
  dexcuges       bencham      hantend         gaged    funter      generate the legal nonwords.
   debsane      overetle      dleised         cerel     lired
  lotingke      gertisos       stadio        penscs    panen
     echoc      corleder      boalans        jurded     rones
    intail        tatzir        mause        sloners    beins
    Table 1: Legal nonwords for which P, a measure of the
sequence encoder’s performance, was closest to each of the
                   integers; 0, 1, 2, 3, and 4.
                                                              2161

                                  1.9                                                                        5
 Sequence encoder's performance
                                  1.8                                                                       4.5
                                  1.7
                                                                                                             4
                                  1.6
                                                                                     Participant's Rating
                                  1.5                                                                       3.5
                                  1.4
                                                                                                             3
                                  1.3
                                  1.2                                                                       2.5
                                  1.1
                                                                                                             2
                                  1.0
                                  0.9                                                                       1.5
                                        1   2            3           4   5
                                                Participant's Rating                                         1
                                                                                                                  0   1                 2                  3   4
 Figure 2: The sequence encoder’s performance on word                                                                     Sequence encoder's performance
forms receiving different legality ratings from participants
                                                                                       Figure 3: Scatter plot of participant and model responses
   To check the experimental paradigm, Cronbach’s alpha
was calculated for the 9 participants’ responses. At 0.87, it                                                               Conclusions
shows the participants’ judgments were largely internally                           The present work examines the sequence encoder’s ability
consistent. We then tested the correspondence between the                           to generate representations that are sensitive to the
model’s performance on novel sequences (P) and the                                  structure of orthographic word forms. The sequence
participants’ ratings for each word form. This was                                  encoder learned representations for more than 75,000
achieved by calculating the model’s average performance                             English words, varying in length from 1 to 18 letters, with
on each of the 5 ratings provided by each participant.                              nearly 69,000 of the learned words being multisyllabic.
Figure 2 was created by averaging this quantity across                                 In processing multisyllabic words, the sequence encoder
participants. As depicted in this figure, each of the 5                             overcame problems associated with representing positions
different ratings provided by a participant tended to be                            in variable-length sequences. While other connectionist
associated with the production of a different response                              systems have been used to process sequences, this
pattern from the model, F (4, 32) = 38.64, p <.001. This                            sequence encoder is particularly suited to learning
statistically significant capacity of the sequence encoder to                       representations of lexical stimuli, an ability which may be
account for behavioral data suggests it has developed a                             useful in future models of lexical processing. To elaborate
sensitivity to the structure of mono and multisyllabic                              on this point, we will briefly contrast the sequence encoder
English word forms. Further, the sequence encoder’s                                 with three related connectionist systems.
sensitivity to dependencies is in some way similar to the                              The sequence encoder is an extension of the SRN, which
sensitivity of native English readers.                                              was designed to process sequences while performing the
   To depict the model’s ability to predict human behavior,                         prediction task (Elman, 1990; Jordan, 1986). These SRNs
we used a regression analysis to compare the model’s                                maintain a representation of prior elements in order to
performance on each of the legal nonwords with the                                  predict an upcoming element. Prediction is accomplished
average of the 9 participants’ response to each item.                               by using whatever conditional probabilities exist in the
Figure 3 is a scatter plot showing the relationship between                         trained sequences (Elman, 1995). The sequence encoder
the sequence encoder’s proxy for legality and the average                           inherits the SRN’s sensitivity to conditional probabilities,
of the participants’ judgments of legality, by item. As                             but goes beyond the standard SRN by creating a task that
depicted, the model predicted 14.5% of the variance in the                          explicitly forces representations to code all the elements of
participants’ judgments. This demonstrates that the model                           a sequence along with their positions. The sequence
became sensitive to some dependencies to which the                                  encoder thus becomes sensitive to conditional probabilities
participants were also sensitive. This suggests that the                            among all the letters of a word, which is also generally true
sequence encoder discovered some structure in English                               of skilled readers (see McClelland & Rumelhart, 1981).
orthography similar to that discovered by a skilled English                            Another connectionist system for processing variable-
reader.                                                                             length sequences is the recurrent auto-associative memory
                                                                                    (RAAM; Pollack, 1990). In RAAMs, connectionist units
                                                                                    are trained to pack and unpack representations in a strictly
                                                                                    recursive fashion. By varying the number of recursive
                                                                                    steps, the model can input or output a variable-length
                                                                                    sequence. However, the strictly recursive nature of this
                                                                                    process makes it ill-suited to the problem of learning word
                                                                             2162

forms.      The RAAM architecture imposes a fixed                    Bishop, C. M. (1995). Neural networks for pattern
hierarchical structure on each sequence, while English             recognition. Oxford: Oxford University Press.
words are characterized by a more rough hierarchical                 Botvinick, M., & Plaut, D. C. (in press). Short-term
organization that is free to vary from one word to the next        memory for serial order: A recurrent neural network
(Andrews et al., 2004).                                            model. Psychological Review.
   A recently-developed connectionist system uses fully              Coltheart, M., Rastle, K., Perry, C., Langdon, R. &
recurrent networks to learn representations of variable-           Ziegler, J. (2001). DRC: A dual route cascaded model of
length sequences (Botvinick & Plaut, in press). In this            visual word recognition and reading aloud. Psychological
architecture, a sequence is input to the model one element         Review, 108, 204-256.
at a time, much like the sequence encoder, until an output           Daugherty, K., & Seidenberg, M. S. (1992). Rules or
                                                                   connections? The past tense revisited. Proceedings of the
cue triggers the model to reproduce the sequence. This
                                                                   14th Annual Confrence of the Cognitive Science Society.
architecture was developed as a model of serial recall, a
                                                                   Hillsdale, NJ: Lawrence Erlbaum Associates. Pp. 259-264.
task with notable differences from lexical processing, and           Elman, J. L. (1990). Finding structure in time. Cognitive
is therefore not well-suited to our current needs. In              Science, 14(2), 179-211.
particular, the model requires extensive training, and it is         Elman, J. L. (1995). Language as a dynamical system. In
unclear how well it would scale and generalize in much             R.F. Port & T. van Gelder (Eds.), Mind as Motion:
larger linguistic domains like the one discussed here.             Explorations in the Dynamics of Cognition. Cambridge,
   Current models of the lexical system incorporate only           MA: MIT Press. Pp. 195-223.
monosyllabic words, because their representational                   Jordan, M. I. (1986). Serial order: A parallel distributed
schemes cannot easily incorporate multisyllabic words              processing approach (No. 8604 ICS Technical Report):
(Coltheart et al., 2001; Plaut et al., 1996). The sequence         University of California at San Diego, La Jolla, CA.
encoder’s ability to learn representations for large numbers         Kello, C. T. (in press). Considering the junction model
of multisyllabic words could be exploited to develop new           of lexical processing. To appear in S. Andrews (Ed.), All
models of impaired and unimpaired lexical processing (see          about words: Current research in lexical processing.
Kello, in press). These models could process a number of           Sydney: Psychology Press.
words approaching an adult’s vocabulary and could                    Kello, C. T., Sibley, D. E., & Colombi, A. (2004). Using
address large quantities of behavioral data regarding              simple recurrent networks to learn fixed-length
multisyllabic words (Balota et al., 2002). These are some          representations of variable-length strings. In Proceedings
of the directions that are currently being pursued with the        of the AAAI Symposium on Compositional Connectionism.
sequence encoder.                                                  Washington, DC.
                                                                     Marcus, M., Santorini, B., & Marcinkiewicz, M. A.
                                                                   (1993). Building a large annotated corpus of English: The
                    Acknowledgements                               Penn Treebank. Computational Linguistics, 19, 313-330.
   This work was funded in part by NIH Grant MH55628,                McClelland, J. L., & Rumelhart, D. E. (1981). An
and NSF Grant 0239595. The computational simulations               interactive activation model of context effects in letter
were run using the Lens network simulator (version 2.6),           perception: I. An account of basic findings. Psychological
written by Doug Rohde (http://tedlab.mit.edu/~dr/Lens).            Review, 88, 375-407.
We thank David Plaut for collaborations on precursors to             Pinker, S. & Prince, A. (1988) On language and
this work.                                                         connectionism: Analysis of a parallel distributed
                        References                                 processing model of language acquisition. Cognition, 28,
                                                                   73-193.
   Andrews, S., Miller, B. & Rayner, K. (2004) Eye                   Plaut, D. C., McClelland, J. L., Seidenberg, M. S., &
fixation measures of morphological segmentation of                 Patterson, K. (1996). Understanding normal and impaired
compound words: There is a mouse in the mousetrap.                 word reading: Computational principles in quasi-regular
European Journal of Cognitive Psychology, 16(2), 285-              domains. Psychological Review, 103(1), 56-115.
311.                                                                 Pollack, J. B. (1990). Recursive distributed
   Balota, A. A., Cortese, M. J., Hutchison, K. A., Neely, J.      representations. Artificial Intelligence, 46, 77-105.
H., Nelson, D., Sompson, G. B., et al. (2002). The English           Rumelhart, D. E., Durbin, R., Golden, R., & Chauvin, Y.
lexicon project: A web-based repository of descriptive and         (1995). Backpropagation: The basic theory. In Chauvin,
behavioral measures for 40,481 English words and                   Yves (Ed); Rumelhart, David E (Ed) (1995)
nonwords. http://elixicon.wustl.edu:                               Backpropagation: Theory, architectures, and applications
                                                                   Developments in connectionist theory (pp 1-34).
                                                              2163

