UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Nonsense and Sensibility: Inferring Unseen Possibilities
Permalink
https://escholarship.org/uc/item/7t32n7z0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Kemp, Charles K.
Schmidt, Lauren A.
Tenenbaum, Joshua B.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                Nonsense and Sensibility: Inferring Unseen Possibilities
                       Lauren A. Schmidt, Charles Kemp & Joshua B. Tenenbaum
                   Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology
                                              {lschmidt, ckemp, jbt}@mit.edu
                           Abstract                                gories do not overlap arbitrarily, but also seem to follow
                                                                   structural rules. Sommers (1971) proposed “the M con-
   How do we distinguish the sensible yet unlikely (blue ba-
   nanas) from the nonsensical (hour-long bananas), given          straint”: categories of objects are organized in a strict
   observations only of what is true in the world (e.g., yel-      hierarchy, and predicates must span subtrees of the hi-
   low bananas)? Judgments like these may be supported             erarchy (thus preventing any “Ms” within the tree). See
   by the M constraint: the assumption that ontological            Figure 1 for an example of such a predicability tree, and
   categories are organized into a predicability tree, and         how it governs the set of pairs which are sensible to-
   that properties apply to different subtrees within this hi-
   erarchy. We provide a computational theory that shows           gether, which in turn governs which pairs may be true.
   how the M constraint can be used to acquire predicabil-            This hierarchical constraint could be extremely useful
   ity trees given observations only of what is true. We also      in making inferences about what is sensible based on
   suggest how the M constraint itself could be learned.           limited evidence. If we know that soccer games cannot
                                                                   sensibly be blue but can be an hour long, and bicycles
   A friend comes home from the market exclaiming that             can be blue but cannot sensibly be an hour long, then
she has just seen the most interesting banana. She tells           according to the M constraint, bananas cannot sensibly
you to guess what was so interesting about it. Which of            have both properties. Knowing this, all we need to do
the following questions are you most likely to ask? “Was           to make a quick inference is figure out whether bananas
the banana blue?” “Was the banana the shopkeeper’s                 are more like bicycles or soccer games. Observations that
fault?” “Was the banana an hour long?” Probably you                bananas and bicycles can both be yellow and green, as
are more likely to ask the first question. You may never           well as sharing other properties, can help us infer that
have seen or heard of a blue banana before, but it seems           bananas could sensibly be blue, but not an hour long.
like one might exist somewhere, and such a banana would               Keil (1979) provided some evidence that people do
indeed be noteworthy. The other two questions, on the              follow the M constraint in reasoning about the world.
other hand, do not make sense. Bananas are not the kind            When asked to judge which statements about predicate-
of thing that can be someone’s fault or an hour long.              object pairs were sensible, adults provided sets of an-
   How can we judge so quickly that it is far more sen-            swers which showed a strict hierarchical organization.
sible for bananas to be blue than an hour long, when               This was true for children as well, though their pred-
we have seen no direct evidence in the world regard-               icability trees were far simpler than those of adults.
ing either blue or hour-long bananas? We may never                 Keil also showed that children can make quick infer-
have seen a blue banana, but we have seen many other               ences based on hierarchical relationships of categories
blue things—blueberries, bicycles, and bedspreads, for             and predicates. Most interestingly, Keil proposed that
example—which share many properties with bananas.                  the M constraint is a part of the innate core knowledge
They are all visible objects, and we can touch them,               that guides children in learning about the world.
move them, or give them to our friends. If these objects              If the M constraint (or a weaker statistical bias) does
can sensibly be blue, then perhaps it would be sensible            exist as a psychological reality, how could we use it to
for bananas to be blue as well. None of the other blue ob-         learn what is sensible? We propose a formal model that
jects is an hour long, however, which might make us more           incorporates the M constraint and can discover a predi-
doubtful that bananas could sensibly have this property.           cability tree given limited evidence about what is true in
   Kinds such as physical objects and events are examples          the world. Our model takes the M constraint as given;
of ontological categories. The properties that apply to            however, we also describe how Bayesian model selection
the members of the ontological categories (“blue,” “hour           can be used to infer that a model with the M constraint
long”) are also known as predicates. When a predicate              accounts better for the observed data than an alterna-
can sensibly be applied to an object, and a truth value            tive model with no hierarchical constraint. We therefore
can be assigned (“Bananas are usually blue” is sensible,           suggest that the M constraint need not be innate.
though false), that predicate is said to span the object
(Keil, 1979).                                                         A Structured Approach to Sensibility
   Predicates do not appear to span objects arbitrarily.           Assume that we are working with a fixed collection of ob-
Instead, predicates seem to cluster together and apply             jects and predicates. We develop a computational the-
to whole categories of objects. In addition, these cate-           ory that assumes that the objects and predicates are
                                                               744

 a)                                                                                b)                                 c)
                                                                                              is on the news               is on the news
                                                          is on the news                      is mentioned in songs        is mentioned in songs
                                                      is mentioned in songs                   is too heavy to lift         is too heavy to lift
                                                                                              is green                     is green
                                                                                              is an hour long              is an hour long
                                                                                              was canceled                 was canceled
                                                                                              is tall                      is tall
                                    is too heavy to lift   is an hour long     idea           has sharp corners            has sharp corners
                                                                               love           forms puddles                forms puddles
                                          is green          was canceled                      is viscous                   is viscous
                                                                                              is still growing             is still growing
                                                                                              is dead                      is dead
                                                                                              is broken                    is broken
                      is tall          forms puddles       soccer game                        is made of wood              is made of wood
                has sharp corners         is viscous        conference               idea
                                                                                      love
                                                                             soccer game
                                                                               conference
is still growing     is broken     rock       milk                                   rock
                                                                                  iceberg
    is dead      is made of wood iceberg      mud                                     milk
                                                                                     mud
                                                                                       girl
                                                                               houseplant
    girl          park bench                                                  park bench
 houseplant        trumpet                                                        trumpet
Figure 1: (a) A predicability tree. The predicates located at a node of a tree span all the objects at that node and
all those in the subtree below it. (b) The corresponding predicability matrix, R. Squares in gray indicate predicable
pairs. (c) A truth matrix, T , indicating the predicable pairs that are actually true.
organized into a predicability tree (Figure 1a). Each                         rameter γ (Aldous, 1985). This prior on zp assigns some
predicability tree can be represented as a predicability                      probability to all possible partitions of the predicates,
matrix, R (Figure 1b). Some of the predicable pairs are                       including the partition where each predicate is assigned
also true, and we represent these pairs using a truth ma-                     to its own cluster, and the partition where all the pred-
trix, T (Figure 1c). We assume that a learner observes                        icates belong to the same cluster. The prior, however,
a set of predication events (such as a yellow banana or                       favors partitions that use small numbers of clusters.
blue bicycle), where each event is an occurrence of a                            Suppose that |zp | is the number of clusters used by
predicate-object pair. The data can be arranged into a                        partition zp . We assume that B is drawn uniformly from
frequency matrix D, where Dij indicates the number of                         all of the |zp ||zp |−1 possible labeled trees with |zp | nodes,
times that predicate i was paired with object j. Our                          and zo is generated by dropping each object at random
model assumes that the large entries in D correspond                          onto one of the nodes of B:
mainly to pairs that are true: more precisely, our model
assumes that each predicate-object pair is observed with                                                               1
                                                                                                     P (B|zp ) =                            (2)
a frequency that depends on the truth of the pair and                                                             |zp ||zp |−1
the individual popularities of both the predicate and the                                                           1
object. Our ultimate goal is to work backward from the                                              P (zo |zp ) =                           (3)
                                                                                                                  |zp |n
data D to learn the predicability matrix R and truth
matrix T .                                                                       To apply Equation 1, we also need to calculate
   We take a generative approach, and define a joint dis-                     P (T |R), the probability of the truth matrix given the
tribution P (D, T, R) = P (D|T, R)P (T |R)P (R). This                         predicability matrix. We assume that the truth of each
generative model can then be inverted to search for the                       predicable pair is determined by flipping a coin with bias
R and T with maximum posterior probability:                                   η:
          P (T, R|D) ∝ P (D|T, R)P (T |R)P (R).                   (1)                           |T |
                                                                                                  η (1 − η)|R|−|T | , if T ⊂ R
                                                                                    P (T |R) =
   To apply Equation 1, we need a prior P (R) on predi-                                           0,                   otherwise
cability matrices. We assume that predicability satisfies                     where |R| is the number of predicable pairs and |T | is
the M constraint, and that each candidate R corresponds                       the number of true pairs.
to a tree like the example in Figure 1. Each predicability                       Finally, we assume that each predication event v, con-
tree can be parameterized as a triple (zo , B, zp ) where zp                  sisting of an occurrence of an object-predicate pair (o, p),
is a partition of the predicates, B is a tree built using                     is drawn from a distribution given by
the predicate clusters in zp , and zo is an assignment of
the objects to the nodes in B. Each triple (zo , B, zp )                                            P (o, p) ∝ eπp +πo +λTpo                (4)
uniquely specifies a tree with objects and predicates at-
tached, which in turn uniquely specifies a predicability                      where πp and πo are parameters representing the popu-
matrix R. The prior probability of any matrix R is                            larities of predicate p and object o, and λ represents the
      P (R) = P (zo , B, zp ) = P (zo |zp )P (B|zp )P (zp )                   extent to which we penalize violations of the truth ma-
                                                                              trix T . If T is uniformly one (i.e. every object-predicate
  We compute P (zp ) by assuming that zp is generated                         pair is both sensible and true) or λ is zero (i.e. we do not
by a Chinese restaurant process with concentration pa-                        penalize violations of matrix T , then the model reduces
                                                                        745

to a simple joint distribution where P (o, p) ∝ P (o)P (p),                          Learning the right tree
P (o) = eπp and P (p) is similarly defined.                             We compared the performance of our model and two
    The popularity parameters could potentially be                      alternate approaches on data sets meant to approximate
learned, but we fix them using the frequencies of objects               real world data.
and predicates in the matrix D:
                                                                        The data sets We generated two data sets of observed
                              πo ∝ log |πo |                    (5)     predication events based on a tree with the same struc-
                                                                        ture as that shown in Figure 1. Our tree had six pred-
where |πo | is the proportion of events in D that involve
                                                                        icates located at each node, and three objects located
object o. πp is defined similarly.
                                                                        at each node except for one internal node, which was
    The probability of the entire dataset is the product of
                                                                        empty, yielding a total of 42 predicates and 18 objects.
the probabilities of the individual predication events, v :
                                    Y                                   This structure yielded the predicability matrix shown at
                      P (D|T ) =           P (v|T ).            (6)     the top of Figure 2.
                                   v∈D                                     We then set parameters π, λ, and η, and sampled
                                                                        data according to our model. We sampled |π| from a
Searching for predicability matrices                                    uniform distribution. λ was set to 10. We used truth
We run a stochastic search to identify the R =                          matrices with η = {0.3, 0.5, 0.7, 0.9}. For each truth ma-
(zo , B, zp ), T , λ, and η that maximize P (R, T |D). The              trix, we generated three different data sets, differing in
search problem is difficult, since the score for a matrix               number of samples drawn, N , where N took the val-
R couples zo and zp : in other words, changing zp is un-                ues {1000, 10000, 100000}. 1% of the samples were drawn
likely to improve the score unless zo is changed as well.               uniformly over all pairings, creating some noise in the
We overcome this issue by integrating out the object                    data. The average number of times each true predication
locations zo , and searching for the B, zp and T that                   event was seen ranged from 3.63 to 1089 across condi-
maximize                                                                tions; values are shown in Table 1. The generative pro-
                                                                        cess and resulting data sets are illustrated in Figure 2.
              P (B, zp |D) ∝ P (D|B, zp )P (B, zp )             (7)
                                                                        The comparison methods We sought a comparison
Suppose that Dj is the set of predication events that                   model that would also learn a tree structure based on
involve object j. Then                                                  clusters of objects and predicates and allow for inference
                                   Y                                    about predicabilities of unseen pairs. Hierarchical clus-
                 P (D|B, zp ) =           P (Dj |B, zp )
                                                                        tering is a popular method for learning tree structures,
                                     j
and                                                                     however, the standard algorithm is insufficient for our
                         |zp |
                                                                        needs. The same is true of Bayesian tree learning tech-
                         X     X                                        niques proposed in the past (Kemp, Perfors & Tenen-
       P (Dj |B, zp ) =           P (Dj , tj , zoj = k|B, zp )  (8)     baum, 2004). Instead of finding hierarchies based on
                         k=1 tj                                         object categories, the trees recovered by both methods
where zoj is the location of object j in the tree, and tj is            branch maximally such that each object is alone at a leaf
a vector indicating which predicates are true of objects.               node. Therefore, neither method can be used to identify
Computing the sum in Equation 8 is intractable, and we                  the large-scale ontological categories that we hope to re-
approximate it as follows:                                              cover. Additionally, the predicates are not clustered to-
                        |zp |
                        X                                               gether or placed at nodes within the tree.
     P (Dj |B, zp ) ≈         P (Dj |t∗j (k))P (zoj = k|B, zp ) (9)        Our comparison method is a modified version of hier-
                        k=1                                             archical clustering which overcomes some of the above
                                                                        shortcomings and generates predicability predictions
where t∗j (k) is the truth vector that maximizes P (tj |zoj =           that serve as a comparison for those of our model. We
k, B, zp ). Equation 3 implies that P (zoj = k|B, zp ) =                used a standard hierarchical clustering algorithm to find
  1
|zp | . We do not include the details here, but if we condi-
                                                                        a hierarchical organization of objects based on the fre-
tion on the number of times each object appears in the                  quency vectors of their occurrences with predicates in
dataset, it is straightforward to compute P (Dj |t∗j (zoj =             the data. We then developed a metric for scoring pos-
k)): in particular, we avoid having to compute the nor-                 sible predicate locations and then placed each predicate
malizing constant of the distribution in Equation 5.                    at the best scoring node in the tree.
    Let us call (B, zp ) an incomplete tree: that is, a pred-              To score a potential predicate location, we looked at
icability tree without the objects attached. Using Equa-                what data would be predicted by that predicate being
tion 7, we run a search by starting from a random in-                   placed at that node. Any objects not spanned by the
complete tree and considering proposals that move the                   predicate were predicted never to occur. Any objects
predicates around the tree, possibly creating new nodes                 spanned by the predicate were predicted to occur with
in the process. For each incomplete tree, we use an ap-                 that predicate a number of times proportional to the
proximation similar to the idea behind Equation 9 to                    product of the object frequency and the predicate fre-
compute a candidate pair (T, R), where T and R are the                  quency in the data set. The score of the location was
matrices that maximize P (T, R|B, zp , D). At the end                   proportional to the inner product of the normalized pre-
of the search, we return the best candidate pair encoun-                dicted and actual data vectors.
tered, where each pair is scored according to Equation 1.                  In addition to this comparison method, we also tried a
                                                                    746

                                                                  Predicability
            Truth (η = 0.3)                       Truth (η = 0.5)                        Truth (η = 0.9)
           Data
   M constraint
    Hierarchical
      clustering
Figure 2: Data sets and results for the M constraint model and the hierarchical clustering model. The three truth
matrices were generated by selecting random subsets of predicable pairs. The input for each model is a frequency
matrix generated by sampling pairs from the truth matrix. Three data sets with increasing numbers of observations
are shown for each truth matrix. The final two rows show predicability judgments made by the two models.
simple thresholding method which predicted that exactly           on all data sets; however, with the sparsest data sets
those pairs that had been seen in the input data one or           none of the algorithms perform well. In those cases, the
more times were predicable.                                       M constraint drastically overgeneralizes, while the other
Results The predicability matrices recovered by the M             models do the opposite. Interestingly, the simple thresh-
constraint and hierarchical clustering models are shown           olding model also outperforms the hierarchical clustering
in Figure 2 (the predicability matrix from the other com-         model on all but some of the sparsest data sets.
parison method is not shown, as it is simply a thresh-               We would suggest that the M constraint model is do-
olded version of the data matrix). The binary matrices            ing something both intelligent and psychologically plau-
recovered can be said to have high precision whenever             sible in the cases where it overgeneralizes based on sparse
most of the predicability predictions made are correct:           data. When someone learning about the world has only
                                                                  made a few observations, then her confidence about
                                   H                              the structure of the world should be very low. Such a
                    precision =
                                 H + FA                           learner cannot yet distinguish between occurrences that
where H is the number of “hits”, or pairs that were cor-          are likely, but have not yet been observed, and ones that
rectly predicted to be predicable, and F A is the number          are extremely unlikely. She also does not know with any
of “false alarms”, or pairs that were incorrectly predicted       confidence which of the observations she has made so
to be predicable. The recovered matrices have high re-            far are due to noise. Because of all this uncertainty, it
call when most of the actual predicable pairs have been
successfully recovered:
                                                                  Table 1: F-measures when the results of the three models
                                  H                               are compared with original predicability matrix. SPT is
                       recall =
                                H +M                              samples per true pair.
where H is again the number of “hits”, and M is the                 η       Samples SPT         M con. H.clus. Thresh.
number of “misses”, or pairs that are actually predicable           0.3: 1000          10.89 0.58        0.45      0.47
but were not predicted. We measured overall success of                      10000      108      0.61     0.58      0.48
the models using the F-measure, the harmonic mean of                        100000     1089 0.61         0.58      0.48
precision and recall. The F-measures of these results               0.5: 1000          6.54     0.98     0.64      0.66
with the true predicability values are shown in Table
                                                                            10000      65.4     0.99     0.65      0.66
1 (results are averaged across the two data sets). The
truth matrices recovered by our model were very close                       100000     654      0.98     0.66      0.67
to the actual truth matrices, having F-measures of 0.967            0.7: 1000          4.67     0.86     0.68      0.82
or higher in all cases; the details of those results are                    10000      46.7     0.97     0.72      0.83
omitted.                                                                    100000     467      0.97     0.75      0.81
   With relatively dense input data, the M constraint re-           0.9: 1000          3.63     0.99     0.75      0.94
covers a perfect or nearly perfect predicability matrix.                    10000      36.3     0.98     0.83      0.95
The M constraint model outperforms both the hierar-                         100000     363      1.00     0.83      0.94
chical clustering method and the thresholding method
                                                              747

P1 P2         P1              P1               P1                         in other words, it assumes that the matrix R is tree-
P3 P4
P5 P6
                                                                          structured. The corresponding cognitive assumption is
         O1      P3        O1 P2     P3    O1 P2      P3
 P7              P4                                                       that the M constraint is innate, an assumption we may
              P2 P5
                             O2 O3      P5     O2 O3 P4    P5             not wish to make.
                 P6                  P4 P6
O1 O2
              O2 P7                     P7
                                                                             In previous work we have argued that Bayesian model
O3 O4                                                 O4 O5 P6 P7
O5 O6                                O4                                   selection helps explain how learners can discover the
                   O3 O4
 O7                O5 O6               O5 O6                O6 O7         structural properties that best characterize a domain
                    O7                  O7                                (Kemp et al., 2004). Here we demonstrate that the M
                                                 O1
                                                                          constraint could be learned by comparing our model to a
                                                 O2
                                                 O3
                                                                          closely related clustering model that does not include the
                                                 O4                       M constraint. The same method could be used to com-
                                                 O5
                                                 O6
                                                                          pare a wider set of possible models, but here we choose
                                                 O7                       only between clustering with and without the M con-
                                                                          straint.
                                                      P1
                                                      P2
                                                      P3
                                                      P4
                                                      P5
                                                      P6
                                                      P7
Figure 3: A developmental progression showing the pred-                   A flat clustering model Our alternative model clus-
icability trees and corresponding predicability matrices                  ters predicates and objects, as the M constraint model
                                                                          does, but it does not impose any structural restrictions
learned by the M constraint model as it receives increas-
                                                                          (such as the M constraint) on how predicate clusters re-
ing amounts of data. The labels P1...P7 and O1...O6 rep-                  late to object clusters.. For instance, predicate cluster 1
resent clusters of predicates and objects, each of which                  could span object clusters 1, 2, and 3, and predicate clus-
has two or three members.                                                 ter 2 might span object clusters 2, 3, and 4. This model,
                                                                          which we refer to as the flat model, is identical to the M
would be unwise for the learner to speculate that the                     constraint model except that it uses a different prior on
structure of the world is some highly complicated tree-                   the predicability matrix R. We parameterize each ma-
structure that fits the handful of observations she has                   trix as a triple (zo , C, zp ). As before zp is a partition of
made. It is better to postulate a simpler structure un-                   the predicates. C is a |zp | by |zp | matrix of constraint
til the learner has more information. Our M constraint                    vectors drawn uniformly from the space of binary matri-
model captures this intuition, choosing relatively simple                 ces of this size. Each row of the matrix corresponds to a
trees until the data warrant otherwise. The hierarchical                  predicate cluster, and zo is a random assignment of the
clustering method, on the other hand, is like a learner                   objects to the columns of this matrix:
who creates maximally complex tree structures to fit ev-
ery data set, regardless of how much evidence she has                                                            1
                                                                                               P (C|zp ) =                         (10)
seen. The thresholding method does not learn a tree                                                            2|zp |
structure to fit the data set, but it also undergeneralizes                                                      1
                                                                                               P (zo |zp ) =                       (11)
in its predicability predictions in a way that does not                                                        |zp |m
match the human developmental data.
   To demonstrate in a more controlled way the behav-                        Each triple (zo , C, zp ) uniquely determines a predica-
ior of the M constraint model when operating on sparse                    bility matrix R where Rij takes the same value as the
data, we generated simpler data sets. In these data sets,                 entry in C corresponding to predicate i and object j.
η = 0.85. Every true pair is observed a fixed number of                   Note that R need not satisfy the M constraint: the model
times (N ). Thus, for N = 1, every true pair has been                     captures the idea that predicates and objects cluster, but
seen one time.                                                            little more. As in the M constraint model, the flat clus-
   We ran the M constraint model on data sets generated                   tering model represents truth as a matrix T that is a
for N = 1, 2, and 3, and 6. The best scoring predicability                subset of R.
trees and matrices can be seen in Figure 3. The develop-                  Bayesian model selection Using Bayesian model se-
mental progression of our model is similar to the human                   lection, we can discover which of these models is best
developmental progression reported by Keil. When the                      supported by a given dataset. Let Mtree indicate the
M constraint model has seen very little evidence, it be-                  M constraint model, and Mf lat indicate the alternative.
haves like the younger learners, choosing simpler trees.                  Given data D, we search for the model M and pred-
As more data are provided, the trees recovered by the                     icability matrix R that have maximum joint posterior
model grow to look more like those of Keil’s older sub-                   probability:
jects, who also had more data about the world. The
similarity between these developmental curves argues for                  P (R, T, M |D) ∝ P (D|M, R, T )P (T |R, M )P (R|M )P (M )
the psychological plausibility of a model that develops
more complex theories only when sufficient evidence is
available.                                                                   We use equal priors on the two models: P (Mtree ) =
                                                                          P (Mf lat ) = 0.5. Let Rtree and Ttree indicate the
          Learning the M constraint                                       predicability matrix and truth matrix that maximize
We have shown that our M constraint model can discover                    P (R, T |D, Mtree ), and Rf lat and Tf lat indicate the ma-
predicability structures given only limited evidence. Our                 trices that maximize P (R, T |D, Mf lat ). If the data are
model, however, assumes that the M constraint is true:                    consistent with a tree structure, then Rtree and Rf lat
                                                                    748

  a)                     b)               c)                          Table 2: Log-posterior scores for the best possible con-
                                                                      figuration each model recovered in each condition.
                                                                          Model            Tree-consistent Tree-inconsistent
Figure 4: a) The tree-consistent data used for model                      M constraint -94726                -94255
selection. b) The tree-inconsistent data. c) The tree-                    Flat             -94748            -93933
consistent predicability matrix recovered by the M con-
straint model, given the data in b).
                                                                      further. Measuring model performance on real world
should be identical (as well as Ttree and Tf lat ), since the         datasets is an important future step. As Carey (1983)
flat model is capable of representing all tree-consistent             and others have pointed out, real world data may contain
predicability matrices. P (Rtree , Ttree |Mtree ), however,           many exceptions to the M constraint. One advantage of
will be greater than P (Rf lat , Tf lat |Mf lat ), since the flat     a probabilistic model is that it can tolerate noise and ex-
model needs to assign some probability mass to the many               ceptions; similarly, it may be possible that learners still
matrices that are not tree-structured. As a consequence,              have a strong hierarchical bias, but can make exceptions
Bayesian model selection will favor the tree model if the             when there is sufficient evidence. Our model could be
data are consistent with a tree structure. If the data are            adapted to do the same. The M constraint may also be
consistent only with the flat model, then the tree model              violated by cross-cutting systems of categorization (e.g.,
will be unable to represent a predicability matrix that               taxonomic vs. ecological categories in biology), but pre-
accounts well for the data, and the flat model will be                vious work suggests that multiple context-sensitive mod-
favored by model selection.                                           els, each representing a different hierarchical (or other)
                                                                      structure, could be learned to capture reasoning about
Model selection simulation We generated data sets                     different aspects of a complex domain (Shafto et al.,
from two different predicability matrices. Each contains              2005). We have only begun to explore the learning prob-
the same number of predicates and objects, but one set                lem of distinguishing nonsense from sensibility, but our
is consistent with a tree structure, and the other violates           Bayesian model demonstrates that this distinction is a
the M constraint (see Figures 4a and 4b). Within the                  statistically learnable one, even in a case with no direct
second data set, predicates span overlapping sets of ob-              evidence, like that of the blue banana.
jects. The frequencies in the dataset were generated by
sampling each predicable pair 100 times.                                                Acknowledgments
   Table        2     compares     posterior        probabilities     We thank Tom Griffiths for his help in developing this work.
P (Rf lat , Tf lat , Mf lat |D) and P (Rtree , Ttree , Mtree |D)      This work is supported by an NSF Graduate Research Fel-
                                                                      lowship (LAS), the William Asbjornsen Albert memorial fel-
for the two datasets. As expected, the M constraint                   lowship (CK), and the Paul E. Newton chair (JBT).
model scores better than the flat model on the tree-
consistent data. The difference in performance may                                            References
appear subtle, but the scores are log-posteriors and                  Aldous, D. (1985). Exchangeability and related topics. In
represent a difference of 22 orders of magnitude. For                    Ecole d’Ete de Probabilites de Saint-Flour, XIII–1983, 1-
the tree-inconsistent data set, the flat model performs                  198. Springer, Berlin.
better than the M constraint model, because the M                     Carey, S. (1983). Constraints on natural kind terms. In T.
constraint model is unable to find a tree that produces                  Seiler and W. Wannenmacher (eds.), Concept Development
                                                                         and the Development of Word Meaning. Springer, 126-146.
predicability patterns that match the data. The M
constraint model recovers the appropriate truth matrix,               Keil, F. (1979). Semantic and conceptual development: an
                                                                         ontological perspective. Cambridge, MA: Harvard Univer-
but must overgeneralize in order to find a tree that could               sity Press.
produce the observed data (see Figure 4c). These results
                                                                      Kemp, C., Perfors, A., & Tenenbaum, J. B. (2004). Learn-
confirm the intuition that a learner with a hypothesis                   ing domain structures. Proceedings of the Twenty-Sixth
space including several different representations could                  Annual Conference of the Cognitive Science Society.
choose the representation best supported by a given                   Shafto, P., Kemp, C., Baraff, L., Coley, J., & Tenenbaum,
dataset.                                                                 J. B. (2005). Context-sensitive induction. Proceedings of
                                                                         the Twenty-Seventh Annual Conference of the Cognitive
                            Conclusion                                   Science Society.
We have shown how a generative Bayesian model incor-                  Sommers, F. (1971). Structural ontology. Philosophia, 1(1),
porating the M constraint can be used to learn a pred-                   21-42.
icability tree and infer what is sensible about the world
given sparse observations of what is true in the world.
Additionally, we have demonstrated how Bayesian model
selection can be used to learn the M constraint given a
hypothesis space including alternate models. If people
do organize predicates and objects hierarchically, this re-
sult suggests that the hierarchical bias could be learned
rather than innate.
   Much work remains to be done in exploring this model
                                                                  749

