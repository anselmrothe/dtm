UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Steps Towards Integrated Models of Cognitive Systems: A Levels-of-Analysis Approach to
Comparing Human Performance to Model Predictions in a Complex Task Environment
Permalink
https://escholarship.org/uc/item/73d4g5rh
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 28(28)
Authors
Gray, Wayne D.
Myers, Christopher W
Neth, Hansjorg
et al.
Publication Date
2006-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                           Steps Towards Integrated Models of Cognitive Systems:
        A Levels-of-Analysis Approach to Comparing Human Performance to Model
                                   Predictions in a Complex Task Environment
                      Michael J. Schoelles, Hansjörg Neth, Christopher W. Myers, & Wayne D. Gray
                                                       Cognitive Science Department
                                                       Rensselaer Polytechnic Institute
                                                            Troy, NY 12180 USA
                                                  [schoem, nethh, myersc, grayw]@rpi.edu
   Attempts to model complex task environments can serve as             performance as well a set of zoom lenses to magnify the
   benchmarks that enable us to assess the state of cognitive           differences between our model and our human subjects at
   theory and to identify productive topics for future research.        increasingly fine levels-of-analyses. Our current quest starts
   Such models must be accompanied by a thorough                        with a multi-component complex task that takes humans 12
   examination of their fit to overall performance as well as their
                                                                        minutes (about 103 sec) to perform and requires a model that
   detailed fit to the microstructure of performance. We provide
   an example of this approach in our Argus Prime Model of a            accurately predicts human performance on this task. We
   complex simulated radar operator task that combines real-            then proceed to zoom in on multiple components of our
   time demands on human cognitive, perceptual, and action              complex task and then to zoom in on components of those
   with a dynamic decision-making task. The generally good fit          components. For each component and subcomponent we
   of the model to overall performance is a mark of the power of        derive detailed measures of human performance and ask
   contemporary cognitive theory and architectures of cognition.        how well our model predicts performance on those
   The multiple failures of the model to capture fine-grained           measures.
   details of performance mark the limits of contemporary theory           Taking snapshots as we zoom-in leaves us with a set of
   and signal productive areas for future research.
                                                                        conflicting images. For many of our components our
                                                                        measures of human and model behavior match fairly well.
                            Introduction                                For other components, they do not. We use the results of
Understanding human cognition requires knowing how                      these matches and mismatches to direct our attention to (a)
control of semi-independent functional modules such as                  our assumptions regarding the task analysis that underlies
visual attention, perception, movement, and memory is                   our model, (b) the theory-based assumptions that underlie
integrated to accomplish complex tasks. Our understanding               the model’s components, and (c) the mechanisms that
of this integration may be furthered by simple laboratory               control the sequencing and interleaving of cognitive
tasks, but as this understanding advances, it must be tested
                                                                        subsystems to produce behavior that is adapted to its task
in increasingly complex task environments. In this paper we
                                                                        environment.
provide a progress report on our ability to predict complex
                                                                           In the next section we describe the complex task
behavior from our current understanding of its underlying
functional components.                                                  environment that provides the behavior for our comparisons.
    Our levels-of-analysis approach is inspired by Newell’s             That section is followed by a description of the actual
famous timescale of human activity (Newell, 1990) that                  experiment. Data from our model and our humans are then
divided mental life into time-based levels where the time               presented and examined under increasingly higher
span of each level’s processes differs from those of its                magnifications. We conclude with a discussion of the
neighbors by an order of magnitude. For example, Newell’s               implications of our zoom lens approach for cognitive theory
operations level emerges at about 1/3 to 3 sec (100 sec)                as well as for cognitive research.
while above it is the unit task level (3–30 sec or about 101
sec) and below it is the deliberate act level (30–300 msec or                        A Complex Task Environment
about 10-1 sec). Our approach is congenial to, but distinct             Argus Prime is a complex but tractable simulated task
from, Anderson’s (2002) challenge to the cognitive                      environment (Gray, 2002) that we have used in a variety of
community to show that our understanding of low level                   studies (see, e.g., Gray & Schoelles, 2003; Gray, Schoelles,
cognitive functions can lead us to manipulations that                   & Myers, 2004; Schoelles, 2002; Schoelles & Gray, 2001b).
differentially influence educational outcomes; specifically,            With a small matter of programming, Argus is a flexible
by manipulating low-level, theory-based, functional                     simulation into which we have incorporated a variety of
components of cognition we can span “seven orders of                    nominally related tasks.
magnitude” to influence educational outcomes that take                     The version of Argus Prime discussed in this paper
weeks, months, or semesters to achieve.                                 combines our basic simulated radar-operator classification
    In contrast to Anderson’s building blocks approach, we              task (Schoelles & Gray, 2001a) with a preferential choice
use a wide-angle lens to characterize overall model                     decision-making task. During the 12-min scenarios used for
                                                                        this study, subjects altered between performing the
                                                                    756

Classification Task and Decision-Making Task. The
Decision-Making Task presented subjects with a list of four
or six targets that they had already classified and asked them
to decide which of the target set had the highest threat value.
When the Decision-Making Task was on the screen the
targets on the radar side of the screen (see Figure 1) kept
moving, but subjects were unable to access the information
required to perform the Classification Task. Hence,
obtaining a high score on both tasks placed some time
pressure on the subject to do the Decision-Making Task
quickly as well as accurately.
Classification Task For the Classification Task the subject
must assess the threat value of each target in each sector of a
radar screen (depicted in Figure 1). The screen represents an
airborne radar console with ownship at the bottom. Arcs
divide the screen into four sectors; each sector is fifty miles       Figure 1: Argus Prime Radar Screen (left) and DMT target
wide. The task is dynamic since the targets have a speed and          column (featuring 6 alternatives, to the right).
course. A session is scenario driven; that is, the initial time
of appearance, range, bearing, course, speed, and altitude of        ended and the classification task resumed when the subject
each target are read from an experimenter-generated file.            clicked the CHOOSE button located below the target-column.
The scenario can contain events that change a target’s                  On making a correct choice, feedback was given via a
speed, course, or altitude. Current targets can fly off the          simulated explosion, the chosen aircraft was removed from
screen and new targets can appear so that 18-22 targets are          the radar screen, and the overall percent score for decision-
on the radar screen at any one time.                                 making on that scenario was increased. If the subject chose
   The subject selects (i.e., hooks) a target by moving the          the incorrect target, the subject’s overall percent score for
cursor to its icon (i.e., track number) and clicking on it.          that scenario was reduced. A running average of Decision-
When a target has been hooked, an information window                 Making Task performance was presented to the right of the
appears (this is not shown in Figure 1, but would appear at          classification score. After classifying or re-classifying 8
the upper-right of the display) that contains the track              more aircraft, another Decision-Making Task was presented.
number of the target hooked and the current value of target          This sequence continued until the end of each scenario.
attributes such as speed, bearing, altitude, and course. The            The key to performing the Decision-Making Task well is
subject’s task is to combine these values into a total score,        to obtain an accurate threat value for each target in the
using an algorithm that we have taught them, and to map the          decision-making table. The threat value for a target could be
total score onto a 7–point threat value scale. (This scale           accessed in one of two ways. First, as the subject had
appears at the bottom of the information window).                    already classified the target, its threat value might be
   Targets must be classified once for each sector that they         accessed by a memory retrieval. Alternatively, if the mouse
enter. If a target leaves a sector before the subject can            cursor were moved to the target’s icon in the radar window,
classify it, it is considered incorrectly classified and a score     its threat value would appear next to the target in a popup
of zero is assigned. A running score that indicates                  window. In considering these two alternatives, it is
percentage of targets correctly classified is shown in the           important to point out that although the Decision-Making
upper-left of the display. For this study, each Argus Prime          Task appeared after every 8 classifications, the targets in the
scenario lasted 12-min. During this period a subject had the         Decision-Making Task were not necessarily from the set
opportunity to calculate the threat value of targets between         that had been classified most recently. Rather, the 8 were
70 and 90 times.                                                     chosen at random from the set of all previously classified
Decision–Making Task (DMT) Each scenario proceeded                   targets with the constraint that the highest threat value in
until the subject had classified 8 targets. At this point, a         each Decision-Making Task set be unique to a single target
Decision-Making Task presented the subject with 4 or 6               (more than one target could share all but the highest threat
targets for which he or she had already calculated the threat        value).
value. The subject’s task was to choose the target with the             In summary, for this complex task environment there are
highest threat value.                                                two major subtasks: the Classification Task and Decision-
   All groups were given the track number for each of the            Making Task. Both tasks heavily rely on interactive
Decision-Making Task alternatives in a target-column that            behavior and incorporate subtasks of visual search, memory
appeared in the lower right of the display (see Figure 1).           encoding and retrieval, and decision making. Information
The subject’s task was to determine which target had the             for the Decision-Making subtask may be obtained by either
highest threat value and select that target by clicking on its       memory retrieval or by moving the mouse cursor to over the
number in the target-column. The Decision-Making Task                target’s icon in the radar window. Hence, a key feature of
                                                                     this version of Argus Prime is that knowledge obtained in
                                                                 757

the course of performing one task component (the                         Both the model and humans know that each decision-
Classification Task) is directly relevant to performing the           making table has one target that has the highest threat value.
other task component (the Decision-Making Task).                      They also know that the highest possible threat value is 7.
                                                                      Hence, the model has the heuristic of choosing a target as
Experimental Procedure1 Subjects were randomly
                                                                      the highest threat value in a decision-making table if it has a
assigned to either the 0-Second Lockout (0-Lock) or 2-
                                                                      threat value of 7.
Second Lockout (2-Lock) condition. These two between-
                                                                         If the threat value is not 7 then a comparison is made as to
subjects conditions differed in their cost of information
                                                                      whether the current threat value is the highest seen in this
access. To obtain a threat value, the 0-Lock and 2-Lock
                                                                      particular Decision-Making Task. The “highest-so-far”
groups had to locate the target on the radar screen and move
                                                                      information is stored in a slot in the current goal. If the
the cursor to it. Similar to a tool-tip, the threat value then
                                                                      threat value just obtained is higher than the current highest,
appeared next to the target. For 0-Lock, the threat value
                                                                      it and its associated track number overwrite the current
appeared as soon as the cursor moved to the target. For 2-
                                                                      highest threat value and track number. At this stage, if the
Lock, the threat value appeared after a 2-sec delay.
                                                                      current highest threat value is a 6, then it tries to remember
                                                                      if it classified any targets higher than 6 (i.e., 7). If it fails on
                      Model Description                               this retrieval the model will gamble that 6 is currently the
The Argus Prime Model is written in ACT-R 6.0 and is fully            highest and choose this target even if not all the targets in
compliant with all changes in the ACT-R architecture. To              the table have been checked. Otherwise the model will try to
perform the task, the model uses the same task environment            find a target in the table that it has not yet checked.
software that the subjects use. The model consists of 276                In summary, the model will process all targets in the
productions of which 56 are specific to the Decision-                 decision-making table unless it encounters a target with a
Making Task and 3 are required to recognize and switch                threat value of 6 or 7. If the threat value is 7, the model will
attention to the Decision-Making Task when the                        immediately know that this is the highest threat value target.
Classification Task is interrupted. A run of the model is the         If the threat value is 6 the model will try to remember if it
length of a scenario, which is 12 minutes. The model runs in          has classified any 7s. If it cannot remember classifying a
real time in order to maintain synchronization with the               target as a 7 it will conclude that the 6 is the highest
dynamic Argus task environment.                                       possible threat value and stop processing table items.
   The Argus Prime Model uses the standard ACT-R                         The model has two potential ways of accessing a target’s
parameters for the activation and decay of declarative                threat value. It randomly decides to either try retrieving the
memory elements. It does not, however, learn the utilities of         threat value from memory or obtaining the threat value from
productions in ACT-R’s procedural memory system. The                  a search of the radar screen. The search of the radar screen
parameters of the vision and motor system are also the                is always successful, whereas memory retrieval might fail,
standard ACT-R values.                                                in which case a search of the radar screen is initiated.
   The Decision-Making Task portion of the Argus Prime
Model is enabled when a production detects the appearance                            Human & Model In Harmony
of the Decision-Making Task table on the screen. The model            Eleven human subjects were run in each of our two lockout
moves visual attention to the table (bottom-right of Figure           conditions. Each subject completed 4 scenarios with just the
1) and finds and reads the first track number (from the top-          classification task followed by 8 scenarios that mixed the
down) that it has not previously read. With equal probability         Classification Task with the Decision-Making Task. In
it tries to find the target on the screen or remember how it          contrast, the model was run 11 times in each condition.
classified this target. The search for the target on the screen       Across conditions, no scenario was used by the model more
is a random search. Search is a three-step process. First, the        than twice.
location of a track on the radar screen is determined.
Second, attention is moved to the track to read it. Third, the        Classification Task
number read is compared to the track number that is the
target of the search. (Remember that at all times the tracks          Although we have much to say concerning how the model
of from 18–22 targets appear on the radar side of the                 compared to human performance in the Classification Task,
display, and only 4 or 6 of these are potential matches to            in this short report we focus on the Decision-Making Task
those listed in the decision-making table.) If the number on          and limit our discussion of the Classification Task to one
the track matches the target of the search then the cursor is         overall measure of performance. Hence, we can report that
moved to the track and kept over it until the threat value            the model compares very well with human performance
appears.                                                              with a mean score of 58% compared to the human mean of
                                                                      61%. The 95% confidence interval (CI) for the human data
                                                                      is 58% to 65%; hence, the model’s performance falls just
                                                                      inside this interval.
    1
      The full study used three between-subjects conditions, only
two of which will be discussed here. (For more procedural details
see, Gray, Schoelles, & Myers, 2004).
                                                                  758

Decision-Making Task                                             Task size with 2.86 for DMT-4 and 2.75 for DMT-6. Again
Overall Accuracy At the highest level of analysis, we can        this falls within the 95% CI for this measure of 2.6 to 3.1 for
compare mean score per scenario of the model and humans          0-Lock and 2.5 to 3.0 for 2-Lock.
on the percentage of correct decisions. Both groups were            We conclude from these comparisons that humans and
overwhelmingly accurate in their decisions. Humans               models did not differ in the number of Decision-Making
showed a small and non-significant difference of 94%             Tasks performed.
accuracy in 0-lock and 92% for 2-lock [F(1, 20) = 2.12, p =      Speed A more exacting measure of performance is the
.161] with a 95% CI of 92% to 97% for 0-Lock and 89% to
                                                                 speed with which model and humans made their choices.
94% for 2-Lock. The Argus Prime Model was also highly
                                                                 For humans the between-subjects comparison of lockout
accurate with a mean of 92% for 0-Lock and 93% for 2-
Lock. Not only does the model do very well, but its              conditions (see Figure 2a) was marginally significant with a
performance falls within the confidence interval of the          mean time per DMT of 16.45 sec for 0-Lock and 23.56 sec
human data.                                                      for 2-Lock [F(1, 20) = 4.13, p = .056]. The model mirrored
   Another measure of overall performance is the                 human performance showing a mean of 15.59 sec for 0-
percentage of Decision-Making Tasks for which a decision         Lock and 22.34 sec for 2-Lock [F(1, 20) = 42.55, p < .001].
was made. Humans and models had a maximum of 60-sec              Both of these times fall within the 95% CI (11.29 to 21.61
for each Decision-Making Task. After 60-sec, the Decision-       sec for 0-Lock and 18.40 to 28.72 sec for 2-Lock).
Making Task was scored as an error and the Classification           The human data also shows a significant main effect of
Task resumed. This 60-sec timeout imposed some time              size (see Figure 2b) with DMT-4 at 17.41 sec being (not
pressure on both humans and model as neither could               surprisingly) faster than DMT-6 at 22.59 sec [F(1, 20) =
deliberate without limit.                                        15.53, p = .001]. Again this difference is captured by the
   Our human subjects in the two lockout conditions made a       model [F(1, 20) = 32.24, p < .001] with mean speeds of
selection in 99.9% of all Decision-Making Tasks. The             15.96 sec (DMT-4) and 21.97 sec (DMT-6) that fall well
model chose a target candidate 100% of the time.                 within the 95% CI for this measure (14.62 to 20.20 sec for
                                                                 DMT-4 and 17.84 to 27.34 sec for DMT-6).
Number of Decision-Making Tasks Another basic
measure of performance is the number of Decision-Making          Summary of Human & Model in Harmony
Tasks the subject or model received. This number depends         As shown by the results reported in this section, the model
on the speed with which the Classification Task is               does a good job of replicating both overall and detailed
performed as well as the speed with which the highest threat     effects found in the human data. Clearly, we have reported
valued target is chosen in the Decision-Making Task.             enough good fits to declare victory and to pat ourselves on
   Humans show a small, but not significant difference           the back for having successfully modeled human
between lockout conditions of 5.75 (0-Lock) versus 5.48 (2-      performance in a complex task environment. Although we
Lock) DMTs per scenario [F(1, 20) = 0.351, p = .56]. There       neither dismiss nor distain our success, we feel we can learn
was, however, a small but significant difference of number       more about human cognition by zooming in on a higher
of DMTs as a function of the number of targets, 2.86 for         resolution of performance.
DMT-4 versus 2.75 for DMT-6, [F(1, 20) = 7.87, p =
0.011].                                                                      Zooming in to Reveal Differences
   The model matched the humans well on these measures
showing an average number of 6.09 DMTs in the 0-lock and         The above measures represent the standard sorts of factors
5.63 in the 2-lock conditions. These figures are within the      on which human or model performance are typically
95% CI for this measure (5.2 to 6.3 for 0-Lock and 4.9 to        compared. In this section we zoom in further on human and
                                                                 model performance in an attempt to find the points at which
6.0 for 2-Lock). Likewise the model matches the humans on
                                                                 they begin to diverge.
this measure when we compare across Decision-Making
 (a) Choice times (sec) by lockout.       (b) Choice times (sec) by DMT-size.            (c) Percentage of the targets checked.
 Figure 2: Comparisons between human and model subjects. (Error bars denote 95% confidence intervals for human data.)
                                                             759

Percentage of Alternatives Checked                                 target check times of the 2-lock group (due to the enforced
Going deeper than the number of Decision-Making Tasks              wait before the threat value was displayed). This procedure
performed, we can ask of human and model what                      makes the simplifying assumption that subjects did nothing
percentage of the targets in the decision-making table were        but checking targets on a Decision-Making Task; i.e., it
checked? Humans apparently guess or rely on memory as              counts the time spent on visual search for targets as part of
they check only 66% of the targets by moving to and                the time per check.
clicking on its trace on the radar screen. The model never            An intriguing finding is that, even with lockout time
guesses and is, apparently, able to rely on its memory as          subtracted, humans spend twice as long on 2-Lock target
across both DMT-sizes it only checks 57% of the table              checks (7.2 sec) as on 0-Lock checks (3.1 sec). Gray et al.
targets. Interestingly, the model and target diverge in that       (2004) interpreted this significant difference (p < .01) as a
the model checks about the same percentage of targets for          strategic effort to adjust to the longer memory retention
DMT-4 as DMT-6 (see Figure 2c) whereas humans check a              requirements in this condition. Unfortunately, the model
significantly larger percentage of targets in the DMT-4            currently has average target check times of 5.8-sec
condition than in the DMT-6 condition [70% vs. 62%,                regardless of condition and does thus not reflect the same
respectively, F(1, 20) = 18.46, p = .000; 95% CI: 60% to           behavioral pattern.
82% for DMT-4 and 50% to 74% for DMT-6].
   Nonetheless, the fact that the model only checks 57% of         Check and Check Again
all targets and achieves accuracy levels above 90% means           Although humans and model check a similar number of the
that we at least partially succeeded in our objective of           table targets, the model differs from humans in its
creating a model that not only successfully performs the           percentage of duplicate checks; i.e., the proportion of all
task, but does so by using potentially fallible memory             checks that were to targets that had already been checked on
retrievals.                                                        the current Decision-Making Task.
                                                                      For humans, duplicate checks make up 19.3% of all
Accuracy                                                           checks, whereas the proportion of duplicate checks for the
As mentioned above, the model captures the overall effect          model is a mere 2.2%. Humans also display a sensitivity to
of level of accuracy of the humans as well as the lack of          the costs of checking that the model does not. In the human
difference in accuracy between lockout conditions.                 0-lock group, 32.9% of all checks were rechecks; that is,
However, zooming in we find that the model mismatches              checks of a target that had been already checked at least
human behavior when we look at the effect of task size.            once. In contrast, for the human 2-lock group, rechecks
Whereas humans perform significantly better on DMT-4s              constitute only 5.8% of all checks. As the costs for checking
than on DMT-6s [96% vs. 90%, respectively, F(1, 20) =              in the 2-lock case are higher (due to the lockout) this is a
15.1, MSE = 21.6, p = .001] the model shows a slight               functional adaptation on part of the humans. By contrast, the
difference in the opposite direction 92% vs. 94%. In               corresponding model data for duplicate checks are only
addition, on this measure the model’s result fall outside of       2.8% and 1.5% for the 0-lock and 2-lock conditions. Thus,
the 95% CIs for human data (93% to 98% for 0-Lock and              the model generally tends to not check targets repeatedly,
88% to 92% for 2-Lock).                                            regardless of the checking costs.
   A close examination of the human data revealed that for
one-third of the errors humans chose a target with a lower         Summary of Zooming In
threat value when one of the checked targets had a higher
value. In the other two-thirds of errors humans did not check      Unlike the pattern in the previous section of the paper, the
                                                                   data reviewed in this section reveal intriguing shortcomings
the target with the highest threat value; that is, they either
                                                                   of the model. A consideration of these differences suggests
satisficed before getting it or relied on an erroneous
                                                                   a profound lack of knowledge on our part as to how to
memory. In contrast, the model never forgets the highest-so-
                                                                   repair them without imperiling our impressive successes.
far threat value and never retrieves an erroneous memory.
(It may fail to retrieve any memory, but in that case it
performs an overt search and check of the radar display.)                         Summary and Conclusions
Clearly, a better handling of memory is needed to bring            In addition to giving us a way to think about processes that
model performance into line with human performance at              emerge at different timescales of human activity (Newell,
this detailed level of analysis.                                   1990), Newell also warned us that an unremitting focus on
                                                                   isolated components of cognition would never enable us to
Time Spent per Target                                              see how these components fit together (Newell, 1973). We
Earlier we looked at the overall speed with which decisions        argue that Newell was right and that the time to build
were made. In this section we examine the durations of sub-        integrated models of cognitive systems is now. In some
components. An important indicator of the methods used by          sense, our position is neither bold nor novel as there are
operators in a Decision-Making Task is the average time per        many examples of other researchers engaged in much the
target check. We computed this time by dividing the total          same enterprise (for a collection of examples, see Gray, in
sum of choice times by the number of targets checked               press).
(including duplicate checks) and subtracting 2-sec from the
                                                               760

    However, our call is a bit different than a call to build       Gray, W. D. (2002). Simulated task environments: The role
models using an architecture of cognition. Ours is a call to          of high-fidelity simulations, scaled worlds, synthetic
build models that faithfully reflect not only the cognitive,          environments, and microworlds in basic and applied
perceptual, and motor operations of embodied cognition, but           cognitive research. Cognitive Science Quarterly, 2(2),
that reflect detailed and accurate models of subsystems of            205–227.
perception (such as visual search and audition) as well as          Gray, W. D. (Ed.). (in press). Integrated models of cognitive
subsystems of cognition (such as memory and attention)                systems. New York: Oxford University Press.
along with more complete models of motor movements.                 Gray, W. D., & Schoelles, M. J. (2003). The nature and
Instead of building complex models for complexity’s sake              timing of interruptions in a complex, cognitive task:
we argue for modeling increasingly complex tasks and then
                                                                      Empirical data and computational cognitive models. In R.
examining the success and failure of the model on this
                                                                      Alterman & D. Kirsch (Eds.), 25th Annual Conference of
complex task through an array of wide-angle and zoom
                                                                      the Cognitive Science Society. Austin, TX: Cognitive
lenses.
                                                                      Science Society.
    Our goal is to gain a better understanding of how the
human control system orchestrates and interleaves the               Gray, W. D., Schoelles, M. J., & Myers, C. W. (2004).
resources at its disposal. A failure of the model to accurately       Strategy constancy amidst implementation differences:
capture increasingly detailed data should be regarded not as          Interaction-intensive versus memory-intensive
a dead end but as an opportunity to increase our                      adaptations to information access in decision-making. In
understanding of one or more components and their                     K. D. Forbus, D. Gentner & T. Regier (Eds.), 26th Annual
integration as part of the cognitive system.                          Meeting of the Cognitive Science Society, CogSci2004
    We believe that our proposed approach can be fruitful in          (pp. 482–487). Hillsdale, NJ: Lawrence Erlbaum.
the scientific sense of leading to interesting research and         Gray, W. D., Sims, C. R., Fu, W.-T., & Schoelles, M. J.
productive advances in theory. Early attempts to model                (2006). The soft constraints hypothesis: A rational
Argus Prime have resulted in research focused on visual               analysis approach to resource allocation for interactive
search (Myers & Gray, 2005; Neth, Gray, & Myers, 2005),               behavior. Psychological Review, in press.
task switching (Altmann & Gray, 2002), stable but                   Myers, C. W., & Gray, W. D. (2005). Influencing saccadic
suboptimal performance (Fu & Gray, 2004, in press), as                selectivity: The effect of and interplay between stimulus-
well as to a theory of resource allocation at the under 1,000         driven and strategic factors on initial fixations during
millisecond level of analysis (Gray, Sims, Fu, & Schoelles,           visual search. Manuscript submitted for publication.
2006).                                                              Neth, H., Gray, W. D., & Myers, C. W. (2005). Memory
                                                                      models of visual search: Searching in-the-head vs. in-the-
                      Acknowledgments                                 world. Journal of Vision, 5(8), 417.
The work reported was supported by a grant from the Air             Newell, A. (1973). You can't play 20 questions with nature
Force Office of Scientific Research AFOSR #F49620-03-1-               and win: Projective comments on the papers of this
0143. Thanks to Chris R. Sims and Vladislav D. Veksler for            symposium. In W. G. Chase (Ed.), Visual information
running subjects as well as many other contributions to this          processing (pp. 283–308). New York: Academic Press.
project.                                                            Newell, A. (1990). Unified theories of cognition.
                                                                      Cambridge, MA: Harvard University Press.
                           References                               Schoelles, M. J. (2002). Simulating Human Users in
Altmann, E. M., & Gray, W. D. (2002). Forgetting to                   Dynamic Environments. George Mason University,
   remember: The functional relationship of decay and                 Fairfax, VA.
   interference. Psychological Science, 13(1), 27–33.               Schoelles, M. J., & Gray, W. D. (2001a). Argus: A suite of
Anderson, J. R. (2002). Spanning seven orders of                      tools for research in complex cognition. Behavior
   magnitude: A challenge for cognitive modeling. Cognitive           Research Methods, Instruments, & Computers, 33(2),
   Science, 26(1), 85–112.                                            130–140.
Fu, W.-T., & Gray, W. D. (2004). Resolving the paradox of           Schoelles, M. J., & Gray, W. D. (2001b). Decomposing
   the active user: Stable suboptimal performance in                  interactive behavior. In J. D. Moore & K. Stenning (Eds.),
   interactive tasks. Cognitive Science, 28(6), 901–935.              Twenty-Third Annual Conference of the Cognitive Science
Fu, W.-T., & Gray, W. D. (in press). Suboptimal tradeoffs             Society (pp. 898–903). Mahwah, NJ: Lawrence Erlbaum
   in information seeking. Cognitive Psychology.                      Associates.
                                                                761

