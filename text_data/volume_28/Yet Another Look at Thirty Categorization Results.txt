larity to a number of pseudo-exemplars, rather than to every                            XXX                XXX
single exemplar in each category as is stipulated in the ex-             category                                            XXX
emplar model, or to a single prototype for each category, as                             XX                 XX                XX
proclaimed by the prototype model. The varying abstraction
model is a family of pseudo-exemplar models.
   In a typical categorization experiment, a participant has to
classify a stimulus Si in one of a limited number of categories.
The probability of categorizing stimulus Si in category CJ , out
                                                                        category        XXX                XXX               XXX
                                                                        partition
of M possible categories, is computed as                                                 XX                 XX                XX
                                 βJ ηiJ
                       piJ =                 .                 (1)
                              ∑K=1 βK ηiK
                               M
The crucial part of this equation is ηiK . It denotes the similar-
                                                                        category
                                                                        representation      X              XXX                  X
ity of stimulus Si to category CK . Equation (1) prescribes that                                            XX                  X
a stimulus is classified in the category it is most similar to.                         prototype          exemplar          pseudo-
Further, βK are free parameters, interpreted as the response                            model              model             exemplar
bias towards category CK . They range from 0 to 1 and satisfy                                                                model
the constraint ∑MK=1 βK = 1.
   The stimulus-to-category similarity is given by
                                                                       Figure 1: The two extreme partitions of a category correspond
                         ηiJ ≡   ∑    ηiq .                    (2)     the two traditional models (i.e., the prototype model and the
                               Fq ∈DJ                                  exemplar model). Any intermediate partition corresponds to
                                                                       a new pseudo-exemplar model.
With Fq we denote a pseudo-exemplar, which is the average
of a subset of exemplars. Literally, such a pseudo-exemplar is
not an exemplar but formally it can be treated as an exemplar,            Finally, since a pseudo-exemplar is defined as the average
hence its name. The set of all pseudo-exemplars of the cate-           of a subset of exemplars, it is natural to define the coordinates
gory CJ is denoted as DJ . Further, ηiq denotes the similarity         of pseudo-exemplar Fq as
between stimulus Si and pseudo-exemplar Fq . This similarity
between stimulus Si and pseudo-exemplar Fq is related to the                                          1
                                                                                                      Rq Si∑
distance diq between these items via                                                           xqk =           xik ,                 (5)
                                                                                                           ∈Bq
                                        α
                        ηiq = exp(−diq    ).                   (3)     where Bq denotes a subset in a partition of a category and Rq
                                                                       denotes the number of items in subset Bq . The coordinates
Two special cases are popular: the one where α = 1, resulting          of the stimuli can be predefined by the experimenter, or can
in an exponential decay function, and the one where α = 2,             be identified in a psychological space using multidimensional
resulting in a Gaussian decay function. When the stimuli are           scaling on pairwise similarity data for all the stimuli (Borg &
fairly discriminable, the exponential decay is favored over the        Groenen, 1997; Lee, 2001).
Gaussian (Nosofsky & Johansen, 2000).                                     A set of N elements has two “extreme” partitions: one
   To be able to conceptualize a distance between items, it            when there is only 1 subset (of N elements), and one when
is assumed that stimuli can be represented as points in a D-           there are N subsets (of 1 element each). Figure 1 shows that
dimensional psychological space. The (psychological) dis-              these extreme partitions pick out the traditional models and
tance between items Si and Fq is calculated as                         that all other partitions pick out new, intermediate models. In
                                                                       such a model, a category is represented by multiple proto-
                            D
                  diq = c[ ∑ wk |xik − xqk |r ]1/r .
                                                                       types.
                                                               (4)
                           k=1
                                                                          In sum, the varying abstraction model reduces to the tra-
                                                                       ditional models when the two extreme partitions are chosen
Here, x jk is the coordinate of item S j or Fj on dimension            for each category and introduces multiple-prototype models
Dk . Further, wk are free parameters that are interpreted as           when non-extreme partitions are picked.
the proportion of attention allocated to dimension Dk . They
are called the weights and satisfy, for all k, 0 < wk < 1 and                              The 30 5-4 data sets
∑Dk=1 wk = 1. The parameter c is a free scaling parameter. It          In their seminal paper, Medin and Schaffer (1978) defined
reflects discriminability in psychological space and runs from         two categories that have fueled the categorization research
0 to ∞. Finally, r denotes the metric. The distance is called          ever since. The first category consists of five elements, while
city-block when r = 1 and Euclidean when r = 2. There is               the second category has four elements, hence it is commonly
some evidence that the city block distance is appropriate for          referred to as the 5-4 structure. Apart from these nine training
stimuli with integral dimensions, while for stimuli with sep-          stimuli, there are seven transfer stimuli. All 16 stimuli vary
arable dimensions the Euclidian distance is more appropriate           on four binary dimensions. Their logical structure is shown
(Shepard, 1991).                                                       is Table 1.
                                                                  2300

                                                                        tions of the 5-4 structure. It is not clear in which subgroup he
      Table 1: Medin and Schaffer’s (1978) 5-4 structure                would put data sets 19, 20, 21, 22 and 23. All in all, accord-
                                                                        ing to Nosofsky (2000), the “exemplar subgroup” is 1, 2, 6,
                                  D1     D2     D3    D4
                                                                        7, 10, 13, 16, 24 and 26.
           category A stimuli                                              According to Smith and Minda, data set 23 did not reflect
           A1                     1      1      1     0                 early learning, while data set 22 did, so it potentially disfa-
           A2                     1      0      1     0                 vored an exemplar strategy. However, after analysing data set
           A3                     1      0      1     1                 22, it became clear that such is not the case, hence, we con-
           A4                     1      1      0     1                 sider only data sets 20 and 21 as reflecting the early stages
           A5                     0      1      1     1                 of learning.2 Further, data sets 6 and 7 come from the same
           category B stimuli                                           study as data sets 8 and 9 and are dubious in the same respect
           B1                     1      1      0     0                 as these two sets. To be safe, we excluded these two sets as
           B2                     0      1      1     0                 well. Taking all remarks together, one can safely consider
                                                                        data sets 1, 2, 10, 13, 16, 22, 23, 24 and 26 as good instantia-
           B3                     0      0      0     1
                                                                        tions of the 5-4 structure that favor an exemplar strategy.
           B4                     0      0      0     0
           transfer stimuli                                                    Results of the analysis of the 30 data sets
           T1                     1      0      0     1
                                                                        There are 15 possible partitions for a set of four elements and
           T2                     1      0      0     0                 52 for a set of five, hence there are 780 different pseudo-
           T3                     1      1      1     1                 exemplar models. All these 780 pseudo-exemplar models
           T4                     0      0      1     0                 were fit to all 30 data sets using maximum likelihood esti-
           T5                     0      1      0     1                 mation (Myung, 2003). There are two categories, so it was
           T6                     0      0      1     1                 assumed that the categorization responses follow a binomial
           T7                     0      1      0     0                 probability distribution with success probability piJ , as ex-
                                                                        pressed in equation (1). For the 5-4 data, every pseudo-
                                                                        exemplar model has five free parameters: three weights, one
                                                                        bias parameter βA , and one scaling parameter c. Both r and α
   Smith and Minda (2000) summarized 30 data sets that                  were set to 1, as in Smith and Minda (2000).
made use of the 5-4 structure, collected by different re-                  For ease of reference, all pseudo-exemplar models dis-
searcher (see their appendix A). The 30 data sets not only              cussed in this section are listed in Table 2. The nota-
differ in their exact instantiation of the 5-4 structure, but also      tion used is the following: model 597 is characterized by
in the specific instructions given to the participants, in the mo-      the sequences 12332 and 1231, indexing subset member-
ment of measuring the transfer performance, etc. These dif-             ship of exemplars A1 A2 A3 A4 A5 and B1 B2 B3 B4 re-
ferences might facilitate the use of an exemplar or prototype           spectively. As such, model 597 is defined by the partition
strategy, so it is useful to divide the data sets in subgroups.         {{A1};{A2,A5};{A3,A4}} for category A and the partition
   There are six data sets from a study where participants              {{B1,B4};{B2};{B3}} for category B. Hence, in this spe-
were trained on the individual dimension values that were               cific pseudo-exemplar model, category A and B are each rep-
prototypical of each category. These six sets are 11, 12, 14,           resented by three pseudo-exemplars.
15, 17 and 18. In a comment on Smith and Minda’s (2000)
article, Nosofsky (2000) argues that the category structure
tested in these sets does not conform to the 5-4 structure. For         Table 2: Pseudo-exemplar models discussed in this paper. Mn
three data sets, participants were instructed to use a prototype        means model number.
or rule-based strategy: the sets 4, 5 and 25. Sets 27, 28 and              mn       A1     A2      A3    A4     A5     B1    B2    B3    B4
29 are produced under deadline conditions. The sets 20, 21,
                                                                           1        1      1       1     1      1      1     1     1     1
22 and 23 are from a study where transfer performance was
tested at different points in time. The number of the data sets            166      1      2       1     2      2      1     1     1     1
follows the chronological order, i.e. data set 20 was collected            597      1      2       3     3      2      1     2     3     1
at the earliest stage, data set 23 at the latest stage. Data set 19        750      1      2       3     4      3      1     2     3     4
averages these data. According to Smith and Minda (2000),                  765      1      2       3     4      4      1     2     3     4
data set 23 does not reflect early learning and thus does not              780      1      2       3     4      5      1     2     3     4
disfavor an exemplar strategy. Further, data set 6 sampled ex-
emplars from an infinite pool without replacement. Finally,
data set 3 can be omitted since it is exactly the same as data
                                                                        Results for individual data sets
set 2. In short, for Smith and Minda (2000), the subgroup of
data sets that “most heavily favored exemplar processing” is            There was no single pseudo-exemplar model that outper-
1, 2, 7, 8, 9, 10, 13, 16, 23, 24, 26 and 30 (p. 13).                   formed all other pseudo-exemplar models for all 30 data sets.
   In his comment, Nosofsky (2000) reduces this set further.            There were 25 different winning pseudo-exemplar models.
First, he argues that data set 30 should be in the deadline con-            2 In fact, it will turn out that data set 22 is one of the two data
dition subgroup too. Second, he leaves out data sets 8 and              sets where the exemplar model outperformed all the other pseudo-
9 because he argues that they are no appropriate instantia-             exemplar models.
                                                                   2301

Model 597 was the best in two cases (sets 4 and 6), model                 For the two data sets collected at early stages of learning,
750 in three cases (sets 1, 16 and 19) and model 780 (the ex-          the exemplar model did, somewhat surprisingly, very well. It
emplar model) in two cases (sets 22 and 23). All other data            reached place 5 with an average position of 19.0. The proto-
sets had a unique best fitting pseudo-exemplar model (except           type model reached place 640 only, with an average position
sets 2 and 3, which are identical). The prototype model was            of 528.0.
never the best.                                                           The only condition where the exemplar model performed
   In both traditional models, all categories are assumed to           poor was the deadline condition. For this subgroup, the ex-
have the same representation: every category is represented            emplar model reached place 81 (average position 138.0). The
by one prototype, or by all exemplars. In contrast, the vary-          prototype model outperformed the exemplar model, reaching
ing abstraction model incorporates models in which only one            the second place (average position 13.00). Including data set
category has a prototype or an exemplar representation.                30 in the deadline condition subgroup, as Nosofsky (2000)
   There were two data sets where the best model had an ex-            suggested, gave a better result for the exemplar model (it
emplar representation for category A (sets 22 and 23). In              climbed to place 59, with an average position of 106.3) and
contrast, there were nine data sets where a model with an              a worse result for the prototype model (it fell to place four,
exemplar representation for category B did best (1, 11, 14,            with an average position of 24.0), but the observation that the
16, 19, 24, 30, and of course 22 and 23 again). As far as a            exemplar model performed poor did not change.
prototype representation is concerned, only for data set 17, a            In short, the exemplar model did not only perform well
model where category A had a prototype representation did              for the “exemplar condition” data sets, as one could expect,
best, while there were two data sets where a model with a              but also, rather unexpectedly, in some “prototype conditions”
prototype representation for category B did best (sets 15 and          data sets, i.c. the data sets with prototype/rule instructions
29).                                                                   and the data sets reflecting early learning. This explains why
   Because of the heterogeneity among the winning pseudo-              the exemplar model was the best model across all data sets.
exemplar models, there is no obvious conclusion to draw                   For two conditions, the varying abstraction model uncov-
from the individual results from the 30 data sets (except per-         ered one or more pseudo-exemplar models outperforming the
haps the heterogeneity itself). We tried to interpret the re-          exemplar model and prototype model. In the early learning
sults across all 30 data sets by calculating, for all 780 pseudo-      condition, the winning pseudo-exemplar model (model num-
exemplar models, its averaged position over the 30 data sets.          ber 765, average position 8.0) was only a slight modification
                                                                       of the exemplar model (see table 2). The opposite pattern
Results averaged across data sets                                      was found for the data sets produced under deadline condi-
Averaging the positions of the pseudo-exemplar models                  tions. Now, the pseudo-exemplar model outperforming all
across all 30 data sets led to a most remarkable finding.              other pseudo-exemplar models (model number 166, average
Although the exemplar model was only the best pseudo-                  position 4.3) was a modification of the prototype model (see
exemplar model for two data sets, overall, the exemplar                table 2). The inclusion of data set 30 did not affect the ob-
model outperformed all other 779 models. Its average posi-             servation that model 166 did best for these data sets (average
tion was 82.6. The prototype model reached place 155, with             position 6.5).
an average position of 251.7. Also when we limited ourselves              It is instructive to go beyond the winning model only and
to the appropriate data sets (i.e., excluding sets 6, 7, 8, 9, 11,     broaden the view by looking at the other models with a good
12, 14, 15, 17 and 18), the exemplar model was the winning             averaged performance. There were four models in the overall
pseudo-exemplar model (with an average position of 29.4).              top 20 where category A had an exemplar representation and
The prototype model reached place 305, with an average po-             12 models in the overall top 20 with an exemplar representa-
sition of 344.8.                                                       tion for category B. Only one model with a category A proto-
   To understand this unexpected finding we looked at the av-          type representation entered the overall top 20, no model with
eraged performance of the pseudo-exemplar models for the               a prototype representation for category B did that well. This
different subgroups of data sets. The first subgroup was the           pattern was exactly mirrored in the “exemplar subgroup”, ex-
“exemplar subgroup” 1, 2, 10, 13, 16, 22, 23, 24 and 26.               cept for the fact the one model with a prototype representation
Also for this subgroup, the exemplar model was overall the             for category A disappeared from the top 20.
best (average position 6.22). The prototype model ranked at               In the two conditions where the exemplar model did sur-
place 396 (average position 400.4).                                    prisingly well (i.e., the prototype/rule instructions and the
   Surprisingly, for the three data sets produced under pro-           early stages of learning), a high number of pseudo-exemplar
totype/rule instructions, the result was much alike the result         models with an exemplar representation for category B were
for the exemplar subgroup. Again, the exemplar model out-              in the top 20: seven and nine respectively. Three models of
performed all other pseudo-exemplar models (average posi-              the top 20 had an exemplar representation for category A in
tion 12.3). The prototype model reached place 357 (average             the prototype/rule subgroup, two such models were present in
position 363.3). This observation coincides with Nosofsky’s            the early learning subgroup. This time, a prototype represen-
(2000) finding that performance on stimulus A2 versus stimu-           tation for category A never entered the top 20, but one model
lus A1 under prototype/rule instructions is qualitatively iden-        with a prototype representation for category B was in the top
tical to performance in the exemplar condition (see his figure         20 for the prototype/rule subgroup.
2, his “standard tasks” correspond more or less to our “ex-               It is apparent that this scheme breaks down for the 20 best
emplar subgroup”, his “instructed tasks” correspond to our             models that account for categorization under deadline con-
“prototype/rule subgroup”)                                             ditions. The prototype representation for category A was
                                                                  2302

present in three top 20 models, and for category B in 2 top
20 models. The exemplar representation for category A was                 A few remarks should be made with respect to these con-
never present in the top 20 and for category B in two top 20           clusions. Fitting and comparing all the pseudo-exemplar
models only. These findings are in sharp contrast with the             models of the varying abstraction model family is an exten-
other subgroups.                                                       sion of comparing the fit of the exemplar and prototype mod-
                                                                       els only. Therefore, all concerns that have been raised against
                      General Discussion                               this endeavor hold in this case too. The two most severe con-
                                                                       cerns regard generality and model complexity.
Three conclusions regarding category learning behavior on
the 5-4 structure can be formulated based on our analyzes.                Smith and Minda (2000) suggested that the 5-4 category
                                                                       structure is not theoretically neutral, in the sense that the 5-
    First, overall, the exemplar model gave the best account of
                                                                       4 category structure “may encourage exemplar-memorization
the 30 5-4 data sets, even when challenged by 779 other mod-
                                                                       processes because of its poor coherence, its difficulty, and
els. When we look at the representations for category A and
                                                                       its small, memorizeable exemplar sets” (p. 3). Therefore,
category B separately, the analysis of the individual sets and
                                                                       the data and results obtained based on the 5-4 structure may
the analysis of the averaged performance suggests the same
                                                                       only generalize narrowly. Maybe the exemplar strategy is a
conclusion: especially for category B, there was an advantage
                                                                       specialized categorization strategy, suitable for, for example,
to have an exemplar representation. This exemplar advantage
                                                                       small and difficult categories, but not a general one. If it is
was also present, but to a lesser extent, for category A. There
                                                                       the case that category representation is sensitive to category
are two obvious reasons to explain this difference. First, there
                                                                       structure, the varying abstraction model might prove to be a
are 52 pseudo-exemplar models with an exemplar representa-
                                                                       useful tool when researchers set out to explore the “space of
tion for category B, while there are only 15 pseudo-exemplar
                                                                       category structure” more intensely. Relevant dimensions in
models with an exemplar representation for category A. Sec-
                                                                       this space could be, among others, category size and category
ond, as noted by Smith and Minda (2000), category B is more
                                                                       complexity (Feldman, 2003).
ambiguous than category A, which might induce the need for
an exemplar representation.                                               Second, all pseudo-exemplar models have been evaluated
    A second conclusion is that there was a lot of heterogeneity       on their ability to account for the data only. Recently, math-
in the set of best pseudo-exemplar models. While the exem-             ematical psychologists have raised the issue that selecting
plar model was overall the best model, it was only the best            computational models by looking at their goodness-of-fit
in two of the 30 data sets. Only one model, model number               alone is problematic (Pitt & Myung, 2002). Model complex-
750, gave the best account of more than two data sets. This            ity, which is the inherent flexibility of a model, should be
heterogeneity among the best fitting models reflects the fact          taken into account as well. Pitt, Myung, and Zhang (2002)
that the data sets themselves are very different in nature. The        found that the exemplar model is about e60 ≈ 1.8 times as
only aspect that is shared by all sets is the category structure       complex as the prototype model.3 This difference in model
used. The instantiation of this structure and the exact experi-        complexity, however moderate, suggests that more trustwor-
mental conditions are different for many data sets. The exact          thy results could be achieved when complexity is taken into
instructions given to the participants, the moment of measur-          account. Comparing the performance of all pseudo-exemplar
ing transfer and the amount of time allowed are only a few of          models using a combined measure for goodness-of-fit and
the most obvious factors influencing a categorization strategy.        complexity is important work for the future.
Interestingly, even when only the two traditional models are
contrasted, the analysis of the 30 data sets does not univocally                                    Conclusion
select one of the two models. In 9 cases (the data sets 6, 8, 9,       Our study illustrated how the varying abstraction model can
12, 14, 17, 27, 28 and 29), the prototype model outperformed           be applied to gain additional insight in human categorization
the exemplar model, so it not surprising that there is no best         behavior when analyzing data from a thoroughly studied cat-
fitting pseudo-exemplar model for all 30 data sets.                    egory structure. The model goes beyond the strict prototype-
    Finally, in the early learning condition, the varying abstrac-     exemplar dichotomy by uncovering plausible intermediate
tion model uncovered a slight modification to the exemplar             pseudo-exemplar models that outperform the traditional mod-
model that recovered the data best. For the deadline con-              els. The analyses gave overall support to the exemplar model,
dition data, the exemplar model was clearly not the appro-             but at the same time indicated a modification of the exemplar
priate model. The varying abstraction model uncovered a                model that accounts best for the data reflecting early learning
prototype-resembling model that accounted for the data best.           and a modification of the prototype model that accounts best
The winning pseudo-exemplar models uncovered have, in                  for the data produced under deadline conditions.
both cases, a strong intuitive appeal. If people use an interme-
diate representation, one would expect that this representation
closely resembles the exemplar representation in cases where
                                                                                               Acknowledgments
the exemplar model performs well, and a prototype resem-               The research reported here is part of an interdisciplinary re-
bling representation in cases where the prototype model per-           search project sponsored by the Research Council of the Uni-
forms well, while the opposite pattern would be suspicious.            versity of Leuven (IDO/02/004), given to the third author.
These examples illustrate how the varying abstraction model
can be used as a tool to find the middle ground between the                3 Pitt et al. (2002) did not include a bias parameter in their anal-
two extreme and often unrealistic representational assump-             ysis. This omission is not expected to influence the finding that the
tions by identifying models with an intermediate representa-           exemplar model is slightly more complex than the prototype model.
tion.
                                                                  2303

                         References                                   head (Eds.), The perception of structure: Essays in honor
                                                                      of Wendell R. Garner. Washington, D.C.: APA.
Borg, I., & Groenen, P. (1997). Modern multidimensional             Smith, J. D., & Minda, J. P. (2000). Thirty categorization
  scaling: Theory and applications. New York: Springer-               results in search of a model. Journal of Experimental Psy-
  Verlag.                                                             chology: Learning, Memory, and Cognition, 26, 3–27.
Feldman, J. (2003). The simplicity principle in human con-          Vanpaemel, W., Storms, G., & Ons, B. (2005). A varying ab-
  cept learning. Current Directions in Psychological Science,         straction model for categorization. In B. Bara, L. Barsalou,
  12, 227–232.                                                        & M. Bucciarelli (Eds.), Proceedings of the 27th annual
Lee, M. D. (2001). Determining the dimensionality of mul-             conference of the Cognitive Science Society (pp. 2277–
  tidimensional scaling representations for cognitive model-          2282). Mahwah, NJ: Lawrence Erlbaum.
  ing. Journal of Mathematical Psychology, 45, 149–166.
Medin, D. L., Altom, M. W., & Murphy, T. D. (1984). Given
  versus induced category representations: Use of prototype
  and exemplar information in classification. Journal of Ex-
  perimental Psychology: Learning, Memory, and Cogni-
  tion, 10, 333–352.
Medin, D. L., Dewey, G. I., & Murphy, T. D. (1983). Re-
  lationships between item and category learning: Evidence
  that abstraction is not automatic. Journal of Experimental
  Psychology: Learning, Memory, and Cognition, 9, 607–
  625.
Medin, D. L., & Schaffer, M. M. (1978). Context theory
  of classiffication learning. Psychological Review, 85, 207–
  238.
Myung, I. J. (2003). Tutorial on maximum likelihood esti-
  mation. Journal of Mathematical Psychology, 47, 90–100.
Nosofsky, R. M. (1986). Attention, similarity, and the
  identification-categorization relationship. Journal of Ex-
  perimental Psychology: General, 115, 39–57.
Nosofsky, R. M. (1987). Attention and learning processes
  in the identification and categorization of integral stimuli.
  Journal of Experimental Psychology: Learning, Memory,
  and Cognition, 13, 87–108.
Nosofsky, R. M. (2000). Exemplar representation without
  generalization? Comment on Smith and Minda’s (2000)
  “Thirty categorization results in search of a model”. Jour-
  nal of Experimental Psychology: Learning, Memory, and
  Cognition, 26, 1735–1743.
Nosofsky, R. M., & Johansen, M. K. (2000). Exemplar-
  based accounts of “multiple-system” phenomena in per-
  ceptual categorization. Psychonomic Bulletin & Review,
  7, 375–402.
Nosofsky, R. M., Palmeri, T. J., & McKinley, S. C. (1994).
  Rule-plus-exception model of classification learning. Psy-
  chological Review, 101, 53–79.
Pitt, M. A., & Myung, I. J. (2002). When a good fit can be
  bad. Trends in Cognitive Sciences, 6, 421–425.
Pitt, M. A., Myung, I. J., & Zhang, S. (2002). Toward a
  method of selecting among computational models of cog-
  nition. Psychological Review, 109, 472–491.
Reed, S. K. (1972). Pattern recognition and categorization.
  Cogntive Psychology, 3, 392–407.
Shepard, R. N. (1991). Integrality versus separability of stim-
  ulus dimensions: From an early convergence of evidence to
  a proposed theoretical basis. In J. Pomerantz & G. Lock-
                                                               2304

