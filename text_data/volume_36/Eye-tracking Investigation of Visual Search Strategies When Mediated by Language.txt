UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Eye-tracking Investigation of Visual Search Strategies When Mediated by Language
Permalink
https://escholarship.org/uc/item/6311g331
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Chiu, Eric
Rigoli, Lillian
Spivey, Michael
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                   Powered by the California Digital Library
                                                                    University of California

                                                                                                                               	  
                                    Eye-tracking Investigation of Visual Search
                                       Strategies When Mediated by Language
                                                Eric M. Chiu (echiu@ucmerced.edu)
                                              Lillian M. Rigoli (lrigoli@ucmerced.edu)
                                             Michael J. Spivey (spivey@ucmerced.edu)
                                      Cognitive and Information Sciences, 5200 North Lake Road,
                                                           Merced, CA 95343 USA
                              Abstract                                      A conjunction search uses multiple features, thus multiple
                                                                         maps are needed to identify the presence and subsequently
   Traditional parallel and serial descriptions of the visual search
   process are often inadequate when describing recent findings.         map the location of each feature in a visual field. According
   Accordingly, literature and computational models have                 to Feature Integration Theory, the mechanism used on
   evolved from a dichotomous parallel and serial explanation to         conjunction-search arrays is referred to as a serial search
   an account of search efficiency that is graded and continuous.        process, which claims that observers allocate complete
   In our current experiment, we replicate findings showing              attentional resources discretely and wholly to individual
   concurrent incremental information processing, via auditory           objects one at a time (Treisman & Gelade, 1980; Treisman
   spoken language, mediates visual search and improves search           & Gormican, 1988).
   efficiency (Spivey et al., 2001; Reali et al., 2006; Chiu &              Recent findings have demonstrated that instead of two
   Spivey, 2012). Novel to this study is the use of eye-tracking
                                                                         apparently dichotomous perspectives, parallel and serial,
   to investigate the role of language in mediating and
   improving strategies for visual search. We find evidence that         attention in visual search may be better described as a single
   search is best described as a purely parallel mechanism that          process of graded enhancement of feature salience. This is
   immediately and rapidly integrates linguistic and visual              supported by observed gradual improvements of efficiency
   information. This finding supports an interactive account of          in visual search tasks (Olds, Cowan, & Jolicoeur, 2000a;
   visual attention and spoken language.                                 2000b; 2000c). In a series of experiments, Olds et al.
   Keywords: visual search, language, eye-tracking, dense-
                                                                         observed facilitatory effects as a result of very brief
   sampling, conjunction                                                 presentations (less than 100 ms in some conditions) of
                                                                         displays with only single-feature distractors before
                          Introduction                                   transitioning to conjunction displays. Although observers’
                                                                         responses were not as fast as with pure pop-out displays, the
Humans are inherently limited capacity creatures and as a                data produced a graded improvement of efficiency.
result crossmodal interactions bestow considerable                          To account for findings like those Maioli, Benaglio, Siri,
behavioral advantages. At any given time, only a small
                                                                         Sosta, and Cappa (2001) argue for a time-limited
amount of the existing information on the retina can be
                                                                         competitive model of attention in visual search, in which
processed and mapped onto motor output. Giving attention
                                                                         both parallel and serial processing mechanisms are
to any one stimulus leaves less processing for any others
                                                                         integrated. This perspective is supported in part by neural
because of the selectivity of attention. The ability to filter           mechanisms in extrastriate visual cortex that exhibit a form
out unwanted information allows for awareness of attended                of “biased competition” between multiple object
stimuli but generally results in unawareness of unattended               representations that are partially active in parallel
ones.                                                                    (Desimone & Duncan, 1995; Desimone, 1998; Reynolds &
   Treisman and Gelade’s (1980) Feature Integration Theory               Desimone, 2001). Findings like Olds et al.’s (2000a, b, c)
distinguishes between two perspectives for processing                    “search assistance,” along with various other studies
visual search arrays. First is the initial parallel processing           (Eckstein, 1998; Wolfe, 1998; Maioli et al., 2001; Watson,
perspective, which institutes a single feature array and
                                                                         Brennan, Kingstone, & Enns, 2010) have largely shifted the
accounts for the majority of parallel processing
                                                                         description of visual search phenomena from a serial-
observations. Parallel processing responses are based on a
                                                                         parallel dichotomy to a graded and continuous account of
single map of partially active representations of objects
                                                                         visual search efficiency (Nakayama & Joseph, 1998).
simultaneously contending for probabilistic mapping onto
                                                                            Further support for this trend comes from work by
motor output. These single feature arrays often induce what              Spivey, Tyler, Eberhard, and Tanenhaus (2001) that
is called a perceptual “pop-out” effect, where the unique                discovered another type of “search assistance” phenomenon.
target object that differs from distractor objects by the only           Observers in an Audio/Visual-Concurrent (A/V-concurrent)
feature (e.g., color, orientation, etc.) in the array appears to         condition, where the conjunction-search display is presented
pop-out (Treisman & Gelade, 1980; Treisman & Gormican,                   concurrently with target identity delivery via auditory
1988).                                                                   linguistic queries (e.g. “Is there a red vertical?”), exhibited
                                                                     331

                                                                                                                                	  
dramatically improved search efficiency. By contrast, in an          Method
Auditory-First control condition, where the same spoken              We utilized a mixed design. The search displays used were
query of target identity was provided prior to visual display        the same for all participants but presented in random order.
onset, visual search was notably inefficient. The findings           Trials were split evenly between the A/V-concurrent and
suggest that in A/V-concurrent trials, upon hearing the first-       auditory-first conditions. Participants were randomly
mentioned adjective in the spoken query, visual attention is         assigned to one of two groups. Participants in the first
able to begin the search with only that feature, thus initiating     group, Group A, received half of the search displays in one
the process more efficiently in a single-feature-like search.        of the two conditions (A/V-concurrent or auditory-first) and
Then after hearing the second adjective, several hundred             the other half of the search displays in the remaining
milliseconds later, observers can quickly identify the target        condition. Participants in the second group, Group B,
among the now smaller and more salient subset of objects.            received the same search displays but had them presented in
Moreover, when target identity is delivered in a more                the opposite condition as the participants in the Group A,
traditional non-linguistic visual method for a conjunction           such that any given display was presented as both
search of this type (Spivey et al., 2001: Experiment 3; Chiu         conditions across both groups. This allows for the between-
& Spivey, 2012) overall reaction time (e.g., y-intercept and         subject comparison of search strategies among conditions
slope) are nearly identical to the auditory-first linguistically     for any given search display. Target-present and –absent
mediated visual search condition, which supports a                   trials along with the four set sizes (5, 10, 15, & 20) appeared
facilitory effect of concurrent visual and auditory linguistic       randomly and equally. The two conditions were presented
delivery. This finding has been repeatedly reproduced and            randomly and intermixed. While performing in the
extended (Reali, Spivey, Tyler, & Terranova, 2006; Chiu &            conjunction search task observers’ eye-movements were
Spivey, 2012). Interestingly, Gibson, Eberhard, and Bryant           recorded for all trials using an Eye-Link II head mounted
(2005) found that with faster speech (4.8 vs. 3.0                    eye-tracker (SR Research Ltd., Mississauga, Ontario,
syllables/second) the A/V-concurrent condition no longer             Canada).
provided an enhanced efficiency effect on conjunction-
search tasks, indicating that linguistic mediation of visual         Participants Sixty-eight undergraduate students from the
search is sensitive to speech rate.                                  University of California, Merced received course credit for
   More recently, experiments by Jones, Kaschak, and Boot            participation in this experiment. All of the participants had
(2011) used eye-tracking to examine an alternative                   normal, non-corrected, vision as well as normal color
perspective to one that proposes search efficiency is                perception. Those participants who did not reach 80%
increased due to language enhancing perceptual processing.           accuracy were omitted from the analysis. Three participants
Jones et al. (2011) observed eye movement patterns that              did not perform to these standards.
suggest previously observed improvements in search
efficiency with concurrent speech is not likely due to               Stimuli and Procedure Identical pre-generated search
linguistic enhancement of perceptual processes but rather            displays were used for each observer. The stimulus bars
from delaying the onset of target-seeking eye movements.             subtended 2.8° X 0.4° of visual angle and neighboring bars
They suggest that the findings by Gibson et al. (2005) are           were separated from one another by an average of 2.0° of
better explained by this “preview” of search display because         visual angle. The green and red bars had the same
slower speech provides observers with additional search              luminance of 13.4 cd/m². Appearance of the target object in
display viewing time, which affords additional information           quadrants (top-left & -right, bottom-left & -right) as well as
about potential target locations independently of the                the type of target (e.g., green horizontal), and set sizes of
information conveyed by auditory linguistic speech stream.           objects (5, 10, 15, & 20) appeared equally. Observers were
   With new advances in eye-tracking techniques                      randomly assigned to participate in one of two groups (A or
researchers can now construct and quantify robust                    B). The two groups were indistinguishable but differed in
illustrations of real-time cognitive processes such as identify      that identical search displays were presented in an auditory-
fixation rich regions over a time period. We use this method         first trial for one group and an A/V-concurrent trial for the
to investigate differences in eye-movement and -fixations            other group. In half of the trials, a spoken query (e.g., “Is
during a linguistically mediated conjunction search task.            there a red vertical?”) informed participants of the targets’
                                                                     identity prior to display onset (auditory-first), and for the
                         Experiment                                  other half, the first adjective of the spoken query coincided
In this experiment we observe, using eye-tracking methods,           with the appearance of the visual display (A/V-concurrent
the mechanisms of visual search during a conjunction search          condition). An identical 1000 ms prelude recording (“Is
task mediated by language.                                           there a…”) was used with two target-identifying adjectives
                                                                     (color & orientation), together averaging 1500 ms (fig. 1).
                                                                        An Eyelink II head mounted video-based eye-tracker with
                                                                     a temporal resolution of 250 Hz and a spatial resolution of
                                                                     0.025º recorded eye movements by tracking pupil and
                                                                 332

                                                                                                                                                                                  	  
corneal reflection. The video-based eye-tracker used two                    Results and Discussion
infrared LEDs mounted on the headband to illuminate each                    A hierarchal linear model (HLM) was used for this analysis
eye. Tracking was monocular although viewing was                            because it accounts for the unbalanced number of
binocular. The eye-tracker classified an eye movement as a                  observations by condition, due to our data culling process
saccade when its distance exceeded 0.2° and its velocity                    and repeated measures design. To fulfill the assumption of
reached 30°/second or when its distance exceeded 0.2° and                   distribution normality the inferential statistics were
its acceleration reached 9500°/second2. The displays were                   performed on log-transformed RTs, as RT response data are
generated using Mathworks MATLAB software and the                           bound on the left but not the right, and thus is naturally
experiment was designed using SR Research Experiment                        positively skewed (Luce, 1986). However, descriptive
Builder. Stimuli were presented on a 22” ThinkVision LCD                    statistics (slopes and intercepts of RTs in milliseconds)
monitor with 1280 x 1024 resolution. The prerecorded                        continue to be reported from an untransformed HLM.
speech queries, all from the same female speaker, are                       Participants’ RTs as well as eye-movement and –fixation
identical to Spivey et al. (2001) and were presented through                data were recorded from display onset. All trials with
Harmon Kardon HK206 desktop computer speakers.                              incorrect responses were removed from the analysis as well
                                                                            as trials with RTs greater than 2.5 interquartile ranges from
            a.#
                                            2500&ms&
                                                             Onset&         the mean.
                  Display&
                                                             Oﬀset&
                  Speech&    &&&“Is&there&a&red&ver.cal?”&                                              2800
                                 1000&ms&                                                                          Auditory-first Target-absent
            b.#                              Onset&                                                                Auditory-first Target-present
                  Display&                                                                              2600       A/V-concurrent Target-absent
                                                                                                                   A/V-concurrent Target-present
                  Speech&    &&&“Is&there&a&red&ver.cal?”& Oﬀset&
            c.#                              d.#                                                        2400
                                                                                                                              y = 29.4x + 1835
                                                                                                                              r2 = 0.903
                                                                                                        2200
                                                                                 Reaction Time (msec)
                                                                                                        2000
                                                                                                        1800             y = 5.5x + 1840.7
  Figure 1: Examples of the auditory and visual stimuli. In                                                              r2 = 0.773
                                                                                                                                                      y = 45.2x + 1009.3
                                                                                                        1600
                                                                                                                                                      r2 = 0.951
the auditory-first control condition (a) the onset of the visual
display coincided with the offset of the spoken target query.
  In the audiovisual-concurrent (A/V-concurrent) condition                                              1400
 (b), the onset of the visual display coincided with the onset                                                                                           y = 8.7x + 1198.2
                                                                                                        1200
                                                                                                                                                         r2 = 0.561
   of the first target-feature word in the spoken query. The
example displays show target-present trials with a set size of
                                                                                                        1000
 10 (c) & 20 (d) where the target is a red vertical bar, which
 is accompanied by green vertical and horizontal as well as                                                    0        5          10            15           20             25
                  red horizontal distractor bars.	  	                                                                               Set Size
                                                                             Figure 2: Experiment results. Shown separately for target-
  The Eyelink eye-tracker was calibrated using the standard                  present (filled symbols) and –absent trials (open symbols)
nine-point calibration method for each participant.                             for cue-first (triangles) and cue-concurrent (circles)
Calibration was followed by 16 practice trials to familiarize                conditions. Each line is accompanied by the best-fit linear
participants with the task and the eye-tracker. The                          equation and the proportion of variance accounted for (r2).
experiment consisted of 128 trials. Observers were                                 Error bars indicate standard error of the mean.
instructed to keep their fingers resting on the marked
response keys and to respond as quickly and accurately as                      In this experiment, we replicated previous findings
possible by pressing “YES” and “NO” if the target was                       demonstrated by Spivey et al. (2001) and Reali et al. (2006).
present or absent, respectively. Before each trial,                         The RT-by-set-size functions are highly linear for the
participants were required to fixate their gaze on a fixation               auditory-first condition in both target-present, r2 = .561, and
cross in the center of the screen; this was also used as a                  -absent, r2 = .951, trials as well as for the A/V-concurrent
“drift correct,” which verified that the initial calibration                condition for target-present, r2 = .773, and -absent, r2 = .903,
remained valid. Participants initiated each trail by pressing               trials, which is typical of standard conjunction search tasks.
the space bar while fixating on the fixation cross. It was                  Mean accuracy across all trials is 95.0%, which is consistent
very rare for an observer to have an invalid drift correct,                 with previous studies (Spivey et al., 2001; Reali et al., 2006,
which required the experimenter to recalibrate. The entire                  Chiu & Spivey, 2012). As expected, the slopes of the RT-
experiment lasted approximately 30 minutes.                                 by-set-size functions reveal that the A/V-concurrent
                                                                      333

                                                                                                                                	  
condition produces more efficient visual search (shallower          consistent with the idea that upon hearing the first target-
slope) when compared with the auditory-first condition (fig.        identifying adjective a rapid parallel-like search process
2). The HLM analysis revealed significantly shallower               weeds out conflicting colored distractors and increases the
slopes for the A/V-concurrent condition compared to the             saliency of fitting objects. Further analysis finds that, across
auditory-first condition in target-present (5.5 vs. 8.7             the four set sizes, the number of fixations is again
ms/item), t(64) = -3.23, p < .001, and -absent (29.4 vs. 45.2       significantly smaller for the A/V-concurrent condition, f(64)
ms/item), t(64) = -10.24, p < .001, trials, as previously           = 116.7, p < .001 (Table 1). It should be noted that the
observed by Spivey et al. (2001), Reali et al. (2006), and          descriptive statistics reported here solely involve target-
Chiu and Spivey (2012). Overall mean RT, as well as y-              present trials because search strategies in target-absent trials
intercepts, were significantly slower in A/V-concurrent             have been found to differ from those of target-present and
conditions because complete delivery of target identity was         are notoriously difficult to simulate. Although many of the
delayed by approximately 1500 ms relative to the auditory-          differences between an A/V-concurrent target-identity
first condition for both target-present, t(64) = 184.79, p <        delivery and an auditory-first delivery are observed with
.001, and -absent, t(64) = 250.27, p < .001, trials.                target-absent trials as well, they are beyond the scope of this
   We see the results of this experiment continue to show           report and will not be discussed.
observers were able to find the target object in a way that
                                                                                    Auditory-first       A/V-concurrent
was substantially less affected by the number of distractors,
                                                                            A.
simply by adjusting the timing of spoken query so the two
target-feature words are heard while viewing the visual
display. It appears that the incremental nature of speech
processing allows the visual search process to begin when
only a single feature of the target identity has been heard.
When the initial feature is identified the search proceeds in               B.
an efficient nearly-parallel fashion so when the second
adjective is heard, a substantial amount of the target
identification process has been completed – and thus the
presence of multiple distractors is less disruptive.
                                                                            C.
      Table 1: Number of fixations for target-present trials.
Set       A/V-concurrent               Auditory-first
Size
      5   M = 12.1      SD = 5.1       M = 13.4      SD = 7.3
    10    M = 12.7      SD = 4.9       M = 14.0      SD = 5.7               D.
    15    M = 14.2      SD = 10.0      M = 14.5      SD = 6.2
    20    M = 13.2      SD = 5.9       M = 14.5      SD = 5.4
   Of primary and novel interest in this experiment is the
analysis of eye-movement patterns during the well
replicated linguistically mediated conjunction search task             Figure 3: Eye-tracking results. Target-present trials are
(Spivey et al., 2001; Reali et al., 2006; Chiu & Spivey,               shown separately for auditory-first control and the A/V-
2011). Figure 3 shows a 100 ms time slice of target-present         concurrent trials. Search displays are overlapped with a heat
trials with fixations; the four set sizes are shown separately         map representing fixation activity (blue = low, yellow =
for A/V-concurrent and auditory-first. One can clearly see            medium, and red = high). A single 100 ms time period is
that for the same time-slice, fixations in the A/V-concurrent         depicted for each set size: 1600-1700 ms for 5 (A), 1900-
trials are primarily focused on color-matched objects, while          2000 ms for 10 (B), 1700-1800 ms for 15 (C), and 1100-
fixations in the auditory-first trials do not appear to exhibit      1200 ms for 20 (D). Targets for each trial are as follows: 5
any pattern. This phenomenon is consistent across all of the         = red vertical, 10 = green vertical, 15 = red horizontal, and
trials. The following analyses investigate the claim that            20 = green horizontal. Fixation patterns differed drastically
A/V-concurrent target-feature delivery does indeed elicit a           between trials thus different time period were chosen for
different and more efficient oculomotor search of the                    each set size but were the same between conditions.
display than the auditory-first condition.
   First, we find significantly fewer fixations across each            Second, the average duration of each fixation is also
trial in the A/V-concurrent condition (M = 13.08, SD =              significantly shorter for the A/V-concurrent condition, 335.7
6.89) as compared to the in the auditory-first condition (M =       ms (SD = 395.0), than for the auditory-first condition, 382.2
17.23, SD = 23.76), t(64) = 17.18, p < .001, which is               ms (SD = 503.4), t(64) = 7.33, p < .001. Since observers in
                                                                334

                                                                                                                                    	  
the auditory-first condition receive both target-identifying            across all four set sizes, f(64) = 14.21, p < .001 (Table 3).
adjectives before the onset of the search display, it is                Furthermore, saccade amplitude appears to be decreasing as
possible that they are judging each fixated object by both              set size increases, which would support a serial like search
features at once in search of the unique target, which would            process because the to-be-next-fixation object would be
explain the longer fixation durations when compared to                  closer with a larger set size than with a smaller set size. We
A/V-concurrent trials. Since observers in the A/V-                      do not see the same pattern with A/V-concurrent trials.
concurrent condition appear to have already isolated
attention to objects that match the identified color feature,                 Table 3: Saccade amplitude for target-present trials.
they would only have to judge each fixated object on the
one remaining feature (orientation). Further analysis of this           Set       A/V-concurrent              Auditory-first
effect has found that fixation durations are significantly              Size
shorter for A/V-concurrent trials when compared to                            5   M = 5.33       SD = 7.00    M = 5.22      SD = 8.06
auditory-first trials across all four set sizes, f(64) = 39.1, p <           10   M = 4.45       SD = 6.14    M = 4.38      SD = 6.55
.001 (Table 2).                                                              15   M = 5.32       SD = 6.91    M = 4.43      SD = 6.50
   Interestingly, across both conditions, eye-fixation                       20   M = 5.84       SD = 7.36    M = 4.96      SD = 7.01
duration decreased (392.6, 356.2, 349.0, and 345.8 ms) as
set size increased; this main effect is significant, f(64) =               The strongest evidence supporting the claim that search
24.83, p < .001. This pattern may be the result of a desire to          patterns differ dramatically between A/V-concurrent and
respond quickly, as instructed at the beginning of the                  auditory-first trials comes from an analysis of a target-
experiment. Thus when observers see there are more                      present trial with a set size of 20 where the target-object is a
objects, they may speed their search strategy, surprisingly             green horizontal bar. We see that the average amount of
with no significant affect on accuracy (5.1, 4.7, 5.0, & 6.2%           time spent fixating green bars is greater than red when
errors), f(64) = 0.46, p < .497. This may be the result of              presented as an A/V-concurrent trial (46.5 vs. 17.5 ms;
including the first fixation (from display onset to the first           difference of 29 ms) than when presented prior in an
saccade) in the analysis, which is longer than most fixations           auditory-first trial (48.4 vs. 21.2 ms; difference of 27.2 ms),
and may have drove up the average fixation duration. A                  f(64) = 7.33, p < .001; given a range of 21.1-14.5 fixations
later report of these findings will test this hypothesis.               per trial this difference would add up. Moreover, when we
                                                                        look at only the first 1000 ms of the trial, we see that
      Table 2: Fixation duration for target-present trials.             observers continue to fixate significantly longer on green
                                                                        bars than red for A/V-concurrent trials (19.7 vs. 9.4 ms),
Set      A/V-concurrent                Auditory-first                   t(64) = -2.02, p = .044, but not for auditory-first trials (12.6
Size                                                                    vs. 12.0 ms), t(64) = -0.13, p = .897. Thus, after hearing the
    5    M = 361.6      SD = 414.4     M = 421.6      SD = 547.5        color feature and only part of the orientation feature, in the
   10    M = 334.6      SD = 376.2     M = 375.5      SD = 488.0        first 1000 ms of an A/V-concurrent trial, observers appear to
   15    M = 327.2      SD = 397.6     M = 368.3      SD = 484.9        be immediately using the concurrent audio-visual
   20    M = 320.8      SD = 391.2     M = 367.9      SD = 494.5        information to bias their eye fixations toward color-
                                                                        appropriate objects, thereby improving efficiency compared
   The average length of saccades, rapid ballistic movements            to when the spoken target query precedes onset of the visual
of the eye between fixation points, referred to as amplitude            display.
and measured in degrees of visual angle, are significantly
longer for A/V-concurrent, 5.24 (SD = 6.88), than for                                       General Discussion
auditory-first, 4.73 (SD = 7.03), t(64) = -4.95, p < .001,              The findings here are consistent with the inferences made in
trials. If it is the case that observers in an auditory-first trial     prior linguistically mediated visual search reports (Spivey et
are performing a traditional serial search process, where               al., 2001; Reali et al., 2006; Chiu & Spivey, 2012). The
they attend to each object wholly and discretely to judge               significantly fewer fixations, shorter fixation durations, and
whether it is the target, then we can presume their attention           larger saccade amplitudes observed when auditory linguistic
would jump from one object to the next closest object to                target features are delivered concurrent with display onset,
optimize their search strategy until the target was found.              in the A/V-concurrent condition, compared to when target
This scenario would describe why saccade amplitudes are                 features were delivered prior to display onset, in the
shorter for auditory-first trials than for A/V-concurrent               auditory-first condition, provides further evidence
trials. Because half of the objects are effectively ruled out in        supporting the notion that observers employ distinctive
A/V-concurrent trials, distance from one viable object to               search strategies when display onset timing is altered in
another viable color-matched object would tend to be longer             relation to feature delivery. Furthermore, the longer dwell
than simply to the next closest object. Further analysis                time observed with color-matched objects than non-matched
found that saccade amplitudes are significantly shorter for             distractors throughout an A/V-concurrent trial, and
auditory-first trials when compared to A/V-concurrent trials
                                                                    335

                                                                                                                           	  
especially in the first 1000 ms, supports the existence of a       Jones, J. J., Kaschak, M. P., & Boot, W. R. (2011).
fast-acting efficient parallel search process that does not          Language mediated visual search: The role of display
occur in an auditory-first trial.                                    preview. In N. Miyake, D. Peebles, & R. P. Cooper
   The novel discoveries here further promote the claim that         (Eds.), Proceedings of the 34th Annual Conference of the
upon hearing the first-mentioned adjective in a spoken               Cognitive Science Society (pp. 2739-2744). Austin, TX:
query, visual attention is able to begin the search with only        Cognitive Science Society.
that single feature. Thus, the process is initiated with a         Luce, R. D. (1986). Response Times: Their Role in Inferring
highly efficient single-feature search such that when the            Elementary Mental Organization3 (Vol. 8). Oxford
second adjective is delivered, several hundred milliseconds          University Press.
later, the target can be quickly found among the attended          Maioli, C., Benaglio, I., Siri, S., Sosta, K., & Cappa, S.
subset of objects. Conversely, trials presented in the               (2001). The integration of parallel and serial processing
auditory-first condition appear to exhibit a search strategy         mechanisms in visual search: Evidence from eye
representative of a traditional serial search processes, by          movement recording. European Journal of Neuroscience.
which each object in the search display is compared to the           13(2), 364-372.
aforementioned target-object one at a time until the target-       Nakayama, K., & Joseph, J. S. (1998). Attention, pattern
object is located in a target-present trial.                         recognition, and pop-out in visual search. The attentive
                                                                     brain, 279–298.
                         Conclusion                                Olds, E. S., Cowan, W. B., & Jolicoeur, P. (2000a). Partial
These results support a robust interactive account of visual         orientation pop-out helps difficult search for orientation.
perception that explains language mediation of visual search         Perception & psychophysics, 62(7), 1341–1347.
is chiefly due to the capacity to rapidly and immediately          Olds, E. S., Cowan, W. B., & Jolicoeur, P. (2000b). The
integrate incremental linguistic information with visual             time-course of pop-out search. Vision Research, 40(8),
information. This study provides us with significant insight         891–912.
into the mechanisms of auditory language mediated visual           Olds, E. S., Cowan, W. B., & Jolicoeur, P. (2000c).
search but also adds to the complexity of the dynamic                Tracking visual search over space and time. Psychonomic
relationship, which escalates the importance and compels             Bulletin and Review, 7(2), 292–300.
the need for additional exploration with additional                Reali, F., Spivey, M. J., Tyler, M. J., & Terranova, J.
experimental tests such as this.                                     (2006). Inefficient conjunction-search made efficient by
                                                                     concurrent spoken delivery of target identity. Perception
                                                                     and Psychophysics, 68(6), 959.
                    Acknowledgments
                                                                   Reynolds, J., & Desimone, R. (2001). Neural mechanisms
We are grateful to Greg Perlman from SR Research for help            of attentional selection. Visual attention and cortical
with experiment design. We are also thankful to Lydia Goes           circuits (Braun J, Koch C, Davis JL, eds), 121–136.
and Maria Vega for their help with data collection.                Spivey, M. J., Tyler, M. J., Eberhard, K. M., & Tanenhaus,
                                                                     M. K. (2001). Linguistically mediated visual search.
                         References                                  Psychological Science, 12(4), 282-286.
Chiu, E. M. & Spivey, M. J. (2012). The role of preview            Treisman, A., & Gormican, S. (1988). Feature analysis in
   and incremental delivery on visual search. In N. Miyake,          early vision: Evidence from search asymmetries.
   D. Peebles, & R. P. Cooper (Eds.), Proceedings of the 34th        Psychological Review, 95(1), 15–48.
   Annual Conference of the Cognitive Science Society (pp.         Treisman, A. M., & Gelade, G. (1980). A feature-integration
   216-221). Austin, TX: Cognitive Science Society.                  theory of attention. Cognitive psychology, 12(1), 97–136.
Desimone, R. (1998). Visual attention mediated by biased           Watson, M. R., Brennan, A. A., Kingstone, A., & Enns, J.
   competition in extrastriate visual cortex. Philosophical          T. (2010). Looking versus seeing: Strategies alter eye
   Transactions of the Royal Society B: Biological Sciences,         movements during visual search. Psychonomic Bulletin &
   353(1373), 1245.                                                  Review, 17(4), 543-549.
Desimone, R., & Duncan, J. (1995). Neural mechanisms of            Wolfe, J. M. (1998). What can 1 million trials tell us about
   selective visual attention. Annual Review of Neuroscience,        visual search? Psychological Science, 9, 33-39.
   18(1), 193–222.
Eckstein, M. P. (1998). The lower visual search efficiency
   for conjunctions is due to noise and not serial attention
   processing, Psychological Science, 9, 111-118.
Gibson, B. S., Eberhard, K. M., & Bryant, T. A. (2005).
   Linguistically mediated visual search: The critical role of
   speech rate. Psychonomic Bulletin and Review, 12(2),
   276.
                                                               336

