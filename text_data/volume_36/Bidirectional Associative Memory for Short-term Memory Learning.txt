UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Bidirectional Associative Memory for Short-term Memory Learning
Permalink
https://escholarship.org/uc/item/6xv9f90m
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Tremblay, Christopher
Berberian, Nareg
Chartier, Sylvain
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                      Bidirectional Associative Memory for Short-term Memory Learning
                                            Christophe Tremblay (ctrem040@uottawa.ca)
                                            School of Psychology, 136 Jean-Jacques Lussier
                                              University of Ottawa, ON K1N 6N5 Canada
                                              Nareg Berberian (nberb062@uottawa.ca)
                                            School of Psychology, 136 Jean-Jacques Lussier
                                               University of Ottawa, ON K1N 6N5 Canada
                                            Sylvain Chartier (sylvain.chartier@uottawa.ca)
                                            School of Psychology, 136 Jean-Jacques Lussier
                                               University of Ottawa, ON K1N 6N5 Canada
                             Abstract                                 performance in some cases (Ritter, Diaz-Deleon, &
                                                                      Sussner, 1999; Wu & Pados, 2000, Acedo, Yanez, &
   Previous research has shown that Bidirectional Associative
   Memories (BAM), a special type of artificial neural network,       Lopez, 2006a; 2006b). Although these improvements have
   can perform various types of associations that human beings        successfully overcome the limitations of the model
   are able to perform with little effort. However, considering a     presented previously, these models were most often applied
   simple association problem, such as associating faces with         to engineering problems rather than providing legitimate
   names, iterative type BAM networks usually take hundreds           models of cognition. For example, good connectionist
   and sometimes thousands of learning trials to encode such          models of cognition should adapt their connection weights
   associations correctly, whereas humans in some conditions
   learn much faster. The present study therefore proposes an
                                                                      locally, without homuncular knowledge, backpropagation
   adjustment to a particular type of BAM network that                of the error or complex learning procedure (O'Reilly,
   increases its performance in a rapid learning condition while      1998). Therefore, models should show the ability to learn
   processing memory capacity is limited. Results show that the       quickly with excellent performance, while possessing
   modification to the original learning rule of the BHM leads        limited memory capacity when encoding time is short (one
   to improved performance when rapid learning is required.           shot or few shot learning). On the other hand, they should
   Moreover, the model preserves its high memory load                 present good performance and good memory load capacity
   capacity in standard learning. This study could lead to
   improved cognitive models that can adapt their behavior in         when the encoding time allowed is longer.
   function of the contextual conditions.                               The present study proposes the introduction of a recency
                                                                      parameter in a modified BAM network (Chartier &
   Keywords: Artificial intelligence; Connectionist models;           Boukadoum, 2006b; 2011), which leads to improved
   Bidirectional associative memory; Short-term memory
   learning.                                                          performance in rapid learning situations, but with limited
                                                                      storing capacity. The proposed process therefore allows for
                           Introduction                               learning in conditions where a limited number of iterations
                                                                      is required. The remainder of the paper is divided as
Like human beings, artificial neural networks can
                                                                      follows: The background section describes the architecture
discriminate, identify, and categorize perceptual patterns
                                                                      of the model and the internal properties of the network,
(Faussett, 1994; Haykin, 2009). Bidirectional Associative
Memories (BAMs) have been proposed as models of                       while the method section presents the learning and recall
                                                                      procedures of the model. The results section describes the
neurodynamics. As such, they are able to develop attractors
                                                                      recall performance tasks of the model under various level
allowing the network to perform various types of recall
                                                                      of noise. The final section discusses the results and
under noisy conditions and successfully carry out pattern
                                                                      provides a conclusion to the study.
completion. Many advances were made since Kosko
introduced the BAM in 1988; see Acevedo-Mosqueda,
                                                                                              Background
Yanez-Marquez & Acevedo-Mosqueda (2013) for a review.
Although the majority of improvements made BAM models                 The model proposed by Chartier & Boukadoum (2006b;
better suited for learning a wider set of associations and            2011) uses an unique matrix for each layer. This
brought greater performance, such improvements have been              Bidirectional Heteroassociative Memory (BHM) is able to
at the expense of learning times. Nowadays, BAM models                learn correlated patterns for bipolar patterns as well as for
often take hundreds and sometimes thousands of learning               real-valued patterns.
cycles to correctly associate patterns. Consequently,
researchers have attempted to enhance convergence times               Architecture
and performance (Nong & Bui, 2012). Several one-shot                  The network is made of two Hopfield-like neural networks
learning rules were developed, showing almost perfect                 interconnected in a head-to-tail fashion, providing a
                                                                  1917

recurrent flow of information that is processed                   which contrasts with other BAMs that can only develop
bidirectionally. The network's architecture is shown in           bipolar attractors (Chartier & Boukadoum, 2006b).
Figure 1 where x(0) and y(0) represent the initial vector-
states, W and V are the weight matrices and t is the current      Learning Rule
iteration number.                                                 The connection weights are modified following a
                                                                  Hebbian/anti-Hebbian approach (Storkey & Valabreque,
                                                                  1999; Bégin & Proulx, 1996):
                                                                                                                            (2)
                                                                  where is the learning parameter controlling for the speed
                                                                  of convergence and k is the learning trial number.
                                                                  Connection weights are initiated at 0 and x(0) and y(0) are
                                                                  the initial inputs to be associated. The network has
                                                                  converged when x(0)=x(t) or y(0)=y(t). Thus, each weight
                                                                  matrix converges when the feedback is equal to the initial
                                                                  inputs. In the BHM, the network convergence is guaranteed
                                                                  if the learning parameter is set according the following
             Figure 1: Architecture of the BAM.                   condition (Chartier & Boukadoum, 2006a):
Transmission Function                                                                                                       (3)
The transmission function is based on the classic Verhulst
equation extended to a cubic form with a saturating limit at
                                                                  where M and N are respectively the dimensionality of the
±1 (Chartier & Boukadoum, 2006b):
                                                                  input and its association. The       parameter was set to a
                                                                  lower value than the threshold found in (3) for every
                                                          (1a)
                                                                  simulation performed. The learning rule (2) acts much like
                                                                  a long-term memory where the learning convergence is
                                                                  longer, but exhibits an increased storage capacity and has a
                                                                  better-defined attractor.
and                                                               Learning Rule Modification
                                                                  In order to lower the time to learn associations, the memory
                                                         (1b)
                                                                  capacity has to be decreased. One way to accomplish this is
                                                                  by introducing a recency parameter (                  ). This
                                                                  parameter removes from the memory associations that are
                                                                  not reinforced enough. The resulting learning rule after
                                                                  modification is given by:
where N and M are the number of units in each layer, i is
the unit index,  is a general transmission parameter and a
                                                                                                                            (4)
and b are the activations. These activations are obtained the
usual way: a(t)=Wx(t) and b(t)=Vy(t). Figure 2 illustrates
the shape of the transmission function for  = 0.2.               If =1 then the learning is accomplished in the same
                                                                  fashion as in Equation 2. This learning rule can be
                                                                  simplified to the following hebbian/anti-hebbian equation
                                                                  in the case of auto association where y(0)=x(0):
                                                                                                                            (5)
         Figure 2: Transmission function for  = 0.2.                                           Simulations
                                                                  The simulations are to assess the performance of the Short-
Contrary to sigmoid-type function, there are no asymptotic        term BHM on a recall task in comparison to the standard
behaviors in the transmission. This function has the              Hopfield, Kosko and BHM networks.
advantage of exhibiting grey-level attractor behaviour
                                                              1918

Methodology                                                          Results
Learning was carried out according to the following                  Figure 4 presents an example of the first 10 patterns
procedure:                                                           recalled in a noiseless (0 flipped pixel) situation for both
     1) Random selection of a pair of patterns (x(0) and             auto-association and hetero-association tasks. The orange
          y(0)).                                                     dashed lines represent the demarcation between previously
     2) Computation of x(t) and y(t) according to the                learned associations and the associations that have just been
          transmission function (1).                                 learned. The model was compared to the results of
     3) Computation of the weight matrices update                    Hopfield's model (1982) as well as Kosko's (1988). For
          according to (4).                                          both networks, contrary to the BHM, there are no memory
     4) Repetition of steps 1) to 3) until all of the pairs          traces between the past and current association. In other
          have been presented.                                       words, the connection weights are reset to zero between the
     5) Repetition of steps 1) to 4) for an a-priori set             learning of a given group. The connection weights had to
          number of epochs.                                          be set to zero since both Hopfield and Kosko's model
The transmission parameter () was set to 0.2 throughout             cannot perform the task otherwise as they suffer from
the simulations and the number of iterations to perform by           memory overload. It is as if we are comparing the
the network before the weight matrices were updated was              performance of a single BHM with several independent
set to       . The network was tested on an auto-association         Hopfield or Kosko models. Although this situation is
and hetero-association task that consisted of 26 stimuli             different, it was included for comparison purposes using
placed on 7x7 grids (Figure 3). The auto-association task            optimal conditions for Hopfield and Kosko.
was an association of uppercase stimuli only, whereas the                 The results (Figure 4) for the auto-association of the
hetero-association consisted of the association between              short-term memory show that previously learned
uppercase and lower case stimuli. The recency parameter              associations tend to be erased as new associations are
() was set to 0.99 and 0.995 for the rapid setting and at 1.0       made, particularly when the correlation is very high
for the standard long-term setting. In the rapid setting,            between two patterns (for example, the stimulus E and F).
instead of presenting all the patterns at once, the network          When patterns are presented in groups of two, the short-
was limited to only one subset at a given time. In other             term network makes no mistakes in associating the patterns
words, rather than learning all stimuli in one epoch, the            presented within the step; this also holds for conditions
network limited itself to grouped associations of a                  where patterns are presented in groups of 5. The Hopfield
maximum of 5 associations.                                           network shows perfect performance when the input patterns
                                                                     are learned in groups of two. However, when presented in
                                                                     groups of 5, the network makes several mistakes even in
                                                                     the absence of noise. These results are even more disastrous
                                                                     for hetero-associations, where the network can barely recall
                                                                     any associations. Hence, Kosko's network is not able to
                                                                     learn any of the associations grouped in pairs, whereas the
                                                                     short-term BAM is able to learn all associations whether
                                                                     they are presented in groups of 2 or 5. Results for the
                                                                     standard BHM were not shown because it could learn and
                                                                     recall perfectly in all of the previous situations.
                                                                               Figure 5 depicts Monte Carlo simulations of the
           Figure 3: Patterns used for the simulation                network performance on recall tasks with flipped pixels. As
                                                                     can be seen in figure 5a and 5b, the short-term memory
   Following the learning phase, the network was tested on           outperforms the standard BHM. However, the results for
a recall task according to the following procedure:                  the independent Hopfield networks with sequences of 2 and
     1) Selection of an input pattern x(0).                          4 items are very similar to the results of the short-term
     2) Computation of y(1) according to the transmission            version of the BHM. Results for sequences of 5 inputs are
          function (1).                                              not reported since it was shown in Figure 4 that the
     3) Comparison with the target value y(0)                        Hopfield networks could not perfectly recall more than 4
     4) Repetition of steps 1) to 3) until all of the patterns       associations even in a noiseless condition. In addition,
          have been presented.                                       more epochs (15 rather than 5) systematically lead to
In this situation a given pattern iterated until a steady state.     increased performance in the BHM. In other words, using
Recall performance was recorded for the level of flipped             the batch of training sets for a greater number of cycles
pixels varying from 0 to 24 (0 to ≈50%). The network was             before updating the weight matrices leads the short-term
tested on grouped associations of 2, 3, 4 and 5 patterns. The        BHM to better recall performance. For example, the
network was tested 200 times for every condition and the             performance for input sequences of 5 improved if the
average performance was computed.                                    number of epoch is increase from 5 to 15 epochs. However,
                                                                     for the remaining conditions, the number of epochs has
                                                                     little or no impact on the performance. Evidently, because
                                                                 1919

                                       Auto-associations
              Short-term BHM                                           Hopfield's model
Groups of 2
Groups of 5
                                      Hetero-association
              Short-term BHM                                            Kosko's model
Groups of 2
Groups of 5
                  Figure 4: Association recall for auto and hetero-association learning
                                                 1920

                               5 learning epochs                                       15 learning epochs
   Auto-association (=0.99)
                                     (5a)                                                     (5b)
Hetero-association
   (=0.99)
                                     (5c)                                                     (5d)
Hetero-association
  (=0.995)
                                     (5e)                                                      (5f)
         Figure 5: Performances of the network on a recall task with pixels flipped. The legend is laid out as follow: Stan. BHM
       are the results for a standard BHM. BHM 2, BHM 3, BHM 4 and BHM 5 are the results for the short-term version of the
      BHM for input sequences of respectively 2, 3, 4 and 5 patterns. Finally, Hopfield and Kosko are the results for the Hopfield
                                 and Kosko networks for input sequences of respectively 2 and 4 patterns
 Hopfield network uses a strict Hebbian learning rule,                associations. In fact, the independent networks cannot learn
 anything other than one epoch will lead to worse results             perfectly sequences of 2 nor 4 associations. In no condition
 (Bégin & Proulx, 1996). Finally, for both the Hopfield and           could Kosko's BAM perform as well as the standard or the
 BHM network, the performance is reduced as the lengths of            short-term BHM.
 the sequences are increased.
       Similar results hold for the more difficult task of                            Discussion and Conclusion
 hetero-associations (5c and 5d) where a trend similar to             The results show that the modification to the learning rule of
 what was presented for auto-associations is presented.               the standard BHM leads to increased performance in
 However, in some conditions, such as simulations                     conditions where rapid learning is necessary. Those results
 performed on sequences of 4 and 5 inputs for the short-term          are similar to other research done in the field of
 BHM, the network could not systematically recall the task            bidirectional associative memory, which led to perfect
 100% of the time (5c and 5d). We therefore tested the                performance on recalls with little or no noise (5e and 5f).
 network using a value of             , which led to perfect          Moreover, the results for Kosko's BAM are drastically
 performance on recalls with little or no noise (5e and 5f).          reduced in comparison to the previous results on auto-
 Moreover, the results for Kosko's BAM are drastically                associations. In fact, the associative memories, such as
 reduced in comparison to the previous results on auto-               Nong & Bui (2012), in which a BAM model was proposed
                                                                 1921

with both faster convergence and improved performance.                 model of BAM alpha-beta bidirectional associative
However, the proposed network remains a compelling                     memories. Lecture Notes in Computer Science, 4263,
model of human cognition as it retains the original                    286-295.
properties of the standard BHM network (Chartier &                 Acevedo, M. E., Yanez, C., & Lopez, I. (2006b). Alpha-beta
Boukadoum, 2006), which was not the case in Nong & Bui                 bidirectional associative memories based translator.
(2012). In other words, the same BHM model can be used                 International Journal on Computational Science and
for both short-term and long-term memory encoding. The                 Network Security, 6, 190-194.
results show that the use of short-term memory is                  Bégin, J., & Proulx, R. (1996). Categorization in
advantageous for short sequences of inputs for both auto-              unsupervised neural networks: The eidos model. IEEE
and hetero-associations.                                               Transactions on Neural Networks, 7(1), 147-154.
    Although the objective of the study was not to model           Chartier, S., & Boukadoum, M. (2006a). A Sequential
human data, the fact that traces of older memories remain              Dynamic Heteroassociative Memory for MultiStep
encoded in the connection weights is an interesting                    Pattern Recognition and One-to-Many Association.
component to the model since real-time problem solving, as             IEEE Transactions on Neural Networks, vol. 17, no. 1,
constantly done by human beings, requires memory traces to             pp. 59-68.
be kept. This could lead to a priming effects, where learning      Chartier, S., Boukadoum, M. (2006b). A bidirectional
performance could increase, resulting in a faster learning             heteroassociative memory for binary and grey-level
during long-term memory encoding. On the other hand,                   patterns. IEEE Transactions on Neural Networks, 17(2),
erasing such information, as is done in Hopfield’s and                 385-396.
Kosko’s models, would force for constant relearning of the         Chartier, S., Boukadoum, M. (2011). Encoding static and
same associations. The BHM network proposed also                       temporal patterns with a bidirectional heteroassociative
exhibits some advantages in comparison to the Hopfield and             memory, Journal of Applied Mathematics, 2011, 1-34.
Kosko networks as the BAM network is much better a                 Fausett, L. (1994). Fundamentals of neural networks:
hetero-association and can tackle longer input sequences.              Architectures, algorithms, and applications. Upper
    In conclusion, this study introduces a modification to the         Saddle River, NJ: Prentice Hall, inc.
original learning rule of the BHM, which leads to improved         Haykin, S. (2009). Neural networks and learning machines.
performance when rapid learning is required. It showed that            (3rd ed.). Upper Saddle River, NJ: Pearson Education,
BAM models are well suited to perform associations using a             inc.
faster learning phase. Hence, they appropriately encompass         Hopfield, J. J. (1982). Neural networks and physical
biological and environmental limitations. Future research              systems with emergent collective computational
should evaluate the performances when the sequence is set              abilities. Procedings of the National. Academy of.
randomly between epochs (i.e. abc, bac, acb). In addition,             Science, 79(8), 2554-2558.
future research should explore the dynamics obtained as  is       Kosko, B. (1988). Bidirectional associative memories. IEEE
varied systematically under various dimensions of stimuli.             Transactions on Systems, Man and Cybernetics, 18(1),
The parameter could allow for more freedom on the number               49-60.
of items that can be kept in memory, which could vary in           Nong, H. T., & Bui, T. D. (2012). A fast iterative learning
relation to contextual settings. These results are a step              strategy for Bi-directional associative memory.
towards the construction of a larger model of working                  Proceedings of the 2012 Conference on Information
memory, since it is shown that the encoding of shorter                 Science and Technology, 2, 182-184.
sequences of input patterns leads to a different dynamic.          O'Reilly, R. C. (1998). Six principles for biologically-based
Therefore, artificial neural networks used for real time               computational models of cortical cognition. Trends in
problem solving (such as in robotics for example) should               Cognitive Sciences, 2(11), 455-462.
have a short-term memory component as well as a long-term          Ritter, G. X., Diaz-Deleon, J., L. & Sussner. (1999).
memory.                                                                Morphological bidirectional associative memories.
                                                                       Neural Networks, 12, 851-867.
                     Acknowledgments                               Storkey, J., & Valabregue, R. (1999). The basins of
This research was partly supported by the Natural Sciences             attraction of a new Hopfield learning rule. Neural
and Engineering Research Council of Canada. The authors                Networks, 12(6), 869-876.
would like to thank Kaia Myers-Stewart for her comments            Wu, Y. & Pados, D. A. (2000). A feedforward bidirectional
on an earlier version of this text.                                    associative memory. IEEE Transactions on Neural
                                                                       Networks, 11(4), 859-866.
                          References
Acevedo-Mosqueda, M. E., Yanez-Marquez, C., &
    Acevedo-Mosqueda, M. A. (2013). Bidirectional
    associative memories: Different approches. ACM
    Computing surveys, 45(2), 18:1-18:30.
Acevedo, M. E., Yanez, C., & Lopez, I. (2006a). A new
                                                               1922

