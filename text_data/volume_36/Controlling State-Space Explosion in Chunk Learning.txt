UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Controlling State-Space Explosion in Chunk Learning
Permalink
https://escholarship.org/uc/item/0kz1m313
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Veksler, Vladislav
Gluck, Kevin
Myers, Chistopher
et al.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                          Controlling State-Space Explosion in Chunk Learning
              Vladislav D. Veksler, Kevin A. Gluck, Christopher W. Myers, Jack Harris, Thomas Mielke
                                      Air Force Research Laboratory, Wright-Patterson AFB, USA
                              Abstract                                  can account for a wide range of behavioral phenomena, in-
                                                                        cluding classical conditioning (Pearce, 1994), base-rate ne-
   Varying combinations of perceptual cues may be relevant for
   learning and action-selection. However, storing all possible         glect (Gluck & Bower, 1988a, 1988b), and categorization
   cue combinations in memory is computationally implausible            (e.g. Gluck & Bower, 1988b; Veksler, Gray, & Schoelles,
   in sufficiently complex environments due to a state-space ex-        2007).
   plosion. Some psychological models suggest that cue combi-
   nations, i.e. chunks, should be generated at a conservative rate        The problem with a full chunk representation, where each
   (EPAM/CHREST; e.g. Feigenbaum & Simon, 1984). Other                  set of cues activates all possible combinations of those cues
   models suggest that chunk retrieval is based on statistical reg-     in memory, is that in complex environments too many chunks
   ularities in the environment (i.e. recency and frequency; An-
   derson & Schooler, 1991). We present a computational model           may be required. A mere ten binary perceptual inputs (e.g.
   of chunk generation based on these two principles, and demon-        black vs white, large vs small) may require over 59 thousand
   strate how combining these principles alleviates state-space ex-     chunks to be present in memory1 . If each perceptual input
   plosion, producing great savings in memory while maintaining
   a high level of performance.                                         allowed for five possible values (e.g. black, dark-grey, grey,
   Keywords: chunking, unitization, configural-cue, state-space         light-grey, white), ten such input dimensions could result in
   explosion, combinatoric explosion, learning, rational analysis       almost ten million chunks. Twenty such perceptual dimen-
                                                                        sions would result in 95 trillion chunks. One hundred inputs
                          Introduction                                  with ten values per input would result in more chunks than
A brown apple is neither just brown, nor just an apple, nor             there are atoms in the observable universe. Given these num-
just a brown apple. It is all of these things simultaneously,           bers, it is safe to say that the brain does not create a memory
and any of these representations may be important for both              element for each combination of input signals, and a com-
learning and action-selection. Concurrent representation of             putational system based on a full chunk representation is not
all cue combinations allows for learning of both generic rules          feasible in complex environments.
(e.g. apples taste good) and exceptions to those rules (e.g.               The exponential growth of memory based on combina-
brown apples are spoiled).                                              tions of perceptual cues is called the state-space explosion
   In the psychological literature a set of perceptual cues is          problem. This problem becomes even more severe when a
known as a chunk (e.g. Feigenbaum & Simon, 1984) or a                   chunk-based memory system is integrated with State-Action
configural-cue (Wagner & Rescorla, 1972). Having a single               and/or State-State connections (e.g. Gluck & Bower, 1988b;
memory chunk for representing a given combination of cues               Pearce, 1994; Veksler et al., 2007), where the total
allows for faster recognition of current state and faster action        amount of required memory may be an exponential function
selection (e.g. Goldstone, 2000). Chunk representation also             of the number of chunks, creating double-exponential mem-
aids in the ability to store greater amounts of information in          ory growth.
working memory (Ericsson, Delaney, Weaver, & Mahadevan,                    There is much psychological evidence that humans do not
2004). A commonly cited example of this phenomenon in-                  hold memory representations for all potential combinations
volves improved recognition of displayed chess positions by             of situational features. Rather, cues get merged over time to
chess experts compared to novices (Chase & Simon, 1973;                 make more and more complex object representations. There
Gobet, 1998). The theory is that experts will have been ex-             is evidence of this phenomenon in the context of percep-
posed to more chess situations, and will have learned many              tual unitization within hour-long psychological experiments
chunks for representing complex states on the chess board.              (e.g. Goldstone, 2000), as well as in life-long acquisition of
Thus a chess expert will need fewer chunks to represent a               subject-matter expertise (e.g. Feigenbaum & Simon, 1984).
chess position than a novice, who may have to memorize the                 In this paper we present a model of gradual chunk learn-
position as a series of figures and their locations. This theory        ing based on widely supported principles of human mem-
is further supported by the fact that experts are no better than        ory, and examine how this helps to alleviate the state-space
novices in recall of chess boards where pieces are placed ran-              1
                                                                              Given n cues (e.g. large, square, white), we can create a
domly (rather than positions encountered during actual game-            chunk for every combination of cue presence and absence ({large},
play).                                                                  {square}, {white}, {large, square}, {large, white}, {square, white},
   Each set of perceptual cues may potentially be recog-                and {large, square, white}). If we represent cue presence as a 1 and
                                                                        cue absence as a 0, we can represent each chunk as a binary number,
nized as multiple concurrently active chunks. Cues large,               and the total number of possible chunks is the total number of possi-
square, and white may activate seven different chunks:                  ble binary numbers (minus the blank chunk), which is 2n − 1. When
{large}, {square}, {white}, {large, square}, {large, white},            each cue dimension can have two potential values, the total number
                                                                        of possible chunks is 3n − 1. With k − 1 possible values on n feature
{square, white}, and {large, square, white}. Recognition of a           dimensions, we can have at most kn − 1 possible chunks to represent
set of perceptual cues as multiple concurrently active chunks           all potential cue combinations.
                                                                    3032

explosion. Specifically, we propose that state-space explo-         initiated it contains one base-level chunk for every percep-
sion is mitigated if new chunks are created via the union of        tual cue that may be observed in the environment, such that
existing memory chunks (1) only when none of the existing           if there are x possible cues, there are also x unique chunks
memory chunks fully represent the current state (similar to fa-     in memory, M, each containing its respective perceptual cue,
miliarization in EPAM/CHREST; e.g. Feigenbaum & Simon,              M = {{cue1 }...{cue x }}. When the model is exposed to some
1984; Gobet et al., 2001), and (2) only when the activation         perceptual input containing a set of cues, F, activation is
of existing chunks goes above threshold, where activation is        added to each existing memory chunk in set:
based on statistical regularities in the environment (i.e. re-
cency and frequency; Anderson & Lebiere, 1998; Anderson                                    S = {s|s ∈ M, s ⊆ F}
& Schooler, 1991). We present two simulations, demonstrat-
ing how combining these two principles alleviates state-space       Chunk activation decays over time. At any point the acti-
explosion, producing significant memory savings while main-         vation of chunk i, Ai , can be calculated as follows (vis-a-
taining a high level of performance. The simulations show           vis base-level activation formula from ACT-R; Anderson &
that the first of these principles slows down chunk generation,     Lebiere, 1998):
                                                                                                     n
although growth remains a linear function of time; whereas                                         X
                                                                                             Ai =        j +σ
                                                                                                        t−d
the second principle aids in more selective “rational” chunk
                                                                                                    j=1
generation, where growth is actually slower in highly variable
environments with more potential stimuli.                           where n is the number of presentations of chunk i, t j is the
                                                                    time elapsed since the jth presentation of i (in steps; t j ≥ 1)3 ,
           Gradual Chunk Learning via the                           d is the decay rate (for all simulations below decay rate is set
   Conservative-Rational principles of memory                       to 1.0, which is equivalent to hyperbolic decay; different de-
                                                                    cay rates would not qualitatively alter simulation results), and
According to Simon’s early computational models of learning         σ is noise (a small random number added to chunk activation
(EPAM; e.g. Feigenbaum & Simon, 1984), agent state may              to add stochasticity to the model).
be represented as a chunk – a mathematical set of perceptual            At each step the model attempts to retrieve two chunks, a
cues (e.g. {large, white, square}), and larger, more specific       and b, so as to add a new chunk to memory, M = M ∪ {a ∪ b},
chunks are generated by adding more cues to existing chunks         where a and b are the most active candidates from the set
(i.e. familiarization or chunking; {large, white, square} ∪         of chunks where no chunk is a subset of any other active
{rotated} = {large, white, square, rotated}). Familiarization       chunk, {i ∈ S |∀ j∈S i 1 j} (e.g. if the set of activated chunks is
in EPAM happens gradually, and only when the model does             {{white}, {large}, {rotated}, {oval}, {white, large}, {white,
not yet contain a chunk that includes the entire set of per-        large, rotated}}, then {white, large, rotated} and {oval} will
ceptual cues in the observed state. That is, early on states        be the only candidates for creating a new chunk). Chunks a
are represented in memory as individual cues, then as small         and b are retrievable only if their activations, Aa and Ab , are
chunks, and then, gradually, as chunks of greater and greater       both above the retrieval threshold parameter, ρ.
specificity.2                                                           Note that for all chunks s ∈ S the most recent elapsed time
   Anderson’s work on memory, in turn, suggests that chunks         t j is 1, and A s >= 1. Thus, when ρ <= 1 this model is
can be retrieved when their activation is above threshold.          just conservative and not rational, since every chunk in S is
Chunk activation is based on the statistical properties of the      above threshold, regardless of recency and frequency of prior
environment (rational analysis; Anderson & Lebiere, 1998;           activations.
Anderson & Schooler, 1991). If the set of cues that make                The following simulations demonstrate how the
up a chunk co-occur in recent history, and/or co-occur fre-         conservative-rational principles of memory can help to
quently, this chunk is deemed relevant, and thus more likely        alleviate the state-space explosion.
to be retrieved.
   Combining the conservative chunk growth and the ratio-            Simulation 1: Alleviating state-space explosion
nal analysis principles together, we propose a conservative-
                                                                    We ran the proposed chunk-learning model for 100,000 steps
rational chunk learning model. In general, the model may
                                                                    in environments with five and ten input dimensions, varying
be described as such: (1) new chunks are generated gradu-
ally via the union of retrieved existing memory chunks only              2
                                                                           EPAM/CHREST models actually replace smaller chunks with
when none of the existing chunks fully represent the current        larger ones during the familiarization process. An additional learn-
state, (2) chunks are retrievable only when their activation        ing process, discrimination, is used to recreate smaller chunks if
                                                                    these are deemed needed later. For current purposes we borrow
level goes above threshold, (3) the more active chunks are          only the EPAM chunk generation principles, and not the processes
more likely to be retrieved, and (4) chunk activation level is      of chunk deletion and recreation, as these are not compatible with
based on the recency and frequency of chunk occurrence in           the Reinforcement Learning model employed in Simulation 2.
                                                                         3
the environment (or, rather, the occurrence of the cues that               This model implementation is event-based, counting each
                                                                    perception-action cycle as a discrete step rather than in continu-
make up that particular chunk).                                     ous time. Assuming constant cycle times, these are computationally
   More formally, when the conservative-rational model is           equivalent. Elapsed time begins at 1, because at t j = 0, t−d
                                                                                                                               j = ∞
                                                                3033

the retrieval threshold parameter, ρ between 1.0 (no recency-              at each step). At each step the number of cues in the state
frequency effect) to 2.5. The results, displayed as averages               was chosen at random, with a minimum value of 2 and a
from ten model runs and compared to a model that created all               maximum value of 5 or 10 in the 5-dimensional and 10-
possible chunks, are displayed in Figure 1. For both five- and             dimensional environments, respectively. The cue value for
ten- dimensional inputs we examined chunk growth for static,               each respective cue dimension was a number between 1 and
constrained, and chaotic environments.                                     7, chosen at random, with replacement. That means any com-
   The static environment simulation consisted of repeatedly               binations of length greater than 2 had the potential to be pre-
exposing the model a set of five cues in a five-dimensional                sented in this environment (e.g. in the 5-dimensional envi-
environment (e.g. step 1: {a, b, c, d, e}, step 2: {a, b, c, d, e}, ...    ronment the states could be {a : 1, b : 1, c : 2}, {a : 7, b : 1},
step 100,000: {a, b, c, d, e}), and ten cues in a ten-dimensional          {b : 4, c : 5, d : 7, e : 7}).
one. In such an environment there are 31 possible chunks to                   A maximum of 32,767 chunks may be created in the 5x7
be created with 5 input dimensions, and 1023 possible chunks               chaotic environment, and 1,073,741,823 chunks in the 10x7
to be created with 10 input dimensions. However, the pro-                  one. The ρ = 1 model (no recency-frequency effects) gen-
posed model (at all ρ values) creates only 9 and 19 chunks in              erates on average 26,895 and 85,841 chunks in the 5x7 and
the 5- and 10- dimensional environments, respectively.                     10x7 environments, respectively. Note that in the 10x7 en-
                                                                           vironment the ρ = 1 model generates chunks at a pace of
                                                                           almost one per step. When the retrieval threshold is above
                                                                           1.0, chunks are generated at a much more conservative pace.
                                                                           At ρ = 1.5 the conservative-rational model creates on aver-
                                                                           age 856 and 2,736 chunks in the 5x7 and 10x7 environments,
                                                                           respectively. At ρ = 2.0 the model creates on average 228
                                                                           and 380 chunks in the 5x7 and 10x7 environments, respec-
                                                                           tively. At ρ = 2.5 the model creates no chunks beyond the
                                                                           initial 35 and 70 chunks in the 5x7 and 10x7 environments,
                                                                           respectively.
                                                                              Finally, the constrained environment was a simulated ran-
                                                                           dom walk in a room with various furniture by a robot with 5
                                                                           proximity sensors (at 0◦ , 10◦ , −10◦ , 45◦ , −45◦ ) or 10 prox-
                                                                           imity sensors (at 2◦ , −2◦ , 10◦ , −10◦ , 30◦ , −30◦ , 45◦ , −45◦ ).4
                                                                           Each proximity sensor reading was converted to an integer
                                                                           between 1 and 7. In this way the room walk had the potential
                                                                           to have similar complexity to the 5x7 and 10x7 chaotic envi-
                                                                           ronments, except that it was constrained by actual simulated
                                                                           robot movements. The room layout is displayed in Figure 2.
                                                                              In the simulated room walk, the all-chunks model gen-
                                                                           erates a total of 9,928 chunks and 1,528,477 chunks for 5-
                                                                           sensor and the 10-sensor robots, respectively. The ρ = 1
                                                                           model (no recency-frequency effects) generates on average
Figure 1. Chunks created after 100,000 steps in static, con-               4,988 and 19,183 chunks in the 5-sensor and 10-sensor envi-
strained, and chaotic 5-dimensional and 7-dimensional envi-                ronments, respectively. At ρ = 1.5 the conservative-rational
ronments. Standard error is negligible.                                    model creates on average 2,430 and 8,947 chunks in the 5-
                                                                           sensor and 10-sensor environments, respectively. At ρ = 2
                                                                           the model creates on average 1,238 and 4,293 chunks in the 5-
   This happens because chunks in the conservative-rational
                                                                           sensor and 10-sensor environments, respectively. At ρ = 2.5
model are created only when necessary. Thus, in the 5-
                                                                           the model creates on average 652 and 1,961 chunks in the
dimensional static environment where the model is con-
                                                                           5-sensor and 10-sensor environments, respectively.
stantly presented the state {a, b, c, d, e}, the model will be ini-
tiated with five chunks – {a},{b},{c},{d},{e} – and will cre-                 Note that as the environment becomes more complex, from
ate only four more chunks (for example, it might create                    static to constrained to chaotic, the number of chunks cre-
chunks in the following sequence {b, c}, {b, c, e}, {a, b, c, e},          ated in the all-chunks and ρ = 1 models increases. However,
and {a, b, c, d, e}). No other chunks will be generated be-                the ρ > 1 (recency-frequency) models create fewer chunks in
cause the chunk {a, b, c, d, e} is sufficient to represent the state       the chaotic than in the constrained environments. When the
{a, b, c, d, e}.                                                           world makes some sense, the principles of rational analysis
   In the chaotic environment the value of each dimensions                     4
                                                                                 All models in the 5-sensor environment were exposed to the
could take on one of seven unique values, or could be absent               same random walk, and all models in the 10-sensor environment
from the input state (with a minimum of two cues present                   were exposed to the same random walk.
                                                                       3034

                                                                    please see Veksler et al., 2007).
                                                                       The environment generator we used for this simulation was
                                                                    developed to examine various binary decision environments
                                                                    (e.g. to send a patient to CCU or not; to classify network
                                                                    activity as a cyber-attack or not). The generator creates de-
                                                                    cision cases based on different binary cue distributions and
                                                                    outcome rules. We used the generator to create an environ-
                                                                    ment approximating a generic threat detection task. On each
                                                                    trial in this environment each model had to classify a set of 10
                                                                    or 12 binary perceptual cues as either a threat or not a threat.
                                                                    Each model had to operate and adapt through four consecu-
                                                                    tive threat scenarios, each scenario lasting 500 trials. In the
                                                                    first scenario the threat could be identified via a combination
                                                                    of three randomly chosen perceptual cue values (e.g. when
                                                                    patient has red spots, and coughing, and reporting chest pain,
                                                                    classify as a threat). In the second, third, and fourth scenarios
                                                                    the threat could be identified via combinations of four, three,
Figure 2. Simulated room environment used in Simulation 1           and four randomly chosen perceptual cue values, respectively.
(drawn to scale from an actual room). All filled areas repre-       The probability of a threat on any given trial was 50%.5
sent obstacles at the robot sensor height (e.g. the twenty dots        The reward values for correctly classifying a threat and
at the top-left of the figure are chair and table legs).            missing a threat were relatively high (+10 and -10, respec-
                                                                    tively), the reward value for correctly rejecting a given sce-
                                                                    nario as a valid threat was relatively low (+1), the value of
help to derive which world features co-occur frequently and         sounding a false alarm was somewhere in between those two
continuously in the environment, whereas when the world is          (-5).5 The models were evaluated based on the average reward
pure noise, these principles suggest that allocating memory         per trial that they were able to achieve during the simulation.
and processing to various cue combinations is not warranted.           Figure 3 displays performance and number of chunks cre-
In other words, rational analysis helps to derive signal from       ated in this simulation for five models: (1) a model with all
noise, and allocate memory only to the most relevant cue            possible chunks, (2) the standard instance-based Reinforce-
combinations.                                                       ment Learning model, where each distinct combination of in-
   This type of “rational” chunk generation is much different       put values was treated as a unique state, (3) a feature-based
than merely creating chunks at a conservative rate. Even if         model, where no new chunks were created beyond the initial
chunks are only created once every few seconds, or only a           20 chunks in the 10-dimensional and 24 chunks in the 12-
small proportion of the time, chunk growth would be a linear        dimensional decision environments (one chunk for each po-
function of time, creating as many chunks from incidental cue       tential cue value), (4) a chunk learning model that employs
co-occurrences in the chaotic environment as from the co-           no recency-frequency principles (ρ = 1), and (5) the full
occurring features in the more ecologically-constrained en-         conservative-rational model (ρ = 1.5).
vironment. Alternatively, the recency/frequency principles             In the 10-dimensional environment the performance rank
provide an informed approach to chunk learning, based on            of the five models was as follows: the all-chunks model per-
the likelihood of chunk occurrence. Thus, in noisier environ-       formed best (average reward of 5.2 per trial over 2000 trials),
ments, where the potential for state-space explosion is great-      followed by the ρ = 1 model (-10.6%), then ρ = 1.5 (-13.2%),
est, the proposed model creates fewer chunks.                       feature-based (-28.6%), and instance-based models (-52.7%).
                                                                    In the 12-dimensional environment the performance rank of
           Simulation 2: But does it work?                          the five models was as follows: the all-chunks model per-
                                                                    formed best (average reward of 5.0 per trials over 2000 trials),
The above simulation demonstrates how the conservative-
                                                                    followed by the ρ = 1.5 model (-13.3%), then ρ = 1 (-13.8%),
rational principles of memory help to mitigate the state-space
                                                                    feature-based (-31.4%), and instance-based models (-91.1%).
explosion in chunk learning, producing as much as 99.99%
                                                                       All models, except the all-chunks model, performed bet-
memory savings. An important question, however, is whether
                                                                    ter in the second half of the simulation, as time was needed
such a low number of chunks can produce a level of perfor-
                                                                    for training and chunk generation. In the second half of the
mance similar to a model that generates all possible chunks.
                                                                    simulation performance shortfalls for the ρ = 1, ρ = 1.5,
To examine this, we integrated chunk-based memory with a
                                                                    feature-based, and instance-based models were -8.4%, -9.6%,
Reinforcement Learning action-selection mechanism (Sutton
& Barto, 1998), and examined model performances in a bi-                5
                                                                          Simulation parameters (e.g. threat probabilities, reward values)
nary decision environment (for more detail as to how chunk-         are reported here for purposes of replicability. Main result trends do
based memory is integrated with Reinforcement Learning,             not vary greatly with different simulation parameters.
                                                                3035

Figure 3. Performance and memory growth over time in 10- and 12- binary dimension decision environments. Error bars
represent standard error.
-26.5%, and -43.0% in the 10-dimensional environment, and        (e.g. Pearce, 1994). Configural (a.k.a. chunk) representa-
-6.1%, -6.2%, -27.3%, and -86.0% in the 12-dimensional en-       tion provides many cognitive advantages, such as extending
vironment, respectively.                                         the size of working memory (e.g. Ericsson et al., 2004), and
   Figure 4 displays the relationship between performance        providing representation for both generic rules and instance
and memory consumption in this simulation for both 10- and       exceptions (e.g. Gluck & Bower, 1988b). However, it is
12-dimensional environments (y-axis in log-scale). Although      computationally implausible to hold all possible cue configu-
it may be obvious that performance can be gained at the ex-
pense of memory, what may be less obvious is that (1) small
performance gains require exponential chunk growth, and (2)
as the environment becomes more complex (from 10 dimen-
sions to 12 dimensions) performance differences become less
distinguishable while memory expenses become more distin-
guishable.
   On a final note, the 10- and 12-dimensional binary state
decision environments are relatively simple. In more com-
plex environments the all-chunks model is computationally
infeasible due to state-space explosion. Additionally, the
ρ = 1 model produces nearly one chunk per time-step, which
makes it computationally infeasible in more persistent envi-
ronments. The full conservative-rational model (ρ > 1) may
be the needed alternative to produce high levels of perfor-
mance while cutting down on memory expenses.
              Summary and Discussion
                                                                 Figure 4. Chunk number versus performance in 10- and 12-
There is much psychological evidence that states are repre-      dimension binary decision environments.
sented as multiple varying configurations of perceptual cues
                                                             3036

rations in memory for environments of even modest complex-         search Associateship with the Cognitive Models and Agents
ity, because the number of chunks grows exponentially as a         branch at the Air Force Research Laboratory.
function of the number of perceptual cues. This is termed the
state-space explosion problem.                                                                  References
   We propose a model that helps to alleviate state-space ex-
plosion via (1) conservative chunk expansion (vis-a-vis famil-     Anderson, J. R., & Lebiere, C. (1998). The atomic components of
                                                                          thought. Mahwah, NJ: Lawrence Erlbaum Associates Pub-
iarization in EPAM/CHREST Feigenbaum & Simon, 1984),                      lishers.
and (2) rational selection of chunks for expansion (vis-a-vis      Anderson, J. R., & Schooler, L. J. (1991). Reflections of the Envi-
base-level activation in ACT-R; Anderson & Lebiere, 1998).                ronment in Memory. Psychological Science, 2(6), 396–408.
                                                                   Bolch, G., Greiner, S., de Meer, H., & Trivedi, K. S. (2006). Queue-
Simulation 1 demonstrates how the proposed model alleviates               ing networks and Markov chains: modeling and performance
the state-space explosion, especially in the extremely noisy              evaluation with computer science applications. John Wiley
environments. When chunk generation is based on statistical               & Sons.
                                                                   Chase, W. G., & Simon, H. A. (1973). Perception in chess. Cogni-
regularities in the environment (recency/frequency), it aids in           tive psychology, 4(1), 55–81.
distinguishing signal from noise among cue co-occurrences.         Derbinsky, N., & Laird, J. E. (2013). Effective and Efficient Forget-
Simulation 2 presents an analysis of model performance in                 ting of Learned Knowledge in Soar’s Working and Procedural
                                                                          Memories. Cognitive Systems Research, 24, 104–113.
generic binary decision environments. Results demonstrate          Douglass, S., Ball, J., & Rodgers, S. (2009). Large declarative mem-
a performance-memory tradeoff, where small improvements                   ories in ACT-R. In 9th international conference of cognitive
in performance require exponential sacrifices in memory use.              modeling, iccm 2009.
                                                                   Ericsson, K. A., Delaney, P. F., Weaver, G., & Mahadevan, R.
As the environment becomes more complex, small perfor-                    (2004). Uncovering the structure of a memoristâĂŹs supe-
mance improvements require even more memory use.                          rior âĂIJbasicâĂİ memory capacity. Cognitive Psychology,
   State-representation, state-space explosion, and memory-               49(3), 191–237.
size in general are among chief concerns for Artificial Intel-     Feigenbaum, E., & Simon, H. (1984). EPAM-like models of recog-
                                                                          nition and learning. Cognitive Science, 8(4), 305–336.
ligence and Cognitive Architectures (e.g. Bolch, Greiner, de       Gläscher, J., Daw, N., Dayan, P., & O’Doherty, J. (2010). States
Meer, & Trivedi, 2006; Douglass, Ball, & Rodgers, 2009;                   versus rewards: dissociable neural prediction error signals
Sutton & Barto, 1998; Uther & Veloso, 1998). The cur-                     underlying model-based and model-free reinforcement learn-
                                                                          ing. Neuron, 66(4), 585–595.
rent work focuses on limiting memory expansion based on            Gluck, M. A., & Bower, G. H. (1988a). Evaluating an Adaptive
recency/frequency principles, expanding only the memory el-               Network Model of Human Learning. Journal of Memory and
ements with the highest activation. An alternative approach,              Language, 27(2), 166–195.
                                                                   Gluck, M. A., & Bower, G. H. (1988b). From Conditioning to
proposed by Derbinsky and Laird (2013), employs the re-                   Category Learning - an Adaptive Network Model. Journal of
cency/frequency principles to delete memory elements with                 Experimental Psychology-General, 117(3), 227–247.
low activation. Future work will focus on employing these          Gobet, F. (1998). Expert memory: a comparison of four theories.
                                                                          Cognition, 66(2), 115–152.
principles in concert, to further decrease memory size.            Gobet, F., Lane, P. C. R., Croker, S., Cheng, P. C. H., Jones, G.,
   The number of connections needed between chunks, ac-                   Oliver, I., & Pine, J. M. (2001). Chunking mechanisms in
tions, and rewards may be another source of severe mem-                   human learning. Trends in cognitive sciences, 5(6), 236–243.
                                                                   Goldstone, R. L. (2000). Unitization during category learning. Jour-
ory growth. The time required at each cycle for learning and              nal of Experimental Psychology: Human Perception and Per-
action-selection processes would increase in proportion with              formance, 26(1), 86.
the number of connections in memory. For systems that focus        Pearce, J. M. (1994). Similarity and discrimination: a selective
                                                                          review and a connectionist model. Psychol Review, 101(4),
on learning State-Action-Reward transitions and employing                 587–607.
these in action-selection (Reinforecement Learning; Sutton         Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An
& Barto, 1998) the number of stored connections would be                  Introduction. Cambridge, Massachusetts: The MIT Press.
                                                                   Uther, W. T. B., & Veloso, M. M. (1998). Tree based discretiza-
a linear function of the number of chunks. For systems that               tion for continuous state space reinforcement learning. In
focus on State-Action-State transitions (e.g. Veksler, Gray, &            Aaai/iaai (pp. 769–774).
Schoelles, 2013; Voicu & Schmajuk, 2002), or systems that          Veksler, V. D., Gray, W. D., & Schoelles, M. J. (2007). Catego-
                                                                          rization and Reinforcement Learning: State Identification in
focus on both transition types (e.g. Gläscher, Daw, Dayan,                Reinforcement Learning and Network Reinforcement Learn-
& O’Doherty, 2010; Veksler, Myers, & Gluck, 2014), it                     ing. In 29th annual meeting of the cognitive science society,
would be an exponential function. In addition to minimizing               cogsci07. Nashville, TN.
                                                                   Veksler, V. D., Gray, W. D., & Schoelles, M. J. (2013). Goal-
the number of chunks in memory, future work will explore                  Proximity Decision Making. Cognitive Science, 37(4), 757–
how the principles discussed in this paper may be employed                774. doi: 10.1111/cogs.12034
to create connections and to prune them in a more conserva-        Veksler, V. D., Myers, C. W., & Gluck, K. A. (2014). SAwSu: An
                                                                          Integrated Model of Associative and Reinforcement Learn-
tive and rational manner.                                                 ing. Cognitive Science.
                                                                   Voicu, H., & Schmajuk, N. (2002). Latent learning, shortcuts
                    Acknowledgements                                      and detours: a computational model. Behavioural Processes,
                                                                          59(2), 67–86.
This research was funded by grant #13RH06COR from the              Wagner, A. R., & Rescorla, R. A. (1972). Inhibition in Pavlovian
Air Force Office of Scientific Research, and partly conducted             conditioning: Application of a theory. Inhibition and learn-
while the first author held a National Research Council Re-               ing, 301–336.
                                                               3037

