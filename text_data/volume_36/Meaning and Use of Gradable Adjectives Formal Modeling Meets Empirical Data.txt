UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Meaning and Use of Gradable Adjectives: Formal Modeling Meets Empirical Data
Permalink
https://escholarship.org/uc/item/52p9b2kj
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Qing, Ciyang
Franke, Michael
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

Meaning and Use of Gradable Adjectives: Formal Modeling Meets Empirical Data
                        Ciyang Qing (qciyang@gmail.com)                  Michael Franke (m.franke@uva.nl)
                               Institute for Logic, Language and Computation, University of Amsterdam
                                  Science Park, Kruislaan 107, 1098 XG Amsterdam, The Netherlands
                              Abstract                                    pragmatic applicability, and it is this component of the model
                                                                          that we test empirically. We fit our model to data that was
    The meaning of gradable adjectives is highly context-
    dependent, and is notoriously difficult to capture precisely. Re-     gathered by Solt and Gotzner (2012) (for a quite different pur-
    cent work in theoretical linguistics suggests that the way we         pose) and test model predictions against data from our own
    use gradable adjectives can be explained in terms of optimal          replication of their experiment. Despite its simplicity, our
    language use. To test this hypothesis we formulate a prob-
    abilistic speaker model that combines ideas from Bayesian             optimality-based model explains the data astonishingly well.
    approaches to pragmatic reasoning as social cognition with                The next section first introduces our speaker-based proba-
    broader optimality considerations, as suggested by evolution-         bilistic model. Thereafter we detail Solt and Gotzner’s (2012)
    ary linguistics. We demonstrate that, despite its simplicity, the
    model explains empirical data on the applicability of adjectives      experiment and our replication. We discuss the model’s fit to
    in context astonishingly well.                                        the data and conclude with a critical reflection.
    Keywords: gradable adjectives; context-dependence; natural
    language production/generation; Bayesian cognitive modeling                    Optimal Use of Gradable Adjectives
                                                                          One of the most fundamental reasons of using gradable ad-
                          Introduction                                    jectives is to ensure communicative success when trying to
The meaning and use of gradable adjectives like tall, big, dark           describe a referent, e.g., such as to pick it out, or learn about
or full is elusive in manifold ways and continues to inspire              its properties. To make this intuition more concrete, we adopt
research in not only linguistics and psychology, but also ma-             a degree-based approach to the semantics of gradable adjec-
chine learning and other related fields. It is notoriously diffi-         tives (Kennedy & McNally, 2005; Kennedy, 2007). Degree-
cult to pin down the meaning and use of gradable adjectives               based approaches hold that a sentence of the form “x is A”
mainly because of their context-dependence and vagueness.                 is true iff the degree dA (x) to which object x has property A
Whether a sentence like “John is tall” is felt to be true or              exceeds a contextually given standard of comparison θ, i.e.,
pragmatically appropriate depends on the context in which                 dA (x) ≥ θ. However, exactly how θ is conventionally derived
the sentence is used, in particular on a contrasting set of indi-         from the context is left open. Following the general tenet of
viduals against which John is to be compared (in terms of his             evolutionary linguistics that language conventions are shaped
height). The goal of the work presented here is to shed light             to achieve optimal communicative success, we propose to fill
on this immediate context-dependence of gradable adjectives.              this gap by defining which values of θ are optimal in a given
    Building on previous work in the same direction (Barner               context. The motivating idea behind our production model is
& Snedeker, 2008; Schmidt, Goodman, Barner, & Tenen-                      then that speakers employ a standard of comparison θ with
baum, 2009; Syrett, Kennedy, & Lidz, 2010), we investi-                   a probability proportional to the communicative efficiency of
gate the dependence of intuitive judgements of applicability              using θ as a general convention.
of gradable adjectives on statistical properties of a visually                Specifically, we measure communicative efficiency of θ in
presented comparison class. The key novelty of the present                terms of the extent to which the utterance “x is A” would help
proposal is a fully predictive probabilistic model that formal-           resolve the (possibly implicit and hypothetical) Question un-
izes the idea that the applicability of gradable adjectives is de-        der Discussion “how A is x?” against the background of a
termined by pragmatic reasoning about optimal language use                contextually given comparison class of objects with varying
in context. The model we present enriches previous Bayesian               levels of A-ness. We use p to denote the common knowl-
approaches to pragmatic reasoning in terms of social cogni-               edge about prior probability distribution of A-degrees in the
tion (e.g. Frank & Goodman, 2012; Goodman & Stuhlmüller,                 comparison class. For example, if the conversation is about
2013; Lassiter & Goodman, 2013) with ideas borrowed from                  basketball players’ heights, then p is the prior distribution of
evolutionary game theory (Potts, 2008; Franke, 2012).                     the height of a basketball player from one’s common world
    More concretely, the model presented here is superficially            knowledge. (Here we only consider discrete degree scales but
similar, but conceptually distinct from the interpretation-               it is easy to adapt the model to continuous density functions.)
based model of Lassiter and Goodman (2013). In contrast to                    When the conventional standard of comparison θ is fixed,
the latter, the model we present here makes straightforward               a literal listener ρ0 , upon hearing the utterance “x is A,” learns
predictions about both comprehension and production. The                  from its semantic truth that the actual degree dA (x) ≥ θ. Thus
key assumption in our modeling is that the speaker’s prag-                his new belief about dA (x), denoted as ρ0 (dA (x) | A; θ), is
matic reasoning is anchored in considerations of global opti-             the conditional probability p(dA (x) | dA (x) ≥ θ). If on the
mality of the conventional usage conditions. We argue that                other hand the speaker says nothing, the literal listener has
it is the production side that is responsible for judgements of           no additional information and thus his belief stays the same:
                                                                      1204

ρ0 (dA (x) | N; θ) = p(dA (x)).                                        vice versa. A positive c means that the gradable adjective is
   The communicative efficiency of using θ as a conventional           generally applicable to every individual in the context, like
standard of comparison for a comparison class, is then de-             in the above example for open. Thus a lower θ is preferred,
fined as the speaker’s expected chance of success in making            modulo the effect of contextual optimality. In contrast, a neg-
the listener believe in the actual degree dA (x) of an individual      ative c means that the gradable adjective is generally inap-
x randomly chosen from that comparison class. For example,             plicable, so a higher θ is preferred. The absolute value of c
to measure how efficient a standard θ of “tall” is for describ-        reflects the interaction between communicative efficiency and
ing basketball players, we calculate on average how likely the         absolute general applicability. If c is close to 0, it means that
speaker will manage to convey the height of a random basket-           communicative efficiency is the dominant factor in determin-
ball player by adopting that standard. Technically, we have:           ing θ, and if c is away from 0, then the absolute sense trumps
                                                                       communicative efficiency. We include this factor to assess
         ES(θ) =      ∑      p(dA (x)) · ρ0 (dA (x) | N; θ)            whether an absolute sense of applicability of adjectives criti-
                   dA (x)<θ
                                                                       cally improves empirical predictions.
                   +     ∑     p(dA (x)) · ρ0 (dA (x) | A; θ) . (1)       Using a standard soft-max function (e.g. Luce, 1959), we
                      dA (x)≥θ                                         capture threshold choices in production as the probability:
The first summand corresponds to individuals whose A-                                   Pr(θ; λ, c) ∝ exp(λ ·U(θ; c)) .               (3)
degrees are lower than the conventional standard θ and thus
the positive form A cannot be truthfully asserted. The second          The intuition is that the higher the utility, the higher the prob-
summand corresponds to individuals whose A-degrees are no              ability with which speakers would adhere to standard θ, if
less than θ. The speaker can truthfully utter “x is A” and the         they use language optimally, but actual speakers might make
listener can update his prior with the information dA (x) ≥ θ,         mistakes of various sorts and thus be sub-optimal, as captured
which increases his chance of believing in the actual degree.          by the degree of rationality parameter λ.
   This formula captures a general tradeoff between informa-              The production probability of using positive form A for de-
tivity and applicability (c.f. Lassiter & Goodman, 2013):              gree d can be naturally defined as the sum probability of all
when the threshold θ is high, ρ0 (dA (x) | A; θ) is high when          threholds no greater than d (Lassiter, 2011):
dA (x) ≥ θ, so the positive form is very informative. E.g., if
θ = 2.2m, the listener would have a good sense of the true                             σ(A | d; λ, c) =  ∑ Pr(θ; λ, c)    .           (4)
height of a basketball player when he is described as “tall.”                                            θ≤d
However, the positive form will seldom be applicable, since
                                                                       We can further derive a pragmatical listener’s interpretation
few individuals will have degrees that exceed the threshold.
                                                                       rule by applying Bayes’ rule:
Thus such a θ is on average inefficient. For lower θ the pos-
itive form is often applicable, but this time ρ0 (dA (x) | A; θ)                      ρ(d | A; λ, c) ∝ p(d) · σ(u | d; λ, c),         (5)
does not improve much from the prior p(dA (x)). E.g., if
θ = 1.8m, then a basketball player being described as “tall”           but we will focus here on the production rule (4).
would tell very little about his actual height, which is ineffi-
cient as well. Hence an optimal θ should strike a good balance                                Empirical Data
between informativity and applicability.                               In order to test the predictive power of the above production
   If communicative efficiency is the only factor that mat-            model, we collected participants’ intuitive judgements of the
ters, then conversational participants should strive for contex-       pragmatic applicability of the positive forms of several ad-
tual standards of comparison that are optimal in this respect.         jectives when confronted with comparison classes of vary-
However, theoretical linguists give good arguments that the            ing statistical composition. Our design is that of Solt and
lexical properties of a gradable adjective also set constraints        Gotzner (2012), with minor modifications. We will introduce
on its general applicability (e.g. Kennedy & McNally, 2005;            our replication first and then mention these minor differences.
Kennedy, 2007). For instance, suppose there is a building
whose windows are open to various extent, one might be in-             Participants, Materials and Methods 96 US participants
clined to describe a window as open even if it is actually the         were recruited via Amazon’s Mechanical Turk. Each of them
least open among all the windows. In order to capture this             received $0.25 for the experiment.
aspect, we define the utility of conventional threshold θ as:             We tested intuitive applicability judgements for four grad-
                                                                       able adjectives: big, dark, tall and full. For each adjective,
             U(θ; c) = ES(θ) + c ·         ∑     p(dA (x)),     (2)
                                        dA (x)≥θ
                                                                       we presented contexts of 36 items. Each item instantiated the
                                                                       adjective in question to one out of 14 possible degrees (balls
where c is a “coverage parameter” that measures the extent             varying in size, grey rectangles varying in lightness, cartoon
to which a gradable adjective’s absolute sense of applicabil-          characters varying in height, glasses varying in water level;
ity affects the standard of comparison. The higher c the more          see Fig. 1a). We chose mostly abstract items so as to mini-
using the adjective is preferred over not saying anything, and         mize the effect of participants’ background world knowledge.
                                                                   1205

                                           Prior           d1    d2   d3     d4     d5    d6   d7   d8     d9    d10     d11
                                           baseline        1     2    3      4      5     6    5    4      3     2       1
                                           left-skewed     2     5    6      6      5     4    3    2      1     1       1
                                           right-skewed    1     1    1      2      3     4    5    6      6     5       2
                                           moved           1     2    3      4      5     6    5    4      3     2       1
            (a) Example items                                (b) Number of items for each degree in each prior condition
                               Figure 1: Stimuli used in our replication of Solt & Gotzner’s study
Stimuli were designed to make all 13 differences between ad-             Results of their experiment are shown in Fig. 3 (blue lines).
jacent degrees perceptually uniform.                                  We can see that the result of our replication is close to theirs
   We included 4 kinds of contextual prior distributions in our       in most conditions, except for the baseline and left-skewed
experiment. Each context consisted of 36 items spanning over          conditions for tall. Since our main purpose here is to use these
11 out of the 14 degrees. The baseline, left-skewed and right-        data to test our model, we skip reporting further statistical
skewed priors span over the lower 11 degrees with different           analysis of the data themselves in the interest of space.
distributions, and the moved prior spans over the upper 11
degrees (4th–14th) and has the same shape of distribution as             Parameters Learning and Model Validation
the baseline. Fig. 1b shows the number of items for each              Our model has free parameters: λ (rationality) and cA (ab-
degree in the 4 distributions.                                        solute applicability of adjective A). We will use Bayesian
   Each participant finished 4 trials. In each trial they saw         inference (MacKay, 2003) to learn likely values of these pa-
a context corresponding to 1 of the 4 adjectives under 1 of           rameters from the data of Solt and Gotzner (2012), and then
the 4 priors and were asked to check all items for which they         test the model’s predictions on our own replication.
would use the adjective in the given context (Fig. 2). We used           We assume the following binomial process that generates
a Latin square design for adjective-prior combinations within         data in both experiments: for each adjective A and prior p,
the 4 trials and counterbalanced the order of adjectives.
                                                                                      A,p             A,p
                                                                                     ni   ∼ Binom(Ni      , σ p (A | di ; λ, cA )),         (6)
                                                                                 A,p
                                                                      where ni is the number of items of degree di checked by
                                                                      participants in the condition with adjective A and prior p, and
                                                                        A,p
                                                                      Ni is the total number of items of degree di in this condi-
                                                                      tion.1 Hence, for a given adjective, λ and cA , for each 1 of
                                                                      the 4 priors, our model makes predictions for all 11 degrees.
                                                                      Thus the model makes 44 predictions for each adjective.
                                                                      Parameters Learning We assume that λ is a constant,
                                                                      while each adjective has its own parameter cA . This is be-
                                                                      cause λ is the general degree of rationality in our sample
                                                                      population, whereas different adjectives could have different
                    Figure 2: A sample trial                          senses of general applicability cA depending on their lexical
                                                                      properties. With this, we use the following priors:
Qualitative Results The results are shown in Fig. 3. As ex-
pected, proportions of intuitive applicability judgements fol-                        λ ∼ Unif(0, 100) cA ∼ Unif(−1, 0),                    (7)
lowed an S-shaped curve rising from lower to higher degrees.
                                                                      where A ∈ {big, dark, tall}. We draw 8000 samples (after
More importantly, the statistical distribution of the contextual
                                                                      a burn-in period of 9000 samples) from the posterior dis-
comparison class had an apparent influence on the applicabil-
                                                                      tribution P(λ, c | DSG ), i.e., we make a joint inference of
ity judgements. E.g., when there are many high-degree items
                                                                      λ, ctall , cdark , cbig from the dataset DSG (Solt & Gotzner,
such as in the right-skewed condition, smaller proportion of
                                                                      2012). For these posterior samples of parameters, we have
low-degree items were chosen.
                                                                          1 Note that we allowed participants to check none of the pictures,
The Original Dataset The experiment by Solt and Gotzner               and some participants did not check all the pictures with the high-
                                                                      est degree. In order to take these possibilities into account, we in-
(2012) had 194 participants in total (47 – 50 participants in         troduce an unobserved maximal degree d12 with prior probability
each condition). Test items included big, tall, and dark, but         p(d12 ) = 0. Since this degree corresponds to a θ according to which
also pointy instead of full. We chose full primarily because          the positive form is never used, the utility associated with it is rather
                                                                      small. Nevertheless, the soft-max function will assign a small non-
pointy is a rather unusual word and it is hard to construct           zero probability to it, and hence the model always predicts that d11
items with uniformly spaced degrees of “pointiness.”                  might have a small probability not to be checked.
                                                                  1206

                                                                    2   4    6    8   10                                    2   4     6     8   10
                                                  big                        big                           big                       big
                                                baseline                left−skewed                  right−skewed                   moved
                                                                                                                                                     1.0
                                                                                                                                                     0.8
                                                                                                                                                     0.6
                                                                                                                                                     0.4
                                                                                                                                                     0.2
                                                                                                                                                     0.0
                                                  dark                       dark                         dark                       dark
                                                baseline                left−skewed                  right−skewed                   moved
                                1.0
                                0.8
  Proportion of Items Checked
                                0.6
                                0.4
                                0.2
                                                                                                                                                           Solt & Gotzner
                                0.0                                                                                                                        Model Pred.
                                                  tall                       tall                          tall                      tall                  Replication
                                                baseline                left−skewed                  right−skewed                   moved
                                                                                                                                                     1.0
                                                                                                                                                     0.8
                                                                                                                                                     0.6
                                                                                                                                                     0.4
                                                                                                                                                     0.2
                                                                                                                                                     0.0
                                                  full                       full                          full                      full
                                                baseline                left−skewed                  right−skewed                   moved
                                1.0
                                0.8
                                0.6
                                0.4
                                0.2
                                0.0
                                       2    4      6       8   10                               2    4     6   8    10
                                                                                       Degree
Figure 3: Observed and predicted applicability judgements for each degree for each adjective-prior pair. The blue and green
curves show the observed proportions of items checked in each condition. The pink curve shows the mean posterior predictive
values of the model when condition on the data by Solt & Gotzner.
¯ = 48.23, sd = 1.14; c̄big = −.064, sd = .003; c̄tall =
λ                                                                                                        with overall R2 = .96 and p < .001 for all cases). Correlations
−.024, sd = .003; c̄dark = −.054, sd = .002.                                                             remain highly significant even when we only keep those data
   Since Solt and Gotzner (2012) did not include full in their                                           points for which our model’s prediction is within the range of
experiment, we cannot learn the parameters directly from                                                 (0.05, 0.95) (R2big = .93, R2dark = .94, R2tall = .88, R2full = .90,
their dataset. Instead, we use the posteriors from their dataset                                         with overall R2 = .90 and p < .001 for all cases). This sug-
to constrain the parameter λ:                                                                            gests that our model does capture the general trend of par-
                                                                                                         ticipants’ choices, rather than by simply assigning extreme
                                 λfull ∼ Norm(48.23, 1.14) cfull ∼ Unif(−1, 0) .            (8)
                                                                                                         probabilities to extreme degrees.
       ¯ full = 46.67, sd = .904; c̄full = −.158, sd = .006.
We get λ
                                                                                                            Second, in order to better diagnose the model’s predictions
                                                                                                         for each data point, we investigate the posterior predictive dis-
Model Validation We validate our model in two ways.                                                      tribution (c.f. Kruschke, 2011). Concretely, for each of the
  First, we use Bayes model averaging (Hoeting, Madigan,                                                 8000 samples of parameters drawn from the posterior distri-
Raftery, & Volinsky, 1999)                                                                               bution described before, we use the binomial generative pro-
                                                                                                         cess (6) to generate a new dataset. Thus in the end we have
                                  σ(u | d, DSG ) = ∑ P(λ, c | DSG ) · σ(u | d; λ, c),       (9)
                                                       λ,c
                                                                                                         8000 simulated datasets. Then for each adjective, each prior
                                                                                                         and each degree, we look at the number of items checked in
to compute the model’s predictions after it learns the free pa-                                          the actual dataset (either DSG or Drep ) and record the fre-
rameters from DSG . The predictions are shown in Fig. 3 (pink                                            quency of this actual observation in the simulated datasets.
lines), and Fig. 4 shows the relation between model predic-                                              Finally, we calculate the posterior predictive credibility value
tions and participants’ choices for each adjective on the repli-                                         as the sum of relative frequencies of all observations that oc-
cation dataset Drep . Model predictions correlate well with                                              curs no more often than the actual observation in the sim-
observations (R2big = .97, R2dark = .98, R2tall = .94, R2full = .95,                                     ulated datasets. This posterior predictive credibility value
                                                                                                    1207

                                                                                            0.0   0.2   0.4   0.6   0.8    1.0                                               0.0   0.2   0.4          0.6   0.8     1.0
                                                                   big                                    dark                                     tall                                        full
                                          1.0
              Participants' Choice Rate
                                          0.8
                                          0.6
                                          0.4
                                          0.2
                                          0.0
                                                0.0   0.2    0.4         0.6   0.8   1.0                                         0.0   0.2   0.4          0.6    0.8   1.0
                                                                                                                     Model Prediction
             Figure 4: The relation between model predictions and participants’ choices on the replication dataset
Table 1: Posterior predictive credibility values. The left value is for Solt and Gotzner’s data, the right for our replication. Values
in bold are those where the test values fall below a critical value of .05 for both data sets.
    Adj     Prior                                           d1            d2               d3           d4            d5               d6                  d7                d8          d9                       d10       d11
    big     baseline                                        1/1           1/1              1/1          .08/.63       .04/.06          .92/.08             .10/.59           .06/.32     .01/.06                  .22/.21   .41/.03
    big     left-skewed                                     1/1           .64/1            .33/.19      .01/0         .16/.02          .19/.01             .12/.42           .12/.24     0/.02                    .25/.01   1/1
    big     right-skewed                                    1/1           1/1              1/1          1/1           .17/1            .01/1               .06/.23           .01/.66     .24/.04                  0/0       .10/1
    big     moved                                           1/1           .27/1            .60/1        .52/.64       .58/.16          .92/.64             .12/.20           .21/.68     0/.05                    .04/.83   .40/.17
    dark    baseline                                        1/1           1/1              1/1          .57/.42       .87/.02          .05/.89             .19/.53           .30/.67     0/.04                    0/.37     .39/1
    dark    left-skewed                                     1/1           .43/1            1/.08        .65/.01       .37/.34          0/.59               .33/.24           .84/.02     .02/.16                  .08/.41   .42/1
    dark    right-skewed                                    1/1           1/1              1/1          1/1           1/1              .19/.12             .09/0             0/0         .01/.13                  1/0       .24/.01
    dark    moved                                           1/1           1/1              .64/1        .39/.07       .62/.80          .30/.89             .07/1             .83/.15     .04/.52                  .21/.36   .64/.13
    tall    baseline                                        1/1           .64/1            .08/.43      0/.03         0/.01            0/.01               0/0               0/0         .09/.48                  .09/1     1/1
    tall    left-skewed                                     1/1           .01/.18          0/0          0/0           0/0              .31/0               .36/0             .58/1       .01/.30                  .01/1     .12/1
    tall    right-skewed                                    1/1           1/1              1/1          .64/1         .19/1            .01/.37             .23/.08           0/.10       0/.19                    .41/.01   .12/0
    tall    moved                                           1/1           .65/1            .08/.41      0/1           .91/.74          .46/1               .17/.08           .77/.53     .30/.14                  .09/1     .05/.07
    full    baseline                                        –/1           –/1              –/1          –/1           –/1              –/.45               –/0               –/0         –/0                      –/1       –/.68
    full    left-skewed                                     –/1           –/1              –/1          –/1           –/.06            –/0                 –/.02             –/.07       –/.43                    –/.83     –/1
    full    right-skewed                                    –/1           –/1              –/1          –/1           –/1              –/1                 –/0               –/0         –/.38                    –/0       –/.06
    full    moved                                           –/1           –/1              –/1          –/1           –/0              –/.01               –/.02             –/0         –/.05                    –/.25     –/.41
then captures the estimated maximal threshold on credibil-                                                                       alizes well in the moved condition but performs poorly on the
ity thresholds under which the observed data would not con-                                                                      baseline and left-skewed conditions.
tradict our model. Concretely, a value of .05 means that the                                                                        The two validation methods both suggest that the model in
observed data falls within a 95% HDI interval of the poste-                                                                      general captures participants’ applicability judgements well.
rior predictive; a value of 1 means that the observation was
the mode of our posterior predictive sampling.
                                                                                                                                 Need for General Applicability It remains to be checked
   The posterior predictive credibility values are shown in Ta-                                                                  whether the data can be explained reasonably well in terms
ble 1. We can see that the model’s predictions generally pass                                                                    of contextually optimal language use alone, or whether there
the predictive check. For those degrees where the model fails                                                                    is reason to believe that there are further absolute criteria that
to meet a critical threshold of .05 on both data sets (marked                                                                    regulate the use of adjectives, beyond the immediate statisti-
in bold), we note two possible sources of bad fit: (1) The                                                                       cal properties of the comparison class. To test this, we can
discrepancy between the two datasets due to noise. As a re-                                                                      use the Savage-Dickey method to calculate the Bayes factor
sult, the model fitting the training set can fail to generalize to                                                               in favor of a model Mc=0 which assumes that there is no ab-
the test set. We also want to emphasize here that since the                                                                      solute re-alignment of θ necessary with the model Mc≤0 we
model needs to fit all degrees under all priors simultaneously,                                                                  fitted to the data (e.g. Wagenmakers, Lodewyckx, Kuriyal, &
noises in one degree might influence performance on another                                                                      Grasman, 2010). By this method we compute the factor with
degree as well. (2) The discrepancy between the two datasets                                                                     which our posterior credence should shift towards Mc=0 as:
due to differences in stimuli. For instance, the stimuli for tall
generally have greater height-to-width ratio in the experiment                                                                                                  P(D|Mc=0 ) P(c = 0|Mc≤0 , D)
                                                                                                                                                                           =                 .
by Solt and Gotzner (2012) than in our replication. As a re-                                                                                                    P(D|Mc≤0 )   P(c = 0|Mc≤0 )
sult, the general applicability parameter is probably greater
for their contextual comparison classes than for those in the                                                                    The denominator of the right-hand fraction is 1, but the nu-
replication dataset. This might explain why the model gener-                                                                     merator is so small that our finite sampling procedure cannot
                                                                                                                          1208

assign a non-zero value to it. This holds true for all adjec-           and short based on the size distributions of novel noun ref-
tives involved. Consequently, we should conclude that the               erents. Child development, 79(3), 594–608.
data suggests strongly that our model needs some absolute             Frank, M. C., & Goodman, N. D. (2012). Predicting prag-
upward shifting of θ. But, as noted before, the estimated pos-          matic reasoning in language games. Science, 336(6084),
terior values of c vary from adjective to adjective. As ex-             998.
pected from formal semantic accounts that distinguish abso-           Franke, M. (2012). On scales, salience & referential lan-
lute adjectives like full from relative adjectives like tall, the       guage use. In M. Aloni, F. Roelofsen, & K. Schulz (Eds.),
data suggests that the absolute re-shifting needed to account           Amsterdam colloquium 2011 (pp. 311–320). Springer.
for the applicability of full is substantially higher.                Gatt, A., van Gompel, R. P. G., van Deemter, K., & Kramer,
                                                                        E. (2013). Are we bayesian referring expression genera-
                          Conclusion                                    tors? In M. Knauff, M. Pauen, N. Sebanz, & I. Wachsmuth
Combining the idea of pragmatic reasoning as social cogni-              (Eds.), Proceedings of CogSci 35.
tion and optimality considerations from evolutionary linguis-         Goodman, N. D., & Stuhlmüller, A. (2013). Knowledge
tics, the presented model is a fully generative cognitive model         and implicature: Modeling lanuage understanding as social
that successfully predicts intuitive applicability judgements           cognition. Topics in Cognitive Science, 5, 173–184.
of gradable adjectives in various contexts.                           Hoeting, J. A., Madigan, D., Raftery, A. E., & Volinsky, C. T.
   Despite the model’s noteworthy empirical success, it                 (1999). Bayesian model averaging: a tutorial. Statistical
should be noted that everyday language use is far more com-             science, 382–401.
plex and implicit than our highly simplified and controlled           Kennedy, C. (2007). Vagueness and grammar: The semantics
experiments suggest. Hence more work needs to be done to                of relative and absolute gradable adjectives. Linguistics and
ultimately account for naturalistic language data. For exam-            Philosophy, 30, 1–45.
ple, we effectively assumed that participants are fully aware         Kennedy, C., & McNally, L. (2005). Scale structure, de-
of the exact distribution of degrees in the comparison class,           gree modification, and the semantics of gradable predi-
which is too idealized even for the artificial contexts in our          cates. Language, 81(2), 345–381.
experiment. A more comprehensive model would include                  Kruschke, J. E. (2011). Doing bayesian data analysis.
participants’ latent representations of degrees and their esti-         Burlington, MA: Academic Press.
mated contextual distribution. Preliminary results from such          Lassiter, D. (2011). Vagueness as probabilistic linguistic
modeling suggest that this improves predictive power. This              knowledge. In R. Nouwen, R. van Rooij, U. Sauerland,
is so because latent priors over degrees, estimated separately          & H.-C. Schmitz (Eds.), Vagueness in communication (pp.
for each adjective-prior pair, can capture participants expec-          127–150). Springer.
tations about unrepresented degrees as well. Ever taller bas-         Lassiter, D., & Goodman, N. D. (2013). Context, scale struc-
ketball players, though increasingly unlikely, are conceivable,         ture, and statistics in the interpretation of positive-form ad-
while glasses will reach a maximally saturated degree of full-          jectives. In T. Snider (Ed.), Proceedings of the 23rd se-
ness. That’s why estimated latent priors are prone to improve           mantics and linguistic theory conference (SALT 23) (pp.
predictive power because they can accommodate this kind of              587–610).
conceptual knowledge implicitly.                                      Luce, D. R. (1959). Individual choice behavior: A theoretical
   On a conceptual level, we advanced the hypothesis that the           analysis. New York: Wiley.
use of gradable adjectives is driven by optimality of descrip-        MacKay, D. J. (2003). Information theory, inference and
tive language use. This contrasts with explanations based on            learning algorithms. Cambridge university press.
optimal contextual categorization (e.g. Schmidt et al., 2009)         Potts, C. (2008). Interpretive Economy, Schelling Points, and
or based on referential language use (e.g. Franke, 2012; Gatt,          evolutionary stability. (Manuscript, UMass Amherst)
van Gompel, van Deemter, & Kramer, 2013). It is possible to           Schmidt, L. A., Goodman, N. D., Barner, D., & Tenenbaum,
think that the identification of x’s degree of A-ness is concep-        J. B. (2009). How tall is tall? compositionality, statistics,
tually prior but subservient also to optimal categorization and         and gradable adjectives. In Proceedings of CogSci 31.
the use of gradable adjectives in referential expressions, but        Solt, S., & Gotzner, N. (2012). Experimenting with degree.
more research is needed to explore this connection.                     In A. Chereches (Ed.), Proceedings of the 22nd semantics
                                                                        and linguistic theory conference (SALT 22) (pp. 166–187).
                     Acknowledgements                                 Syrett, K., Kennedy, C., & Lidz, J. (2010). Meaning and
We thank Stephanie Solt and Nicole Gotzner for sharing their            context in children’s understanding of gradable adjectives.
data, and Will Frager for help with our experiments. Thanks             Journal of semantics, 27(1), 1–35.
to Noah Goodman and Dan Lassiter for discussion. Michael              Wagenmakers, E.-J., Lodewyckx, T., Kuriyal, H., & Gras-
Franke was supported by NWO-VENI-grant 275-80-004.                      man, R. (2010). Bayesian hypothesis testing for psycholo-
                                                                        gists: A tutorial on the Savage–Dickey method. Cognitive
                          References                                    Psychology, 60, 158–189.
Barner, D., & Snedeker, J. (2008). Compositionality and
   statistics in adjective acquisition: 4-year-olds interpret tall
                                                                  1209

