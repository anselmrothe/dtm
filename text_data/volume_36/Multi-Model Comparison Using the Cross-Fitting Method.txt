UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Multi-Model Comparison Using the Cross-Fitting Method
Permalink
https://escholarship.org/uc/item/7544w9b0
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Schultheis, Holger
Naidu, Praneeth
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                       Multi-Model Comparison Using the Cross-Fitting Method
                                     Holger Schultheis (schulth@informatik.uni-bremen.de)
                     Cognitive Systems, University of Bremen, Enrique-Schmidt-Str. 5, 28359 Bremen, Germany
                                                              Praneeth Naidu
                                 Computer Science and Engineering, IIT Bombay, Mumbai 400076, India
                                Abstract                               compared models captures the actual processes that generated
   When comparing the ability of computational cognitive mod-          the to-be-fitted data, the PBCM has been considered to per-
   els to fit empirical data, the complexity of the compared mod-      form optimally in selecting this model (Shiffrin et al., 2008;
   els needs to be taken into account. A promising method for          Cohen, Sanborn, & Shiffrin, 2008).
   achieving this is the parametric bootstrap cross-fitting method
   (PBCM) proposed by Wagenmakers, Ratcliff, Gomez, and                   Given these properties, employment of the PBCM instead
   Iverson (2004). We contribute to a wider applicability of the       of the naı̈ve use of GOF measures seems highly desirable. At
   PBCM in two ways: First, we compare the performance of the          the same time, two aspects of the PBCM – as so far discussed
   data-informed and the data-uninformed variant of the PBCM.
   Our simulations suggest that only the data-uninformed variant       in the literature – may hamper or even preclude use of the
   successfully controls for model complexity in model selection.      PBCM in certain modeling situations. For one, in the article
   Second, we propose an extension of the PBCM, called MMP-            introducing the PBCM, Wagenmakers et al. (2004) propose
   BCM, that is applicable to, in principle, arbitrarily many com-
   peting models. We evaluate the MMPBCM by applying it to             two different variants of the PBCM called the data-informed
   the comparison of several sets of competing models. The ob-         PBCM (DIPBCM) and the data-uninformed PBCM (DUP-
   tained results suggest that the MMPBCM constitutes a more           BCM). Since these two variants differ considerably in their
   powerful approach to model comparison than the PBCM.
                                                                       computational complexity, it would be important to know to
   Keywords: Model Evaluation, Multi-Model Comparison,
   Parametric Bootstrap Crossfitting Method.                           what extent their performance in model comparison differs.
                                                                       Initial analyses presented in Wagenmakers et al. (2004) sug-
                           Introduction                                gest that the DIPBCM may generally perform worse than the
It is often considered an advantage of computational cog-              DUPBCM, but information that allows more detailedly quan-
nitive models that they allow generating data by simulation            tifying potential differences between the two variants is cur-
and this article concerns this type of data-generating models.         rently not available from the literature considering the PBCM
One way to evaluate and compare such models is to gener-               (Wagenmakers et al., 2004; Cohen, Sanborn, & Shiffrin,
ate data from them and to compare the model-generated data             2008; Cohen, Rotello, & MacMillan, 2008; Jang, Wixted, &
to empirical data pertinent to the phenomenon that is being            Huber, 2011; Perea, Gomez, & Fraga, 2010). Furthermore,
modeled. The degree of correspondence between the model-               both PBCM variants are currently restricted to the compari-
generated and the empirical data is often called the goodness          son of pairs of models. When more than 2 competing models
of fit (GOF) and it may be used to assess the quality of the           need to be compared, this comparison must be broken down
competing models: The higher the GOF, the better the model.            to multiple comparisons of model pairs or the PBCM cannot
   However, such a naı̈ve use of GOF measures for model                be applied at all.
comparison is problematic, because it neglects model com-                 In this article, we provide a first systematic quantitive com-
plexity. Due to overfitting, more complex models may pro-              parison of the DIPBCM and the DUPBCM regarding their
vide high GOF measures solely by virtue of their complex-              model selection performance. We also propose and evalu-
ity. As a result, the naı̈ve use of GOF measures may lead to           ate an extension of the PBCM that allows comparing more
the selection of a more complex model even if a less com-              than two competing models. Both contributions facilitate the
plex model actually provides a better approximation to the             use of the PBCM and, thus, more generally, are conducive
processes that underlie the phenomenon that is being investi-          to increasing the frequency with which more sophisticated
gated (Pitt & Myung, 2002).                                            comparison methods instead of the naı̈ve approach will be
   To address this problem, a number of methods have been              employed for model evaluation and comparison.
proposed that take model complexity into account when com-
paring how well models can account for empirical data (see                                       The PBCM
Shiffrin, Lee, Kim, & Wagenmakers, 2008; Schultheis, Sing-             Let A and B be two competing models and x a set of observed
haniya, & Chaplot, 2013, for overviews). One of these meth-            data (e.g., response times from different experimental condi-
ods is the parametric bootstrap cross-fitting method (PBCM)            tions). Furthermore, let ∆go fABx be the GOF difference of the
proposed by Wagenmakers et al. (2004). Two properties of               two models on the data set x, that is, ∆go fABx = go f x − go f x ,
                                                                                                                               A      B
the PBCM render it particularly appealing for model eval-                           x          x
                                                                       where go fA and go fB are the goodness of fits the models A
uation and selection: First, the PBCM is applicable to any             and B achieve on x, respectively. A naı̈ve approach to model
type of model, since it imposes no constraints on the model-           comparison would select A if ∆go fAB     x ≥ 0 and B otherwise.
ing paradigm or the models’ structure. Second, if one of the           The PBCM aims to improve on the naı̈ve approach by tak-
                                                                   1389

 ing into account how well the models are able to mimic each                                     ∆gofxAB Distributions
 other, that is, the ability of each model to provide good fits to                                                          distA
 data generated by the other model.                                                                                         dist
                                                                                                                                 B
    To achieve this, the PBCM generally proceeds as follows:
1. generate a set of parameter values for all parameters of
    model A,
2. generate a data set xA by running model A with the param-
    eter values from the first step,
                                            xA
3. fit both models to xA to obtain ∆go fAB     ,
4. repeat the above three steps NBS number of times.
 These steps will result in NBS many GOF differences for data                                                x
                                                                                                        ∆gofAB
 that has been generated from model A. If the same four steps
 are repeated with model B as the data-generating model, one
 obtains a second set of NBS many GOF differences. These               Figure 1: GOF difference distributions as they may arise in
 two sets of GOF differences constitute two distributions, distA       using the PBCM. The blue / green curve indicates GOF dif-
 and distB , respectively, that provide information on how well        ferences obtained when model A / B have generated data.
 the two models are able to mimic each other (see Figure 1).
    In particular, the two distributions can inform model com-
 parison and selection. Distribution distA allows to gauge how         generating data are then sampled from the ranges according
 likely it is to obtain the models’ GOF difference on the ob-          to the associated distributions. Uniform distributions were
 served data, ∆go fAB  x , if model A is the generating model.         used for all parameter ranges of all models in the simulations
 Distribution distB allows to gauge how likely it is to obtain         reported below.
 ∆go fABx , if model B is the generating model. The PBCM se-
 lects the model that is associated with the distribution under                           Multi-Model PBCM
 which ∆go fAB  x is more likely: If ∆go f x is more likely under
                                          AB                           Wagenmakers et al. (2004) have introduced the PBCM as
 distA , model A is selected; otherwise, model B is selected.
                                                                       a method for comparing pairs of models. While comput-
    A crucial question related to the PBCM as described so far
                                                                       ing GOF differences works well when considering pairs of
 is how to sample the parameter values for the data-generating
                                                                       models, a comparable measure for 3 or more models does
 model in step 1 above. Wagenmakers et al. (2004) propose
                                                                       not carry much meaningful information about the relation be-
 two different ways of generating parameter values. First, pa-
                                                                       tween the models. Accordingly, it is not immediately clear
 rameter values can be determined based on the data that is to
                                                                       whether and, if yes, how the PBCM may be extendable to the
 be modeled, x. This approach gives rise to the first variant of
                                                                       direct comparison of more than two models.
 the PBCM, the DIPBCM. Second, parameter values may be
                                                                          In the following we propose an extension of the PBCM that
 generated independently of x, which yields the variant called
                                                                       allows to compare, in principle, arbitrarily many competing
 DUPBCM. Both variants are described in more detail in the
                                                                       models. The key to our extension lies in taking a different
 subsequent sections.
                                                                       view on the classification problem involved in the PBCM. In
 DIPBCM                                                                its original formulation, use of the PBCM requires solving a
 In this variant of the PBCM, parameter values are generated           classification problem with two classes (data generated from
 by fitting the data-generating model to bootstrap samples of          model A vs. data-generated from model B) that contain 1-
                                                                       dimensional instances (the GOF differences): ∆go fAB         x needs
 the observed data x. A bootstrap sample is a new data set x∗
 that is obtained by sampling with replacement n values from           to be assigned to either of the two classes. An alternative
 x, where n is the number of observations in x (see Efron &            but closely related classification problem is to treat the pairs
 Tibshirani, 1993, for more details on bootstrapping). Fitting         of GOF values from which the differences are computed in
 a model, say A, to a bootstrap sample x∗ yields a set of param-       the original PBCM as instances of the two classes. Thus,
                                                                       instead of classifying a single value (∆go fAB    x ) on the basis
 eter values that provides the best fit to x∗ . These parameter
 values are then used to generate data from A in step 2 of the         of two 1-dimensional distributions, the alternative problem
 PBCM procedure (see above). Drawing NBS many bootstrap                consists of classifying a value pair, (go fAx , go fBx ), on the basis
 samples from x thus allows to generate NBS many sets of pa-           of two 2-dimensional distributions (see Figure 2). This type
 rameter values for each of the two models.                            of classification problem is easily extendable to more than
                                                                       two models: For each additional model a new class is added
 DUPBCM                                                                and the dimensionality of the instances increases by one.
 To generate parameters in the DUPBCM, one first fixes a                  More generally, for a set of k models, M1 , . . . , Mk (k ≥ 2)
 range of possible values for each model parameter and a prob-         our PBCM extension, the multi-model PBCM (MMPBCM)
 ability distribution across each of these ranges. Values for          proceeds as follows:
                                                                   1390

                           Distribution of (gof xA , gof xB )                  straightforward in use, but requires more decisions on set-
                                                                distA          tings to be made than the k-NN. The ANN is more complex
                                                                distB          in its use than any of the other two classifiers. It requires
                                                                               decisions on a number of aspects that may be crucial for per-
                                                                               formance but may be hard to make for non-experts (e.g., re-
                                                                               garding network structure, learning algorithm, and stopping
          B
        gofx                                                                   criterion). One crucial question to be answered by the sim-
                                                                               ulations reported below is to what extent the use of a more
                                                                               complex classifier pays off in the form of better model com-
                                                                               parison performance when employing the MMPBCM.
                                                                                  Substantial detail on the workings and properties of all
                                                                               three employed classifiers can be found in Duda et al. (2001).
                                             x
                                         gofA                                  Below we list the settings and procedural detail associated
                                                                               with each of the classifiers in our simulations.
Figure 2: Two-dimensional GOF distributions for comparing                      k-NN This classifier only requires setting k, the number of
two models with the MMPBCM. Blue ’+’ / red cross indicate                      nearest neighbors to be considered in classification. Our sim-
GOF pairs when model A / B generated data.                                     ulations used k = 10, because this value yielded near optimal
                                                                               classification accuracies across different PBCM situations in
1. generate a set of parameter values for all parameters of                    a previous study (Schultheis & Singhaniya, 2013).
   model M1 ,                                                                  ANN The ANN used one hidden layer with 10 units. For
2. generate a data set xM1 by running model M1 with the pa-                    training, the available data was partitioned into training data
   rameter values from the first step,                                         (75%), validation data (15%), and test data (10%). The ini-
3. fit all models to xM1 to obtain the GOF vector                              tial weights were randomly chosen from [−0.5, 0.5] and then
         xM                xM
   (go fM1 1 , . . . , go fMk 1 ),                                             trained using backpropagation with a learning rate of 0.1.
4. repeat the above three steps NBS number of times,                           This learning rate was chosen to optimize performance on (a)
5. repeat the above 4 steps for models M2 , . . . , Mk ,                       standard classification problems and (b) data that was simi-
                                                                               lar to data arising during our simulations. The network was
6. determine the class of (go fMx 1 , . . . , go fMx k ) based on the re-
                                                                               trained in batch mode for 300 epochs and the weights that
   sulting k-dimensional distributions and select the model
                                                                               gave minimum validation error across these 300 epochs were
   that is the data-generating model of the class.
                                                                               employed for classification.
   The ability of the MMPBCM to directly compare arbitrar-
                                                                               DT The number of divisions in each dimension was set to
ily many models comes at the cost of a potentially increased
                                                                               100 and the information gain criterion was used for split-
complexity of the involved classification problem. Multidi-
                                                                               ting at each node: A given node was split further only when
mensional multi-class classification can be a hard problem
                                                                               this yielded an information gain greater or equal to 0.01.
that is often considered to require sophisticated classification
                                                                               This threshold yielded better performance than a value of 0.1
methods (Duda, Hart, & Stork, 2001). In particular, many
                                                                               and as good results as a values of 0.001. Since a value of
of the comparatively simple methods to classify GOF differ-
                                                                               0.001 would have been more prone to over fitting, we chose
ences in the PBCM (see Schultheis & Singhaniya, 2013, for
                                                                               a threshold of 0.01
an overview) are not feasible for use in the MMPBCM. This
creates a tension w.r.t. the aim to foster use of the PBCM for                 Using these parameterizations each of the classifiers
model evaluation, because not all cognitive modelers can be                    performed very well on five benchmark classification
expected to be experts in classification.                                      problems from the UCI Machine Learning Repository
   To address this potential issue of the usability of the MMP-                (http://archive.ics.uci.edu/ml/).
BCM, we tested its performance with three different classi-                       As with the original PBCM, the MMPBCM can be real-
fiers: k-Nearest Neighbor (k-NN), Decision Tree (DT), and                      ized as a data-informed (DIMMPBCM) or a data-uniformed
Artificial Neural Network (ANN). The k-NN was chosen, be-                      (DUMMPBCM) variant.
cause it is one of the simplest – if not the simplest – existing
multi-class, multi-dimension classifier that has often been re-                                        Approach
ported to yield good results. Due to its simplicity it is easy                 The preceding considerations give rise to 8 different variants
to implement and use even for the non-expert. The DT and                       of the PBCM: Two variants are the DIPBCM and DUPBCM
the ANN were chosen because they are two well-known and                        as originally proposed by Wagenmakers et al. (2004). Both
well-established classifiers. In particular, both of them are                  of these employed a k-NN classifier with k = 10 for solv-
available from standard classification libraries (e.g., WEKA,                  ing the classification problem, because a recent study found
Hall et al., 2009) and thus do not require implementation to                   that the k-NN yields near optimal results in many situations
be used for the MMPBCM. The DT is also comparatively                           (Schultheis & Singhaniya, 2013). The remaining six are the
                                                                            1391

DIMMPBCM and the DUMMPBCM each combined with all                     Tightness of fit Model fits may often be suboptimal to a
three classifiers.                                                   greater or lesser extent. In view of this, we considered three
   To assess the performance of the different variants of the        levels of tightness of fits (loose, medium, and tight fit) by
PBCM, we conducted model recovery studies with three hy-             varying how thoroughly the Metropolis algorithm searches
pothetical models of memory decay. Each of these models,             the models’ parameter space. More precisely, we varied
M1, M2, and M3, predicts the probability of recall in depen-         the number of sets of parameters that were sampled (called
dance on the time that has passed since the to-be-remembered         swaps) for model fitting. Three different swaps values, 100,
items have been learned. The models are defined as follows           1000, and 10000, were used.
(see Pitt & Myung, 2002):                                            Strength of noise Since the only noise in the data is sam-
                                                                     pling noise, the amount of noise in the data is determined
               M1 : (1 + t)−a , a ∈ [0, 2]                           exclusively by the number of learned items (NL). The higher
               M2 : (b + t)−a , a ∈ [0, 2] , b ∈ [1, 2]              NL, the lower is the influence of sampling noise. We em-
                                                                     ployed NL = 5, 50, and 1000 in our simulations.
               M3 : (1 + bt)−a , a ∈ [0, 2] , b ∈ [0, 2]
                                                                     Number of data points The amount of data available to
                                                                     evaluate and compare competing models may vary consider-
All 8 PBCM variants were applied to all 3 possible pairs of
                                                                     ably depending on the modeling situation. Accordingly, we
models, M1 vs. M2, M1 vs. M3, and M2 vs. M3. Further-
                                                                     also varied the number of data points (NDP), using NDP =
more, the six MMPBCM variants were applied to the full set
                                                                     5, 20, and 100.
of all 3 models and compared to the performance of an exist-
ing multi-model comparison method: The bootstrap method              Number of bootstrap samples The number of bootstrap
for model comparison (Efron & Tibshirani, 1997). Despite             samples (called NBS) determines the amount of information
the similarities in names the bootstrap method of Efron and          about the compared models that is generated in the scope of
Tibshirani (1997) works quite differently than the PBCM. To          applying the methods. Two different NBS values, 100 and
gauge to what extent any of the methods outperforms the              1000, were used.
naı̈ve approach (i.e., selecting the model with the best fit),       The combination of all factor variations yields 54 different
we also applied this approach, called simple recovery (SR),          modeling situations. Assessing the performance of the 8
to all sets of competing models.                                     PBCM variants and the bootstrap method across modeling
   Given a set of competing models (i.e., either one of the          situations and different sets of competing models allowed ob-
model pairs or the complete set of the three models), the com-       taining information on a number of key issues related to the
parison methods were applied using the following procedure.          employment of the (MM)PBCM. First, it allowed quantifying
For one of the competing models, first, a set of parameter val-      the performance differences between the data-informed and
ues was determined by sampling uniformly from the param-             the data-uniformed variants of the (MM)PBCM. Second, it
eter ranges. Second, probabilities for this set of parameter         allowed determining to what extent the type and complexity
values were generated from the model. Third, these probabil-         of the employed classifiers has a substantial (if any) impact
ities were used to randomly sample the number of successful          on the performance of the PBCM. Third, it allowed evalu-
recalls from a binomial distribution assuming a certain num-         ating the MMPBCM by both comparing its performance on
ber of learned items. Fourth, this set of numbers of successful      model pairs to the original PBCM and comparing its perfor-
recalls was treated as if it was a set of empirical observations     mance on all 3 models to an established method for model
for which to identify the most appropriate model. Accord-            comparison (i.e., the bootstrap).
ingly, the comparison method in question was applied to the
set of competing models and the observations. Fifth, which of                                   Results
the compared models was found to be more appropriate was             The main performance measure we used was the percentage
noted. This procedure was repeated R = 100 times for each            of correct recovery averaged across all models in the com-
model in the set of competing models. Across all sets of com-        petition set. For sets with two / three models, a value of
peting models and methods the measure to assess model fits           50% / 33% signifies chance performance and for all model
was always the mean squared error and the models were fit            sets, a value of 100% signifies optimal performance.
using a variant of the Metropolis algorithm (Madras, 2002).             This performance characteristic was computed for each of
   Our simulations varied 4 factors that potentially impact the      the considered modeling situations thus obtaining 54 perfor-
performance of the comparison methods. These factor vari-            mance measurements for each method-set of models combi-
ations ensured a more comprehensive view on the methods’             nation. To convey an impression of the central tendency and
performance, that is, a view that is not specific to only one        variability of each method’s performance, we computed the
particular combination of factor levels. The considered fac-         first, second, and third quartile of the performance character-
tors are tightness of fit, strength of noise, number of data         istic across modeling situations. These quartiles and associ-
points and the number of bootstrap samples and are described         ated standard errors are displayed in Figures 3 and 4. The fig-
in the following.                                                    ures highlight marked performance differences between the
                                                                 1392

                                2 Models DU                                                         2 Models DI
                   95                                                                    95
                   90                                                                    90
                   85                                                                    85
                   80                                                                    80
                   75                                                                    75
     Performance                                                           Performance
                   70                                                                    70
                   65                                                                    65
                   60                                                                    60
                   55                                   Original                         55                              Original
                                                        ANN                                                              ANN
                   50                                   DT                               50                              DT
                                                        k−NN                                                             k−NN
                   45                                   SR                               45                              SR
                   40                                                                    40
                        1st        2nd               3rd                                      1st     2nd              3rd
                                  Quartile                                                           Quartile
Figure 3: Quartiles with standard errors of model recovery performance for model pairs across modeling situations. Left / Right
panel shows DU / DI variants of (MM)PBCM. ANN: Artificial Neural Network classifier; DT: Decision Tree classifier; k-NN: k Nearest
Neighbor classifier; SR: Simple Recovery
methods and in the following we discuss these differences              and stopping criteria. It is currently not clear why the ANN
with respect to the three key issues mentioned above.                  classifier should perform well on standard benchmark prob-
                                                                       lems, but should fail in the scope of the MMPBCM.
Data-Informed vs. Data-Uninformed                                         In sum, our simulations found no evidence that more com-
The data-informed variants of the (MM)PBCM perform                     plex classifiers yield better model comparison results: The
worse than the corresponding data-uninformed variants. For             simplest employed classifier, the k-NN, performed best. Even
model pairs (Figure 3), the performance measure of the DU              if the poor performance of the ANN were due to inapt appli-
variants is consistently higher than of the DI variants across         cation, this would shed doubt on the usability of the ANN for
all quartiles. This indicates a general advantage of the DU            cognitive modelers that are not experts in classification.
variants that is not specific to a small subset of the considered
modeling situations. The same holds for the set of 3 compet-           MMPBCM Performance
ing models (Figure 4): The DIMMPBCM performs generally                 At the heart of the ability of the MMPBCM to compare more
– and sometimes considerably – worse than the DUMMP-                   than 2 models is the transformation of the classification prob-
BCM across a large proportion of modeling situations.                  lem in the original PBCM into a closely related but different
   While one may have expected that the DI variants per-               classification problem. Therefore, assessment of the MMP-
form worse than the DU variants given the analyses of                  BCM should test to what extent the transformation of the clas-
Wagenmakers et al. (2004), the extent of this difference may           sification problem impacts model comparison performance.
be more pronounced than anticipated. In fact, the DI variants          The quartile plots in Figure 3 suggest that there is no reliable
rarely perform better and frequently worse than simple re-             difference between the original PBCM and the best perform-
covery. In contrast, the DU variants consistently outperform           ing MMPBCM for both DI and DU variants. There seems
the naı̈ve approach. Considering that the computational com-           to be a tendency towards slightly better performance by the
plexity of the DI variants is higher than the complexity of the        original PBCM, but given the standard errors associated with
DU variants, our results strongly suggest not to employ the            the quartile estimates the PBCM variants do not clearly out-
DI variants for model comparison and selection.                        perform the MMPBCM variants.
                                                                          Evaluation of the MMPBCM regarding comparisons of
Classifier Performance                                                 more than 2 models is less straightforward, because there
For each set of competing models, the k-NN classifier per-             is no original available in this case. What then would be
forms best, the DT classifier second best, and the ANN                 a reasonable comparison? If one employs another, arbitrar-
classifier worst. Particularly noteworthy is the poor perfor-          ily chosen, model comparison method M , the results may be
mance of the ANN classifier, which rarely performs reli-               hard to interpret. If, for example, M outperforms the MMP-
ably above chance and never better than the simple recovery            BCM, does that imply (a) that the multi-model extension of
method. This is so much more surprising since each classi-             the PBCM is somehow flawed or (b) that the PBCM as such is
fier performed very well on standard classification problems.          inferior to M ? To avoid such ambiguities, we chose the boot-
Furthermore, the poor model recovery performance of the                strap method proposed by Efron and Tibshirani (1997) for
ANN persisted across different learning parameter settings             comparison with the MMPBCM, because our previous inves-
                                                                    1393

                                   3 Models                             esting to examine to what extent the poor performance of the
               90
                    DI ANN                                              data-informed variants can be explained by this difference.
                    DI DT
               80   DI k−NN
                    DU ANN
                                                                                          Acknowledgments
                    DU DT                                               This work was done in the project R1-[ImageSpace] of the
               70   DU k−NN
                    Bootstrap                                           SFB/TR 8 Spatial Cognition. Funding by the German Re-
 Performance
               60   SR                                                  search Foundation (DFG) is gratefully acknowledged. We
                                                                        thank the reviewers for insightful and helpful suggestions.
               50
                                                                                               References
               40
                                                                        Cohen, A. L., Rotello, C. M., & MacMillan, N. A. (2008).
                                                                          Evaluating models of remember–know judgments: Com-
               30
                                                                          plexity, mimicry, and discriminability. Psychonomic Bul-
               20
                                                                          letin & Review, 15, 906-926.
                    1st             2nd           3rd                   Cohen, A. L., Sanborn, A. N., & Shiffrin, R. M. (2008).
                                   Quartile
                                                                          Model evaluation using grouped or individual data. Psy-
                                                                          chonomic Bulletin & Review, 15, 692-712.
Figure 4: Quartiles with standard errors of model recov-                Duda, R. O., Hart, P. E., & Stork, D. G. (2001). Pattern
ery performance for three models across modeling situations.              classification. New York: Wiley.
ANN: Artificial Neural Network classifier; DT: Decision Tree clas-      Efron, B., & Tibshirani, R. J. (1993). An introduction to the
sifier; k-NN: k Nearest Neighbor classifier; SR: Simple Recovery          bootstrap. New York: Chapman & Hall.
                                                                        Efron, B., & Tibshirani, R. J. (1997). Improvements on cross-
                                                                          validation: The .632+ bootstrap method. J Am Stat Assoc,
tigations indicated that this bootstrap method and the PBCM
                                                                          92, 548-560.
perform very similarly (Schultheis et al., 2013).
                                                                        Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann,
   In line with these previous observations and as shown in
                                                                          P., & Witten, I. H. (2009). The weka data mining software:
Figure 4, the DUMMPBCM and the bootstrap perform virtu-
                                                                          An update. SIGKDD Explorations, 11(1).
ally identical for the set of three competing models. Accord-
                                                                        Jang, Y., Wixted, J. T., & Huber, D. E. (2011). The diagnos-
ingly, assessment of the MMPBCM on model pairs as well as
                                                                          ticity of individual data for model selection: Comparing
on the 3-model set support the validity of the MMPBCM.
                                                                          signal-detection models of recognition memory. Psycho-
                                                                          nomic Bulletin & Review, 18, 751-757.
                                Conclusion
                                                                        Madras, N. (2002). Lectures on monte carlo methods. Provi-
The MMPBCM proposed in this contribution appears to con-                  dence, Rhode Island: American Mathematical Society.
stitute a more powerful model comparison method than the                Perea, M., Gomez, P., & Fraga, I. (2010). Masked nonword
original PBCM. In addition to performing similarly to the                 repetition effects in yes/no and go/no-go lexical decision:
PBCM on 2-model comparisons it also performs well on                      A test of the evidence accumulation and deadline accounts.
comparisons outside the reach of the PBCM: Direct com-                    Psychon B & Rev, 17, 369-374.
parisons of more than 2 models. Our simulations further-                Pitt, M., & Myung, J. (2002). When a good fit can be bad.
more indicated (a) that the MMPBCM yielded best results                   Trends in Cognitive Sciences, 6, 421-425.
when employing the simplest classifier (k-NN) and (b) that              Schultheis, H., & Singhaniya, A. (2013). Decision crite-
the computationally more complex data-informed variant of                 ria for model comparison using cross-fitting. In 22nd An-
the (MM)PBCM did perform considerably worse – not even                    nual Conference on Behavior Representation in Modeling
better than the naı̈ve approach – than the less complex                   & Simulation (BRiMS 2013).
data-uninformed variant. Accordingly, the data-uninformed               Schultheis, H., Singhaniya, A., & Chaplot, D. (2013).
MMPBCM provides a generally applicable model compari-                     Comparing model comparison methods. In M. Knauff,
son method, which takes model complexity into account, has                M. Pauen, N. Sebanz, & I. Wachsmuth (Eds.), Proceed-
comparatively low computational cost, and is easy to use.                 ings of the 35th annual conference of the Cognitive Science
   Our future work aims to provide a broader and more solid               Society. Austin, TX: Cognitive Science Society.
basis for these conclusions by investigating the performance            Shiffrin, R. M., Lee, M. D., Kim, W., & Wagenmakers, E.-J.
of the (MM)PBCM on additional sets of competing models.                   (2008). A survey of model evaluation approaches with a tu-
A further interesting issue concerns a possible relation be-              torial on hierarchical bayesian methods. Cognitive Science,
tween parameter generation and (MM)PBCM performance.                      32, 1248-1284.
While the data-uninformed variants use the same distribution            Wagenmakers, E.-J., Ratcliff, R., Gomez, P., & Iverson, G.
(uniform) for sampling parameters as employed for generat-                (2004). Assessing model mimicry using the parametric
ing data from the ”true” model, the data-informed variants                bootstrap. Journal of Mathematical Psychology, 48, 28-
use a non-uniform distribution for sampling. It seems inter-              50.
                                                                     1394

