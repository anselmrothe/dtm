UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Modeling Perspective-Taking by Correlating Visual and Proprioceptive Dynamics
Permalink
https://escholarship.org/uc/item/6nw2b977
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Schrodt, Fabian
Layher, Georg
Neumann, Heiko
et al.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

 Modeling Perspective-Taking by Correlating Visual and Proprioceptive Dynamics
                                  Fabian Schrodt1 (tobias-fabian.schrodt@uni-tuebingen.de)
                                             Georg Layher2 (georg.layher@uni-ulm.de)
                                          Heiko Neumann2 (heiko.neumann@uni-ulm.de)
                                         Martin V. Butz1 (martin.butz@uni-tuebingen.de)
                                    1 Department of Computer Science, Chair of Cognitive Modeling
                         University of Tübingen, Sand 14, Tuebingen, Baden-Württemberg, 72076 Germany
                    2 Faculty of Engineering and Computer Sciences, Institute for Neural Information Processing
                           Ulm University, James-Franck-Ring, Ulm, Baden-Württemberg, 89081 Germany
                              Abstract                                     seems likely that parietal visuo-proprioceptive correlations
   How do we manage to step into another person’s shoes and                are learned in early mental development. We investigate how
   eventually derive the intention behind observed behavior? We            such self-induced correlations can be learned and how this
   propose a connectionist neural network (NN) model that learns           knowledge can be used to adapt the visual processing of ob-
   self-supervised a prerequisite of this social capability: it adapts
   its internal perspective in accordance to observed biological           served actions. As Johansson pointed out, showing a moving
   motion. The model first learns predictive correlations between          set of light points representing the locations of a walking per-
   proprioceptive motion and a corresponding visual motion per-            son’s joints is sufficient to perceive the underlying biological
   spective. When a novel view of a biological motion is pre-
   sented, the model is able to transform this view to the closest         motion (1973). Thus, it seems that this ability is at least par-
   perspective that was seen during training. In effect, the model         tially based on the perception of the relative motion of bodily
   realizes a translation-, scale-, and rotation-invariant recogni-        feature locations. Hence, we use the motion of relative fea-
   tion of biological motion. The NN is an extended adaptive
   resonance model that incorporates self-supervised error back-           ture locations as the visual input to our model. Seeing that
   propagation and parameter bootstrapping by neural noise. It             the noise-tolerance of subjects identifying biological motion
   segments and correlates relative, visual and proprioceptive ve-         from point-light walkers decreases dramatically if the presen-
   locity kinematics, gradually refining the emerging representa-
   tions from scratch. As a result, it is able to adjust its internal      tation is inverted top-down (Pavlova & Sokolov, 2003), it ap-
   perspective to novel views of trained biological motion pat-            pears that canonical views of motions affect biological mo-
   terns. Thus, we show that it is possible to take the perspective        tion recognition similar to how canonical views of objects in-
   of another person by correlating proprioceptive motion with
   relative, visual motion, and then allowing the adjustment of            fluence object recognition (Shepard & Metzler, 1971). Thus,
   the visual frame of reference to other views of similar motion          we propose an NN model that learns canonical views of bio-
   patterns.                                                               logical motion, later adjusting the internal frame of reference
   Keywords: Proprioception; vision; associative learning; self-           to deduce another person’s perspective while monitoring her
   supervised learning; mental rotation; canonical views; point-
   light display; biological motion; recurrent neural networks.            or his biological motion patterns.
                                                                              The neural network we propose segments a continuous sen-
                          Introduction                                     sory stream in a common visual and proprioceptive space at
Do we build up and use our own sensorimotor knowledge to                   nonlinearities in the coincident motion dynamics. Persistent
adapt our mental perspective for putting ourselves into an-                spatiotemporal congruencies in this domain are memorized
other person’s shoes? We present an artificial neural network              by Hebbian-inspired learning rules. The model is scale and
(NN) model that is able to do just that: it enables a view-point           translation invariant, because it processes directions of rela-
independent perception of biological motion, correlating rel-              tive velocities in the considered sensory spaces. Assuming
ative visual motion to corresponding proprioceptive motion,                that joint angles can be considered a part of the propriocep-
and later mentally transforming novel visual motion perspec-               tion that can also be perceived visually, a view-point invari-
tives to previously learned, canonical perspectives. In several            ant perception of observed biological motion is enabled by
fields of research, the ability to mentally transform the own              rotating visual feature locations in accordance to propriocep-
coordinate system to match an observed one is referred to as               tive experience. The model realizes this by top-down error
self-projection, view-point adaptation, or perspective-taking.             projections that minimize the divergence between perceived
To enable such a mental transformation, we focus here on the               positional motion and the previously associated angular mo-
perception of biological motion.                                           tion. We show that our model is able to progressively adapt its
   The perception of biological motion is assumed to be re-                internal perspective on visual data in a self-supervised man-
lated to the Superior Temporal Sulcus (STS) (Pavlova, 2012;                ner, effectively slipping into another person’s shoes. In sum,
Pyles et al., 2007), where temporal and parietal pathways                  we present a model that is able to derive another person’s per-
of visual information processing converge. However, since                  spective by exploiting sensorimotor knowledge about the own
the parietal path is also strongly involved in the percep-                 body kinematics. Since it seems necessary to know another
tion of body-relative spaces (Holmes & Spence, 2004), it
                                                                       1383

person’s perspective to some degree for deriving her or his           and proprioceptive information. That is, it visually receives
current intentions, the cognitive capabilities we investigate         relative locations of joints, and absolute joint angles by the
may set the stage for the development of the mirror neuron            proprioceptive system. Subsequently, stage I convolves these
system. Thus, the model offers one highly embodied, de-               data separately into directional velocities. In this process,
velopmental path for bootstrapping the capability to imitate          mental rotation is applied to the visual information. Note that
another person, to derive her or his intentions, and even to be       the angular information is rotation invariant and may thus also
empathetic.                                                           be derived from vision without transformation.
   A related NN modeling STS was developed previously                    Stage II performs a modulatory normalization of infor-
(Layher et al., 2014) to fuse visual information from mo-             mation and then pools the information from the visual and
tion and form pathways. This model did not include pro-               proprioceptive streams of information. Stage III implements
prioceptive dynamics and was not able to adjust the inter-            a self-supervised adaptive resonance model. It uses instar-
nal perspective. Another recent NN architecture modeled               learning to segment the sensory stream given by stage II and
view-independence during object interactions (Fleischer et            to memorize permanent correlations, and outstar-learning to
al., 2012), but it did neither learn correlations nor canonical       recall or predict the learned correlations. Compared to the
views, and it did not investigate the adjustment of the internal      classical unsupervised approach, this allows to derive a pre-
perspective, either.                                                  diction error, which is backpropagated through the network.
   The remainder of the paper is structured as follows: First,
we introduce our point-light simulation environment to de-
scribe example inputs to the model. Then, we introduce the
neural architecture for matching dynamics, building canoni-
cal views, and progressive mental transformation. In this re-
spect, we also explain how to flexibly bootstrap the network’s
weights on the basis of noise without prior knowledge about
the input data. Next, we evaluate the model in three experi-
mental setups. Finally, we summarize the results, sketch-out
implications, and point to future research options.
                   Simulation Environment
To exemplify a setup of our model, we implemented a sim-
ple 2D, 2DOF arm-simulation (see Fig. 1). The arm exe-
cutes a continuous forward and backward swing, somewhat
similar to the arm of a walking person. The simulation pro-
vides the relative locations of all joints in retinal coordinates
(d1 , d2 , d3 ), as well as the (proprioceptive) shoulder and el-
bow joint angles (α, β). Visual information can be trans-
formed by an experimenter (that is, rotated, mirrored, etc.)
before serving as input for the neural network model. The             Figure 2: Overview of the three-stage neural modeling ap-
corresponding visual rotation angle of the entire arm is de-          proach. Boxes numbered with n indicate layers consisting
noted ν in the following.                                             of n neurons. Black arrows describe weighted forward con-
                                                                      nections between layers, while bullet-heads indicate modu-
                                                                      lations. Dashed lines denote delay-connections. Red arrows
                                                                      denote the backward propagation of prediction errors.
                                                                      Stage I - Feature Processing
                                                                      The visual input path of the network is driven by relative (2D)
                                                                      coordinates of simulated arm joints. Analogously, the propri-
                                                                      oceptive path of the model is driven by the (1D) angles be-
                                                                      tween limbs. We momentarily assume that the coordinates
Figure 1: Point-light arm simulation. The big dot represents          of hand, elbow and wrist and their according angles can be
the shoulder location, followed by elbow and wrist.                   identified and assigned to the respective input neurons reli-
                                                                      ably. We chose the angular information as the only propri-
                                                                      oceptive input, and assume that this information can also be
                    Neural Network Model                              derived from vision upon action observation. Fig. 3 shows
The model consists of three successive stages illustrated in          the feature processing for a single, two-dimensional visual
the overview given in Fig. 2. The first stage processes visual        limb relation in the visual path. The input is, for example,
                                                                  1384

                                                                      real-time normalization of a layer’s output-vector to the Eu-
                                                                      clidean length 1. In our model, a common neuron indexed
                                                                      by j can formally be described by its input net j , its activation
                                                                      function f j (net j ), its output o j , some noise-term ξ j (which
                                                                      we will address later) and the axonic modulatory factor a j :
                                                                                          o j = a j · f j (net j )                             (1)
                                                                                      net j = ξ j + ∑ wi j · oi .                              (2)
                                                                                                          i
                                                                      Normalization of the neural activity in a layer a is realized by
     Figure 3: Processing path of a relative joint location           modulating all neurons j of that layer by the output oa of a
                                                                      single, layer-specific normalizing neuron (a j := oa )1 , with
                                                            T
the hand-location minus the elbow-location d1 = x y in                                           oa (t−1)
retinal coordinates.                                                                oa (t) =                   ,                               (3)
   In interstage Ia, this information is transformed into a di-                                 ∑ j o j (t)2
rectional velocity by time-delayed subtraction. In this way,
                                                                      where o j denotes a delayed moving average of the output of
the model becomes translation-invariant. In interstage Ib, the
                                                                      neuron j:
information of directional velocity is transformed by means
of a presynaptic, gain field-like modulation (Andersen et al.,
                                                                                     o j (t) = (1 − λ) · o j (t−1) + λ · o j (t−1)             (4)
1985), simulating a mental transformation. It is realized by
a two dimensional matrix multiplication of the directional            with decay parameter λ ∈ (0, 1].
                    T
velocity ∆x ∆y into a transformed directional velocity                   After normalization, all direction-sensitive fields are
            T
  ∆x0 ∆y0 :                                                           pooled by one-to-one connections into a single, bigger pool-
                              0                                 ing layer, which serves as input to stage III. The connections
                     a b       ∆x        ∆x                                                    √
                                    =          .                      are weighted by 1/ n, where n denotes the number of sensory
                     c d       ∆y        ∆y0                          information sources being processed (5 in our example). In
When the rotation applies, the neurons a, b, c, d implement           this way, also the pooling layer input is normalized, which is
the elements of the rotation matrix, which is driven by an ad-        important for the applied learning rules.
justable rotation angle µ realized by a bias neuron. While the        Stage III - Correlation Learning
same mental rotation angle is applied to all visual informa-
                                                                      Stage III realizes a segmentation of the normalized and
tion, no mental rotation is applied to the single-dimensional
                                                                      pooled information from stage II (neurons indexed by i) by
angular pathway. Interstage Ic implements directional con-
                                                                      means of a number of pattern responsive neurons (indexed j,
volution over time, converting the directional velocities into
                                                                      quasilinear in the range [0, 1]). Each pattern neuron becomes
direction-responsive activities. The weighing matrix is set up
                                                                      the representative for a unique constellation of positional and
in a combinatorial fashion, as every single dimension of the
                                                                      angular directions of variability. For segmentation, we use
feature input may increase, not change, or decrease. W is fur-
                                                                      instar learning (Grossberg, 1976a):
thermore normalized per direction. This procedure can easily
be performed for features of every dimensionality D, result-             1/η · ∂wi j (t)/∂t    = ∆wi j (t) = o j (t) · (neti (t) − wi j (t)) , (5)
ing in 3D − 1 direction sensitive neurons.
   In all, stage I provides a population of neurons for each          with learning rate η. The rule implies that the weight vector
sensory feature processed, which is either sensitive to direc-        w j to each single pattern neuron j approaches the input vector
tional velocities in the visual position of a feature (8 neurons)     of the preceding layer at a rate determined by the pattern’s
or sensitive to directional velocities of a proprioceptive angle      activity. To avoid “catastrophic forgetting” of patterns, we
(2 neurons).                                                          use winner-takes-all (WTA) competitive learning (Rumelhart
                                                                      & Zipser, 1985) in the sense that only the weights to the most
Stage II - Normalization and Pooling
                                                                      active neuron in the pattern layer are adapted.
Stage II firstly accounts for a separate normalization of activ-         Grossberg’s “sparse patterns theorem” (Grossberg, 1976b)
ity in the direction-sensitive populations, which is indicated        states that learned patterns can in general only be guaranteed
by feedback connections in Fig. 2. In this way, absolute ve-          to be stable if the initial weight vectors underlie a certain dis-
locities are ignored and only the directions of changes in vi-        tribution, which depends on the actual subspace of input vec-
sual/proprioceptive information are taken into consideration,         tors. Since the input space is initially typically unknown, we
by which the model becomes scale-invariant. Normaliza-                bootstrap the weight vectors from scratch (wi j (t0 ) = 0) by a
tion of a layer’s activity-vector can be accomplished by axo-
axonic modulation. The method we propose approximates a                   1 No square root is necessary for the normalization to length 1.
                                                                  1385

neural noise mechanism. Initially, no sensory information is                    model, the perspective adaptation is thus driven by the vi-
propagated to the pattern layer (o j (t0 ) = 0). Instead, we add                sual kinematics expected for the proprioceptive dynamics and
normally distributed noise ξ j = N (0, σ) to the input net j of                 vice versa. The prediction error δ j , which is backpropagated
each neuron in the pattern layer (see Eq. 2), such that pat-                    over the outstar weights to a pattern neuron, can be described
tern neurons are driven by the sum of signal and noise. Thus,                   by the weighted sum of negative prediction deviations in the
some random pattern neuron is initially the winner, and its                     pooling layer2 :
weight vector adapts from 0 to a novel pattern.
   To account for the issue that the weight vectors are not be-                               δ j (t) = ∑ w ji (t) · (w ji (t) − neti (t)) ,      (9)
                                                                                                         i
ing normalized to a length of 1 in this way – which is an
important property of instar-learning – we assume that the
                                                                                where j is again the winner neuron of time step t − 1. This is
excitability of a pattern neuron decreases proportional to its
                                                                                equivalent to
overall synaptic strength:
                  f j (net j ) = net j · min(||w j ||−1 , r) ,           (6)                  δ j (t) = 1 − ∑ neti (t) · w ji (t) ,             (10)
                                                                                                             i
where r denotes the upper excitability boundary. By that, the
weight vector to a pattern neuron is normalized if its length                   under the assumption that the outstar weights have completed
exceeds r. In contrast to other approaches, this procedure                      training (hence have length 1). Thereby, the error of a pat-
does not initialize the pattern neurons’ instar weights with a                  tern neuron is determined by the angle between the predicted
random direction but changes their response randomly, and                       and the actual pooled information. Assuming furthermore
with it, their probability to win. During the development of                    that the predictions have been learned correctly, the error-
a pattern, the winning probability is magnified by the angle                    driven adaptation of the network’s parameters maximizes the
between the presented pattern and the weight vector of a pat-                   response of patterns that represent the momentary constella-
tern neuron, while the relative influence of neural noise de-                   tion of positional and angular dynamics best.
creases. Thus, both the amount of neural noise – determined
by σ – and the initial responsiveness r play a major role for                                              Experiments
the distribution of the network’s pattern capacity: While σ in-                 In the following, we evaluate our NN model on psycholog-
fluences the probability that a developed pattern is retrained,                 ical findings. The network architecture was parametrized
σ · r determines the probability that an undeveloped pattern                    with σ = 0.002, r = 100, a learning rate of η = 0.04 for in-
wins over a developed one and is thus consulted to increase                     star/outstar and mental rotation learning, and with 64 neurons
the spatial resolution of this episode in dynamics.                             in the pattern recognition layer. We took the average of 100
   We furthermore use predictive outstar learning as an atten-                  independent runs for all experiments.
tional gain control mechanism, by which stage III becomes
an adaptive resonance (or self-stabilizing) model (Grossberg,
                                                                                Baby-Mirror-Test
1976c). This is realized by feedback connections from the                       In a psychological experiment done by Rochat and Morgan
pattern layer to the pooling layer, which are trained by                        (1995), two real-time videos with different spatial transfor-
                                                                                mations were presented to infants, showing their own leg-
   1/η · ∂w ji (t)/∂t    = ∆w ji (t) = o j (t−1) · (neti (t) − w ji (t)) ,      movements. The infants paid significantly more attention to
                                                                         (7)    a mirrored view of their movements than to an untransformed
where neuron j is the winner of time step t −1. This means                      presentation. This underlines the importance of movement
that the outgoing weight vector of the last most active pattern                 directionality in self-perception.
neuron also approaches the input of the pooling layer (which                       In our experiment, we interpret the attention of an infant
again activates the neuron itself) and thereby learns a predic-                 to be guided by surprise (see e.g. Itti & Baldi, 2006), or an
tion over a marginal time span.                                                 error in the prediction of visual feedback, assuming that a
   The absolute outstar learning signal is also used on forward                 normal self-perspective has been learned. To show that an
propagation from the winner of time step t −1 as axo-axonic                     exogenous visual transformation produces such an error, we
modulatory gain in the pooling layer i:                                         trained our model on the specified arm simulation. The for-
                                                                                ward and backward swings of the arm were clearly distin-
                      ai (t) = 1 − ∆w ji (t) ∈ [0, 1] .                  (8)    guished after presenting the whole movement (forward and
By this modulation, the last winner inhibits the pooling                        backward) 300 times. We then decorrelated the visual feed-
layer’s output (via Eq. 1): the larger the error in and the larger              back from the associated proprioception by left-right inver-
the reliability of the prediction, the stronger is the resulting                sion of the network’s visual input. As can be seen in Fig. 4,
inhibition (cf. Eq. 7). In result, the pattern distinction is im-               the prediction error (RMS of the winner pattern neuron) rises
proved further.                                                                 and stays constant since we neither allow further learning nor
   The prediction error is in turn also being backpropagated                        2 All other backpropagation terms (including those for gain
top-down through the network to adapt the mental transfor-                      fields) and the weight adaptation rules for the model’s internal per-
mation in an error-minimizing manner (see Fig. 2). In our                       ception angle µ follow from gradient descent.
                                                                            1386

Figure 4: Baby-mirror-test. The prediction error decreases
during training, and increases afterwards when the simulated       Figure 6: Learning multiple canonical views. The prediction
vision is left-right inverted.                                     error decreases for each canonical view trained repeatedly.
                                                                   The smaller peaks in the error when the same view is shown
                                                                   again indicate pattern recognition.
the adjustment of the model’s internal perspective µ, effec-
tively simulating the surprise of the babies when confronted
with an inverted video display.                                    that were exclusively winning in all repetitions of a single per-
Canonical Views                                                    spective, stacked for the forward and backward swing. Both
                                                                   the forward and backward swings as well as the canonical
View-based representations of goals and biological motion
                                                                   views of the whole movement were represented by a com-
have been found in the macaque premotor cortex area F5
                                                                   parable amount of patterns, while movements learned early
(Caggiano et al., 2011) as well as in the (posterior) STS,
                                                                   were slightly favored in terms of the number of patterns.
respectively, which are both considered to be part of – or
                                                                      Fig. 6 shows the trend of the prediction error over time.
contributing to – the mirror neuron system. Those view-
                                                                   It can be seen that the prediction error decreased separately
dependent cells are assumed to play a part in the resulting
                                                                   for each trained canonical view. Thus, several, independent
view-independence of action recognition associated to further
                                                                   canonical views can be learned and maintained by the NN
cells found both in F5 (Caggiano et al., 2011) and (anterior)
                                                                   model.
STS (Jellema & Perrett, 2006). We show that our model is
able to learn multiple view-dependent representations of bio-      Mental Transformation
logical motion, which we term ‘canonical views’.
   In this experiment, we trained the model on three ro-           Motivated by the fact that humans appear to mentally ro-
tated perspectives (ν ∈ {0, 120, −120}°) of the arm move-          tate objects to their respective closest, known canonical view
ment and repeated that training 4 times. 50 full arm move-         (Shepard & Metzler, 1971), in the final experiment we in-
ments were presented in each perspective, resulting in 600         vestigate if the model is able to transform biological mo-
full arm swings altogether. As in the baby mirror test, we did     tion to the closest canonical motion view. In order to show
not allow the adaptation of µ.                                     that the model is able to change its internal perspective using
   Fig. 5 shows that multiple canonical views could generally      the expected directional correlations, we set the exogenous
be learned by the same network without relearning patterns in      rotation ν of the visual feedback to a random value within
between: The bar plot counts the number of pattern neurons         [−180, 180]° after learning three canonical views as in the
                                                                   last experiment. In doing so, we did not allow new patterns
                                                                   to arise, but allowed the model/mental rotation µ to adapt ac-
                                                                   cording to the prediction error backpropagated to the mental
                                                                   rotation module via the individual Ib stages in the visual path-
                                                                   way.
                                                                      Fig. 7 shows the adaptation of the overall rotation (ν + µ)
                                                                   over time for all tested trials. The model adapted its men-
                                                                   tal rotation angle µ progressively to the nearest (in terms of
                                                                   orientation difference) canonical view that was learned be-
                                                                   fore without explicit knowledge about the simulation angle ν.
                                                                   Thus, the NN model is able to derive the perspective of an-
Figure 5: Separate representation of canonical views and mo-       other person (in this case a simple arm) by learning to asso-
tion directions. The three canonical views and two motion          ciate visual motion of relative joint locations with the angular
directions (color-coded) are learned in six disjunct groups of     motion of the joints. This adaptation is driven by the error in
pattern neurons.                                                   self-generated predictions.
                                                               1387

                                                                                               References
                                                                     Andersen, R. A., Essick, G. K., & Siegel, R. M. (1985).
                                                                        Encoding of spatial location by posterior parietal neurons.
                                                                        Science, 230(4724), 456–458.
                                                                     Caggiano, V., Fogassi, L., Rizzolatti, G., Pomper, J. K., Thier,
                                                                        P., Giese, M. A., et al. (2011). View-based encoding of
                                                                        actions in mirror neurons of area f5 in macaque premotor
                                                                        cortex. Current Biology, 21(2), 144–148.
                                                                     Fleischer, F., Christensen, A., Caggiano, V., Thier, P., &
                                                                        Giese, M. A. (2012). Neural theory for the perception of
                                                                        causal actions. Psychological research, 76(4), 476–493.
                                                                     Grossberg, S. (1976a). On the development of feature detec-
                                                                        tors in the visual cortex with applications to learning and
Figure 7: Perspective taking. When a novel view on the                  reaction-diffusion systems. Biological Cybernetics, 21(3),
trained motion is shown, the perspective gradually converges            145–159.
to the nearest canonical view.                                       Grossberg, S. (1976b). Adaptive pattern classification and
                                                                        universal recoding: I. parallel development and coding of
       Summary, Conclusion & Future Work                                neural feature detectors. Biological cybernetics, 23(3),
                                                                        121–134.
The presented results have shown that our NN model is able
to simulate (a) surprise when being presented with unex-             Grossberg, S. (1976c). Adaptive pattern classification and
pected directionalities in visual kinematics; (b) the learning          universal recoding: II. feedback, expectation, olfaction, il-
of multiple canonical views of biological motion and thus the           lusions. Biological cybernetics, 23(4), 187–202.
generation of both, view-dependent and view-independent vi-          Holmes, N. P., & Spence, C. (2004). The body schema and
sual mirror neurons; (c) the prediction-error-driven adapta-            multisensory representation(s) of peripersonal space. Cog-
tion of the visual perspective to derive the perspective of an-         nitive Processing, 5, 94-105.
other person.                                                        Itti, L., & Baldi, P. (2006). Bayesian surprise attracts hu-
   Despite the rather large degree of abstraction in our model,         man attention. Advances in neural information processing
our experiments confirm that directionalities in a visuo-               systems, 18, 547.
proprioceptive space alone may suffice to put oneself into           Jellema, T., & Perrett, D. I. (2006). Neural representations
another person’s shoes. While the brain may certainly use               of perceived bodily actions using a categorical frame of
other clues as well, it appears that the perception of biolog-          reference. Neuropsychologia, 44(9), 1535–1546.
ical motion plays a crucial factor (Pavlova, 2012). In effect,       Johansson, G. (1973). Visual perception of biological motion
our model offers an embodied pathway towards learning mir-              and a model for its analysis. Perception & psychophysics,
ror neuron capabilities, imitating other people, deriving their         14(2), 201–211.
intentions, and even showing empathy. Further model eval-            Layher, G., Giese, M. A., & Neumann, H. (2014). Learn-
uations may even yield implications for understanding social            ing representations of animated motion sequencesa neural
dysfunctions, such as autism.                                           model. Topics in Cognitive Science, 6(1), 170–182.
   Despite the capabilities of the model, further investigations     Pavlova, M. A. (2012). Biological motion processing as a
are necessary. Most importantly, here we assigned bodily fea-           hallmark of social cognition. Cerebral Cortex, 22(5), 981-
tures to neural inputs directly. However, when visual input             995.
is presented, the observed features still need to be properly        Pavlova, M. A., & Sokolov, A. (2003). Prior knowledge about
mapped to the respective body parts and thus to the corre-              display inversion in biological motion perception. Percep-
sponding neural inputs. Also, additional information sources            tion, 32(8), 937–946.
may be considered such as motor activity, the axis of gravity,       Pyles, J. A., Garcia, J. O., Hoffman, D. D., & Grossman, E. D.
information about the floor / the ground, acceleration, or fur-         (2007). Visual perception and neural correlates of novel
ther visual features about the observed body. Moreover, the             biological motion. Vision Research, 47(21), 2786–2797.
capability of dealing with information missing or distorted
                                                                     Rochat, P., & Morgan, R. (1995). Spatial determinants in
on action observation needs to be further investigated. Fi-
                                                                        the perception of self-produced leg movements in 3- to 5-
nally, along with object-relative information, a model for self-
                                                                        month-old infants. Developmental Psychology, 31(4), 626-
supervised and view-independent imitation learning could be
                                                                        636.
established.
                                                                     Rumelhart, D. E., & Zipser, D. (1985). Feature discovery by
Acknowledgments                                                         competitive learning. Cognitive science, 9(1), 75–112.
GL and HN have been supported by the SFB Transregio 62               Shepard, R. N., & Metzler, J. (1971). Mental rotation of
funded by the German Research Foundation (DFG).                         three-dimensional objects. Science, 171(3972), 701-703.
                                                                 1388

