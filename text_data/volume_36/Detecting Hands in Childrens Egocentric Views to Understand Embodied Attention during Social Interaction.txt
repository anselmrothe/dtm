UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Detecting Hands in Children's Egocentric Views to Understand Embodied Attention during
Social Interaction
Permalink
https://escholarship.org/uc/item/3cq8m6fh
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Bambach, Sven
Franchak, John
Crandall, David
et al.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

        Detecting Hands in Children’s Egocentric Views to Understand Embodied
                                           Attention during Social Interaction
                             Sven Bambach†, John M. Franchak, David J. Crandall†, Chen Yu
                                            {sbambach, jmfranch, djcran, chenyu}@indiana.edu
                                        † School of Informatics and Computing, Indiana University
                                  Department of Psychological and Brain Sciences, Indiana University
                                                          Bloomington, IN, 47405 USA
                              Abstract                                   2011). In more natural interactions, there are multiple objects
   Understanding visual attention in children could yield insight
                                                                         competing for attention, various manual actions toward those
   into how the visual system develops during formative years            toy objects, and spontaneous goals. Visual attention changes
   and how children’s overt attention plays a role in development        from moment to moment according to the child’s own actions
   and learning. We are particularly interested in the role of hands     and the parent’s actions toward the child and objects. Though
   and hand activities in children’s visual attention. We use head-
   mounted cameras to collect egocentric video and eye gaze data         complex, these are the contexts in which real-world learning
   of toddlers during playful social interaction with their parents,     occurs. Compared with adults, young children’s attentional
   and developed a computer vision system to track and label dif-        systems may be even more tied to bodily actions.
   ferent hands within the child’s field of view. We report detailed
   results on appearance frequencies and spatial distributions of           The goal of the present study is to understand how sensory-
   parents’ and children’s hands both in the child’s field of view       motor behavior supports effective visual attention in toddlers.
   and as the target of the child’s attentional fixation.
                                                                         Towards this goal, we developed a more naturalistic exper-
   Keywords: Attention; Development; Eye tracking; Vision
                                                                         imental paradigm in which a child and parent wear head-
                          Introduction                                   mounted eye trackers while freely engaged with a set of toys.
                                                                         Each eye tracking system captures egocentric video from a
The visual world is cluttered with objects and events gener-
                                                                         first person perspective as well as gaze direction in the first-
ated by oneself and others. To efficiently process a cluttered
                                                                         person view. In this way, we precisely measure the visual
and complex visual world, perceptual and cognitive systems
                                                                         attention of both the parent and child, and also their manual
must selectively attend to a subset of this information. Atten-
                                                                         actions. Recent findings using the same paradigm show that
tion can be viewed as a spatial spotlight (Posner, 1980) that
                                                                         in toy play, both children and parents visually attend to not
can be implemented both internally and externally. Although
                                                                         only the objects held by oneself but also the objects held by
adults can attend to a location outside the area targeted by
                                                                         the social partner (Yu & Smith, 2013); in doing so, they create
eye gaze (Shepherd, Findlay, & Hockey, 1986), attention is
                                                                         and maintain coordinated visual attention by looking at the
often tied to the body and sensory-motor behaviors — adults
                                                                         same object at the same time. The target object is likely to be
typically orient gaze direction to coincide with the focus of
                                                                         held by child or parent. Similarly, other work has shown that
the attentional spotlight. Studies of adults engaged in com-
                                                                         by holding objects, parents increase the likelihood that infants
plex tasks from making sandwiches to copying block patterns
                                                                         will look at parents’ hands (Franchak et al., 2011). These re-
(Ballard, Hayhoe, Pook, & Rao, 1997; Hayhoe & Ballard,
                                                                         sults suggest the important role of hands and hand activities
2005) suggest that the momentary disposition of the body in
                                                                         (of both children and parents) in toddlers’ visual attention.
space serves as a deictic (pointing) reference for binding sen-
sory objects to internal computations (Ballard et al., 1997;                Given previous findings, the present study focuses on pro-
Spivey, Tyler, Richardson, & Young, 2000). These studies                 viding new evidence on how eye and hand actions interact to
analyzed the coordination of eye, head, and hands by mea-                support effective visual attention to objects in toddlers. We
suring multiple streams of behavior in free-flowing tasks with           first describe a new method to automatically detect hands and
multiple goals and targets for attention.                                faces in egocentric video, allowing us to locate (at a pixel
   Attention and information selection are critical in early de-         level) both one’s own hands and the social partner’s hands in
velopment and learning (Mundy & Newell, 2007) as early                   the first person view. Next, we report a series of results that
attention is predictive of later developmental outcomes (Ruff            link hands and hand actions with visual attention, to show
& Rothbart, 1996). Most studies of the development of at-                how the child’s and parent’s hands contribute to visual infor-
tention employ highly-controlled experimental tasks in the               mation selection in the child’s view.
laboratory. Many studies use remote eye tracking systems
to measure looking behaviors, revealing much about the vi-                                       Experiment
sual attention of toddlers as they passively examine visual
stimuli displayed on a computer screen. However, more re-                To realize our overall goal of measuring visual attention in
cent studies using head-mounted eye tracking have addressed              natural interactions, we developed a multi-modal sensing sys-
visual selection in freely-moving toddlers when they are en-             tem that allows us to capture a wide variety of video and sens-
gaged in everyday tasks (Franchak, Kretch, Soska, & Adolph,              ing data from participants in our lab.
                                                                     134

         child’s egocentric view               parents’s egocentric view
                                                                              constraints on where parents or children looked or what they
                                                                              should do or say. Each experiment consisted of four trials and
                                                                              each trial lasted about 1 to 2 minutes. In between trials, the
                                                                              toy sets were replaced to keep the children interested.
                                                                              Data
                                                                              We collected a total of 67,913 frames (about 38 minutes) of
                                                                 from eye     video data from the 6 children. Of those frames, 54,367 had
      from eye                                                    camera
       camera                                                                 valid gaze data (i.e. located within the camera’s field of view)
                                                                              in the form of an x-y coordinate, indicating the gaze center.
              from head
                                                          from head           To detect, track, and distinguish all hands that appear in our
                camera
                                                            camera            video data (including the child’s left and right hands, and the
                                                                              parent’s left and right hands), we developed a special hand
                               experimental setup
                                                                              tracking algorithm that is described in the following section.
Figure 1: Experimental setup. We use 4 cameras to record                                             Hand Tracking
joint play between a child and parent. The head-mounted eye                   Given the large amount of video data collected in our exper-
tracking systems (worn by both) each consist of a head cam-                   iments, we needed automated techniques to track and label
era to capture its wearer’s egocentric view and an eye camera                 the positions of the hands in each video frame. Tracking is
that tracks the eye’s pupil. All cameras work with a temporal                 a well-studied problem in the computer vision literature, and
resolution of 30Hz and a spatial resolution of 480×720px.                     some work has specifically studied hand tracking (Chen, Fu,
                                                                              & Huang, 2003) in the context of gesture recognition. How-
Muti-modal Sensing System                                                     ever, most of that work studies video from stationary cam-
Our sensing environment allows us to monitor parents and                      eras. The fact that our video comes from head-mounted cam-
children as they engage in free-playing interaction with toy                  eras introduces significant new challenges because observers’
objects, as shown in Figure 1. A child and parent sit at a table              heads are free to move, continually changing the locations of
in the lab and face one another. Each wears a lightweight,                    hands in the field of view. In fact, we are not aware of any
head-mounted eye tracking system consisting of two cameras                    work that has studied multiple hand tracking in egocentric
(Franchak et al., 2011): a wide-angle outward-facing cam-                     video; the closest is that of Ren and Gu (2010), who propose
era (100◦ diagonal) capturing the egocentric field of view of                 a system for recognizing objects held by the camera wearer.
the participant, and an inward-facing infrared camera pointed                    Fortunately, the constraints of our lab environment help to
at the participant’s left eye, which tracks the pupil in order                ease our tracking problem: we know there are at most two
to measure eye gaze position (shown by green cross-hairs in                   people in each frame, that the child’s hands are closer to the
Figure 1). The eye tracker was calibrated by encouraging par-                 camera than the adult’s hands, that in general the children
ticipants to look at known points in the environment; once                    and parents are facing one another, and that the participants’
calibrated, the accuracy of the eye tracker is about 3◦ . In                  clothing is white. Our goal is to identify which of the four
addition, two scene cameras, two microphones and two head-                    hands (child’s hands and parent’s hands) are visible in each
mounted motion sensors with 6 degree-of-freedom tracking                      frame, and then to identify the position of the visible ones.
allowed for a variety of multi-modal coding. As the purpose                   Our approach consists of four major steps: (1) identifying
of this study is to investigate the role of hands in a toddler’s              potential skin pixels based on color; (2) clustering these pix-
visual attention, the focus of this paper will be on the child’s              els into candidate hand and face regions; (3) tracking these
egocentric video and eye gaze data.                                           regions over time; and (4) labeling each region with its body
                                                                              type (face, child left or right hand, parent left or right hand).
Subjects
For the study, we considered 6 child-parent dyads. The chil-                  Step 1: Skin Detection
dren’s mean age was 19 months (SD = 2.56 months). Dyads                       To look for faces and hands, we first identify pixels having
were chosen based on hand tracking performance (see next                      skin-like colors. Although human skin colors are surprisingly
section) among a pool of 14 candidates to ensure the great-                   consistent across people when represented in an appropriate
est accuracy in the reported results. Although the sample size                color space (we use YUV here), pixel-level skin classification
was small, analyzing high-density data with more than 10,000                  is difficult because illumination can dramatically alter skin
frames per child yielded highly consistent results.                           appearance and because many common objects (like walls)
                                                                              often have skin tones. We thus tuned our skin classifier for
Procedure                                                                     each individual subject, by sampling 20 frames at random and
Parents were told to engage their child with toys (three pos-                 having a human label the skin regions in each frame. We
sible toys were on the table) and otherwise interact as natu-                 then used these labeled pixels as training exemplars to learn a
rally as possible, leading to a free-flowing interaction with no              simple Gaussian classifier, in which each pixel is encoded as
                                                                          135

a 2d feature vector consisting of the two color dimensions (U
and V). To detect skin in unlabeled images, we evaluate the
likelihood of each pixel under this model, threshold to find
candidate skin pixels, and use an erosion filter to eliminate
isolated pixels.
Step 2: Skin Grouping
Given the skin detection results from Step 1, we apply Mean
Shift clustering (Comaniciu & Meer, 2002) to each frame to           Figure 2: Two sample frames showing the results of our ego-
group skin pixels into candidate skin regions. Mean Shift            centric hand-tracking algorithm. The red circles denote the
does not require knowing the number of clusters ahead of             “hotspots” used to label skin blobs. The black crosshair in-
time (as K-means does), but instead requires an estimate of          dicates eye gaze center. Left: Hotspots in their default center
the size and shape of the clusters; we use circular disks of         location. Right: Hotspots aligned based on detected face.
radius 75 pixels in our implementation.
                                                                     with respect to the face. Anchoring the expected spatial lo-
Step 3: Tracking                                                     cations of hands to the parent’s head helps compensate for
We next attempt to find correspondences between the skin             view changes due to the child’s head motion. In particular, we
blobs estimated in temporally-adjacent frames, in order to           create a configuration of five points (“hotspots”) that roughly
create tracks of skin regions over time. To do this, we scan         correspond to the expected (mean) position of the four hands
the frames of the video in sequence. For each frame i, we            relative to the face, illustrated as red circles in Figure 2. For
assign each skin region to the same track as the closest region      each non-face candidate track generated by Step 3, we com-
in frame i − 1 as long as the Euclidean distance between the         pute the centroid of its location across the frames in which it
region centroids is below a threshold (we use 50 pixels), and        is visible, find the hotspot closest to the centroid, and assign
otherwise we start a new track. Each track thus consists of a        the track to the corresponding body part. When no face is
starting frame number indicating when the region appears, an         detected, the hotspots take a default position that assumes the
ending frame number indicating when it disappears, and an            face is in the top-center (Figure 2, left pane).
(x, y) position of the region within each intervening frame.
                                                                     Evaluation
Step 4: Labeling Skin Regions
                                                                     We manually tested the accuracy of our hand tracking algo-
Finally, we need to identify which tracks from Step 3 cor-           rithm on 600 randomly-selected frames (100 frames for each
respond to actual skin regions, and then to label each track         of 6 subjects), and counted the proportion of correctly-labeled
with one of five possible body parts (parent’s head, child left      regions. We found that the overall accuracy was 71%, rang-
or right hand, parent left or right hand). We experimented           ing from 67% to 75% across the subjects. In comparison, a
with various strategies and settled on a relatively simple ap-       baseline method that randomly assigns labels to skin regions
proach that uses the relative spatial location of regions within     (and assuming that the skin segmentation and clustering per-
the frame (and in particular the observation that the parent’s       form correctly) achieves 20% accuracy. Labeling errors are
head is usually above and between the parent’s hands, which          caused by a variety of factors, but the two most common are:
are in turn above the child’s hands). We thus first try to find      (1) when hands are close together and the clustering algo-
tracks corresponding to the face, and then check the relative        rithm incorrectly combines them into a single body part, and
position of other regions to find and label the hands.               (2) when hands spend a significant amount of time away from
Detecting Face Tracks. We tried off-the-shelf face detec-            their expected location relative to the head.
tors, but they are not reliable in our context because the par-
ent’s head is often not fully visible (e.g. in left frame of                           Results and Discussion
Figure 2). Instead we built a very simple face detector that         The hand tracking algorithm provides frame-by-frame data
uses the fact that the parents in our experiments wear a black       about the position, size, and shape of each hand in the child’s
head-mounted camera. In particular, we trained a linear Sup-         field of view. We analyzed this data in terms of spatial distri-
port Vector Machine classifier (Burges, 1998) on manually-           butions of the different hand classes and in terms of how often
labeled head regions (using the same 20 frames that we used          each class appears over time. Further, we used children’s eye
to learn skin color, with the remaining skin regions serving         gaze data to investigate moments where hands were the target
as negative exemplars), where the features consist of a 256-         of the child’s overt visual attention.
bin grayscale histogram over the pixels in the track region.
We then identify faces by finding tracks for which the trained       Hands in the Child’s Field of View
SVM classifies over half of the regions in the track as faces.
                                                                     To determine how often children had the opportunity to view
Labeling Hands. Once face tracks have been found, we                 their own hands and their parents’ hands, we calculated how
mark potential hand tracks based on their relative position          often hands are present in the field of view.
                                                                 136

                                                1.0
      Present in Field of View (Prop. Frames)
                                                      Right Hand                                    expanded more horizontally than vertically: σx was roughly
                                                      Left Hand                                     twice as much as σy for each hand. Parents’ left and right
                                                0.8                 *                               hands also have similar distributions in terms of variance
                                                                                                    (Figure 4C). A 2 (agent: child, parent) × 2 (hand: left, right)
                                                0.6
                                                                                                    × 2 (direction: horizontal, vertical) ANOVA confirmed the
                                                                                                    main effect of direction, F(1, 5) = 36.4, p = .002. However,
                                                                                                    a significant agent × direction interaction, F(1, 5) = 10.5,
                                                0.4                                                 p = .023 and follow-up pairwise comparisons show that par-
                                                                                                    ents’ hands occupy a larger vertical space (right hand σy = 59,
                                                0.2                                                 left hand σy = 60) compared to children’s hands (right hand
                                                                                                    σy = 37, left hand σy = 43, p = .009). Horizontal variance
                                                                                                    terms did not differ between the hands of children and par-
                                                 0
                                                      Child’s Own       Parent’s   Parent’s         ents, and no other effects approached significance.
                                                        Hands            Hands      Face
                                                                                                       Children’s and parents’ hands were spatially segregated in
                                                                                                    visual space. Overall, children’s hands were lower in the vi-
Figure 3: Bar graphs showing the proportions of frames in
                                                                                                    sual field compared to parents’ hands and were often seen to-
which each class of hands was detected (error bars show 1
                                                                                                    wards the lower boundary of the field of view (µy = −172 for
SE). For comparison, the value for the parent’s face is shown.
                                                                                                    the left hand and µy = −178 for the right hand). A 2 (agent:
                                                                                                    child, parent) × 2 (hand: left, right) ANOVA on µy revealed
Frequency of Hands in View. Figure 3 shows the propor-                                              that parents’ hands were significantly higher than children’s
tion of frames in which each hand class was detected. As                                            hands (main effect of agent, F(1, 5) = 184.6, p < .001). In
the hand tracking algorithm needs to distinguish hands from                                         the horizontal dimension, the child’s right hand and parents’
faces and thus implicitly tracks the parent’s face as well, we                                      left hand tended to reside in the right half of the visual field,
include results for the face for reference. Overall, hands                                          while the child’s left hand and parents’ right hand tended to
were frequently in view, although the child’s own hands (right                                      reside in the left half of the visual field. A 2 (agent: child,
hand = 38% and left hand = 40%) are in view less frequently                                         parent) × 2 (hand: left, right) ANOVA on µx confirmed a sig-
than the parent’s hands (right hand = 57% and left hand =                                           nificant agent × hand interaction, F(1, 5) = 1377.7, p < .001.
66%). A 2 (agent: child, parent) × 2 (hand: left, right)                                               Since our automatic hand labeling is not perfect and makes
repeated-measures ANOVA confirmed a main effect of agent,                                           spatial assumptions, these results could potentially be biased
F(1, 5) = 21.74, p = .006. The main effect of agent × hand                                          by our algorithm. We manually labeled the location of hands
interaction did not reach significance.                                                             in 2,800 randomly sampled frames and repeated our analyses.
   The parent’s face also appeared frequently in the child’s                                        A 2 (agent: child, parent) × 2 (hand: left, right) ANOVA on
view and was detected in 77% of the frames. We note here                                            the µy ’s of manually labeled frames confirmed that parents’
that the set of subjects we chose (based on hand detection ac-                                      hands were located higher than those of children (main effect
curacy) might be slightly biased to have parent’s faces in view                                     of agent, F(1, 5) = 111.1, p < .001). In addition, a 2 (agent:
more often than others as our algorithm uses face information                                       child, parent) × 2 (hand: left, right) ANOVA on the µx ’s in
to improve its prediction and thus tends to perform better for                                      hand labeled frames showed a significant agent × hand inter-
subjects where the face is in view more frequently.                                                 action as in frames labeled by our algorithm, F(1, 5) = 529.4,
                                                                                                    p < .001). We conclude that our results on spatial locations of
Spatial Distribution of Hands in View. Spatial asymme-                                              hands in the field of view are not an artifact of our algorithm.
tries might account for the different frequencies with which                                           Different spatial distributions of hands may account for dif-
children’s and parents’ hands were visible. Next, we present                                        ferent frequencies of hands being visible. Most likely, chil-
spatial distributions of hands in the children’s field of view in                                   dren’s hands were not as frequent as parents’ hands because
the form of heat maps. The first row of Figure 4B-C shows                                           they occupied locations towards the lower boundary of the
the distributions of the child’s left hand, the child’s right hand,                                 field of view. If children moved their hands down or tilted
the parent’s right hand and the parent’s left hand, respectively.                                   their heads up, their own hands would leave the field of view.
Each data point in the heat map corresponds to the centroid
(mean of the hand blob) of the detected hand. The distribu-                                         Hands as Target of the Child’s Overt Attention
tions are accumulated over all 6 subjects where N depicts the
number of frames with the hand in view. To allow quantitative                                       Next we examined how often and where hands were targeted
comparison, we calculated robust (60% trimmed) statistics in                                        by children’s gaze. We counted a gaze fixation on the hand
the form of horizontal and vertical mean (µ) as well as hori-                                       whenever a 10◦ hot spot (corresponding to a circle with radius
zontal and vertical standard deviation (σ) of the distributions                                     of 32 pixels) around the gaze center overlapped with the area
(off-diagonal co-variances are not shown).                                                          of a detected hand.
   Children’s left and right hands had very similar distribu-                                       Frequency of Hands Being Targeted by Gaze. Figure 5
tions in terms of variance (Figure 4B) with distributions that                                      (left) shows mean values for the overall proportion of frames
                                                                                              137

A. Reference frame &                     B. Distributions of Child’s Own Hands (top) C. Distributions of Parents’ Hands (top) &
 distribution of gaze                    & Gaze when Fixating Own Hands (bottom) Gaze when Fixating Parents’ Hands (bottom)
                                         left hand         N = 29540 right hand                               N = 28321                        right hand                N = 44540 left hand             N = 53922
          y
                                                     μ = (-137, -172)                                    μ = (174, -178)                                             μ = (-134, -17)                 μ = (177, -10)
                                                     σx = 81, σy = 43                                    σx = 79, σy = 37                                           σx = 80, σy = 59                σx = 90, σy = 60
                          x      Hands
                   N = 54367             left hand          N = 2844 right hand                              N = 1585                          right hand                 N = 5177 left hand              N = 4264
                 μ = (10, -22)                        μ = (-23, -125)                                   μ = (67, -140)                                                μ = (-22, -26)                   μ = (76, -19)
              σx = 79, σy = 82                       σx = 56, σy = 45                                  σx = 65, σy = 53                                             σx = 68, σy = 59                σx = 58, σy = 56
                                 Gaze
Figure 4: Spatial distributions of hands and eye gaze. Column A: The top image shows a sample frame from the child’s view,
while the bottom image shows the spatial distribution of the children’s eye gaze across all valid frames. Column B: The top row
shows the distributions of children’s own hands (based on hand centroids) within their field of view. The bottom row shows the
distributions of children’s eye gaze while looking at their own hands. Column C: The top row shows the distributions of parent’s
hands within the children’s field of view. The bottom row shows the distributions of children’s eye gaze while looking at their
parent’s hands. Also shown are robust (60% trimmed) estimates of mean (µ) and standard deviation (σ) of the distributions as
well as the number of data points (N). Heat maps are 480×720px and a small Gaussian blur (σG = 10px) was applied.
                                                                                                                               0.20
                                                                                        Gaze Directed at Hand (Prop. Frames)
in which children’s gaze overlapped with each hand. Children                                                                           Right Hand
                                                                                                                                       Left Hand
spent about twice as long looking at parent’s hands (about
9.5% for the right hand and 7.8% for the left hand) than they                                                                  0.15
                                                                                                                                                      *
did looking at their own hands (3.0% right hand and 5.3%
left hand). A 2 (agent: child, parent) × 2 (hand: left, right)                                                                 0.10
on proportion of frames targeting hands confirmed a main ef-
fect of agent, F(1, 5) = 8.52, p = .03, and found no other
                                                                                                                               0.05
significant effects.
   Higher rates of looking to parents’ hands may be the re-
sult of parents’ hands being in view more often. Thus, we                                                                      0.00
                                                                                                                                      Child’s Hands       Parent’s Hands           Child’s Hands   Parent’s Hands
recalculated the proportion of looking to hands based on the                                                                               Across All Frames                        Over Frames When Hand is
number of frames where each hand was present in the field                                                                                                                            Present in Field of View
of view (right side of Figure 5). This normalization increased                          Figure 5: Bar graphs showing the proportions of frames in
the proportion of looking for both the child’s own hands and                            which each class of hands was looked at (based on a 10◦
the parent’s hands. Furthermore, the difference between the                             gaze hot spot). Left: Fractions based on all frames with valid
time spent looking at parent’s hands and looking at their own                           eye gaze (N = 54367). Right: Fractions based on all frames
hands is no longer significant when taking the availability of                          where the corresponding hand was in the field of view.
hands into account (no effects found in a 2 (agent: child,
parent) × 2 (hand: left, right) on normalized proportions of                            gaze data, taking only into account moments when gaze was
frames targeting hands).                                                                fixated on one of the hands. Across children’s and parents’
                                                                                        hands, we observed that distributions when gaze targeted
Spatial Distribution of Gaze when Targeting Hands. Fi-                                  hands were more centrally located compared to the overall
nally, we present the spatial distributions of children’s eye                           distributions of hands in the field of view. To verify this statis-
gaze (bottom row of Figure 4) when viewing hands. The gaze                              tically, we calculated the distance from the center of the field
heat maps are composed similarly to the hand heat maps, ex-                             of view for the means of the distributions of the hands and
cept that each data point now corresponds to the eye gaze cen-                          the gaze locations when hands were fixated (top and bottom
ter as opposed to a hand centroid. Prior work has shown that                            rows of Figure 4B-C). A 2 (agent: child, parent) × 2 (hand:
gaze tends to be biased towards the center of the field of view                         left, right) × 2 (distribution: hands overall, gaze-targeted)
(Foulsham, Walker, & Kingstone, 2011). Figure 4A shows                                  revealed a main effect of agent, F(1, 5) = 66.1, p < .001,
the overall distribution of eye gaze during the experiment, ac-                         distribution, F(1, 5) = 13.7, p = .014, and a significant 3-
cumulated over all 6 subjects. Indeed, the distribution has this                        way interaction, F(1, 5) = 17.7, p = .008. Overall, parents’
center bias with a mean close to the center of field of view and                        hands (M = 131.4 pixels) were closer to the center of the
similar variances in horizontal and vertical direction.                                 field of view compared to children’s hands (M = 195.5 pix-
   The bottom row of Figure 4B-C shows subsets of the eye                               els). Follow-up tests on the 3-way interaction showed that
                                                                                  138

child’s left hand, child’s right hand, and parent’s left hand          Bambach, S., Crandall, D. J., & Yu, C. (2013). Understanding
were more centrally located when targeted by gaze compared               embodied visual attention in child-parent interaction. In
to their overall distributions (p < .05), while the parent’s right       IEEE Joint International Conference on Development and
hand location did not change when targeted by gaze (p = .48).            Learning and Epigenetic Robotics, (pp. 1–6).
                                                                       Burges, C. J. C. (1998). A tutorial on support vector ma-
Discussion                                                               chines for pattern recognition. Data mining and knowledge
Hands are an important visual stimulus. One’s own hands                  discovery, 2(2), 121–167.
are relevant for guiding reaching actions and manipulating             Chen, F.-S., Fu, C.-M., & Huang, C.-L. (2003). Hand gesture
objects (Hayhoe & Ballard, 2005; Franchak et al., 2011),                 recognition using a real-time tracking method and hidden
while the hands of others can convey information about the               markov models. Image and Vision Computing, 21(8), 745–
attention and goals of social partners (Olofson & Baldwin,               758.
2011; Ullman, Harari, & Dorfman, 2012). But for toddlers               Comaniciu, D., & Meer, P. (2002). Mean shift: A robust
to learn from hands, they must be able to see them. Here,                approach toward feature space analysis. IEEE Transactions
we demonstrate that for toddlers playing with adults, hands              on Pattern Analysis and Machine Intelligence, 24(5), 603–
are frequently in view. However, what infants see depends                619.
on where they actively point their heads: the resulting spatial        Foulsham, T., Walker, E., & Kingstone, A. (2011). The
constraints (e.g., child’s hands being low in the field of view)         where, what and when of gaze allocation in the lab and
mean that children’s own hands are in view less often than               the natural environment. Vision Research, 51(17), 1920 -
their parents’ hands. Consequently, children overtly attend to           1931.
parents’ hands more often than their own hands. Moreover,              Franchak, J. M., Kretch, K. S., Soska, K. C., & Adolph, K. E.
we show that when children fixate on hands, they do so more              (2011). Head-mounted eye tracking: A new method to
often when hands are centrally located in their fields of view,          describe infant looking. Child development, 82(6), 1738–
suggesting that children move their heads to bring visual tar-           1750.
gets into the center of their visual fields. Most likely, children     Hayhoe, M., & Ballard, D. (2005). Eye movements in natural
coordinate their eyes and heads to focus on areas relevant to            behavior. Trends in Cognitive Sciences, 9(4), 188–194.
the task at hand, looking down towards their own hands when            Mundy, P., & Newell, L. (2007). Attention, joint attention,
reaching and looking up towards their parents’ hands when                and social cognition. Current Directions in Psychological
parents present objects (Yu & Smith, 2013).                              Science, 16(5), 269–274.
                      Future Directions                                Olofson, E. L., & Baldwin, D. (2011). Infants recognize
There are two major directions that we are exploring in future           similar goals across dissimilar actions involving object ma-
work. First, we want to improve the performance of our hand              nipulation. Cognition, 118(2), 258–264.
tracking algorithm. Towards this goal, we are working on               Posner, M. I. (1980). Orienting of attention. Quarterly jour-
probabilistic frameworks that will allow us to jointly take the          nal of experimental psychology, 32(1), 3–25.
spatial configurations of all hands into account when deciding         Ren, X., & Gu, C. (2010). Figure-ground segmentation im-
on a hand label. Better performance will allow us to evalu-              proves handled object recognition in egocentric video. In
ate more participants in the future to further validate these            IEEE Conference on Computer Vision and Pattern Recog-
results in a larger sample. Second, we want to take advantage            nition.
of the idea that hands can be useful clues towards predicting          Ruff, H. A., & Rothbart, M. K. (1996). Attention in early
overt visual attention in children by building models that at-           development: Themes and variations. Oxford University
tempt to predict children’s eye gaze. We briefly experimented            Press.
with very simple models that predict the child’s gaze loca-            Shepherd, M., Findlay, J. M., & Hockey, R. J. (1986). The
tion based on the most visualy dominant hand in view and                 relationship between eye movements and spatial attention.
achieved accuracies that could compete with saliency-based               The Quarterly Journal of Experimental Psychology, 38(3),
predictions on our data (Bambach, Crandall, & Yu, 2013).                 475–491.
                                                                       Spivey, M., Tyler, M., Richardson, D., & Young, E. (2000).
                     Acknowledgments                                     Eye movements during comprehension of spoken scene de-
This work was funded in part by the NIH (R01 HD074601                    scriptions. In Proceedings of the 22nd Annual Conference
and R21 EY017843), the NSF (IIS-1253549), and the Indiana                of the Cognitive Science Society (pp. 487–492).
University Office of the Vice President for Research through           Ullman, S., Harari, D., & Dorfman, N. (2012). From simple
an IU Collaborative Research Grant. JMF was supported by                 innate biases to complex visual concepts. Proceedings of
NICHD Training Grant 5T32HD7475-17.                                      the National Academy of Sciences, 109(44), 18215-18220.
                                                                       Yu, C., & Smith, L. B. (2013). Joint attention without gaze
                          References                                     following: Human infants and their parents coordinate vi-
Ballard, D. H., Hayhoe, M. M., Pook, P. K., & Rao, R. P.                 sual attention to objects through eye-hand coordination.
   (1997). Deictic codes for the embodiment of cognition.                PloS one, 8(11), e79659.
   Behavioral and Brain Sciences, 20(04), 723–742.
                                                                   139

