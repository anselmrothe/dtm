UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Learning and Variability in Spiking Neural Networks
Permalink
https://escholarship.org/uc/item/66s515j6
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Rodny, Jeffrey
Kello, Chris
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                           Learning and Variability in Spiking Neural Networks
                                            Jeffrey J. Rodny (jrodny@ucmerced.edu)
                                          Christopher T. Kello (ckello@ucmerced.edu)
                                                      University of California, Merced,
                                           Department of Cognitive and Information Sciences,
                                            5200 North Lake Road, Merced, CA 95343 USA
                              Abstract                                   mechanisms to lower spike rates. Some learning
                                                                         mechanisms have been proposed that inherently regulate
   Neural networks exhibit ongoing, spatiotemporal patterns of
   spiking activity. Evidence shows that these patterns are              spike activity, most notably spike timing dependent
   metastable, i.e. temporary, transient, and non-stationary.            plasticity (Markram, Lübke, Frotscher, & Sakmann, 1997).
   Metastability is theorized to be adaptive for neural and              However, STDP and similar learning mechanisms have not
   cognitive function, but learning must somehow remain stable           been shown to produce the pervasive power laws that
   in the context of highly variable spike dynamics. In the              characterize intrinsic variations of neural and behavioral
   present study, a neural network learning algorithm is                 activity.
   developed to co-exist with intrinsic variability that arises from
   regulating spike propagation to stay near its critical branching
                                                                            Let us approach the puzzle from the starting point of
   point. The learning algorithm is based on reinforcement               homeostatic regulation instead. Regulatory mechanisms
   traces stored at synapses that change much more slowly than           have been shown to produce power laws, particularly
   synaptic switches triggered to maintain critical branching. As        mechanisms based on critical branching (Beggs & Plenz,
   a result, learning establishes a stable synaptic space within         2003). Critical branching is a general dynamic of event
   which variability and metastability can arise from critical           propagation over networks, whereby each event on a given
   branching. Model efficacy is demonstrated using time-                 node triggers exactly one subsequent event on a downstream
   delayed XOR learning, and spike dynamics are compared
   with evidence of metastability in hippocampal recordings.             node, on average. This general dynamic leads a very
                                                                         specific power law—the sizes of contiguous event cascades
   Keywords: Spiking neural network, critical branching,                 (i.e. number of propagated events till extinguishing) should
   metastability, learning mechanism.                                    follow an inverse power law distribution with an exponent
                                                                         of two, P(S)~1/Sα=2. Beggs and Plenz referred to such
                          Introduction                                   cascades as neuronal avalanches and found evidence for
All neural and behavioral activities are characterized by                them in multi-cell recordings of rat somatosensory cortex.
intrinsic variability—variations that are not attributable to            Many studies have since found the same power law in many
forces outside the system in question (Kello, Beltz, Holden,             kinds of neural activity, including human electrophysiology
& Van Orden, 2007). These variations are found to follow                 (Poil, van Ooyen, & Linkenkaer-Hansen, 2008).
power laws of various kinds (Kello, Brown, Ferrer-i-                        Critical branching is relevant to cognitive science because
Cancho, Holden, Linkenkaer-Hansen, Rhodes, & Van                         it serves to not only regulate spike propagation, but also
Orden, 2010), which means they cannot be discounted as                   maximize memory and computational capacities of spike
Gaussian noise. Kello (2013) recently reported a spiking                 dynamics. Beggs and Plenz (2003) showed this consequence
neural network model that exhibits pervasive power laws in               with an abstracted probabilistic model, and Kello (2013)
its intrinsic dynamics. It does so by virtue of a simple, local          showed it in a spiking neural network model using reservoir
mechanism designed to regulate spike propagation. Such                   computing techniques. Kello formulated a general,
homeostatic regulation is a basic, necessary function of any             biologically plausible mechanism that maintains network-
network with propagating activity. Too much or too little                wide critical branching using only information local to a
activity leads to sub-optimal communication and transport                given neuronal unit and its synapses. The mechanism was
over networks (Beggs & Plenz, 2003). For cognitive                       shown to generate pervasive power laws, i.e. not only
science, this means diminished memory and computational                  neuronal avalanches, but also power law inter-spike
capacity (Bertschinger & Natschlager, 2004).                             intervals (ISIs), and 1/f noise in neural activity as well as
   A long-standing puzzle in neuroscience is how                         simulated behavioral response dynamics.
homeostatic regulation and its concomitant variability in                   In summary, Kello’s (2013) model simulated homeostatic
spike dynamics are coordinated with learning (Turrigiano &               regulation and its concomitant variability, and it related
Nelson, 2000). Both regulation and learning are expressed                critical branching to metastability and cognitive function.
through potentiation and de-potentiation of synapses, which              However, the model did not address learning. In the present
suggests these functions should be prone to interfering with             study, a reinforcement learning mechanism is formulated to
each other. For instance, long-term potentiation of                      work in conjunction with critical branching in a spiking
excitatory synapses may instantiate learning while also                  neural network. The network is shown to learn a temporal
increasing overall spike rates. Learning may then be                     nonlinear function of spike inputs in the face of ongoing
undone when de-potentiation is triggered by regulatory                   variability due to ongoing actions of the critical branching
                                                                     1305

mechanism. This variability is shown to be metastable by                                           k  Ak t   1 :
                                                                                                     k :  k t   i    k
comparison with multi-cell hippocampal recordings (Sasaki,
Matsuki, & Ikegaya, 2007), yet learning is stable even after
reinforcement signals are removed and synapses continue to
                                                                                   Vi t    ,
change as a function of critical branching. Thus the model
provides one solution to the puzzle of homeostatic                                                               k  Ak t '   0 : k max C  
                                                                                   Bi t   0
                                                                                                                 Bi t '   1 ? : Ak t   1
regulation, variability, and learning in spiking neural
                                                                                              Vi t    ?
networks.
                         Methods                                                                                         j  Aj t '   1 : jmin C  
The model is composed of excitatory and inhibitory leaky                                     Vi t                       B j t   1 ? : Aj t   0
                                                                                 j t   Vi t ' e  i t t '  
integrate-and-fire (LIF) neurons, with both feed forward and
recurrent synaptic connections. As in Kello (2013),
connectivity is arranged so that input spikes impinge on a
set of source units, propagate to a set of reservoir units, and
then circulate and propagate to a set of sink units. Model
architecture is shown in Fig 1. Model parameters were
chosen as defaults, based on preliminary simulations.
                                                                                    j t       C j t   C j t '   1   Ri t sgn j 
Further work is needed to investigate the impacts of certain
parameter choices on performance, but in general, small
variations have little or no effect on results.
                                                                               Figure 2: Pseudo-code for spike propagation and regulation
                                                                                 guided by learning, overlaid on a diagram showing one
                 Sink/Reward Units                                             neuronal unit i (circle) with its input j and output k synapses
                                                                                  (triangles). Synapses are enabled or disabled (filled or
                100 XOR “0”      100 XOR “1”                                    unfilled), and an example chain of events is shown for one
                                                                               synaptic potential δj at time t. Other variables and functions
                                                                                                  are explained in the text.
                                                        Connect   0.1
                                                                                 LIF dynamics Each neuronal unit has a membrane
               3000 Reservoir Units                                            potential (“activation”) that is updated instantaneously
                                                                               according to V(t) update equation in Figure 2, where t and t’
             75% Excitatory / 25% Inhibitory                                   are the current and previously updated times, i and j are
                                                                               indices for units and incoming synapses, λ is the decay rate,
                                                                               and δ(t) is the instantaneous input. Inputs come from
                                                                               synaptic potentials for reservoir and sink units, and from
                                                                               external sources for source units. A spike (action potential)
                      Source Units                                             occurs whenever membrane potential crosses threshold, V(t)
                   20 bit “0”    20 bit “1”                                    > θ with θ = 1. V(t) is immediately reset to its resting state ζ
                                                                               = 0.5 and remains in its refractory period for 1 time interval
                                                                               (denoted by Γ), during which no inputs are applied.
                                                                               Synaptic potentials i.e. weights were held constant at υ =
Figure 1: Model diagram with arrows showing connectivity,                      ±0.75 for excitatory/inhibitory synapses. This absolute
with each unit in the sending group connected to each in the                   magnitude was chosen to be somewhat under threshold so
   receiving group with a probability of 0.1. Source units                     that spikes would often but not always be triggered or
 represented input bit values 0 or 1 (half and half), and sink                 inhibited. External inputs were set to always trigger a spike.
  units represented the XOR function on consecutive input                        Each spike also triggers events over its unit’s synapses
        bits. All source and sink units were excitatory.                       that serve to propagate spikes, and also regulate propagation
                                                                               according to critical branching, and guide propagation
  The critical branching algorithm relies on temporal                          according to reinforcement signals. With synaptic strengths
ordering of spikes, and so the model was simulated                             held constant, additional variables are needed to support
asynchronously, unlike most clock-based models. Here we                        homeostatic regulation and learning. We introduced three
describe the model algorithmically, because its temporal                       into our simplified LIF model, described next.
discontinuities would complicate an analytic description.                        Critical Branching First, each synapse can be either
Pseudo-code for the algorithm is diagrammed in Figure 2.                       disabled or enabled, A = 0 or 1. When a spike occurs,
                                                                               synaptic potentials are triggered over a unit’s output
                                                                               (axonal) synapses at a delay of t+τ (τ sampled uniformly
                                                                            1306

between 1 and 2), but only for enabled synapses A = 1. The          for enabling and disabling. Very simply, the disabled output
model is initiated with all synapses disabled, and the critical     synapse with the highest trace value is chosen for potential
branching algorithm enables (and eventually disables)               enabling, and the enabled input synapse with the lowest
synapses as spikes propagate through the network.                   trace value is chosen for potential disabling. A small amount
   The objective function of the algorithm is for each unit to      of noise (ε sampled uniformly with ±0.1) is included with
be blamed for exactly one spike across its output synapses,         the trace value at each choice point. The function of this
during each of its ISIs. This objective embodies critical           noise is to implement a random choice when reinforcement
branching, i.e. exactly one spike propagated for every spike        signals are very weak or unavailable (i.e. when C ~ 0).
generated. The objective function is achieved by adding a
unit variable B that counts the number of times a unit is           Temporal XOR Classification
blamed during each of its ISIs. When a spike occurs on unit            The spiking network model presented herein is very
i, one of its enabled input (dendritic) synapses is chosen,         general in terms of network architecture and learning task.
and blame Bj for corresponding unit j is incremented. Also          Any pattern of connectivity can be specified, as long as
unit i’s blame is checked. If unit i has not been blamed since      there is an external source to drive activity, and a sink where
its last spike (Bi < 1), then one of its disabled axonal            activity can (eventually) exit the network. Any synapse can
synapses is enabled with probability 0.05. If unit j has been       receive reinforcement signals, and any schedule of direct
blamed more than once (Bj > 1), then input synapse j is             reinforcement can be applied. Currently the model handles
disabled with probability 0.05. Finally, Bi is reset to 0.          delayed reinforcement only to the extent that spike
   Over time, synapses will be enabled when units are               dynamics have memory. That is, learning will be contingent
propagating less than one spike per spike on average, and           on the past to the extent that effects of past events are
disabled when units are propagating more than one spike per         reflected in current synaptic potentials.
spike on average. The algorithm as just described is a                 Kello (2013) showed that critical branching maximizes
canonical type of critical branching. It leaves unspecified         the “fading memory” property of spike dynamics as tested
which synapses are chosen to switch between                         using the paradigm of reservoir computing (Maass,
enabled/disabled states. Kello (2013) showed that this              Natschlager, & Markram, 2002). The model had the same
choice can be random or based on a rule biased towards              three-group architecture as herein, and external inputs drove
choosing recently switched synapses. Thus our algorithm             recurrent spike dynamics in the reservoir. Memory was
has a natural free parameter for learning—instead of                tested by examining whether the reservoir spike pattern
choosing synapses randomly or based on recency, they can            during a given time interval T held information about past
be chosen on the basis of learning signals, as described next.      input patterns T-τ. The model’s computational capacity was
   Reinforcement Learning To integrate a simple learning            tested by examining whether this information could be used
algorithm with critical branching, each synapse is provided         to compute a nonlinear function of past inputs. In particular,
with a reinforcement signal R that directly rewards or              successive input patterns were treated as successive input
punishes sets of synaptic potentials associated with                bits (i.e. categorized as input pattern 0 or input pattern 1),
individual units (analogous to extracellular dopamine               and least squares regression was used to compute the XOR
released to signal reward or stimuli predictive of reward;          function on past pairs of input bits using only the current
Schultz, Dayan, & Montague, 1997). Excitatory potentials            reservoir spike pattern. XOR accuracy fell off slowly as
are rewarded when downstream spikes are signaled, and               function of τ, most slowly when spike dynamics were near
punished when not. Inhibitory potentials are conversely             their critical branching point (rather than sub- or super-
punished or rewarded. Reinforcement signals do not have             critical).
direct effects on enabling/disabling synapses. Instead, a              Reservoir computing methods were useful for
running average is stored as a reinforcement trace C on each        demonstrating the memory and computational capacities of
synapse, with the weighting of its current value set to 0.9         critical branching, but least squares regression is not a
(i.e. traces change relatively slowly over time). Traces are        neural learning mechanism. More problematically, reservoir
updated only when a reinforcement signal is present                 computing methods require stationary spike dynamics,
(indicated by Ω). Such “synaptic tags” or traces have been          which means that Kello (2013) had to disengage the critical
suggested to be vital in the long term potentiation of              branching algorithm at test, because the algorithm creates
neurons (Frey & Morris, 1997). Similar reinforcement traces         non-stationary spike dynamics. These dynamics reflect the
have been hypothesized previously as a biologically                 metastability property of critical branching, which is
plausible and effective learning mechanism for spiking              hypothesized to benefit cognitive function and needed to
networks with STDP (Izhikevich, 2007). However, previous            account for neuroscience evidence (Tognoli & Kelso, 2014).
studies did not integrate traces with critical branching or            Here we applied the temporal XOR classification test as
other homeostatic regulation mechanisms, to our                     implemented by Kello (2013), but least squares regression
knowledge.                                                          was replaced with the learning mechanism described in the
   The critical branching algorithm determines when a               previous section. The stability of learning was tested by
synapse should be chosen for enabling or disabling, and             engaging the critical branching algorithm while
reinforcement traces determine which synapses are chosen
                                                                1307

reinforcement signals were applied, and also while
reinforcement signals were subsequently removed.                                                         1
  Each input bit was converted into a sequence of 20 spikes
over source units (evenly spaced over one unit time
                                                                                                    0.9
                                                                           XOR Proportion Correct
interval), where half of the units represented bit 0 and half
bit 1. Spikes were always sequenced in the same order for
each bit, and random bit sequences were input to the model.
Reinforcement signals were applied to sink units at a delay                                         0.8
of 3-4 unit time intervals from the corresponding input bits.
For example, reinforcement signals for the XOR output 1
were applied at time interval T when the two input bits were                                        0.7
both 0 or both 1 at time intervals T−3 and T−4. Half the sink
units represented XOR output 0 and the other half                                                                                         +CB+RWD
represented XOR output 1. A reinforcement signal of +1                                              0.6
                                                                                                                                          -CB-RWD
was applied to input synapses of sink units representing the
correct output for each time interval T, and −1 for incorrect                                                                             +CB-RWD
outputs. As with other reservoir computing models,                                                  0.5
                                                                                                       0     0.2        0.4   0.6        0.8       1     1.2
performance drops off for larger delays, with delays larger                                                                                    5
than 9-10 unit time intervals nearing chance performance.                                                      Simulation Time (x10 )
                         Results                                         Figure 3: Mean XOR accuracies as a function of time,
Sixty simulations were run using the model architecture                shown for each of the three model conditions. Dashed line
shown in Figure 1. Each simulation was run for 200,000                 shows separation of the initial tuning/learning period, and
time intervals, with different random initializations of                  the subsequent period during which conditions were
parameters. Critical branching and reinforcement learning                                    distinguished.
were always engaged for the first 40,000 time intervals.
                                                                                                    3
There were three different conditions for the remaining                          10
160,000 time intervals (20 simulations per condition). For
the “+CB+Rwd” condition, both critical branching and
reinforcement learning continued to be engaged throughout.                                                                                     -CB-Rwd
                                                                                                    2
For the “+CB−Rwd” condition, critical branching continued                        10
                                                                                                                                               +CB-Rwd
to be engaged, but reinforcement learned was stopped. For
the “−CB−Rwd” condition, both mechanisms were                                                                                                  +CB+Rwd
                                                                       Power
disengaged which means that synapses were held constant.                                            1
                                                                                 10
There was no “−CB+Rwd” condition because our
reinforcement learning cannot be engaged without
engagement of critical branching. Model performance was
always tested after the initial 40,000 time intervals.                           10
                                                                                                    0
   Average XOR accuracies are shown in Figure 3, where
chance performance is 0.5. Performance can be seen to ramp
up to ~95+% in all conditions, and remain there for the
                                                                                                    -1
duration of the simulations. These results demonstrate the                       10 -4                             -3               -2              -1          0
efficacy of the reinforcement learning algorithm, despite                          10                          10              10                  10          10
ongoing variability due to homeostatic regulation. Perhaps                                         Frequency
most impressive was steady performance near the 95% level              Figure 4: Spectral analysis of fluctuations in reservoir spike
even after reinforcement signals were stopped (+CB−Rwd).                 counts per unit time, shown in log-log coordinates, with
The reason for this steady performance is that the values of                      logarithmically spaced spectral bins.
reinforcement traces remain constant so long as there are no
reinforcement signals to trigger updates. Therefore, the             We now move on to results concerning the variability and
critical branching algorithm continues to enable/disable           metastability of spike dynamics. First we examine whether
synapses in accordance with reinforcement traces laid down         the 1/f power law reported by Kello (2013) is replicated
for temporal XOR classification. It is this combination of         when reinforcement learning is integrated with critical
stable traces and synaptic switching that allows stable            branching (in the interest of space, we do not report results
learning to co-exist with homeostatic regulation and its           for the other power laws). As in the original study, the
concomitant variability.                                           number of reservoir spikes was counted per unit time
                                                                   interval, creating a time series of spike counts for each
                                                                   simulation. Spectral analysis was conducted on each time
                                                                1308

series, and spectra were averaged and plotted in Figure 4.          over time. For the model, one can also see input bits as
As can be seen, the present model replicated the 1/f power          ordered sequences over two sets of units, as well as the
law in lower frequencies that is observed while critical            relative stability of sink units compared with reservoir units.
branching is engaged. Fluctuations become random and                This comparison demonstrates that stable learning can co-
uncorrelated when critical branching is disengaged, as in the       exist with metastable dynamics.
previous model without reinforcement learning.                         Metastable dynamics are expressed more clearly through
  1/f fluctuations and other power laws have been                   auto-correlation and principle components analysis (PCA).
associated with metastability, but this hypothesized property       If spike patterns are organized locally in time, but transition
of spike dynamics was not examined in Kello (2013). Here            through different pattern spaces over time, then auto-
we test metastability using methods applied in a previous           correlation of the spike pattern time series should reveal
study of hippocampal spike dynamics (Sasaki et al., 2007).          temporally local correlations but a lack thereof at more
The authors used functional multi-neuron imaging to record          distant time delays. PCA is one way to visualize the
spontaneous network activity in rat hippocampal slice               hypothesized transitions through different pattern spaces. In
cultures. Principal component analysis revealed that spike          particular, by projecting dynamics onto the first two
patterns varied over a diverse but organized set of broadly         principle components of a given spike pattern time series,
defined pattern spaces. Moreover, each space was visited            metastability should be visualized as organized movement
only once or a small number of times. It is this transitioning      through different regions of the 2D space.
between diverse sets of patterns, without settling into any
one of them, that defines metastability.                                       10
                                                                                                          PC2 (4.18%)
                                                                                8                                       10
                                                                                6
                                                                        Time    4
                                                                                                                         0
                                                                                2                                       -10
                                                                                0                                        -10        0    10
                                                                                 0   2   4   6   8 10
                                                                                         Time                                  PC1 (4.68%)
           50
Neuron #
            0
                2          4           6        8         10
                               Time
           X1                                                                  10                                       10
                                                                                                          PC2 (11.9%)
           X0                                                                  8
                                                                                                                         5
                                                                               6
                                                                        Time
            R                                                                                                            0
           S1                                                                  4
           S0                                                                  2
                                                                                                                         -5
                0.2       0.4         0.6       0.8         1
                                                                                                                        -10
                                Time                                           0
                                                                                0    2   4 6     8   10                       -10   0   10    20
                                                                                         Time                                  PC1 (30.1%)
   Figure 5: Spike trains from spontaneous hippocampal
 recordings [top; (Sasaki et al., 2007)], and from reservoir
 neurons in our spiking neural network model (middle). At           Figure 6: Auto-correlation (left) and PCA (right) analyses of
bottom are shown example series of bit inputs, S0 & S1 that             spike pattern time series for +CB+Rwd model (top),
  drive reservoir spiking, with a sample of corresponding              hippocampal data from Sasaki et al. (middle), and the
       reservoir and output spikes, R and X0 & X1.                   –CB–Rwd model (bottom). Temporal windows were 10 s
                                                                    for hippocampal data, and 10 time intervals for model data.
   To give the reader a visual sense of both empirical and           For auto-correlation analyses of the model (top & bottom),
model spike dynamics, Figure 5 shows spike trains from the             one axis unit of time represents 20,000 time intervals.
study by Sasaki et al. (2007), along with spike trains from
model in the +CB+Rwd condition. Heterogeneity in                      Sasaki et al. (2007) quantified spike patterns as vectors of
patterning can be seen for both model and empirical data, in        windowed spike counts over hippocampal neurons recorded
terms of locally correlated patterns of clustering that change      during spontaneous activity. Successive windows created
                                                                 1309

time series of spike patterns, and the middle row of Figure 6        Science Foundation (BCS 0842784 and 1031903), and
shows the results of auto-correlation and PCA. In the time           DARPA under contract No. HR0011-09-C-0002.
period visualized, one can see three or four distinct pattern
spaces. In auto-correlation, they appear as square regions of                                References
high correlation around the diagonal. In PCA, they appear as         Beggs, J. M., & Plenz, D. (2003). Neuronal avalanches in
successive clusters of points in the 2D space.                            neocortical circuits. The Journal of Neuroscience, 23,
   We conducted the same analyses on reservoir units in our               11167-11177.
model. It is debatable whether activity driven by input bits         Bertschinger, N., & Natschlager, T. (2004). Real-time
is comparable to spontaneous hippocampal activity, but the                computation at the edge of chaos in recurrent neural
same basic results hold for the spontaneous input conditions              networks. Neural Computation, 16(7), 1413-1436.
examined in Kello (2013; not reported here). In any case,            Frey, U., & Morris, R. G. M. (1997). Synaptic tagging and
the model shows the same basic earmarks of metastability,                 long-term potentiation. Nature, 385(6616), 533-536.
but only when critical branching is engaged. When critical           Izhikevich, E. M. (2007). Solving the Distal Reward
branching is disengaged, spike dynamics fluctuate randomly                Problem through Linkage of STDP and Dopamine
within one region of the pattern space, as evidenced by high              Signaling. Cerebral Cortex, 17(10), 2443-2452.
auto-correlations throughout time, and one cluster of                Kello, C. T. (2013). Critical branching neural networks.
random movements in PCA space.                                            Psychological Review, 120(1), 230-254.
                                                                     Kello, C. T., Beltz, B. C., Holden, J. G., & Van Orden, G.
                         Discussion                                       C. (2007). The emergent coordination of cognitive
The model presented herein provides one solution to the                   function. Journal of experimental psychology. General,
puzzle of how learning can be stable in the face of ongoing               136(4), 551-568.
variability and metastability. The model is cast as a                Kello, C. T., Brown, G. D. A., Ferrer-i-Cancho, R., Holden,
biologically plausible spiking neural network, but the                    J. G., Linkenkaer-Hansen, K., Rhodes, T., & Van
principles and mechanisms may be applied to complex                       Orden, G. C. (2010). Scaling laws in cognitive sciences.
adaptive networks in general. The model’s intrinsic, power                Trends in Cognitive Sciences, 14(5), 223-232.
law variability derives from homeostatic regulation that             Kello, C. T., & Van Orden, G. C. (2009). Soft-assembly of
supports and enhances memory and computation. The                         sensorimotor      function.    Nonlinear     Dynamics,
model is designed so that regulatory enabling and disabling               Psychology, and Life Sciences, 13(1), 57-78.
of synapses does not interfere with changes to reinforcement         Kitano, H. (2004). Biological robustness. Nat Rev Genet,
traces driven by learning. Instead, these synaptic traces act             5(11), 826-837.
as a stable memory for learning. This memory carves out a            Maass, W., Natschlager, T., & Markram, H. (2002). Real-
broad portion of synaptic space in which performance is                   Time Computing Without Stable States: A New
maintained. Critical branching serves to drive synaptic                   Framework for Neural Computation Based on
changes within this space. As a result, spike patterns form               Perturbations. Neural Computation, 14(11), 2531-2560.
and reform over time, but always within the space carved             Markram, H., Lübke, J., Frotscher, M., & Sakmann, B.
out by learning.                                                          (1997). Regulation of Synaptic Efficacy by
   The present study shows how learning can be integrated                 Coincidence of Postsynaptic APs and EPSPs. Science,
with homeostatic regulation and metastability, but further                275(5297), 213-215.
work is needed to investigate how the latter might enhance           Poil, S.-S., van Ooyen, A., & Linkenkaer-Hansen, K.
the former. Based on the current results, it is safe to say that          (2008). Avalanche dynamics of human brain
learning becomes highly redundant as a result of critical                 oscillations: Relation to critical branching processes
branching. That is, the same XOR function was                             and temporal correlations. Human Brain Mapping,
accomplished using many different sets of neurons and                     29(7), 770-777.
spike patterns. Thus the variability from homeostatic                Sasaki, T., Matsuki, N., & Ikegaya, Y. (2007). Metastability
regulation may enhance the robustness of learning (Kitano,                of Active CA3 Networks. The Journal of Neuroscience,
2004). Previous studies also have associated metastability                27(3), 517-528.
with the flexibility and context-sensitivity of cognitive            Schultz, W., Dayan, P., & Montague, P. R. (1997). A neural
function (Kello & Van Orden, 2009). The demonstrated                      substrate of prediction and reward. Science, 275(5306),
redundancy may provide a way for a given process or                       1593-1599.
representation to emerge differently in different contexts,          Tognoli, E., & Kelso, J. A. S. (2014). The Metastable Brain.
yet achieve the same underlying function. This is one of a                Neuron, 81(1), 35-48.
number of different future directions.                               Turrigiano, G. G., & Nelson, S. B. (2000). Hebb and
                                                                          homeostasis in neuronal plasticity. Current Opinion in
                   Acknowledgments                                        Neurobiology, 10(3), 358-364.
This research was supported in part by awards from the
National Academies Keck Futures Initiative, the National
                                                                 1310

