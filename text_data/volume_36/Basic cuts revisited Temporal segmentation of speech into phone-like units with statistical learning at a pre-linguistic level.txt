UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical
learning at a pre-linguistic level
Permalink
https://escholarship.org/uc/item/86g8f0rz
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Author
Rasanen, Okko
Publication Date
2014-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

    Basic cuts revisited: Temporal segmentation of speech into phone-like units with
                                     statistical learning at a pre-linguistic level
                                             Okko Räsänen (okko.rasanen@aalto.fi)
                                   Department of Signal Processing and Acoustics, Aalto University
                                                PO Box 13000, 00076 Aalto, FINLAND
                              Abstract                                 word boundaries while the high-probability regions form
                                                                       representational units (Saffran et al., 1996). This strategy is
   Considerable effort has been put to understand how infants          valid as long as the TPs within words are higher than the
   may utilize statistical regularities of speech in early word        TPs across word boundaries. However, the infant’s access to
   segmentation. Some studies suggest that infants are able to         linguistic units such as phones or syllables and their
   discover word boundaries at the points of high
   unpredictability across subsequent linguistic units such as
                                                                       statistics cannot be taken for granted. It is still unclear
   phonemes or syllables. Meanwhile, the possible role of the          whether early adaptation to phonetic units drives lexical
   statistical regularities in the temporal organization of the        learning (c.f., NLM-e theory by Kuhl et al., 2008) or
   speech at a pre-linguistic acoustic level has not been widely       whether early lexical learning actually precedes, or at least
   addressed. The current work examines how the short-term             parallels, the acquisition of sub-word representation of
   temporal predictability of the acoustic speech signal               spoken language (e.g., Werker & Curtin, 2005). The “sub-
   correlates with linguistically motivated phone-, syllable-, and     word units –first” approach is challenged by the fact that the
   word-level units. The results indicate that the points of low
   predictability correlate mainly with the boundaries between         bottom-up organization of speech signal into temporally and
   phone-like segments. This suggests that the same statistical        categorically discrete units is far from trivial. Learning a
   learning mechanisms hypothesized to operate at the word             phonetic or syllabic representation of the spoken language
   level can also aid in temporal organization of the speech           includes both the segmentation problem (division of the
   stream into phone-like temporal segments before knowing the         signal in time) and the categorization problem (assigning
   phonemic or syllabic units of the language.                         context-, talker-, and speaking style-dependent acoustic
   Keywords: distributional learning; language acquisition;            observations into a correct number of linguistic categories).
   phone segmentation; speech segmentation; statistical learning       Importantly, infants do not have access to any ground truth
                                                                       in either of the two tasks while learning the native language,
                           Introduction                                suggesting that some speech-external factors such as
Segmentation of continuous speech into linguistically                  feedback from lexical level or social interaction are required
relevant units is essential for successful language acquisition        for successful learning.
(LA). Segmentation can take place at a number of levels, as               Still, it seems that even the basic problem of segmenting
the speech can be linguistically characterized in terms of             speech into sub-word units has been largely overlooked in
units such as phones, syllables, and words, and with the               the existing LA research. For example, it is unclear how
latter always consisting of the former.                                well natural co-articulated speech can be segmented into
   In the early LA research, infants’ ability to segment words         sub-word units before learning the phonetic or lexical units
from speech has received a large amount of attention as the            of the language, and whether infants actually do such
words are the main functional units of the language,                   segmentation. Possibly the most concrete reference to early
standing for entities, events, actions, and states of the              sub-word segmentation in the existing literature is the
surrounding world. In the word segmentation studies, one of            Kuhl’s concept of basic cuts: a perceptual mechanism that
the major findings is that the infants can use statistical             provides an initial low-level chunking of the speech stream
regularities in the speech input in order to discover                  into primitive phone-like units and which then gradually
boundaries between words (Saffran, Aslin & Newport,                    improves towards native language phone system through
1996). Also, these statistical learning mechanisms do not              language exposure (Kuhl, 2004, and references therein).
seem to be specific to words or even language faculty but              Segmentation into syllabic units is also central to many
operate across many levels of representation and perceptual            theories of LA (e.g., Jusczyk, 1993) although explicit and
domains (see, e.g., Romberg & Saffran, 2010, for a recent              well-controlled studies on the segmentation process itself
review).                                                               are few.
   Importantly, a large body of the existing work on                      In the speech engineering community, both phone- and
statistical word learning assumes that the infants are capable         syllable-level segmentation have been widely studied. The
of representing speech input in terms of linguistically                general finding is that the spectral changes (or “jumps”) in
relevant units such as phones or syllables. Given the                  speech are good candidates for phone boundaries as they
representational units, the infants are supposedly tracking            correlate with the changes in articulator positions (e.g.,
transitional probabilities (TPs) between these units across            Almpanidis & Kotropulos, 2008; Esposito & Aversano,
time and use low-probability transitions as indications for            2005; ten Bosch & Cranen, 2007; Scharenborg et al., 2007).
                                                                       On the other hand, it is known that syllabic segmentation
                                                                   2817

can be achieved by detecting minima from the smoothed              between plosive closures and bursts (e.g., [k] + [kcl]) since
temporal envelope of speech signals (see Villing, Ward &           they can be considered as articulatory distinct segments.
Timoney, 2006, for a performance overview). It is likely              In the simulations, the standard TIMIT NIST training set
that the auditory system achieves “basic cuts” based on an         (462 talkers, 4620 utterances, both male and female talkers)
innate perceptual mechanism that detects sufficiently large        was used to learn the TPs between the acoustic events (see
spectral changes in the input and/or uses the temporal             Methods). Then the NIST core test set containing 192
envelope to parse speech into rhythmic units.                      previously unseen utterances from 24 talkers was used to
   However, there is another open possibility that it is not       evaluate the segmentation performance. Overall duration of
the magnitude of the spectral or envelope change as such           the data was approx. 4 hours (177080 phone segments) for
that drives the segmentation processes, but maybe the short-       training and 10 minutes (7333 phones) for testing.
term statistical regularities of the acoustic speech signal
enables segmentation of the input into perceptually relevant                                 Methods
units. As it is already known that the distributional learning     The basic acoustic unit analyzed in the current work consists
plays a role in the word segmentation (Saffran et al., 1996)       of spectral features that are computed from fixed-size short-
and in the categorization of native speech sounds (e.g.,           term (millisecond scale) segments of speech. These features
Maye, Werker & Gerken, 2002; Kuhl, 2004), it is of interest        are then quantized into Q possible signal states in an
whether similar learning mechanism could aid the                   unsupervised manner and TPs between the states are used as
organization of the speech into syllabic or phonetic units in      a model for acoustic predictability of the speech (Figure 1).
time. If this would be the case, then only a single learning       Finally, points of low TP are extracted as candidate segment
mechanism operating on different levels of representation          boundaries. As the TP analysis is carried out in an abstract
would be needed to explain both early low-level sub-word           state space, the model is agnostic to the exact magnitude of
organization, word-level segmentation (Saffran et al., 1996),      the spectral changes but the changes are simply reflected in
and many other aspect of perceptual processing associated          the state changes across time.
with statistical learning (see Romberg & Saffran, 2010 and            Importantly, the obtained signal states do not correspond
references therein).                                               to phonetic categories of the language as the bottom-up
   In order to investigate the sub-word segmentation from          clustering of spectral features into talker- and context-
statistical learning point of view, the current paper presents     independent phonemic units is not possible without
results from simulations where the transition probability          additional information such as lexical knowledge or
analysis is carried out at the level of millisecond-scale          articulatory constraints (e.g., Feldman, Griffiths & Morgan,
acoustic features. The hypothesis is that the points of low        2009; see Räsänen, 2012, for a review). The quantization
TP in time have some correspondence to the boundaries              simply acts as a conversion from the continuous
between linguistically motivated units, and therefore we           multivariate input into a discrete categorical sequence
compare the model output to manual transcription of the            suitable for standard TP analysis. However, the clustering
signals at the phone-, syllable-, and word-levels.                 used to create the quantization codebook will necessarily
                                                                   introduce a rough “perceptual re-organization by language
                             Data                                  exposure” as the cluster boundaries will reflect the
TIMIT corpus (Garofolo et al., 1993) containing American           distributional characteristics of the speech spectra.
English continuous speech from multiple talkers and                   Note that the current work does not imply that infants
dialects was chosen for the experiments due to its rich and        would analyze acoustic signal in terms of Q different
balanced phonetic content and due to the availability of           discrete units or categories (as it is unlikely that infant brain
high-quality phone- and word-level transcriptions of the           would represent a discrete probability distribution for TPs
utterances. Since TIMIT is recorded in a controlled noise-         between discrete syllables; cf., Saffran et al., 1996). Instead,
free environment, the focus is purely on the analysis of           the goal of the pre-processing and quantization is to simply
speech structure without any interfering effects from              enable the analysis of statistical regularities in the signal
background noise or, e.g., multiple overlapping talkers.           using the simplest possible mathematical form similarly to
   As the original TIMIT only contains phone- and word-            the discussion on “tracking of TPs” in the context of
level transcriptions, syllable annotation was generated from       perceptual learning.
the phonetic transcription using the tsylb2-algorithm
(Fisher, 1996) that uses the phonological rules described in
Kahn (1976) for the transformation. Phonetic alphabet used
in tsylb2 was matched to the TIMIT in a similar fashion to
the study of Villing, Ward & Timoney (2006). The syllabic
transformation was carried out using the tsylb2 parameters
associated with “ordinary conversational speech”. The
phone level boundaries were used as they are described in          Figure 1: A schematic view of the TP-based segmentation
the original TIMIT format. This includes the boundaries            process. VQ stands for vector quantization.
                                                               2818

Pre-processing of speech
                                                                   amplitude
                                                                                     0.05
   One of the challenges with the acoustic analysis is that the                                0
relevant units are not known in advance. This means that the                   −0.05
raw speech signal has to be represented using non-linguistic                                    0    0.2   0.4   0.6   0.8        1     1.2   1.4   1.6     1.8
features that capture similar time-frequency information                                      12
                                                                                coefficient
                                                                                              10
than what the auditory system is capable of extracting. Here,                                  8
                                                                                               6
standard Mel-frequency cepstral coefficients (MFCCs) were                                      4
                                                                                               2
used as they compactly represent the essential spectral                                         0    0.2   0.4   0.6   0.8        1     1.2   1.4   1.6     1.8
content of speech with low-dimensional feature vectors and                                    30
                                                                                state index
approximate the spectral resolution of human hearing.                                         20
   MFCCs are obtained by first computing the power                                            10
spectrum of the speech signal using fast Fourier transform                                      0    0.2   0.4   0.6   0.8        1     1.2   1.4   1.6     1.8
(FFT) in a sliding window of length 20 ms and a step size of
10 ms. For each window position, the obtained FFT-                                            0.2
                                                                               p(t)
spectrum is filtered through a Mel-scale filterbank with 26                                   0.1
triangular bandpass filters in order to approximate the                                        0
                                                                                                0    0.2   0.4   0.6   0.8        1     1.2   1.4   1.6     1.8
frequency resolution of the auditory system. Finally, the                                                                    time (s)
logarithm of the Mel-spectrum is taken and discrete cosine
transform is applied to the log-Mel spectrum to obtain the        Figure 2: An example of the processing stages for an
MFCC coefficients (Figure 2, second panel). The first 12          utterance “His shoulder felt as if it were broken”. Top: The
coefficients c1…c12 and the c0 coefficient corresponding to       original speech waveform. Second panel: MFCC spectrum
the signal energy were chosen for further processing as they      computed from the speech signal. Third panel:
are sufficient for describing the spectral envelope of speech.    Corresponding VQ-state indices. Bottom: Transition
Mean and variance of each cepstral coefficient was z-score        probability (TP) curve. Red vertical lines show the minima,
normalized across each utterance before further processing.       a.k.a. the boundary hypotheses, extracted from the TPs.
   In order to perform TP analysis on the spectrum, MFCCs
were quantized into a discrete state space by first clustering    and that the human auditory system also analyzes signal
10000 randomly chosen MFCC vectors of the training data           content on the same time scale (Räsänen & Laine, 2013).
into a codebook of Q clusters with the standard k-means              During the segmentation stage, probabilities of the
algorithm. Then all MFCC vectors were assigned to the             transitions in a previously unseen signal X’ = {w1, w2, …}
nearest cluster centroid in terms of Euclidean distance and       were simply evaluated according to Eq. (1), leading to a
replaced by the corresponding state index. As a result, the       probability curve as a function of time (Figure 2, bottom).
speech signal of L frames becomes represented as a                The final set of low probability points (LPPs) were
sequence of discrete states X = {w1, w2, …, wL}, w ∈ [1, Q],      extracted from the probabilities by using a simple valley
t ∈ [0, L], with one state occurring every 10 ms (Figure 2,       detection procedure. A segment boundary was hypothesized
third panel; see also Räsänen, 2011).                             to each local minimum that was preceded by a TP-value
                                                                  larger by at least δ units, where δ is a user set parameter.
Transition probability analysis                                   The use of a fixed global threshold for minima detection
During training, the TPs between subsequent states were           was also studied and it was found to lead to very similar
computed for a number of lags k = {1, 2, 3, …, K}, where a        results than the local minima detection procedure. However,
lag k transition means a state-pair {wt-k, wt} with any           the fixed threshold requires additional rules to deal with
undefined elements wt-k+1…wt-1 in between. The probability        multiple neighboring points that are all below the threshold
of the signal X as function of time was defined as                in order to avoid unnecessary over-segmentation.
                               K
          p(t − ⎣k / 2⎦ | X ) = ∑ p(wt | wt−k )      (1)        Evaluation
                              k =1
where ⎣ denotes downward rounding to an integer. The             The overall segmentation quality was evaluated in terms of
                                                                  the overall agreement between the LPPs and the reference
p(wt|wt-k) were simply calculated from the transition
                                                                  annotation, quantified by the F-value in Eq. (2) that is
frequencies f(wt|wt-k)/f(wt-k) counted from the training data.
€                                                                 obtained as the harmonic mean of the precision in Eq. (3)
As defined in Eq. (1), the statistics of the signal were
                                                                  and recall in Eq. (4).
modeled as a mixture of TPs at different temporal distances
across the current time frame of analysis, corresponding to                 F = 2 * PRC * RCL/(PRC + RCL)             (2)
an approximation of a higher-order Markov chain but                                                 PRC = Nhit/Nhypo                                      (3)
making it learnable from finite data. This allowed the model
to capture the temporal dependencies that extend beyond the                 RCL =Nhit/Nref                            (4)
neighboring states as the acoustic dependencies in speech         €
                                                                   In the equations, Nhit is the number of correctly detected
are known to extend up to approximately 250-ms in time             segment boundaries, Nhypo is the total number of boundary
                                                              2819

             1                                                    1                                                        1
                                    phones                                                                                                           phones
            0.8                     syllables                    0.8                                                      0.8                        syllables
                                    words                                                                                                            words
                                                     precision
                                                                               phn
                                                                                                                F−value
            0.6                                                  0.6                                                      0.6
   recall                                                                      syl
            0.4                                                  0.4           wrd                                        0.4
            0.2                                                  0.2                                                      0.2
             0                                                    0                                                        0
                  1      2      3        4                                 1          2      3     4                                1     2      3       4
                        threshold                                                    threshold                                           threshold
          0.73                                              0.74                                                       0.75
                                                            0.72                                                          0.7
          0.72
F−value                                           F−value                                                    F−value
                                                                 0.7                                                   0.65
                                                            0.68                                                          0.6                            Q=8
          0.71                                                                                                                                           Q = 16
                                                            0.66                                                       0.55                              Q = 32
                                                                                                                                                         Q = 64
            0.7                                             0.64                                                          0.5       2       3        4        5
                  2     4      6    8        10                        4   6 8      16    32    64     128                        10      10      10        10
                      max lag (frames)                                           codebook size Q                                number of phone tokens trained
Figure 3: Results from the simulations. Top row: phone, syllable, and word segmentation results with the best parameter
combination of K = 10 and Q = 8 as a function of the detection threshold δ. Y-axis shows the recall (left), precision (center),
and F-value (right). The thick blue solid lines, the red dashed lines, and the black dash-dotted lines correspond to the results
calculated with respect to the annotated phone, syllable, and word boundaries, respectively. Thin lines show the
corresponding baseline performances from the random boundary generation. Standard deviations (SDs) across multiple runs
are shown with horizontal bars. SDs of the random baselines are not shown for the sake of visual clarity but are of the same
scale as the SDs of the other results. Bottom row: Phone segmentation F-value as a function of the maximum number of lags
K in Eq. (1) with fixed Q = 8 (left), F-value as function of the quantization codebook size with fixed K = 10 (center), and F-
value as a function of the training data length (measured in phone tokens) for different codebook sizes (right).
hypotheses generated by the model, and Nref is the total                                    The main observation is that the short-term acoustic
number of reference boundaries in the annotation.                                        dependencies are mainly associated with phone-level
  For a reference phone boundary to be considered as                                     structure, TP minima detection leading to notably above
correctly detected, the algorithm was required to produce a                              chance-level phone segmentation accuracy. In contrast, the
hypothesized boundary within ±20 ms of the reference                                     syllable- and word-level performances are much worse.
boundary as this roughly corresponds to the variability in                               Recall for all three levels of representation is approximately
the phonetic annotation across multiple annotators (Kvale,                               equal for all thresholds. On the other hand, precision for
1993). Since the syllable- and word boundaries are a subset                              phones is always superior to syllables, while precision for
of the phone boundaries in the annotation, the allowed                                   syllables is always superior to words. This suggests that the
deviation for syllables and words was also set to ±20 ms.                                syllable boundaries are simply a subset of the detected
  Chance-level performance was measured for all test                                     phone boundaries without any specific threshold level
conditions by generating the same number of boundaries for                               (depth of minima) being more associated with syllabic
each utterance than what was produced by the actual                                      structure in comparison to the phones.
algorithm and randomizing the final locations of the                                        In overall, the best phone segmentation result is F = 0.73,
boundaries along the utterance duration.                                                 corresponding to approximately 70% of boundaries
                                                                                         correctly detected with a precision of 74%. This is a
                               Results                                                   surprisingly good performance level considering the lack of
Figure 3 shows the results from the TIMIT core test set                                  specially tailored signal processing solutions typically used
segmentation. Top row shows the performance as a function                                to fine-tune the phone segmentation performance. As a
of the detection threshold δ using quantization codebook                                 reference, the typical performances of dedicated phone
size of Q = 8 and a maximum TP analysis lag of K = 10                                    segmentation algorithms are in the range of 0.74–0.76 for
(100 ms). In the plots, the variability and the associated SDs                           the F-value on the same TIMIT corpus (e.g., Almpanidis &
in the results are caused by the random initialization in the                            Kotropulos, 2008; Esposito & Aversano, 2005; Scharenborg
generation of the quantization codebook.                                                 et al., 2007).
                                                                                   2820

   Regarding the statistical significance, it is evident that the     the best performance, although the size of the codebook
mean phone segmentation performance is far above the                  may impose an implicit tradeoff between change detection
chance level for the majority of the threshold values. As for         and statistical segmentation.
the syllables, the performance is significantly above the
chance-level (p < 0.01) for thresholds δ < 2.65. In the case                         Discussion and conclusions
of word boundaries, the performance is above chance level             The current work shows that there is clear temporal
(p < 0.01) for δ < 2.71.                                              statistical structure associated with speech that helps
   As for the model parameters, there are four main factors           segmentation of the input into phone-like units before any
that can affect the results: the size of the codebook, MFCC           linguistic knowledge is acquired. However, the statistical
window size and step size, and the maximum lag K up to                approach does not exceed the traditional spectral change
which TPs are measured. The size of the MFCC window                   detection in performance, especially when dedicated phone
was found be optimal around 12–30 ms with a step size of              segmentation algorithms are considered. Actually, the
10 ms. No qualitative changes in the relative performance of          spectral “jumps” and unpredictability of the spectrum can be
different linguistic units were observed when these two               seen as the two sides of a same coin where one always has a
parameters were adjusted. This finding is expected as the             consequence to another. Therefore the current study does
speech signal is quasi-stationary within the given time scale         not argue that the “basic cuts” in the auditory system would
and the FFT step in MFCC computation assumes signal                   be necessarily based on statistical predictability of the
stationarity within the analysis window.                              signal. Instead, the current work simply shows that there is a
   Bottom left panel in Figure 3 shows the phone                      probabilistic interpretation to the low-level temporal
segmentation performance as a function of the maximum                 organization of the speech signal and a simple statistical
temporal lag K up to which TPs were measured. The                     learning mechanism has the potential to adapt to this
performance seems to saturate after the maximum lag of K =            structure in order to parse the signal into units that roughly
6, confirming that there is useful structure beyond                   correspond to linguistically defined phones. Note that the
neighboring frames. As for the codebook sizes, the best               statistical learning here refers broadly to the use of recurring
results are obtained with surprisingly small codebooks of             similarities in the signal and not to the explicit analysis of
size Q = 6 and 8 (Figure 3, bottom middle). However, the              TPs between abstract discrete states. Instead, the TP
performance is relatively good even for the largest                   analysis should be seen as a methodological tool to probe
codebook sizes tested. The syllable- and word-level                   the existence or absence of such statistical structure.
performances (not shown) follow similar saturating trend as              Although it is questionable whether a learning-based
a function of lags as the phone level but with notable lower          mechanism to segmentation is more plausible than a simple
overall performance. As for the codebook size, the syllable-          hard-wired spectral change detector in terms of human
or word-level performances do not change significantly                auditory processing, the current model is attractive due to its
when the codebook size is adjusted (also not shown                    similarity to the behavioral findings on TP-based word-level
separately). This is in contrast with the phone level where           segmentation (Saffran et al., 1996; see Romberg & Saffran,
larger codebooks tend to decrease the overall agreement               2010, for a review) and also to the existing computational
between the algorithm output and the manually annotated               models on statistical learning at the acoustic level (see
reference.                                                            Räsänen, 2012, for a review). For example, if the global TP
   Finally, the right panel at the bottom of Figure 3 shows           model in Eq. (1) is partitioned into multiple different models
the F-value as a function of training data length for different       with their own local TP statistics (as in Räsänen, 2011), or
codebook sizes. The result shows that the finer the spectral          gains support from cross-situational visual cues (see
resolution of the model, the more there is improvement with           Räsänen, 2012), the TP analysis leads to the learning of
more learning. Interestingly, it seems that only one or two           words instead of phones. Short-term statistical dependencies
utterances are sufficient for reasonable performance with             of speech also explain the how and why the auditory system
very small codebooks. This suggests that part of the phone            combines signal input over time in order to form coherent
segmentation with small codebooks is achieved due to the              auditory percepts (Räsänen & Laine, 2013), while TP
spectral change detection, realized as a transition from a            analysis at the level of prosodic features reveals that points
state to another. Since there are more transitions from a state       of low predictability in these features correlate with
to itself than to other states with small codebooks (see also         perception of stress in speech (Kakouros & Räsänen,
Figure 2), it may be the case that the majority of the non-self       accepted for publication). All this evidence suggests that the
transitions have zero probability at an early stage, leading to       same basic computational mechanisms operating on signal-
a segment boundary. Still, there is significant improvement           level regularities has explanatory power over both sub-word
from longer training times even for Q = 8. For larger                 and word level segmentation and on suprasegmental
codebooks, the effect of learning is more evident as the              perception of speech. The main difference is only the time-
simple state change detection in these cases would lead to            scale of the statistical analysis, acoustic features that are
large amounts of over-segmentation. In general, these                 analyzed, and the potential access to additional constraints
results confirm that the segmentation is not only based on            such as cross-situational cues in other perceptual modalities.
change detection but properly learned TPs are required for
                                                                  2821

   As for the syllable level, it seems that the syllabic            Garofolo, J. S., Lamel, L. F., Fisher, W. M., Fiscus, J. G.,
segmentation is not straightforward with the spectral                 Pallett, D. S., Dahlgren, N. L., & Zue, V. (1993). TIMIT
features. It seems as if the syllable boundaries are simply a         acoustic-phonetic continuous speech corpus. Linguistic
random subset of the phone boundaries in the current                  Data Consortium, University of Pennsylvania, 1993.
simulations. No studied parameter combination (temporal or          Jusczyk, P. W. (1993). From general to language-specific
spectral) was able to provide clear indication of increased           capacities: the WRAPSA model of how speech perception
precision at the syllable level in comparison to the phone            develops. Journal of Phonetics, 21, 3–28.
level. However, this is partially expected as the syllabic          Kahn, D. (1976). Syllable based generalizations in English
structure mainly provides a rhythmic frame to the                     phonology. Ph.D. dissertation, Department of Linguistics
phonetic/phonemic content of speech and is primarily                  and Philosophy, MIT, Cambridge, 1976.
conveyed by the energy envelope of the speech signal, not           Kakouros, S. & Räsänen O. (accepted for publication).
by the spectral content studied in the current work.                  Statistical Unpredictability of F0 Trajectories as a Cue to
   Finally, a note regarding the overall quantitative                 Sentence Stress. Proc. 36th Annual Conference of the
segmentation performance is in place. Due to the                      Cognitive Science Society, Quebec, Canada.
uncertainties associated with the annotation process (see           Kuhl, P. (2004). Early language acquisition: cracking the
Kvale, 1993), the reference annotation should not be taken            speech code. Nature Reviews Neuroscience, 5, 831–843.
as the ultimate ground truth for a perfect division of the          Kuhl, P., Conboy, B. T., Coffey-Corina, S., Padden, D.,
speech signal into linguistically defined units. This is even         Rivera-Gaxiola, M., & Nelson, T. (2008). Phonetic
more emphasized in the syllabic reference that is based on a          learning as a pathway to language: new data and native
conversion from the phonetic transcription to syllabic units          language magnet theory expanded (NLM-e). Phil. Trans.
using a set of linguistic rules (Kahn, 1976), not direct              R. Soc. B., 363, 797–1000.
annotation of syllabic units based on subjective perception.        Kvale, K. (1993). Segmentation and Labelling of Speech.
   In the future work, it would be beneficial to investigate          Doctoral Thesis, The Norwegian Institute of Technology,
combination of the current model with a statistical model of          Department of Telecommunications.
categorical and lexical learning from real speech. As the           Maye, J., Werker, J. F., & Gerken, L. (2002). Infant
quantization of the acoustic input could be gradually                 sensitivity to distributional information can affect
improved with distributional learning of the spectral                 phonetic discrimination. Cognition, 82, B101–B111.
properties related to actual lexical contrasts, this could also     Romberg, A. R., & Saffran, J. R. (2010). Statistical learning
lead to improvement in the temporal segmentation. In this             and language acquisition. Wiley Interdisciplinary Review
way, the entire spectrotemporal parsing of the speech into            of Cognitive Science, 1, 906–914.
linguistically relevant units would gradually improve with          Räsänen, O. (2011). A computational model of word
experience, as already suggested by Kuhl (2004). Also,                segmentation from continuous speech using transitional
given a suitable speech corpus, it would be beneficial to             probabilities of atomic acoustic events. Cognition, 120,
replicate the current study using speech from only one or             149–176.
two talkers and infant directed speech to see how the               Räsänen, O. (2012). Computational modeling of phonetic
complexity of the data affects the results.                           and lexical learning in early language acquisition: existing
                                                                      models and future directions. Speech Communication, 54,
                    Acknowledgments                                   975–997.
  This research was funded by the Academy of Finland. The           Räsänen, O., & Laine, U. (2013). Time-frequency
 author thanks all reviewers for their very useful comments.          integration characteristics of hearing are optimized for
                                                                      perception of speech-like acoustic patterns. Journal of the
                                                                      Acoustical Society of America, 134, 407–419.
                        References                                  Saffran, J., Aslin, R. N., & Newport, E. L. (1996). Statistical
Almpanidis, G., & Kotropoulos, C. (2008). Phonemic                    learning of 8-month-old infants. Science, 274, 1926–1928.
   segmentation using the generalized Gamma distribution            Scharenborg, O., Ernestus, M., & Wan, V. (2007).
   and small sample Bayesian information criterion. Speech            Segmentation of speech: Child’s play? Proc.
   Communication, 50, 38–55.                                          Interspeech’07, Antwerp, Belgium, pp. 1953–1956.
Esposito, A., & Aversano, G. (2005). Text independent               ten Bosch, L., & Cranen, B. (2007). A computational model
   methods for speech segmentation. In G. Chollet et al.              for unsupervised word discovery. Proc. Interspeech’07,
   (Eds.), Lecture Notes in Computer Science: Nonlinear               Antwerp, Belgium, pp. 1481–1484.
   Speech Modeling. Springer Verlag, Berlin, pp. 261–290.           Villing, R., Ward, T., & Timoney, J. (2006). Performance
Feldman, N., Griffiths, T., & Morgan, J., (2009). Learning            limits for envelope based automatic syllable
   phonetic categories by learning a lexicon. Proc. 31st              segmentation. Proceedings of ISSC’2006, Dublin, Ireland,
   Annual Conference of the Cognitive Science Society,                June 28–30, pp. 521–526.
   Austin, Texas, pp. 2208–2213.                                    Werker, J. F., & Curtin, S. (2005). PRIMIR: A
Fisher, M. W. (1996). tsylb2. National Institute of Standards         developmental framework of infant speech processing.
   and      Technology.       Available      online      from:        Language Learning and Development, 1, 197–234.
   http://www.nist.gov/speech/tools.
                                                                2822

