UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
An Embodied Real-Time Model of Language-Guided Incremental Visual Search
Permalink
https://escholarship.org/uc/item/14g4t9vn
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Scheutz, Matthias
Krause, Evan
Sadeghi, Sepideh
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

  An Embodied Real-Time Model of Language-Guided Incremental Visual Search
                                    Matthias Scheutz and Evan Krause and Sepideh Sadeghi
                                        {matthias.scheutz,evan.krause,sepideh.sadeghi}@tufts.edu
                     Human-Robot Interaction Laboratory, Department of Computer Science, Tufts University
                                                        Medford, MA 02155, USA
                              Abstract                                 Brick & Scheutz, 2007; Chiu & Spivey, 2012; Krause et al.,
   A recent body of work has demonstrated that the incremen-           2013). However, computational models are often necessary
   tal presentations of linguistic search cues can speed up visual     to show that conclusions drawn about the processing archi-
   processing in conjunctive visual search. In this paper, we in-      tecture based on experimental evidence or theory alone might
   vestigate different processing configurations using a real-time
   embodied computational model and demonstrate that, differ-          not be warranted.
   ent from previous hypotheses, the same incremental process-            In this paper, we present an embodied real-time model of
   ing configuration can explain all experimental conditions.          interactive incremental vision and natural language process-
   Keywords: Incremental interactive processing, embodied              ing that can explain previous experimental findings in a novel
   real-time model, natural language and vision interaction
                                                                       way by showing that divergent results found in different ex-
                           Introduction                                perimental conditions by Spivey et al. (2001) might not be
A large body of work in cognitive science has demonstrated             due to differences in processing configurations (such as serial
that much of human information processing is interactive               vs. parallel), but rather the specific effects of these experi-
and incremental: “interactive” means that information is ex-           mental manipulations on the same processing configuration.
changed and shared among multiple processes; “incremen-                   We start by reviewing some of the empirical evidence for
tal” means that the received information is integrated as it be-       the hypothesis that natural language can incrementally con-
comes available. Hence, interactive incremental processing             strain vision processing and describe, in particular, the exper-
modules can incorporate information from other modules as              iments in (Spivey et al., 2001) which we use for our model
constraints in their own processing and thus potentially finish        simulations. Next, we introduce the model architecture and
their processing sooner.                                               provide a more detailed description of its vision system which
   A well-studied case of such interactive incremental pro-            is critical for the replication of the human data. We then spec-
cessing is the interaction between visual and natural language         ify the simulation setup, which used the same human stimuli
processes. Converging evidence from studies using, in par-             as Spivey et al. (2001), and report the results from extensive
ticular, the “visual words paradigm” demonstrate that vision           simulation experiments with different configurations of the
and natural language processing in humans are highly interac-          processing system. The analysis of the simulation data con-
tive and incremental, being able to utilize constraints from the       firms many of the expected properties, but also shows that the
other modality to reduce processing effort and improve pro-            same configuration can explain different experimental con-
cessing performance (Eberhard, Spivey-Knowlton, Sedivy, &              ditions that have been assumed to be the result of different
Tanenhaus, 1995). For example, a visual search process at-             processing configurations. This point is further elaborated in
tempting to find a target object in a visual scene such as a           the subsequent discussion section and summarized in the con-
particular pen on a cluttered desk can be modulated through            clusion which also points to future work.
natural language instructions that provide additional informa-
tion about the object (e.g., “small black”), leading to a more                        Background and Motivation
targeted, faster search (Spivey, Tyler, Eberhard, & Tanenhaus,         It has long been hypothesized that early stages of bottom-
2001; Krause, Cantrell, Potapova, Zillich, & Scheutz, 2013).           up visual processing are highly parallel as single-feature vi-
Conversely, visual processing of a scene can influence natu-           sual search is not affected by the number of co-present dis-
ral language processing by helping to disambiguate otherwise           tractors, while later stages must include a “serial bottleneck”
ambiguous referential phrases such as the syntactic ambigu-            since conjunctive visual search (assumed to tap into later pro-
ity due to different possible prepositional attachments in “put        cessing stages) takes longer as the number of distractors in-
the black pen on the book on the table” where the black pen            creases (Wolfe, 2007). While various stimuli properties can
could be put either on the table or on the book that is on the ta-     affect search speed in conjunctive search, Spivey and col-
ble (Eberhard et al., 1995; Scheutz, Eberhard, & Andronache,           leagues demonstrated in a series of experiments that the in-
2004; Brick & Scheutz, 2007).                                          cremental presentation of linguistic search cues can reduce
   While various theoretically motivated hypotheses have               the effect of distractors in the visual search process (Spivey
been proposed about an underlying processing architecture              et al., 2001; Reali, Spivey, Tyler, & Terranova, 2006; Chiu &
that could enable such incremental natural language and vi-            Spivey, 2011, 2012). They hypothesized that the incremen-
sion interaction and information integration, only a few com-          tal presentation of search cues (which is natural in spoken
putational models actually demonstrate possible computa-               language) enforced a serialization of the search process, al-
tional mechanisms (Scheutz et al., 2004; Hamker, 2004;                 lowing search results based on the first cue to be utilized in
                                                                   1365

                                                                                                                                      Other System Components
the second search, thus shortening its duration. In contrast,
no such incremental processing was hypothesized in experi-                                                                      Vision Component Interface
mental conditions where both search dimensions were simul-
taneously presented.                                                                                                              Search
                                                                                                                                    SearchManager
                                                                                                                                   Search   Manager
                                                                                                                                            Manager
                                                                                    Available Saliency Operators
                                                                                                                                   Image
                                                                                                                                    Image  Processor
                                                                                                                                           Processor
   Spivey et al. (2001) used a standard conjunctive visual                               Image
                                                                                          Image  Processor
                                                                                                 Processor
                                                                                          Saliency Operator
                                                                                                                 Advertisements     Saliency Operator
search paradigm where the presence or absence of a colored
                                                                                    Available Detectors
bar (red or green) in a particular orientation (horizontal or ver-                           Detector
                                                                                              Detector
                                                                                                                 Advertisements         Detector
                                                                                              Detector
tical) has to be detected. For the task presentation, they con-
sidered two linguistic conditions: in the “audio first” (A1st)                          Available Trackers                               Tracker
                                                                                                                                                             Tracked
                                                                                                                                                              Objects
condition the auditory target cue precedes the onset of the                                   Tracker
                                                                                              Tracker
                                                                                               Tracker
visual stimulus, while in the the concurrent “audio-vision”
condition (A/V) the auditory cue and visual scene have the                     Figure 1: The processing architecture of the vision system.
same onset. Consistent with the visual search literature, the
results showed that the slopes of the best fitting lines relating
response times (RTs) to stimulus set size for both target ab-                              The Embodied Real-Time Model
sent and target present cases were positive, with larger slopes
for target absent compared to target present cases. Critically,              Over the last decade we have developed a complex integrated
the slopes in the A/V concurrent conditions were smaller than                embodied cognitive architecture called “DIARC” (Scheutz,
the slopes in the A1st conditions. The effect persisted when                 Schermerhorn, Kramer, & Anderson, 2007) which has been
the cue order was altered and when stimuli were presented                    shown to exhibit several qualitative features of human-like
visually in the A/V condition (instead of through a preceding                natural language and vision processing (e.g., incremental ref-
natural language instruction).                                               erence resolution, Scheutz et al., 2004; incremental infor-
   Exploring alternative explanations, Reali et al. (2006)                   mation integration, Brick & Scheutz, 2007; dialogue-based
replicated the findings from Spivey et al. (2001) with mixed                 constraints on speech recognition, Veale, Briggs, & Scheutz,
A/V and A1st trials in random order. They also mixed color                   2013; visual search constrained by linguistic expressions,
first conjunction searches with orientation first conjunction                Krause et al., 2013; and others). Here we will use DIARC for
searches to show that the search improvement in the A/V con-                 the first time to investigate potential quantitative models of
dition is not due to subjects’ listening strategies, nor does                human performance that differ only in their processing con-
it disappear as the complexity of the utterances increases.                  figuration in order to evaluate hypotheses about processing
Moreover, slopes are still smaller for A/V concurrent con-                   modes in linguistic-guided visual search. We depart from the
ditions than in the A1st conditions in triple visual searches.               requirement that our computational model be able to use the
                                                                             same real-world linguistic and visual input as in Spivey et
   Finally, Chiu and Spivey (2011) argued that the visual
                                                                             al. (2001) for two reasons: (1) to be able to model human
search utilizes a combination of serial and parallel strategies
                                                                             performance of real-time incremental interactive information
as opposed to purely parallel or serial strategies. By manipu-
                                                                             processing and (2) to avoid complications about subtle tim-
lating the stimulus onset asynchrony (SOA) between the end
                                                                             ing effects that can arise with discrete-event simulation mod-
of the first and the onset of the second cue (for 0, 200, 400,
                                                                             els that only have simulated paralellism.2 Since the focus of
and 600ms SOAs), they found significantly shallower slopes
                                                                             the model is on configurations of visual search, we skip the
for the A/V compared to the A1st conditions when the SOA
                                                                             overview of the natural language subsystem which has been
was equal to 400 or 600 ms. However, when the SOA was 0
                                                                             described in detail elsewhere (Cantrell et al., 2010; Krause et
or 200 ms, they did not observe significantly shallower slopes
                                                                             al., 2013) and focus on the visual subsystem (only describing
for the A/V compared to A1st conditions, indicating that the
                                                                             those parts of the natural language subsystem necessary for
search improvement in the A/V condition is dependent on the
                                                                             understanding the model configurations and runs).
SOA which acts as a “buffer” for the end of first cue search
process.                                                                        The vision system is consistent with empirically grounded
                                                                             views on guided visual search in humans (e.g., Wolfe, 2007)
   The overarching question posed by this whole line of re-
                                                                             and consists of three main components: saliency operators
search then is why incremental processing should only oc-
                                                                             that compute different types of saliency maps, object detec-
cur in the A1st condition and not also in the A/V condition.
                                                                             tors that can use various features (including information from
To answer this question, we investigated different processing
                                                                             saliency maps, textures, shapes, etc.) to detect objects, and
configurations of a computational model, described next, that
                                                                             object trackers that can track previously detected objects over
can perform the task from Spivey et al. (2001) to find the con-
figurations that most closely matched the human data in both                     2 An additional reason, not directly relevant to the effort in this
A1st and A/V conditions.1                                                    paper, is our aim to run those models on robots in the context of
                                                                             human-robot interaction scenarios where robots have to respect hu-
    1 We will restrict our modeling efforts here on Spivey et al. (2001)     man timing and modes of information processing in natural language
for lack of space, but note that the model generalizes to different          dialogues (Scheutz et al., 2007; Cantrell, Scheutz, Schermerhorn, &
variations of the experiments.                                               Wu, 2010).
                                                                         1366

                                          Saliency op.S1
                                                                          background pixels from object pixels (i.e., modeling back-
             PI                                                           ground pixels as a particular RGB value), and Euclidean clus-
                                          Saliency op.S2
                                          Partial results in              tering to grow an “object pixel cluster” (from a single pixel)
                                          Incremental proc.
                                                                          corresponding to the most salient image region. Once a can-
             PN                                                           didate object has been segmented, it is checked against the
                                                                          individual saliency maps in order to confirm that it has all nec-
                                                                          essary salient features. Thus, the most salient objects are de-
             SI                                                           tected first and are immediately available as “target objects”
                                                                          in the search, while less salient objects follow later. In tar-
                                                                          get detection search it is thus possible to terminate the visual
             SN                                                           search early (compared to a search requiring a count of all
                                                                          target objects, say).
              t0 = 0             tPI tPN      tSI tSN
                                                                                               Simulation Results
Figure 2: Four saliency operator configurations for conjunc-              The goal of the model simulations was to find the model con-
tive visual search and their relative times to completion.                figuration that most closely matched the data from Experi-
                                                                          ment 1 in Spivey et al. (2001), i.e., the differences in response
                                                                          times over stimuli sets with an increasing number of items
time (a function not required for the current paper).3                    in the target absence vs. target presence conditions in both
                                                                          the audio first vs. audio-vision conditions. We thus defined
                                                                          four different processing configurations based on two pro-
Saliency operators. Saliency operators are massively par-
                                                                          cessing dimensions: incremental (I) vs. non-incremental (N),
allel, computationally cheap bottom-up processes that oper-
                                                                          and serial (S) vs. parallel (P). We used the four sets of 32
ate directly on regions of the input image, thus constituting
                                                                          image stimuli from (Spivey et al., 2001) which contain 5, 10,
the first visual processing stage. They are used to extract ba-
                                                                          15, and 20 vertical/horizontal and red/green bars, respectively
sic visual features such as color, size, orientation, and motion
                                                                          (see Figure 3).
(analogous to the system in Itti & Koch, 2001). The result of
applying a saliency operator to a region in the input image is
a (partial) saliency map with values between 0 (no salience)
and 1 (maximal salience) for each pixel. Multiple saliency
operators can be configured to perform computations either in
parallel (P) or serially (S) (i.e., one map at a time, or multiple
maps simultaneously even though they might take different
times to compute). Different from other bottom-up saliency
models where computations for individual maps are modular,
it is possible in both cases for saliency operators to interact by
using values from existing saliency maps (generated by other
operators) to modulate the saliency computation in their own
map – this incremental (I) mode of operation is contrasted
with the non-incremental (N) mode of only performing cal-
culations based on the input image. Hence, saliency operators
can interact in four ways based on their configuration in as-
cending order of processing efficiency: PI, PN, SI, SN (see               Figure 3: Examples of stimulus images for each of the four
Figure 2).                                                                item set sizes.
                                                                             We ran 100 replications of each visual stimulus with color
Object detection. Objects are detected by segmenting re-
                                                                          terms followed by orientation terms for the four combinations
gions in the input image based on their saliency as deter-
                                                                          of color (“red” vs. “green”) and orientation (“vertical” vs.
mined in (possibly combined) saliency maps. Different from
                                                                          “horizontal”) for a total of 100 · 32 · 4 = 12800 runs for each
saliency computations, this is a serial process where regions
                                                                          of the four model configurations (i.e., over 50000 runs total)
with the highest saliency are considered first. Segmentation
                                                                          in the “audio first” condition. 4 For each run, we measured
is performed using a simple background model to distinguish
                                                                              4 For the large evaluation, we did not run the complete archi-
    3 Note that the vision system does not realize biologically plau-     tecture but only the vision subsystem because we were only inter-
sible computations “all the way down to individual neurons”, but          ested in the response time from the onset of the visual stimulus in
allows for different sequencing and interactions of processing mod-       this condition. Otherwise, the model can perform the whole exper-
ules, which is necessary for investigating human processing config-       iment in the same setup as human would (with real-time audio and
urations.                                                                 video). Note that replications are important because computational
                                                                      1367

the processing duration for each feature (color and orienta-              We performed a 2x2x2x4 ANOVA with target condition
tion) as well as the time required for information integration         (present vs. absent), integration mode (incremental vs. non-
and decision-making in the object detector (the vision system          incremental), processing mode (parallel vs. serial), and item
was especially instrumented with time measurement code for             set size (5, 10, 15, or 20 items) as independent, and total
that purpose). Since we were not interested in examining per-          time (to processing completion from visual stimulus onset)
formance errors (such as false starts and wrong outputs), we           as dependent variables. We found highly significant main
set parameters in the vision system in a way that the model            effects (all F(1, 12784) > 100, p < .001) on all four inde-
had perfect performance in all runs.                                   pendent variables as expected: the absence of the target,
   Instead of running separate simulations for the audio-              non-incremental processing, serial processing, and increase
vision conditions, we were able to reuse the data from the             in item set size all lead to longer RTs. In addition, we found
audio-first condition. Recall that in the audio-first condition,       significant two-way interactions (all F(1, 12784) > 100, p <
both visual search features have been already determined be-           .001 except for the first with F(1, 12784) > 5, p = .024):
fore the onset of the image and thus the visual search can             between processing mode and target presence/absence in-
either be carried out in parallel or serially. In contrast, in the     dicating that the difference in RTs between target absence
audio-vision condition the visual features are given sequen-           and presence are increasing when processing is serial; be-
tially while the target image is already present. Since the            tween item set size and incremental processing indicating
vision system always completes the color processing of any             that as item size increases the advantage of incremental pro-
stimuli in less time than it takes to pronounce the correspond-        cessing increases too; between item set size and target pres-
ing color terms, processing in the audio-vision condition is           ence/absence indicating that item set size increases in the
always be serial regardless of the model configuration (serial         target absence condition increase the RTs massively while
or parallel). Therefore, instead of running separate simula-           RTs in the target presence conditions show only a moder-
tions for the “audio-vision” conditions, we were able to reuse         ate increase; between incremental processing and target pres-
the data from the audio-first condition by taking the duration         ence/absence indicating that the advantage of incremental
from the onset of the image (which is also that of the first           processing in the target presence condition is greater than in
linguistic cue) and the onset of the second linguistic cue, and        the target absence condition. The last three two-way inter-
then adding the model’s response time measured from the on-            actions are explained by a significant three-way interaction
set of the second cue until the decision (target or no-target) is      (F(1, 12784) > 100, p < .001) between item set size, incre-
reached.                                                               mental processing and target presence/absence which corrob-
                                                                       orates the human data: increases in item set size lead to larger
                                                                       increases in RTs in the non-incremental compared to the in-
                                                                       cremental processing configuration thus closing the initially
                                                                       wider gap between the target presence vs. target absence con-
                                                                       ditions relative to the overall differences between incremental
                                                                       and non-incremental processing (see Figure 4).
                                                                          Figure 5 then shows the best fitting lines for the four model
                                                                       configurations and Table 1 shows the intercepts and slopes for
                                                                       the models compared to the linear fits from the human data in
                                                                       the A1st vs. A/V conditions in (Spivey et al., 2001). The dif-
                                                                       ferent intercepts and slopes are indicative of both differences
                                                                       in processing style but also differences in the duration of in-
                                                                       dividual subcomponent processes (e.g., the time it takes to
                                                                       compute a saliency map). Overall, the fit lines confirm the
                                                                       results from the previous analysis (in part shown in Figure 4)
                                                                       that parallel processing is faster than serial, that incremental
                                                                       processing is much faster than non-incremental processing,
                                                                       and that the target-presence conditions scale much better over
                                                                       item set size compared to the target-absence conditions, all of
                                                                       which is in line with the human data.
                                                                          To be able to directly compare these linear fits while taking
Figure 4: The three-way interaction between data size, incre-          into account the differences in processing times in individual
mental processing and target presence/absence conditions.              human and model subsystems (as we were not attempting to
                                                                       model the details of human vision and natural language pro-
processes, even though specified by deterministic programs, have       cessing, but rather the overall processing configuration), we
stochastic run-times given the many concurrently running processes
in the Ubuntu Linux operating system on the employed quadcore PC       use a relational comparison. Following Spivey et al. (2001),
with Intel i7-3820 CPU at 3.60GHz.                                     we consider the ratio of slopes in the human target-absent to
                                                                   1368

                                                                               Instr./             A/V                        A1st
                                                                               Mode            incr. non-incr.            incr. non-incr.
                                                                                serial    2.492462 40.87944          4.633394 75.99336
                                                                              parallel    4.036554 36.40101               N/A          N/A
                                                                            Table 2: Model proximity values for the six model conditions
                                                                            (see text for explanation).
                                                                            numeric results in both configurations, it is clear that incre-
                                                                            mental processing reflects the human data much better com-
                                                                            pared to non-incremental processing.
                                                                                                     Discussion
                                                                            The fact that both instruction conditions seem to utilize
                                                                            the same processing configuration is quite surprising at first
                                                                            glance, because it seems that at the very least in the A1st
                                                                            condition the parallelism of the feature search should be ex-
                                                                            ploitable. And, indeed, the results in Table 1 confirm that par-
                                                                            allel incremental search is the fastest in this condition. Spivey
         Figure 5: The best fit lines for all four models.                  et al. (2001) argue that “in the auditory-first condition, the
                                                                            search process may employ a conjunction template to find the
       Cond.        target present               target absent              target, thus forcing a serial-like process akin to sequentially
                  intercept        slope     intercept       slope          comparing each object with the target template. However, in
                                                                            the A/V-concurrent condition, it appears that the incremental
        I-P         48.409         0.173        42.690       2.058
                                                                            nature of the speech input allows the search process to be-
        I-S         60.549         0.264        58.397       1.941
                                                                            gin when only a single feature of the target identity has been
        N-P        202.056         0.045      195.379        4.797
                                                                            heard [which then] proceeds in a more parallel fashion (with
        N-S        215.726         0.039      211.851        4.706
                                                                            the second-mentioned target feature being used to find the
       A1st        830.000 19.800             911.000 31.400                target amidst an attended subset).” Based on our modeling
        A/V      1539.000          7.700    1628.000 22.700                 results, we would like to propose an alternative explanation
                                                                            that is consistent with the experimental differences observed
Table 1: Summary of the regression lines from four sets of                  by Spivey et al. (2001) and does not require the stipulation of
model simulations and the data from Experiment 1 (A/V vs.                   a different processing configuration.
A1st) in Spivey et al. (2001) (see text for explanation).                      The explanation rests on the assumption that with none of
                                                                            the visual stimuli it took humans longer to process the color
                                                                            cue than the time it took to pronounce the color word in the
target-present conditions Sh,p /Sh,a and now compare it to the              A/V condition, see Figure 6 for an illustration which shows
same ratio for the model Sm,p /Sm,a , which – in the ideal case             the time course for processing the two visual cues in the A/V
– should be identical.5 Table 2 shows the calculated “model                 and A1st conditions for parallel and serial configurations (cue
                      S ·Sm,a
proximity values” Sh,p h,a ·Sm,p
                                 (closer to 1 is better) for each of the    integration time is absorbed in the second cue processing for
four models in the “audio first” (A1st) and the two models in               simplicity). Note that there is no chance to exploit parallelism
the “audio-vision” (A/V) conditions.                                        at any point in the A/V condition precisely because the vision
   As can be seen from the model proximity values, the serial               system will have finished processing the color of the items
incremental models fare best in both instruction conditions.                before the orientation cue occurs (although those processes
While it is premature to draw conclusions about the differ-                 take increasingly longer as the set size increases, hence there
ence between sequential and parallel search given the close                 will be a set size where visual processing will exceed the du-
                                                                            ration of the spoken color cue). Thus, both serial and parallel
    5 Note that comparing slope ratios between target absent and tar-       processing configurations require the same overall process-
get present conditions addresses the data comparison problem raised         ing time in the A/V condition, different from the A1st condi-
by the fact that intercepts and slopes are different. If we had at-
tempted an individual comparison of best fitting lines from model           tions, where parallel and serial yield different results. More-
and human data, we would have had to scale the model intercept Im           over, if the vision system used a parallel configuration in the
to the human level Ih by the factor λ = Im /Ih and then adjust the          A1st condition, then the slopes in that condition would be the
model slope Sm accordingly: Sm · λ. Note that scaling factors cancel
out in a ratio comparison, hence no scaling is necessary for compar-        same as in the A/V. However, Spivey et al. (2001) found a
ing ratios of human and model data.                                         steeper slope in the A1st condition suggesting that the sys-
                                                                        1369

                                              color processing                                                                        Acknowledgments
                                              orientation processing
                                                                                                                   This work was in part supported by NSF grant IIS-1111323
                                                                                                                   and ONR grants #N00014-11-1-0289 and #N00014-14-1-
   Time
                                                                                                                   0149 to the first author. Thanks to Mike Spivey and his group
                                                                                                                   for providing the stimuli and for helpful discussions.
                                                                                                                                           References
      orientation word presentation
                                                                                                                   Brick, T., & Scheutz, M. (2007). Incremental natural lan-
                                                                                                                      guage processing for HRI. In Proceedings of the second
                                                                                                                      acm ieee international conference on human-robot inter-
                                                                                                                      action (pp. 263–270). Washington D.C..
                   color word presentation
                                                                                                                   Cantrell, R., Scheutz, M., Schermerhorn, P., & Wu, X.
                                                                                                                      (2010). Robust spoken instruction understanding for HRI.
                                                                                                                      In Proceedings of the 2010 human-robot interaction con-
                                                                                                                      ference (p. 275-282).
                                                                                                                   Chiu, E., & Spivey, M. (2011). Linguistic mediation of visual
                                             A/V S A1st P          A/V S A1st P   A/V S A1st P   A/V S A1st P
                                                                                                                      search: Effects of speech timing and display. In European
   Set size
                                                    5                    10           15             20               perspectives on cognitive science.
                                                                                                                   Chiu, E., & Spivey, M. (2012). The role of preview and
                                                                                                                      incremental delivery on visual search. In Proceedings of
Figure 6: The influence on fixed color word duration on the
                                                                                                                      the 34th annual conference of the cognitive science society.
overall RTs in the A/V compared to the A1st serial and paral-
                                                                                                                   Eberhard, K., Spivey-Knowlton, M., Sedivy, J., & Tanenhaus,
lel conditions (see text for details).
                                                                                                                      M. (1995). Eye movements as a window into real-time spo-
                                                                                                                      ken language comprehension in natural contexts. Journal
tem was configured serially. While it is possible that process-                                                       of Psycholinguistic Research, 24, 409–436.
ing is configured in parallel in the A/V condition, this seems                                                     Hamker, F. (2004). A dynamic model of how feature cues
implausible given that the parallelism has no effect in that                                                          guide spatial attention. In Vision research (Vol. 44, pp.
condition and that the system was not configured in a paral-                                                          501–521).
lel fashion in the A1st condition where the parallelism could                                                      Itti, L., & Koch, C. (2001). Computational modeling of visual
have been exploited. Additional evidence comes from our                                                               attention. In Nature reviews, neuroscience (pp. 194–203).
model simulations where serial (incremental) configurations                                                        Krause, E., Cantrell, R., Potapova, E., Zillich, M., & Scheutz,
have the best fit to the human data. Notice that the above                                                            M. (2013). Incrementally biasing visual search using natu-
argument does not rely on the incremental/non-incremental                                                             ral language input. In Proceedings of aamas (pp. 31–38).
distinction, hence the argument holds for both incremental                                                         Reali, F., Spivey, M., Tyler, M., & Terranova, J. (2006). In-
and non-incremental configurations.                                                                                   efficient conjunction search made efficient by concurrent
                                                                                                                      spoken delivery of target identity. Perception and Psy-
                                                                       Conclusion                                     chophysics, 68(6), 959–974.
                                                                                                                   Scheutz, M., Eberhard, K., & Andronache, V. (2004). A real-
We introduced a real-time embodied computational model                                                                time robotic model of human reference resolution using vi-
that was used to investigate different processing configu-                                                            sual constraints. Connection Science, 16(3), 145–167.
rations of a cognitive system that can perform conjunc-                                                            Scheutz, M., Schermerhorn, P., Kramer, J., & Anderson, D.
tive visual searches based on spoken natural language cues.                                                           (2007). First steps toward natural human-like HRI. Au-
We replicated the empirical findings from Experiment 1 in                                                             tonomous Robots, 22(4), 411–423.
(Spivey et al., 2001) that show a significant difference be-                                                       Spivey, M., Tyler, M., Eberhard, K., & Tanenhaus, M. (2001).
tween audio-visual concurrent instruction compared to audio-                                                          Linguistically mediated visual search. Psychological Sci-
first instruction, which has been hypothesized to be due to a                                                         ence, 12, 282–286.
difference in processing configuration of the visual system                                                        Veale, R., Briggs, G., & Scheutz, M. (2013). Linking cog-
in the two conditions. Based on our modeling results, we                                                              nitive tokens to biological signals: Dialogue context im-
conclude that the same processing configuration is responsi-                                                          proves neural speech recognizer performance. In Proceed-
ble for both conditions and that the differences in the exper-                                                        ings of the 35th annual conference of the cognitive science
imental data in the two conditions are fully explained by the                                                         society.
way the experimental manipulations impose processing con-                                                          Wolfe, J. (2007). Guided search 4.0: Current progress with a
straints on the system. Future work will extend the model                                                             model of visual search. In W. Gray (Ed.), Integrated mod-
to the different published variations of the experiment in an                                                         els of cognitive systems (pp. 99–119). New York: Oxford
attempt to show that the same underlying processing config-                                                           University Press.
uration can explain the results in all of these conditions.
                                                                                                                1370

