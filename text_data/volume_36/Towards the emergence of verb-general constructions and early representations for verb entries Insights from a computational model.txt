UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Towards the emergence of verb-general constructions and early representations for verb
entries: Insights from a computational model
Permalink
https://escholarship.org/uc/item/5zt8x64j
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Gaspers, Judith
Foltz, Anouschka
Cimiano, Philipp
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

   Towards the emergence of verb-general constructions and early representations
                          for verb entries: Insights from a computational model
                                         Judith Gaspers (jgaspers@cit-ec.uni-bielefeld.de)
                                        Semantic Computing Group, CITEC, Bielefeld University
                                       Anouschka Foltz (anouschka.foltz@uni-bielefeld.de)
                                       Emergentist Semantics Group, CITEC, Bielefeld University
                                        Philipp Cimiano (cimiano@cit-ec.uni-bielefeld.de)
                                        Semantic Computing Group, CITEC, Bielefeld University
                              Abstract                                   tic information alone, iii) how information about possible ref-
                                                                         erents and co-occurrence statistics might be stored with verb
   Recent findings suggest that i) children can build initial verb
   entries on the basis of syntactic information alone without any       entries, and iv) how this information is updated incrementally
   additional information provided by a visual context, and ii) that     over time, thus allowing for learning of verb meanings across
   the early representation of verbs encompasses statistical infor-      situations.
   mation on the co-occurrence of these verbs with their potential
   meanings/referents, enabling children to infer verb meanings          In this paper we present a computational model that acquires
   under referential uncertainty. In this paper we present a com-        verb-general constructions under referential uncertainty in or-
   putational model that acquires verb-general constructions un-         der to shed light on the potential learning mechanisms in-
   der referential uncertainty. The model stores linguistic knowl-
   edge in line with construction grammar in the form of an in-          volved in early verb acquisition. Specifically, we extend
   terrelated network of constructions. Learning proceeds in line        a previous usage-based computational model that can learn
   with usage-based theories in an item-based fashion. Computa-          verb-specific constructions (Gaspers & Cimiano, in press) to
   tional results show that the model can account for the above-
   mentioned findings: The model produced patterns similar to            also acquire verb-general constructions, which can explain
   those observed in these studies. Our findings hence shed light        the empirical findings of the above mentioned studies. In
   on the potential mechanisms involved in the emergence of              line with construction grammar (Goldberg & Suttle, 2010),
   early verb entries and verb-general constructions as well as the
   representation and refinement of verb entries.                        the model represents linguistic knowledge in the form of an
                                                                         interrelated network of constructions containing both item-
                          Introduction                                   based knowledge and generalizations. Linguistic knowledge
                                                                         evolves over time in an item-based or bottom-up fashion.
Unlike nouns, verbs describe actions that involve a number
                                                                         Abstraction from the observed input occurs as proposed in
of participants who play certain (thematic) roles in the event.
                                                                         usage-based approaches (Behrens, 2009). The existing model
Hence, sentence structure, i.e. syntactic frames, may serve as
                                                                         first establishes mappings for lexical units. Only once some
a “zoom lens” to guide the child’s attention to relevant aspects
                                                                         lexical knowledge has been learned with sufficient confi-
of verb meaning, in particular to thematic relations during
                                                                         dence, verb-specific constructions are learned in a bottom-up
verb learning (e.g. Gleitman & Fisher, 2005). In line with this
                                                                         fashion by bootstrapping on the mappings for single lexical
assumption, Arunachalam & Waxman (2010) showed that 27-
                                                                         units. Thus, learning occurs through a generalization process
month-old children can create an initial verb entry based on
                                                                         which searches for syntactic variation corresponding to se-
information from the syntactic context and without any visual
                                                                         mantic variation.
information, and retrieve this information when encountering
                                                                         The extended model presented here builds on these basic
the verb later on.
                                                                         principles: Verb-general constructions are learned bottom-
Scott & Fisher (2012) further provide evidence that 2.5-year-
                                                                         up from verb-specific constructions only once verb-specific
old children are able to use cross-situational statistics to infer
                                                                         knowledge has been derived with sufficient confidence.
verb meanings under referential uncertainty (Quine, 1960).
                                                                         Again, generalization occurs in an item-based fashion (al-
This mechanism is also called cross-situational learning (e.g.
                                                                         beit with respect to more complex structures/mappings) by
Siskind, 1996) and has typically been investigated in the con-
                                                                         searching for variation at the linguistic layer which has corre-
text of mapping nouns/single words to objects (e.g. Smith &
                                                                         sponding variation at the meaning layer. We present empiri-
Yu, 2008). Scott & Fisher (2012) showed that children can
                                                                         cal results replicating Arunachalam & Waxman’s (2010) and
abstract across different actors and objects, suggesting that
                                                                         Scott & Fisher’s (2012) studies with the model. Depending
they can attach information about possible referents to novel
                                                                         on its “age”, the model behaves very similarly to the children
verb entries along with their co-occurrence statistics and re-
                                                                         in these studies. The results suggest possible learning mech-
fine these entries over time.
                                                                         anisms implicated in the early acquisition and representation
However, what remains unclear is i) how verb-general con-
                                                                         of verbs and verb-general constructions.
structions emerge and how they are represented, ii) how they
can guide attention to establish verb entries based on syntac-
                                                                     2252

                The computational model                             syntactic pattern. These slots can be filled with elements con-
In the following, we will first briefly describe the existing       tained in specific groupings. This layer also encodes the as-
model that can learn verb-specific constructions (Gaspers &         sociated semantic frames. For instance, in Fig. 1 a syntactic
Cimiano, in press). Then, we present extensions to this model       construction is represented as a path p which expresses a pat-
that allow it to acquire verb-general constructions.                tern “SE1 sees SE2 ”, where SE1 and SE2 represent syntactic
                                                                    slots in the pattern, which can be filled with groupings of ele-
Background                                                          ments such as “Mia” and “Tim” in the case of SE1 or “pizza”
The existing computational model relies on an interrelated          and “candy” in the case of SE2 . The semantic frame associ-
network of constructions, which are acquired incrementally          ated with the pattern is see(AGENT,THEME). The mapping
on the basis of observed input. It encodes a construction           layer contains networks representing construction-specific
grammar as a set of nodes and (weighted) edges. The model’s         argument mappings between syntactic patterns and seman-
input is a natural language (NL) sentence, i.e. a sequence of       tic frames together with mappings of the syntactic arguments
words, coupled with a symbolic description of the visual con-       to semantic arguments. For example, in Fig. 1 an individual
text in the form of meaning representations (MR) expressed          mapping network captures the correspondences between SE1
by way of predicate logic. Each action mri ∈ MR consists            and AGENT as well as SE2 and THEME.
of a predicate ξ along with a set of thematic relations. It is      The network contains nodes of two types: Nodes representing
important to note that the model can learn under referential        linguistic entities include i) simple phrases including noun
uncertainty, i.e. given a situational context with several dif-     phrases, e.g. “Tim” or “the cat”, ii) syntactic patterns repre-
ferent actions out of which at most one corresponds to the          sented as paths with slots, e.g. “SE1 sees SE2 ”, and iii) syn-
utterance. The model input thus consists of two (temporally         tactic slots of constructions represented as sets of elements
paired) channels: a language channel and a channel with in-         containing all the simple phrases that can fill the slot, e.g. SE1
formation about the visual context. The learning process and        = [Mia → mia, Tim → tim]. Nodes representing semantic
an example of a verb-specific construction stored in the net-       entities include i) simple semantic referents, i.e. individuals
work are illustrated in Fig. 1.                                     such as tim, ii) semantic frames, e.g. see(AGENT,THEME),
 The learned network consists of two subnetworks, one repre-        and iii) argument slots of frames, e.g. AGENT.
                                                                    Our computational model applies Hebbian-style cross-
                                                                    situational learning to establish connections between linguis-
                  Figure 1: Learning process.                       tic and semantic nodes that are activated concurrently in a
                                                                    certain linguistic and visual context. The model thus learns
                                                                    correspondences between linguistic and semantic nodes, i.e.
                                                                    the semantics of linguistic constructions. In all cases, we ap-
                                                                    ply associative networks to capture co-occurrence frequen-
                                                                    cies (cf. Rojas, 1993).
                                                                    During learning, input examples are processed one-by-one,
                                                                    causing immediate changes in the network structure. Learn-
                                                                    ing is roughly divided into two learning steps: i) update of
                                                                    the lexical layer, where connections between lexical units and
                                                                    semantic referents are established, and ii) update of the con-
                                                                    struction layer, where the model mainly attempts to merge
                                                                    paths, and thus generalizes. The model implements a usage-
                                                                    based approach to generalize over observed examples and
                                                                    paths contained in the network. In particular, it exploits
                                                                    type variations that have a semantic implication to general-
                                                                    ize observed NL sentences and (partially generalized) pat-
                                                                    terns to more abstract patterns. Consider the following ex-
                                                                    ample: A learner hears “Mia eats” and “Peter eats” in the
senting lexical and one representing syntactic constructions.       above-mentioned visual context. To learn across situations,
The syntactic subnetwork builds on the lexical subnetwork           the model would use its knowledge that the linguistic phrase
and is divided into two sublayers: a slot-and-frame (S&F)           “Mia” refers to the semantic entity mia and that the phrase
pattern layer and a mapping layer. The lexical subnetwork           “Peter” refers to the semantic entity peter to bootstrap that
encodes simple phrases, i.e. (short sequences of) words, as         the type variation in the sentences’ first position (“Mia” vs.
nodes together with the associated semantic referents, e.g. the     “Peter”) reflects the meaning difference in the AGENT role
word “Tim” and the corresponding semantic referent tim in           of eat. The model would use its knowledge to acquire the
Fig 1. The S&F pattern layer represents syntactic construc-         more general pattern shown in (1), where SE1 = [Mia → mia,
tions as sequences of nodes that together constitute a path.        Peter → peter].
Paths can contain variable nodes that represent slots in the
                                                                2253

                   Syntactic pattern   SE1 eats                        sets of lexical elements. An additional associative network
             (1)   Semantic frame      eat(AGENT)
                   Mapping             SE1 → AGENT                     AV captures the co-occurrence of the lexical units contained
                                                                       in these sets with syntactic patterns. AV is included into the
   Given an input NL sentence, the model finds a meaning by            network structure and models associations between specific
searching the network for corresponding paths/lexical nodes            verbs and syntactic frames.
and ranking all possible meanings based on the weights stored          An additional third learning step is executed while process-
in the associative networks. An NL sentence is parsed by first         ing input examples: The S&F layer is updated to yield verb-
replacing units contained in groupings expressing syntactic            general constructions. In particular, the model searches for
slots (e.g. Mia) by the set (e.g. SE1 ). Then, the model de-           paths which show minimal variation in the surface structure
termines the semantic frame that corresponds to the path in            and where exchanging elements at the position that varies
the graph, if such a path exists. Finally, the model retrieves         yields a corresponding change in the associated meaning with
the meanings of lexical units at positions of syntactic slots          respect to predicates. To illustrate the underlying intuition,
from the lexical network. It uses the construction’s mapping,          consider the two verb-specific constructions in (4) and (5).
i.e. the mapping specifying that SE1 is the AGENT, to in-              These two examples can be merged into the verb-general con-
sert these meanings into the corresponding argument slots in           struction shown in (6), assuming that “see” and “take” mean
the semantic frame. For details, please see Gaspers & Cimi-            see and take, respectively.
ano (in press). The important aspect to bear in mind is that
the same Hebbian-style learning approach is used to train all                   Syntactic pattern    SE1 sees SE2
                                                                           (4)  Semantic frame       see(AGENT, T HEME)
layers of the network, in particular to learn the correspon-                    Mapping              SE1 → AGENT, SE2 → THEME
dences between linguistic and semantic units/nodes. Also
note that the model incorporates a disambiguation bias (Mer-                    Syntactic pattern    SE1 takes SE2
riman & Bowman, 1989) in the lexical subnetwork: Weights                   (5)  Semantic frame       take(AGENT, T HEME)
                                                                                Mapping              SE1 → AGENT, SE2 → THEME
for new connections are initialized such that new lexical units
are preferably associated with referents which have not yet                     Syntactic pattern    SE1 V E1 SE2
been associated with other lexical units.                                  (6)  Semantic frame       ACTION(AGENT,THEME)
                                                                                Mapping              SE1 → AGENT, SE2 → THEME
Extension to learning verb-general constructions
                                                                       More precisely, we define that two paths encoding syntactic
The model’s extension relies on the same learning and rep-             constructions are mergeable if both differ in at most one posi-
resentational devices in order to learn verb-general construc-         tion. Moreover, both paths must already have a learned (verb-
tions that abstract form particular verbs and thus represent a         specific or verb-general) meaning (see Gaspers & Cimiano,
generic signature of verbs, e.g. transitive, intransitive, ditran-     in press, for a definition) that includes the same mapping be-
sitive etc. verbs. It also detects cross-situational type variance     tween sets of elements and thematic relations. Each element
and uses Hebbian-style reinforcement of edges to connect lin-          at a variable position must either have a learned meaning
guistic and semantic entities that are observed “together”. To         which corresponds to the predicate of the associated semantic
illustrate how the model learns verb-general constructions,            frame or to a set of elements mapping to an ACTION predi-
consider the verb-specific construction given above in exam-           cate. The latter possibility allows the model to directly merge
ple (1) and the additional construction in (2), where SE1 =            verb-specific with verb-general paths. All mergeable paths –
[Mia → mia, Peter → peter]. From these verb-specific con-              if any – are then merged into a single path and elements at
structions, we would like to acquire the verb-general con-             a variable position are replaced by a node that expresses a
struction in (3), where SE1 = [Mia → mia, Peter → peter],              set of elements that maps to the ACTION predicate in an as-
VE1 = [eats → eat, sleeps → sleep], and V E1 maps to the               sociated frame. Note that merging paths involves summing
ACTION predicate.                                                      up weights in the corresponding associative networks (see
                   Syntactic pattern   SE1 sleeps                      Gaspers & Cimiano, in press).
             (2)   Semantic frame      sleep(AGENT)                    Subsequently, AV is updated, regardless of whether or not
                   Mapping             SE1 → AGENT
                                                                       verb-general merging is possible. This update involves
                 Syntactic pattern    SE1 V E1                         searching for paths with learned verb-general meanings
           (3)   Semantic frame       ACTION(AGENT)                    which differ from the modified NL sentence only in the posi-
                 Mapping              SE1 → AGENT
                                                                       tion expressing an ACTION predicate, e.g. a path p = “SE1
   Since verbs map to action frames taking arguments ex-               V E1 SE2 ” in case of the modified NL sentence “SE1 eats
pressing thematic roles of participants involved in the action,        SE2 ”. The associative network AV then learns that the lexi-
the model should also capture cross-situational statistics of          cal unit “eats” can occur within the syntactic pattern repre-
verbs and syntactic frames, i.e. associate verbs with syntactic        sented by the path (e.g., “eat” with path p). A meaning for an
frames (and hence with possible argument structures).                  NL sentence that corresponds to a verb-general construction
In the network, we model sets of elements mapping to an                can be determined in the same way as for verb-specific con-
ACTION predicate analogously to sets of elements express-              structions. Given a verb (lexical unit), an associated syntactic
ing slots in syntactic patterns, namely, as nodes which group          frame can be retrieved from AV .
                                                                   2254

        Experimental results and discussion                         During each of the 12 experimental trials, children heard two
Our model requires initial linguistic knowledge to evaluate         intransitive (such as “she’s pimming”) or transitive (such as
it with respect to the psycholinguistic studies with children.      “she’s pimming her toy”) sentences, each containing a dif-
This section describes how input data were generated, and           ferent novel verb, while watching two videos showing two
then presents how data from our experiments compare to the          different actors, each performing a novel action. In the tran-
psycholinguistic findings.                                          sitive condition the action was performed with different ob-
                                                                    jects. Children in the intransitive condition were significantly
Input data                                                          above chance in choosing the target actions over the distrac-
Input data were generated similarly to Alishahi & Stevenson         tor actions. Performance in the transitive condition depended
(2008) using the Eve corpus from the CHILDES database               on children’s vocabulary size: Only children with large vo-
(Brown, 1973), which contains transcriptions of interactions        cabularies performed significantly above chance.
with the child Eve. As input we used utterances spoken by           We tested whether our model can infer meanings for novel
Eve’s mother. Since Arunachalam & Waxman (2010) and                 verbs without receiving unambiguous label trials for any of
Scott & Fisher (2012) only consider transitive and intransitive     the verbs. Thus, we tested whether the model can set up
structures (in one case including conjoined subjects), we ex-       verb entries that contain information about possible refer-
tracted all patterns of the form “AGENT verb” and “AGENT            ents and update co-occurrence frequencies over time. Note
verb THEME” from the corpus. We considered the same 13              that since we used symbolic input, we cannot investigate
verbs as Alishahi & Stevenson (2008). Since two of the verbs        the influence of abstraction over different actors and ob-
did not occur in the considered forms, we included the fol-         jects at the visual level. We used the same verbs, i.e.
lowing 11 verbs in our experiments: come, eat, fall, get, go,       “pim”, “nade”, “rivv”, and “tazz”, and pairings of verbs as
look, make, put, see, sit and take.                                 Scott & Fisher (2012). Referents for verbs were selected
The input generation lexicon contained all patterns along with      from the input generation lexicon (i.e., “mom” and “cel-
their occurrence frequencies as well as the concrete nouns ap-      ery”). Since the model processes one sentence at a time,
pearing at the AGENT and THEME positions of each verb               each input example contained one sentence and two possible
along with their occurrence frequencies. Two nouns con-             mrs. For example, the first two intransitive input examples
joined by “and” were also included. “Me”, “you”, and “we”           (which correspond to one trial in the study) were NL: “mom
were treated as “Eve”, “Mom” and “Mom and Eve”, re-                 pim”; mr1 :pim(AGENT:mom); mr2 : nade(AGENT:mom)
spectively. We created NL examples from the input lexicon           and NL: “mom nade”; mr1 :pim(AGENT:mom); mr2 :
by randomly selecting patterns and referents according to           nade(AGENT:mom). After receiving the examples, the
their distribution in the lexicon/dataset. Semantic represen-       model was asked to retrieve the semantic representations
tations mr were created automatically using words appearing         for the novel verbs, e.g. for “mom nade”, and counted how
in generated NL sentences to denote the corresponding se-           often the model returned the correct representation, e.g.
mantic referents. For example, “Mom sees” is represented            nade(AGENT:mom). We performed the experiment with dif-
as see(AGENT:mom). Semantic referents are only arbitrary            ferent numbers of examples observed prior to the experimen-
symbols to the model: It still needs to establish connections       tal trials, corresponding to different “ages” of the model. Fig.
between words and referents. Two referents conjoined by             2 shows the results. In line with the children in the experi-
“and” were treated as separate arguments having the same
thematic relation. For instance, “mom and eve see” was rep-
                                                                    Figure 2: Proportion of the model’s choice of the correct se-
resented as see(AGENT1:mom,AGENT2:eve). In this paper,
                                                                    mantic representation for the novel verbs in the transitive and
we do not address learning morphology. Hence, all words
                                                                    intransitive sentences.
appear in their root form only. Ten different simulations con-
taining 500 examples of the form (NL,mr) were generated and
used for the following experiments. Presented results are av-
eraged over the ten simulations. Model parameters were op-
timized on an independent data set.
Cross-situational verb learning
As mentioned above, Scott & Fisher (2012) investigated
cross-situational verb learning and found that 2.5-year-old
children can use cross-situational statistics to infer verb
meanings under referential uncertainty, even if this requires
                                                                    ments, the model can solve the task i) in both conditions from
abstraction across different actors and objects. This suggests
                                                                    a certain “age” on and ii) earlier in the intransitive condition
that children can attach information about possible referents
                                                                    compared to the transitive condition, which was also more
to novel verb entries along with their co-occurrence statis-
                                                                    difficult for children. In particular, several children in Scott &
tics and refine this information across trials. The study in-
                                                                    Fisher (2012) failed in the transitive condition, while even 12-
vestigated learning of both transitive and intransitive verbs.
                                                                2255

to-14-month-old children typically master such a task when it         transitive condition were significantly above chance in choos-
involves mapping nouns to objects (Smith & Yu, 2008). This            ing the causative scene. In contrast, toddlers in the intransi-
led Scott & Fisher (2012) to conclude that in cross-situational       tive condition performed at chance level.
learning the same learning mechanisms may not apply uni-              We tested the model in a similar manner both in a transi-
formly for words of different categories. However, our model          tive and intransitive condition. Training trials were omitted
shows that applying the same learning mechanism for track-            since there was no need to make the model familiar with the
ing co-occurrence statistics at different levels can yield be-        task. Thus, there were four experimental trials, each featur-
havior similar to that observed in psycholinguistic studies.          ing a different novel verb. Each verb was presented to the
In our model, sentence-/verb-to-action mapping lags behind            model eight times in either a transitive or subject-conjoined
word-to-object mapping because it involves more complex               intransitive sentence (depending on the experimental condi-
structures whose acquisition depends on the prior acquisi-            tion). Referents for verbs were chosen from the input data,
tion of less complex structures, i.e. nouns. In particular,           and the same referents were used in both conditions. The
in order to establish a mapping for a verb “pim” in a sen-            model was trained using these sentences (without accom-
tence “mom pim celery”, an NL pattern like “SE1 pim SE2 ”             panying mrs). Then, for each trial the model was asked
must previously have been derived, and “mom” and “celery”             to “find new-verb” in the presence of two mrs, a causative
must be contained in the groupings SE1 and SE2 , respec-              and a synchronous one. Since toddlers do not stop learning
tively. Furthermore, a necessary condition for deriving the           during experimental test periods, each test input (e.g. NL:
pattern is that meanings for “mom” and “celery” must have             “find moop”; mr1 : moop(AGENT1:mom,AGENT2:eve); mr2 :
been learned. Thus, similar to the children, the model’s abil-        moop(AGENT:mom,THEME:eve)) included a learning step.
ity to solve the task depends on vocabulary, though not on            We then asked the model to retrieve the mr associated with the
the absolute vocabulary size, but rather on whether the mean-         syntactic frame linked to the novel verb. Again, results were
ings for the words observed at argument positions have al-            computed for different numbers of examples observed. Fig.
ready been learned (though, of course, the probability that the       3 shows that, similarly to the children, at a certain “age” the
needed lexical units have already been learned may be higher          model picks the causative scene (significantly) above chance
for larger vocabularies).                                             in the transitive condition, but performs at chance level in the
The model learns faster in the intransitive compared to the           conjoined-subject intransitive condition. In line with the psy-
transitive condition because it must have acquired only one           cholinguistic results, the model can create an initial verb entry
word instead of two words for referents. In addition, pat-            based on syntactic information alone, and it can retrieve this
terns containing fewer groupings are in general learned ear-          information when encountering the verb later on, if a suit-
lier because the model generalizes based on type variation            able verb-general syntactic frame has been learned prior to
observed in one position. Notice, however, that we do not             the experimental trials. The model performs at chance level in
claim that children learn “mom” or “celery” at a specific age;        the conjoined-subject condition because it learns the syntac-
these words were chosen arbitrarily for our experiments be-           tic frame for this condition after that of the transitive condi-
cause they appear in our input data. Notice further that in           tion: Further experiments revealed that with a greater number
contrast to the following experiment, the model can solve the         of input examples, the model also performs above chance in
above task even without the proposed extension.                       the conjoined-subject condition (i.e. it chooses the causative
                                                                      scene significantly less often). Thus, the model associates
Syntax as a zooming lens into semantics                               conjoined-subject intransitives with non-causal events at a
As mentioned above, Arunachalam & Waxman (2010)                       later “age” than transitives with causal events, i.e. it learns
showed that 27-month-old children can use the syntactic con-          the corresponding verb-general construction later. Similarly,
text to set up an initial verb entry and retrieve this entry when
encountering the verb later on. The study comprised two
training trials involving known verbs (to familiarize children        Figure 3: Proportion of the model’s choice of the causative
with the task) and four experimental trials involving different       scene in the transitive and conjoined-subject intransitive con-
novel verbs. During each trial of the study, toddlers first heard     ditions.
verbs presented within a dialogue without any relevant refer-
ent scenes. Each verb was presented eight times, either in
transitive (e.g., “the lady mooped my brother”) or conjoined-
subject intransitive sentences (e.g., “the lady and my brother
mooped”), without accompanying visual information. Tod-
dlers then viewed two different scenes side-by-side depict-
ing the same two participants: a synchronous scene and a
causative scene. Toddlers were then instructed to find the
scene that corresponded to the syntactic structure in which
the verb had been presented. This instruction (e.g. “find             children do not succeed in the conjoined-subject intransitive
moop”) provided no syntactic information. Toddlers in the             task until the age of 3;4, while they can succeed in the tran-
                                                                  2256

sitive task at the age of 2 (Nobel, Rowland, & Pine, 2011).         anisms that are similar to those implemented in the model.
Since the model acquires both types of verb-general construc-       For instance, cross-situational verb learning can be explored
tions in the same manner, the same proposed learning mech-          through more detailed analyses of children’s vocabularies and
anisms can account for the different results. In the model,         by testing children with novel vs. known nouns as referents
this result is due to the input data: The model acquires the        for verbs.
conjoined-subject intransitive later because conjoined-subject
intransitive sentences appear in the data much less frequently                          Acknowledgments
than transitive sentences.                                          This work has been funded by the DFG within the CRC 673
                                                                    and the Cognitive Interaction Technology Excellence Center.
         General discussion and conclusions                         Thanks to Frederike Strunz for annotating the Eve Corpus.
We have presented a computational model for the acquisition
of verb-general constructions under referential uncertainty.                                 References
The model establishes form-meaning mappings under refer-            Alishahi, A., & Stevenson, S. (2008). A computational model
ential uncertainty by relying on cross-situational learning at         of early argument structure acquisition. Cognitive Science,
different levels. Several models that acquire constructions            32(5), 789–834.
have been proposed (see Gaspers & Cimiano, in press), in-           Arunachalam, S., & Waxman, S. R. (2010). Meaning from
cluding models that acquire verb-general constructions (e.g.           syntax: Evidence from 2-year-olds. Cognition, 114, 442-
Alishahi & Stevenson, 2008). However, most models assume               446.
that words or lexical mappings are already learned and/or do        Behrens, H. (2009). Usage-based and emergentist approaches
not address learning under referential uncertainty. However,           to language acquisition. Linguistics, 47(2), 383–411.
such learning is relevant for the experiments simulated here        Brown, R. (1973). A first language: the early stages. Harvard
since they address the acquisition of verb entries, including          University Press, Cambridge MA.
the establishment of lexical mappings under referential un-         Frank, M., Goodman, N. D., & Tenenbaum, J. B. (2007).
certainty. Several computational models can also use cross-            A bayesian framework for cross-situational word-learning.
situational learning to establish form-meaning mappings un-            In J. C. Platt, D. Koller, Y. Singer, & S. T. Roweis (Eds.),
der referential uncertainty (e.g. Frank, Goodman, & Tenen-             Nips. Curran Associates, Inc.
baum, 2007). However, these models have mainly focused
                                                                    Gaspers, J., & Cimiano, P. (in press). A computational
on establishing mappings between words and referents. In
                                                                       model for the item-based induction of construction net-
contrast, our model applies the same cross-situational learn-
                                                                       works. Cognitive Science.
ing mechanism consistently to establish correspondences be-
tween form and meaning beyond simple word-referent map-             Gleitman, L. R., & Fisher, C. (2005). Universal aspects of
pings, in particular, between NL patterns/syntactic frames and         word learning. In J. A. McGilvray (Ed.), The Cambridge
actions, including thematic relations. Hence, our model can            companion to Chomsky. Cambridge University Press.
represent verb entries in the framework of these NL patterns,       Goldberg, A., & Suttle, L. (2010). Construction grammar.
allowing it to store additional information about possible ref-        Wiley Interdisciplinary Reviews: Cognitive Science, 1(4),
erents with verb entries.                                              468–477.
We have presented empirical results that show how the model         Merriman, W., & Bowman, L. (1989). The mutual exclusivity
can establish verb meanings under referential uncertainty.             bias in children’s word learning. Monographs of the Soci-
Moreover, we have shown how the model can learn verb-                  ety for Research in Child Development, 54(3–4), 1–129.
general constructions, and how it can use this knowledge            Nobel, C. H., Rowland, C. F., & Pine, J. M. (2011). Com-
to create initial verb entries based on syntactic information          prehension of argument structure and semantic roles: Evi-
alone. In line with usage-based approaches, the model’s be-            dence from english-learning children and the forced-choice
havior depends on the input data, taking into account both             pointing paradigm. Cognitive Science, 35, 963–982.
token frequency and type variation. Overall, our results            Quine, W. V. O. (1960). Word and object. MIT Press.
suggest that enough suitable input data in combination with         Rojas, R. (1993). Theorie der neuronalen Netze. Springer
the model’s learning mechanisms can yield the behavior ob-             Verlag.
served in children, and the model hence provides one possible       Scott, R. M., & Fisher, C. (2012). 2.5-year-olds use cross-
formal explanation for the observed behavior. While several            situational consistency to learn verbs under referential un-
models that acquire constructions and/or word-to-meaning               certainty. Cognition, 122(2), 163–180.
mappings have been proposed (see Gaspers & Cimiano, in
                                                                    Siskind, J. M. (1996). A computational study of cross-
press), we are not aware of any other model that describes all
                                                                       situational techniques for learning word-to-meaning map-
the specific learning mechanisms and representations that our
                                                                       pings. Cognition, 61(1–2), 39-91.
model explores with respect to early verb learning. Experi-
ments with children which test the model’s predictions may          Smith, L., & Yu, C. (2008). Infants rapidly learn word-
establish whether or not children indeed apply learning mech-          referent mappings via cross-situational statistics. Cogni-
                                                                       tion, 106(3), 1558–1568.
                                                                2257

