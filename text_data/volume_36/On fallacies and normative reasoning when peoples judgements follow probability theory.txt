UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
On fallacies and normative reasoning: when people's judgements follow probability theory.

Permalink
https://escholarship.org/uc/item/96n309wt

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Authors
Costello, Fintan
Mathison, Travis

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

On fallacies and normative reasoning:
when people’s judgements follow probability theory.
Fintan J. Costello (fintan.costello@ucd.ie)
Travis Mathison (tmathiso@gmail.com)
School of Computer Science and Informatics
University College Dublin
Dublin 4, IRELAND
Abstract

In the first section of the paper we give some background
by briefly describing four biases in people’s probabilistic reasoning. In the second section we describe our model and
show how it explains these biases. In the third section we
show that this model predicts that bias will be cancelled when
probability estimates are combined according to probability
theory’s addition rule, which requires that

The systematic conjunction and disjunction fallacies seen in
people’s probability judgments appear to show that people do
not reason according to the rules of probability theory. In
an experiment examining people’s judgments of the probability of different medical conditions, we find evidence against
this view. In this experiment people’s probability judgments
closely followed the fundamental ‘addition rule’ of probability
theory. This close match to probability theory comes alongside
frequent occurrence of the conjunction and disjunction fallacies in those same probability judgments. These results support a model where people reason about probability via probability theory but are subject to random variation or noise in
the recall of items from memory. In this model the effect of
random variation is cancelled out by the mathematical form of
the addition rule, producing agreement with probability theory; however, noise is not cancelled out for conjunctive or disjunctive comparisons, producing conjunction and disjunction
fallacy responses.
Keywords: Probabilistic reasoning; conjunction fallacy; rationality

P(A1 ) + P(A2 ) − P(A1 and A2 ) − P(A1 or A2 ) = 0
for all events A1 and A2 . In the fourth section we describe
an experiment testing this prediction and showing that when
people’s probability estimates are combined via the addition
rule, the result is 0 just as required by probability theory. The
final section draws some conclusions from this result.

Background:Biases in Probabilistic Reasoning
We consider four biases in probabilistic reasoning: conservatism, subadditivity, the disjunction fallacy, and the conjunction fallacy. Perhaps the best known of these is the conjunction fallacy. Probability theory’s ‘conjunction rule’ requires
that, for any pair of events A1 and A2 where P(A1 ) ≤ P(A2 )
the relationship

Introduction
Probability theory provides a calculus of chance describing
how to make optimal predictions under uncertainty. Up to
the 1960s the standard view in psychology was that people’s
probabilistic reasoning essentially followed probability theory. However, various systematic biases in people’s probability judgements (many identified in the 1970s and 1980s by
Tversky, Kahneman and colleagues) led researchers to conclude that, in fact, people do not follow probability theory but
instead estimate probabilities using various heuristics. While
these heuristics often yield reasonable judgments, they can
also produce strong biases in people’s probabilistic reasoning
in certain cases (Tversky and Kahneman, 1973).
In this paper we return to the view that people follow probability theory when reasoning about uncertainty. We present
a simple model where people estimate probabilities according to probablity theory, but are subject to random error or
noise in recall from memory: we show that this model can
explain various systematic biases seen in people’s probabilistic reasoning. Importantly, this model predicts that bias will
be ‘cancelled’ for certain combinations of people’s probability estimates, and so those combinations should agree closely
with probability theory. In an experiment testing this prediction by gathering people’s probability estimates for various
medical conditions, we find extremely close agreement with
probability theory, demonstrating the predicted cancellation.

P(A1 and A2 ) ≤ P(A1 )
must hold. This follows from the fact that A1 and A2 can
only occur if A1 itself occurs. People reliably violate this requirement for some events, giving probability estimates for
conjunctions that are greater than the estimates they gave for
one or other constituent of that conjunction. Tversky & Kahneman’s original demonstration of this fallacy (Tversky and
Kahneman, 1983) concerned Linda:
“Linda is 31 years old, single, outspoken, and bright.
She majored in philosophy. As a student she was deeply
concerned with issues of discrimination and social justice, and participated in anti-nuclear demonstrations”
Participants in Tversky & Kahneman’s study read this description and were asked to rank various statements by their
probability. Two of these statements were
• (A1 ) Linda is a bank teller.
• (A1 and A2 ) Linda is a bank teller and an active feminist.

361

Tversky & Kahneman found that more than 80% of participants ranked A1 and A2 as more probable than A1 , showing a strong bias toward violating the conjunction rule. A
large number of subsequent studies have shown that this conjunction fallacy occurs reliably in people’s probabilistic judgments, although studies controlling for various extraneous
factors suggest that typical conjunction fallacy rates are lower
than those seen in the Linda example: between 20% and 40%.
These studies also show that (1) the fallacy occurs most frequently when the objective probabilities P(A1 and A2 ) and
P(A1 ) are close together, and declines as the difference between these probabilities increases, and (2) the fallacy occurs most frequently when P(A1 ) is low and both P(A2 ) and
P(A1 |A2 ) are high (for reviews see Costello, 2009a, Gavanski and Roskos-Ewoldsen, 1991, Fantino et al., 1997). These
patterns are seen in the Linda example, where the description of Linda is designed so that P(A2 ) (the probability that
Linda is a feminist) is very high: this necessarily implies that
P(A1 and A2 ) will be close to P(A1 ) (since Linda is almost
certainly a feminist, if she is a bank teller she is almost certain to be a feminist bank teller).
A similar sort of fallacy occurs for disjunctions. Again,
probability theory’s ‘disjunction rule’ requires that, for any
event A2 and any disjunction A1 or A2

Our final bias, conservatism (or ‘underconfidence’) involves comparison between people’s subjective probability
estimates for events and the true probablities for those events.
In experiments where people are shown events occurring with
a certain probability P(A) (known to the experimenter) and
are asked to generate subjective estimates for those events, a
reliable pattern of deviation between known and estimated
probabilities occurs: people’s estimates tend to avoid the
boundary probabilities of 0 and 1. More specifically, the
the closer the true probabiliity P(A) is to 0, the more people’s estimates are greater than P(A), while the closer P(A)
is to 1, the more people’s estimates are less than P(A). Differences between true and estimated probabilities are lowest
when P(A) is close to 0.5 and increase as P(A) approaches
the boundaries of 0 or 1 (Erev et al., 1994).

Our Model of Probabilistic Reasoning with
Noisy Recall
In our model of probabilistic reasoning we assume a rational
reasoner with a long-term episodic memory. For simplicity
we assume that, apart from random variation, the reasoner is
‘perfect’: that is, if the reasoner were not subject to random
variation then their probability estimates for any event would
be equal to the true probability of that event. We take P(A)
to represent the true probability of some event A, PE (A) to
represent a reasoner’s estimate of that probability, and PE (A)
to represent the mean or expected value of PE (A) (the average
estimate of the probability of event A).
We assume a simple form of long-term memory containing m episodes where each recorded episode i contains a flag
fi (A) that is set to 1 if i contains event A and set to 0 otherwise, and the reasoner estimates the probability of event A by
counting these flags. We assume a minimal form of transient
error, in which there is some small probability d that when the
state of some flag fi (A) is read, the value obtained is not the
correct value for that flag. We take C(A) to be number of flags
that were read as 1 in some particular query of memory, and
TA be the number of flags whose correct value is actually 1.
Note that due to random error C(A) = TA does not necessarily
hold.1
Our reasoner computes a probability estimate PE (A) by
querying episodic memory to count all episodes containing
A and dividing by the total number of episodes, giving

P(A2 ) ≤ P(A1 or A2 )
must hold. This follows from the fact that if A2 occurs,
then A1 or A2 necessarily occurs. People reliably violate
this requirement for some events, giving probability estimates for disjunctions that are greater than the estimates they
gave for one or other constituent. Again, this fallacy occurs
most frequently when the objective probabilities P(A2 ) and
P(A1 or A2 ) are close together, and declines as the difference
between those probabilities increases (Costello, 2009b).
Our third bias, subadditivity, is similar to the conjunction
fallacy in that it involves people’s probability estimates violating a required upper bound. Let A1 . . . An be a set of n
mutually exclusive events, and let A = A1 ∨ . . . ∨ An be the
disjunction (the ‘or’) of those n events. Then probability theory requires that
n

∑ P(Ai ) = P(A)

i=1

Experimental results show that people reliably violate this requirement, and in a characteristic way. On average the sum of
people’s probability estimates for events A1 . . . An is reliably
greater than their estimate for the probability of A, with the
difference (the degree of ‘subadditivity’) increases reliably as
n increases. An additional, more specific pattern is also seen:
for pairs of mutually exclusive events A1 and A2 whose probabilities sum to 1 we find that the sums of people’s estimates
for A1 and A2 are normally distributed around 1, and so on
average this sum is equal to 1 just as required by probability theory. This pattern is sometimes referred to as ‘binary
complementarity’ (Tversky and Koehler, 1994).

PE (A) =

C(A)
m

Random variation afffects PE (A) when it causes some flag
fi (A) be read incorrectly. The expected value of PE (A) is
PE (A) =

TA (1 − d) + (m − TA )d
m

1 This type of sampling error is only one of many possible sources
of nose. While we use this simple form of sampling error to motivate and present our model, our intention is to demonstrate the role
of noise - from whatever source - in causing systematic biases in
probability estimates.

362

(since on average 1 − d of the TA flags whose value is 1 will
be read as 1, and d of the m − TA flags whose value is 0 will
be read as 1). Since we assume that if the reasoner were not
subject to random variation then each estimate PE (A) would
be equal to P(A), we have

probabilities P(A1 ) and P(A2 ) and in conjunctive and disjunctive probabilities P(A1 and A2 ) and P(A1 or A2 ). Taking
P(A1 ) ≤ P(A2 ), the conjunction fallacy occurs in our reasoners probability estimates when
PE (A1 ) < PE (A1 and A2 )

TA
P(A) =
m

Since the reasoner is subject to random variation we can
write the mean or average estimates as

and so
PE (A) = P(A) + d − 2dP(A)

PE (A1 ) = P(A1 ) + d − 2dP(A1 )

(1)

PE (A1 and A2 ) = P(A1 and A2 ) + d − 2dP(A1 and A2 )

From this expression we see that the average value of
PE (A) deviates from P(A) in a way that systematically depends on P(A). This deviation matches the pattern of conservatism seen in people’s probability estimates: if P(A) = 0.5
this deviation will be 0, if P(A) < 0.5 then since d cannot be
negative we have PE (A) > P(A) with the difference increasing
as P(A) approaches 0, and if P(A) > 0.5 then PE (A) < P(A)
with the difference increasing as P(A) approaches 1.
This deviation also explains the patterns of subadditivity
seen in people’s probability estimates. Recall that subadditivity applies to a set of n mutually exclusive events A1 to An
where A = A1 ∨ . . . ∨ An is the disjunction (the ‘or’) of those
events. Then from probability theory we have

and we see that the reasoner’s individual estimates for P(A1 )
and P(A1 and A2 ) will vary randomly around those means.
More specifically the reasoner’s estimates for those probabilites at any given moment will be equal to
PE (A1 ) = PE (A1 ) + e(A1 )
PE (A1 and A2 ) = PE (A1 and A2 ) + e(A1

PE (A1 ) + e(A1 ) < PE (A1 and A2 ) + e(A1 and A2 )
or, substituting and rearranging, when

P(A1 ) + . . . + P(An ) = P(A)

(P(A1 ) − P(A1 and A2 ))(1 − 2d) < e(A1

From Equation 1 the expected value of the sum of people’s
probability estimates for events A1 to An is given by
n
i=1

=

and A2 ) − e(A1 )

holds. Given that e(A1 and A2 ) and e(A1 ) vary randomly and
can be either positive or negative, this inequality can hold
in some cases. The inequality is most likely to hold when
P(A1 ) − P(A1 and A2 ) is low, and so this model predicts that
the closer the conjunctive probability P(A1 and A2 ) is to the
lower constituent probability P(A1 ), the more likely the conjunction fallacy is to occur. Since we have

n

∑ PE (Ai )

and A2 )

where e(A1 ) and e(A1 and A2 ) represent positive or negative random deviation from the mean at that time. The conjunction
error will occur when

∑ (P(Ai ) + d − 2dP(Ai ))

i=1

and using the fact that P(A1 ) + . . . + P(An ) = P(A) this gives
n

∑ PE (Ai ) = P(A) + nd − 2dP(A)

P(A1 ) − P(A1 and A2 ) = P(A1 ) − P(A1 |A2 ) × P(A2 )

i=1

the model also predicts that the lower P(A1 ) is, and the higher
P(A2 ) and P(A1 |A2 ) are, the more likely the conjunction fallacy is to occur. Both these patterns are just as seen experimental studies of the conjunction fallacy.
Reasoning in just the same way for disjunctions, we see
that the disjunction fallacy will occur when

Taking the difference between this expression and that for
PE (A) in equation (1) we get
n

∑ PE (Ai ) − PE (A) = (n − 1)d

i=1

(where n is the number of individual mutually exclusive
events). This difference increases as n increases, producing
subadditivity as seen in people’s probability judgments. In
the case of two mutually exclusive events A1 and A2 whose
probabilities sum to 1, from Equation (1) we get

PE (A1 or A2 ) + e(A1

or A2 )

< PE (A2 ) + e(A2 )

or, substituting and rearranging as before, when
(P(A1 or A2 ) − P(A2 ))(1 − 2d) < e(A2 ) − e(A1

PE (A1 ) + PE (A2 ) = P(A1 ) + P(A2 ) + 2d − 2d(P(A1 ) + P(A2 ))
=1
producing binary complementarity as in people’s judgments.
We now turn to the conjunction and disjunction fallacies.
In our model these fallacies are a consequence of random
variation in individual probability estimates for constituent

363

or A2 )

holds. Again, since the error terms here vary randomly and
can be either positive or negative, this inequality can hold in
some cases, and is is most likely to hold when P(A1 or A2 ) −
P(A2 ) is low: this model predicts that the closer the higher
constituent probability P(A2 ) is to the disjunctive probability P(A1 or A2 ), the more likely the disjunction fallacy is to
occur. Again, this is just as seen in people’s probability judgments.

The Addition Rule: Predicting Unbiased
Probability Estimates

Alzheimer’s disease, Arthritis, Cardiovascular disease, Depression, Diabetes, Osteoporosis, High blood pressure, and
Glaucoma. From these we selected 16 pairs of conditions
and used each pair to construct conjunctive and disjunctive
conditions such as: Alzheimer’s disease and Arthritis; Cardiovascular disease and Depression; Alzheimer’s disease or
Arthritis; Cardiovascular disease or Depression; and so on.
This gave 16 matched conjunctions and disjunctions, and 40
conditions in total: 8 single conditions, 16 conjunctive conditions, and 16 disjunctive conditions. Every participant was
asked to estimate the probablity of every one of these 40 conditions occurring in a particular population (people in Ireland
over 65 years of age).
Previous research has shown that the conjunction and disjunction fallacies ocucr less often when people are asked
about event frequencies as opposed to event probabilities. To
control for this we asked one group of participants (N = 11)
to give estimates in terms of frequencies and the other group
(N = 10) to give estimates in terms of probabilities. Examples
of the questions are

Our simple account where probability estimates are normatively correct but influenced by random noise can explain various patterns of bias in people’s probability judgements, and
also explain some specific situations in which those biases
vanish (when probabilities are close to 0.5, for conservatism;
and when two complementary probabilities sum to 1, for subadditivity). We now present a third situation in which this
account predicts that bias will disappear.
Consider an experiment where we ask people to estimate,
for any pair of events A and B, the probabilities of A, B,
A and B and A or B. For each participant’s estimates for each
pair of events A and B we can compute a derived sum
XE (A, B) = PE (A) + PE (B) − PE (A and B) − PE (A or B)
We can make a specific prediction about the average value of
XE (A, B) for all events A and B. This value will be
XE (A, B) = PE (A) + PE (B) − PE (A and B) − PE (A or B)

(probability format) Imagine a randomly selected person in
Ireland over 65 years of age. What are the chances, as
a percentage (0% - 100%), that this person presents with
Alzheimer’s disease?

Substituting from Equation 1 and rearranging we get
XE (A, B) = (1 − 2d)(P(A) + P(B) − P(A and B) − P(A or B))

(frequency format) Imagine 100 randomly selected people
in Ireland over 65 years of age. How many of these people
(0 - 100) present with Alzheimer’s disease?

However, probability theory’s addition rule requires that
P(A) + P(B) − P(A and B) − P(A or B) = 0
for all events A and B, and we see that XE (A, B) = 0. Our
prediction, therefore, is that on average XE (A, B) will be equal
to 0 for all pairs of events A and B. More specifically, we
predict that values of XE (A, B) will be symetrically distributed
with a peak around the mean of 0.

For each group 40 questions were printed in a paper booklet,
accompanied by instructions explaining the task. Question
order was randomised. The task took around 30 minutes to
complete.

An Experiment

In analysing people’s responses in this experiment we considered two factors: the extent to which people committed
conjunction and disjunction fallacies, and the extent to which
people followed probability theory’s addition rule.

Results

We previously tested this addition rule prediction in experiments examining people’s judgements of probability for everyday events (different types of weather). For these weather
events, we found close agreement between people’s probability estimates and the prediction (Costello and Watts, 2014).
These weather events have not typically been used in studies examining people’s probabilistic reasoning, however; and
there therefore may have been some characteristics of those
events that made people more likely to follow probability theory. In this section we describe a test of this prediction in
an experiment examining people’s judgements of the probability of different medical conditions. Such judgments have
been used in a number of previous studies, particularly those
examining the conjunction fallacy.

Fallacy occurrence. Both the conjunction fallacy and the
disjunction fallacy occurred reliably in the frequency format
group (where there was a 16% rate of conjunction fallacy occurrence and a 20% rate of disjunction fallacy occurrence)
and in the probability format group (a 17% rate of conjunction fallacy occurrence and a 20% rate of disjunction fallacy
occurrence). There was no significant difference in fallacy
occurrence between the two groups, and so we collapsed the
groups in further analyses. As predicted, there was a significant negative correlation between the rate of conjunction
fallacy occurrence for a given conjunction A1 and A2 and
the average difference between PE (A1 and A2 ) and PE (A1 )
(r = −0.78, p < 0.01). Also as predicted, there was a significant negative correlation between the rate of disjunction
fallacy occurrence for a given disjunction A1 or A2 and the
average difference between PE (A1 or A2 ) and PE (A2 ) (r =
−0.77, p < 0.01).

Materials and procedure
This experiment asked 21 UCD students to complete a survey asking them to estimate the likelihood of different medical conditions or combination of conditions. To construct
the materials we selected 8 different medical conditions:

364

140
120
100

Number of
fallacy
responses by
participant

80

Frequency

60
40
20
0
-100

-50

0

50

14
12
10
8
6
4
2
0
-10

100

-5

0

5

10

Average value of XE for participant

Value of XE

Figure 1: This scatterplot shows the frequency of occurrence of values of XE in the experiment. Values of XE are
grouped into ‘bins’, each containing 10 values of XE from
v − 5 . . . v + 5 for v from −100 to 100 in steps of 10 (probablity estimates in the experiment were given on a scale of 0 to
100). The critical point here is that these values are symmetrically distributed around 0 as predicted in our model.

Figure 2: This scatterplot shows the total number of (conjunction and disjunction) fallacy responses for each participant against the average value of XE for that participant’s
responses. The critical point here is that fallacy responses
are relatively frequent even for participants whose average
XE values are close to 0.
result is thus hard for the heuristics view to explain.

Addition rule. For each participant we computed a value
for the expression XE for every set of conjunctions, disjunctions and constituents. For each participant there were thus
16 values of XE , one for each pair of conditions used, giving
21 × 16 = 336 values for XE . Our prediction is that these XE
values should vary randomly around a mean of 0 (the value
required by probability theory for these XE expressions). This
prediction was strongly confirmed: the mean value of XE
across all responses was −0.018 (SD = 16.5) on the 100
point scale used in the experiment (that is, within one fiftieth of a unit of the predicted value), with a sample median
of 0 and a sample mean of 0. The predicted mean of 0 lies
within the 99.9% confidence interval of this observed mean.
Figure 1 gives a summary representation of the distribution
of these values of XE . These values are symmetrically distributed around 0, as predicted by the model.
This result demonstrates an overall cancellation of bias
across the 4 terms in XE , where each term is subject to its own
individual degree of bias: PE (A) and PE (B) being subject to
conservatism (and with that bias cancelling only when ‘binary
complementarity’ holds; that is, only when P(A) = 1 −P(B)),
and PE (A and B) and P(A or B) being subject to conjunction and disjunction fallacy effects. For the heuristics account to explain this overall cancellation it is not enough to
say that people overestimate P(A and B) and underestimate
P(A or B): it is necessary to calibrate the varying degrees
of bias affecting estimates for P(A), P(B), P(A and B) and
P(A or B). Further, to ‘know’ that the bias in these 4 probabilities should cancel requires access to the rules of probability theory (as embodied in the addition law): access which,
in the heuristics view, people do not have. This cancellation

Relation between fallacies and the addition rule. To examine the relationship between the addition rule expression
XE and the occurrence of the conjunction and disjunction fallacies, we obtained the average XE value produced by each
participant and compared it with the total number of fallacy
responses produced by that participant. Figure 2 shows a scatterplot of these two measures. There was no relationship between these two measures (r = −0.03, p > 0.1); it is clear
from the scatterplot that participants’ whose average XE value
was close to 0 were just as likely to produce fallacy responses
as those whose XE value was further from 0.

Discussion and Conclusions
We have presented a simple model where people estimate the
probability of some event A by estimating the proportion of
instances of A in memory, but are subject to random errors
in the recall of instances. These random errors in recall produce randomly varying, noisy estimates for the probability of
A, and cause these estimates to be systematically biased in
a way that depends on the value of P(A). This systematic
bias explains a range of biases and errors observed in people’s probability estimation: conservatism, subadditivity, the
conjunction fallacy and the disjunction fallacy.
We used this simple model to construct a probabilistic expression (the addition rule) that cancels the bias in estimates
for one event against the bias in estimates for another. Our
experimental results show that, even though there were systematic patterns of bias in people’s probability estimates (producing reliable rates of conjunction fallacy and disjunction
fallacy responses), when people’s individual responses were

365

combined in the addition rule, the results showed no systematic bias and gave a very close match to the requirements of
probability theory. It is hard to explain this result without
assuming that people’s probabilistic reasoning in some way
embodies the addition rule.
Our model has a number of advantages over other recent
accounts of the conjunction fallacy, such as Busemeyer et al.
(2011)’s model based on the logic of quantum theory, and
Crupi et al. (2008)’s model based on causal support and confirmation. First, our model gives a general account for a range
of different biases, not just the conjunction fallacy. Second,
our model makes explicit predictions about cases where bias
will vanish: as far as we can see, other models do not make
such predictions. Third,without the observed patterns of conjunction fallacy occurrence in people’s probability estimates,
there would be no reason for suggesting that people reason
about probability using the logic of quantum theory or causal
support. There is, however, a clear reason for suggesting that
people use probability theory: probability theory provides the
optimal mechanism for such probabilistic reasoning. Because
our model is based on probability theory, it has an a priori
motivation which other models lack.
While our results demonstrate that people’s probability estimates follow probability theory (when bias due to noise is
cancelled) we do not think people are consciously aware of
the equations of probability theory when estimating probabilities. Indeed we doubt whether the participants in our experiment were aware of the probablity theory’s requirement
that the addition law expression should equal 0 or would be
able to apply that requirement to their estimations. Instead we
propose that people’s probability judgments are derived from
a ‘black box’ module of cognition that estimates the probability of an event A by retrieving (some analogue of) a count
of instances of A from memory. Such a mechanism is necessarily subject to the requirements of set theory and therefore
embodies the equations of probability theory.
We expect this probability module to be based on observed
event frequencies, and to be unconscious, automatic, rapid,
relatively undemanding of cognitive capacity and evolutionarily ‘old’. Support for this view comes from that fact that
people make probability judgments rapidly and typically do
not have access to the reasons behind their estimations, from
evidence that event frequencies are stored in memory by an
automatic and unconscious encoding process (Hasher and Zacks, 1984), and from results showing that animals effectively
judge probabilities (for instance, of obtaining food from a
given source) and that their probabilities are typically close
to optimal (Kheifets and Gallistel, 2012).
These results pose a particular challenge to the view that
people estimate probabilities using various heuristics. Notice
that the heuristics view assumes that people estimate probabilities using heuristics that in some cases yield reasonable
judgments (that is, judgments in accordance with probability theory) but in other cases lead to systematic error. To
give evidence against the heuristics view it is therefore not

enough to show that some of people’s probability judgments
agree with probability theory (that is expected in the heuristics view). Instead, our results challenge the heuristics view
because they show that that, even when people’s probability estimates show systematic bias (relatively frequent occurrence of the conjunction and disjunction fallacies), when
those estimates are combined to form expressions that cancel out the biasing effects of noise, the results are on average
strikingly close to those required by probability theory.

References
Busemeyer, J. R., Pothos, E. M., Franco, R., and Trueblood,
J. S. (2011). A quantum theoretical explanation for probability judgment errors. Psychological Review, 118(2):193.
Costello, F. (2009a). How probability theory explains the
conjunction fallacy. Journal of Behavioral Decision Making, 22(3):213–234.
Costello, F. J. (2009b). Fallacies in probability judgments for
conjunctions and disjunctions of everyday events. Journal
of Behavioral Decision Making, 22(3):235–251.
Costello, F. J. and Watts, P. (2014). Surprisingly rational: Evidence that people follow probability theory when judging
probabilities, and that biases in judgment are due to noise.
Psychological Review (in press).
Crupi, V., Fitelson, B., and Tentori, K. (2008). Probability,
confirmation, and the conjunction fallacy. Thinking & Reasoning, 14(2):182–199.
Erev, I., Wallsten, T. S., and Budescu, D. V. (1994). Simultaneous over- and underconfidence: The role of error in
judgment processes. Psychological Review, 101(3):519–
527.
Fantino, E., Kulik, J., Stolarz-Fantino, S., and Wright, W.
(1997). The conjunction fallacy: A test of averaging hypotheses. Psychonomic Bulletin & Review, 4(1):96–101.
Gavanski, I. and Roskos-Ewoldsen, D. (1991). Representativeness and conjoint probability. Journal of Personality
and Social Psychology, 61(2):181.
Hasher, L. and Zacks, R. (1984). Automatic processing of
fundamental information: the case of frequency of occurrence. The American Psychologist, 39(12):1372–1388.
Kheifets, A. and Gallistel, C. (2012). Mice take calculated
risks. Proceedings of the National Academy of Sciences,
109(22):8776–8779.
Tversky, A. and Kahneman, D. (1973). Availability: A
heuristic for judging frequency and probability. Cognitive
Psychology, 5:207–232.
Tversky, A. and Kahneman, D. (1983). Extensional versus
intuitive reasoning: The conjunction fallacy in probability
judgment. Psychological Review, 90(4):293–315.
Tversky, A. and Koehler, D. (1994). Support theory: A
nonextensional representation of subjective probability.
Psychological Review, 101(4):547–566.

366

