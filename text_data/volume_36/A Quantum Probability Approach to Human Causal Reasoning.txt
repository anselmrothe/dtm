UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Quantum Probability Approach to Human Causal Reasoning
Permalink
https://escholarship.org/uc/item/4s67v5rt
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Trueblood, Jennifer
Pothos, Emmanuel
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                    A Quantum Probability Approach to Human Causal Reasoning
                                                Jennifer S. Trueblood (jstruebl@uci.edu)
                                       Department of Cognitive Sciences, University of California, Irvine
                                                               Irvine, CA 92697 USA
                                        Emmanuel M. Pothos (emmanuel.pothos.1@city.ac.uk)
                                               Department of Psychology, City University London
                                                             London, EC1V 0HB, UK
                                  Abstract                                  power in this domain, thereby producing a disparate account
                                                                            of casual reasoning across tasks.
   When people make inferences about causal situations with
   vague and imperfect information, their judgments often devi-                We propose a unified explanation of human causal reason-
   ate from the normative prescription of classical probability. As         ing using quantum probability theory.2 In our approach, we
   a result, it is difficult to apply popular models of causal reason-      postulate a hierarchy of mental representations that could be
   ing such as ∆P and causal power, which provide good accounts
   of behavior in casual learning tasks and tasks where statistical         adopted for different situations. Classical probability mod-
   information is provided directly. We propose a unified expla-            els such as causal power represent one class of models in our
   nation of human causal reasoning using quantum probability               hierarchy.
   theory that can account for causal reasoning across many dif-
   ferent domains. In our approach, we postulate a hierarchy of
   mental representations, from fully quantum to fully classical,                                   Experiment 1
   that could be adopted for different situations. We illustrate our        We begin by introducing the experimental paradigm that we
   approach with new experiments and model comparisons.
                                                                            use to illustrate our modeling approach. This paradigm is
   Keywords: Causal reasoning, quantum probability
                                                                            based on one developed by Rehder (2003) to study causal
                                                                            reasoning with novel categories. In our task, participants are
                              Introduction                                  given a linguistic description of a novel category, Lake Vic-
Everyday we face situations where we must reason about                      toria Shrimp, and asked to judge the likelihood that certain
causes and effects. Often, causal relationships are well es-                features cause others. Specifically, participants are given in-
tablished through practice - when I plug in and turn on an                  formation about two independent features that can influence
electric kettle, water boils. In other situations, we must rea-             a third feature. The language used to describe the features
son about novel situations where there is vague and imperfect               and their relationships is purposely vague as many real life
information - if I vote for the new mayoral candidate, will                 situations do not involve precise information.
property taxes be lowered next year?                                           We were interested to see whether the order in which in-
   In general, causal judgment tasks can be divided into three              formation is presented in our task affected final judgments.
types: experienced, statistical descriptions, and linguistic de-            Order effects are well established in inference tasks (Hogarth
scriptions (Shanks, 1991). In experienced situations, partici-              & Einhorn, 1992) and are difficult to explain using simple
pants witness firsthand the relationships between causes and                classical probability models such as ∆P and causal power.
effects. In situations with statistical descriptions, participants          However, quantum probability theory has been successful in
are given summary information about the frequency that spe-                 accounting for these effects (Trueblood & Busemeyer, 2011).
cific causes or groups of causes produce effects. In situations
with linguistic descriptions, participants are asked to make                Methods
causal inferences from narratives, stories, texts, etc. These               Participants learned about a novel biological category, Lake
tasks often involve vague and imperfect information about                   Victoria Shrimp, that had three binary features: ACh neuro-
causes and effects. It is this last class of problems that is               transmitter (high or low amount), sleep cycle (accelerated or
our focus.                                                                  decelerated), and body weight (normal or high). Participants
   Models based on classical probability theory such as ∆P                  were given information about the typicality of feature values.
(Jenkins & Ward, 1965) and power PC theory1 (Cheng, 1997)                   For example, they were told that “Most Lake Victoria Shrimp
have provided good accounts of causal reasoning in both ex-                 have a high amount of ACh whereas some have a low amount
perienced situations and those described by statistical infor-              of ACh”. Participants were also given the causal relationships
mation. However, there is evidence that people’s judgments                  between features. These relationships were described as one
about causal systems with linguistic descriptions often devi-               feature causing another. Specifically, the ACh neurotransmit-
ate from the normative prescription of classical probability                ter and sleep cycle were described as affecting body weight.
(Sloman & Fernbach, 2011; Trueblood & Busemeyer, 2012).                     The strengths of causal relationships were described using the
As a result, it is difficult to use models such as ∆P and causal            terms “often” and “sometimes”. For example, participants
    1 We will refer to power PC theory as causal power throughout               2 We use the mathematical formalism of quantum theory without
the remainder of the paper.                                                 the associated physical meaning.
                                                                        1616

were told that “An accelerated sleep cycle often causes a high         an accelerated sleep cycle, and high body weight for the re-
body weight”.                                                          spective features. Likewise, a feature value of 0 denotes a low
   Participants first studied the three features and the typi-         amount of ACh, a decelerated sleep cycle, and a normal body
cality of their values. After studying this information, par-          weight.
ticipants took a multiple-choice test with six questions that
tested them on this knowledge. Participants were required                     Table 1: Sequences of judgments in Experiment 1.
to answer each question correctly before moving on to the                           Condition 1                       Condition 2
next one. Next, participants studied the two causal relation-            p(e1 )    p(e1 |x1 ) p(e1 |x1 , y1 ) p(e1 ) p(e1 |y1 ) p(e1 |y1 , x1 )
ships and took another multiple-choice test with eight ques-             p(e1 )    p(e1 |x1 ) p(e1 |x1 , y0 ) p(e1 ) p(e1 |y0 ) p(e1 |y0 , x1 )
                                                                         p(e1 )    p(e1 |x0 ) p(e1 |x0 , y1 ) p(e1 ) p(e1 |y1 ) p(e1 |y1 , x0 )
tions testing them on this new knowledge. As before, par-                p(e1 )    p(e1 |x0 ) p(e1 |x0 , y0 ) p(e1 ) p(e1 |y0 ) p(e1 |y0 , x0 )
ticipants were required to answer each question correctly be-                      p(x1 |e1 ) p(x1 , y1 |e1 )        p(y1 |e1 ) p(y1 , x1 |e1 )
fore moving on to the next one. Finally, participants were                         p(x1 |e1 ) p(x1 , y0 |e1 )        p(y0 |e1 ) p(y0 , x1 |e1 )
asked to take a few minutes to review the features and rela-                       p(x0 |e1 ) p(x0 , y1 |e1 )        p(y1 |e1 ) p(y1 , x0 |e1 )
                                                                                   p(x0 |e1 ) p(x0 , y0 |e1 )        p(y0 |e1 ) p(y0 , x0 |e1 )
tionships one more time. After they finished reviewing this
information, they completed a third multiple-choice test with
10 questions. In this final test, participants were only given            Participants were asked to enter their likelihood judgments
one opportunity to answer each question. Their score on this           about specific features as numbers between 0 and 100 in a
test was used to gauge how well they learned the features and          text box after reading each question. They were told that a
causal relationships.                                                  judgment of 0 implied that they were certain the shrimp did
                                                                       not have the feature, a response of 50 implied that the shrimp
   After completing the learning stage, participants were              was equally likely to have the feature or not, and a response
asked to make sequences of two or three judgments about                of 100 implied that they were certain the shrimp did have the
a particular shrimp as they learned new information about it.          feature. On each trial, there was a picture of a scale from
For example, they might first learn “A Lake Victoria shrimp            0-100 reminding participants of this information.
is caught” and asked “How likely is it for the shrimp to have             122 University of California, Irvine undergraduates partic-
a high body weight?”. Then, they learned information about             ipated in the experiment online at a time of their choosing for
different causes revealed during a series of lab tests. For ex-        course credit. There were 60 participants in condition 1 (i.e.,
ample, participants might read “After lab testing, you learn           ACh neurotransmitter first) and 62 participants in condition 2
that the shrimp has a high quantity of ACh neurotransmitter.           (i.e., sleep cycle first).
Given this new information, how likely is it that this shrimp
has a high body weight?”. For the final judgment in the se-            Results
quence, participants might read, “After further observation in         The average score on the 10 question multiple choice test
the lab, you also learn that the shrimp has a decelerated sleep        was 7.8 indicating most participants correctly learned the fea-
cycle. Given this new information, how likely is it that this          ture values and causal relationships during the learning stage.
shrimp has a high body weight?”.                                       Each multiple choice question had three possible responses.
   Participants were randomly assigned to one of two order             If participants guessed for all of the questions, their expected
conditions. In one condition, participants were always asked           score would be 3.3. We excluded participants with a score
to make judgments about the ACh neurotransmitter before                less than 7 out of 10 because it is crucial that participants
making judgments about the sleep cycle. In the other condi-            understand the causal structure. This criterion excludes 38
tion, this ordering was reversed. The between subjects design          participants leaving 41 in condition 1 and 43 in condition 2.
ensured subjects did not make judgments for reverse order-                We examined order effects in the predictive judgment se-
ings as these judgments could be influenced by memory. In              quences by comparing final probability judgments in the se-
both conditions, participants were asked to make two types of          quences across conditions. Figure 1 compares the mean judg-
judgments - predictive and diagnostic. Predictive judgments            ments from both conditions. In each plot, the left-most judg-
involved judging the likelihood of an effect given a cause             ment is the prior probability of the effect, the next judgment
(e.g., the likelihood of a shrimp having a high body weight            is the probability of the effect given one of the causes (either
given a low amount of ACh). Diagnostic judgments involved              x or y), and the right-most judgment is the probability of the
judging the likelihood of a cause given an effect (e.g., the like-     effect given both causes. If order effects exists, then the final,
lihood of a low amount of ACh given a high body weight).               right-most judgments will be different. Two of the four com-
The order in which the sequences were presented was ran-               parisons produced order effects. The judgment p(e1 |x1 , y0 )
domized across participants. As shown in table 1, each par-            was significantly different (t(82) = -3.04, p = 0.003) from the
ticipant completed four predictive (top) and four diagnostic           judgment p(e1 |y0 , x1 ). Similarly, the judgment p(e1 |x0 , y1 )
(bottom) sequences. In the table, the predictive effect (i.e.,         was significantly different (t(82) = 5.51, p < 0.001) from the
body weight) is denoted by e, and the two causes, ACh neu-             judgment p(e1 |y1 , x0 ). The type of order effect in these two
rotransmitter and sleep cycle, are denoted by x and y respec-          comparisons is a recency effect which results from dispropor-
tively. A feature value of 1 denotes a high amount of ACh,             tionate importance of recent information. Judgments where
                                                                   1617

                                                p(e = 1| x = 1, y = 1) vs p(e=1| y = 1, x = 1)                                        p(e=1| x = 1, y = 0) vs p(e=1| y = 0, x = 1)
                                          100                                                                                   100
                                                  x = 1, y = 1                                                                          x = 1, y = 0
                                                  y = 1, x = 1                                                                          y = 0, x = 1
                                          80                                                                                    80
                  Probability of Effect                                                                 Probability of Effect
                                          60                                                                                    60
                                          40                                                                                    40
                                          20                                                                                    20
                                           0                                                                                     0
                                                Prior         After First Cause After Both Causes                                     Prior        After First Cause After Both Causes
                                                p(e=1| x = 0, y = 1) vs p(e=1| y = 1, x = 0)                                          p(e=1| x = 0, y = 0) vs p(e=1| y = 0, x = 0)
                                          100                                                                                   100
                                                  x = 0, y = 1                                                                          x = 0, y = 0
                                                  y = 1, x = 0                                                                          y = 0, x = 0
                                          80                                                                                    80
                  Probability of Effect                                                                 Probability of Effect
                                          60                                                                                    60
                                          40                                                                                    40
                                          20                                                                                    20
                                           0                                                                                     0
                                                Prior         After First Cause After Both Causes                                     Prior        After First Cause After Both Causes
Figure 1: Mean judgments for the predictive sequences in Experiment 1. Condition 1 is shown in red and condition 2 in blue.
both feature values matched (i.e., both x and y equal 1 or both                                                                                  A Hierarchy of Models
equal 0), did not produce significant order effects (p > 0.05).
                                                                                                    In classical probability theory, it is assumed there is a sin-
   Similar to the predictive sequences, we examined order ef-                                       gle space that provides a complete and exhaustive descrip-
fects in the diagnostic sequences by comparing final proba-                                         tion of all events, which follows from the closure property
bility judgments in the sequences across conditions. Only the                                       of Boolean logic because if x and y are events in the sam-
comparison of p(x0 , y0 |e1 ) to p(y0 , x0 |e1 ) produced a signif-                                 ple space, so is the joint event x ∩ y. Repeated application
icant result (t(82) = -2.68, p = 0.009). The remaining three                                        of this principle yields the elementary events of the sample
comparisons did not produce significant order effects (p >                                          space - those events that cannot be broken down any further.
0.05). This is consistent with findings showing violations of                                       For example, consider a situation where there are three binary
classical probability in predictive judgments, but not diagnos-                                     (1 or 0) events, e, x, and y as in experiment 1. The elemen-
tic judgments (Fernbach, Darlow, & Sloman, 2011).                                                   tary events of the sample space arise from the intersection of
                                                                                                    these three events and include events such as e1 ∩x1 ∩y0 . This
   It is possible that participants misinterpreted the sec-                                         sample space has 23 = 8 elementary events. As the number
ond question so that they were judging p(e1 |y) instead of                                          of events increases, the size of the sample space rapidly in-
p(e1 |x, y) and similarly for the reverse ordering of x and y. To                                   creases. For only six binary events, the dimension of the sam-
control for this possible confound we ran a version of the ex-                                      ple space is 64. Plausibly, at some point, a person’s capacity
periment where all relevant feature information was included                                        to combine events into a unified sample space and assign joint
before each sequential judgment. For example, the second                                            probabilities will be exceeded.
question in the predictive case might be “After further ob-                                            Quantum probability theory is a geometric approach to
servation in the lab, you learn that the shrimp with a high                                         probability where events are represented as subspaces of a
quantity of ACh neurotransmitter also has a decelerated sleep                                       vector space. Unlike classical probability theory, quantum
cycle. Given this new information, how likely is it that this                                       probability allows for multiple sample spaces, which are each
shrimp has a high body weight?”. In experiment 1, partici-                                          associated with different bases for the same vector space. Dif-
pants were not reminded about the high quantity of ACh neu-                                         ferent sample spaces (i.e., bases) are related geometrically by
rotransmitter before making the final judgment of the effect.                                       rotations. When two events are described by two different
As in experiment 1, predictive judgments where feature val-                                         sample spaces, they are called incompatible and their joint
ues mismatched produced significant recency effects. Thus, it                                       event does not exist. Psychologically, this implies that indi-
is unlikely participants were misinterpreting the second ques-                                      viduals do not have a mental representation of the joint event.
tion in experiment 1.                                                                               That is, they cannot think about these two events simultane-
                                                                                                 1618

ously. Rather, the events are processed in a sequence by first           the corresponding row vector), we have
thinking of one event and then “rotating” to think about the                                            
                                                                                                        1
                                                                                                                       
                                                                                                                        0
other.                                                                                         |e1 i =     , |e0 i =       .
                                                                                                        0               1
   We hypothesize that individuals use multiple sample
spaces when reasoning about unfamiliar causal problems with              Then, we define the two rotation matrices
vague and imperfect information. Our hypothesis is moti-                                                                              
                                                                                    cos(θx ) − sin(θx )               cos(θy ) − sin(θy )
vated by the idea that causal learning is structurally local                Rx =                            , Ry =
                                                                                     sin(θx ) cos(θx )                 sin(θy ) cos(θy )
(Fernbach & Sloman, 2009). That is, when people are faced
with a complex learning problem, they break the problem up               where the basis associated with feature x is {Rx |e1 i, Rx |e0 i}
by focusing on individual causal relationships rather than the           and the basis associated with feature y is {Ry |e1 i, Ry |e0 i}.
full causal structure. Inferences about the full structure are              Quantum probability theory postulates the existence of a
constructed by combining local inferences piece by piece.                state represented by a unit length vector |ψi in the vec-
   In experiment 1, participants learned about three binary              tor space. Psychologically, we assume this 2-dimensional
features labeled e, x, and y in table 1. There are at least three        state vector is a knowledge state that represents an individ-
possible geometric approaches to modeling the features and               ual’s beliefs about the different features. Specifically, the
their relationships using either two, four, or eight dimensions.         two components of the state vector (i.e., probability ampli-
These different approaches form a hierarchy of models asso-              tudes) correspond to the individual’s beliefs about the two
ciated with different types of mental representations.                   possible feature values, 1 and 0. Using probability ampli-
   In the 2-dimensional approach, we assume that individu-               tudes α1 andqα0 , we write the state vector as |ψi = [α1 , α0 ]0
als consider one feature at a time and do not have mental
                                                                         where α0 = 1 − α21 in order to ensure the state vector is unit
representations of joint events. Mathematically, each feature
is represented as a different basis for a 2-dimensional vector           length. In total, the 2-dimensional model has three parame-
space where the two dimensions correspond to the two possi-              ters: θx , θy , and α1 .
ble feature values. This is equivalent to defining three sepa-              Probabilities are calculated by projecting the state vector
rate samples spaces that each have two elementary events.                onto different subspaces. For example, the prior probability
                                                                         of the effect, e1 , is given by p(e1 ) = ||Pe1 |ψi||2 where Pe1
   In the 4-dimensional approach, it is assumed that individ-
                                                                         is the 2 x 2 projection matrix with a 1 on the upper diag-
uals form mental representations for single cause and effect
                                                                         onal and zeros elsewhere. To calculate the predictive prob-
relationships, but do not think about multiple causal relation-
                                                                         ability of the effect given a single cause such as p(e1 |x1 ),
ships simultaneously. In this case, the two causal relation-
                                                                         the state vector is first updated to a conditional state |ψx1 i =
ships are represented as different bases for a 4-dimensional
                                                                         Px1 |ψi/||Px1 |ψi|| where the projector Px1 is defined by the
vector space where the four dimensions are associated with
                                                                         outer product |x1 ihx1 | where |x1 i = Rx |e1 i. The probability of
the four joint events, e1 ∩ x1 , e1 ∩ x0 , e0 ∩ x1 , e0 ∩ x0 for the
                                                                         the effect is then calculated by projecting the conditional state
causal relationship between e and x and similarly for e and y.
                                                                         onto the e1 subspace: ||Pe1 |ψx1 i||2 . Calculations for other
This is equivalent to defining two separate sample spaces that
                                                                         probabilities proceed in a similar manner.
each have four elementary events.
   In the 8-dimensional approach, it is assumed individuals              4-dimensional model
have mental representations of all joint events. In this case,           The 4-dimensional model assumes that individuals have men-
the eight dimensions are associated with the eight joint events          tal representations of single cause-effect relations so that joint
such as e1 ∩ x1 ∩ y0 . This is equivalent to a classical probabil-       events such as e ∩ x exist (i.e., the features can be thought
ity model with a single sample space describing all events.              about simultaneously). However, individuals do not have
                                                                         mental representations of joint events involving all three fea-
2-dimensional model                                                      tures such as e1 ∩ x1 ∩ y0 .
The 2-dimensional model assumes that the three binary fea-                  In this model, two different 4-dimensional bases are used
tures, e, x, and y, are each represented by different bases of a         rather than three 2-dimensional bases. The two bases associ-
2-dimensional vector space. Judgments of joint events such               ated with the two causal relationships are related to one an-
as p(x ∩ y) are formed by thinking of one feature first, say x,          other by a rotation matrix. Because only the angle between
and then the other. That is, individuals do not think about the          bases matters, we can fix one of the bases and define the
two features simultaneously.                                             other in relationship to it. We set the basis vectors for the
   Mathematically, the three bases associated with the three             x-e causal relationship to the standard basis for R4 and de-
features are related to one another by rotation matrices. Be-            fine the basis for the y-e causal relationship with the rotation
cause only the angle between bases matters, we can fix one               matrix R = R1 R2 R3 where
                                                                                                                                    
of the bases and define the others in relationship to it. We                             cos(θ1 ) − sin(θ1 )          0         0
set the basis vectors for e to the standard basis for R2 . Using                        sin(θ1 ) cos(θ1 )            0         0    
Dirac notation, according to which |vi just denotes a column                   R1 =                                                 
                                                                                        0              0         cos(θ1 ) − sin(θ1 )
vector and hv| is the conjugate transpose of this vector (i.e.,                             0           0          sin(θ1 ) cos(θ1 )
                                                                     1619

                                                               
               cos(θ2 )       0       − sin(θ2 )          0                effect. For experiment 1, there are two power parameters,
              0           cos(θ2 )        0         − sin(θ2 )           wx and wy , for the two causes of e. Cheng (1997) also as-
       R2 =  sin(θ2 )
                                                                
                              0         cos(θ2 )          0               sumed there could be alternative causes for the effect which
                  0        sin(θ2 )        0           cos(θ2 )            might be known or unknown. These alternative causes are
                                                                         also associated with a power parameter labeled wa . By the
              cos(θ3 )        0            0        − sin(θ3 )
                                                                           axioms of classical probability theory and independence of x
             0           cos(θ3 ) − sin(θ3 )            0      
      R3 =                                                     .         and y we can write the joint probabilities for the three fea-
             0           sin(θ3 ) cos(θ3 )              0      
                                                                           tures as p(ei , x j , yk ) = p(ei |x j , yk )p(x j )p(yk ) where i, j, and
              sin(θ3 )        0            0          cos(θ3 )
                                                                           k ∈ {0, 1}. Causal power theory assumes the conditional
                                                                           probability of the effect given the causes is computed us-
The matrix R1 rotates the 2-dimensional subspaces associated
                                                                           ing a “noisy-or” equation: p(e1 |x j , yk ) = 1 − (1 − wx ) j (1 −
with the different feature values of the effect. That is, the
                                                                           wy )k (1 − wa ). Thus, five parameters are needed to define all
subspace associated with e1 (i.e., events e1 ∩ x1 and e1 ∩ x0 )
                                                                           eight joint probabilities in experiment 1: the three power pa-
and the subspace associated with e0 (i.e., events e0 ∩ x1 and
                                                                           rameters, wx , wy , and wa , and the prior probabilities of the
e0 ∩ x0 ). The matrix R2 rotates the 2-dimensional subspaces
                                                                           causes, p(x1 ) and p(y1 ). The eight joint probabilities can then
associated with the two values of the cause. That is, the
                                                                           be mapped directly to squared probability amplitudes. For ex-
subspace associated with x1 (i.e., events e1 ∩ x1 and e0 ∩ x1 )
                                                                           ample, α2111 = [1 − (1 − wx )(1 − wy )(1 − wa )]p(x1 )p(y1 ). As
and the subspace associated with x0 (i.e., events e1 ∩ x0 and
                                                                           in the 2 and 4-dimensional models, probabilities are calcu-
e0 ∩ x0 ). The matrix R3 rotates the 2-dimensional subspaces
                                                                           lated by projecting the state vector onto different subspaces.
defined by whether or not the feature values of the cause and
effect match. That is, the subspace associate with matching                Model Fits to Experiment 1
feature values (i.e., events e1 ∩ x1 and e0 ∩ x0 ) and the sub-
                                                                           We fit the three models to the mean judgments for the 40
space associated with mismatching feature values (i.e., events
                                                                           inference questions listed in table 1 by minimizing the sum
e1 ∩ x0 and e0 ∩ x1 ).
                                                                           of squared error (SSE) between each model and data. We
   As before, we assume there is a unit length vector |ψi in               then compared the models by using BIC values, which can be
the vector space representing an individual’s beliefs about the            converted into Bayes factors by eBICi −BIC j (Kass & Raftery,
different feature combinations
                         q          with coordinates α11 , α10 , α01 ,     1995). Using this approximation of the Bayes factor, the 2-
and α00 where α00 = 1 − (α211 + α210 + α201 ) in order to en-              dimensional model is strongly preferred to the 4-dimensional
sure the state vector is unit length. In total, the 4-dimensional          model and very strongly preferred to the 8-dimensional model
model has six parameters: θ1 , θ2 , θ3 , α11 , α10 , and α01 . As          (Kass & Raftery, 1995). Figure 2 compares model predic-
in the 2-dimensional model, probabilities are calculated by                tions to the observed data using the best fit parameters.
projecting the state vector onto different subspaces.
                                                                                       Predictions from the 2-D Model
8-dimensional model                                                        In fitting the data from experiment 1, the 2-dimensional
The 8-dimensional model assumes that individuals form men-                 model outperformed the 4 and 8 dimensional models with re-
tal representations of all joint events. Each of the eight dimen-          spect to BIC. However, a good model should not only fit ob-
sions is associated with one of the possible eight joint events            served data, but also generate new testable predictions. The
such as (e1 ∩ x1 ∩ y0 ) and (e0 ∩ x0 ∩ y1 ). This is equivalent            2-dimensional model makes two parameter free predictions
to a classical probability model with a single sample space                about invariances. The first is called reciprocity (also known
describing all of the events. That is, joint probabilities as              as the inverse fallacy (Villejoubert & Mandel, 2002)) and oc-
calculated using classical probability theory are equal to the             curs when the probability of the effect given a cause is the
probabilities that are calculated geometrically by projecting              same as the probability of the cause given the effect, e.g.
the state vector onto subspaces. Because all joint events ex-              p(e1 |x1 ) = p(x1 |e1 ). The second prediction is a memoryless
ist, there are no rotation matrices. The model simply assumes              effect where the probability of a feature only depends on the
there exists an 8-dimensional state vector where probabilities             most recent information given. For example, the memoryless
are calculated by projecting it onto different subspaces.                  property implies p(x1 |y1 ) = p(x1 |e1 , y1 ) since y1 is the most
   The state vector is composed of eight probability ampli-                recent given information. We developed a new experiment to
tudes α111 , α110 , α101 , α100 , α011 , α010 , α001 , and α000 where      test both predictions.
the indices represent the feature values of e, x, and y respec-
tively. There are seven degrees of freedom in the model since              Methods
the state vector must be unit length. The degrees of freedom               58 University of California, Irvine undergraduates partici-
can be reduced by adopting specific parameterizations. For                 pated in the experiment online for course credit. It was iden-
example, we can parameterize the model in accordance with                  tical to experiment 1 except we used a within subjects design
causal power.                                                              and asked participants to judge the probabilities listed in table
   In causal power, each cause i is associated with a power                2. Participants judged each probability twice in two different
parameter wi capturing the power of the cause to produce the               randomized blocks.
                                                                       1620

                                     2−Dimensional Model                                                             4−Dimensional Model                                               8−Dimensional Model (Causal Power)
                1                                                                               1                                                                             1
               0.9                                                                             0.9                                                                           0.9
               0.8                                                                             0.8                                                                           0.8
               0.7                                                                             0.7                                                                           0.7
               0.6                                                                             0.6                                                                           0.6
       Model                                                                           Model                                                                         Model
               0.5                                                                             0.5                                                                           0.5
               0.4                                                                             0.4                                                                           0.4
               0.3                                                                             0.3                                                                           0.3
               0.2                                                                             0.2                                                                           0.2
                                                         BIC = -186.41                                                                         BIC = -181.99                                                                 BIC = -174.20
               0.1                                                                             0.1                                                                           0.1
                0                                                                               0                                                                             0
                     0   0.1   0.2    0.3   0.4    0.5   0.6   0.7   0.8   0.9     1                 0   0.1   0.2    0.3   0.4    0.5   0.6   0.7   0.8   0.9   1                 0    0.1   0.2   0.3   0.4    0.5   0.6   0.7   0.8   0.9   1
                                                  Data                                                                            Data                                                                          Data
Figure 2: Predicted values compared to observed data from experiment 1 for three models. The 2-D model has the lowest BIC.
                                                                                                                                         produce and distribute reprints for Governmental purpose
                         Table 2: Judgments in Experiment 2.
           Probabilities                                                         Bayes Factor                                            notwithstanding any copyright notation thereon.
 reciprocity p(x1 |e1 )                             p(e1 |x1 )                      5.45
             p(y1 |e1 )                             p(e1 |y1 )                      6.59                                                                                                 References
 memoryless p(x1 |y1 )                              p(x1 |e1 , y1 )                 2.40                                                 Cheng, P. W. (1997). From covariation to causation: A causal
             p(x1 |y0 )                             p(x1 |e1 , y0 )                 1.68
             p(y1 |x1 )                             p(y1 |e1 , x1 )                 1.65
                                                                                                                                           power theory. Psychological Review, 104, 367-405.
             p(y1 |x0 )                             p(y1 |e1 , x0 )                 6.93                                                 Fernbach, P. M., Darlow, A., & Sloman, S. A. (2011). Asym-
                                                                                                                                           metries in predictive and diagnostic reasoning. Journal of
                                                                                                                                           Experimental Psychology: General, 140 (2), 168-185.
Results                                                                                                                                  Fernbach, P. M., & Sloman, S. A. (2009). Causal learning
                                                                                                                                           with local computations. Journal of Experimental Psychol-
The average score on the 10 question multiple choice test was
                                                                                                                                           ogy: Learning, Memory, and Cognition, 35, 678-693.
7.8. We excluded 17 participants with scores less than 7 out
                                                                                                                                         Hogarth, R. M., & Einhorn, H. J. (1992). Order effects in
of ten. For our analyses, we averaged each individual’s judg-
                                                                                                                                           belief updating: The belief-adjustment model. Cognitive
ments from the two blocks. Because invariances correspond
                                                                                                                                           Psychology, 24, 1–55.
to null hypotheses and it is not possible to state evidence for
                                                                                                                                         Jenkins, H. M., & Ward, W. C. (1965). Judgment of con-
the null hypothesis in standard significance testing, we cal-
                                                                                                                                           tingency between responses and outcomes. Psychological
culated the Bayes factor for each comparison in table 2 fol-
                                                                                                                                           Monographs: General and Applied, 79, 1-17.
lowing Rouder, Speckman, Sun, Morey, and Iverson (2009).
                                                                                                                                         Kass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal
Bayes factors greater than 1 tend to favor the null hypothesis
                                                                                                                                           of the American Statistical Association, 90(430), 773-795.
and values greater than 3 are considered positive evidence for
                                                                                                                                         Rehder, B. (2003). Categorization as causal reasoning. Cog-
the null hypothesis (Kass & Raftery, 1995). In general, the
                                                                                                                                           nitive Science, 27, 709-748.
predictions from the 2-dimensional model are supported.
                                                                                                                                         Rouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., &
                                                  Discussion                                                                               Iverson, G. (2009). Bayesian t-tests for accepting and re-
                                                                                                                                           jecting the null hypothesis. Psychonomic bulletin & review,
The 2-dimensional model predicts that individuals break up                                                                                 16(2), 225–237.
complex reasoning problems into several simpler pieces.                                                                                  Shanks, D. R. (1991). On similarities between causal judg-
Judgments are made by sequentially processing these indi-                                                                                  ments in experienced and described situations. Psycholog-
vidual pieces. In causal situations with vague and imperfect                                                                               ical Science, 2(5), 341–350.
information, such as the experiments discussed in this pa-                                                                               Sloman, S. A., & Fernbach, P. M. (2011). Human represen-
per, a mental representation without joint events might be a                                                                               tation and reasoning about complex causal systems. Infor-
simpler and more efficient way to evaluate events. Perhaps                                                                                 mation, Knowledge, Systems Management, 10, 1-15.
in situations where individuals have the opportunity to learn                                                                            Trueblood, J. S., & Busemeyer, J. R. (2011). A quantum
about casual relationships or are given statistical descriptions                                                                           probability account of order effects in inference. Cognitive
of these relationships, they can build more complete mental                                                                                Science, 35, 1518-1552.
representations which include more joint events.                                                                                         Trueblood, J. S., & Busemeyer, J. R. (2012). A quantum
                                                                                                                                           probability model of causal reasoning. Frontiers in Cogni-
                                      Acknowledgments                                                                                      tive Science, 3, 1-13.
JST was supported by NSF grant SES-1326275. EMP                                                                                          Villejoubert, G., & Mandel, D. R. (2002). The inverse fal-
was supported by Leverhulme Trust grant RPG-2013-00 and                                                                                    lacy: An account of deviations from bayes’s theorem and
AFOSR, Air Force Material Command, USAF, grant FA                                                                                          the additivity principle. Memory & Cognition, 30(2), 171–
8655-13-1-3044. The U.S Government is authorized to re-                                                                                    178.
                                                                                                                              1621

