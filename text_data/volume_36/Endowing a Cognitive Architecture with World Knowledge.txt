UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Endowing a Cognitive Architecture with World Knowledge
Permalink
https://escholarship.org/uc/item/6hg94304
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Author
Salvucci, Dario
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                      Endowing a Cognitive Architecture with World Knowledge
                                            Dario D. Salvucci (salvucci@drexel.edu)
                                        College of Computing & Informatics, Drexel University
                                            3141 Chestnut St., Philadelphia, PA 19104 USA
                              Abstract                                they include the cognitively plausible properties—such as
                                                                      the accessibility of knowledge elements—that some
   Although computational models developed in cognitive               architectures rely on for modeling cognition (see Ball,
   architectures are often rich in their knowledge of procedural
   skills, they are often poor in their knowledge of declarative      Rodgers, & Gluck, 2004, for further discussion). More
   facts about the world. This work endows the ACT-R                  recent efforts to create knowledge bases for cognitive
   cognitive architecture with world knowledge derived from           architectures (e.g., Douglass & Myers, 2010; Derbinsky,
   Wikipedia, compiling a knowledge base of over 37 million           Laird, Smith, 2010; Emond, 2006) have explored the
   declarative facts that can be accessed by a cognitive model        practical challenges inherent in such work, especially in
   via standard memory retrievals. Estimates of the accessibility     understanding and reducing the computational demands of
   of these facts are also derived from Wikipedia text, allowing
                                                                      retrieving information from a large-scale database.
   ACT-R to utilize the likelihood of knowing a fact and
   associations between related facts. Integration with a simple         This project uses the Wikipedia knowledge base to derive
   procedural model demonstrates how the knowledge base may           a declarative database for the ACT-R cognitive architecture
   serve not only to answer simple factual questions, but also to     (Anderson, 2007), complete both with tens of millions of
   disambiguate among multiple possible meanings based on             world-knowledge facts and with estimates of the
   context. The resulting knowledge base can be queried quickly       accessibility (activation) of these facts. In doing so, the
   (typically well under one second) and is easily generalizable      project addresses theoretical challenges (e.g., an appropriate
   to other cognitive architectures.
                                                                      representation of these facts) and practical challenges (e.g.,
   Keywords: Cognitive architectures; Wikipedia; ACT-R                computational efficiency) in a way that generalizes to other
                                                                      cognitive architectures beyond ACT-R.
                          Introduction
Cognitive architectures, particularly production-system                            Declarative Knowledge Base
architectures (e.g., Anderson, 2007; Laird, Newell, &                 Wikipedia [http://www.wikipedia.org] is the largest open
Rosenbloom, 1987; Meyer & Kieras, 1997; Newell, 1990),                body of general knowledge on the Internet today, with over
have been used for a number of years as a computational               4 million articles in English alone, written by thousands of
framework for representing human cognition and behavior.              active contributors. Both its breadth of topics and its open
Researchers have employed such architectures to model                 licensing makes Wikipedia extremely amenable to use as a
behavior in a large array of task domains. The vast majority          knowledge base for cognitive modeling. Unfortunately, the
of these models were developed with an emphasis on the                primary content of Wikipedia comes in the body of its full-
procedural skills necessary to perform particular tasks; for          text articles, and until cognitive architectures have large-
instance, models have been developed to simulate behavior             scale robust natural-language capabilities, they cannot make
in the domains of piloting (Jones et al., 1999), game playing         direct use of such articles. Fortunately, other aspects of the
(Laird, 2002; Taatgen et al., 2003), and driving (Salvucci,           Wikipedia knowledge base are available in representations
2006). At the same time, these models often have minimal              that more easily interface with modern architectures.
declarative, factual knowledge; while they may include tens
of facts to represent, say, the addition tables up to 9+9, they       Knowledge Content
typically have little to no general knowledge about the               The primary content for this work comes from the DBpedia
world—for instance, what is the capital of Pennsylvania, or           [http://www.dbpedia.org] project, which extracts and
who invented the light bulb, or what sport is played by the           disseminates structured representations of Wikipedia
Pittsburgh Steelers.                                                  knowledge. Specifically, DBpedia makes available several
   This project aims to develop a large-scale knowledge base          large datasets that served useful in building a knowledge
that can easily be integrated into cognitive architectures to         base for cognitive architectures. The datasets, and the
provide models with general world knowledge. Although                 resulting knowledge arising from them, are described here.
past efforts have created large-scale knowledge databases
(e.g., Cyc: Lenat, 1994; Scone: Fahlman, 2006; WordNet:               Relations. The first dataset includes information from
Miller, 1995), these databases do not necessarily integrate           Wikipedia “infoboxes” that appear alongside the full-text
easily with a cognitive architecture: they cannot be accessed         articles and provide knowledge in terms of relations. Table
in a straightforward way from a production system, nor do             1 shows the (partial) infobox for “Harrison Ford” as it
                                                                  1353

appears in Wikipedia, including basic information about his        name is redirected (forwarded) to a common page. Some of
life and work. The DBpedia project extracts Wikipedia              the redirects are intended for misspelled entries (e.g.,
infobox content and cleans up these data based on the              “Harison Ford”) or entries with variant spellings
DBpedia ontology of objects, ensuring that key attributes          (“Muammar Qaddafi” or “Gadaffi” redirecting to
are handled in a uniform way (e.g., all birthdates are             “Muammar al-Gaddafi”). But the redirects also encode
translated to a common format associated with the attribute        important differences in how people refer to common
“birth date”). The cleaned version of this information is          objects, such as nicknames (“Bill” for “William”, “Big
included in Table 1. This version comprises relations as           Apple” for “New York City”). The redirect database is
object-attribute-value triplets: objects (“Harrison Ford”)         incorporated into the knowledge base via a “name”
with attributes from the ontology (“spouse”) and values for        attribute; for example, with “Jimmy Stewart” as the value of
these attributes (“Calista Flockhart”). Note that, in some         the name attribute for the object represented canonically as
cases, two sets of triplets may encode redundant information       the symbol James_Stewart_(actor).
(e.g., Harrison Ford’s spousal relationship to Calista
                                                                      The final knowledge base comprises 11,862,387 unique
Flockhart). These data serve as the core knowledge for this
                                                                   symbols and 37,100,782 facts (including all relations, types,
effort, with a wide variety of object, attributes, and values.
                                                                   and names as object-attribute-value triplets).
[Note that the triplets here are equivalent to a predicate-
argument-value representation like spouse(HarrisonFord) =
CalistaFlockhart.]
                                                                   Representation and Implementation
                                                                   The integration of the knowledge base into the ACT-R
     Table 1: Sample infobox and relation representation.          architecture brings up two important issues, one theoretical
                                                                   and one practical. The critical theoretical issue is one of
   Infobox [Wikipedia]                                             representation. Retrievals of declarative facts, or so-called
   Born:               July 13, 1942 (age 71)                      “chunks,” in ACT-R arise from requests to the architecture’s
                       Chicago, Illinois, U.S.                     memory resource (see Anderson, 2007). ACT-R declarative
   Occupation:         Actor, producer                             chunks are typically modeled in representations such as:
   Years active:       1966–present                                     Harrison Ford
   Spouse:             Calista Flockhart (2010–present)                     isa       actor
                                                                            film      Star Wars IV
   Relation Representation [DBpedia]                                        spouse Calista Flockhart
   Harrison Ford             isa          actor                    When an ACT-R model retrieves such a chunk, it provides a
   Harrison Ford             isa          producer                 partial pattern that specifies some or all of the attributes and
   Harrison Ford             isa          person                   values, and the memory resource chooses and returns the
   Harrison Ford             birth date   1942-07-13               best-matching chunk (explained further in the next section).
   Harrison Ford             birth place  Chicago                  Although this representation works as needed for many
                                                                   domains (especially those that are not knowledge-intensive),
   Harrison Ford             spouse       Calista Flockhart        it is less desirable than the previously described triplet
   Calista Flockhart         spouse       Harrison Ford            representation in two ways. First, ACT-R chunks can only
   Star Wars Episode IV      starring     Harrison Ford            have one value for a particular attribute, and thus in the fact
                                                                   above, multiple films cannot be included in the same chunk
   Raiders of the Lost Ark starring       Harrison Ford            representation. Second, retrieval of an entire chunk is overly
   Witness                   starring     Harrison Ford            powerful: once the model recalls one bit of information
                                                                   about the object (say, that Harrison Ford starred in Star
                                                                   Wars), it immediately has access to all bits of information
Types. DBpedia also provides information about the                 (including the name of his spouse, his place of birth, etc.).
ontology types of Wikipedia objects. In essence, these types          In contrast, the triplet representation can incorporate
can be thought of as the categories to which the objects           multiple values for an attribute, and can account for
belong—very much analogous to the “isa” relationship               variations in the accessibility of knowledge for different
common to cognitive architectures and artificial intelligence      units of information about a particular object. For these
frame representations. For example, “Harrison Ford” is             reasons, the current implementation of the proposed
listed as belonging to three categories (“actor”, “producer”,      knowledge base stores and retrieves chunks in the triplet
and “person”) and thus these three “isa” relationships             representation defined earlier—that is, each chunk is
included in the object-attribute-value triplets in Table 1.        represented like the following:
This information is critical in providing the knowledge base
with an understanding of object membership in categories.               Fact-1234
                                                                            object    Harrison Ford
Names. A third dataset available through DBpedia is the list                attribute spouse
of Wikipedia “redirects,” whereby the entry of a particular                 value     Calista Flockhart
                                                               1354

   As a practical issue, the implementation of the knowledge      representing a less widely known musician. ACT-R posits
database must allow for flexible queries that mimic the           that base-level activation changes as that chunk of
architecture’s memory processing, must be fast enough to          information is used, or neglected, over time. Specifically,
ensure reasonable simulation times, but must be sufficiently      the base-level activation B of a concept can be approximated
lightweight to facilitate portability and use across systems.     as follows (Anderson, 2007):
For these reasons, SQLite [http://www.sqlite.org] was
                                                                                      B = ln(n/(1-d)) – d*ln(L)
chosen as the back-end database for the project (most
similar to Douglass, Ball, & Rodgers, 2009), and Java ACT-        In this equation, n is the number of times the chunk has
R [http://cog.cs.drexel.edu/act-r] was chosen as the front-       been used (i.e., created or retrieved by the memory system);
end implementation of ACT-R. The integration with the             L is the lifetime of the chunk (the time since chunk
ACT-R architecture strived to make the new knowledge              creation); and d is a decay parameter. We assume that L has
base transparent to the cognitive model, in that retrievals       a constant value for all chunks in the knowledge base (i.e.,
were requested and processed in the normal way. The               that they were all created at roughly the same long-ago
implementation in Java ACT-R uses a “extended memory”             time), and because all computations in the remainder of this
module to augment the standard memory module: when no             paper will only need to compare chunks, we ignore the
chunk satisfying a retrieval request is found in ACT-R’s          constant second term in the equation. In addition, we
standard memory, the system accesses the full SQLite              assume the ACT-R default value of 0.5 for d. Thus, the
database for retrieval. (In principle, all memory elements        equation simplifies to:
could be stored in the database; however, having a two-level
approach with standard and extended memory greatly                                             B = ln(2n)
facilitates the code, and allows computation of chunk                The knowledge base assumes that Wikipedia links can
properties to be performed only on the recently created           serve as a conceptual surrogate to chunk usage in ACT-R.
chunks in standard memory, described shortly.)                    Specifically, the number of links to a particular Wikipedia
                                                                  concept can be treated as roughly proportional to the
Estimation of Knowledge Accessibility                             number of times a person would encounter and recall the
The core knowledge base described above contains a great          chunk associated with that concept (e.g., the number of
many facts, but does not distinguish among them in terms of       times a person would encounter a thought or perceptual
accessibility for an average person; the most commonly            input about “Bob Dylan”). Thus, for a given chunk relation,
known people, places, and so on (e.g., Bob Dylan,                 we set n to the number of times the relation’s object appears
Muhammad Ali, New York City) are not treated any                  in the triplet slots of any chunk, and compute B using this
differently than the (much more numerous) scarcely known          value. For example, the base-level activation of the sample
people, places, and so on. In contrast, facts in the human        chunk Fact-1234 shown earlier (representing that Harrison
memory system may be more or less accessible, and we              Ford’s spouse is Calista Flockhart) would be set according
would like the computational knowledge base to reflect this       to the number of times the symbol Harrison_Ford appears
feature. Of course, individuals may differ themselves with        in all chunks. This process makes an assumption that each
respect to accessibility of particular knowledge: most people     chunk with a given object (such as Harrison_Ford) is
may be able to name, say, only a few Civil War battles,           equally accessible. Of course, there are several ways in
whereas a history buff might be able to name a great many         which this assumption might not be accurate—for instance,
more. To maintain simplicity for this first effort, the           Harrison Ford’s birth date or birthplace may not be as
proposed knowledge base aims to represent the accessibility       widely known as his spouse.1 Nevertheless, the assumption
of knowledge for an “average” person.                             provides a good baseline for accessibility, as demonstrated
   Accessibility of knowledge, when instantiated in the           in the upcoming examples.
ACT-R architecture, can be broken down into two primary           Associative Activation. Whereas base-level activation
components: base-level activation representing general            represents a chunk’s overall accessibility, ACT-R also
accessibility, and associative activation representing            posits that a chunk can receive additional activation from
accessibility based on the current task context. For the          associated chunks in the current task context. In ACT-R,
proposed knowledge base, both quantities are derived from         “context” is defined as the other chunks in the processing
Wikipedia’s infobox link structure, using links as a              buffers, especially those in the “imaginal” buffer that serves
surrogate for the strengths of, and relationships among,          as a working scratchpad of information for the current task.
knowledge elements. Each component is detailed below.             First, we define a strength of association Sji between
Base-Level Activation. For each factual chunk, ACT-R              symbols i and j as
maintains a base-level activation that represents the chunk’s                             Sji = Smax – ln(fanj)
general accessibility: a chunk with a higher base-level
activation is more accessible than another with a lower base-
level activation. For example, the chunk representing a well-     1
                                                                    Note also that the accessibility of the object and value cannot be
known musician (e.g., Bob Dylan) would, for most people,          combined; many people familiar with both Harrison Ford and
have a higher base-level activation than a chunk                  Chicago may not know that the actor was born in Chicago.
                                                              1355

where fanj is the number of other chunks that contain                  One useful way to understand the interactions of
symbol j in one of its slots. For example, the symbol                declarative and procedural knowledge in the model is to
Chicago would have a relatively high fan value, since, as a          examine the behavior of the whole system for illustrative
populous and popular city, it is referenced in a relatively          examples. We present a number of examples below.
large number of other chunks. Smax represents a value larger
                                                                     "What is the capital of Pennsylvania?"
than all values ln(fanj) (currently set at 20).
                                                                     For this straightforward question, the model processes each
   When attempting to retrieve a relation chunk, the system
                                                                     word in order, mapping each to an appropriate semantic
first identifies all potential matches for the given pattern and
                                                                     symbol and finally attempting to retrieve a relation chunk
sets their initial activations as their base-level activations
                                                                     with object Pennsylvania and attribute capital. The correct
described earlier. Next, it spreads associative activation
                                                                     answer is successfully retrieved and used to generate a
from the current context: for all symbols j in the imaginal
                                                                     spoken response to the question (“Harrisburg”).
buffer, if any potential matches contain a symbol i for which
Sji is non-zero, the value Sji is added to its activation. For       "What is Philadelphia?"
example, Harrison_Ford and Chicago appear in the same                This deceptively simple question illustrates the workings of
chunk; therefore, if Chicago appears in the current context,         the base-level activations in the knowledge base. Although
it will spread activation to any potentially matching chunk          most people would associate “Philadelphia” with the city in
that includes Harrison_Ford.                                         Pennsylvania, this term can refer to other things as well; in
                                                                     fact, the knowledge base contains 8 possible mappings of
                  Procedural Knowledge                               this term (to Philadelphia, NY or IN; to the film or
                                                                     magazine with this name; and so on). The base-level
Although the declarative knowledge base is the focus of this
                                                                     activation of Philadelphia, PA, however, is more than twice
work, we require procedural knowledge to demonstrate how
                                                                     that of any of the other interpretations, and thus the model
the declarative knowledge can be retrieved in realistic and
                                                                     retrieves its semantic symbol as the assumed interpretation
useful ways. To this end, this work includes a cognitive
                                                                     and responds with its isa properties (“city”, “place”, etc.).
model with a simple production system that understands and
responds to basic questions about common facts. The model            "Name a musician."
takes a similar approach to earlier work on sentence                 This open-ended request also demonstrates the importance
processing in ACT-R (e.g., Anderson, Budiu, & Reder,                 of base-level activation in the model’s responses. Although
2001; Lewis & Vasishth, 2005).                                       there are 37,872 musicians identified in the knowledge base,
   The model parses and responds to a given question as              most are not familiar to most people. Guided by base-level
follows. First, the model listens to a question word-by-word,        activation, however, the model’s first responses are well-
and when encountering a lexical item (word or logical                known musicians (though certainly their exact ordering is
phrase) through vision or audition, the system associates the        debatable and would realistically be variable among
item to a semantic symbol by retrieving a name relation              individuals): “David Bowie”, “Prince”, “Bob Dylan”,
chunk; for instance, the phrase “Harrison Ford” initiates a          “Kanye West”, and “James Brown”.
retrieval for the symbol with that name, that is, the symbol
Harrison_Ford. Note that often, a single phrase can map to           "What is the population of Philadelphia?"
different symbols, such as “New York” to the city or the             "Who is the director of Philadelphia?"
state; both base-level activation and associative activation         Because the model processes lexical items in order, items
play a critical role here in resolution of ambiguity, as seen in     encountered earlier in the sentence can guide interpretation
the examples shortly.                                                of later items because of associative activation from the
   Second, the model places the found symbol into the                current context. In the first question above, the term
imaginal buffer in a slot associated with its role in the            “population” spreads associative activation to the city
sentence (e.g., subject, verb, object). This basic parser does       “Philadelphia”, although as noted in the previous example,
not attempt to form a parse tree, but rather fills out a simple      this interpretation is already the dominant one because of
flat structure with the noted grammatical elements. When             base-level activation. In the second question, the term
the entire question has been encoded, the model performs a           “director” spreads activation to a different interpretation,
retrieval to answer the question based on the structure of the       that of the film; this associative activation, when added to
question; for example, “What is the capital of                       base-level activation, makes the film interpretation more
Pennsylvania?” would eventually lead to a retrieval request          active than the city, and Philadelphia_(film) is retrieved as
for a chunk with object Pennsylvania and attribute capital.          the semantic symbol for this term. As a result, the model
Again, as for retrieval of a lexical item’s semantic symbol,         answers each question correctly (“1,526,006” and “Jonathan
retrieval of the question’s answer is guided by both base-           Demme” respectively).
level and associative activations. The model uses the                "Who is the author of No_Country_for_Old_Men?"
retrieved chunk to respond verbally to the question. Because         "Who is a star of No_Country_for_Old_Men?"
some questions have multiple answers, the model will                 There are many examples for which associative activation
attempt a few additional retrievals (suppressing recently            helps in understanding and responding to a question. The
retrieved items) and generate those responses as well.               examples above demonstrate the resolution of an ambiguity
                                                                 1356

with respect to the book versus film version of “No Country                             General Discussion
for Old Men.” The term “author” activates the correct
                                                                    Whereas most efforts related to cognitive architectures like
response for the book (“Cormac McCarthy”). The term
                                                                    ACT-R have focused primarily on procedural knowledge,
“star” activates the film interpretation, and the model in fact
                                                                    the work here aims to develop a usable large-scale
generates three responses in the order of their base-level
                                                                    declarative knowledge base for easy integration with
activations (“Tommy Lee Jones”, “Javier Bardem”, “Josh
                                                                    existing models. From a theoretical standpoint, this work is
Brolin”)—a measure of their overall familiarity (quantified
                                                                    somewhat atypical in that it does not compare data directly
by references within the knowledge base) as opposed to the
                                                                    to human behavior and performance. Nevertheless, its
importance of their roles within the film (which is not
                                                                    important theoretical contribution is the demonstration that
encoded in any way in the knowledge base).
                                                                    ACT-R’s memory constructs—specifically base-level and
"What actor is a star of Airplane?"                                 associative activation—scale well to very large knowledge
"What athlete is a star of Airplane?"                               bases. Although ACT-R’s base-level and associative
In these two examples, associative activation from the              activation calculations have always been assumed to operate
context guides both the retrieval of semantic information           over the entire span of memory, the vast majority of models
and retrieval of the response itself. The term “star” helps the     include only tens or hundreds of chunks, a number too small
model disambiguate the meaning of the term “Airplane”,              to thoroughly test this assumption. In contrast, the work
mapping this term to the film Airplane!. When the model             here calculates base-level and associative activations from a
attempts to retrieve a chunk for a star of Airplane!, the terms     more realistic set of tens of millions chunks, and
“actor” and “athlete” appear in the current context (in ACT-        demonstrates that such activations can guide cognitive
R terms, in the imaginal buffer) and spread associative             processing to produce reasonable interpretations of
activation to particular responses. Thus, the term “actor”          questions and generation of familiar responses. This is a
guides the response to those identified as actors in the            critical step for cognitive architectures: as architectures gain
knowledge base (“Robert Hays”, “Leslie Nielsen”, etc.),             in their ability to learn and expand their procedural
while the term “athlete” guides the response to the                 knowledge base (e.g., Salvucci, 2013; Taatgen, 2013), they
prominent athlete in the film (“Kareem Abdul-Jabbar”).              will require an equally powerful declarative knowledge base
                                                                    with which to reason about the world.
"What is Jackson the capital of?"                                      From an engineering standpoint, this work aims to show
"What film is Robert_De_Niro the star of?"                          that modern architectures can successfully simulate
Because of the flexible nature of the triplet representation,       cognition and behavior with large-scale knowledge bases.
the model can retrieve responses from the attribute and             There have been several efforts to incorporate large-scale
value just as well as from the object and attribute. The two        knowledge into production-system cognitive architectures
questions above are reversals of earlier examples, providing        (e.g., Ball, Rodgers, & Gluck, 2004; Douglass, Ball, &
the city (instead of the state) and the actor (instead of the       Rodgers, 2009; Douglass & Myers, 2010; Emond, 2006);
film). In both cases, the model is able to retrieve the same        the work here represents by far the largest effort to date
relation chunks and respond to the questions (“Mississippi”         (over 37 million chunks). In addition, some efforts have
and “The Godfather Part II”, “Taxi Driver”, etc.).                  focused specifically on real-time performance of memory
"Where is the Baseball_Hall_of_Fame?"                               retrieval mechanisms (e.g., Derbinsky, Laird, Smith, 2010;
"Who is Theodore_Geisel?"                                           Douglass, Ball, & Rodgers, 2009; Douglass & Myers, 2010;
The inclusion of names and aliases in the knowledge base is         Laird, Derbinsky, Voigt, 2011). Although real-time speed is
critical in that it allows the model to understand commonly         not the primary goal of the work here, it should be noted
used aliases for semantic items. The term “Baseball Hall of         that the model performs almost all retrievals in well less
Fame” maps to its canonical representation National_                than one second. (Real-time latency is primarily a function
Baseball_Hall_of_Fame_and_Museum, yielding the correct              of the number of potentially matching chunks, so only open-
response (“Cooperstown, New York”). Similarly,                      ended retrievals like “isa musician” typically take more than
“Theodore Geisel” maps to his more commonly recognized              one second of real time.) Another benefit of the current
alias, “Dr. Seuss”, and the model responds accordingly              knowledge base is its portability to other architectures: the
(“cartoonist”, “writer”, etc.).                                     flexible representation of knowledge and implementation in
                                                                    a commonly used database format greatly facilitate use and
"What actor was born in Philadelphia?"                              extension by other frameworks.
"What musician was born in New_Jersey?"                                Moving forward, one of the major challenges with this
Again we see the role of the various activations at play in         effort is in more flexible processing of concepts and more
these examples: context helps to retrieve the appropriate           general natural language understanding. For example, one
semantic item for “Philadelphia”; associative activation            can imagine cases in which the first retrieved interpretation
guides the response to an actor or musician; and base-level         for a lexical item is not the correct one, and the model might
activations guide the response to the most familiar names           re-retrieve alternative interpretations; such an extension
(“Bill Cosby” as the top response for the first question,           could be incorporated into the procedural knowledge for
“Bruce Springsteen” for the second).                                helping to disambiguate sentences. More generally, a more
                                                                1357

complex inference engine as embodied in other systems             Fahlman, S. E. (2006). Marker-passing inference in the
(e.g., Bello & Cassimatis, 2006; Cassimatis, 2006; Lenat,           scone knowledge-base system. In Knowledge Science,
1995) has not yet been attempted here, and a translation of         Engineering and Management (pp. 114-126). Springer
these ideas into ACT-R would be a difficult but worthwhile          Berlin Heidelberg.
effort to make further use of the large-scale knowledge base.     Jones, R. M., Laird, J. E., Nielsen P. E., Coulter, K., Kenny,
                                                                    P., & Koss, F. (1999). Automated intelligent pilots for
                   Acknowledgments                                  combat flight simulation. AI Magazine, 20, 27-42.
                                                                  Laird, J. E. (2002). Research in human-level AI using
This work is supported by Office of Naval Research grant
                                                                    computer games. Communications of the ACM, 45, 32-35.
#N000140910096.
                                                                  Laird, J. E., Derbinsky, N., Voigt, J. (2011). Performance
                                                                    evaluation of declarative memory systems in Soar.
                        References                                  Proceedings of BRIMS 2011.
Anderson, J. R. (2007). How can the human mind occur in           Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987). Soar:
  the physical universe? New York: Oxford University                An architecture for general intelligence. Artificial
  Press.                                                            Intelligence, 33, 1–64.
Anderson, J. R., Budiu, R., & Reder, L. M. (2001). A theory       Lenat, D. B. (1995). CYC: A large-scale investment in
  of sentence memory as part of a general theory of                 knowledge infrastructure. Communications of the ACM,
  memory. Journal of Memory and Language, 45, 337–367.              38, 32-38.
Ball, J., Rodgers, S., & Gluck, K. (2004). Integrating ACT-       Lewis, R. L., & Vasishth, S. (2005). An activation-based
  R and Cyc in a large-scale model of language                      model of sentence processing as skilled memory retrieval.
  comprehension for use in intelligent agents. In Papers            Cognitive Science, 29, 375– 419.
  from the AAAI Workshop, Technical Report WS-04-07,              Meyer, D. E., & Kieras, D. E. (1997). A computational
  (pp. 19-25). Menlo Park, CA: AAAI Press.                          theory of executive cognitive processes and multiple-task
Bello, P., & Cassimatis, N. (2006). Developmental accounts          performance: Part 1. Basic mechanisms. Psychological
  of theory-of-mind acquisition: Achieving clarity via              Review, 104, 3–65.
  computational cognitive modeling. In Proceedings of the         Miller, G. A. (1995). WordNet: A lexical database for
  Twenty-Eighth Annual Conference of the Cognitive                  English. Communications of the ACM, 38, 39-41.
  Science Society (pp. 1014-1019).                                Newell, A. (1973). You can’t play 20 questions with nature
Cassimatis, N. L. (2006). A cognitive substrate for                 and win: Projective comments on the papers of this
  achieving human-level intelligence. AI Magazine, 27, 45-          symposium. In Visual Information Processing (pp. 283-
  55.                                                               308). New York: Academic Press.
Derbinsky, N., Laird, J. E., Smith, B. (2010). Towards            Newell, A. (1990). Unified theories of cognition.
  efficiently Supporting large symbolic declarative                 Cambridge, MA: Harvard University Press.
  memories. Proceedings of the Tenth International                Salvucci, D. D. (2006). Modeling driver behavior in a
  Conference on Cognitive Modeling.                                 cognitive architecture. Human Factors, 48, 362-380.
Douglass, S., Ball, J., & Rodgers, S. (2009). Large               Salvucci, D. D. (2013). Integration and reuse in cognitive
  declarative memories in ACT-R. In Proceedings of the              skill acquisition. Cognitive Science, 37, 829-860.
  9th International Conference of Cognitive Modeling.             Taatgen, N.A. (2013). The nature and transfer of cognitive
Douglass, S. A., & Myers, C. W. (2010). Concurrent                  skills. Psychological Review, 120, 439-471.
  knowledge activation calculation in large declarative           Taatgen, N.A., van Oploo, M., Braaksma, J. &
  memories. In Proceedings of the 10th International                Niemantsverdriet, J. (2003). How to construct a
  Conference on Cognitive Modeling (pp. 55-60).                     believable opponent using cognitive modeling in the
Emond, B. (2006). WN-LEXICAL: An ACT-R module                       game of set. In Proceedings of the Fifth International
  built from the WordNet lexical database. In Proceedings           Conference on Cognitive Modeling (pp. 201-206).
  of the Seventh International Conference on Cognitive              Bamberg: Universitätsverlag Bamberg.
  Modeling.
                                                              1358

