UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Bounded Rationality Account of Wishful Thinking
Permalink
https://escholarship.org/uc/item/3s74k58c
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Neuman, Rebecca
Rafferty, Anna
Griffiths, Thomas
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                          A Bounded Rationality Account of Wishful Thinking
                                        Rebecca Neumann (beckieneumann@gmail.com)
                           Cognitive Science Program, University of California, Berkeley, CA 94720 USA
                                          Anna N. Rafferty (rafferty@cs.berkeley.edu)
                           Computer Science Division, University of California, Berkeley, CA 94720 USA
                                       Thomas L. Griffiths (tom griffiths@berkeley.edu)
                           Department of Psychology, University of California, Berkeley, CA 94720 USA
                              Abstract                                 agent performance in practice (e.g. Munos & Teytaud, 2006),
                                                                       although theoretical bounds show that this effect is not guar-
   People tend towards wishful thinking, in which they overesti-
   mate the probability of favorable outcomes and underestimate        anteed (Coquelin & Munos, 2007).
   the probability of unfavorable outcomes. Many explanations             We use Markov decision processes to determine whether
   for this phenomenon focus on its irrationality. We explore          these advantages for optimism in exploring actions with un-
   whether wishful thinking could actually help people make bet-
   ter decisions given that they have limited cognitive resources.     known consequences might also hold for the problem of
   We consider a situation in which multiple decisions must be         choosing actions when one has limited cognitive resources.
   made over a period of time, where the consequences of these         Using a simple decision making situation in which an agent
   decisions are not fully determined. We model this situation as
   a Markov decision process, and incorporate limited cognitive        must make choices about whether to keep or quit its job over
   resources by varying the amount of time in the future that the      a period of time, we examine how an optimistic bias affects
   agent considers the consequences of its decisions. Through          the agent’s performance when it can only consider the con-
   simulations, we show that with limited cognitive resources,
   this model can exhibit better performance by incorporating a        sequences of its actions for a limited period of time into the
   bias towards wishful thinking. This advantage occurs across         future. The limited horizon that agents consider mimics peo-
   a range of decision-making environments, suggesting that the        ple’s limited computational resources. In our simulations, we
   same effect could be applicable to many real life scenarios.
                                                                       find that agents with moderate optimism perform better than
   Keywords: rational process models; Markov decision pro-
   cesses                                                              agents with no bias when the horizon is relatively small com-
                                                                       pared to the true time period over which agents can act.
                          Introduction                                    We begin by describing existing theories for the wishful
People tend to overestimate the probability that their pre-            thinking bias, and background about the Markov decision
ferred outcomes will occur and to underestimate the proba-             processes that we use to model human action planning. We
bility of non-preferred outcomes (e.g., Camerer & Lovallo,             then demonstrate how we model limited computational re-
1999; Larwood & Whittaker, 1977; Lyles & Thomas, 1988;                 sources by restricting the horizon that agents consider when
Svenson, 1981; Weinstein, 1980). This “wishful think-                  choosing actions. Through four simulations, we determine
ing” or optimistic bias occurs in a variety of situations,             under what environmental and computational constraints an
from estimating the likelihood of a desired candidate win-             optimistic bias improves performance.
ning an election (Babad, 1997) to predicting one’s future
salary (Weinstein, 1980). This phenomenon seems irrational:
                                                                                               Background
people have unrealistic expectations, and these expectations           We briefly review existing theories about the wishful thinking
could lead to risky choices. Most explanations of wishful              bias and provide an overview of Markov decision processes.
thinking have focused on it as an irrational cognitive bias.
   While wishful thinking appears to be detrimental, we ex-            Wishful Thinking
plore whether this bias could be a rational strategy given peo-        Several theories of wishful thinking have been proposed that
ple’s limited cognitive resources. Determining the best de-            aim to explain why this bias occurs and how prevalent it
cision in a given situation requires considering all possible          is, although there remain significant gaps in the theories ac-
outcomes and their long range consequences. Yet, consid-               counting for the phenomenon (Krizan & Windschitl, 2007).
ering all of these possibilities is computationally intractable.       Ego-utility theory suggests that the bias protects the agent’s
Wishful thinking might help to compensate for the fact that            self image, and thus should only occur in situations that af-
people cannot fully evaluate the long term consequences of             fect self image. For example, most people estimate that they
their actions. Reinforcement learning models for how artifi-           are better drivers than the average person (Svenson, 1981),
cial agents should make decisions have shown that employ-              and students believe they will have fewer health problems
ing optimistic beliefs when faced with actions with unknown            than their peers (Weinstein, 1980). In contrast to this the-
consequences can lead to improved performance (Kaelbling,              ory, the strategic theory asserts that the bias should occur in a
1993). Similarly, using optimistic confidence bounds when              broader range of situations, and that it can be tempered by in-
exploring a search tree to choose actions can lead to improved         centives for accuracy (Akerlof & Dickens, 1982). However,
                                                                   1210

both of these theories conflict with some experimental evi-            ing. Given unlimited computational resources, an agent with
dence showing that wishful thinking occurs in decisions not            accurate beliefs about the transition and reward models will
involving self image and that incentives for accuracy have             perform at least as well as an agent with biased beliefs. How-
limited effects on people’s beliefs (Babad, 1997; Mayraz,              ever, with limited resources, calculating the optimal policy
2011). Wishful thinking has also been explained as emerg-              may not be possible. Instead, approximations, such as lim-
ing from other cognitive biases, such as confirmation bias,            iting the horizon that the agent considers when planning its
cognitive dissonance, and failure to correct for information           actions, are necessary. The complexity of solving for the op-
asymmetries rather than from a causal link between prefer-             timal policy scales exponentially with the number of years
ences and beliefs (Kahneman, Slovic, & Tversky, 1982; Knox             in the future that are considered. Thus, limiting this hori-
& Inkster, 1968). For instance, Knox and Inkster (1968) ex-            zon of consideration significantly reduces the complexity of
plain observations that individuals’ perceptions of a horse’s          the problem that the agent must solve. We explore whether
likelihood of winning a race increased after betting on the            a wishful thinking bias could prove advantageous when the
horse as instances of reducing post-decision cognitive disso-          planning horizon is small. We first consider this scenario in a
nance. While experimental evidence supports the existence              specific decision-making environment, defined by the transi-
of wishful thinking, none of these theories suggests it plays a        tion and reward models, and then conduct additional simula-
part in improving decision making.                                     tions that consider a broader set of environments to determine
                                                                       when the wishful thinking bias results in higher rewards.
Markov Decision Processes                                                 To explore possible computational benefits of wishful
A Markov decision process (MDP) is a decision-theoretic                thinking, we set up a Markov decision process in which var-
model of sequential decision making that naturally incorpo-            ious degrees of bias are expressed as inflated and deflated
rates actions with uncertain effects and takes into account            transition probabilities to high and low value states in agents’
both immediate and future consequences (see Sutton & Barto,            perception of the decision-making problem. Because agents’
1998, for an overview). MDPs model both the environment                beliefs about the transition matrices affect their beliefs about
in which actions are being taken and the effect of the agent’s         the value of taking an action in a given state, differences in
actions on this environment. Formally, MDPs are defined by             beliefs result in different action policies. For all of our sim-
a tuple hS, A, T, R, βi. At each time t, the environment is in         ulations, we consider the problem of an agent making job-
some state si ∈ S. The agent chooses an action a ∈ A, and              related choices and measure performance as the total earned
the transition model T encodes the conditional probabilities           rewards (salary) over the course of a fixed number of years.
    (t+1)      (t)
p(si      |a, s j ) that the state at time t + 1 will be si given
that the current state is s j and the action chosen is a. MDPs                    Simulation 1: A Simple Example
may have either finite or infinite horizons. In a finite horizon       We first set transition and reward matrices reflecting typical
MDP, which is the type we consider, the number of time steps           trends for salaries and ease of acquiring a particular job.
in which an agent acts is limited to some N. Finite horizon
MDPs must take into account both the current state and the             Methods
amount of time remaining for the agent to act.                         The simulation covers the problem of an agent choosing to
   The incentive structure or goals are encoded in the reward          keep or quit its current job over the course of N = 40 years.
model R. For each state-action pair, R(s, a) is equal to the           The states S correspond to five possible jobs, which we label
immediate reward of choosing action a in state s. Usually, an          ‘unemployed,’ ‘waiter,’ ‘police officer,’ ‘banker,’ and ‘movie
agent is trying to choose actions that will result in large re-        star.’ Since the MDP has a finite horizon, the policy defines
wards over the period of time in which it can act. Thus, the           which action to take at each time and in each state. Choosing
agent must determine the value of each action for each state           keep means that the agent retains its current job for the next
and timestep, taking into account both immediate and long              year and earns the full annual salary of that job. This reward
term rewards. This value is known as the Q-value, and the              is constant: there is no increase in salary over time. Choosing
optimal values Qt∗ are defined as those achieved by always             quit means that the agent takes a new job in the next year and
choosing the action with highest expected value for the cur-           earns a reward of half the annual salary of the current job.
rent state s and timestep t:                                              The reward model reflects the fact that unemployment is
                                                                       typically the least remunerative of the jobs and movie star is
    Qt∗ (s, a) = R(s, a) + β ∑ p(s0 |a, s) max Qt+1
                                                 ∗
                                                    (s0 , a0 ), (1)    the most: unemployment earns $0, waiter earns $30,000, po-
                              s0 ∈S          a0
                                                                       lice officer earns $50,000, banker earns $150,000, and movie
where β is a discount factor that weights the value of imme-           star earns $1,000,000. The transition matrix for choosing quit
diate versus future rewards. The optimal Q-function can be             follows a realistic ordering of the prevalence and difficulty of
calculated using dynamic programming (Bellman, 1957).                  achieving each profession (Figure 1(a)) The four agents we
                                                                       consider are shown in Figure 1: realistic agents have accurate
           Approximating Optimal Planning                              beliefs that are equal to the true transition matrix, optimistic
Finite horizon MDPs provide a framework for calculating the            agents believe that higher valued states are more likely and
best action in any state and with any amount of time remain-           lower valued states are less likely, highly optimistic agents
                                                                   1211

        (a)                Unemployed       Waiter     Police    Banker    Movie Star                (b)               Unemployed        Waiter      Police   Banker     Movie Star
           Unemployed          0.135         0.650     0.200     0.010         0.005                   Unemployed           0.070         0.600      0.300    0.0200        0.010
              Waiter           0.785           0       0.200     0.010         0.005                      Waiter            0.670            0       0.300    0.0200        0.010
              Police           0.335         0.650       0       0.010         0.005                      Police            0.370         0.600        0       0.020        0.010
              Banker           0.145         0.650     0.200        0          0.005                      Banker            0.090         0.600      0.300       0          0.010
            Movie Star         0.140         0.650     0.200     0.010            0                     Movie Star          0.080         0.600      0.300     0.020           0
        (c)                                            PolicekeepBanker
                                              for the action
                            transition matrixWaiter
         Figure 1: (a) TrueUnemployed                                       Movie1.Star
                                                                   in Simulation                     Figureconsidered
                                                                                                            1: (a) Trueby
                                                                                     (b) Transition matrix
                                                                                                     (d)                 transition
                                                                                                                           the optimistic
                                                                                                                       Unemployed   matrix  for the action
                                                                                                                                          agent.
                                                                                                                                         Waiter     Policekeep  in Simulation
                                                                                                                                                              Banker    Movie1.Star
                                                                                                                                                                                  (b) Transition matrix considered b
         (c)Unemployed
             Transition matrix considered
                                  0        by  the highly
                                             0.050         optimistic
                                                       0.050      0.100agent. (d)
                                                                               0.800                 (c) Transition
                                                                                  Transition matrix considered      matrix
                                                                                                                by the
                                                                                                       Unemployed            considered
                                                                                                                        pessimistic
                                                                                                                            0.240       by
                                                                                                                                     agent   the highly
                                                                                                                                          0.650          optimistic
                                                                                                                                                     0.100    0.010agent. (d)0Transition matrix considered by the 
               Waiter             0            0       0.100      0.100        0.800                      Waiter            0.890            0       0.100    0.010           0
               Police             0          0.100        0       0.100        0.800                      Police            0.340         0.650        0      0.010           0
              Banker              0          0.150     0.050         0         0.800                     Banker             0.250         0.650      0.100       0            0
            Movie Star            0          0.850     0.050      0.100           0                     Movie Star          0.240         0.650      0.100    0.010           0
         Figure 1: (a) True transition matrix for the action keep in Simulation 1. (b) Transition matrix
                                                                                                     Figureconsidered
                                                                                                            1: (a) Trueby  the optimistic
                                                                                                                                    matrixagent.
                                                                                                                        transition         for the action keep in Simulation 1. (b) Transition matrix considered b
Figure (c)    (a) Truematrix
          1: Transition   transition     matrix
                                considered          for the
                                           by the highly        action
                                                            optimistic    quit
                                                                       agent. (d)in  Simulation
                                                                                  Transition          1.Transition
                                                                                                          (b) Transition
                                                                                             matrix considered
                                                                                                     (c)                         matrix
                                                                                                                    the pessimistic
                                                                                                                 by matrix           agent
                                                                                                                            considered  byconsidered         by the
                                                                                                                                             the highly optimistic     optimistic
                                                                                                                                                                    agent.              agent.
                                                                                                                                                                           (d) Transition matrix considered by the 
(c) Transition matrix considered by the highly optimistic agent. (d) Transition matrix considered by the pessimistic agent
exaggerate the beliefs of optimistic agents, and pessimistic                                                     Simulation 2: Sampled Rewards
agents believe higher valued states are less likely and lower                                      The results of Simulation 1 demonstrate that it is possible
valued states are more likely.                                                                     for an optimistic agent to outperform a realistic agent when
   We simulated 100, 000 episodes for each agent and pos-                                          the agent considers the effects of a decision over only a lim-
sible horizon. We considered eight possible horizons over                                          ited amount of time. However, these results do not illustrate
which the agent could plan: 1, 2, 4, 5, 8, 10, 20, or 40 years.                                    whether this advantage holds for a variety of different types
At a horizon of one year, only the immediate value of the ac-                                      of reward and transition matrices. To explore how far these
tion is considered, and thus agents always keep their initial                                      results generalize, we next consider a more general set of pos-
jobs. A horizon of 40 years is equivalent to an agent with no                                      sible reward matrices, sampled from different distributions.
computational limitations on planning. We model planning
with a horizon h as an agent finding an optimal policy for                                         Methods
h years, carrying out this contingent policy, and then planing                                     Simulation 2 was conducted in the same way as Simulation 1,
for h additional years, repeating this process until the full time                                 except that the reward matrix was varied for each episode. We
has elapsed. This pattern might be thought of as analogous to                                      consider sampling annual salaries from three distributions:
a “five year plan”: decisions are made to maximize the re-                                         an exponential distribution, a power law distribution, and a
ward from the next h years, and while the five year plan is                                        uniform distribution. For each distribution, we set the mean
contingent upon the effects of each action in the plan, conse-                                     µ = 100, 000, and for the uniform distribution, we set the al-
quences after the length of the plan are not considered.1 Each                                     lowed range of rewards to 2µ. The exponential distribution
agent follows an optimal policy for their beliefs and the plan-                                    produces the most skewed distribution, favoring small val-
ning horizon, with a small probability ε of deviating from this                                    ues, while the power law distribution is also skewed but has
policy at each timestep. This probability reflects the fact that                                   a heavier tail. To maintain the structure of higher salaries for
human decision making is noisy; for all simulations, ε = 0.05.                                     harder to acquire jobs, we sort the sampled salaries such that
We set the discount factor β = 1, resulting in future rewards                                      the ordering matches Simulation 1: the highest salary goes to
having the same value as immediate rewards.                                                        the movie star job and the lowest to unemployed.
Results                                                                                               We simulated 10, 000 episodes for each horizon, agent, and
                                                                                                   reward distribution. For each episode, we sampled a reward
As shown in Figure 2, the optimistic and highly optimistic                                         matrix, then generated episodes for each agent with that re-
agents earned more money on average than the realistic agent                                       ward matrix. To compare earnings across episodes, we record
for small planning horizons. Both of these policies are more                                       the proportion of possible earnings that were earned in a given
likely to take the risk of quitting a low paying job than the                                      episode, where the possible earnings are the number of years
realistic policy. For small horizons, this risk taking is advan-                                   in the episode (N = 40) multiplied by the maximum salary.
tageous as it helps to compensate for the limited amount of
time that the agent is considering. For larger horizons, the re-                                   Results
alistic agent can better estimate the value of risk taking, and                                    As shown in Figure 2, the average proportion of earnings ac-
thus outperforms the other policies. Across all horizons, there                                    quired varies across the three reward distributions, but the
is no advantage for the pessimistic agent.                                                         agents show similar trends in earnings relative to one another.
    1 An alternative possibility for incorporating the constraint of                               Unlike in Simulation 1, we do not see an advantage for the
planning over a limited number of years into an MDP is to have                                     highly optimistic agent, even at short horizons; instead, this
the agent use the Q1 -values for the first action until the N − h − 1th                            agent underperforms all other agents. However, a small ad-
year, and then use the remaining Qt values for the final h − 1 years.                              vantage for the optimistic agent persists: at planning horizons
All simulations in this paper have also been conducted with this ver-
sion of the policy, and results are very similar to using the “h year                              of two, four, and five years, this agent has higher earnings
plan” version of the policy.                                                                       than the realistic agent.
                                                                                            1212

                                                                                                                                                                                                                                                                                                                                          Fixed Rewards                                                           Uniformly Distribu
                                                                                                                                                                                                                                                                                                                           0.08
                                                                                                                                                                                                                                                                                                                          0.075
                                                                                                                                                                                                                                                                                        Proportion of possible earnings                                                   Proportion of possible earnings
                                                                                                                                                                                                                                                                                                                                                                                                            0.5
                                                                                                                                                                                                                                                                                                                           0.07
                                                                                                                                                                                                                                                                                                                                                                                                            0.4
                                                                                                                                                                                                                                                                                                                          0.065
                                                                                                                                                                                                                                                                                                                           0.06                                                                             0.3
                                                                                                                                                                                                                                                                                                                          0.055
                                                                                                                                                                                                                                                                                                                                                                                                            0.2
      (a)                                          Average Earnings:        (b)                                           Average Earnings:          (c)                                           Average Earnings:         (d)                                            Average Earnings:
                                                                                                                                                                                                                                                                                          0.05 Rewards
                                                                                                                                                                                                                                                                                                                                                Realistic agent
                                                    Fixed Rewards                                                   Uniformly Distributed Rewards                                            Power Law Distributed Rewards                                           Exponentially Distributed                                                  Optimistic agent
                                          0.08
                                                                                                                                                                                                                                                                                                                                                Highly optimistic agent                                     0.1
                                                                                                                                                                                                                                                                                                                          0.045
                                         0.075
       Proportion of possible earnings                                      Proportion of possible earnings                                          Proportion of possible earnings                                         Proportion of possible earnings
                                                                                                                                                                                                                                                                                                                                                Pessimistic agent
                                                                                                              0.5                                                                      0.5                                                                     0.5
                                          0.07                                                                                                                                                                                                                                                                             0.04                                                                              0
                                                                                                                                                                                                                                                                                                                               0        10     20        30         40                                        0        10      20
                                                                                                              0.4                                                                      0.4                                                                     0.4
                                         0.065                                                                                                                                                                                                                                                                                               Horizons                                                                       Horizons
                                          0.06                                                                0.3                                                                      0.3                                                                     0.3
                                         0.055
                                                                                                              0.2                                                                      0.2                                                                     0.2
                                          0.05
                                                                                                              0.1                                                                      0.1                                                                     0.1
                                         0.045
                                          0.04                                                                 0                                                                        0                                                                       0
                                              0   10     20       30   40                                       0        10     20       30     40                                       0        10      20       30   40                                       0         10      20                                        30    40
                                                       Horizons                                                               Horizons                                                                  Horizons                                                                 Horizons
Figure 2: Average proportion of possible earnings acquired by each agent, Simulations 1 and 2. Error bars reflect 1.96 standard
errors. (a) Simulation 1: At small horizons, the optimistic and highly optimistic agents earn more than the realistic agent.
(b-d) Simulation 2: The proportion of rewards earned varies across sampling distributions, but the relative performance of the
strategies remains the same. There is a slight advantage at very small horizons for the optimistic agent, although the highly
optimistic agent performs poorly.
    Simulation 3: Sorted Transition Matrices                                                                                                                                                           sitioning to state s0 after quitting s is proportional to the true
                                                                                                                                                                                                       probability of reaching s0 from s multiplied by the salary of s0
The previous simulation demonstrated that there is not always
                                                                                                                                                                                                       raised to the power of γ:
an advantage at short horizons for highly optimistic agents,
but suggested that some advantage for an optimistic (wishful                                                                                                                                                       pbias=γ (s0 |s, a = quit) ∝ p(s0 |s, a = quit) · R(s0 )γ                                                                                               (2)
thinking) bias may exist more generally than in the specific
conditions in Simulation 1. One of the difficulties in gen-                                                                                                                                            where p(s0 |s, a = quit) is the true transition probability. When
eralizing from these simulations is that they do not quantify                                                                                                                                          γ = 0, there is no bias: the agent’s beliefs are the same as the
the differences between agents: each of the biases was imple-                                                                                                                                          true transition matrix. When γ > 0, the agent is optimistic:
mented by hand by altering a specific transition matrix. To                                                                                                                                            states with larger salaries will be deemed more probable out-
quantify different levels of bias and develop a better under-                                                                                                                                          comes than in reality, while states with smaller salaries will be
standing of how much optimism is useful in what situations,                                                                                                                                            deemed less probable. When γ < 0, the opposite occurs, re-
we now generalize the simulations such that both the tran-                                                                                                                                             sulting in a pessimistic agent. γ with larger magnitudes result
sition matrix governing the likelihood of attaining particular                                                                                                                                         in greater degrees of optimism or pessimism. We considered
jobs and the salaries for these jobs are sampled. As in Simu-                                                                                                                                          γ = −5, −1, 0, 1, 5.
lations 1 and 2, we constrain these matrices to pair jobs that                                                                                                                                            We simulated 10, 000 episodes for each horizon, α, γ, and
are hard to achieve with higher salaries. To implement differ-                                                                                                                                         reward distribution. As in Simulation 2, we record the pro-
ent levels of bias, we transform the transition matrix such that                                                                                                                                       portion of possible earnings earned in each episode to enable
the agents’ beliefs are skewed to be optimistic or pessimistic.                                                                                                                                        comparison across episodes with different rewards.
By including a parameter in the transformation representing
the desired degree of bias, we can determine whether there                                                                                                                                             Results
are advantages to moderate levels of optimism outside of the                                                                                                                                           The results of the simulations are similar across the three re-
specific scenario explored in Simulations 1 and 2.                                                                                                                                                     ward distributions: as in Simulation 2, the absolute propor-
                                                                                                                                                                                                       tion of rewards earned does vary, but the strategies’ perfor-
Methods
                                                                                                                                                                                                       mance relative to one another remains the same. We thus
The basic structure of Simulation 3 mirrored previous simu-                                                                                                                                            show only the results of the exponential distribution in Fig-
lations, with forty years for the agent to act and five possible                                                                                                                                       ure 3. As this figure shows, the benefits of a bias towards
jobs. Both the salary and transition matrices were sampled                                                                                                                                             optimism are dependent on the characteristics of the transi-
in this simulation. The sampling of the salaries was the same                                                                                                                                          tion matrix. When distributions are very sparse, as occurs
as in Simulation 2. To construct the transition matrix, we                                                                                                                                             with small α, there is little potential to move between jobs,
sampled each distribution from a symmetric Dirichlet distri-                                                                                                                                           so all strategies perform relatively similarly. With slightly
bution with parameter α. Larger values of α favor uniform                                                                                                                                              larger α = 0.1, there is a clear disadvantage for extreme pes-
distributions, while smaller values favor sparse distributions.                                                                                                                                        simism (γ = −5) and extreme optimism (γ = 5); mirroring
We let α = 0.01, 0.1, 1, 10. To ensure that harder to achieve                                                                                                                                          the results of Simulation 2, the highly optimistic strategy is
jobs have higher salaries, we sort the transition matrix such                                                                                                                                          the worst performing strategy. However, this level of α also
that for each row, the transition probabilities decrease as the                                                                                                                                        begins showing a limited benefit for a slightly optimistic strat-
salaries increase.                                                                                                                                                                                     egy (γ = 1), dependent on reward distribution. There is no ad-
   Different levels of wishful thinking are introduced into this                                                                                                                                       vantage for the exponential distribution, while the slightly op-
simulation through a bias parameter γ. The transition matri-                                                                                                                                           timistic strategy outperforms the realistic strategy by at least
ces assumed by biased agents are tied to the rewards associ-                                                                                                                                           two standard errors for horizons of two and four for the uni-
ated with different states. The perceived likelihood of tran-                                                                                                                                          form distribution and horizon two for the power law distribu-
                                                                                                                                                                                             1213

                                                  Average Earnings: ↵ = 0.01                                             Average Earnings: ↵ = 0.1                                              Average Earnings: ↵ = 1                                                Average Earnings: ↵ = 10
                                          0.23                                                                   0.23                                                                    0.5                                                                   0.75
                                                                                                                                                                                                                                                                                                     γ = −5
                                                                                                                                                                                                                                                                0.7                                  γ = −1
        Proportion of possible earnings                                        Proportion of possible earnings                                        Proportion of possible earnings                                        Proportion of possible earnings
                                          0.22                                                                   0.22
                                                                                                                                                                                        0.45                                                                                                         γ=0
                                                                                                                                                                                                                                                               0.65
                                                                                                                                                                                                                                                                                                     γ=1
                                          0.21                                                                   0.21                                                                                                                                                                                γ=5
                                                                                                                                                                                                                                                                0.6
                                                                                                                                                                                         0.4
                                           0.2                                                                    0.2                                                                                                                                          0.55
                                                                                                                                                                                        0.35
                                                                                                                                                                                                                                                                0.5
                                          0.19                                                                   0.19
                                                                                                                                                                                                                                                               0.45
                                                                                                                                                                                         0.3
                                          0.18                                                                   0.18
                                                                                                                                                                                                                                                                0.4
                                          0.17                                                                   0.17                                                                   0.25                                                                   0.35
                                              0      10     20       30   40                                         0      10     20       30   40                                         0      10     20       30   40                                         0       10     20       30   40
                                                          Horizons                                                               Horizons                                                               Horizons                                                                Horizons
Figure 3: Average lifetime earnings as a proportion of possible earnings in Simulation 3, with rewards sampled from an
exponential distribution. γ dictates the degree and direction (optimistic or pessimistic) of the agent’s bias. When α is large
enough, optimistic agents (γ > 0) have an advantage over the realistic agent (γ = 0) at small horizons. Error bars reflect 1.96
standard errors.
tion. As α increases further, the benefit to an optimistic strat-                                                                                                                               Results
egy also increases. The highly optimistic strategy tends to be                                                                                                                                  The results of Simulation 4 are very similar to those of Sim-
best at the lowest horizons, eventually being outperformed by                                                                                                                                   ulation 3, demonstrating that the sorting constraint does not
both the slightly optimistic and realistic strategies.                                                                                                                                          have a large impact on relative earnings. Earnings are in gen-
   Overall, the results of this simulation suggest that an ad-                                                                                                                                  eral higher in this simulation, reflecting the fact that higher
vantage for optimism in cases with very limited lookahead                                                                                                                                       paying jobs are no longer the hardest to achieve. As in
holds for many more transition and reward matrices than the                                                                                                                                     Simulation 3, the three reward distributions result in similar
example in Simulation 1. The effect is strongest where there                                                                                                                                    relative advantages for optimistic and pessimistic strategies:
is non-negligible probability on all possible states, as occurs                                                                                                                                 none of the reward distributions show any advantage for pes-
with the larger α values, and does not fade even when the                                                                                                                                       simism, but with larger α, there is a bias for optimistic strate-
transition distributions are unlikely to be skewed (α = 10).                                                                                                                                    gies at short horizons. As shown in Figure 4, smaller horizons
The advantage for optimism with short horizons across a                                                                                                                                         with α ≥ 0.1 result in advantages for the highly optimistic
range of environments suggests that there be many scenar-                                                                                                                                       and slightly optimistic strategies, with greater advantages for
ios in which people must make decisions in which a similar                                                                                                                                      α ≥ 1 and a much more robust advantage for the slightly opti-
advantage holds.                                                                                                                                                                                mistic strategy. Only at α = 0.01 are the results of Simulation
                                                                                                                                                                                                3 characteristically different than in Simulation 4. In Simu-
   Simulation 4: General Transition Matrices                                                                                                                                                    lation 3, the sparse, sorted transition matrices meant that the
In Simulation 3, we ensured that the lowest rewards were                                                                                                                                        lookahead horizon had very little impact on rewards, as quit-
paired with the easiest-to-achieve states, mirroring the idea                                                                                                                                   ting one’s job rarely held any possibility of improved salary.
of high paying jobs being in limited supply. However, the                                                                                                                                       In Simulation 4, rewards and transition probabilities are unas-
results of the non-sparse transition matrices in Simulation 3                                                                                                                                   sociated, so improvements in rewards from longer lookahead
suggest that this condition may not be necessary for an opti-                                                                                                                                   are possible. However, this α still has the smallest range of
mistic advantage: the non-sparse transition matrices actually                                                                                                                                   possible rewards, demonstrating the limited impact of strate-
resulted in the largest advantage for optimism, even though                                                                                                                                     gies when transition probabilities are sparse.
high reward jobs were not much less likely than other jobs.
Our final simulation explores this more general case: is there                                                                                                                                                                                                  Conclusion
an advantage for optimism in cases where transition matrices                                                                                                                                    In this paper, we have explored whether an optimistic or wish-
and rewards are unrelated?                                                                                                                                                                      ful thinking bias can improve performance when agents have
                                                                                                                                                                                                limited foresight into the consequences of their actions. Us-
Methods                                                                                                                                                                                         ing Markov decision processes, we have shown that when the
The methods for this simulation were identical to Simulation                                                                                                                                    horizon an agent can consider when planning is relatively lim-
3, except that the transition matrices were not sorted. Thus, if                                                                                                                                ited compared to the true time horizon of a task, some bias
one quits the banker job, one might be highly likely to transi-                                                                                                                                 towards optimism results in higher total reward. As the hori-
tion to the movie star job, while if one quits the police officer                                                                                                                               zon that the agent can consider increases, the gain for an op-
job, the most likely transition might be to the waiter job. As in                                                                                                                               timistic policy decreases, and optimism eventually becomes
Simulation 3, we sampled rewards from exponential, power                                                                                                                                        detrimental to performance. Overall, these results demon-
law, and uniform distributions with the same mean, and sam-                                                                                                                                     strate the possibility that wishful thinking could be a compu-
pled the transition matrices from a symmetric Dirichlet distri-                                                                                                                                 tational heuristic for improving performance rather than sim-
bution, considering α = 0.01, 0.1, 1, 10. The same five agents                                                                                                                                  ply a mistake in people’s reasoning.
were used, with biases set using Equation 2.                                                                                                                                                       Our model provides a proof of concept for the possibil-
                                                                                                                                                                      1214

                                                 Average Earnings: ↵ = 0.01                                              Average Earnings: ↵ = 0.1                                               Average Earnings: ↵ = 1                                               Average Earnings: ↵ = 10
                                         0.56                                                                    0.65                                                                    0.8                                                                    0.8
                                                                                                                                                                                                                                                                                                       γ = −5
                                                                                                                                                                                                                                                                                                       γ = −1
                                         0.54                                                                                                                                           0.75                                                                   0.75
                                                                                                                                                                                                                                                                                                       γ=0
                                                                                                                  0.6
       Proportion of possible earnings                                         Proportion of possible earnings                                        Proportion of possible earnings                                        Proportion of possible earnings
                                         0.52                                                                                                                                            0.7                                                                    0.7                                    γ=1
                                                                                                                                                                                                                                                                                                       γ=5
                                          0.5                                                                                                                                           0.65                                                                   0.65
                                                                                                                 0.55
                                         0.48                                                                                                                                            0.6                                                                    0.6
                                                                                                                  0.5
                                         0.46                                                                                                                                           0.55                                                                   0.55
                                         0.44                                                                                                                                            0.5                                                                    0.5
                                                                                                                 0.45
                                         0.42                                                                                                                                           0.45                                                                   0.45
                                          0.4                                                                     0.4                                                                    0.4                                                                    0.4
                                             0      10      20       30   40                                         0      10     20       30   40                                         0      10     20       30   40                                         0      10     20       30      40
                                                          Horizons                                                               Horizons                                                               Horizons                                                               Horizons
Figure 4: Average lifetime earnings as a proportion of possible earnings in Simulation 4, with rewards sampled from an
exponential distribution. Just as in Simulation 3, optimistic agents (γ > 0) have an advantage over the realistic agent (γ = 0)
at small horizons, demonstrating that the advantage for optimism does not require a link between relative salary and ease of
acquiring a job. Error bars reflect 1.96 standard errors.
ity that wishful thinking is a strategy. The simple model is                                                                                                                                    it is not necessary to assume that all cases of a wishful think-
only an approximation for real world decision making, which                                                                                                                                     ing bias are detrimental or irrational: instead, this bias may
is typically more complex. Additionally, our model exam-                                                                                                                                        represent a better approximation to an optimal solution when
ines only one possible computational approximation: limit-                                                                                                                                      only limited computational resources are available.
ing the time over which one considers the consequences of
                                                                                                                                                                                                Acknowledgements. This work was supported by ONR MURI
one’s actions. Other approximations, such as feature-based
                                                                                                                                                                                                grant number N00014-13-1-0341 to TLG.
reinforcement learning algorithms to model time without an
explosion of the state space or forward search approxima-                                                                                                                                                                                                      References
tions (e.g., Ross, Pineau, Paquet, & Chaib-draa, 2008), are                                                                                                                                     Akerlof, G. A., & Dickens, W. T. (1982). The economic conse-
possible. Considering these other types of approximations                                                                                                                                         quences of cognitive dissonance. The American Economic Re-
would further develop our understanding of whether there are                                                                                                                                      view, 72(3), 307–319.
                                                                                                                                                                                                Babad, E. (1997). Wishful thinking among voters: Motivational
situations in which wishful thinking is advantageous across a                                                                                                                                     and cognitive influences. International Journal of Public Opinion
broad set of rational process models.                                                                                                                                                             Research, 9(2), 105–125.
                                                                                                                                                                                                Bellman, R. E. (1957). Dynamic programming. Princeton, NJ, USA:
   The model we have presented provides a starting point for                                                                                                                                      Princeton University Press.
                                                                                                                                                                                                Camerer, C., & Lovallo, D. (1999). Overconfidence and excess en-
future work. First, simulations of wider set of decision prob-                                                                                                                                    try: An experimental approach. The American Economic Review,
lems are necessary to establish whether an optimistic bias                                                                                                                                        89(1), 306–318.
holds in more complex situations. In the case of choosing                                                                                                                                       Coquelin, P.-A., & Munos, R. (2007). Bandit algorithms for tree
                                                                                                                                                                                                  search. In Proceedings of the Twenty-Third Annual Conference
a job, we assumed that people could retain the same job for                                                                                                                                       on Uncertainty in Artificial Intelligence (pp. 67–74).
indefinite periods of time and that each job had a constant                                                                                                                                     Kaelbling, L. P. (1993). Learning in embedded systems. MIT Press.
salary; thus, if one job was better than some other job for one                                                                                                                                 Kahneman, D., Slovic, P., & Tversky, A. (1982). Judgment under
                                                                                                                                                                                                  uncertainty: Heuristics and biases. Cambridge University Press.
year, it would be much better than the other job if retained                                                                                                                                    Knox, R. E., & Inkster, J. A. (1968). Postdecision dissonance at
over many years. In this type of structure, it is intuitive that                                                                                                                                  post time. Journal of Personality and Social Psychology, 8(4, Pt.
an optimistic bias might help to compensate for a bounded                                                                                                                                         1), 319.
                                                                                                                                                                                                Krizan, Z., & Windschitl, P. D. (2007). The influence of out-
horizon. However, other situations may be more complex,                                                                                                                                           come desirability on optimism. Psychological Bulletin, 133(1),
such as a job that initially has a higher salary leading to less                                                                                                                                  95–121.
potential to switch to other high paying jobs than a job with                                                                                                                                   Larwood, L., & Whittaker, W. (1977). Managerial myopia: Self-
                                                                                                                                                                                                  serving biases in organizational planning. Journal of Applied Psy-
a lower initial salary. Exploring such situations will allow                                                                                                                                      chology, 62(2), 194.
us to establish a more general theory for what features of a                                                                                                                                    Lyles, M. A., & Thomas, H. (1988). Strategic problem formulation:
situation result in an advantage for optimistic biases. Experi-                                                                                                                                   Biases and assumptions embedded in alternative decision-making
                                                                                                                                                                                                  models. Journal of Management Studies, 25(2), 131–145.
ments are also a necessary next step for determining whether                                                                                                                                    Mayraz, G. (2011). Wishful thinking (Tech. Rep. No. CEP Dis-
wishful thinking is used mainly in situations where it provides                                                                                                                                   cussion Paper 1092). Centre for Economic Performance, London
an advantage or is modulated by the computational complex-                                                                                                                                        School of Economics.
                                                                                                                                                                                                Munos, S. G. W., & Teytaud, O. (2006). Modification of UCT with
ity of the task. For example, one might ask participants to                                                                                                                                       patterns in Monte-Carlo go. Technical Report RR-6062.
make decisions in situations with short or long time horizons.                                                                                                                                  Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online
If wishful thinking is used to deal with computational com-                                                                                                                                       planning algorithms for POMDPs. Journal of Artificial Intelli-
                                                                                                                                                                                                  gence Research, 32(1), 663–704.
plexity, one would expect it to be less common for simpler                                                                                                                                      Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning. MIT
decisions with shorter time horizons. While there are a num-                                                                                                                                      Press.
ber of steps necessary to establish whether the model we have                                                                                                                                   Svenson, O. (1981). Are we all less risky and more skillful than our
                                                                                                                                                                                                  fellow drivers? Acta Psychologica, 47(2), 143–148.
presented bears on how humans actually cope with predicting                                                                                                                                     Weinstein, N. D. (1980). Unrealistic optimism about future life
the long term results of their choices, our results suggest that                                                                                                                                  events. Journal of Personality and Social Psychology, 39(5), 806.
                                                                                                                                                       1215

