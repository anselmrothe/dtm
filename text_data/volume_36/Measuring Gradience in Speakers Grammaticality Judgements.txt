UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Measuring Gradience in Speakers’ Grammaticality Judgements
Permalink
https://escholarship.org/uc/item/5g99t77d
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Lau, Jey Han
Clark, Alexander
Lappin, Shalom
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                 Measuring Gradience in Speakers’ Grammaticality Judgements
                                      Jey Han Lau, Alexander Clark, and Shalom Lappin
                    jeyhan.lau@gmail.com, alexander.clark@kcl.ac.uk, shalom.lappin@kcl.ac.uk
                                            Department of Philosophy, King’s College London
                               Abstract                                 dark). To sustain a categorical view of grammaticality it is
   The question of whether grammaticality is a binary categorical       necessary to show that when gradience does arise in speak-
   or a gradient property has been the subject of ongoing debate        ers’ judgements, it is entirely the result of extra-grammatical
   in linguistics and psychology for many years. Linguists have         influences, such as processing factors, semantic acceptabil-
   tended to use constructed examples to test speakers’ judge-
   ments on specific sorts of constraint violation. We applied ma-      ity, or real world knowledge. To the best of our knowledge,
   chine translation to randomly selected subsets of the British        the advocates of the categorical view have not managed to
   National Corpus (BNC) to generate a large test set which con-        demonstrate this in a convincing way over large amounts of
   tains well-formed English source sentences, and sentences that
   exhibit a wide variety of grammatical infelicities. We tested        grammatically varied experimental data.
   a large number of speakers through (filtered) crowd sourc-              Grammaticality is a theoretical concept, while acceptabil-
   ing, with three distinct modes of classification, one binary and     ity can be experimentally tested. To maintain the categori-
   two ordered scales. We found a high degree of correlation in
   mean judgements for sentences across the three classification        cal view of grammaticality in the face of pervasive gradience
   tasks. We also did two visual image classification tasks to ob-      in acceptability judgements across a wide range of syntactic
   tain benchmarks for binary and gradient judgement patterns,          structures, one must provide an independent, empirically vi-
   respectively. Finally, we did a second crowd source experi-
   ment on 100 randomly selected linguistic textbook example            able criterion for identifying grammaticality, which does not
   sentences. The sentence judgement distributions for individ-         assume the view that is at issue.1 In the absence of such a cri-
   ual speakers strongly resemble the gradience benchmark pat-          terion, solid experimental evidence for gradience in accept-
   tern. This evidence suggests that speakers represent grammat-
   ical well-formedness as a gradient property.                         ability for a large number of speakers, across a wide range
   Keywords: grammaticality, acceptability, gradient classifiers,       of data provides strong prima facie support for the hypothesis
   binary categories, speakers’ judgements                              that grammaticality is a gradient property.
                                                                           Second, both sides of the debate have generally relied
                          Introduction                                  solely on linguistic judgements in order to motivate their con-
The question of whether grammaticality is a binary categor-             clusions. In fact the discussion would be advanced if indepen-
ical or a gradient property has been the subject of a long-             dent non-linguistic paradigms of binary classifiers and gradi-
standing debate in linguistics and psychology (Keller, 2000;            ent properties were identified and used as benchmarks with
Manning, 2003; Crocker & Keller, 2005; Sorace & Keller,                 which to compare the judgement patterns that speakers ex-
2005; Fanselow, Féry, Schlesewsky, & Vogel, 2006; Sprouse,             hibit with respect to a wide range of grammaticality data.
2007; Ambridge, Pine, & Rowland, 2012). While it has been                  We used Google statistical machine translation to map sen-
recognized since the very beginning of modern linguistics               tences randomly selected from the BNC into a number of
(Chomsky, 1965) that there are degrees of grammaticality, in            languages and then back into English. The errors that were
practice grammaticality is standardly taken in theoretical lin-         generated across these target languages ranged from mild in-
guistics to be dichotomous: a binary division between gram-             felicities of lexical choice and awkward ordering of modi-
matical and ungrammatical sentences. Most grammar for-                  fiers, through missing arguments, deleted prepositions, and
malisms are specified so that the grammars that they allow              misplaced subordinate clauses, to word salads.
generate sets of sentences, with a binary set membership cri-              We then tested these sentences on speakers through crowd
terion corresponding to a categorical notion of grammatical-            sourcing with Amazon Mechanical Turk (AMT). Each HIT
ity.                                                                    (Human Intelligence Task) contained an original English sen-
   Advocates of a categorical view of grammaticality have               tence from the BNC and four translated sentences randomly
tended to limit themselves to experimental results involving a          selected from each of the four target languages. We used three
small number of constructed examples. These examples ap-                presentation modes for these experiments: a binary classifica-
pear to show the inviolability of specific kinds of syntactic           tion task, a four point acceptability scale, and a sliding scale
constraints (such as wh-island conditions). While this work             with 100 underlying discrete points. We observed a very high
is interesting and important, it suffers from at least two prob-        Pearson correlation among the mean judgements across all
lems. First, it is difficult to see how the existence of a number
                                                                            1 For example, Sprouse (2007) presents some evidence that cer-
of cases in which speakers’ judgements are robustly binary
                                                                        tain types of syntactic island violations fail to show syntactic prim-
in itself entails the categorical nature of grammaticality, even        ing effects under experimental conditions. He takes this to motivate
when these cases exhibit clearly identifiable syntactic errors          a categorical view of grammaticality on the grounds that priming
that are well described by a particular theory of syntax. Gra-          is only possible for grammatical sentences. He does not demon-
                                                                        strate that all types of ungrammaticality fail to exhibit priming ef-
dient judgments will inevitably appear to be sharp for clear            fects. Also, his experimental results suggest that speakers assign
paradigm cases (very tall vs. very short, very light vs very            different levels of acceptability to distinct types of island violations.
                                                                    821

modes of classification.2                                                  O’Connor, Jurafsky, & Ng, 2008).3 To keep the task trans-
   We also used AMT for two visual classification tasks. One               parent and to avoid biasing the judgements of non-experts,
tested male–female judgements for photographs of men and                   we asked annotators to classify the sentences for naturalness,
women. The second required classifying a set of graphic rep-               rather than for grammaticality or well-formedness
resentations of human figures as fat or thin. We used variants                We employed three modes of presentation: (1) binary
of the two non-binary scales that we employed in the gram-                 (henceforth “MOP2”), where users choose between two
maticality judgement experiment for these tasks. When we                   options: unnatural and natural; (2) 4-category (henceforth
compared the distribution patterns of individual judgements                “MOP4”), where they are presented with 4 options: ex-
for the linguistic experiments with those for the visual classi-           tremely unnatural, somewhat unnatural, somewhat natural
fication tasks, we saw that the linguistic judgements closely              and extremely natural; and (3) a sliding scale (henceforth
resembled the gradient fat–thin pattern rather than the binary             “MOP100”) with two extremes: extremely unnatural and ex-
male–female distribution.                                                  tremely natural. For MOP100 we sampled only 10% of the
   We performed a second AMT annotation experiment in                      sentences (i.e. 250 sentences) for annotation, because a pre-
which we randomly selected 100 linguist’s examples (50                     liminary experiment indicated that this mode of presentation
good ones and 50 starred ones) from Adger (2003)’s text                    required considerably more time to complete than MOP2 and
on syntax. We found a pattern of gradience in acceptabil-                  MOP4.
ity judgements for these textbook examples that is similar to                 To ensure the reliability of annotation, an original English
that displayed for the sentences of our initial experiment                 sentence was included in the 5 sentences presented in each
   In current work we are studying the extent to which en-                 HIT. We assume that the English sentences are (in general)
riched lexical n-gram models and other probabilistic models                fully grammatical, and we rejected workers who did not con-
track the AMT judgements in our experiment. We are also                    sistently rate these sentences highly. Even with this constraint
employing machine learning techniques to identify the most                 an annotator could still game the system by giving arbitrar-
significant features of these models. We briefly describe this             ily high ratings to all (or most) sentences. We implemented
work in the Discussion and Conclusions Section.                            an additional filter to control for this possibility by reject-
                                                                           ing those annotators whose average sentence rating exceeds
                                                                           a specified threshold.4
                Data Set and Methodology
                                                                              We used the sentence judgements only from annotators
For our experiments, we needed a data set of human judge-                  who passed the filtering conditions. Each sentence received
ments of grammaticality for a large variety of sentences. We               approximately 14 annotations for MOP2 and 10 annotations
extracted 600 sentences of length 8 to 25 words from the BNC               for MOP4 and MOP100 (post-filtering). The acceptance
(BNC Consortium, 2007). To generate sentences of varying                   rate for annotators was approximately 70% for MOP2 and
level of grammaticality, we used Google Translate to map the               MOP4,and 43% for MOP100.
600 sentences from English to 4 target languages — Norwe-
gian, Spanish, Chinese and Japanese — and then back to En-                                    Experiments and Results
glish. We chose these target languages because a pilot study               Correlation of Aggregated Sentence Rating and
indicated that they gave us a ranked distribution of relative              Sentence Length
grammatical well-formedness in English output. Norwegian                   A potential confounding factor that could influence the ag-
tends to yield the best results, and Japanese the most dis-                gregated rating of a sentence (i.e. the mean rating of a sen-
torted. However, the distribution is not uniform, with various             tence over all annotators) is the sentence length. To better
levels of acceptability appearing in the English translations              understand the impact of sentence length, we computed the
from all four target languages.                                            Pearson correlation coefficient of the mean sentence rating
   To keep only sentences of length 8 to 25 words, we sub-                 and the sentence length for each mode of presentation. The
sampled a random set of 500 sentences from the 600 sen-                    results are summarised in Table 1.
tences in each language (the original English sentence and                    We see that although the correlations vary slightly, depend-
the four back-translated sentences) that satisfy the length re-            ing on the translation route, they are relatively small and sta-
quirement. This produced a test set of 2,500 sentences.                        3 Sprouse (2011) in particular reports an experiment showing that
   We used AMT to obtain human judgements of acceptabil-                   the AMT grammatical acceptability tests that he conducted were as
ity, as it has been demonstrated that it is an effective way               reliable as the same tests conducted under laboratory conditions.
                                                                               4 Internally, the MOP2 ratings are represented by integer scores
of collecting linguistic annotations (Sprouse, 2011; Snow,
                                                                           1 (unnatural) and 4 (natural); MOP4 by integer scores from 1 (ex-
                                                                           tremely unnatural) to 4 (extremely natural); and MOP100 by inte-
    2 We use Pearson rather than Spearman correlation to test the cor-     ger scores from 1 (extremely unnatural) to 100 (extremely natural).
respondences of judgements under different modes of presentations          A “correct” rating is defined as judging the control English sentence
because we are comparing the mean annotation scores for sentences          greater than or equal to 4, 3 and 75 in MOP2, MOP4 and MOP100,
in these comparisons, and these scores are continuous. Moreover,           respectively. An annotator was rejected if either of the following two
the Pearson metric is more accurate because it measures the mag-           conditions were satisfied: (1) their accuracy for original English sen-
nitude of difference among rankings, as well as the correspondence         tences was less than 70%, or (2) their mean rating was greater than
between rankings themselves.                                               or equal to 3.5 in MOP2, 3.5 MOP4, and 87.5 in MOP100.
                                                                       822

                             0.3                                                               0.3                                                                       0.3
                            0.27                                                              0.27                                                                      0.27
                            0.24                                                              0.24                                                                      0.24
     Normalised Frequency                                              Normalised Frequency                                                      Normalised Frequency
                            0.21                                                              0.21                                                                      0.21
                            0.18                                                              0.18                                                                      0.18
                            0.15                                                              0.15                                                                      0.15
                            0.12                                                              0.12                                                                      0.12
                            0.09                                                              0.09                                                                      0.09
                            0.06                                                              0.06                                                                      0.06
                            0.03                                                              0.03                                                                      0.03
                             0.01.0   1.6    2.2     2.8   3.4   4.0                           0.01.0   1.6    2.2         2.8   3.4   4.0                               0.00.0   20.0     40.0      60.0   80.0   100.0
                                            (a) MOP2                                                          (b) MOP4                                                                   (c) MOP100
                                      Figure 1: Histograms of mean sentence ratings using MOP2, MOP4 and MOP100 presentations.
                               Language            MOP2      MOP4      MOP100                                                                 Presentation Pair                                   Pearson’s r
                               en original         -0.06     -0.15      -0.24                                                                 MOP2 and MOP4                                          0.92
                                en-es-en           -0.12     -0.13      -0.11                                                                MOP2 and MOP100                                         0.93
                                en-ja-en           -0.22     -0.28      -0.36                                                                MOP4 and MOP100                                         0.94
                                en-no-en           -0.08     -0.13      0.03
                                en-zh-en           -0.22     -0.22      -0.08
                              All sentences        -0.09     -0.13      -0.13                                              Table 2: Pearson’s r of mean sentence rating for different
                                                                                                                           pairs of presentation.
Table 1: Pearson’s r of mean sentence rating and sentence
length. The “Language” column denotes the path of transla-
tion for the sentences. Language Codes: English = en; Nor-                                                                 not affected by mode of presentation. Whether annotators
wegian = no; Spanish = es; Chinese = zh; and Japanese =                                                                    are presented with a binary choice, 4 categories, or a slid-
ja.                                                                                                                        ing scale, aggregating the ratings produces similar results, as
                                                                                                                           shown by the high correlations in Table 3.
                                                                                                                              Moreover when we examine the histograms of the average
ble when computed over all sentences, across all modes of                                                                  judgments for each sentence, as shown in Figure 1, we see
presentation. This implies that for short to moderately long                                                               that qualitatively there are only a few clear differences. Most
sentences, length has little influence on acceptability judge-                                                             prominently, under the binary presentation on the far left, we
ments. Therefore, in the experiments that we describe here                                                                 see a prominent increase in the 100% correct bin of the his-
we used all sentences in the data set. We did not find it nec-                                                             togram compared to the other presentations. Otherwise, we
essary to discriminate among these sentences with respect to                                                               see very similar distributions of mean ratings. Recall that in
their lengths.5                                                                                                            the binary presentation, all ratings are binary, and so the rat-
                                                                                                                           ings in the middle of the histogram correspond to cases where
Correlation of Aggregated Sentence Rating and                                                                              annotators have given different ratings in various proportions
Modes of Presentation                                                                                                      to the particular sentences.
The form of presentation in the questionnaire — how is the
task phrased, what type of options are available — for collect-
                                                                                                                           Gradience in Grammaticality Judgements
ing human judgements of grammaticality has been the subject                                                                The gradience we have observed here might, however, merely
of debate.                                                                                                                 reflect variation among individuals, each of whom could be
   As we have indicated above, our data set contains human                                                                 making binary judgments (Den Dikken, Bernstein, Tortora,
annotations for three modes of presentation: MOP2, MOP4                                                                    & Zanuttini, 2007). If this were the case, the aggregated
and MOP100. To investigate the impact of these presenta-                                                                   judgments would be variant, even if the underlying individ-
tion styles on judgements, we computed the Pearson correla-                                                                ual judgments are binary.
tion coefficient of mean sentence ratings between each pair of                                                                To establish that gradience is intrinsic to the judgments that
presentation modes. The results are summarised in Table 3.6                                                                each annotator is applying we looked at the distribution pat-
   The results strongly suggest that the aggregated rating is                                                              terns for individual annotators on each presentational mode.
    5 The translation path through Japanese seems to yield a stronger
                                                                                                                           A histogram that summarises the frequency of individual rat-
                                                                                                                           ings for MOP4 and MOP100 can demonstrate whether mid-
correlation between sentence rating and sentence length. This effect
is probably the result of the lower machine translation quality for                                                        dle ground options are commonly selected by annotators.
Japanese on longer sentences.                                                                                                 But a further question remains: Are middle ground options
    6 Note that for any pair that involves MOP100, only the 250 sen-
                                                                                                                           selected simply because they are available in the mode of pre-
tences common to the pair are considered. Recall that for MOP100
we solicited judgements for only 10% of the 2500 sentences in our                                                          sentation? As Armstrong, Gleitman, and Gleitman (1983)
test set.                                                                                                                  show, under some experimental conditions, subjects will rate
                                                                                                                     823

                            0.8                                                                             0.8                                                                             0.8
                            0.7                                                                             0.7                                                                             0.7
                            0.6                                                                             0.6                                                                             0.6
     Normalised Frequency                                                            Normalised Frequency                                                            Normalised Frequency
                            0.5                                                                             0.5                                                                             0.5
                            0.4                                                                             0.4                                                                             0.4
                            0.3                                                                             0.3                                                                             0.3
                            0.2                                                                             0.2                                                                             0.2
                            0.1                                                                             0.1                                                                             0.1
                            0.0 Extremely   Somewhat    Somewhat   Extremely                                0.0   Female    Somewhat   Somewhat        Male                                 0.0 Extremely   Somewhat   Somewhat   Extremely
                                Unnatural   Unnatural    Natural    Natural                                                  Female      Male                                                      Thin       Thin        Fat        Fat
                                      (a) Sentence Ratings                                                             (b) Gender Ratings                                                          (c) Body Weight Ratings
                                  Figure 2: Histograms of individual sentence, gender and body weight ratings using MOP4 presentations.
                            0.7                                                                             0.7                                                                             0.7
                            0.6                                                                             0.6                                                                             0.6
                            0.5                                                                             0.5                                                                             0.5
     Normalised Frequency                                                            Normalised Frequency                                                            Normalised Frequency
                            0.4                                                                             0.4                                                                             0.4
                            0.3                                                                             0.3                                                                             0.3
                            0.2                                                                             0.2                                                                             0.2
                            0.1                                                                             0.1                                                                             0.1
                   0.0
                  Extremely            20       40        60       80    Extremely                          0.0
                                                                                                            Female     20       40         60     80          Male                 0.0
                                                                                                                                                                                  Extremely            20       40       60       80    Extremely
                  Unnatural                                               Natural                                                                                                    Thin                                                  Fat
                                      (a) Sentence Ratings                                                             (b) Gender Ratings                                                          (c) Body Weight Ratings
                              Figure 3: Histograms of individual sentence, gender and body weight ratings using MOP100 presentations.
some odd numbers as being more typically odd than others.                                                                                  filtered out. On average we collected 15–20 annotations per
The fact that we have gradient results for individual judge-                                                                               image in each task.
ments may not, in itself, be evidence for an underlying gradi-                                                                                In Figure 2 and Figure 3 we present histograms giving the
ent category.                                                                                                                              (normalised) frequencies of individual ratings for the sen-
   To resolve these questions we ran two additional experi-                                                                                tence, gender and body weight experiments using MOP4
ments testing judgments for visually observable properties,                                                                                and MOP100 presentations, respectively. The results are
where one is clearly binary and the other gradient. Gender                                                                                 clear. For the gender experiment, middle ground ratings have
is generally regarded as a binary property, while body weight                                                                              a very low frequency, indicating that annotators tend not to
(fat vs. thin) exhibits gradience. We wanted to compare the                                                                                choose middle ground options, even when they are available.
frequency with which middle range values were selected for                                                                                 We see that the distribution of sentence ratings and body
each of these judgments, in order to secure a benchmark for                                                                                weight ratings display roughly similar patterns, suggesting
the distinction between binary and gradient judgement pat-                                                                                 that grammaticality is intrinsically a gradient judgment, like
terns.                                                                                                                                     body weight, rather than a binary one, like gender.
   For the gender experiment, we used 50 human portraits
and asked users on AMT to rate their genders. We used two                                                                                       A Second Sentence Annotation Experiment
modes of presentation: (1) MOP4 (female, somewhat female,
somewhat male and male); and (2) MOP100 (100-point scale                                                                                   We ran a second sentence annotation experiment using 100
from female to male). For the body weight experiment, we                                                                                   randomly selected linguistic textbook sentences, half of them
used 50 illustrations of body weights from very thin to very                                                                               good, and half of them starred, as described in the Introduc-
fat, and the same two modes of non-binary presentation.                                                                                    tion. Each HIT in this experiment contained 1 textbook sen-
   As with our syntactic acceptability experiments, we filtered                                                                            tence, 1 BNC original control sentence that had been highly
annotators to control for the quality of judgements. We in-                                                                                rated in the first experiment, and 3 back translated sentences
cluded a photograph of a male in each HIT for the gender                                                                                   that had previously received high, intermediate, and low rat-
experiment, and an image of an obese person in each HIT for                                                                                ings, respectively. We selected sentences with low variance
the body weight experiment. Annotators who were unable                                                                                     in annotation, and we limited sentence length so that all sen-
to identify these images to a minimal level of accuracy (i.e.                                                                              tences in a HIT were of comparable length. We filtered anno-
identifying them as male or rating them as being fat) were                                                                                 tators as in the first experiment. We tested each of the three
                                                                                                                                     824

                                                             0.8                                                                                      0.8
                                                             0.7                                                                                      0.7
                                                             0.6                                                                                      0.6
                                      Normalised Frequency                                                                     Normalised Frequency
                                                             0.5                                                                                      0.5
                                                             0.4                                                                                      0.4
                                                             0.3                                                                                      0.3
                                                             0.2                                                                                      0.2
                                                             0.1                                                                                      0.1
                                                             0.0 Extremely    Somewhat    Somewhat   Extremely                                        0.0 Extremely    Somewhat    Somewhat   Extremely
                                                                 Unnatural    Unnatural    Natural    Natural                                             Unnatural    Unnatural    Natural    Natural
                                                                        (a) Bad Sentences                                                                       (b) Good Sentences
                    Figure 4: Histograms of individual ratings of Adger’s sentences using MOP4 presentation.
                                                        0.6                                                                                      0.6
                                                       0.54                                                                                     0.54
                                                       0.48                                                                                     0.48
                                Normalised Frequency                                                                     Normalised Frequency
                                                       0.42                                                                                     0.42
                                                       0.36                                                                                     0.36
                                                        0.3                                                                                      0.3
                                                       0.24                                                                                     0.24
                                                       0.18                                                                                     0.18
                                                       0.12                                                                                     0.12
                                                       0.06                                                                                     0.06
                                                        0.01.0          1.6       2.2      2.8       3.4         4.0                             0.01.0          1.6       2.2      2.8       3.4         4.0
                                                                        (a) Bad Sentences                                                                       (b) Good Sentences
                       Figure 5: Histograms of mean ratings of Adger’s sentences using MOP4 presentation.
modes of presentation that we used in the first experiment.7                                                                                                                     Presentation Pair               Pearson’s r
                                                                                                                                                                                                                Bad Good
   We found that the mean and individual ratings for this ex-                                                                                                               MOP2 and MOP4                       0.83   0.91
periment yielded the same pattern of gradience for the two                                                                                                                 MOP2 and MOP100                      0.89   0.85
non-binary modes of presentation that we observed in our first                                                                                                             MOP4 and MOP100                      0.89   0.87
experiment. As we would expect, the good sentences tend
heavily towards the right side of the graph, and the starred                                                                             Table 3: Pearson’s r of mean rating of Adger’s sentences for
sentences to the left. But for the starred sentences there is                                                                            different pairs of presentation.
substantial distribution of judgements across the points in the
left half of the graph. Analogously judgements for the good
sentences are spread among the points of the right side.8                                                                                tinct HIT sets.
   We again observed a high Pearson correlation in the pair-
wise comparison of the three modes of presentation. These                                                                                                     Current and Future Work: Modelling
are displayed in Table 3. The histograms for the individual                                                                                                         Speakers’ Judgements
and the mean ratings for the four category presentation are
                                                                                                                                         We are interested in developing models that assign a degree of
given in Figures 4 and 5, respectively.
                                                                                                                                         grammaticality to a sentence that tracks speakers’ judgements
   Interestingly, we also found very high Pearson correlations
                                                                                                                                         of acceptability. Clark, Giorgolo, and Lappin (2013) propose
(0.93-0.978) among the annotations of the non-textbook sen-
                                                                                                                                         an enriched n-gram model and a number of scoring functions
tences in the first and second experiments, across each mode
                                                                                                                                         — which are procedures for mapping log probability distri-
of presentation, for each pairwise comparison. This indicates
                                                                                                                                         butions into scoring distributions — to estimate acceptability
that judgements were robustly consistent across the experi-
                                                                                                                                         judgements for a range of passive sentences.
ments, among different annotators, and in the context of dis-
                                                                                                                                            To test the feasibility of constructing such a model of gram-
    7 The full data sets of annotated sentences for both sentence anno-                                                                  maticality, we trained a trigram word model on the full BNC
tation experiments are available from the Experiments and Results                                                                        (with our 2500 test sentences removed), using the method-
page of the SMOG project website at
http://www.dcs.kcl.ac.uk/staff/lappin/smog/.                                                                                             ology that they describe. We applied their suite of scoring
    8 All 469 of Adger’s examples appear in the appendices of                                                                            functions to the log probability distributions for our 2500
Sprouse and Almeida (2012). Our results are compatible with those                                                                        sentence test set, and we computed the Pearson correlation
that Sprouse and Almeida report, but, to the extent that acceptability                                                                   between the values of each scoring function and the human
judgements provide the primary data for identifying grammaticality,
they are not consistent with the view that grammaticality is a binary                                                                    ratings from our experiments. The results are summarised in
property.                                                                                                                                Table 4, where ”CGL Scoring Function” denotes the scoring
                                                                                                                       825

     CGL Scoring Function          MOP2       MOP4       MOP100                                       References
            Logprob                 0.238      0.292       0.278
         Mean Logprob               0.319      0.349       0.327            Adger, D. (2003). Core syntax: A minimalist approach. Ox-
    Weighted Mean Logprob           0.384      0.412       0.379              ford, UK: Oxford University Press.
    Syntactic Log Odds Ratio        0.387      0.409       0.391
            Minimum                 0.328      0.344       0.368            Ambridge, B., Pine, J. M., & Rowland, C. F. (2012). Seman-
      Mean of First Quartile        0.386      0.412       0.410              tics versus statistics in the retreat from locative overgener-
                                                                              alization errors. Cognition, 123(2), 260–279.
Table 4: Pearson’s r of various scoring functions and sen-                  Armstrong, S. L., Gleitman, L. R., & Gleitman, H. (1983).
tence ratings collected using MOP2, MOP4 and MOP100                           What some concepts might not be. Cognition, 13(3), 263–
presentations. The best results for each mode of presentation                 308.
are indicated in boldface.                                                  BNC Consortium. (2007). The British National Corpus, ver-
                                                                              sion 3 (BNC XML Edition). Distributed by Oxford Univer-
                                                                              sity Computing Services on behalf of the BNC Consortium.
functions proposed in Clark et al. (2013). We see an encour-                  Retrieved from http://www.natcorp.ox.ac.uk/
aging correlation for several of these functions. This is still a           Chomsky, N. (1965). Aspects of the theory of syntax. MIT
very tentative result, but it offers grounds for optimism, given              Press.
that it was achieved with a simple extension of a very basic                Clark, A., Giorgolo, G., & Lappin, S. (2013). Statistical
trigram model.                                                                representation of grammaticality judgements: the limits of
   For future work, we will be experimenting with more so-                    n-gram models. In Proceedings of the acl workshop on
phisticated probabilistic models of sentence grammaticality,                  cognitive modelling and computational linguistics (pp. 28–
and we will be exploring supervised models for combining                      36).
these scoring functions to identify the most significant fea-               Clark, A., & Lappin, S. (2011). Linguistic nativism and the
tures of the models and the functions.                                        poverty of the stimulus. Malden, MA: Wiley-Blackwell.
                                                                            Crocker, M. W., & Keller, F. (2005). Probabilistic gram-
               Discussion and Conclusions                                     mars as models of gradience in language processing. In
                                                                              G. Fanselow, C. Féry, R. Vogel, & M. Schlesewsky (Eds.),
The experiments that we describe here provide clear evidence                  Gradedness. Oxford University Press.
that syntactic acceptability is a gradient rather than a binary             Den Dikken, M., Bernstein, J. B., Tortora, C., & Zanuttini,
concept. This much is relatively uncontroversial.                             R. (2007). Data and grammar: Means and individuals.
   In the absence of strong considerations for positing an in-                Theoretical Linguistics, 33(3), 335–352.
dependent binary notion of grammaticality, we take this ev-                 Fanselow, G., Féry, C., Schlesewsky, M., & Vogel, R. (Eds.).
idence as motivation for the view that speakers’ acceptabil-                  (2006). Gradience in grammar: Generative perspectives.
ity judgements reflect a representation of grammatical well-                  Oxford University Press.
formedness that is intrinsically gradient.                                  Keller, F. (2000). Gradience in grammar: Experimental and
   This conclusion has significant consequences for the na-                   computational aspects of degrees of grammaticality. Un-
ture of syntactic representation. If gradience is intrinsic to                published doctoral dissertation, University of Edinburgh.
syntax, then the formal system that encodes human grammat-                  Manning, C. (2003). Probabilistic syntax. In R. Bod, J. Hay,
ical knowledge must generate the range of variation in well-                  & S. Jannedy (Eds.), Probabilistic linguistics. The MIT
formedness that is reflected in speakers’ judgements. Classi-                 Press.
cal formal grammars cannot do this.                                         Snow, R., O’Connor, B., Jurafsky, D., & Ng, A. Y. (2008).
   One obvious alternative is the class of probabilistic gram-                Cheap and fast—but is it good?: Evaluating non-expert an-
mars that have been developed in statistical parsing. How-                    notations for natural language tasks. In Proceedings of the
ever, as Clark and Lappin (2011) point out, grammatical well-                 conference on empirical methods in natural language pro-
formedness cannot be directly reduced to probability. Our                     cessing (EMNLP-2008) (pp. 254–263). Honolulu, Hawaii.
current work on tracking speakers’ acceptability judgements                 Sorace, A., & Keller, F. (2005). Gradience in linguistic data.
with enriched language models is an effort to modify systems                  Lingua, 115(11), 1497–1524.
originally designed for language technology to capture im-                  Sprouse, J. (2007). Continuous acceptability, categorical
portant cognitive features of linguistic representation.                      grammaticality, and experimental syntax. Biolinguistics,
                                                                              1, 123–134.
                       Acknowledgments                                      Sprouse, J. (2011). A validation of amazon mechanical turk
                                                                              for the collection of acceptability judgments in linguistic
We are grateful to three anonymous reviewers for helpful comments
                                                                              theory. Behavior Research Methods, 43, 155–167.
on an earlier draft of this paper. The research described in this paper
                                                                            Sprouse, J., & Almeida, D. (2012). Assessing the reliability
was done in the framework of the Statistical Models of Grammati-
                                                                              of textbook data in syntax: Adger’s core syntax. Journal of
cality (SMOG) project at King’s College London, funded by grant
                                                                              Linguistics, 48(3), 609–652.
ES/J022969/1 from the Economic and Social Research Council of
the UK.
                                                                        826

