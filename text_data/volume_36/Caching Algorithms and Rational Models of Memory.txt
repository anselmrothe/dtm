UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Caching Algorithms and Rational Models of Memory
Permalink
https://escholarship.org/uc/item/4x26t6dm
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Press, Avi
Pacer, Michael
Griffiths, Thomas
et al.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                          Caching Algorithms and Rational Models of Memory
                                                Avi Press (avipress@berkeley.edu)
                                              Michael Pacer (mpacer@berkeley.edu)
                                      Thomas L. Griffiths (tom griffiths@berkeley.edu)
                      Department of Psychology, University of California, Berkeley, Berkeley, CA 94720 USA
                                        Brian Christian (brian.christian@berkeley.edu)
             Institute of Cognitive and Brain Sciences, University of California, Berkeley, Berkeley, CA 94720 USA
                             Abstract                                 yet fast block of memory (normally due to its hardware de-
                                                                      sign and proximity to the processor), where a limited set of
   People face a problem similar to that faced by algorithms that
   manage the memory of computers: trying to organize informa-        items can be stored. Every time the computer needs data that
   tion to maximize the chance it will be available when needed       is not in the cache it must fetch it from somewhere that will
   in the future. In computer science, this problem is known as       take far more time to access, such as a hard disk. Whenever
   “caching”. Inspired by this analogy, we compared the prop-
   erties of a model of human memory proposed by Anderson             an item is added to the cache, the computer must use an al-
   and Schooler (1991) and caching algorithms used in computer        gorithm to decide which other item to evict. The computer is
   science. We tested each algorithm on a dataset relevant to hu-     thus constantly deciding what to forget, managing its limited
   man cognition: headlines from the New York Times. In addi-
   tion to overall performance, we investigated whether the algo-     memory resources to maximize the probability of the cache
   rithms from computer science replicated the well-documented        containing the items most likely to be needed.
   effects of recency, practice, and spacing on human memory.            In this paper, we explore the consequences of a rational
   Anderson and Schooler’s model performed comparably to the
   worst caching algorithms, but was the only model that captured     analysis of human memory that assumes finite, rather than
   the spacing effects seen in human memory data. All models          infinite, capacity. By looking at what happens when an envi-
   showed similar effects of recency and practice.                    ronment is filtered through a finite cache, we can determine
   Keywords: memory, caching algorithms, rational analysis            whether the statistical patterns corresponding to practice, re-
                                                                      cency, and spacing effects are relevant to successfully man-
                          Introduction                                aging a finite memory. This is also potentially valuable for
Between our own experience forgetting things and the vol-             computer science, as we can see whether a caching scheme
umes of literature describing how fallible our memory is, it is       based on human memory improves on existing algorithms.
easy to be critical of human memory. On the other hand, those            The plan of the paper is as follows. First, we summarize
with hyperthymestic syndrome (who can’t help but remember             the memory phenomena that have been used to evaluate ra-
excessive details about every day of their lives) struggle to         tional models, and describe the models themselves. Next, we
deal with their inability to forget useless information (Parker,      define the problem of caching and introduce a set of caching
Cahill, & McGaugh, 2006). How does our brain know what                algorithms. These algorithms are then evaluated in a set of
information should be kept and what should be forgotten?              four simulations. The first assesses overall performance. The
   An analogue of the problem faced by human memory – as              others explore practice, recency, and spacing effects in turn.
pointed out by Anderson and Milson (1989) – is a library try-
ing to determine which books to keep in its collection. With
                                                                                            Human Memory
finite shelf space, the library needs to decide which books are       Following Anderson and Schooler (1991), we will focus on
most likely to be needed in the future, relegating the others         three properties of human memory: practice, recency, and
to long-term storage. Anderson and Milson used this obser-            spacing. We review these phenomena, then turn to how they
vation as inspiration for a rational model of human memory,           have been explained using rational models of memory.
which prioritizes stored information by how likely it is to be        Behavioral Phenomena
needed in the future. Anderson and Milson showed that this
                                                                      Practice The first two phenomena – practice and recency
approach captured several phenomena of human memory.
                                                                      – are based on data collected by Ebbinghaus (1885/1913).
   However, the rational model proposed by Anderson and
                                                                      Through rigorous self-experimentation, Ebbinghaus was able
Milson deviates from the library analogy in allowing in-
                                                                      to discover some of the most basic aspects of human memory.
finitely many items to be stored in memory, with retrieval
                                                                      The practice effect is simply that more times an item has been
failure being the result of the need probability of a target item
                                                                      encountered, the more likely it can be recalled. Subsequent
being below a threshold determined by the cost of searching.
                                                                      work has attempted to identify the form of the relationship
But an alternative construal of the problem is much closer to
                                                                      between practice and retention, and found that a power-law
the original analogy: What if human memory really did have
                                                                      best captures this relationship (Newell & Rosenbloom, 1981).
finite capacity? How should we choose what to forget?
   The problem of choosing what to forget is an instance of           Recency Ebbinghaus also noted that the more recently an
what computer scientists term caching. A cache is a small             item was encountered, the more likely it can be recalled.
                                                                  1198

The form of this relationship has been debated (e.g., Lof-             parameter setting in the analyses we present in this paper.
tus, 1985), but Anderson and Schooler (1991) showed that
the data from Ebbinghaus (1885/1913) followed a power law.                         An Alternative Rational Analysis
Spacing Memory research has taken significant steps since              The strength function proposed by Anderson and Schooler
the original work conducted by Ebbinghaus, with one impor-             (1991) – like the need probability considered by Anderson
tant discovery being the existence of spacing effects (e.g.,           and Milson (1989) – is simply used to prioritize items in
Glenberg, 1976). The number and recency of encounters                  memory. The set of items is assumed to be infinite, with
with items are not sufficient to determine performance: peo-           search through memory proceeding in decreasing order of
ple are also sensitive to the amount of time that passes be-           strength and terminating when strength falls below a thresh-
tween successive encounters. This pattern can be described             old. While the set of things a person can remember is not ex-
by the amount of time passing in between individual encoun-            plicitly limited, the recency effect imposes an implicit upper
ters (study lag) and how much time passed since the last en-           bound. As time passes, strength of memories are decaying,
counter (test lag). For a short test lag, recall is better with a      and eventually their strengths fall below the threshold.
smaller study lag. However, for longer test lag, recall actually           An alternative rational analysis might consider a different
increases with study lag. A longer interval between encoun-            cost function: what if memory is finite, and a cost is incurred
ters thus seems to establish longer-lasting memories.                  for failing to retrieve an item? The goal then is to maximize
                                                                       the chance that an item is already contained in memory when
Rational Models of Memory                                              it is needed. Under this alternative view, need probabilities re-
Anderson and Milson (1989) proposed that the problem hu-               main critical – memory should contain only those items with
man memory faces is organizing information in order to facil-          the highest need probabilities. But forgetting is also obliga-
itate retrieval. More formally, if accessing an item in memory         tory, as new items force out old.
incurs a cost C and finding the target item results in gain G,             While the change from an infinite capacity with a cost for
then a rational agent should access items in decreasing or-            searching to a finite capacity with a cost for failure might
der of the probability p that they are the target item, stopping       seem minor, it potentially has significant effects on the re-
when pG < C. The challenge, then, is to calculate the prob-            sulting models. For example, even though Anderson and
ability that an item is likely to be the target. One component         Schooler showed that practice, recency, and spacing all af-
of this is the probability that an item is likely to be needed at      fect need probability, their degree of influence could differ.
a given moment – the need probability.                                 Optimally managing finite memory resources might require
   Subsequent work by Anderson and Schooler (1991) sug-                attending to some of these factors more than others.
gested that human memory need not perform complex cal-                     In the remainder of the paper, we explore the consequences
culations of need probabilities. They showed that effects of           of adopting this alternative view of the problem faced by hu-
practice, recency, and spacing are consistent with statistical         man memory. Importantly, adopting this view allows us to
patterns that appear in human environments – such as which             explore potential links between human memory and the algo-
words appear in the headlines of articles in the New York              rithms used for cache management by computers.
Times. They calculated the probability that a word would ap-
pear in a headline as a function of its pattern of occurrences in                          Caching Algorithms
headlines over the previous 100 days. This analysis showed             Many caching algorithms can be cast in similar terms to
that need probability increased as a power-law in the num-             Anderson and Schooler’s (1991) memory model, assigning
ber of previous occurrences (a practice effect), decreased as          a strength to items and evicting the item with the lowest
a power-law in the amount of time since the last occurrence            strength when a new item is added to the cache. This pro-
(a recency effect), and was higher for less recent items when          vides a natural basis for comparison of these approaches. In
those items had been more widely spaced (a spacing effect).            this section, we summarize a set of algorithms that computer
   On the basis of these results, Anderson and Schooler pro-           scientists have developed to solve the problem of memory
posed that a reasonable proxy for need probability could be            caching. Some of the cache policies encode recency, others
defined by assigning a “strength” to each item in memory.              frequency, and some try to balance both. In the remainder of
The strength function they suggested was                               the paper, we evaluate the performance of these algorithms
                                     n                                 on data from the human cognitive environment (ie., the New
                        SAS91 = A ∑ s(ti ),                     (1)    York Times) and compare them to Anderson and Schooler’s
                                    i=1                                (1991) model (Equation 1; henceforth AS91) with respect to
where A is a constant, n is the number of times the item has           the way in which recency, practice, and spacing affect recall.
occurred, ti is the time of the ith occurrence, s(ti ) = ti−di and
di = max[d1 , b(ti − ti−1 )−d1 ]. being the how the strength from      Least Recently Used (LRU)
the ith occurrence decays with time. d1 is a tunable parameter         Perhaps the simplest of caching algorithms, LRU evicts the
of the model. With d1 = 0.125, the model was able to repro-            item that was used least recently (Belady, 1966). Because the
duce the practice, recency, and spacing effects, so we use this        only information needed to implement LRU is order of uses,
                                                                   1199

it can be implemented with just a simple list that preserves         1968). If an item is in the queue that will be evicted from,
this information. A strength function consistent with LRU is         then its strength will be equivalent to the LRU or LFU func-
                                                                     tions (depending on which queue it is). If the item is not in
                                     1                               this queue, its strength is essentially infinite; it will never be
                       SLRU =                .               (2)
                               tcurrent − tn                         evicted until the size of the LFU queue changes. An issue
where tcurrent is the current time and tn is the time of the nth     with 2Q is that it may be hard to estimate the appropriate size
(ie., last) occurrence of the item. LRU has proved to be very        for the LFU queue ahead of time.
successful in computer science applications since it can ex-         Adaptive Replacement Cache (ARC)
ploit the “locality” of computer behavior, where an item being
                                                                     ARC (Megiddo & Modha, 2004) is very similar to 2Q but
used once makes it likely to be used again in a short interval.
                                                                     with the capacity to adapt to the environment. In ARC, the
Least Frequently Used (LFU)                                          maximum size of the LFU queue changes as the data comes
Another straightforward (yet extreme) approach is to evict the       in. The algorithm keeps track of items that have been evicted
item that has been used least. LFU’s strength function is            from each queue. Upon a cache miss (ie., a failed retrieval)
                                                                     for an item that was recently evicted from one of the queues,
                            SLFU = n,                        (3)     ARC will make that queue larger, as it got rid of an item that
                                                                     it should have kept.
where n is the number of times the item has occurred. An
item’s strength thus grows the more times it is used. LFU            LRFU
can be very effective when items are used often but not nec-         LRFU (Kim, 2001) subsumes both LRU and LFU. Each
essarily in temporal proximity. A major drawback to LFU is           item has a combined recency-frequency count which is its
that the cache can become littered with items that were once         strength. Intuitively, an item’s strength continually climbs
extremely popular but might never be used again. This issue          each time it is used but that strength decays with time. The
can actually be quite extreme; a pure LFU system is fairly           exact function can vary, but as suggested by Kim (2001) we
uncommon in computer science.                                        used the strength function
LRU-2                                                                                              n
                                                                                                      1 λ(tcurrent −ti )
                                                                                         SLRFU = ∑                       ,          (5)
LRU-2 evicts the item with the least recent penultimate use.                                      i=1 2
Both LRU and LRU-2 can be described as being part of the
LRU-k family of algorithms (O’Neil, O’Neil, & Weikum,                where λ is a tunable parameter. For our purposes, λ = 0.001
1999), where the item with the least recent kth use is evicted       worked well. We note that LRFU particularly closely resem-
(LRU being LRU-k for k = 1). This corresponds to                     bles the memory model proposed by Anderson and Schooler
                                                                     (1991), with an exponential rather than a power-law decrease
                                       1                             in strength (compare Equations 1 and 5).
                   SLRU−k =                                  (4)
                              tcurrent − tn−k+1
                                                                     Random
where ti is the time of the ith occurrence. Simple LRU does          A simple alternative to these complex caching algorithms is
not account for frequency, so LRU-k is a way to introduce            to assign each item a random strength. This means that a ran-
frequency into an algorithm that is also sensitive to recency.       dom item will be evicted whenver a new item is introduced.
Behavior becomes closer to LFU as k increases, and closer            This provides a reasonable lower bound on performance.
to LRU as k approaches 1. A common compromise between
frequency and recency is to take k = 2 (O’Neil et al., 1999).        Belady’s Algorithm
                                                                     Belady’s algorithm (Belady, 1966) is the optimal caching al-
2Q                                                                   gorithm, providing an upper bound on performance. How-
2Q (Johnson & Shasha, 1994) tries to find a balance between          ever, it achieves this optimality by being able to see the future,
accounting for recency and frequency by splitting the cache          providing it with an unfair advantage over other algorithms.
into two queues (and technically a third queue, but that will        The algorithm works simply by evicting the item that will
be mentioned later). The first queue is managed as an LRU            be used the furthest in the future. This is not a real caching
queue. If a hit (ie., successful retrieval of an item) occurs in     policy, because we will never know the sequence of accesses
this queue, the item is promoted to the second queue, which          ahead of time. We use it in this paper in order to compare
is managed as a LFU queue. The LFU queue has a predefined            each algorithm to the optimal caching policy, and to examine
maximum size, so items will be evicted from the LFU queue            what environmental statistics it exploits.
if the queue is larger than the predefined size, and evicted
from the LRU queue otherwise. This policy can be interpreted                    Simulation 1: Overall Miss Rate
in an interesting way: The first queue could be thought of as        Our first analysis compared the overall performance of all of
short term memory, and items used enough get promoted to             the algorithms as caching policies, focusing on the rate of
the second queue, long term memory (Atkinson & Shiffrin,             cache misses on data from a human environment.
                                                                 1200

Methods                                                                 was maintained when data were filtered through a cache.
A dataset of headlines from the New York Times from January
                                                                        Methods
1986 to December 1987 was used to test the various caching
algorithms. This is one of the datasets used by Anderson and            The New York Times headlines were used again for this simu-
Schooler (1991) in their analysis of environmental statistics.          lation (and all subsequent simulations). We used a cache of a
Words from the headlines were sequentially cached, each                 fixed size, |C| = 128 items, although effects were similar for
word being treated as a separate item. Words were sanitized             other cache sizes. For each 100 day interval in the dataset,
of punctuation, and made lower-case. We calculated the ratio            we looked at a word’s probability of being in the cache on the
of misses to hits for a range of cache sizes. We also ran a             101st day, based on the number of times it was used in the in-
test where we removed the top ten most used English words               terval. This is very similar to what Anderson and Schooler’s
(“the”, “and”, etc.) according to Wikipedia. The differences            (1991) analysis, except that they looked at what words were
were negligible, and thus not included.                                 actually used on the 101st day.
                                                                        Results and Discussion
                  10
                                                       LRU              An effect of practice was observed for all cache policies, as
                                                       LFU              shown in Figure 2. Effects of practice were, as might be ex-
                                                       LRU2
                                                       2Q               pected, strongest for LFU and weakest for LRU. LFU and
                      8
                                                       ARC              then LRFU were most affected by practice, both even more
                                                       LRFU-0.001
     Miss/hit ratio
                                                       Random           so than Belady’s algorithm. None of the algorithms showed a
                      6                                AS91
                                                       Belady
                                                                        clear power-law relationship between practice and hit proba-
                                                                        bility, but this may be due to the rapid saturation of the per-
                      4
                                                                        formance curves.
                      2                                                                 Simulation 3: Recency
                                                                        We next examined whether a recency effect – higher prob-
                      0                                                 abilities for more recent items – was shown by the caching
                          100     200      300       400       500
                                Cache size (items)                      algorithms.
Figure 1: Overall miss rates on words in headlines from the             Methods
New York Times as a function of cache size. Miss/hit ratio is           The same methods were used in this simulation as used in
the ratio of the number of misses to the number of hits.                Simulation 2. This time we calculated probability that a word
                                                                        was in the cache on the 101st day given the word’s recency
                                                                        (ie., how many days had passed since it had been used).
Results and Discussion
The miss to hit ratio dropped at a similar rate across caching          Results and Discussion
policies, as shown in Figure 1. Anderson and Schooler’s                 All algorithms showed a strong recency effect, as shown in
(1991) model performed similarly to the worst algorithms,               Figure 3, although LFU and AS91 were the only algorithms
being only slightly better than random eviction. LFU and                to show a power-law relationship. Not surprisingly, the drop-
LRFU were the best algorithms for this data set, producing              off was steepest for LRU. Interestingly, Belady’s algorithm
performance closest to Belady’s algorithm. LFU performed                showed a recency effect that did not follow a power-law rela-
worse than LRFU for small cache sizes, but then overtook it             tionship, indicating that a faster decrease in the influence of
as cache size approached and exceeded 256 items.                        recency is optimal for this dataset.
   The high performance of LFU is surprising, given that LRU
generally outperforms LFU in comparisons based on calls                                 Simulation 4: Spacing
to computer memory (e.g., Kim, 2001; Megiddo & Modha,                   Our final simulation examined whether the caching algo-
2004). The superior performance of LFU on this dataset may              rithms were sensitive to spacing, analyzing the interaction
reflect two important things about human language. First,               between how recently a word had been encountered and how
word frequency follows a power-law distribution, with a few             much time passed between successive instances of that word.
words accounting for a large proportion of overall occur-
rences (Zipf, 1949). Second, this distribution is only be sub-          Methods
ject to weak influences of locality. For example, “the” will            Our analysis followed the approach taken by Anderson and
remain a high frequency word even if “Gaddafi” appears in               Schooler (1991). We identified words that had appeared ex-
the headlines over a series of days.                                    actly twice in the previous 100 days, and divided them into
                                                                        those that had a “short lag” (10 days or less between occur-
                          Simulation 2: Practice                        rences) and those that had a “long lag” (more than 10 days
Our second analysis examined whether the practice effect –              between occurrences). We then examined the effect of re-
higher probabilities for items that appear more frequently –            cency for these two sets of words.
                                                                     1201

                           1.0
                                                                                                                                               LRU
(a)                        0.9                                                                        (b)                         6            LFU
                                                                                                                                               LRU2
                           0.8                                                                                                                 2Q
                                                                                                                                  4            ARC
                           0.7                                                                                                                 LRFU-0.001
         Hit probability                                                                                       Log odds of hit
                                                                                                                                               AS91
                           0.6                                                                                                                 Belady
                                                                                   LRU                                            2
                           0.5
                                                                                   LFU
                           0.4                                                     LRU2
                                                                                   2Q                                             0
                           0.3                                                     ARC
                                                                                   LRFU-0.001
                           0.2                                                     AS91
                                                                                   Belady                                        −2
                           0.1
                              0   50   100      150        200        250    300      350   400                                       0             1           2             3            4
                                             Number of times seen                                                                                       Log number of times seen
Figure 2: Practice effects. (a) Probability of a cache hit as a function of number of occurrences in the preceding 100 days. (b)
Log odds of a hit as a function of log number of occurrences.
                        0.25                                                                                                                        LRU          2Q                   AS91
                                                                                   LRU                                                              LFU          ARC                  Belady
(a)                                                                                LFU                (b)                                           LRU2         LRFU-0.001
                                                                                                                                 10
                                                                                   LRU2
                        0.20                                                       2Q
                                                                                   ARC
                                                                                   LRFU-0.001
                                                                                                            Log odds of hit
                                                                                                                                  5
      Hit probability
                        0.15                                                       AS91
                                                                                   Belady
                                                                                                                                  0
                        0.10
                                                                                                                              −5
                        0.05
                                                                                                                              −10
                        0.00
                                  10     20           30         40         50       60         70                                        0             1           2             3            4
                                  Number of days since last occurrence                                                                        Log number of days since last occurrence
Figure 3: Recency effects. (a) Probability of a cache hit as a function of number of days since last occurrence. (b) Log odds of
a hit as a function of log of number of days since last occurrence.
Results and Discussion                                                                                  problem of caching as opposed to prioritization.
The results are shown in Figure 4. In short, no caching algo-                                              Second, the high performance of LFU and related algo-
rithms other than AS91 showed a spacing effect. Only AS91                                               rithms is at odds with previous results in computer science,
showed a higher hit rate for less recent items with a long lag.                                         and suggests that different caching algorithms may be ideal in
                                                                                                        human environments than computer environments. This is an
                                        Conclusions                                                     interesting finding that deserves further investigation through
Our analysis of Anderson and Schooler’s (1991) model as a                                               the analysis of other datasets derived from human environ-
caching policy, and of algorithms from computer science as                                              ments. However, it creates an opportunity to explore other
potential models of human memory, yields three conclusions.                                             algorithms that may be effective for caching in contexts rele-
First, Anderson and Schooler’s model performed poorly as a                                              vant to humans.
caching policy on the New York Times dataset. This is not                                                  Finally, while analogues of practice and recency effects
a critique of the model, as it was not designed to solve this                                           appear in the behavior of caching algorithms, tracking the
problem, but it is surprising that a model that was designed                                            spacing of items does not appear to improve performance for
to capture need probabilities performs so poorly. In particu-                                           this dataset in particular. Furthermore, higher-performing
lar, it shows that there is potentially a bigger gap than might                                         caching algorithms did not show a power-law effect of
have been anticipated between the construals of memory as a                                             recency. The inclusion of Belady’s algorithm in our analysis
                                                                                                     1202

                                 0.25
                                                   LRU                  0.009
                                                                                              LFU                        0.7
                                                                                                                                         LRU2                  0.35
                                                                                                                                                                                    2Q
                                                                        0.008                                            0.6
                                 0.20                                                                                                                          0.30
                                                                        0.007
                                                                                                                         0.5
                                                                                                                                                               0.25
                                 0.15                                   0.006
                                                                                                                         0.4
              Hit probability
                                                                        0.005                                                                                  0.20
                                                          Short Lag
                                 0.10                                                                                    0.3
                                                          Long Lag      0.004                                                                                  0.15
                                                                                                                         0.2
                                 0.05                                   0.003
                                                                                                                                                               0.10
                                                                                                                         0.1
                                                                        0.002
                                 0.00                                                                                                                          0.05
                                                                        0.001                                            0.0
                                −0.05                                   0.000                                          −0.1                                    0.00
                                     5   10 15 20 25 30 35 40 45             5   10   15    20   25     30   35   40       5    10 15 20 25 30 35 40 45            0   5     10 15 20 25 30 35 40
                                 0.30
                                                   ARC                   0.20
                                                                                           LRFU-0.001                  0.028
                                                                                                                                         AS91                  0.12
                                                                                                                                                                                  Belady
                                                                                                                       0.026
                                 0.25                                                                                                                          0.10
                                                                         0.15                                          0.024
                                 0.20
                                                                                                                       0.022                                   0.08
                                                                         0.10
              Hit probability
                                 0.15                                                                                  0.020
                                                                                                                                                               0.06
                                 0.10                                                                                  0.018
                                                                         0.05
                                                                                                                       0.016                                   0.04
                                 0.05
                                                                         0.00                                          0.014
                                 0.00                                                                                                                          0.02
                                                                                                                       0.012
                                −0.05                                   −0.05                                          0.010                                   0.00
                                     5   10 15 20 25 30 35 40 45             5   10 15 20 25 30 35                40        5   10 15 20 25 30 35 40 45            0   5     10 15 20 25 30 35 40
                                           Days between presentations             Days between presentations                      Days between presentations               Days between presentations
Figure 4: Spacing effects. Probability of a cache hit as a function of number of days since last occurrence, plotted separately
for pairs of occurrences separated by a short lag (up to 10 days) and a long lag (more than 10 days) with 1 standard error.
is particularly instructive, as it indicates what statistical                                                               Proceedings of the 20th International Conference on Very Large
patterns are relevant to optimal performance, and shows                                                                     Data Bases, 439-450.
                                                                                                                          Kim, C. S. (2001). LRFU: A spectrum of policies that subsumes
neither spacing nor a power-law for recency. To the extent                                                                  the least recently used and least frequently used policies. IEEE
that these phenomena appear in human memory, and human                                                                      transactions on Computers, 50(12), 1352–1361.
memory is assumed to be adaptive, these findings provide                                                                  Loftus, G. R. (1985). Evaluating forgetting curves. Journal of
                                                                                                                            Experimental Psychology: Learning, Memory, and Cognition,
evidence that the assumption of infinite capacity may be                                                                    11(2), 397-406.
more appropriate as the basis for a rational analysis.                                                                    Megiddo, N., & Modha, D. S. (2004). Outperforming LRU with an
                                                                                                                            adaptive replacement cache algorithm. Computer, 37(4), 58–65.
                                                                                                                          Newell, A., & Rosenbloom, P. S. (1981). Mechanisms of skill
Acknowledgments. This work was supported by grant number                                                                    acquisition and the law of practice. Cognitive skills and their
                                                                                                                            acquisition, 1–55.
SMA-1228541 from the National Science Foundation.                                                                         O’Neil, E. J., O’Neil, P. E., & Weikum, G. (1999). An optimality
                                                                                                                            proof of the LRU-K page replacement algorithm. Journal of the
                                              References                                                                    ACM (JACM), 46(1), 92–112.
Anderson, J. R., & Milson, R. (1989). Human memory: An adaptive                                                           Parker, E. S., Cahill, L., & McGaugh, J. L. (2006). A case of unusual
  perspective. Psychological Review, 96(4), 703.                                                                            autobiographical remembering. Neurocase, 12, 35-49.
Anderson, J. R., & Schooler, L. J. (1991). Reflections of the envi-                                                       Zipf, G. K. (1949). Human behavior and the principle of least effort.
  ronment in memory. Psychological science, 2(6), 396–408.                                                                  New York: Addison-Wesley.
Atkinson, R. C., & Shiffrin, R. M. (1968). Human memory: A
  proposed system and its control processes. The psychology of
  learning and motivation, 2, 89–195.
Belady, L. A. (1966). A study of replacement algorithms for a
  virtual-storage computer. IBM Systems Journal, 5(2), 78–101.
Denning, P. J. (1980). Working sets past and present. IEEE Trans-
  actions on Software Engineering, 6(1), 64–84.
Ebbinghaus, H. (1885/1913). Memory: A contribution to experi-
  mental psychology (No. 3). Teachers college, Columbia univer-
  sity.
Glenberg, A. M. (1976). Monotonic and nonmonotonic lag effects
  in paired-associate and recognition memory paradigms. Journal
  of Verbal Learning and Verbal Behavior, 15(1), 1-16.
Johnson, T., & Shasha, D. (1994). 2Q: A low overhead high per-
  formance buffer management replacement algorithm. VLDB ’94
                                                                                                                  1203

