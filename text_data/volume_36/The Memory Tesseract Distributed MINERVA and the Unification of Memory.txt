UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The Memory Tesseract: Distributed MINERVA and the Unification of Memory
Permalink
https://escholarship.org/uc/item/5qk129kd
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Kelly, Matthew
Mewhort, Douglas
West, Robert
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                                    The Memory Tesseract:
                          Distributed MINERVA and the Unification of Memory
                                       Matthew A. Kelly (matthew_kelly2@carleton.ca) 1
                                       Douglas J. K. Mewhort (mewhortd@queensu.ca) 2
                                           Robert L. West (robert_west@carleton.ca) 1
                                          1 Institute of Cognitive Science, Carleton University
                                       1125 Colonel By Drive, Ottawa, Ontario, K1S 5B6 Canada
                                            2 Department of Psychology, Queen’s University
                                          62 Arch Street, Kingston, Ontario, K7L 3N6 Canada
                              Abstract                                    Variations on the MINERVA 2 model address an even
                                                                       broader range of phenomena. MINERVA-AL makes and
   We prove that MINERVA 2, a widely-used model of biologi-
   cal long-term memory, is mathematically equivalent to an            corrects predictions to capture numerous associative learn-
   auto-associative memory implemented as a fourth order ten-          ing phenomena from both the animal and human learning
   sor. We further propose an alternative implementation of            literature (Jamieson, Crump, & Hannah, 2012). Kwantes
   MINERVA 2 as a holographic lateral inhibition network. Our          (2005) used MINERVA to study how semantic similarity
   work clarifies the relationship between MINERVA 2 and other         can be learned from word co-occurrence in the language.
   memory models, and shows that MINERVA 2 and derivative              Thomas et al. (2008) used MINERVA to study hypothesis
   models can be neurally implemented and scaled-up to long-           generation and probability judgement in humans.
   term learning tasks.                                                   In this paper, we use the term MINERVA 2 to refer spe-
   Keywords: memory; cognitive modelling; MINERVA 2;                   cifically to Hintzman’s model, and MINERVA to refer col-
   vectors; tensors; holographic reduced representations; HRRs.        lectively to MINERVA 2 and any model based on it.
                                                                          Our central contribution is to prove that MINERVA is
                          Introduction                                 mathematically equivalent to an auto-associative fourth or-
                                                                       der tensor memory, or memory tesseract. We further demon-
Most memory phenomena can be explained by unified,                     strate that MINERVA is approximately equivalent to a holo-
computational memory models (e.g., Franklin & Mewhort,                 graphic lateral inhibition network (Levy & Gayler, 2009).
2013; Hintzman, 1984; Humphreys, Bain, & Pike, 1989;                   These demonstrations have three implications for memory
Jamieson & Mewhort, 2011). Simulation has led to parsi-                modelling: (1) the relationship between MINERVA and
monious theories of memory, but at a cost of a profusion of            other memory models is clarified, suggesting that MIN-
competing models. As different models focus on different               ERVA may be suitable as a basis for all memory modelling,
phenomena, there is no best model.                                     (2) MINERVA is scalable to arbitrarily long-term learning,
   Simulation models share many characteristics indicating             and (3) MINERVA is neurally plausible.
wide agreement about the mathematics of how memory
works. Here, we argue that memory models, including the                              How does MINERVA work?
MINERVA 2 (Hintzman, 1984) model and variants (e.g.,
Jamieson, Crump, & Hannah, 2012; Jamieson & Mewhort,                   Hintzman (1986) describes MINERVA’s key assumptions:
2011; Kwantes, 2005; Thomas et al., 2008), as well as holo-            (1) only episode traces are stored in memory,
graphic models of short/long term memory (e.g., Eich,                  (2) repetition produces multiple traces of an item,
1982; Franklin & Mewhort, 2013; Murdock, 1993), and the                (3) a retrieval cue contacts all traces simultaneously,
DSHM model of declarative memory (Rutledge-Taylor &                    (4) each trace is activated according to similarity to the cue,
West, 2008) which uses the BEAGLE learning algorithm                   (5) all traces respond in parallel, the retrieved information
(Jones & Mewhort, 2007), can be understood in terms of a                    reflecting their summed output.
single neurally plausible memory framework.                            Each individual experience, or episode, is represented by a
   This effort at unification is based on the MINERVA 2                high dimensional vector. Memory is a table, where each row
model (Hintzman, 1984), a computational model of biologi-              is a vector representing an ‘episode trace’ corresponding to a
cal memory, intended by Hintzman to describe long-term                 stored experience. Each new experience is stored as a new
memory (both episodic and semantic). It has been applied               row in the memory table. New experiences do not need to
to several experimental paradigms, including judgement of              be novel. A repeated experience is also stored as a new row,
frequency tasks (Hintzman 1984), recognition tasks (Hintz-             separate from previous instances of that experience.
man, 1984), “schema-abstraction” or category learning                     In MINERVA, memory retrieval is not a look-up process,
(Hintzman, 1984; 1986), implicit learning tasks such as arti-          it is a reconstruction process. In the words of Tulving and
ficial grammar learning (Jamieson & Mewhort, 2011), as                 Watkins (1973), “a probe combines or interacts with the
well as speech perception (Goldinger, 1998), and naming                stored information to create the memory of a previously
words from print (Kwantes & Mewhort, 1999).                            experienced event”. When a retrieval cue is presented, each
                                                                   2483

vector in the table “resonates” with the cue in proportion to      to store different memories, whereas vector (e.g., Eich,
its similarity to the cue. Similarity is computed as a cosine      1982; Franklin & Mewhort, 2013; Murdock, 1993), matrix
(i.e., normalized dot-product) of the cue with the stored vec-     (e.g., Humphreys, Pike, Bain, & Tehan, 1989; Howard &
tor. Each vector is activated by its cubed similarity to the       Kahana, 2002), and tensor memory models (e.g., Hum-
cue. Information is retrieved from memory in the form of a         phreys, Bain, & Pike, 1989; Smolensky, 1990) use distrib-
new vector, called an echo. The echo is a weighted sum of          uted stores. Across these four categories, there are strong
the vectors in the table, each vector weighted by its activa-      similarities between models, as we will now discuss.
tion. By computing activation as the cube of similarity, the
contribution of the most similar vectors (or experiences) is       Matrix and tensor models
emphasized and that of the least similar (and least relevant)      Humphreys et al. (1989) note that their matrix memory,
is minimized. The echo is used by the model to respond to          MINERVA 2, and TODAM (Murdock, 1993) retrieve in-
the cue as appropriate for the given task.                         formation as a sum of all traces in memory, each trace
   Abstract, conceptual, and categorical information reflect       weighted by its similarity to the cue. As we will show, these
aggregate retrieval over many episode traces. The blending         models are not different in kind, and so we elect to use the
of experiences in the echo is one source of our ability to use     term echo, normally reserved for MINERVA, to refer to the
abstractions (e.g., Goldinger, 1998).                              retrieved vector in all of these memory models.
   According to Hintzman (1990), MINERVA 2 can be un-                 A key point of comparison is how vector-based models
derstood as an artificial neural network. A layer of input         represent associations between items. Smolensky (1990)
nodes represent the cue. A layer of output nodes represent         notes that the tensor product, a generalization of the matrix
the echo. Between the two is a hidden layer of nodes. In the       product, can be used to form associations between an arbi-
hidden layer, each node corresponds to an episode trace. It        trary number of items, though at the cost of producing pro-
follows that MINERVA’s hidden layer is a localist network:         gressively larger and more unwieldy tensors.
specific nodes represent specific pieces of information.              The order of the tensor used to store memories indicates
   Modellers using MINERVA are generally agnostic as to            the number of vectors the memory model associates to-
how the model is related to the brain. No one claims that for      gether. An association between a pair of items is represented
each new experience one grows a new neuron that is forever         as the tensor product of the items’ vectors, which is a second
singly dedicated to that particular experience. But no other       order tensor, or matrix. A matrix memory is the sum of those
interpretation of how MINERVA is related to the brain has          associations. Howard and Kahana’s (2002) matrix memory
been previously proposed, leaving open the question of             is context x item, that is, it associates a vector representing
MINERVA’s neural and, hence, theoretical plausibility.             an item with a vector representing a context. Humphreys et
                                                                   al. (1989) matrix memory is item a x item b, that is, it asso-
          A comparison of memory models                            ciates two different items together.
The memory models discussed here use vector and tensor                If three items are being associated, the result is a “3D
representations to simulate the processes of storage and re-       matrix”, or third order tensor. Humphreys, Bain, and Pike’s
trieval. Tensors are a generalization of matrices. A vector is     (1989) third order tensor memory is context x item a x item
a first order tensor. A matrix is second order tensor. A third     b, that is, it associates two different items together with a
order tensor is a “3D matrix” or a stack of matrices.              representation of context.
   Memory models can use either localist or distributed rep-          There is a problem here. Across these models, the archi-
resentations and have either localist or distributed memory        tecture of memory is being modified to suit the particulars
stores. Vector-based memory models use distributed repre-          of the tasks being modelled. If we just need one cue (be it an
sentations, that is to say, an item to be remembered is repre-     item or context), we use a matrix memory. If we use two
sented as a high dimensional vector, which can be thought          cues (be it an item and context, or two items) we use a third
of as a pattern of activation across nodes in a network. Con-      order tensor. But what if we need to use three cues? Do we
versely, in a network that uses localist representation, an        then need to use a fourth order tensor? What about four
item is represented by the activation of a single node, as         cues? Using this approach, not only does the architecture of
opposed to a pattern of activation across a group of nodes.        memory need to be changed depending on the particulars of
   In vector-based memory models, the memory store can be          the task, but the architecture becomes increasingly unwieldy
either localist or distributed. By a localist store, we mean       as the task becomes more complex.
that a model stores different data in different places. By a
distributed store, we mean a model that stores all data in a       Vector models
single place or all data in all places. For our purposes,          Holographic vectors can represent arbitrarily complex asso-
vector-based memory models can be divided into four cate-          ciations of items and context, making it unnecessary to use
gories: vector memory, where all memories are stored in a          matrices or higher order tensors to represent association,
single vector, matrix memory, where all memories are stored        allowing modellers to adopt a memory architecture that is
in a single matrix, tensor memory, where all memories are          invariant with respect to the complexity of the associations.
stored in a single, higher-order tensor (e.g., a “3D” or “4D”         In a vector memory, an association between a set of items
matrix), and multi-vector memory, where multiple vectors           is the convolution of the vectors representing those items.
are used to store memories. Multi-vector memory models,            Convolution is a noisy compression of the tensor-product
such as MINERVA, and DSHM (Rutledge-Taylor & West,                 (Plate, 1995; for discussion see Kelly, Blostein, & Mewhort,
2008) use localist stores because different vectors are used       2013), thus vector models differ from matrix and tensor
                                                               2484

models only in that the highly lossy nature of holographic            noisy and requires a secondary (or long-term) memory in
vector storage adds noise to the echo, and that individual            order to identify which memory the echo most resembles
items and associations between sets of any size can all be            (Murdock, 1993). This is acceptable for TODAM, as it is
stored together as a sum in a single vector memory.                   intended as a model of primary (or working) memory.
   Holographic vectors do have one weakness: they are                 MINERVA, however, is a model of long-term memory and
highly lossy. This is the only reason one might prefer the            thus it would “violate the spirit of MINERVA 2” (Hintzman,
aforementioned matrix or tensor memories over a holo-                 1986) to rely on an external memory to clean-up the echo.
graphic one. However, combining holographic vectors with                 A disadvantage of MINERVA is that it is a localist mem-
MINERVA creates a system that can store arbitrarily com-              ory store, which raises the question of neural plausibility, as
plex associations between items and contexts, and retrieve            well as scalability. How is MINERVA neurally imple-
them with fidelity (Jamieson & Mewhort, 2011).                        mented, if not by growing new neurons for each new expe-
                                                                      rience? Is MINERVA practical for very long-term learning
MINERVA                                                               given that each experience adds another row to the memory
MINERVA 2 differs from vector or matrix models in that:               table? While MINERVA has been applied to large problems
(1) MINERVA 2 associates items by either adding together              (Kwantes, 2005; Kwantes & Mewhort, 1999), doing so re-
    or concatenating the vectors representing those items,            quires abandoning key assumptions of the model for the
    rather than using the tensor-product or convolution.              sake of computational feasibility.
(2) All traces in the echo of MINERVA 2 are weighted by                  Given the usefulness of using the cube of the similarity in
    the cubed similarity to the cue. In a vector or matrix            MINERVA 2, we want to keep this feature in a variant of the
    model, the similarity is not raised to an exponent.               model that uses a distributed memory store.
The first difference is not essential. The Holographic Exem-
plar Model (Jamieson & Mewhort, 2011) is a MINERVA                             MINERVA as a fourth order tensor
that uses convolution rather than concatenation, gaining the          In what follows, we prove equivalence between the MIN-
ability to represent arbitrarily complex associations.                ERVA 2 model and an auto-associative fourth order tensor
   The second difference is critical. Raising similarity to an        memory. To do so, we first prove that a variant of MIN-
exponent of 3 sets the MINERVA models apart from the                  ERVA that raises similarity to an exponent of 1 is equivalent
vector and matrix models. The purpose of this exponent is to          to an auto-associative second order tensor (i.e., a matrix)
make MINERVA nonlinear, as Hintzman explains (1990):                  memory. Then we prove that a MINERVA that uses an ex-
                                                                      ponent of 2 is equivalent to a third order tensor. Finally, we
   This model escapes being just a less efficient version of          prove that the MINERVA 2 model, which uses an exponent
   the vector model by using nonlinearity. In particular, the         of 3, is equivalent to a fourth order tensor.
   activation of each hidden unit is a positively accelerated            Consider a variant on the MINERVA model that uses dot
   function of its match to the input vector, limiting the            product (denoted by •) to measure similarity and weights
   number of units that will respond significantly to any in-         each trace by its similarity raised to the exponent of 1. Each
   put, and thereby reducing noise.                                   episode trace in memory is represented by a vector vi where
                                                                      i = 1 ... m and m is the number of traces in memory. When
By weighting each episode trace by the cube of its similar-           the model is presented with a cue x, the echo y is:
ity, the traces that are most similar to the cue contribute              y = (x•v1)v1 + ... + (x•vm)vm
much more to the echo than traces that have only partial              This is equivalent to an auto-associative matrix memory.
similarity to the cue, or traces that have tiny, incidental simi-        In an auto-associative matrix memory, each episode trace
larity to the cue. Cubing similarity weights retrieval in fa-         is represented by a vector vi. To store a trace in memory, the
vour of an exact match to a single item in memory over par-           trace is associated with itself (hence auto-associative) by
tial matches to several items in memory or slight matches             taking the outer-product of the vector with itself, vi viT, then
with a very large number of items in memory.                          taking the sum of all the outer-product matrices to create the
   This characteristic allows MINERVA to “clean-up” the               memory matrix, M:
echo using iterative retrieval. MINERVA can “clean up” an                M = v1 v1T + ... + vm vmT
echo by using the echo as a cue to produce a new echo. With           The echo, y, is the inner-product of the cue and the matrix:
each pass through the memory system, the contribution of                 y = Mx
the most similar episode traces grows. This process can be               y = (v1 v1T + ... + vm vmT ) x
repeated until the echo reaches a steady-state where it no               y = v1 v1T x + ... + vm vmT x
longer changes, at which point the echo will closely resem-           Because viT x is the dot-product of vi and x:
ble the trace in memory most similar to the cue.                         y = (x•v1)v1 + ... + (x•vm)vm
   The clean-up process also serves as a possible explanation         which is identical to MINERVA with an exponent of 1 or to
for why we are faster to remember some things than others:            a matrix memory (e.g., Humphreys et al., 1989). Consider a
echoes formed from frequently occurring and distinctive               MINERVA that raises similarity to the exponent of 2:
episode traces reach a steady-state more quickly.                        y = (x•v1)2 v1 + ... + (x•vm)2 vm
   Linear systems cannot perform a clean-up process and so            This variant of MINERVA, as we shall demonstrate, is
require an external clean-up memory. The echo produced by             mathematically equivalent to an auto-associative third order
a holographic vector memory (e.g., TODAM; Murdock,                    tensor memory. Using the tensor product, denoted by ⊗, we
1993) or matrix memory (e.g., Humphreys et al., 1989) is
                                                                  2485

can store each trace as vi ⊗ vi ⊗ vi , which is a third order                 Is the memory tesseract practical?
tensor. The memory tensor M is the sum of the third order
                                                                    MINERVA is equivalent to a distributed memory system
tensor outer-products of each episode trace:
                                                                    implemented as an auto-associative fourth order tensor, or
   M = v1 ⊗ v1 ⊗ v1 + ... + vm ⊗ vm ⊗ vm
                                                                    memory tesseract. Unfortunately, fourth order tensors are
The echo, y, can be computed from the cue, x, by taking the
                                                                    very large. For most applications of MINERVA, the dimen-
inner product twice:
                                                                    sionality n of a vector will be larger than the number of
   y = (M x) x
                                                                    memories m stored in the model. MINERVA, as standardly
If each vi is a vector of n dimensions, then M is an n x n x n
                                                                    implemented, is an m x n table, whereas a memory tesseract
tensor. M can be thought of as n matrices of n x n dimen-
                                                                    is an n4 data structure. A typical MINERVA 2 has 10 ≤ n ≤
sions. When we compute the inner-product of M with the
                                                                    200. In general, the number of memories stored is smaller
cue x, we compute the inner product of x with each of those
                                                                    than n and much smaller than n3. For applications where m <
n matrices. This results in n vectors that can be rearranged
                                                                    n3, the implementation of MINERVA as a table is more effi-
into a new n x n matrix. The second inner product with x
                                                                    cient. However, for large scale applications where m ≥ n3,
then produces a vector, the echo y.
                                                                    the fourth order tensor is more efficient.
   To illustrate, let us break M into its components:
                                                                       Alternatively, a holographic approximation to the memory
   y = (M x) x
                                                                    tesseract can be implemented as an n x p data structure for
   y = ((v1 ⊗ v1 ⊗ v1 + ... + vm ⊗ vm ⊗ vm) x) x
                                                                    the p of your choice, as is discussed in the next section.
   y = (v1 ⊗ v1 ⊗ v1 x + ... + vm ⊗ vm ⊗ vm x ) x
The tensor product vi ⊗ vi ⊗ vi can be understood as n ma-           Using holographic vectors rather than tensors
trices, where each matrix is the outer-product vi viT weighted
by a different element j of vi , for all j = 1 ... n.               In a holographic vector system, trying to clean-up the echo
    vi ⊗ vi ⊗ vi = {vi 1 vi viT , ... , vi n vi viT}                by iteratively using the echo as a cue to retrieve a new echo
Taking the inner-product of the cue x with vi ⊗ vi ⊗ vi, we         is like trying to clean a pair of glasses with an oily cloth: the
get n vectors, each weighted by the dot-product of x with vi:       more you try to clean it, the worse it becomes. Yet Levy and
    vi ⊗ vi ⊗ vi x = {vi 1 vi viT x, ... , vi n vi viT x}           Gayler (2009) have demonstrated that this is possible using
    vi ⊗ vi ⊗ vi x = {vi 1 (x • vi) vi, ... , vi n (x • vi) vi}     a lateral inhibition network implemented as a fully distrib-
                                                                    uted vector architecture. To store a trace vi in Levy and Gay-
If we factor out the dot-product of x and vi , the result is n
                                                                    ler’s model, the trace is associated with itself twice, then
vectors, or rather, the outer-product matrix of vi viT:
                                                                    each trace is added to the memory vector m:
    vi ⊗ vi ⊗ vi x = (x • vi) {vi 1 vi, ... , vi n vi}
                                                                       m = v1 * v1 * v1 + ... + vm * vm * vm
    vi ⊗ vi ⊗ vi x = (x • vi) vi viT                                where * is a binding operation. In holographic reduced rep-
Thus, when we take the outer-product of x with M, the re-           resentations (Plate, 1995), binding uses circular convolution
sult is a sum of m matrices vi viT, each matrix weighted by         and unbinding uses circular correlation. Given a cue, x, we
the dot-product of x with vi.                                       can unbind, denoted by #, to recover an echo:
   y = (v1 ⊗ v1 ⊗ v1 x + ... + vm ⊗ vm ⊗ vm x ) x                      y = x # (x # m)
   y = ((x • v1) v1 v1T + ... + (x • vm) vm vmT ) x                    y = x # (x # (v1 * v1 * v1 + ... + vm * vm * vm ))
By then taking the second inner-product with x, we reduce           Unbinding is such that given a bound pair, vi * vj,
each matrix to a vector weighted by the squared similarity to          x # vi * vj = (x•vi) vj + noise
x, producing an echo like MINERVA with an exponent of 2:            Thus:
   y = (x • v1) v1 v1T x + ... + (x • vm) vm vmT x                     y = x # (x # (v1 * v1 * v1 + ... + vm * vm * vm ))
   y = (x • v1) (x • v1) v1 + ... + (x • vm) (x • vm) vm               y = x # ((x•v1)v1*v1 + ... + (x•vm)vm*vm + noise)
   y = (x • v1)2 v1 + ... + (x • vm)2 vm                               y = (x•v1)2 v1 + ... + (x•vm)2 vm + noise
The dot product of two vectors, where each vector has a             However, if we wish to imitate MINERVA 2 as closely as
magnitude of one, produces a value in the range of one, if          possible (and preserve the sign of the similarity) we need to
the vectors are identical, to zero, if the vectors are orthogo-     add another association to Levy and Gayler’s model. We
nal, to negative one, if one vector is the negation of the          compute the memory vector, m, and unbind the echo, y, as:
other. Thus it is important to preserve the sign of the dot            m = v1 * v1 * v1 * v1 + ... + vm * vm * vm * vm
product. By taking the square of the dot product, the sign is          y = x # (x # (x # m))
lost. For this reason, MINERVA 2 uses an exponent of 3.                y = x # (x # (x # (v1*v1*v1*v1 + ... + vm*vm*vm*vm )))
   MINERVA 2 is equivalent to an auto-associative memory               y = (x•v1)3 v1 + ... + (x•vm)3 vm + noise
implemented as a fourth order tensor. Memory is con-                While this allows the most similar traces to the cue to domi-
structed as a sum of fourth order tensors:                          nate the echo, the noise term threatens to overwhelm the
   M = v1 ⊗ v1 ⊗ v1 ⊗ v1 + ... + vm ⊗ vm ⊗ vm ⊗ vm                  signal. Because holographic vectors use lossy compression,
Given a cue x, an echo y is computed by taking the inner            by iterating, the noise will only grow.
product three times:                                                   Levy and Gayler solve this problem by using random
   y = ((M x) x) x                                                  permutations. Given three random permutation matrices P1,
   y = (((v1 ⊗ v1 ⊗ v1 ⊗ v1 + ... + vm ⊗ vm ⊗ vm ⊗ vm) x)x)x        P2, P3, we can permute the vectors as follows:
   y = (((x • v1) v1 ⊗ v1 ⊗ v1 + ... + (x • v1) vm ⊗ vm ⊗vm)x)x        m = (P1 v1) * (P2 v1) * (P3 v1) * v1 + ... + (P1 vm) * (P2 vm) *
   y = (((x • v1)2 v1 v1T + ... + (x • vm)2 vm vmT ) x                       (P3 vm) * vm
   y = (x • v1)3 v1 + ... + (x • vm)3 vm                            To recover an echo, we can use inverse permutations:
                                                                2486

         y = ((P1T x) * (P2T x) * (P3T x)) # m                              BEAGLE (Jones & Mewhort, 2007) is a learning algo-
To eliminate the noise, Levy and Gayler use hundreds of                  rithm that models how people abstract the meaning of words
such vector memories in parallel, each of which uses its own             from their lifetime language experience. DHSM (Rutledge-
pair, or in our variant, triple, of permutations. Let p be the           Taylor & West, 2008) uses a similar approach to BEAGLE
number of vector memories being used, then the echo y is                 but re-purposes the algorithm as a general memory model.
the sum of echoes for each p:                                            DSHM is a multi-vector memory system, like the standard
   y = y1 + ... + yp                                                     implementation of MINERVA, but unlike MINERVA, each
where for each yj, j = 1 ... p,                                          vector stands for a concept rather than an individual experi-
   yj = ((Pj,1T x) * (Pj,2T x) * (Pj,3T x)) # mj                         ence. In DSHM, each experience updates the vector for each
such that:                                                               concept that comprises the experience. An experience is
   y = ((P1,1T x) * (P1,2T x) * (P1,3T x)) # m1 + ... + ((Pp,1T x) *     stored in the associations between concept vectors.
        (Pp,2T x) * (Pp,3T x)) # mp                                         In MINERVA, concepts are echoes, artifacts of aggregate
Because each echo yj is produced using a different triple of             retrieval across experiences. We speculate that the vectors in
permutations, each echo’s noise term will be different. Be-              BEAGLE and DSHM are akin to echoes. The memory tes-
cause the noise in each echo is different, a different part of           seract now provides a means of re-implementing BEAGLE
the signal is preserved in each echo. By taking the sum of all           or DSHM as MINERVA models.
these echoes, we average across them to get a close ap-                     In BEAGLE, an experience is a sentence. To re-
proximation to the true signal. With large enough p, we                  implement BEAGLE in the memory tesseract, each time a
minimize the noise sufficiently that we can clean-up the                 word is encountered in a sentence, memory is updated with
echo by iterating. We can divide the sum of all the echoes,              a new episode trace by summing a new fourth order tensor
y, by its magnitude to normalize, then use it as the new cue.            with the memory tensor:
By varying the dimensionality of the vectors n and the num-                 Mt = Mt-1 + vi ⊗ vi ⊗ vi ⊗ vi
ber of memory vectors operating in parallel p, we can ma-                Each episode trace is the sum of a word’s orthographic in-
nipulate how fast the echo is cleaned up by iterating.                   formation as calculated in Cox et al. (2011) with the word’s
   The number of iterations to clean-up the echo is a meas-              semantic information gleaned from that particular sentence,
ure of the time the model takes to perform a memory re-                  as calculated in Jones and Mewhort (2007):
trieval. For this reason, the number of iterations is an ideal              vi = vi orthographic + vi semantic
candidate as a predictor for human reaction time. Being able             However, holographic vectors typically have n ≥ 512. 512 4
to map a measure of the model’s processing time onto hu-                 is about 1678 times larger than the BEAGLE model. If we
man reaction time would help to provide evidence for strong              assume that BEAGLE and MINERVA use vectors of the
equivalence (Pylyshyn, 1989) between what the model is                   same size, given a literate adult with a vocabulary of 80 000
doing and what the brain is doing.                                       words, the memory tesseract will be larger than BEAGLE
   The number of iterations to clean-up the echo in MIN-                 for vectors with more than 43 dimensions.
ERVA 2 is typically too few to provide sufficient granularity               If we were to run a memory tesseract model that learns
to map well onto human reaction times. If we instead use                 word meaning from context by processing a large corpus, as
Levy and Gayler’s model, by varying p we can control the                 BEAGLE does, we would need to keep n as small as possi-
average number of iterations that the system takes to clean-             ble without sacrificing too much fidelity. We could also use
up the echo. Using smaller p, the network takes, on average,             the holographic approximation to the memory tesseract,
longer to converge to a steady-state than an iterated MIN-               which is, again, scalable depending on the desired fidelity.
ERVA 2 model, such that the iterations map onto smaller                     The proposed model escapes being just a less efficient
units of human reaction time. Thus, the lateral inhibition               version of BEAGLE because it raises similarity to the expo-
network, when coupled with a suitable decision mechanism                 nent of 3. BEAGLE stores all experiences of a particular
to end retrieval, can provide more fine-grained reaction time            word in a single vector. This storage is highly lossy and the
predictions than an iterated MINERVA 2.                                  individual experiences are difficult to recover. MINERVA
   The lateral inhibition network is also more tractable than            retains more of each experience, information that can be
a fourth order tensor for large n (i.e., more space efficient            recovered by iterating to “clean up” the echo.
for p < n3 and more time efficient for p log n < n3).                       A memory tesseract implementation of BEAGLE would
                                                                         have another benefit. Individual vectors in BEAGLE are
 The semantic tesseract: Scaling MINERVA up                              sensitive to first-order (word co-occurrence) and second
                                                                         order (synonymy) associations. Kwantes (2005) found that
Applying MINERVA to large semantic memory tasks, such
                                                                         aggregating across vectors with first-order associations pro-
as learning the meaning of words (Kwantes, 2005) or learn-
                                                                         duces an echo with second order associations. Likewise, we
ing how to sound-out written words (Kwantes & Mewhort,
                                                                         suspect that aggregating across BEAGLE’s vectors would
1999), requires abandoning a key assumption of MINERVA
                                                                         produce an echo with third order associations. Third-order
for purely pragmatic reasons, namely, that repetition of an
                                                                         associations (and higher) may be useful for identifying part
item produces multiple traces of that item. But if these mod-
                                                                         of speech. This change to BEAGLE is a benefit of adopting
els were re-implemented using the memory tesseract (or its
                                                                         the MINERVA memory retrieval mechanism.
holographic approximation), we would not have to abandon
                                                                            The preceding discussion is intended only as an extended
that key assumption because the tesseract does not grow
                                                                         example of how MINERVA, when implemented as a tensor,
with the addition of new memories.
                                                                         can be applied to larger scale problems.
                                                                     2487

                        Conclusion                                Humphreys, M. S., Bain, J. D., & Pike, R. (1989). Different
                                                                    ways to cue a coherent memory system: A theory for epi-
We demonstrate that the influential MINERVA 2 (Hintzman,            sodic, semantic, and procedural tasks. Psychological Re-
1984) model is mathematically equivalent to an auto-                view, 96, 208-233.
associative fourth order tensor memory system, or memory          Humphreys, M. S., Pike, R., Bain, J. D., & Tehan, G.
tesseract. We further show that this is approximately               (1989). Global matching: A comparison of the SAM,
equivalent to a variant of the holographic lateral inhibition       Minerva II, Matrix, and TODAM models. Journal of
network proposed by Levy and Gayler (2009). These dem-              Mathematical Psychology, 33, 36-67.
onstrations have three theoretical implications:                  Jamieson, R. K., Crump, M. J. C., & Hannah, S. D. (2012).
(1) Viewing MINERVA 2 and its variants (collectively,               An instance theory of associative learning. Learning &
    MINERVA models) as a fourth order tensor clarifies the          Behavior, 40, 61-82.
    relationship between MINERVA and third order, second          Jamieson, R. K., & Mewhort, D. J. K. (2011). Grammatical-
    order (i.e., matrix), and compressed tensor (i.e., holo-        ity is inferred from global similarity: A reply to Kinder
    graphic vector) memory models, allowing us to move              (2010). The Quarterly Journal of Experimental Psychol-
    toward a unified understanding of memory.                       ogy, 64, 209-216.
(2) MINERVA can be scaled up to model long term learn-            Jones, M. N., & Mewhort, D. J. K. (2007). Representing
    ing, broadening the scope of tasks to which MINERVA             word meaning and order information in a composite holo-
    can be applied and allowing for unification with models         graphic lexicon. Psychological Review, 114, 1-37.
    of semantic learning, such as BEAGLE.                         Kelly, M. A., Blostein, D., & Mewhort, D. J. K. (2013). En-
(3) A naïve neural interpretation of MINERVA might sug-             coding structure in holographic reduced representations.
    gest that a new neuron is grown for each new experi-            Canadian Journal of Experimental Psychology, 67, 79-93.
    ence, corresponding to the addition of another row to         Kwantes, P. J. (2005). Using context to build semantics.
    MINERVA’s memory table. Understood as a memory                  Psychonomic Bulletin & Review, 12, 703-710.
    tesseract, MINERVA is fully distributed across neural         Kwantes, P. J., & Mewhort, D. J. K. (1999). Modeling lexi-
    connectivity, and memories can be added by changing             cal decision and word naming as a retrieval process. Ca-
    the connectivity without requiring additional neural re-        nadian Journal of Experimental Psychology, 53, 306-315.
    sources. Eliasmith’s (2013) neural engineering frame-         Levy, S. D., & Gayler, R. W. (2009). “Lateral inhibition” in
    work provides a system for translating linear algebra           a fully distributed connectionist architecture. In Howes,
    computations (e.g., convolution, permutation) into spik-        A., Peebles, D. & Cooper, R. (Eds.), 9th International
    ing neuron models. The memory tesseract, or its holo-           Conference on Cognitive Modeling (pp.318-323). Man-
    graphic approximation, could be easily implemented as a         chester, UK.
    realistic neural model using Eliasmith’s framework.           Murdock, B. B. (1993). TODAM2: a model for the storage
                                                                    and retrieval of item, associative and serial-order informa-
                         References                                 tion. Psychological Review, 100, 183–203.
Cox, G. E., Kachergis, G., Recchia, G., & Jones, M. N.            Rutledge-Taylor, M. F., & West R. L. (2008). Modeling the
  (2011). Towards a Scalable Holographic Representation             fan-effect using dynamically structured holographic
  of Word Form. Behavior Research Methods, 43, 602-615.             memory. In B. C. Love, K. McRae, & V. M. Sloutsky
Eich, J. M. (1982). A composite holographic associative             (Eds.), 30th Annual Conference of the Cognitive Science
  recall model. Psychological Review, 89, 627–661.                  Society (pp. 385-390). Washington, DC.
Eliasmith, C. (2013). How to build a brain: A neural archi-       Plate, T. A. (1995). Holographic reduced representations.
  tecture for biological cognition. Oxford University Press.        IEEE Transactions on Neural Networks, 6, 623–641.
Franklin, D. R. J., & Mewhort, D. J. K. (2013). Control           Pylyshyn, Z. (1989). Computing in Cognitive Science. In
  Processes in Free Recall. In R. West & T. Stewart (eds.),         Posner, M. (Ed.) Foundations of Cognitive Science. Cam-
  12th International Conference on Cognitive Modeling               bridge: MIT Press.
  (pp. 47-52), Ottawa, CA.                                        Smolensky, P. (1990). Tensor product variable binding and
Goldinger, S. D. (1998). Echoes of echoes? An episodic              the representation of symbolic structures in connectionist
  theory of lexical access. Psychological Review, 105, 251-         systems. Artificial Intelligence, 46, 159-216.
  279.                                                            Thomas, R. P., Dougherty, M. R., Sprenger, A. M., & Harbi-
Hintzman, D. L. (1984). MINERVA 2: A simulation model               son, J. I. (2008). Diagnostic hypothesis generation and
  of human memory. Behavior Research Methods, Instru-               human judgment. Psychological Review, 115, 155-185.
  ments, and Computers, 16, 96-101.                               Tulving, E., & Watkins, M.J. (1973). Continuity between
Hintzman, D. L. (1986). “Schema abstraction” in multiple-           recall and recognition. American Journal of Psychology,
  trace memory models. Psychological Review, 93, 441-               86, 739-748.
  428.
Hintzman, D. L. (1990). Human learning and memory:
  Connections and dissociations. Annual Review of Psy-
  chology, 41, 109-139.
Howard, M. W., & Kahana, M. J. (2002). A distributed rep-
  resentation of temporal context. Journal of Mathematical
  Psychology, 46, 269-299.
                                                              2488

