UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
The value of approaching bad things
Permalink
https://escholarship.org/uc/item/4qh2v3tt
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Rich, Alexander
Gureckis, Todd
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                         The value of approaching bad things
                                                Alexander S. Rich (asr443@nyu.edu)
                                            Todd M. Gureckis (todd.gureckis@nyu.edu)
                 New York University, Department of Psychology, 6 Washington Place, New York, NY 10003 USA
                               Abstract                                 of mostly-good lectures. The critical point is that the poten-
                                                                        tial future payoff of attending that second lecture if you were
   Adaptive decision making often entails learning to approach          wrong about the series being boring is far greater than the
   things that lead to positive outcomes while avoiding things that
   are negative. The decision to avoid something removes the risk       potential loss from attending a few bad lectures if you were
   of a negative experience but also forgoes the opportunity to ob-     right.
   tain information, specifically that a seemingly negative option         This intuitive example highlights a key aspect of approach-
   is actually positive. This paper explores how people learn to
   approach or avoid objects with uncertain payoffs. We provide         avoid decisions. First, optimal decision making should take
   a computational-level analysis of optimal decision making in         into account not just the estimated valence (good or bad) of
   this problem which quantifies how the probability of encoun-         an option but also the uncertainty about that estimate (Berry
   tering an object in the future should impact the decision to ap-
   proach or avoid it. A large experiment conducted online shows        & Fristedt, 1985; Denrell, 2007; Daw et al., 2006). Second, it
   that most people intuitively take into account both their uncer-     can be optimal in some cases to approach seemingly negative
   tainty and the value of information when deciding to approach        outcomes due to the potential gain that could be experienced
   seemingly bad things.
   Keywords: decision making, approach-or-avoid behavior,               in the future. This is due to the value of information which is
   value of information, sequential decision making                     obtainable from continued approach decisions.
                                                                           The goal of the present paper is to explore if and how peo-
   From talking to a stranger to trying a new restaurant, peo-          ple intuitively utilize these aspects of approach/avoidance de-
ple often enter into situations with uncertain outcomes. Ap-            cision making. While there is a large literature on the effect
proaching, sampling, or interacting with unknown options al-            of experience dependent sampling on approach/avoid behav-
lows highly rewarding situations to be discovered, but also             ior (e.g., Denrell & March, 2001; Niv et al., 2002; Biele et
carries the risk of encountering negative outcomes (e.g., an            al., 2009; Fazio et al., 2004), there have been fewer empirical
awkward conversation or bad meal). As a result, there is often          tests of the idea that decision makers consider both uncer-
a tension between approaching new alternatives to discover              tainty and the value of information when making such deci-
which are positive and avoiding those that could be negative.           sions (but see Meyer & Shi, 1995).
   Certain forms of adaptive approach-or-avoid behavior can
lead to systematically suboptimal behavior (March, 1996;                A computational analysis of approach-avoidance
Denrell, 2007). For example, suppose there is a lecture series          decision making
that features an engaging speaker on most days but which also
has the occasional boring talk. You attend the series for the           The scenario we consider here concerns a decision maker
first time at the start of the semester, happen to hear a boring        who is presented with a single prospect (which might repre-
talk, and decide not to attend in the future. As a result, you          sent an object, a person, a product, or a situation) at each point
gain no information about how good or bad the subsequent                in time and must decide to either approach or avoid it. Ap-
talks actually are.                                                     proach decisions result in experience with the prospect which
                                                                        may be either negative or positive. Avoid decisions result in
   The tendency to avoid alternatives and cease learning about
                                                                        no experience. Such a task corresponds to many real-world
them after an early negative experience is known as the “hot
                                                                        decision problems people face in their lives. For scientists this
stove effect” and has been used to explain suboptimal behav-
                                                                        includes which seminar series to attend and which to skip.
ior and risk-aversion in both individuals and organizations
(Denrell & March, 2001; Denrell, 2005). While adaptive (in              Optimal approach-avoidance decision making Consider
the sense of changing based on experience), this strategy is            the case in which a single prospect with Bernoulli payoffs
suboptimal because it doesn’t take into account the value of            may be approached or avoided an indefinite number of times.
the information gained by approaching or interacting with an            When it is approached, it has unknown probability p of yield-
uncertain alternative. After a single boring talk, your uncer-          ing a reward of 1, and probability 1 − p of yielding a reward
tainty about the seminar series might remain high. If you go            of −1. Given a subjective belief about the value of p de-
to a second lecture and it is also boring, your uncertainty in          scribed by a Beta distribution, Beta(α, β), the agent’s predic-
                                                                                                                                      α
your belief should drop and you likely will not attend a third.         tion about the probability of reward is E[Beta(α, β)] = α+β      .
   But suppose that the second lecture is really good. This             The posterior belief about p is updated to Beta(α + 1, β) af-
new information should help revise your belief about the                ter an additional positive outcome, and to Beta(α, β + 1) af-
quality of the series, and increase the chances you will go to          ter an additional negative outcome. A natural initial setting
a third lecture. If the third is also good, you will likely go to       is α = β = 1 which corresponds a uniform subjective prior
a fourth, and so on until you have enjoyed a whole semester             distribution for p over the range [0,1]. Figure 1 (left panel)
                                                                    1281

                                                # of Negative Outcomes                                            Deterministic Optimal Model                                                                  # positive =
                                                                                                                       # of Negative Outcomes                                                                  # negative
                                 0                  1                2              3              4      ...                       0     2   4   6   8   10       0     2    4   6   8   10       0   2   4   6   8    10
                                                                                                                # of Positive
                                                                                                                            0                                  0                               0
                         0                                                                                                  1                                  1                               1                              proa
# of Positive Outcomes
                                                                                                                            2                                  2                               2
                                                                                                                 Outcomes
                             0   0.5   1.   0       0.5     1.   0   0.5   1.   0   0.5   1.   0   0.5   1.
                                                                                                                                                                                                                                  ch
                                                                                                                            3                                  3                               3
                         1
                                                                                                                                                                                                                              ap
                                                                                                                                        Low Discount Rate              High Discount Rate                Difference
                             0   0.5   1.   0       0.5     1.   0   0.5   1.   0   0.5   1.   0   0.5   1.
                                                                                                                                            (myopic)                      (farsighted)             (farsighted - myopic)
                         2
                             0   0.5   1.   0       0.5     1.   0   0.5   1.   0   0.5   1.   0   0.5   1.       Stochastic Optimal Model                                                                                     d
                                                                                                                                    0     2   4   6   8   10       0      2   4   6   8   10       0   2   4   6    8   10
                                                                                                                                                                                                                              oi
                         3                                                                                                      0                              0                               0
                                                                                                                                                                                                                              av
                             0   0.5   1.   0       0.5     1.   0   0.5   1.   0   0.5   1.   0   0.5   1.
                                                                                                                                1                              1                               1
                         4                                                                                                      2                              2                               2
                                                                                                                                3                              3                               3
                             0   0.5   1.   0       0.5     1.   0   0.5   1.   0   0.5   1.   0   0.5   1.
                   ...
 Figure 1: Left: Evolution in the posterior belief in the probability of a positive outcome p as a function of experienced outcomes. The model
 starts with a uniform prior belief at (0,0) in the grid (Beta(α = 1, β = 1)). The grey region depicts the area under the probability density
 function for p > 0.5 (i.e., mainly positive outcomes) Posteriors with one more negative than positive outcome are highlighted in blue. Top
 right: Optimal choice policies for agents with a low discount rate (γ = 0.0, left), high discount rate (γ = 0.999, center) and the difference
 between the choice policies (right). Grid axes denote different numbers of positive and negative experiences with the prospect. Bottom right:
 An illustration of two stochastic policies (γ = {0.0, 0.999}, τ = 1, ε = 0.1). Ultimately the assumptions tend to “blur” the hard line between
 approach and avoid in the optimal policy and make the probability of approaching slightly higher for negative prospects.
 illustrates how posterior belief about the prospect changes as                                                                               als before avoidance begins–are expected to occur quickly,
 positive and negative experiences accumulate.                                                                                                this threshold changes as a function of how much the model
    Given a current observation set of α positive and β negative                                                                              values the future.
 outcomes, the total expected value of following the optimal                                                                                     Observe also that as the number of positive outcomes expe-
 policy is V (α, β) = max{Qapproach (α, β), Qavoid } where                                                                                    rienced by the farsighted agent grows, the region in which a
                                                                                                                                              seemingly negative prospect is still sampled becomes wider.
    Qapproach (α, β) = E[Beta(α, β)][1 + γV (α + 1, β)]
                                                                                                                                              For example, with zero positive outcomes the farsighted
                                                          + (1 − E[Beta(α, β)])[−1 + γV (α, β + 1)]                                           model stops sampling at five negative outcomes, but at three
                                                                                                                                              positive outcomes the model stops sampling after not eight
                                                            α
                                                =              [1 + γV (α + 1, β)]                                              (1)           but ten negative outcomes. This is because as the number of
                                                          α+β                                                                                 total experiences increases, it takes more negative outcomes
                                                              β                                                                               relative to positive ones to become highly certain that the
                                                          +       [−1 + γV (α, β + 1)]
                                                            α+β                                                                               prospect is negative. This can be seen most clearly by ex-
                                                                                                                                              amining the posteriors highlighted in blue in the left panel of
    and Qavoid = 0. The optimal decision policy is to approach
                                                                                                                                              Figure 1. In all of these cases, there has been one more neg-
 when Qapproach is greater than Qavoid , and avoid otherwise.
                                                                                                                                              ative outcome than positive outcome. But as the total num-
    The optimal decision strategy depends on a recurrence re-
                                                                                                                                              ber of trials increases the proportion of the posterior where
 lation between V (α, β), V (α + 1, β), and V (α, β + 1) To solve
                                                                                                                                              p > 0.5 grows larger, so the uncertainty about whether the
 the relation, it is possible to estimate V for all pairs α + β = N
                                                                                                                                              prospect is positive increases as well. Thus, one behavioral
 for some large N and then work backwards towards V (1, 1)
                                                                                                                                              signature of the optimal model is a region of uncertainty in
 using dynamic programming (Gittins et al., 2011).
                                                                                                                                              which the agent continues to sample negative prospects and
    In Equation 1, γ is a free parameter denoting the degree to
                                                                                                                                              that grows with the total number of experiences.
 which future rewards are discounted. When γ → 0 the opti-
 mal policy cares only about immediate reward and will not                                                                                    Optimal decision making among multiple prospects The
 consider the value of information possible from approaching.                                                                                 logic behind the model just described generalizes to a situa-
 As γ → 1, the policy begins to take into account future en-                                                                                  tion where instead of one prospect, there are multiple, which
 counters with the prospect and uncertainty about the current                                                                                 have independent probabilities of positive outcome pi and
 estimate. The dependence of the current value of an action                                                                                   which appear with some base rate or frequency fi , ∑ fi = 1
 upon the future (weighted by γ) is what helps the model to                                                                                   (e.g., each day a scientist can go or not go to a different sem-
 avoid the “hot stove effect” (but see Denrell, 2007).                                                                                        inar, some of which meet more frequently than others). If
    To help the reader develop an intuition, Figure 1 (right                                                                                  there is no uncertainty about the identity of the prospect pre-
 panel, top) shows the optimal policy of two hypothetical                                                                                     sented on a given trial and assuming that the value of pi is
 agents that value future rewards to different degrees. Each                                                                                  independently sampled from a uniform distribution for each
 agent ceases to approach the alternative when the potential                                                                                  option, the optimal approach policy for each prospect can be
 future payoffs, weighted by their likelihood, become less                                                                                    calculated independently.
 valuable than the potential future losses. Because the poten-                                                                                   However, differences in base rate of occurrence between
 tial positive outcomes are distributed far out into the future,                                                                              different prospects has a subtle influence on the optimal
 while the potential negative outcomes–the few more bad tri-                                                                                  policy. In particular, the nth future experience with a rare
                                                                                                                                         1282

prospect is further away in time than the nth future experi-         influenced by the current uncertainty in the estimate of p as-
ence with a common prospect. One way to account for this             sociated with a prospect. Second, the optimal decision policy
difference is through adjustment to the temporal discounting         for γ > 0 takes into account the value associated with future
parameter (γ) for each prospect. If the base value of the next       interaction with a prospect. This interacts in an interesting
trial compared to the current one is γb , then the expected value    way with the base rate or frequency of the prospect; informa-
of the next trial for an alternative occurring with frequency f      tion gained about a rare prospect is expected to be applica-
is                                                                   ble less frequently and thus has less utility, such that at the
                         ∞
                                                                     same level of overall uncertainty it is more advantageous to
                  γf =  ∑ f · (1 − f )n−1 · (γb )n            (2)
                                                                     approach a frequent prospect than an infrequent one.
                        n=1
which decreases monotonically as f decreases. This means
                                                                                                 Experiment
that an optimal agent faced with rare and common prospects
will behave as though it values the future less when encoun-         To investigate the effects of base rate and uncertainty on
tering rare prospects, and will begin avoiding these prospects       approach-avoid decisions, we created an online video game
with less negative evidence (similar to the difference depicted      called the Mushroom Game. In the game, participants played
in Figure 1, top right).                                             the role of field biologists cataloging and learning about the
                                                                     edibility of mushroom species growing in different habitats.
Comparing human behavior and the optimal model An                    The base rate of occurrence for different mushrooms was ma-
optimal agent will approach a prospect on every trial until it       nipulated across the environments and our goal was to see
determines with sufficiently high certainty that the prospect        if participants adjusted their approach/avoid behavior in line
is negative, and then will never approach again. However,            with the predictions of the optimal model.
it is plausible that human decision makers behave in a way
generally consistent with the model but decide more stochas-         Method
tically. Following typical assumptions in the decision making        Participants One hundred fifty-two participants (65 women and
                                                                     87 men) age 18 to 66 years (M = 33.0, SD = 10.2) completed the
literature (Luce, 1959; Sutton & Barto, 1998), we assume the         task via Amazon Mechanical Turk. All participants were paid $2 for
model’s probability of approach, Pm is                               participation with the possibility of earning a bonus that averaged
                                                                     $1.02 and ranged from $0 to $1.60. Participants were instructed on
                                     eQapproach ·τ                   all aspects of the task and were required to pass a quiz demonstrating
             Pm (approach) =     Qapproach ·τ
                                                              (3)    comprehension of the instructions before entering the experiment.
                                e             + eQavoid ·τ           Three participants required more than three tries to pass a quiz on
                                                                     the instructions, and were excluded from all further analyses.
Where τ is a parameter that determines how deterministic the
participant is in responding, and Qapproach and Qavoid are the       Materials Each participant played the Mushroom Game in two
                                                                     habitats, which shared the same overall structure. Each habitat con-
expected values of approaching or avoiding on the current            tained four unique mushroom species which were taken from illus-
trial and subsequently following the optimal policy (α and β         trations of actual mushroom species found online.
are implicit in the notation now). In general, the probability       Procedure and design The experiment was divided into two
of approaching is an increasing function of its value relative       “habitats” within which the participant was asked to learn about lo-
to avoiding.                                                         cal mushroom species (e.g., “New England Forest”, or “Amazonian
                                                                     Rainforest”). Within each habitat there were four distinct mushroom
   In addition we observed that participants occasionally ap-        species, two of which occurred with frequency 4/10 and two of
proached a negative prospect even after avoiding it for several      which appeared with frequency 1/10. One high-frequency and one
trials. This behavior is not well captured by Equation 3 with-       low-frequency species were healthy (i.e., rewarding) with probabil-
                                                                     ity 0.7, while the other two species were poisonous (i.e., punishing)
out an extreme value of τ (near zero) but may plausibly reflect      with probability 0.7. The assignment of base rates, reward probabil-
an incorrect assumption about the non-stationarity of reward         ities, and the identity of fictitious mushroom species was randomly
probability for the prospects (basically “checking” to see if        determined for each participant.
                                                                         Within each habitat, the game was broken into two phases. In
reward probabilities have changed, e.g., Tversky & Edwards,          the first phase, participants observed a large, representative sample
1966; Knox et al., 2011). To account for these choices, what-        of the mushrooms in the habitat. Mushrooms encountered in this
ever their underlying cause, we let                                  sample were depicted by gray dots which appeared on the screen
                                                                     without participant input. Once the entire sample has been shown,
            P(approach) = (1 − ε)Pm (approach) + ε            (4)    the species were highlighted one at a time and participants submit-
                                                                     ted a “field report” by answering questions of the form “If you saw
                                                                     10 mushrooms on your hike back through the [Habitat Name], how
such that the model follows the stochastic optimal rule with         many would you expect to be from the species [Species Name]?”
probability (1 − ε) and approaches regardless of Qapproach           This ensured that participants noticed and encoded the relative fre-
with probability ε. With ε > 0, the model has a small con-           quency of each species.
                                                                         Figure 2 shows an example of the interface of the game. The
stant probability of approaching on any trial. Figure 1 (right       main feature of the interface is the “Field Log”, a row of icons rep-
panel, bottom) illustrates how the form of the optimal policy        resenting the local species with dots above each icon to represent the
is generally modified by these additional assumptions.               mushrooms that have been observed from that species. These dots
                                                                     effectively form a histogram showing the relative frequency of the
Two key principles of the optimal model The optimal de-              species. At the end of the first phase, the screen would look similar
                                                                     to Figure 2 except with only gray dots visible.
cision maker just described exposes two key behavioral prin-             In the decision-making phase, participants’ goal was to learn
ciples. First, the decision to approach or avoid is strongly         which of the habitat’s mushroom species were healthy and which
                                                                 1283

                                                                           Model-Based Analysis The predicted relationship between
                                                                           uncertainty, base rates, and approach/avoid decision is com-
                                                                           plex and multivariate. Thus, the most direct test of our hy-
                                                                           pothesis is obtainable through model-based analyses of par-
                                                                           ticipants’ trial-by-trial choice behavior. We consider six dif-
                                                                           ferent models.
                                                                              Stochastic optimal model with fixed base-rate (SO-F).
                                                                           The first model behaves in accordance with the optimal
           P. aurivella   M. conica     L. manzanitae A. ponderosa         model described above but assumes that each of the four
                                                                           mushrooms occurs with probability 1/4 on each trial. This
       Potential Bonus:
       $0.55
                                                                           model effectively represents our null hypothesis that the
                                                                           manipulation of base rate has no effect on participants’
                                                                           choice behavior. This model is endowed with three free
                                        eat                 avoid
                                                                           parameters per participant (γ, τ, and ε).
Figure 2: An example of the task interface from the decision phase            Stochastic optimal model with variable base-rate (SO-V).
(see text for description).                                                The second model represent the alternative hypothesis that
                                                                           participants adjust their sampling behavior based on the base
were poisonous by eating or avoiding the mushrooms they came
                                                                           rate of the mushrooms. This model is identical to the SO-F
across. During each trial of the decision-making phase, partici-           model but adds one additional parameter: a freely varying
pants encountered a randomly selected mushroom and its species             base rate parameter fl representing the (subjective) frequency
was highlighted. The probability a mushroom was selected on any            of each of the low-frequency mushroom. The base rates of
trial was matched to the relative frequencies from the first phase.
Participants then chose to either eat or avoid the mushroom by click-      the high-frequency mushrooms were set to (1 − 2 fl )/2.
ing the appropriate button. Eating a mushroom revealed whether the            Critically, if participants behave optimally but do not take
mushroom was healthy or poisonous, and added a green or red dot            the difference in the value of information caused by the dif-
to the species histogram on top the observation-phase observations
(which remained visible). Avoiding a mushroom revealed no infor-           ferent base rates into account in their sampling choices, they
mation about its healthiness, and added a gray dot like those seen         will be better fit by SO-F. If they do take the difference of
in the first phase to the species histogram. Since each mushroom           base rates into account, they will be better fit by SO-V.
species had both a non-zero probability of being healthy and a non-
zero probability of being poisonous, participants had to sample each          We also tested several alternative models which exhibit
species more than once to gain an accurate estimate of its healthi-        similar properties to the optimal model but which should be
ness.                                                                      distinguishable based on behavioral data.
    To aid visual estimation of the outcomes and frequencies, the dots
were organized on the screen so that grey dots appeared below red             Random choice model. The first model is a baseline which
dots which in turn appeared below green ones as seen in Figure 2.          assumes that participants chose to approach all mushrooms
    Participants earned a cash bonus based on their ability to eat         with some constant probability p.
healthy mushrooms while avoiding poisonous ones. Each partici-                Softmax reinforcement learning model (SM) . The SO-F
pant’s potential bonus started at $0.50, and increased by $0.05 for
each healthy mushroom eaten but was reduced $0.05 for every un-            and SO-V models can be contrasted with a standard non-
healthy mushroom eaten. Avoiding a mushroom had no effect on the           forward-looking reinforcement learning based model (Sutton
potential bonus. At the end of the experiment, the actual bonus was        & Barto, 1998). In this model, the probability of approaching
chosen at random from the potential bonuses earned in the first and
second habitat.                                                            is based solely on the estimate of the value of approaching
                                                                           learned from experience so far and the model is thus sus-
Results                                                                    ceptible to the hot stove effect (Denrell, 2007). The esti-
                                                                           mate of the value is updated after each approach according
Performance Participants generally learned which species                   to Qapproach = Qapproach + α · δ where δ = r − Qapproach , r is
were usually healthy and which were usually poisonous, ap-                 the received reward, and α is a learning rate/recency parame-
proaching healthy species with probability 0.89 (SD = 0.16)                ter (0 ≤ α ≤ 1) that controls the degree to which the current
and poisonous species with probability 0.38 (SD = 0.20). The               estimate depends on the most recent rewards. The probability
average potential bonus was $1.08, compared with an aver-                  of approaching is again determined by a probabilistic choice
age of $1.36 earned by a deterministic optimal model with                  rule, given by
γ = 0.995. We found no difference in the probability of ap-
proaching healthy and poisonous species or the bonus earned                                                     eQapproach ·τ
                                                                                            Pm (approach) =                            (5)
between participants’ first and second habitat and so collapse                                                eQapproach ·τ + 1
across habitats for all analyses. Participants’ predictions in             To this choice rule we added the same ε parameter as in the
the “field report” about how many times out of ten they would              optimal model, and used equation 4 to determine the actual
see each species in the future were on average 0.91 off from               approach probability.
the true frequencies (SD = 1.01), showing that they were able                 This model is an interesting competitor to the optimal
to estimate base rates with reasonable accuracy given his-                 model for a few reasons. First, it maintains only a point esti-
togram data.                                                               mate of the value of each decision option rather than the full
                                                                       1284

                 Table 1: Summary of model fits
                                                                                                 # of Negative Outcomes
                                                                                                 0      4        9        14   19   24
                                                                    # of Positive Outcomes
        Name      # Param.    BIC(mean)      % best fit                                      0                                            Best fitting
                                                                                                                                          stopping rule
        SO-V          4         174.0           56                                                                                        # positive =
                                                                                             3
        SO-F          3         179.4           21                                                                                        # negative
        SM-V          5         189.8            7                                           5                                           approach
        SM-F          4         189.6            4                                           7
        SM-0          3         202.7            6
                                                                                    10                                                   avoid
        Rand.         1         302.1            7
                                                                    Figure 3: Approximate approach policy of participants for frequent,
        Table 2: Median(mean) optimal model parameters              usually-poisonous mushrooms. To construct this figure, we first de-
                                                                    termined the positive-negative grid location after the final trial for
 Name          γ             τ             ε              fl        each experienced mushroom. Then for each square within each row
 SO-V      0.99(0.91)    0.29(0.39)    0.05(0.09)     0.05(0.08)    of the grid, we determined the proportion of participants who sam-
                                                                    pled exactly that many positive outcomes who also sampled at least
 SO-F      0.80(0.70)    0.14(0.43)    0.07(0.11)                   that many negative ones. These proportions were shaded from green
                                                                    (1.0) to red (0.0), providing an estimate of participants’ likelihood
                                                                    of continuing to sample in a given grid cell. Linear regression was
                                                                    used to find the best-fitting average stopping rule.
uncertainties depicted in Figure 1 (left). Second, it is not sen-
sitive to the future utility of particular outcomes. It can how-    Do people take uncertainty into account? Figure 3 shows
ever mimic some aspects of the optimal policy. For example,         choice behavior for the high-frequency, usually-poisonous
if the model is given an additional set of parameters corre-        mushrooms, from which the most data on participants’ ap-
sponding to the initial values of Qapproach for each prospect,      proach policy towards negative prospects is available (see
this can be used to encourage exploratory approach behav-           caption). As can be seen from comparison to the solid “# pos-
ior since it may take several negative outcomes to lower the        itive = # negative” line, virtually all participants continue to
Qapproach value past zero (“optimistic initialization” in the       sample from seemingly negative alternatives for several more
RL literature Sutton & Barto, 1998). The softmax model may          negative trials before avoiding the prospect. This is consistent
also account for differences in behavior between base rates by      with the generally high discount parameter γ of participants
allowing separate optimistic Qapproach for high-frequency and       best fit by the optimal models1 .
low-frequency mushrooms. However, the model provides no                Comparison of the “# positive = # negative” line to the
a-priori rationale for such parameter differences.                  best-fitting stopping rule shows that the region of uncertainty
   We fit participants to three versions of the softmax model.      in which participants continued to sample seemingly negative
The SM-0 had initial values for Qapproach set to zero for all       prospects widened as the number of total experiences grew, as
prospects, and is most similar to the adaptive models de-           predicted by the optimal model. This observation also gives
scribed by Denrell & March (2001) and Denrell (2007). The           insight into why the softmax models do not provide a good fit
fixed starting Q model (SM-F) allowed the initial Qapproach         to the data, even with optimistic initial Q values. While ini-
to vary per participant, but set it equal for all mushrooms.        tial optimism can explain perseverance through the first few
The variable starting Q model (SM-V) allowed the initial            negative outcomes from a prospect, as the number of trials
Qapproach to be set to separate values for high-frequency and       increases, the agent’s optimism is washed away by experi-
low-frequency mushrooms. The number of parameters per               ence. In other words, the point-estimate Q-values regress to
participant for each of these models is summarized in Table 1.      the mean of the sample.
Model comparison We fit each of the six models to indi-                Thus if participants were behaving like softmax agents,
viduals’ trial-by-trial choices to maximize the likelihood of       their stopping rule would be expected to converge to the “#
participants’ approach/avoid decisions. We then calculated          positive = # negative” line as the number of total outcomes
the Bayesian Information Criterion (BIC) for each model,            increased. The fact that it instead moves farther away sug-
which compares the quality of the model fits while penalizing       gests that people are not purely adaptive to past experience in
models for their number of free parameters.                         their approach decisions, and instead look towards the future
                                                                    and take their uncertainty into account.
   Average model BIC, and the proportion of subjects best fit
by each model, are shown in Table 1. The SO-V model, which          Do people take base rate into account? Among the 83
used a probabilistic version of the optimal choice rule and         participants best fit by the SO-V model, 78 had fl < 0.25,
was sensitive to differences in mushroom frequency, provided        meaning their behavior was best fit by an optimal model
the best fit for more than half of all participants. The SO-        which correctly assumed the low-frequency prospects oc-
F model provided the best fit for roughly twenty percent of         curred less often than the high-frequency ones. The mean
participants, while the three softmax models and the random         best-fit fl (see Table 2) is less than the true frequency of 1/10,
model best accounted for the final quarter of participants. The        1 While the mean γ across the two models is only 0.85, this low
mean and median parameters for subjects best fit by the SO-V        value is due to a strong left skew; 47% of these participants had
and SO-F models are listed in Table 2.                              γ > 0.99, and 71% had γ > 0.9.
                                                                1285

indicating that participants actually responded to the base rate                               # of Negative Outcomes
manipulation more strongly than was optimal and undersam-                                          0 1 2 3 4 5 6 7 8             0 1 2 3 4 5 6 7 8
                                                                                               0                             0
pled rare items relative to common ones. This behavior is                                      1                             1
                                                                                                                                                                 ch
consistent with evidence that people underestimate the prob-                                                                                                ap
                                                                      # of Positive Outcomes
                                                                                               2                             2                                proa
ability of rare events when they learn about them through ex-                                  3                             3
                                                                                               Participant choices for high- Participant choices for low-
perience rather than description (Hertwig et al., 2004).                                          frequency prospects           frequency prospects
   The right panel of Figure 4 shows the choice policies of
                                                                                                   0 1 2 3 4 5 6 7 8             0 1 2 3 4 5 6 7 8
participants best fit by SO-V, along with the difference be-                                   0                             0                               oi d
tween their policies for high- and low-frequency prospects                                     1                             1                              av
                                                                                               2                             2
and the predicted difference based on SO-V model simula-                                       3                             3
tion. As shown at bottom left, participants tended to per-                                          Choice difference              Choice difference
sist in sampling a negative prospect slightly longer when the                                         (high - low)                (model simulation)
prospect occurred frequently. This trend is consistent with
                                                                       Figure 4: Top: Approximate approach policies calculated for high-
participants’ low fl parameter fits, and its shape is similar to       and low-frequency, usually-poisonous mushrooms, for participants
the model simulation (bottom right).                                   best fit by SO-V. Method of generation matches that described in
                                                                       Figure 3, except only the first 11 trials with each prospect are
                                                                       considered. Bottom: Difference in participants’ approach policy
                        Conclusions                                    between high-and low-frequency prospects, with model simulation
In summary, participants were able to modulate their ap-               from SO-V using median best-fit parameters.
proach behavior in response to their beliefs about the future
                                                                       Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B., & Dolan, R. J.
utility of those interactions. While humans may not generally            (2006). Cortical substrates for exploratory decisions in humans.
make approach-avoid decisions in an optimal fashion, at the              Nature, 441(7095), 876–879.
very least they appear to take into account two decision vari-         Denrell, J. (2005). Why most people disapprove of me: experience
ables (uncertainty and the value of information) in a way con-           sampling in impression formation. Psychological review, 112(4),
                                                                         951-978.
sistent with optimal sequential decision making policies. This         Denrell, J. (2007). Adaptive learning and risk taking. Psychological
finding is interesting in light of the large literature on the “hot      review, 114(1), 177.
stove effect” (e.g., Biele et al., 2009; Fazio et al., 2004) and       Denrell, J., & March, J. G. (2001). Adaptation as information re-
also contrasts with findings which show that exploration be-             striction: The hot stove effect. Organization Science, 12(5), 523–
                                                                         538.
havior in multi-armed bandit tasks is not particularly sensitive       Fazio, R. H., Eiser, J. R., & Shook, N. J. (2004). Attitude formation
to uncertainty (Daw et al., 2006). Interestingly, previous stud-         through exploration: valence asymmetries. Journal of personality
ies of bandit tasks with short, finite horizons have found that          and social psychology, 87(3), 293–311.
people tended to choose uncertain alternatives more when the           Gittins, J., Glazebrook, K., & Weber, R. (2011). Multi-armed bandit
                                                                         allocation indices. John Wiley & Sons, Ltd.
horizon was longer (Lee et al., 2011; Meyer & Shi, 1995).              Guez, A., Silver, D., & Dayan, P. (2012). Efficient bayes-adaptive
However, the present experiment is the first, to our knowl-              reinforcement learning using sample-based search. Advances in
edge, to show that people are sensitive to the future value of           Neural Information Processing Systems, 25, 1034–1042.
information in a more naturalistic, indefinite-horizon environ-        Guez, A., Silver, D., & Dayan, P. (2013). Towards a practical bayes-
                                                                         optimal agent. (Poster presented at The 1st Multidisciplinary
ment.                                                                    Conference on Reinforcement Learning and Decision Making.)
   Unlike the present task, in real-world environments peo-            Hertwig, R., Barron, G., Weber, E. U., & Erev, I. (2004). Decisions
ple are often face additional uncertainty about the category             from experience and the effect of rare events in risky choice. Psy-
                                                                         chological Science, 15(8), 534–539.
membership of individual prospects (e.g., “Is this mushroom
                                                                       Knox, W. B., Otto, A. R., Stone, P., & Love, B. C. (2011). The
the same species as another?”). While research into these                nature of belief-directed exploratory choice in human decision-
kinds of more complex approach-or-avoid problems has thus                making. Frontiers in psychology, 2.
far only been considered in machine learning (e.g. Guez et al.,        Lee, M. D., Zhang, S., Munro, M., & Steyvers, M. (2011). Psy-
                                                                         chological models of human and optimal performance in bandit
2013, 2012), we hope that the current study is a step towards            problems. Cognitive Systems Research, 12(2), 164–174.
understanding how humans learn through experience-based                Luce, R. D. (1959). Individual choice behavior: a theoretical anal-
interactions with their environment.                                     ysis. John Wiley and sons.
Acknowledgments. This work was supported by grant number               March, J. G. (1996). Learning to be risk averse. Psychological
                                                                         Review, 103, 309-319.
BCS-1255538 from the National Science Foundation and the Intel-        Meyer, R. J., & Shi, Y. (1995). Sequential choice under ambigu-
ligence Advanced Research Projects Activity (IARPA) via Depart-          ity: Intuitive solutions to the armed-bandit problem. Management
ment of the Interior (DOI) contract D10PC20023 to TMG.                   Science, 41(5), 817–834.
                                                                       Niv, Y., Joel, D., Meilijson, I., & Ruppin, E. (2002). Evolution
                                                                         of reinforcement learning in uncertain environments: A simple
                         References                                      explanation for complex foraging behaviors. Adaptive Behavior,
Berry, D., & Fristedt, B. (1985). Bandit problems. London: Chap-         10(1), 5–24.
  man & Hall/CRC.                                                      Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An
Biele, G., Erev, I., & Ert, E. (2009). Learning, risk attitude and       introduction (Vol. 1) (No. 1). Cambridge Univ Press.
  hot stoves in restless bandit problems. Journal of Mathematical      Tversky, A., & Edwards, W. (1966). Information versus reward in
  Psychology, 53(3), 155–167.                                            binary choices. Journal of Experimental Psychology, 71(5), 680.
                                                                  1286

