UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Reappraising Lexical Specificity in Children’s Early Syntactic Combinations
Permalink
https://escholarship.org/uc/item/5836p4hs
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
McCauley, Stewart M.
Christiansen, Morten H.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                    Powered by the California Digital Library
                                                                     University of California

       Reappraising Lexical Specificity in Children’s Early Syntactic Combinations
                                         Stewart M. McCauley (smm424@cornell.edu)
                                      Morten H. Christiansen (christiansen@cornell.edu)
                                Department of Psychology, Cornell University, Ithaca, NY 14853 USA
                             Abstract                                 construction grammar (e.g., Croft, 2001; Goldberg, 2006).
                                                                      Such approaches dispense with the words and rules
  The flexibility and unbounded expressivity of our linguistic        framework entirely, holding instead that grammatical
  abilities is unparalleled in the biological world. Explaining       processing—and, by extension, children’s grammatical
  how children acquire this fundamental aspect of human               development—is primarily memory-based, driven by stored
  language is a key challenge for cognitive science. A recent
  corpus study by Yang (2013) has cast doubt on the lexical           linguistic units of varying granularity and complexity. That
  specificity of children’s productivity, as hypothesized by          is, knowledge of grammar is inseparable from lexical
  usage-based approaches. Focusing on determiner-noun                 knowledge; the two can only be distinguished insofar as
  combinations, he suggests that children possess an adult-like       they constitute polar ends of a spectrum of unit complexity
  determiner category. In this paper, we show that Yang’s             ranging from the level of simple symbols (such as
  results may depend too heavily on an idealized notion of            morphemes and simple words) to complex symbols (such as
  frequency distributions. We propose that these issues may be
                                                                      grammatical constructions). While such approaches to
  resolved by sidestepping sampling considerations and directly
  modeling children’s actual language processing. We therefore        explaining linguistic productivity allow syntactic categories
  evaluate the abilities of two computational models to capture       to be learned, they do not converge on a single notion of the
  children's productions of determiner-noun combinations. The         nature of such categorical knowledge (e.g., Croft, 2001), nor
  first model implements a probabilistic context-free grammar,        do they seek to directly explain the development of abstract
  which acquires statistical information incrementally. A             categories themselves. They have, however, inspired
  second model, the Chunk-based Learner (CBL), provides a             developmental research in what has become known as the
  simple instantiation of item-based learning. CBL outperforms
  the rule-based model, successfully producing the vast               usage-based framework. A number of researchers in the
  majority of the determiner-noun combinations in a dense             usage-based tradition have identified item-based patterns in
  corpus of child speech. The results thus suggest that the case      children’s early language use, such as verb-island
  against lexical specificity in children’s early determiner-noun     phenomena (e.g., Tomasello, 1992).
  sequences may be overstated.                                           With respect to the development of abstract syntactic
  Keywords: Language Learning; Grammatical Development;               categories, a number of usage-based corpus studies have
  Computational       Modeling;      Usage-based      Approaches;     focused on the English determiner category as a test case,
  Sampling; Syntactic Categories; Lexical Specificity                 inspired by early proposals that the categorical knowledge
                                                                      driving children’s early speech is quite limited (e.g., Braine,
                         Introduction                                 1976). In response to work arguing for an early abstract
Much of the debate on language learnability has centered on           determiner category (e.g., Valian, 1986), Pine and
the nature of children’s early productivity. Given the finite         Martindale (1996) analyzed seven corpora of child and
and noisy nature of the input, how are children able to               child-directed speech. Controlling for the number of multi-
generalize to a seemingly unbounded capacity for                      word utterances in each sample, as well as vocabulary
communicating novel information? The traditional answer               range, the authors found that children in the age range of 1;1
invokes a system of words and rules, in which processing is           to 2;4 exhibited far less overlap in their determiner use than
memory-based at the word level, but algorithmic at the                did their caretakers. Pine and Lieven (1997) extended this
multiword level; compositional operations are performed               general finding to a group of 11 child corpora.
over word classes corresponding to items from a mental                   Researchers have subsequently criticized the Pine and
lexicon (e.g., Chomsky, 1957; Pinker, 1999). Under this               Martindale (1996) and Pine and Lieven (1997) studies for
view, children are assumed to possess innate syntactic                the sparseness of the data used as well as the inclusion of
categories, such as noun and determiner. While various                nouns that children produced with a determiner only once,
theoretical approaches differ with respect to the way in              making it impossible for there to be any overlap (e.g.,
which innate word classes are mapped onto words                       Valian, Solt, & Stewart, 2009). More recently, Yang (2013)
themselves (e.g., Pinker, 1984), they converge on the idea            expanded on this criticism by noting that linguistic
that children’s early language use is—like adult language,            frequency distributions conform to a Zipfian pattern (Zipf,
under such a perspective—class-based. That is, children’s             1949), in which the frequency of a word is inversely
early comprehension and production abilities are governed             proportional to its rank in a frequency table. Yang argued
by computations over their innate syntactic categories.               that such Zipfian patterns have the consequence that, even
  In recent decades, a number of theoretical alternatives             for adult speech, most nouns appear so infrequently in a
have emerged from the field of cognitive linguistics, such as         corpus that they are unlikely to occur with more than one
                                                                      type of determiner. Yang used calculations based on Zipf’s
                                                                  1000

law to evaluate a memory-based language model trained on           artifacts depends heavily on the assumption that nouns in
1.1 million utterances drawn from the CHILDES database             child-directed speech follow Zipf’s law (a power law
(MacWhinney, 2000). Across 1,000 simulations, the model            function). Pine et al. (2013) question this assumption,
randomly selected determiner-noun sequences from the               demonstrating that the frequencies of the top 10 nouns in the
training data. The amount of determiner overlap in these           corpora used by Yang are different than what would be
randomly sampled pairs was then compared to the amount             expected based on Zipf’s law. While this result is
of overlap exhibited by selected target children’s                 informative, much larger samples are necessary in order to
determiner-noun productions. Yang’s finding was that the           establish definitively whether frequencies conform to a
memory-based random selection model significantly under-           given distribution. In what follows, we describe statistical
predicts the amount of overlap in children’s actual                tests performed on the frequencies of the entire set of nouns
determiner-noun productions, while a class-based                   in several corpora of child and child-directed speech. Our
calculation using Zipf’s law more accurately captures              results suggest that the nouns in each corpus are highly
children’s productivity. Based on this finding, Yang               unlikely to be drawn from a power law distribution, and
concluded that previous findings of lexical specificity in         thus do not follow Zipf’s law as Yang’s analyses assume.
children’s determiner use are sampling artifacts.
   Arguing against the idea that children’s item-based             Corpus Selection and Preparation
patterns in determiner usage are simply artifacts of Zipfian       Previous computational studies on the acquisition of
distributions, Pine, Freudenthal, Krajewski, and Gobet             syntactic categories have focused on a variety of publicly
(2013) presented a series of corpus analyses. In the first         available corpora of child-directed speech from the
analysis, they compared the overlap of determiners used            CHILDES database (MacWhinney, 2000). However, these
with nouns appearing in the speech of both children and            studies have been subject to problems of data sparseness, as
their caretakers against the overlap of nouns used only by         they have primarily relied on multiple small corpora that
caretakers. They showed that the results of the comparison         typically account for only 1-2% of the input to and speech
are sensitive to sample size, and that when this variable is       of a given child (cf. Maslen, Theakston, Lieven, &
controlled for, caretakers showed more overlap with nouns          Tomasello, 2004). Here, we focus primarily on the linguistic
appearing in child speech than nouns that did not. This lead       information available to a single child. This is achieved by
Pine et al. to control for vocabulary range in their second        using a dense corpus of child and child-directed speech,
analysis, demonstrating that once size and vocabulary are          which covers over 10% of the speech of and directed to the
both controlled for, there were significant differences            target child (the Thomas/Brian corpus; Maslen et al., 2004).
between children and their caretakers in terms of flexible         This provides an advantage over previous studies that have
determiner usage. A third analysis demonstrated                    relied on comparisons across several small sets of data.
increasingly flexible usage of determiners with a fixed set of     Nonetheless, for purpose of comparison, we also include the
nouns across two developmental stages.                             six smaller corpora of child-directed speech analyzed by
   Pine et al. additionally show that the top 10 nouns in the      Yang (2013): the Adam, Eve, Naomi, Nina, Peter, and Sarah
corpora do not conform to Zipf’s law. While this result is         corpora from the CHILDES database.
informative, the sample is too small to allow any decisive            Tags and codes were removed from each corpus, leaving
conclusions to be made. In the present paper, we therefore         only the speaker identifier and the original sequence of
conduct an exhaustive test of Yang’s (2013) assumption that        words. Nouns and determiner-noun sequences were then
nouns in child-directed speech conform to a Zipfian                identified and extracted using TreeTagger (Schmid, 1994).
distribution, evaluating the consequences of this analysis for
Yang’s case against item-based patterns. We then propose           Methods
an alternative approach that is less susceptible to sampling
                                                                   To evaluate the hypothesis that the noun frequency data
issues; while corpus analyses have provided great insight
                                                                   from the corpora follow a power law distribution, we use
into the nature of children’s early productivity, it remains
                                                                   the Kolmogorov-Smirnov (KS) goodness-of-fit test (Press,
for computational studies to explore the psychological
                                                                   Teukolsky, Vetterling, & Flannery, 1992), with
mechanisms involved in acquiring adult-like determiner use.
                                                                   corresponding p-values for the power law fit calculated
As an initial step, we present a computational study that
                                                                   according to the method described by Clauset, Shalizi, and
instantiates the principles of item- and class-based learning
                                                                   Newman (2007). The KS test evaluates the null hypothesis
in two distinct, simple models of language learning and use.
                                                                   that a sample is drawn from a given distribution (in this
The models are evaluated with respect to their ability to
                                                                   case, a power law). We also compare the power law fit to
capture children’s actual determiner-noun combinations
                                                                   alternative fits of lognormal and exponential distributions—
through generalization to unseen input.
                                                                   both appropriate candidates for frequency data with a long
                                                                   tail—using likelihood ratio testing (cf. Clauset et al., 2007).
   Experiment 1: Analyzing the Distribution of
              Nouns in Child-directed Speech                       Results and Discussion
Yang’s (2013) claim that previous findings of lexical              The results of the KS test for the distribution of nouns
specificity in children’s determiner use are merely sampling       across the entire dense corpus strongly suggest that the noun
                                                               1001

frequencies do not conform to a power law distribution (D =                              Experiment 2: Modeling Children’s
0.19, p < 0.001)1. The same pattern followed for the six                              Production of Determiner-Noun Sequences
smaller corpora originally used by Yang (2013) (with D
statistics ranging from 0.11 to 0.21, all p’s < 0.001).                         As an initial step toward modeling children’s actual
   Comparison to alternative distributions using likelihood                     comprehension and production processes during learning,
ratio testing confirmed that while the power law distribution                   we evaluate the ability of a simple, developmentally
provided a better fit to the dense corpus noun data than the                    motivated model of item-based learning processes, the
exponential distribution (R = 25.34, p < 0.001), the                            Chunk-based Learner (based on McCauley & Christiansen,
lognormal distribution was a far better fit than either the                     2011), to account for children’s determiner-noun
power law distribution (R = 17.8, p < 0.001) or the                             combinations. This ability is compared to that of a class-
exponential distribution (R = 28.75, p < 0.001). A                              based model with built-in grammatical categories, based on
complementary cumulative distribution function (CCDF)                           a standard probabilistic context-free grammar model
plot comparing the noun data from the dense corpus to the                       (PCFG; cf. Manning & Schütze, 1999). Unlike most
three distributions is shown in Figure 1. The data from the                     computational approaches to acquisition, both models are
six smaller corpora followed the same pattern in each case                      designed to capture the incremental nature of the task facing
(all p’s < 0.001).                                                              the learner: each is trained and evaluated in an incremental
                                                                                rather than batch fashion, and is only able to draw upon
                                                                                what has been learned from previously encountered input.
                                                                                After describing the models, we compare their ability to
                                                                                capture the determiner-noun combinations of the target
                                                                                children in corpora of child-directed speech, using the same
                                                                                seven corpora described above (the dense corpus and the six
                                                                                corpora used by Yang, 2013). Unlike the approach
                                                                                described by Yang, both models are evaluated on their
                                                                                ability to generalize to previously unseen input.
                                                                                Modeling Children’s Determiner Productivity
                                                                                        Using Grammatical Categories
                                                                                The class-based model involves a developmentally
Figure 1: CCDF plot depicting the distribution of nouns in the dense corpus     motivated modification to the standard PCFG language
        fit to power law, lognormal, and exponential distributions.             model; statistical information tied to each rewrite rule is
                                                                                acquired incrementally, during a single pass through a
   Our results strongly suggest that the distribution of nouns                  corpus. This allows the language model to maintain the
in the selected corpora do not follow Zipf’s law. As our                        generative capacity of the traditional PCFG through pre-
analysis covers not only the largest currently available                        established word classes and rewrite rules while also
corpus of English child-directed speech, but all of the                         simulating a gradual buildup of lexical information, as is
corpora used by Yang (2013), it suggests that Yang’s                            necessary even under nativist accounts of language
calculations based on Zipf’s law depend on a highly                             acquisition (e.g., Pinker, 1999). For the current simulation,
idealized notion of the distribution of noun frequencies, and                   we focus on a single fragment of the PCFG, corresponding
mischaracterize the degree of determiner-noun overlap that                      to two syntactic categories and a single rewrite rule:
would result from following the actual distributions of
nouns in corpora of child-directed speech.                                         NP → DET + N
   Rather than attempting to control for sampling                                  DET: {the, a, an}
considerations (as in Pine et al., 2013), we propose an                            N: {set of nouns encountered thus far in the corpus}
alternative approach that more directly evaluates the nature
of children’s early syntactic combinations. Specifically, we                       Thus, we focus only on simple noun phrases involving
suggest that to resolve these issues we need to move beyond                     definite or indefinite nouns (as in Yang, 2013). The
corpus analyses to the explicit modeling of the mechanisms                      simulation involves two simultaneous tasks: 1)
children are hypothesized to use in acquisition, and test how                   comprehension, in which distributional information tied to
well they account for children’s actual linguistic behavior.                    determiners is acquired, and 2) production, in which noun
In what follows, we take an initial step towards modeling                       phrases are produced stochastically according to the
children’s actual comprehension and production processes,                       information gleaned during comprehension up to the given
focusing on determiner-noun combinations.                                       point during the simulation at which a production attempt is
                                                                                made. Lexical knowledge in the model contains only two
                                                                                categories. The determiner category is pre-established, as
   1
     A separate test performed on only those nouns produced by the              depicted above, while the noun category is gradually built
child in a determiner-noun combination met with similar results (D              up on the basis of the input.
= 0.15, p < 0.001).
                                                                            1002

   Each time an adult utterance is encountered, the model          maintains frequency information for each chunk in the
engages in the comprehension task. During comprehension,           chunkatory. The model also uses the chunkatory to make
frequency information tied to each word type is                    on-line predictions for which words will form a chunk,
incremented. This allows the probability of a given word           based on previously learned chunks. Each time a word-pair
type to be calculated as the number of tokens of that type         is encountered, it is checked against the chunkatory; if it has
normalized by the total number of tokens encountered in a          occurred before as a complete chunk or as part of a larger
given category. Each time a child utterance of the form            chunk, the words are grouped together and the model moves
DET+N is encountered, the model engages in the                     on to the next word. If the word-pair is not found in the
production task. In this task, the PCFG is used in an active       chunkatory, the BTP is compared to the running average,
rather than passive fashion; given the target noun, the model      with the same consequences as before. Because there are no
stochastically produces a new DET+N sequence by                    a priori limits on the number or size of the multi-word
selecting one of the available determiners probabilistically,      building blocks that can be learned, the resulting chunkatory
according to the probability of each terminal (which is            will contain a mix of words and multi-word chunks.
updated incrementally during learning). The determiner is             For example, consider the following scenario in which the
then concatenated with the noun from the utterance, thus           model encounters the phrase the blue ball for the first time
directly implementing Yang’s (2013: p. 6324) assertion that        and its chunkatory includes the blue car and blue ball.
“very young children’s language is consistent with a               When processing the and blue, the model will not place a
grammar that independently combines linguistic units (…).”         boundary between these two words because the word-pair is
   The model was scored according to the number of                 already represented in the chunkatory (as in the blue car).
correctly produced determiner-noun combinations. The total         Instead, it predicts that this bigram will form part of a
number of correctly produced noun phrases was normalized           chunk. Next, when processing blue and ball, the model
by the total number of attempted DET+N productions,                reacts similarly, as this bigram is also represented in the
yielding a production accuracy score (percentage). As the          chunkatory. The model thereby combines its knowledge of
model is stochastic, 100 separate iterations were performed        two chunks to discover a new, third building block, the blue
on each input corpus. The mean score across all 100                ball, which is added to the chunkatory, and the model then
simulations was then taken as the final score.                     goes on to process the next word in the utterance.
                                                                      Thus, the model gradually creates an inventory of
Chunk-based Learner (CBL)                                          building blocks and uses these to segment the corpus into
Language learning in CBL involves improving the model’s            phrasal units—akin to shallow parsing—favoring sequential
ability to perform two tasks: “comprehension” of child-            information. This shallow processing approach was adopted
directed speech, through the statistical discovery and use of      because it is consistent with evidence on the relatively
chunks as building blocks, and “production,” which utilizes        underspecified nature of human sentence comprehension
the same chunks and statistics involved in comprehension.          (e.g., Frank & Bod, 2011; Sanford & Sturt, 2002) and
Comprehension is approximated in terms of the model’s              provides a mechanistic approximation of the item-based
ability to segment a corpus into phrasal units, and                way in which children are hypothesized to process sentences
production is approximated in terms of the model’s ability         by usage-based theories (cf. Tomasello, 2003). CBL’s
to reconstruct utterances produced by the child. While             ability to do phrasal segmentation compares well with off-
comprehension and production in the model are two sides of         the-shelf shallow parsers in English, German and French
the same coin, we describe them separately for simplicity.         (see McCauley & Christiansen, 2011, for details).
   Comprehension Although the model’s comprehension                   Determiner-Noun Production Each time the model
performance is not directly assessed in the current study, it      encounters a multi-word child utterance featuring a
drives the model’s ability to create utterances during             determiner-noun combination, it is required to produce its
production, including the determiner-noun combinations             own determiner-noun combination using the corresponding
that are the focus of this paper. Comprehension begins with        noun. The chunkatory is searched for chunks featuring the
the tracking of simple distributional statistics: As the model     target noun with a, the, or an, and the chunk with the
processes utterances word-by-word, it tracks frequency             highest frequency count is output. This provides a lexically-
information for words and word-pairs, which is used on-line        specific analogue to the PCFG production task, and thus
to track the backward transition probability (BTP) between         scoring is identical: the determiner-noun sequence must
words and maintain a running average BTP for previously            match the child’s.
encountered pairs. When the model calculates a BTP that is            In summary, as the model is exposed to a corpus, one
greater than expected, based on the running average, it            word at a time, it 1) builds a chunkatory—an inventory of
groups the word together with the previous word(s). When           single- and multi-word building blocks—and uses these to
the calculated BTP falls below the running average, a              segment and learn from the incoming input, and 2) uses the
boundary is placed and the chunk thereby created                   same chunks to attempt to reproduce the child’s determiner-
(consisting of one or more words to the left of the inserted       noun sequences as it comes across them in the corpus.
boundary) is added to the chunkatory, the model’s inventory           Sentence Production As an initial step towards capturing
of single- and multi-word units. Importantly, the model            the semantic dimension of children’s determiner-noun
                                                               1003

combinations within a more comprehensive item-based               the child wishes to convey (using a bag-of-words approach)
model of production, we report an additional set of               to incrementally produce entire utterances based on chunks
simulations involving the full version of CBL (cf.                and transition probabilities learned previously (the
McCauley & Christiansen, 2011). This version differs from         determiner-noun sequences in the utterance are then
that described above only with respect to the production          compared to those of the child).
task: each time a multi-word child utterance is encountered,
the model attempts to reproduce the entire utterance using        Results and Discussion
only building blocks discovered in the previously                 In all cases (see Figure 2), CBL outperformed the PCFG by
encountered input. Following Chang, Lieven and Tomasello          a wide margin. For the dense corpus, CBL successfully
(2008), we assume that the overall message, which the child       produced 70% of the child’s determiner-noun sequences
wants to convey, can be approximated by treating the              (94.3% on the sentence production task), while the PCFG
utterance as a randomly-ordered set of words: a “bag-of-          achieved a performance score of just 49.2%. Across the six
words.” The task for the model, then, is to output these          smaller corpora used by Yang (2013), CBL attained a mean
words in the correct order (as originally produced by the         score of 69.2% (87.3% on the alternative production task),
child). Following usage-based approaches, the model               while the PCFG achieved a mean score of 51.6%2.
utilizes building blocks from its chunkatory to reconstruct
the child’s utterances. In order to model retrieval of stored
chunks during production, word combinations from the
utterance that are represented as multi-word chunks in the
chunkatory will be placed in the bag-of-words instead of the
individual words that make up those chunks. E.g., consider
a scenario in which the model encounters the child utterance
the dog chased a cat and has both the dog and a cat as
chunks in its chunkatory. These two chunks would then be
placed in the bag along with chased, and the order of these
three chunks is randomized. The model then has to
reproduce the child’s utterance using the unordered chunks
in the bag. We model this as an incremental, chunk-to-
chunk process rather than one of whole-sentence
optimization. Thus, the model begins by removing from the
bag the chunk with the highest BTP given the # tag (which
marks the beginning of each utterance in the corpus), and
outputs it as the start of its new utterance. Next, the
remaining chunk with the highest BTP given the most
                                                                      Figure 2: Production accuracy as a function of time: CBL during full
recently produced chunk is removed from the bag and                sentence production task (green), CBL during determiner-noun task (blue),
output as the next part of the utterance. In this manner, the               PCFG (red). Trend lines derived from linear regression.
model uses chunk-to-chunk BTPs to incrementally produce
the utterance, outputting chunks one-by-one until the bag is         The incremental nature of both models allows us to
empty. Using this method, CBL is able to produce the              further compare the development of model performance
majority of utterances produced by children in 24 different       over time. After dividing each individual simulation into ten
Old World languages (McCauley & Christiansen, 2011).              bins of equal size (with the first bin representing the first
For the present study, the overall percentage of correctly        tenth of the model’s pass through the corpus, the second bin
produced determiner-noun sequences (i.e., those that are          representing the second tenth, and so forth), we examined
identical to the determiner-noun sequences in the target          the trajectory of model performance using bin as a temporal
child’s original utterance) is evaluated.                         dimension to predict production scores. This yielded a small
                                                                  but reliable correlation between performance and time bin
Models Summary                                                    for CBL across all simulations (R2 = 0.1, F1,68 = 7.46, p <
To summarize, we test the following models: 1) an                 0.01), with a mean score of 63.6 during the first phase and
incrementally trained PCFG (with built-in classes and             75.4 during the last. The correlation was also present for the
rewrite rules) which stochastically selects a determiner for      sentence production task (R2 = 0.15, F1,68 = 11.76, p < 0.01),
each target noun, and thus provides a straightforward             with a mean score of 79.9 during the first phase and 90.9
implementation of Yang’s (2013) claim that children               during the last. However, the PCFG did not exhibit a
combine linguistic units independently, 2) the CBL model,         significant difference in performance as a function of time
which produces determiner-noun combinations based on the
                                                                     2
frequencies of lexically-specific chunks learned and stored            To counter the potential objection of a lack of phonological
in its chunk inventory, and 3) a more comprehensive version       constraints, we re-ran the PCFG simulations treating both a and an
                                                                  as a single indefinite article. The mean production accuracy across
of CBL which roughly approximates the overall message
                                                                  all 7 corpora improved by less than one percentage point.
                                                              1004

(R2 = 0.01, F1,68 = 0.76, p = 0.38), with a mean score of 53.5     even as grammars grow more abstract, consistent with
during the first phase and 51.4 during the last.                   theoretical proposals emerging from cognitive linguistics
   Thus, the item-based model improved with exposure to            (e.g., Croft, 2001; Goldberg, 2006).
more input, which is further underscored by the slightly
better overall performance for the dense corpus than the                                 Acknowledgments
smaller corpora. This trend is significant despite the target      Thanks to Julia Ying for helpful comments. This study was
children’s increasingly flexible use of determiners over time      partially supported by BSF grant 2011107, awarded to MHC.
(cf. Pine et al., 2013), suggesting that purely item-based
processes continue to play a role even as children’s                                          References
grammatical categories appear to grow more abstract. This
                                                                   Arnon, I., & Snider, N. (2010). More than words: Frequency
idea resonates with recent psycholinguistic evidence for              effects for multi-word phrases. Journal of Memory and
item-based processing in adults (e.g., Arnon & Snider,                Language, 62, 67-82.
2010), and is consistent with usage-based theory more              Braine, M. D. (1976). Children’s first word combinations.
generally.                                                            Monographs of the Society for Research in Child Development,
                                                                      41, 1-104.
                    General Discussion                             Chang, F., Lieven, E., & Tomasello, M. (2008). Automatic
                                                                      evaluation of syntactic learners in typologically-different
The aims of the present study were twofold: firstly, to               languages. Cognitive Systems Research, 9, 198-213.
evaluate the claims of Yang (2013) that item-based patterns        Chomsky, N. (1957). Syntactic Structures. The Hague: Mouton.
in children’s determiner-noun combinations are merely              Croft, W. (2001). Radical construction grammar: Syntactic theory
artifacts of sampling from a Zipfian distribution, and                in typological perspective. Oxford: Oxford University Press.
secondly, to offer a further computational approach to             Goldberg, A. E. (2006). Constructions at work: The nature of
studying children’s early productivity—complementary to               generalization in language. New York: Oxford University Press.
previous corpus analyses—based on modeling the                     MacWhinney, B. (2000). The CHILDES Project: Tools for
                                                                      Analyzing Talk, Volume II: The Database. Lawrence Erlbaum.
mechanisms involved in children’s incremental language
                                                                   Manning, C. D. & Schütze, H. (1999). Foundations of statistical
learning and use.                                                     natural language processing. Cambridge: MIT press.
   Our statistical tests of the distributions of nouns from        Maslen, R. J., Theakston, A. L., Lieven, E. V. ., & Tomasello, M.
each of the corpora used by Yang, in addition to the                  (2004). A dense corpus study of past tense and plural
currently largest available corpus of English child/child-            overregularization in English. Journal of Speech, Language, and
directed speech, strongly suggest that the nouns in each case         Hearing Research, 47, 1319-1333.
do not conform to Zipf’s law. Consequently, Yang’s                 McCauley, S.M. & Christiansen, M.H. (2011). Learning simple
calculations, which are based on the assumption of a Zipfian          statistics for language comprehension and production: The
distribution, likely underestimate the degree of determiner-          CAPPUCCINO model. In L. Carlson, C. Hölscher, & T. Shipley
                                                                      (Eds.), Proceedings of the 33rd Annual Conference of the
noun overlap that would be expected based on the                      Cognitive Science Society (pp. 1619-1624). Austin, TX:
distribution of nouns alone. This would mean that lexically           Cognitive Science Society.
specific patterns found in previous corpus analyses (e.g.,         Pine, J. M., Freudenthal, D., Krajewski, G., & Gobet, F. (2013).
Pine & Lieven, 1997; Pine & Martindale, 1996) are more                Do young children have adult-like syntactic categories? Zipf's
than mere sampling artifacts.                                         law and the case of the determiner. Cognition, 127, 345-360.
   In this context, we argue that corpus analyses should be        Pine, J. M., & Lieven, E. (1997). Slot and frame patterns and the
complemented by an approach that sidesteps sampling                   development      of    the   determiner     category.   Applied
considerations, focusing instead on modeling the                      Psycholinguistics, 18, 123-138.
                                                                   Pine, J. M., & Martindale, H. (1996). Syntactic categories in the
mechanisms involved in language acquisition according to
                                                                      speech of young children: The case of the determiner. Journal
the particular theoretical approaches being evaluated. Our            of Child Language, 23, 369–395.
simulations provide an initial step in this direction: we          Pinker, S. (1984) Language learnability and language
report a simple, developmentally motivated model of item-             development. Cambridge, MA: Harvard University Press.
based language learning and use which successfully                 Pinker, S. (1999). Words and rules: The ingredients of language.
captures a large proportion of the actual determiner-noun             New York: Harper Collins.
combinations made by the target child of a dense corpus.           Tomasello, M. (1992). First verbs: A case study of early
That this simple approach dramatically outperforms a class-           grammatical development. Cambridge University Press.
based model in which determiners and nouns are combined            Valian, V. (1986). Syntactic categories in the speech of young
                                                                      children. Developmental Psychology, 22, 562–579.
independently (a notion key to Yang’s approach) lends
                                                                   Valian, V., Solt, S., & Stewart, J. (2009). Abstract categories or
support to usage-based approaches to children’s early                 limited- scope formulae? The case of children’s determiners.
syntactic combinations. The finding that the production               Journal of Child Language, 36, 743–778.
attempts of the item-based model improved over the course          Yang, C. (2013). Ontogeny and phylogeny of language.
of the simulations, despite the increasingly flexible use of          Proceedings of the National Academy of Sciences, 110, 6324-
determiners by the target child as a function of age (Pine et         6327.
al., 2013), resonates with the idea that item-based                Zipf, G. K. (1949). Human behavior and the principle of least
processing continues to play a role throughout development,           effort. Reading, MA: Addison-Wesley.
                                                               1005

