UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Coherence in the Visual Imagination: Local Hill Search Outperforms Thagard’s Connectionist
Model
Permalink
https://escholarship.org/uc/item/8jv346j7
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Vertolli, Michael
Davies, Jim
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                  Powered by the California Digital Library
                                                                    University of California

                                     Coherence in the Visual Imagination:
                 Local Hill Search Outperforms Thagard’s Connectionist Model
                                    Michael O. Vertolli (michaelvertolli@gmail.com)
                                                 Jim Davies (jim@jimdavies.org)
                             Carleton University, Institute of Cognitive Science, 1125 Colonel By Dr.
                                                     Ottawa, ON K1S 5B6 Canada
                            Abstract                                might search visual memory for other objects that often
                                                                    occur with that query.
  A cognitive model of the visual imagination will produce
  “incoherent” results when it adds elements to an imagined            Recent research in cognitive neuroscience supports this
  scene that come from different contexts (e.g., “computer” and     co-occurrence view. Under the title of “scene construction
  “cheese” with “mouse”). We approach this problem with a           theory,” this research has demonstrated that the
  model that infers coherence relations from co-occurrence          hippocampus plays a primary role in the construction of a
  probabilities of labels in images. We show that this              “coherent spatial context” for the integration of the
  algorithm’s serial traversal of networks of co-occurrence         components of imagined experiences among other cognitive
  relations for a particular query produces greater coherence
  than one leading model in the field of computational
                                                                    phenomena (Hassabis & Maguire, 2007; Maguire &
  coherence: Thagard’s connectionist model.                         Mullally, 2013). Parallel research on the role of the
                                                                    hippocampus in the emergence of conceptual knowledge
   Keywords: imagination; coherence; artificial intelligence        and knowledge transfer (see Kumaran, Summerfield,
                                                                    Hassabis & Maguire, 2009) endorses the “memory space”
                        Introduction                                hypothesis of hippocampal function. In this view, neurons in
The imagination is implicated in a wide range of abilities          the hippocampus encode the co-occurrence of the
related to human cognition. The list of abilities includes but      components of a given experience or event through their
is not limited to planning, problem solving, hypothetical           spatiotemporal associations (Konkel & Cohen, 2009). This
thinking, counterfactual thinking, theory of mind, and              means that objects can co-occur as a consequence of
mental time travel (Davies, Atance, & Martin Ordas, 2011).          associations due to spatial relationships (e.g., mice often are
Despite a plethora of research on the imagination as a              seen next to cheese) or temporal relationships (e.g.,
facilitator for these abilities (see, for example, Markman,         gunshots are often followed by death or injury). We only
Klein & Suhr, 2012), the generative capacity of the                 address the former in the current work.
imagination is an untapped area of research in this domain.
This work focuses on the visual faculty as it is the most
studied.
  When someone constructs a visual scene with the
imagination (e.g., a mouse eating a piece of cheese) they
might use visual memories from many different experiences
as the components of the new scene. How these components
are selected from the range of possible experiences is not
obvious. If more than just the mouse and cheese are
included in the scene, it is unclear what makes the selection
of some elements (e.g., a cat, mousetrap, floorboards, or a
countertop) more likely or appropriate than others (e.g., a
rollercoaster, map of Spain, or cruise ship).                       Figure 1. An incoherent scene generated by SOILIE for the
  What is known is that people do not arbitrarily select the        query ‘mouse,’ containing elements from both the computer
components for their imaginings, even if those imaginings           and animal senses of ‘mouse.’
are entirely fictional (see Cockbain, Vertolli, Davies, in
press). There is an intuitive coherence imposed on imagined            In sum, when one imagines a mouse eating a piece of
scenes that inhibits unusual and sometimes even highly              cheese, it is not surprising that mousetraps, cats, etc. are
creative combinations.                                              more likely to come to mind than unrelated elements. They
  One way that humans might make this selection is through          all often occur together in the world and, thus, are
the co-occurrence of objects in visual memory (by visual            associated in the brain. In this case, “a mouse eating a piece
memory we mean only the memory of visual things, and not            of cheese” serves as the query and the other elements are
a specific subsystem like the visuo-spatial sketchpad). Thus,       what are returned by some imagination process. One way to
when one is imagining a scene given an environmental                further explore this idea is through the use of computational
query (e.g., a novel, question, or problem), mental processes       models.
                                                                1676

  The Science of Imagination Laboratory Imagination                 affected differently; Cowan, 2001; Edin, et al., 2009). Thus,
Engine (SOILIE) is a computational model of a functional            it is assumed that on average an imagined scene has
description of processes in the imagination that generate           approximately three to five elements in it at any given time;
scenes from an environmental query (Vertolli, Breault,              though aggregates (i.e., combining two or more elements
Ouellet, Somers, Gagné, Davies, 2014). In place of human            into a single element) are entirely possible. Similarly, four
‘experiences’ and ‘objects’ SOILIE uses labeled images              labels, excluding the query, are retrieved by SOILIE from
from the web. When generating a novel scene, SOILIE must            the co-occurrence data and five labels in total are selected
determine which labels are appropriate to select, given a           for every imagined scene. We decided that this number,
particular query. And, in keeping with the descriptions             despite being in the upper part of the range, was the most
given above, SOILIE currently uses co-occurrence relations          useful: preliminary research suggested that larger sets of
to make this selection. In this context, co-occurrence is           labels increased the divergence in the success of the
determined by the frequency with which one label is present         underlying subsystems, five is still in the accepted range,
in the same image with another label.                               and the query does not really need to be maintained in
  SOILIE derives these co-occurrence relations from the             working memory to the same degree (an individual could
Peekaboom database of labelled images. With over fifty              always re-query) nor does it need to be retrieved.
thousand images and ten thousand labels, the Peekaboom                 However, after working with earlier instantiations of
database is one of the largest of its kind. The dataset is the      SOILIE (see Breault, Ouellet, Somers, & Davies, 2013), a
combined result of two online games: the ESP Game and               problem became apparent. When images are selected purely
Peekaboom (Von Ahn, Liu, & Blum, 2006). In the ESP                  on the basis of their co-occurrence with the initial query,
Game, pairs of players are shown the same image and                 that is, selecting the labels with the highest co-occurrence or
without communicating try to enter the same words (Von              “top-n,” the scenes produced are often contextually
Ahn & Dabbish, 2004). Words that both players enter are             incoherent in an intuitive sense.
associated with the image and, consequently, common                    For example, SOILIE was queried with the word ‘mouse,’
labels are applied to images collected from the internet. To        which is polysemous (i.e., it has multiple, related meanings;
prevent a narrow set of the most common words, labels               e.g., a computer mouse and the animal mouse). Each
would become unusable after repeated use. This increased            meaning of a polysemous word is represented by different
the diversity and size of the resulting label set for each          images in the database—assuming that a single image
image.                                                              would be highly unlikely to contain both meanings of a
  SOILIE’s dataset comes from a related game, Peekaboom,            given label (e.g., have the animal on a desk with a
which uses ESP game data and results in images with                 computer). Each of these different images is similarly
labelled selections of pixels. Both games are designed to           associated with a different collection of labels and each set
produce data that can be used in vision research. Thus, they        of labels has a different set of co-occurrence relations.
are particularly relevant for SOILIE’s task.                        Problematically, by being reduced to a collection of co-
  SOILIE uses co-occurrence probabilities extracted from            occurrence probabilities in visual memory, the sets of
the Peekaboom database described above. Co-occurrence               images, labels, and co-occurrence relations that separate the
probabilities are calculated by dividing the total number of        two polysemous meanings of the word “mouse” are no
images (I) in the Peekaboom database that contain the co-           longer directly detectable. They are collapsed into a single
occurring label (l) and a particular query (q) by the total         dimension associating pairs of labels (see Table 1).
number of images with just the query. Using set theory                The result is that models, like SOILIE’s original “top-n”
notation, this yields:                                              model, which act on this single dimension, will be unable to
                                                                    infer the appropriate contextual distinctions from the
                                   |𝐼𝑞 ∩ 𝐼𝑙 |                       differences in the underlying images. Thus, they will often
                       𝑃(𝑙 | 𝑞) =                                   produce incoherent images (see Figure 1).
                                      |𝐼𝑞 |
                                                                      The problem of ‘coherence’ is not exclusive to SOILIE.
where ∩ indicates set intersection and || indicates cardinality     Generally, models that address context need to be able to
(i.e., the total number of elements in the set). One important      select coherent combinations (Hullett & Mateas, 2009).
feature of this formalization is that it is non-commutative         Most models, due to memory limitations, will also need to
(i.e., P yields a different value for mouse-cheese than it does     reduce the original input (e.g., images) into some form of
for cheese-mouse). Parallel research on co-occurrence in the        compressed data (e.g., co-occurrence probabilities). In this
machine learning literature suggests that this is both more         compression, some of the information will be lost. Thus,
realistic (e.g., most weddings have flowers but most flowers        there will almost always be the dual task of 1) finding
are not in weddings) but most models do not account for it          compression techniques that are better able to capture
(see Huang, Yu & Zhou, 2012; Zhang & Zhou, 2013).                   greater quantities of salient information and 2) build
   Research in neuroscience suggests that visual working            decompression procedures that can re-derive information
memory can hold approximately three to five objects of              that was lost through higher-order patterns of the remaining
average complexity (i.e., they leave open the possibility that      data.
very simple objects and very complex objects might be
                                                                1677

Table 1: Label co-occurrence probability of two images               with success (e.g., Thagard, 1989, 1992, 2000; Eliasmith &
alone and in SOILIE’s complete database                              Thagard, 1997). However, before continuing his exploration
                                                                     of his preferred algorithm, Thagard makes one caveat;
Image 1 labels:                                                      mainly, that incremental algorithms, or what are commonly
mouse, eye, rodent, rat, animal, ear, ears                           referred to as local hill searchers in the machine learning
                                                                     and optimization literature, might offer valuable insights
Image 2 labels:                                                      into human cognition as both are known to perform sub-
mouse, wires, monitor, screen, headphones, computer                  optimally in many domains, including coherence.
                                                                        In what follows, SOILIE’s Coherencer system will be
Co-occurrence of each label with query “mouse” given only            compared to one instantiation of Thagard’s connectionist
those two images: 0.5                                                algorithm in the current domain (i.e., visual coherence in the
                                                                     human imagination). In previous research, Coherencer was
Co-occurrence of label with query “mouse” using all images           shown to better capture coherence than SOILIE’s original,
in the database:                                                     top-n model and a random search (Vertolli & Davies, 2013).
  rat            0.29            rodent           0.08               It was also shown that Coherencer falls under Thagard’s
  ear            0.19            ears             0.07               incremental class of algorithms. Thus, this comparison
  computer       0.17            eye              0.06               provides both a more robust test of Coherencer’s efficacy,
  animal         0.13            headphones 0.01                     and insight into the cognitive implications that Thagard
  monitor        0.12            wires            0.01               pointed to in his caveat.
  screen         0.10                                                   It is worth noting that both Coherencer and Thagard’s
                                                                     model, in as much as they exist in the brain, are both
Top-4 labels from the two images using co-occurrence from            necessarily instantiated in neural processes. One should not
database:                                                            confuse the semantic convenience of calling connectionist
rat, ear, computer, animal                                           models “neural networks” with a literal network of neurons
                                                                     and Coherencer with “something else.” What is being
   SOILIE’s coherence subsystem, Coherencer (Vertolli &              tested, then, is whether the higher-order functionality of
Davies, 2013), falls into the second category. Thus, it is not       hippocampal or similar processes is better replicated with a
a memory system, per se, but rather defines a form of                serial process or a parallel process, with the corresponding
contextual optimization of a given type of memory system,            implications for optimality in the system (in as much as
mainly, one that uses co-occurrence as per SOILIE and the            those implications are in fact accurate). A serial virtual
hippocampus. This characterization is consistent with both           machine can be implemented on a parallel computational
the cognitive neuroscience literature on the hippocampus,            architecture, neurological or otherwise. Thus, both types of
which sees the hippocampus as a system of constructive               processes are cognitively plausible when considering only
integration of memory components and with Paul Thagard’s             this aspect.
(2000) research on coherence.
   Thagard (2000) describes the problem of coherence as an                                   Implementation
optimization problem. That is, given a particular structure          We will proceed by giving a very brief description of
(e.g., co-occurrence), coherence is the dynamic construction         Coherencer (for a more detailed description, see Vertolli &
of the best combination of components to maximize or                 Davies, 2013) and a detailed description of the current
minimize a particular set of criteria. Thagard takes these           implementation of Thagard’s connectionist algorithm.
criteria to be a set of positive constraints (i.e., inclusion of       Coherencer operates as follows. First, Coherencer creates
one component increases the likelihood of inclusion of               a pool of all labels that co-occur with the query. From this
another component) and negative constraints (i.e., inclusion         pool, it initially puts the top-4 labels with the highest co-
of one component decreases the likelihood of inclusion of            occurrence in its memory buffer. Then, a square matrix of
another component). These constraints are optimized by               all the labels is created where each cell holds the co-
maximizing the number of positive constraints in a                   occurrence probability for the row-column pair or P(row(n),
collection and minimizing the negative constraints.                  column(m)). The average of the entire matrix is calculated
   After formalizing coherence in this way, Thagard                  and, if it passes a threshold (λ)1, the collection is accepted
proceeds to outline a number of general classes of                               1
                                                                     (i.e., if      ∑5 ∑5 𝑃(𝑙𝑚 | 𝑙𝑛 ) > 𝜆). We ignore the
computational models that can resolve this type of problem.                      20 𝑛=1 𝑚=1
He then dismisses all of them in favour of one: a                    diagonals, where n = m, in this calculation. If it fails to pass
connectionist algorithm. His argument, roughly, is that the          the threshold, the label with the lowest co-occurrence with
parallel approach inherent in these algorithms is both better        all other labels (i.e., for the ith label, the sum of all row(i)
at finding the global optimum (i.e., the best coherence for a
given set of constraints) and, for those algorithms that are            1 We take the threshold to be a coarse representation of a learned
comparable, it is more cognitively plausible. Thagard has            sense of coherence in the world. This means that the number would
implemented a number of such networks in related domains             vary depending on one’s experiences (i.e., given a different
                                                                     database) or in different contexts (e.g., when trying to be creative).
                                                                 1678

values and column(i) values) is discarded and cannot be                   is reduced to the maximum and minimum activation values
reselected. A new label is then swapped in from the pool                  if it exceeds them.
and the process repeats until either the pool is empty or a set              In the larger process, activations update until the average
passing the threshold is found. If the pool is empty,                     change in the sum of all differences is less than a threshold
Coherencer fails to create an imagined scene.                             (𝜃) or until 500 iterations occur. The following equation
   The construction of the connectionist model will proceed               illustrates the former:
as described by Thagard (2000). A node is constructed for
the query and every label co-occurring with the query. For                                  1     𝑡       𝑛
every positive constraint between two labels, an excitatory                       ∆𝑎⃗𝑡 =      ∑        ∑ (|𝑎𝑡,𝑖 − 𝑎𝑡−1,𝑖 |) < 𝜃
                                                                                          10𝑛     𝑡−10    𝑖=0
link is constructed between the corresponding nodes with a
weight equal to the co-occurrence probability. For every                  where Δa is the change in activation over the past 10
negative constraint (i.e., when two labels have a co-                     iterations, 𝑎𝑡,𝑖 means activation at time t and node i, || here
occurrence of 0), an inhibitory connection is constructed                 indicates absolute value, and 𝜃 is the threshold. The 4 labels
between corresponding nodes with a weight set to the                      with the highest activation are selected providing a sort of
average co-occurrence of all non-zero values (φ).2 An initial             top-4 filter.
activation (0.01) is assigned to each node with a special                    In what follows, we will describe an outline of the
locked activation (1.0) on the query node. All nodes then                 comparison of these two models.
have their activation updated in parallel3 using the following
formula4:                                                                                      The Comparison
                                                                          The comparison followed the basic structure of a memory
                                                ⃗⃗⃗⃗⃗⃗⃗)
                        𝑎⃗𝑡+1 = 𝑎⃗𝑡 (1 − 𝑑) + 𝑓(𝑛𝑒𝑡
                                                                          task, where a participant is given a collection of data; this
                                                                          data is compressed in memory, and then it is recalled. In our
where 𝑎⃗ is a vector of all the node activations at time t with
                                                                          abstraction of this structure, each model is given a collection
each label as a cell in that vector, d is a scalar decay
                                                                          of images, which are compressed into co-occurrence
parameter (0.05) that decrements each node at every cycle.
                                                                          probabilities in memory. The models are then tasked with
The vector 𝑛𝑒𝑡  ⃗⃗⃗⃗⃗⃗⃗ is computed by matrix multiplication as per:
                                                                          recalling this information.
                                                                             Unlike a memory task, the goal is not to test the bounds of
                                ⃗⃗⃗⃗⃗⃗⃗ = 𝑎⃗𝑡 𝑊
                                𝑛𝑒𝑡                                       human or animal functionality. Instead, it is to assess the
                                                                          efficacy of the decompression step that re-generates the
where W is the weight matrix for the network with its rows                coherence information that was lost during compression in
corresponding to the node being updated and the columns                   memory. Thus, the quantity of images remembered is not
corresponding to the linked nodes (i.e., neighbours of node               what is of interest. It is a certain quality in the generated
i). The values at 𝑊𝑖,𝑖 (i.e., the diagonal of the matrix) are set         images, namely coherence. This quality can be tested
to 0 so the activation passed from a node to itself is 0. 𝑊               quantitatively by determining if the elements (in this case,
also corresponds to Coherencer’s co-occurrence matrix with                labels) selected by the model when given a particular label
all co-occurrence values of 0 set to φ. Finally, f from the               or query do in fact occur in one of the original images. If
original equation is a function that performs element-wise                they do, then the original coherence information has been
multiplication with a different number depending on the                   successfully re-generated from the compressed data.
elements direction from zero as per this equation:                           It is worth noting that, outside of the methodological
                                                                          advantages just outlined, this conceptual method is also
                                   𝑥 = 𝑎𝑚𝑎𝑥 − 𝑎𝑖 if 𝑛𝑒𝑡𝑖 > 0              theoretically more plausible than approaches that do not
            ⃗⃗⃗⃗⃗⃗⃗) = 𝑛𝑒𝑡𝑖 𝑥 {
         𝑓(𝑛𝑒𝑡
                                   𝑥 = 𝑎𝑖 − 𝑎𝑚𝑖𝑛 if 𝑛𝑒𝑡𝑖 ≤ 0              account for memory. The research in cognitive neuroscience
                                                                          previously outlined suggests that the imagination, spatial
where x is the variable multiplier, 𝑎𝑖 is the ith value of 𝑎⃗,            navigation, and memory are all associated through the
𝑎𝑚𝑎𝑥 is the maximum activation of a node (1.0), and 𝑎𝑚𝑖𝑛 is               underlying functionality of the hippocampus. Thus, by
the minimum activation (-1.0). After the update, each node                testing the models through this sort of generative recall (an
                                                                          integrated imagination-memory process), one might better
                                                                          approach the mechanism that underlies all of these
   2 This was found to be 0.14878295850321488. The rounding               processes: cognitive generation proper.
occurred where it naturally does in the Python computer language             In either case, we hypothesize that Coherencer will
(double precision float). As a consequence, different languages           outperform Thagard’s model in the current comparison. We
may get slightly different results unless this is controlled.             anticipate that serial processes better capture the contextual
   3 The nodes are updated serially but the results of those updates
                                                                          transitions necessary to appropriately frame a given scene.
are not used until the next serial update of all nodes. Thus, the end     And, the advantages of using a parallel, non-linear
product is a parallel process implemented on a serial machine.            optimization process are lost when dealing with a single
   4 All formulas are vectorized implementations of those described
                                                                          feature.
by Thagard (2000). I chose to use row vectors instead of column
vectors as this more closely mirrors Coherencer’s implementation.
                                                                      1679

                         Method                                   anticipate both categories of systems we believe lends
                                                                  credence to it.
There are two models that were compared: Coherencer and
                                                                    The purpose of this comparison is to extend the theory in
Thagard’s model. The entire Peekaboom database was
                                                                  order to better comprehend the subtle nuances implicated
initially filtered to remove all images with fewer than five
                                                                  within it. For example, under what conditions are
labels and any labels that only occurred on those images. A
                                                                  incremental algorithms present? Here, the evidence suggests
total of 8,372 labels and 23,115 images remained after this
                                                                  that low level (i.e., co-occurrence), low dimensionality (i.e.,
filtration. All of the remaining images were compressed to
                                                                  just co-occurrence probabilities), with high combinatoric
their corresponding co-occurrence probabilities.
                                                                  load     (approximately      3.42x1017     possible   5-label
  Each of the 8,372 labels was run through both of the
                                                                  combinations) requires incremental, heuristic approaches.
algorithms 100 times and the results were averaged. Each
                                                                  Assuming the connectionist, parallel approach is optimal,
query plus four returned labels are the elements of a new
                                                                  how might that incremental approach switch into a
generated scene. The results for each of the algorithms were
                                                                  functionally parallel one? Or, does it only approximate a
assessed with regard to the original images. If at least one
                                                                  parallel approach, which forces sub-optimal solutions in
image in the test set contained the five labels that were
                                                                  higher-order domains? Thagard (2000) explicitly mentions
selected by a particular algorithm, including the query, the
                                                                  the tendency for humans to make sub-optimal decisions and
algorithm scored one point. If there were no images
                                                                  the potential association between incremental approaches
containing the five labels, they did not score a point. The
                                                                  and bounded rationality (Simon, 1991). This project
results on each of the labels were paired for comparison.
                                                                  supports this association.
The total number of points scored by a model where the
other model failed to score a point (i.e., excluding labels
                                                                  Table 2: McNemar χ2 calculation between Coherencer and
where both models failed or both models succeeded) were
                                                                  Thagard’s model.
used to compare Coherencer to Thagard’s algorithm.
                                                                                                                  Coherencer    Coherencer
                          Results                                                                                                                Total
                                                                                                                 failure        success
As hypothesized, Coherencer had more successful matches           Thagard’s                        Actual        2099.0         1166.0           3265.0
than the connectionist algorithm. The statistical details are     algorithm
as follows.                                                       failure
   McNemar’s repeated            measures chi-square test                                          Expected      1222.2         2042.8
demonstrates that Coherencer performed significantly better       Thagard’s                        Actual        1035.0         4072.0           5107.0
than Thagard’s algorithm, χ2(1, N=8372) = 7.80, p = .006,         algorithm
φ = 0.44. The average scores in each of the categories are        success
listed in Table 2. In this test, model runs where Coherencer                                       Expected      1911.8         3195.2
and Thagard’s algorithm both fail or both succeed on a            Total                            Count         3134.0         5238.0           8372.0
given query (i.e., the models perform identically) are
ignored; thus, the comparison occurs between the runs
                                                                                                              SD = 18.19
where one model failed and the other succeeded and vice                                         1200
                                                                      Average Number of Model
versa. All values are reported for completion and evaluation
                                                                                                1100
purposes. As is standard with chi-square tests, both the                                                                            SD = 10.23
actual number of runs and the statistically expected number                                     1000
of runs for a given category are reported. Figure 2 shows the
                                                                                                 900
                                                                               Runs
standard deviation for the 100 model runs for the values that
are used in the comparison (i.e., when the models are not                                              Thagard Failure and     Thagard Success and
                                                                                                       Coherencer Success       Coherencer Failure
performing identically). Even when the difference between
the two categories is the smallest, the result is still                                                Success to Failure Comparison Between Models
statistically significant, p = .031.
                                                                  Figure 2: Failure-Success and Success-Failure average
                       Discussion                                 scores with standard deviation bars for 100 model runs
The results support the idea that Coherencer generates
                                                                    Research in working memory has also described a
elements that create a more coherent scene than Thagard’s
                                                                  limited, serial system—the episodic buffer—that roughly
model. However, the intent is not to falsify Thagard’s claim
                                                                  matches what Thagard is describing (Baddeley, 2000). The
to the formal optimality of connectionist algorithms over
                                                                  episodic buffer is believed to be the means of integration for
incremental algorithms in the domains he considers, which
                                                                  the different sense modalities as well as the retrieval
are largely about higher-order epistemological relations and
                                                                  mechanism for long-term memories. That is, it is mapped to
constraints. Co-occurrence probabilities are part of a much
                                                                  a roughly identical, functional domain as the hippocampus.
lower system. The fact that Thagard’s theory could
                                                                  The limitations of these systems might result in downstream
                                                               1680

limitations, and this suggests a rather simple explanation for     Hassabis, D., & Maguire, E. A. (2007). Deconstructing
bounded rationality.                                                 episodic memory with construction. Trends in cognitive
   In the current research, these observations suggest               sciences, 11(7), 299-306.
interesting implications for Coherencer. With respect to           Huang, S. J., Yu, Y., & Zhou, Z. H. (2012). Multi-label
human cognition, Coherencer might better model the                   hypothesis reuse. In Proceedings of the 18th ACM
bounds of human rationality than the alternatives, including         SIGKDD international conference on Knowledge
Thagard’s connectionist models. Local hill searchers (i.e.,          discovery and data mining (pp. 525-533). ACM.
incremental algorithms) might be optimal if the compression        Kumaran, D., Summerfield, J. J., Hassabis, D., & Maguire,
in memory reduces the feature space to a low dimensionality          E. A. (2009). Tracking the emergence of conceptual
where non-linear techniques like Thagard’s model give too            knowledge           during          human          decision
little advantage for their increased cost in time and                making. Neuron, 63(6), 889-901.
resources. The parallels with both the hippocampus and the         Hullett, K., & Mateas, M. (2009, April). Scenario generation
episodic buffer suggest that Coherencer might also provide           for emergency rescue training games. In Proceedings of
a functional model and computational implementation that             the 4th International Conference on Foundations of
better describes contemporary research in these domains              Digital Games (pp. 99-106). ACM.
than the competitors. Additionally, it can provide a means         Konkel, A., & Cohen, N. J. (2009). Relational memory and
for cross-pollination and integration across the domains of          the hippocampus: Representations and methods. Frontiers
cognitive neuroscience, working memory, computational                in Neuroscience, 3, 166–174.
modelling, and artificial intelligence (or at least heuristic      Markman, K. D., Klein, W. M., & Suhr, J. A. (Eds.).
optimization, for the latter). Both of these parallels give          (2012). Handbook of imagination and mental simulation.
credence to Coherencer as a useful model of certain                  Psychology Press.
processes in human cognition. Future research will focus on        Mullally, S. L., & Maguire, E. A. (2013). Memory,
more advanced thresholds and feature spaces (e.g., spatial           Imagination, and Predicting the Future A Common Brain
relations in addition to co-occurrence), comparisons with            Mechanism?. The Neuroscientist, 19(3), 224-328.
other heuristic optimization models, and decreasing the            Simon, H. A. (1991). Bounded rationality and
divide between the formalization and research in cognitive           organizational learning.Organization Science, 2(1), 125-
neuroscience.                                                        134.
                                                                   Thagard, P. (1989). Explanatory coherence. Behavioral and
                         References                                  Brain Sciences, 12, 435-467.
Baddeley, A. (2000). The episodic buffer: a new component          Thagard, P. (1992). Adversarial problem solving: modeling
   of     working      memory?. Trends        in    Cognitive        an opponent using explanatory coherence. Cognitive
   Sciences, 4(11), 417-423.                                         Science, 16, 123-149.
Breault, V., Ouellet, S., Somers, S., & Davies, J. (in press).     Thagard, P. (2000). Coherence in thought and action.
   SOILIE: A computational model of 2D visual                        Cambridge, MIT Press.
   imagination. In R. West & T. Stewart (eds.), Proceedings        Vertolli, M. O. & Davies, J. (2013). Visual imagination in
   of the 11th International Conference on Cognitive                 context: Retrieving a coherent set of labels with
   Modeling, Ottawa: Carleton University.                            Coherencer. In R. West & T. Stewart (eds.), Proceedings
Cockbain, J., Vertolli, M. O. & Davies, J. (2013). Creative          of the 12th International Conference on Cognitive
   imagination is stable across technological media: the             Modeling, Ottawa: Carleton University.
   Spore Creature Creator versus pencil and paper. The             Vertolli, M. O., Breault, V., Ouellet, S., Somers, S., Gagné,
   Journal of Creative Behavior.                                     J. & Davies, J. (under review). Theoretical assessment of
Cowan, N. (2001). The magical number 4 in short-term                 the SOILIE model of the human imagination,
   memory: A reconsideration of mental storage                       Proceedings of the 36th Annual Conference of the
   capacity. Behavioral and Brain Sciences, 24(1), 87-114.           Cognitive Science Society. Quebec City, QC: Cognitive
Davies, J., Atance, C. & Martin Ordas, G. (2011). A                  Science Society.
   framework and open questions on imagination in adults           Von Ahn, L., & Dabbish, L. (2004). Labeling images with a
   and children. Imagination, Cognition, and Personality,            computer game. In Proceedings of the SIGCHI
   Special Issue on Mental Imagery in Children. 31:1-2,              Conference on Human Factors in Computing Systems (pp.
   143-157.                                                          319-326). ACM.
Edin, F., Klingberg, T., Johansson, P., McNab, F., Tegnér,         Von Ahn, L., Liu, R., & Blum, M. (2006). Peekaboom: a
   J., & Compte, A. (2009). Mechanism for top-down                   game for locating objects in images. In Proceedings of the
   control of working memory capacity. Proceedings of the            SIGCHI Conference on Human Factors in Computing
   National Academy of Sciences, 106(16), 6802-6807.                 Systems (pp. 55-64). ACM.
Eliasmith, C., & Thagard, P. (1997). Waves, particles, and         Zhang, M., & Zhou, Z. (2013). A Review on Multi-Label
   explanatory coherence. British Journal for the Philosophy         Learning Algorithms. IEEE Transactions on Knowledge
   of Science, 48, 1-19.                                             and Data Engineering, (99).
                                                               1681

