UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Categorical Perception of Facial Expressions Is Not a Homogeneous Effect

Permalink
https://escholarship.org/uc/item/0rs79692

Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)

Author
Korolkova, Olga

Publication Date
2014-01-01
Peer reviewed

eScholarship.org

Powered by the California Digital Library
University of California

Categorical Perception of Facial Expressions Is Not a Homogeneous Effect
Olga A. Korolkova (olga.a.kor@gmail.com)
Moscow State University of Psychology and Education, 29 Sretenka Street, Moscow, 127051 Russia
Moscow Institute of Psychoanalysis, 34 Kutuzovsky Avenue, Moscow, 119334 Russia

1992), and later also on gray-scale images (Calder et al.,
1996; de Gelder et al., 1997; Young et al., 1997). Its specific
aspects were revealed in children (Cheal, Rutherford, 2011;
Kotsoni et al., 2001), in patients with amygdala damage
(Adolphs et al., 1999), schizophrenia (Kee et al., 2006) and
autism spectrum disorders (Teunisse, de Gelder, 2001).
Although, several studies did not show the CP (Fiorentini,
Viviani, 2009; Schiano et al., 2004) or reported the factors
reducing this effect (namely, verbal interference: Roberson,
Davidoff, 2000; Roberson et al., 2007).
The concept of CP implies that there is a number of
discrete emotional categories with qualitative differences
between each other, and that each entity of facial emotion
can be placed by an observer into one of these alternative
emotional categories. Another way of describing emotion
and facial expression are continuous spatial models, where
the perceived differences between faces are measured as
distances in a face-space (Valentine, 1991), or the diversity
of emotions is located in a multi-dimensional space with the
axes corresponding to physiological properties such as
arousal, sleepiness or valence (Russell, 1980). A possible
solution of the long-standing debate between categorical
(discrete) and dimensional (continuous) understanding of
emotion and face perception is by incorporating the both
notions into consolidated hierarchical model of higher-level
categorical system that interacts with low-level perceptual,
non-categorical one (e.g., Fujimura et al., 2012; Roberson et
al., 2010).
In line with the discrete emotions theory, at least seven
basic emotions, universal among cultures, were revealed by
P. Ekman (Ekman, Friesen, 1976), and 21 emotional
continua, representing transitions from one basic emotion to
another, can be constructed between each pair of emotions
to test the categorical perception hypothesis. Although, the
majority of the facial expressions CP studies explored the
perception and differentiation of face images belonging to
only a few of these emotional continua. In particular, CP has
been shown on continua between happiness and six other
emotional expressions, but results on CP of happiness /
surprise and happiness / sadness are controversial (see
Calder et al., 1996; de Gelder et al., 1997; Etcoff, Magee,
1992). The continua most frequently used in studies are
those between anger, sadness, fear and happiness. Though,
even on these expressions, the results are ambiguous and do
not always show the CP (Calder et al., 1996; de Gelder et
al., 1997; Fiorentini, Viviani, 2009). Other continua were
rarely included in the stimuli for discrimination task, and for
8 out of 21 continua, no experimental data is available.
An extensive study of facial expressions CP was
conducted by Young et al. (1997) and included all possible

Abstract
We studied the categorical perception on transitions between
seven basic emotional facial expressions and explored the
influencing factors. In Experiment 1, participants performed a
multiple-choice emotion labeling task while observing basic
or morphed (blended between a pair of basic emotions) facial
expressions. In Experiment 2, other participants completed
AB-X discrimination task. They observed pairs of images
adjacent in a morphing continuum, and matched the test
image to one of the pair. The results of Experiment 1 revealed
influence of emotional context, formed by the presented
expressions, on perception of surprise, anger, disgust, and
neutral face. The “categorical field” of morphed expressions
includes not only the two relevant emotions (morphing basis)
but a number of additional ones. Based on the data of
Experiment 1, we selected the pairs of stimuli crossing the
categorical boundary, and pairs falling within the category, to
predict the discriminability obtained it the Experiment 2. A
generalized linear mixed model was fitted to the data. We
show the main effect of within/between category pair, type of
continuum and continuum/category interaction on the
probability of correct discrimination. Overall, our results
showed the categorical perception, but its strength depends on
particular pair of emotional categories.
Keywords: face, facial expression, emotions, categorical
perception, generalized linear model.

Introduction
When we want to test whether the perception of a particular
class of objects is categorical, we generally compare the
ability to perceive physical differences in pairs of objects
belonging to the same and to different categories. Assuming
that these differences are equal, the enhanced
discriminability in cross-boundary pairs compared to
within-category pairs reflects the categorical perception
(CP). If categories do not influence the discriminability, we
refer to continuously perceived differences.
The classic experimental paradigm for testing the CP was
proposed by Liberman et al. (1957) in the studies on
phonemes perception. It included an identification (labeling)
task aimed at defining the boundary between categories, and
a discrimination task, in which subjects were asked to
differentiate objects in both cross-boundary and withincategory pairs.
Since then, the CP has been extensively studied on a
variety of objects, including facial emotional expressions
(Calder et al., 1996; Cheal, Rutherford, 2011; Etcoff, Magee,
1992; Fugate, 2013; de Gelder et al., 1997; Herba et al., 2007;
McCullough, Emmorey, 2009; Roberson et al., 2007;
Roberson, Davidoff, 2000; Suzuki et al., 2005; Teunisse, de
Gelder, 2001; Young et al., 1997; etc.). CP has been shown
first on faces presented as linear drawings (Etcoff, Magee,

767

21 continua between pairs of seven basic emotions, but only
in the labeling (alternative forced-choice) task. In the study
of expression discrimination, 6 continua (happiness / surprise
/ fear / sadness / disgust / anger / happiness) were used, and
significant low to moderate correlations were found between
the discrimination results and the predicted discriminability
which was based on identification task results.
Thus, the results obtained up to date, suggest that the CP
can vary in its degree depending on a particular continuum
between pair of basic emotional expressions. However, this
issue has not been the primary focus of any research yet. To
investigate, whether CP depends on the type of emotional
continua, and in what continua the CP does not appear at all,
we constructed 21 morphing transitions between all pairs of
7 basic emotional facial expressions and carried out two
experiments, identification and discrimination tasks, using
the whole range of the basic emotional expressions. The
procedure of identification task was changed compared to
the study by Young et al. (1997) to extend the previously
obtained results. The discrimination task procedure was also
slightly modified, and the stimuli range was substantially
broadened to include all transitions between basic emotions.
To analyze the results, we propose using a generalized
linear mixed model, which is generalization of logistic
regression and allows to correctly modeling categorical data
dependence on various predictors. Mixed logistic regression
(multi-level logit modeling) is preferred method of analysis
over traditional ANOVA (Jaeger, 2008) as it allows
accounting for multiple fixed and random effects
simultaneously. This method of discrimination data analysis
can be helpful in revealing the influence of continua type, as
well as any other significant predictors of the CP.

Twenty-one emotional continua were divided into seven
stimuli sets so that each set included three separate
transitions organized into a meta-continuum in the form:
Emotion1→Emotion2→Emotion3→Emotion4, where “→”
stands for sequence of 4 intermediate morphs. The inner
structure of these “meta-continua” is similar to the structure
of the transitions used in earlier studies, for example,
between artificial phonemes /ba/, /da/ and /ga/ in the study
by Liberman et al. (1957), where the stimuli with linear
change in voice onset time were perceived as a sequence of
distinct phonetic categories. In our study, each of the seven
meta-continua served as stimuli in two separate
experimental series.

Experiment 1: Identification
Participants were 138 university students (median age 21
years, range 18 to 53 years; 41 males) with normal or
corrected-to-normal vision. They participated in the study
for credits. Each subject performed the identification task
with one of the seven stimuli sets, thus, 18 to 23 subjects
worked with each set.
Procedure Each of 16 images from a meta-continuum was
presented using PXLab software (Irtel, 2007) to a subject
for 3 seconds in the center of 85 Hz CRT display (visual
angle of 6.7°×9.3°), followed by invitation to choose one or
several out of seven basic emotions that were expressed on
the face. Each image was presented twice; the order of
presentation was randomized. The trial sequence is shown
on Figure 1, a. The participants were not instructed to
answer as quickly as possible, but to be attentive to the
images presented to them and to give accurate responses.

Method
The aims of the present study were to explore the CP of
facial expressions and the factors that influence the
discriminability of morphed faces from the same or different
emotional categories, and to fit a model to our data.

Stimuli
We used computer morphing to obtain equal distance
transitions between seven basic emotions of actor JJ
(Ekman, Friesen, 1976), as proposed by P. Ekman:
happiness, surprise, fear, sadness, disgust, anger, and neutral
face. Before applying the morphing procedure, the grayscale images (245×350 pixels) were corrected for mean
brightness, size and rotation. Then up to 300 reference
points were placed onto each image, and 21 morphing
continua were produced for each pair of the initial images.
We then hid all non-facial features of the images under a
black mask, and selected two basic and 4 morphed images
(20%, 40%, 60% and 80% impact of one of the two basic
emotions) from each continuum, so that the formal
differences between the adjacent images remained the same.
We estimated the inter-image differences as dot products of
vectors composed of the two images’ pixels brightness
levels.

Figure 1: a) Identification task trial. Sixteen images
belonging to one of meta-continua were presented in
random order, each image twice. The task was to match
one or several emotion labels to the presented
expression; b) Discrimination task trial. A pair of
adjacent images from a meta-continuum was presented
side-by-side, preceded by fixation cross. Then they were
masked by black-and-white noise pattern, and the test
image appeared, equal to one of the pair. The task was
to match the test image to one from the pair.

768

As same basic expressions were included into several
stimuli sets, we are able to assess possible influences of the
overall emotional context to their perception and
identification. Using Fisher’s exact test, we compared the
labeling proportions for the same images presented in
different settings. The identification of happy, sad, and
feared expressions in different contexts did not change. The
surprise expression was significantly more frequently
labeled as fear when presented in absence of fear expression
in the experiment series (Fisher’s exact test p < 0.04). The
same pattern was observed for anger expression labeled as
surprise or fear without these two expressions in the
emotional context (Fisher’s exact test p < 0.04). In the same
manner, disgust expression was confused with anger and
sadness (Fisher’s exact test p < 0.039), and neutral
expression – with happiness and disgust (Fisher’s exact test
p < 0.042).
Next, we analyzed the emotional labeling of intermediate
morphs, depending on their position in a particular
continuum. In several continua, intermediate images were
mainly identified as one of the two basic emotions
corresponding to those that form the continuum. Morphs
were labeled as other emotions in less than 20% of trials.
This was true for happiness / surprise, sadness / neutral,
surprise / fear, disgust / happiness, sadness / disgust, neutral /
surprise, surprise / disgust, and neutral / happiness continua.
In all of them, the categorical boundary is defined as the
point of cross of two emotion functions. In the other
continua, the intermediate morphs in the center of
continuum were identified as other emotions in up to 40%
of trials. Intermediate fear / sadness and sadness / anger
morphs were confused with disgust; fear / disgust morphs –
with sadness, and neutral / fear ones – with surprise. As this
occurs in the absence of the “misidentified” emotions in the
overall context, we determined the categorical boundary in
such continua as the point of crossing for the two relevant
emotion functions.

Experiment 2: Discrimination
Participants were 140 university students (median age 20
years, range 17 to 47 years; 40 males) with normal or
corrected-to-normal vision, none of them performed the
identification task. Twenty of the subjects worked with each
of 7 stimuli sets, similarly to the identification task.
Procedure We used AB-X discrimination task to estimate
the perceived differences between images of facial
expressions. Sixteen stimuli from a meta-continuum were
organized in 15 pairs of adjacent images. In each trial, a
fixation cross was presented for 600 ms, followed by the
stimuli pair simultaneously presented side-by-side for 1500
ms (each image occupied visual angle of 6.7°×9.3°, the
distance between them was 2.3°), and a black-and-white
noise pattern (400 ms, 17.5°×9.3°). After that, the test
stimulus, being one of the pair, appeared in the center of the
screen for 1500 ms, and the subject was prompted to answer
using the keyboard, which of the two images in the pair
corresponded to the test stimulus. Each pair of images was
presented 20 times in four possible orders of stimuli in the
AB-X triad. The trial sequence is shown on Figure 1, b. All
300 trials in the discrimination task were randomized. A
short training was provided before the main experiment to
allow subjects to familiarize with the task. Images from the
training series were not shown in the following experiment.
The simultaneous presentation of the AB pair allows
reducing the known sequence, or memory, effects, namely,
better performance for sequentially presented stimuli ABB
compared to ABA.

Results
Emotions identification

proportions of identification

The results of the Experiment 1 are proportions of choosing
each of seven emotion labels as matching the presented
expressions (see Figure 2).

Figure 2: Identification task results. The proportions of choosing each of seven emotion labels are plotted against
stimuli from seven meta-continua. Basic, non-morphed, images are indicated with labels: ha – happiness; fe – fear; di –
disgust; ne – neutral; an – anger; su – surprise; sa – sadness. Intermediate, morphed, images are placed between the
corresponding basic ones. Panels are organized by stimuli sets.

769

Discrimination of facial expressions

probability of correct discrimination

The results obtained from the Experiment 2 are
dichotomous correct / incorrect discriminations. We fit a
generalized linear mixed model to the data, using R (R Core
Team, 2013) package lme4 (Bates, 2010; Bates et al., 2013).
The input data for fitting the model has the following
structure. The Response variable comprises two levels
(correct / incorrect). The covariates, possibly influencing the
response, include Category (3 levels: pair of stimuli
crossing the boundary; those containing one basic (nonmorphed) image and one 20% morph, both being within the
same category; intermediate within-category pairs including
only morphs), Continuum types (21 levels), arranged in
stimuli Sets (7 levels), Age, Sex and unique ID of
participants, unique index of stimuli Pair across all continua
(105 levels), and the Order of images in an AB-X trial (4
levels). The levels of Category covariate were determined
on the basis of the Experiment 1 results. We consider a pair
of images as crossing the categorical boundary if in the
identification task, the emotion identification functions cross
between these two images or at one of them. All the other
pairs, except for those including a non-morphed emotion,
are considered intermediate. We excluded the intermediate
pairs from the further analysis and compared only crossboundary and basic pairs. Thus, the remaining dataset
consisted of 27600 observations.
We started fitting the model from including all covariates:
Category, Continuum, Category × Continuum interaction
and Sex as fixed factors; stimuli Order and Set, and
participants’ ID and Age as random effects. Then we refitted
the model in several steps, each time excluding one of the
factors. The initial and refitted models were compared using
likelihood ratio test (LRT) statistic, –2 (l1–l0), where l1 and
l0 are maximized log-likelihood of the compared models
with (l1) and without (l0) the factor excluded. To assess,
whether the contribution of a factor is significant to fitting
the model, we use χ2 test with degrees of freedom equal to
the number of levels of the excluded parameter.
The random effects of stimuli Set [Pearson’s χ2(1) = 0, p =
0.996; SD = 0.000015] and participants’ Age [χ2(1) = 0, p =
1; SD = 0.00011] in the initial model did not change
significantly the model fit, but excluding Order [χ2(1) =
67.403, p < 0.001; SD = 0.118], and participants’ ID [χ2(1)
= 280.72, p < 0.001; SD = 0.301], resulted in considerably
lower fit.
The fixed effect of participants’ Sex did not contribute to
the model’s fit [χ2(1) = 0.0004, p = 0.984], so we excluded it
as not influencing the result. The fixed effects of Category
[χ2(1) = 224.2, p < 0.001], Continuum [χ2(20) = 279.02, p <
0.001], and Category × Continuum interaction [χ2(20) =
94.953, p < 0.001] were highly significant.
The probabilities of correct discrimination in pairs of
adjacent images, computed from the final model, are
presented on Figure 3. The computed probabilities show
high correlation with the mean values for raw data [r =
0.9998, p < 0.0001], therefore, we can conclude that the final
model describes the discrimination data rather well.

Figure 3: Probability of correct discrimination in pairs
of images that cross the categorical boundary (between)
and include basic emotion image (within). The
probabilities are computed using the fixed effects of the
fitted model. The continua are named by the
corresponding basic emotions: Ha – happiness; Fe –
fear; Di – disgust; Ne – neutral; An – anger; Su –
surprise; Sa – sadness.

Discussion
The two experiments described in the present paper
accounted for categorical perception of emotional facial
expressions. This phenomenon has been of interest for many
researchers over the last 20 years, but the issue whether we
perceive emotions categorically or continuously is not fully
resolved yet. In our study, we tested the hypothesis of CP
dependence on emotion continua type. We implemented
modified procedures and advanced method of analysis to
our experimental data.
In Experiment 1, the multiple-choice labeling task let us
to obtain not only matching of the basic and morphed
images with one most relevant emotion category, but to
estimate broad “category fields” of facial expressions, and
therefore, explore the structure of possible systematic

770

“misidentifications” of emotional expressions.
The results of the identification task revealed strong
context-based effects for expressions of surprise, anger,
disgust and neutral face. When we do not present the full
range of the emotions in the same experimental series, these
four expressions can be “confused” with those not being
presented. In classical model (Liberman et al., 1957), the CP
is considered to be independent of any contextual
interference, although, such influences in fact can be
prominent. More recently Treisman et al. (1995) proposed a
criterion setting theory, according to which, the subjective
criterion of signal detection is reset in each new
experimental trial, and, therefore, depends on short-term
influence of the previous trials (the two mechanisms of
criterion setting are its stabilization – a shift towards the
stimulus value after each trial, and continuous assessment of
signal probability causes shift in opposite direction), as well
as on long-term overall context influence and individual
experience.
Global context effects on perception of objects from
different categories and within the same category are also
incorporated into category adjustment model (Huttenlocher
et al., 2000), in which categories are described in terms of
prototypes theory, and pertinence of each object to each
category is probabilistic. According this model, the global
aim of an observer is to increase mean accuracy of
judgments by combining the information about a particular
stimulus with information about its category prototype
stored in long-term memory. If the former is incomplete, it
can be partially of entirely substituted by the latter. The
model was tested in experiment with facial expression
differentiation (Roberson et al., 2007). Our identification
data is in line with this model, as our subjects were probably
adjusting their judgments to the full range of basic emotion
categories presented to them as possible labels for the
restricted stimuli set, and, therefore, were more sensitive to,
say, subtle disgust cues in ambiguous morphed images
between sadness and anger, if they were not shown the
intense disgust expression.
The context adaptation can be avoided if one would
present the whole range of facial expressions in the same
experimental series. This paradigm was used in Experiments
1 and 2 by Young et al. (1997), with alternative choice of
one out of 6 or 7 basic emotions labels. Though, even in this
study, pure surprise, sadness / anger and anger / surprise
morphs were sometimes confused with fear; and fear /
disgusts morphs – with anger of sadness. Another effect
revealed in our study is inclusion of additional categories in
the center of continuum, between the two main categories
(surprise identified in intermediate morphs between neutral
and feared face; disgust – between sadness and anger or
between fear and sadness; sadness between fear and
disgust). It is also similar to Young et al. (1997) results and
cannot be fully explained by context adaptation or category
adjustments. We propose to explain this issue using the
concept of “category field” of facial expression, which
include the core emotion – the center of category – and a

number of additional emotions that form the periphery, with
probabilistic identification of the face images with these
emotions. If an expression is far from category center, it can
be associated with several adjacent categories. To be so, a
direct categorical boundary should exist between each pair
of confused categories. The existence of such direct
boundaries is to be verified in future research.
Based on the results of Experiment 1, we chose the pairs
of adjacent stimuli that cross the categorical boundaries, and
presented them along with other pairs in parallel AB-X
discrimination task in Experiment 2. Compared to other
studies of facial expressions discrimination, the present one
included transitions between all possible basic emotions,
and along with substantial amount of individual raw data, it
allowed us to construct an integral model describing the
factors that underlie the categorical perception.
In our discrimination data modeling, the influence of
stimuli Set (the context effect), though, was not significant.
Rather, a multi-level logit modeling of the data showed
main effects of within- versus between-category position of
equidistant stimuli pairs; influence of the type of continua,
and the interaction of the two factors, on the proportions of
correct discrimination of the images. We therefore obtained
the categorical perception of facial expressions found
previously in many studies.
Moreover, according to our results, the CP of facial
emotional expressions indeed has different intensity,
depending on the pair of emotional categories being
considered. In particular, there were no large differences in
discrimination of within- and between-category expressions
in happy / neutral, happy / feared, happy / angry, happy / sad,
surprised / neutral, sad / disgusted, angry / neutral, feared /
angry continua. On the contrary, in other continua (happy /
disgusted, happy / surprised, neutral / disgusted, feared / sad,
feared / disgusted, feared / neutral, surprised / sad, surprised /
angry, surprised / feared, surprised / disgusted, sad / angry,
sad / neutral, disgusted / angry) the CP effect is pronounced.
Possible explanations of such pattern may include: initial
difference of particular emotion categories in their affective
or perceptive power; perception of other, probably nonemotional, but rather conversational meaning in
intermediate morphed images; or relying on low-level
configural cues of the stimuli (opened or closed mouth,
morphing artifacts, etc.). Further studies should reveal the
right one.
In conclusion, the present study provided new data of
emotional labeling and discrimination of facial expressions,
contributed to better understanding the categorical
perception of emotional faces. We revealed dependence of
this well-documented and previously considered as
homogeneous effect on the type of emotional categories.
Finally, we proposed using the “category field” concept to
describe the results of emotional misidentification and to
discuss them not as accidental errors of perception, but as
systematical “adjustments” or “re-calibration” of categorical
structure of emotions depending on particular context.

771

Kee, K. S., Horan, W. P., Wynn, J. K., Mintz, J., & Green,
M. F. (2006). An analysis of categorical perception of
facial emotion in schizophrenia. Schizophrenia Research,
87, 228–237.
Kotsoni, E., de Haan, M., & Johnson, M. H. (2001).
Categorical perception of facial expressions by 7–monthold infants. Perception, 30, 1115–1125.
Liberman, A. M., Harris, K. S., Hoffman, H. S., & Griffith,
B. C. (1957). The discrimination of speech sounds within
and across phoneme boundaries. Journal of Experimental
Psychology, 54 (5), 358–368.
McCullough, S., & Emmorey, K. (2009). Categorical
perception of affective and linguistic facial expressions.
Cognition, 110 (2), 208–221.
R Core Team (2013). R: A language and environment for
statistical computing. R Foundation for Statistical
Computing, Vienna, Austria. URL: http://www.Rproject.org/.
Roberson, D., & Davidoff, J. (2000). The categorical
perception of colours and facial expressions: The effect of
verbal interference. Memory & Cognition, 28, 977–986.
Roberson, D., Damjanovic, L. & Kikutani, M. (2010) Show
and tell: the role of language in categorizing facial
expression of emotion. Emotion Review. 2, 255–260.
Roberson, D., Damjanovic, L., & Pilling, M. (2007).
Categorical Perception of Facial Expressions: Evidence
for a 'Category Adjustment' model. Memory & Cognition,
35, 1814–1829.
Russell, J. A. (1980). A circumplex model of affect. Journal
of Personality and Social Psychology, 39, 1161–1178.
Schiano, D.J., Ehrlich, S.M., & Sheridan, K. (2004).
Categorical imperative not: facial affect is perceived
continuously. CHI 2004 (pp. 49–56). N.Y.: ACM.
Suzuki, A., Shibui, S., & Shigemasu K. (2005).Temporal
characteristics of categorical perception of emotional
facial expressions. Proceedings of the Twenty-Sixth
Annual Conference of the Cognitive Science Society (pp.
1303–1308). Hillsdale, NJ: Lawrence Erlbaum
Associates.
Teunisse, J. P., & de Gelder, B. (2001).Impaired categorical
perception of facial expressions in high-functioning
adolescents with autism. Child Neuropsychology, 7 (1),
1–14.
Treisman, M., Faulkner, A., Naish, P.L., & Rosner, B.S.
(1995). Voice-onset time and tone-onset time: the role of
criterion-setting mechanisms in categorical perception.
The Quarterly Journal of Experimental Psychology A, 48
(2), 334–366.
Valentine, T. (1991). A unified account of the effects of
distinctiveness, inversion and race in face recognition.
Quarterly Journal of Experimental Psychology, 43A,
161–204.
Young, A., Rowland, D., Calder, A., Etcoff, N., Seth, A., &
Perrett, D. (1997). Facial expression megamix. Cognition,
63, 271–313.

Acknowledgments
The study was supported by Russian Foundadtion for
Humanities, project № 12-36-01257a2 “Features of
perception and perceptual space of facial expressions” to
Olga A. Korolkova.

References
Adolphs, R., Tranel, D., Hamann, S., Young, A. W., Calder,
A. J., Phelps, E. A., Anderson, A., Lee, G. P., & Damasio,
A. R. (1999). Recognition of facial emotion in nine
individuals
with
bilateral
amygdala
damage.
Neuropsychologia, 37, 1111–1117.
Bates, D. M. (2010). lme4: Mixed-effects modeling with R.
Springer.
Bates, D. M., Maechler, M., Bolker, B., & Walker, S.
(2013). lme4: Linear mixed-effects models using Eigen
and S4. R package version 1.0-5.URL: http://CRAN.Rproject.org/package=lme4.
Calder, A., Young, A., Perrett, D., Etcoff, N., & Rowland,
D. (1996). Categorical perception of morphed facial
expressions. Visual Cognition, 3, 81–117.
Cheal, J. L., & Rutherford, M. D. (2011). Categorical
perception of emotional facial expressions in
preschoolers. Journal of Experimental Child Psychology,
110 (3), 434–443.
de Gelder, B., Teunisse, J. P., & Benson, P. J. (1997).
Categorical perception of facial expressions: categories
and their internal structure. Cognition and Emotion, 11, 1,
1–23.
Ekman, P., & Friesen, W. V. (1976). Pictures of facial
affect. Palo Alto, CA: Consulting Psychologists Press.
Etcoff, N. L., & Magee, J. J. (1992). Categorical perception
of facial expressions. Cognition, 44, 281–295.
Fiorentini, C., & Viviani, P. (2009). Perceiving facial
expressions. Visual Cognition, 17, 373–411.
Fugate, J. M. B. (2013).Categorical Perception for
Emotional Faces. Emotion Review, 5, 84.
Fujimura, T., Matsuda, Y., Katahira, K., Okada, M., &
Okanoya, K. (2012). Categorical and dimensional
perceptions in decoding emotional facial expressions.
Cognition & Emotion, 26 (4), 587–601.
Herba, C. M., Heining, M., Young. A. W., Browning, M.,
Benson, P. J., Phillips, M. L., & Gray, J. A. (2007).
Conscious and nonconscious discrimination of facial
expressions. Visual Cognition, 15, 1, 36–47.
Huttenlocher, J., Hedges, L. V., & Vevea, J. L. (2000). Why
do categories affect stimulus judgment? Journal of
Experimental Psychology: General, 129, 220–241.
Irtel, H. (2007). PXLab: The Psychological Experiments
Laboratory [online]. Version 2.1.11. Mannheim
(Germany):
University
of
Mannheim.
URL:
http://www.pxlab.de.
Jaeger, T. F. (2008). Categorical data analysis: Away from
ANOVAs (transformation or not) and towards logit mixed
models. Journal of Memory and Language, 59 (4), 434–
446.

772

