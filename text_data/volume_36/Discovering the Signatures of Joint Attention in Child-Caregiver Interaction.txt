UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Discovering the Signatures of Joint Attention in Child-Caregiver Interaction
Permalink
https://escholarship.org/uc/item/8z45f7cw
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Pusiol, Guido
Soriano, Laura
Frank, Michael C.
et al.
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                    Powered by the California Digital Library
                                                                      University of California

       Discovering the Signatures of Joint Attention in Child-Caregiver Interaction
          Guido Pusiol                         Laura Soriano                        Li Fei-Fei                    Michael C. Frank
  guido@cs.stanford.edu                  lsoriano@stanford.edu             feifeili@stanford.edu              mcfrank@stanford.edu
Department of Computer Science Department of Psychology Department of Computer Science Department of Psychology
  Department of Psychology                  Stanford University                Stanford University               Stanford University
       Stanford University
                                Abstract                                  members attention to an object, (2) both members focus vi-
    Joint attention—when child and caregiver share attention to           sually on the object, and (3) the child indicates awareness of
    an object or location—is an important part of early language          the caregiver (Tomasello & Farrar, 1986).
    learning. Identifying when two people are in joint attention is          Previous work has used children’s gaze as the main indi-
    an important practical question for analyzing large-scale video
    datasets; in addition, identifying reliable cues to joint atten-      cator of JA, but, from the perspective of both the child and
    tion may provide insights into how children accomplish this           the data analyst, this method has several issues. First, gaze is
    feat. We use techniques from computer vision to identify fea-         neither necessary nor sufficient for JA. It is possible to attend
    tures related to joint attention from both egocentric and fixed-
    camera videos of children and caregiver interacting with ob-          jointly through the hands—as with a child reading a picture-
    jects. We find that the presence of caregivers’ faces in the          book on a parent’s lap—or for the child to follow gaze to a
    child’s egocentric view and the motion of objects in the fixed        distal target and then signal awareness by moving towards it
    camera both correlate with human-annotated joint attention.
    We use a classifier to predict joint attention using these fea-       or reaching for it (Yu & Smith, 2013). Indeed, eye-tracking
    tures and find some initial success; in addition, classifier per-     studies investigating signals to reference find that manual sig-
    formance is substantially increased by interpolating features         nals are far more effective than gaze in manipulating young
    across automatically-extracted “attention chunks” in the ego-
    centric video.                                                        children’s attention (Yurovsky, Wade, & Frank, 2013). Sec-
    Keywords: Joint attention; computer vision; child develop-            ond, young children may not have perceptual access to their
    ment; social cognition.                                               caregiver’s gaze most of the time. Recent studies using head-
                                                                          mounted cameras and eye-trackers suggest that children are
                            Introduction                                  more often looking at the objects in front of them than at the
 How do young children begin learning the meanings of                     faces of their caregivers (L. B. Smith, Yu, & Pereira, 2011;
 words? Across cultures, early vocabulary includes names for              Franchak, Kretch, Soska, & Adolph, 2011; Frank, Simmons,
 people, simple social routines, animals, and objects (Tardif             Yurovsky, & Pusiol, 2013). Third, parents most often look at
 et al., 2008), suggesting that the earliest words are learned            their children, not at the object they are talking about (Frank,
 through interaction and play with others (Bruner, 1985).                 Tenenbaum, & Fernald, 2013). Thus, gaze alone is at best a
 Identifying a caregiver’s intended referent is a critical part of        noisy cue for the identification of JA, either for the child or
 learning meaning within these interactions, and this identifi-           for the researcher attempting to identify JA in a large dataset.
 cation is often accomplished through joint attention.                       The goal of our current work is to discover other signals of
    Joint attention describes the situation when both child and           joint attention. There are two purposes to this investigation.
 caregiver are attending to the same thing and when both know             The first is data analytic: A better understanding of how to
 that the other is attending to it. For the remainder of the paper        extract JA episodes from video could be a powerful tool for
 we will talk informally about joint attention—JA—as both                 analyzing large video corpora. The second is psychological:
 the phenomenon and the period of time during which it hap-               The unsupervised extraction of JA episodes from video could
 pens (Carpenter & Liebal, 2011). A typical example of JA                 give hints regarding robust cues that children might use in
 is a situation where an adult and child are playing with a toy           addition to, or even in lieu of, gaze.
 and the infant alternates gaze between the adult and the toy                We use two data sources to gain information about the so-
 (Carpenter, Nagell, & Tomasello, 1998).                                  cial interaction between child and caregiver: head-mounted
    The capacity for JA gradually develops over the first two             and fixed camera videos. Our approach is unsupervised dis-
 years of life and usually begins to emerge between 9 and                 covery. We hypothesized that the most effective strategy for
 12 months of age, coinciding with the beginnings of lan-                 capturing JA would be the extraction of high-level, seman-
 guage learning. In addition, both the skills that enable JA              tic features that correspond relatively closely to the kinds of
 (e.g. pointing, following a caregiver’s gaze to a distal target)         constructs described in prior work manually coding joint at-
 and the amount of time that children spend in JA with their              tention (e.g. Tomasello & Farrar, 1986). Of course, the chal-
 caregivers are strong predictors of children’s early vocabulary          lenge is that many such features can be extremely difficult to
 growth (Carpenter et al., 1998; Brooks & Meltzoff, 2008).                extract in an automated fashion. To compromise, we identi-
    How do children know that they are in joint attention with            fied three features that we could extract with relatively high
 a caregiver? From an external perspective, joint attention has           accuracy in an automated fashion: (1) caregivers’ faces in
 typically been defined by a sequence of events: (1) one mem-             the egocentric camera, (2) objects that were in motion due
 ber of the interaction (child or caregiver) directs the other            to being actively manipulated, and (3) periods of time during
                                                                      2805

                                                                    Annotation of Joint Attention
                                                                    We used the DataVyu software package (Adolph, Gilmore,
                                                                    Freeman, Sanderson, & Millman, 2012) to annotate periods
                                                                    of time during which child and caregiver were in joint atten-
                                                                    tion. Joint attention was defined if it satisfied the criteria
                                                                    given by (Tomasello & Farrar, 1986). First, the interaction
                                                                    had to begin with either parent or child initiating. For exam-
                                                                    ple, a parent could hold up an object and label it, or a child
                                                                    could bring an object over the parent. Second, both members
                                                                    were required to focus on the object in JA for at least 3 sec-
                                                                    onds; we allowed this period to include brief glances away.
                                                                    Third, at some point during the interaction the child was re-
                                                                    quired to display some overt behavior towards the parent to
                                                                    show that he or she acknowledged the interaction.
Figure 1: An example of our synchronized dataset: The left
                                                                                  Defining Semantic Features
side of each panel shows the egocentric video, while the right
side shows the motion-filtered 3rd person video. The rectan-        We describe automatic and semi-automatic methods for cre-
gle in the middle of the egocentric camera shows the attention      ating high-level semantic features capturing caregivers’ faces
chunk tracker, while the label “zem” indicates that the object      and episodes of static attention (“attention chunks”) from
detector has found the yellow feather duster in the 3rd person      egocentric video and moving objects from the third-person
video.                                                              video.
                                                                    Face Detection
which the child’s attention was relatively static. We hypothe-      Traditional off-the-shelf face detection algorithms (e.g. Vi-
sized that each might have some relationship to JA.                 ola & Jones, 2001; Zhu & Ramanan, 2012) perform poorly
                                                                    at detecting parent faces in the kinds of egocentric video that
   The plan of the paper is as follows. We begin by describ-
                                                                    we collected. Face detectors work accurately when the test
ing our dataset, and then we describe how we use compu-
                                                                    dataset has low variance from the training dataset and the
tational methods to extract semantic features from these data.
                                                                    distance between the camera and the face is >1 meter (e.g.
We then examine the correlations between these features (and
                                                                    Facebook-style pictures). From the egocentric perspective,
higher-level clusters of these features) and hand-coded joint
                                                                    however, many other face configurations are prevalent. Faces
attentional episodes. Our results suggest that there are a num-
                                                                    appear partially occluded or cropped, blurred by motion, and
ber of redundant perceptual cues to JA, and that some of these
                                                                    with large size and texture variability making detecting them
may be more readily accessible to children than gaze. In fu-
                                                                    very challenging (Figure 2).
ture work, some of these cues could form a robust basis for
                                                                       We addressed the problem using a semi-automated adap-
the automatic detection of joint attentional episodes.
                                                                    tive algorithm (Kalal, Mikolajczyk, & Matas, 2012) that
                                                                    makes use of minimal user input for initialization (selecting
                           Dataset                                  one example face per video). The algorithm uses new pixel
                                                                    patches in the trajectory of an optical-flow based tracker to
Videos                                                              train and update a face detector. The optical flow tracker and
                                                                    the face detector work in parallel. If the face detector finds
We make use of a dataset of in-lab caregiver-child play ses-
                                                                    a location in a new frame exhibiting a high similarity to its
sions initially described in Frank, Simmons, et al. (2013).
                                                                    stored template, the tracker is re-initialised on that location.
In this dataset, parents were invited to play one-on-one with
                                                                    Otherwise, the tracker uses the optical flow to decide the lo-
their children on the floor of a friendly, colorful room. The
                                                                    cation of a face in the new frame.
children wore a small head-mounted (egocentric) camera that
                                                                       The primary advantage of the algorithm is the use of mo-
captured their approximate visual experience, and a tripod-
                                                                    tion for face detection: Following the movement of the pixels
mounted camera captured the third-person perspective from
                                                                    that define a face it is possible for the algorithm to adapt to
one corner of the room. Child and caregiver played with a
                                                                    new morphologies (i.e. different face poses). More broadly,
set of toys organized into pairs, with each pair containing a
                                                                    this method allows for a face that is partially occluded or
known object (e.g. a ball) and a novel object (e.g. a yel-
                                                                    poorly lit to be tagged as a face by virtue of its relationship
low feather duster). The novel objects were clearly labeled
                                                                    with previous frames where the face information was clearer.
so that parents knew what to call them (e.g. the duster was a
“zem”). For purposes of the current study, we chose a set of        Evaluation As part of an ongoing study following Frank,
nine videos containing five eight-month-old children and four       Simmons, et al. (2013), we evaluated this face detector using
sixteen-month-old children.                                         a set of 37 egocentric videos gathered in the circumstances
                                                                2806

                                                                                             0
                                                                                         10
                                                                                                                               8 months
                                                                                                                               12 months
                                                                                             −1                                16 months
                                                                                         10
                                                                      Proportion fixations
                                                                                             −2
                                                                                         10
  (a) Motion blur    (b) Partial occlusion   (c) Low texture
                                                                                             −3
                                                                                         10
Figure 2: Three examples of challenging faces for traditional
detectors.                                                                                   −4
                                                                                         10
                                                                                             −5
described above (with ages ranging from 8 – 16 months).                                  10
Our evaluation compares automatically detected faces with
human ground truth annotations over a sample of both high                                    −6
                                                                                         10           −1           0                 1
face-density and randomly selected frames. We found that                                         10           10                10
                                                                                                           Fixation time (s)
our algorithm had precision of .90 and recall of .86, achieving
a relatively high level of accuracy in this challenging dataset.
Detecting Episodes of Static Attention                                  Figure 3: The binned distribution of attention chunk lengths
                                                                        for 8-, 12-, and 16-month-old children. Fixation time and
One important aspect of joint attention is that it should be            proportion are both plotted on a log scale, because most fixa-
(relatively) static if the child is focused on a single ob-             tions are very short. The younger (8 month old) children show
ject. Congruent with that, previous work has found that                 longer attention fixation episodes compared to the other two
episodes where a single object dominates the field of view              groups. A very small number of chunks longer than 10s are
(and hence the view field is static) are predictive of word             not shown.
learning (L. B. Smith et al., 2011; Pereira, Smith, & Yu,
2013). We attempted to identify such moments of fixed at-
tention (“attention chunks”) in an automated way. Our strat-
                                                                        Detection and Tracking of Moving Objects
egy is to track a large-scale region of the video (e.g. back-
ground texture) across frames; if this texture remains in a rel-        As described in our earlier work, the vertical field of view of
atively static location, we can infer that the child’s head has         the head-mounted camera is relatively limited (∼ 40◦ visual
not moved significantly. If the texture deforms substantially,          angle). Thus, to be able to capture faces high in the visual
then the head is likely to be in motion (W. Smith, 2010). This          field, the camera must be at a relatively high angle; this angle
approach is supported by prior experimental work indicating             in turn precludes capturing the objects that the child is hold-
that eye gaze and head pose are typically coupled (Yoshida &            ing. Because of this, we made use of the 3rd person static
Smith, 2008).                                                           video to detect the objects that were being handled by the
   The algorithm is initialized by modeling a pixel-texture             child and the caregiver.
patch (Pi ). For each new frame the algorithm will seek for                Detection of deformable objects in a colorful, dynamic
a similar patch to the one observed in the previous frame. If           context is currently an open challenge for computer vision
the patch is matched, a new point is added to the patch tra-            algorithms. Our data contained a wide variety of deforma-
jectory. If the matching is not achieved, a new patch (Pi+1 )           tions due to the child-friendly nature of the objects and the
is learned and the tracking algorithm is re-initialized. The            consistent occlusion of parts of the objects by caregivers’ and
base algorithm used for tracking is a version of the “tracking          children’s hands. To circumvent this difficult challenge, we
by detection” algorithm used above (Kalal et al., 2012). A              made use of motion as a convenient, psychologically-inspired
chunk is defined as the video segment defined by the startP             “filter.” Objects that are in motion are more likely to be at-
and endP frames of the tacked patch trajectory.                         tended by the child and/or caregiver; in addition, considering
Evaluation We evaluated the attention chunk method us-                  only those pixels that are in motion significantly constrains
ing the larger dataset egocentric videos. The distribution of           the object-detection problem (Figure 4).
chunk durations is shown in Figure 3. The method yielded                Foreground Modeling The goal of foreground modeling
a distribution that included many very short chunks (presum-            is to construct and maintain a statistical representation of the
ably while the head was in motion) as well as some longer               scene so that new information can be accurately extracted.
episodes of attention. We additionally found that the younger           We chose to utilize both texture information and color infor-
children in the sample (8 months) had somewhat more long                mation when modeling the background. We follow the ap-
attention chunks; we speculate that this pattern is due to the          proach of Yao and Odobez (2007), which exploits the Local
older children’s greater autonomy and mobility.                         Binary Pattern (LBP) feature as a measure of texture because
                                                                   2807

                                                                                                (a) “Manu” detected.
       (a) Object of interest (marked by the yellow square)
       could be confused with other objects with similar tex-
       tures and colors (marked in red). Considering only pix-
       els that are in motion effectively filters these distractors.
                                                                                                 (b) “Zem” detected.
                                                                         Figure 5: Four examples of object detections within the fore-
                                                                         ground of the static, 3rd person video.
                                                                         nearest dominant mode (peak). In our case, this distribution is
       (b) Objects of interest are being handled and are there-          based in color values. The algorithm is initialized by selecting
       fore moving (yellow square). Unattended objects (red              a region containing the object of interest and building a color
       square) are filtered out.
                                                                         histogram over the region. In a new frame, the algorithm will
                                                                         match the region’s size and the peaks of the color distribution
Figure 4: Foreground computation. In each image, top                     using both mean-shift and euclidean distance metrics. Figure
left shows original color frame, top right shows background              5 shows examples.
model color vector, bottom left shows foreground weights,
bottom right shows foreground extraction (orange/green pix-                                 Feature Aggregation
els correspond to the background mask). (a) shows the prob-              In the previous sections we detected and computed features
lem of texture and color overlap between the object of interest          (faces, objects, etc.) from different cameras. The goal of the
and other objects. (b) shows how this method can also filter             next stage is to merge and prune these features into a single
out unattended objects.                                                  matrix describing the detected features for each video frame.
                                                                         This aggregation required a number of decisions to be made.
                                                                         First, we synchronized frames across the cameras (which had
                                                                         different frame rates). Next, we calculated six features for
of its good properties (Heikkila & Pietikainen, 2006), along
                                                                         each frame f (all normalized to the same interval based on
with an illumination invariant photometric distance measure
                                                                         the observed maximum and minimum):
in the RGB space. However, we modified the LBP algorithm
to include a larger amount of texture from neighboring pixels.          1. Chunk length: The number of frames in the attention chunk
In brief, this approach computes summary statistics over the                containing ft .
background and searches for local deviations to those sum-              2. Chunk speed: The average speed of the attention chunk’s
mary statistics (due to motion).                                            trajectory.
Object Tracking We used the extracted foreground pixels                 3. Face speed: The speed (L2 norm) of the face position at ft
as the input to object-tracking algorithms and experimented                 and ft−1 .
with a number of appearance-based object detectors with rel-            4. Face size: The diagonal of the bounding box for the face (a
atively poor results. Our solution was to detect and track ob-              proxy for distance between parent and child).
jects by their color and relative size. We modified the cam-            5. Object speed: The speed (L2 norm) of the detected object’s
shift algorithm (Bradski, 1998), a specialization of the mean-              position at ft and ft−1 .
shift algorithm. Mean shift is a non-parametric technique that          6. Object size: The maximum diameter of the ellipse contain-
climbs the gradient of a probability distribution to find the               ing the pixels of the detected object.
                                                                     2808

                      Attn Chunks        Independent                  correctly classified as JA), and accuracy (overall proportion
          Child      P     R     A      P      R     A                frames correctly classified).
          08-01     .41 .19 .67        .45 .08 .69                       Results for individual children and aggregate results are
          08-05     .63 .45 .86        .48 .18 .67                    shown in Table 1. Although there was substantial variability
          08-07     .47 .94 .95        .22 .14 .94                    in accuracy across children, there were no systematic differ-
          08-11     .64 .44 .80        .60 .15 .77                    ences across ages. In particular, identifying JA episodes was
          08-15     .74 .80 .95        .47 .12 .90                    largely unsuccessful for children like 08-01, while for other
          16-04     .42 .86 .96        .21 .21 .95                    children like 08-15 our features were more diagnostic.
          16-12     .56 .80 .55        .54 .93 .54
                                                                         We additionally examined the feature weights learned by
          16-22     .54 .54 .89        .45 .27 .87
                                                                      the classifier. Figure 6B shows average weights on each of
          16-35     .38 .53 .93        .34 .28 .93
                                                                      the six features. We see very little difference in weight for
           Total    .53 .61 .84        .42 .26 .81
                                                                      attention chunk length and speed, suggesting that these char-
Table 1: Precision, recall, and accuracy for classifying JA,          acteristics of the chunks did not differ within JA episodes.
listed for each child and across all children. “Attn chunks”          The most positive weight is given to the object features (es-
refers to the model where features are propagated across at-          pecially object speed—which perhaps acted as a proxy for
tention chunks; “independent” refers to the model where each          engagement with the object), congruent with the independent
frame’s features are determined independently. Child ID               feature analyses. Weights for face features mismatched the
codes include the child’s age (08 refers to 8 months of age).         independent feature analysis, however, with less weight given
                                                                      to faces inside JA episodes. We speculate that this interaction
                                                                      might be due to the fact that faces were present in dyadic
   Because object and face features were computed frame by            play episodes where no object was present as well as in JA
frame, we experimented with using the attention chunks as a           episodes.
way to propagate features across larger ranges of time. Using            We evaluate the same approach while removing the at-
this method, all those objects and faces detected in a single         tention chunk propagation step. Evaluation metrics for the
video frame that fell within an attention chunk were propa-           classifier dropped for nearly all children; in particular, recall
gated to all of the frames of the chunk. Assuming that the            dropped considerably (Table 1). While the attention chunks
attention chunks have some value as indicators of the child’s         we identified did not directly correlate with JA, they neverthe-
attention, this step should improve the quality of detections.        less provided a useful temporal unit for classification. Further
We report results both with and without this propagation step.        optimizations of the attention chunk detector could improve
                                                                      JA identification.
                          Evaluation
Independent Feature Analyses                                                                  Conclusions
In our first analysis, we examined the proportion of face and
object detections that fell inside hand-coded JA episodes. A          “Joint attention” (JA) is an important construct in understand-
first indication of the informativeness of these features would       ing children’s social interactions with their caregiver. Yet this
be greater proportions of detections within JA. Our findings          construct is often defined from the perspective of a knowl-
supported that conclusion: Both faces and objects were more           edgeable third-person observer. Such definitions have both
prevalent within JA episodes on average (Figure 6A), though           psychological and practical consequences. Psychologically, a
this trend was much more pronounced for objects. Faces were           growing body of evidence suggests that children may not al-
1.8x more prevalent in JA episodes, while moving objects              ways have access to their parents’ gaze (Franchak et al., 2011;
were 3.0x more prevalent.                                             Yu & Smith, 2013; Frank, Simmons, et al., 2013), and so they
                                                                      may have to infer whether they are in joint attention from a
Classification Analysis                                               host of noisy signals (Frank, Tenenbaum, & Fernald, 2013).
Our next analysis used all of the features described above            Practically, identifying JA in large datasets using automated
to classify frames as being in or out of joint attention. Al-         methods may be exceedingly difficult.
though in principle we could have used a complex model that              In the current paper, we looked for other features that were
took into account temporal dependencies between frames, we            related to JA. Two findings emerged from our analysis. First,
chose to begin by using a simple Naive Bayes classifier. The          the motion of an object is a simple but highly diagnostic cue
value of this initial approach is that it allows the straightfor-     to JA—more so than the presence of the caregiver’s face. Sec-
ward examination of the weights on each feature.                      ond, the propagation of features across the “attention chunks”
   We trained the classifier on our hand-coded JA data and            that we identified improved our classification accuracy, sug-
then used it to predict held-out data using 10-fold cross-            gesting that they carried some information about the child’s
validation. We then evaluated classifier performance on pre-          sustained attention. These findings suggest that, with fur-
cision (proportion of frames classified as JA that were ac-           ther development, automated analysis of joint attention from
tually JA), recall (proportion of actual JA frames that were          video may be possible in the future.
                                                                  2809

      A                                                       B
                              0.5                                            0.025
Proportion feature presence
                                            no JA
                              0.4           JA                                0.02
                                                          Parameter weight
                              0.3                                            0.015
                              0.2                                             0.01
                                                                                                                                        Figure 6: (A) Proportion of faces and ob-
                              0.1                                            0.005                                                      jects detected both in and out of joint at-
                                                                                                                                        tentional episodes. (B) Mean parameter
                               0                                                0
                                    Faces       Objects                                                                                 weights on each of the six features we con-
                                                                                     Length   Speed
                                                                                                      Size
                                                                                                             Speed           Speed
                                                                                                                      Size
                                                                                                                                        sidered for the JA and non-JA categories in
                                                                                                                                        the Naive Bayes Classifier. Legend is as in
                                                                                      Chunks             Faces          Objects         (A).
                                                    References                                                         learning-detection. Pattern Analysis and Machine Intelli-
                                                                                                                       gence, IEEE Transactions on, 34, 1409–1422.
 Adolph, K. E., Gilmore, R. O., Freeman, C., Sanderson, P.,                                                          Pereira, A. F., Smith, L. B., & Yu, C. (2013). A bottom-
   & Millman, D. (2012). Toward open behavioral science.                                                               up view of toddler word learning. Psychonomic bulletin &
   Psychological Inquiry, 23, 244–247.                                                                                 review, 1–8.
 Bradski, G. R. (1998). Real time face and object tracking as                                                        Smith, L. B., Yu, C., & Pereira, A. F. (2011). Not your
   a component of a perceptual user interface. In Proeedings                                                           mothers view: The dynamics of toddler visual experience.
   of the Fourth IEEE Workshop on Applications of Computer                                                             Developmental science, 14, 9–17.
   Vision (pp. 214–219).                                                                                             Smith, W. (2010). Whip my hair. CD Single.
 Brooks, R., & Meltzoff, A. N. (2008). Infant gaze fol-                                                              Tardif, T., Fletcher, P., Liang, W., Zhang, Z., Kaciroti, N., &
   lowing and pointing predict accelerated vocabulary growth                                                           Marchman, V. A. (2008). Baby’s first 10 words. Develop-
   through two years of age: A longitudinal, growth curve                                                              mental Psychology, 44, 929.
   modeling study. Journal of Child Language, 35, 207–220.                                                           Tomasello, M., & Farrar, M. J. (1986). Joint attention and
 Bruner, J. (1985). Child’s talk: Learning to use language.                                                            early language. Child development, 1454–1463.
   Child Language Teaching and Therapy, 1, 111–114.                                                                  Viola, P., & Jones, M. (2001). Rapid object detection using
 Carpenter, M., & Liebal, K. (2011). Joint attention, commu-                                                           a boosted cascade of simple features. In Computer vision
   nication, and knowing together in infancy. Joint Attention:                                                         and pattern recognition, 2001. cvpr 2001. proceedings of
   New Developments in Psychology, Philosphy of Mind, and                                                              the 2001 ieee computer society conference on (Vol. 1, pp.
   Social Neuroscience, 159–182.                                                                                       I–511).
 Carpenter, M., Nagell, K., & Tomasello, M. (1998). Social                                                           Yao, J., & Odobez, J. M. (2007). Multi-layer background sub-
   cognition, joint attention, and communicative competence                                                            traction based on color and texture. Computer Vision and
   from 9 to 15 months of age. Monographs of the society for                                                           Pattern Recognition, IEEE Computer Society Conference.
   research in child development, i–174.                                                                             Yoshida, H., & Smith, L. B. (2008). What’s in view for
 Franchak, J. M., Kretch, K. S., Soska, K. C., & Adolph, K. E.                                                         toddlers? using a head camera to study visual experience.
   (2011). Head-mounted eye tracking: A new method to de-                                                              Infancy, 13, 229–248.
   scribe infant looking. Child development, 82, 1738–1750.                                                          Yu, C., & Smith, L. B. (2013). Joint attention without gaze
 Frank, M. C., Simmons, K., Yurovsky, D., & Pusiol, G.                                                                 following: Human infants and their parents coordinate vi-
   (2013). Developmental and postural changes in childrens                                                             sual attention to objects through eye-hand coordination.
   visual access to faces. In Proceedings of the 35th Annual                                                           PLoS ONE, 8, e79659.
   Conference of the Cognitive Science Society of the.                                                               Yurovsky, D., Wade, A., & Frank, M. C. (2013). Online
 Frank, M. C., Tenenbaum, J. B., & Fernald, A. (2013). Social                                                          processing of speech and social information in early word
   and discourse contributions to the determination of refer-                                                          learning. In Proceedings of the 35th Annual Conference of
   ence in cross-situational word learning. Language Learn-                                                            the Cognitive Science Society of the.
   ing and Development, 9, 1–24.                                                                                     Zhu, X., & Ramanan, D. (2012). Face detection, pose esti-
 Heikkila, M., & Pietikainen, M. (2006, April). A texture-                                                             mation, and landmark localization in the wild. In Computer
   based method for modeling the background and detecting                                                              vision and pattern recognition, ieee computer society con-
   moving objects. IEEE Transactions on Pattern Analysis                                                               ference (pp. 2879–2886). IEEE.
   and Machine Intelligence, 28, 657–662.
 Kalal, Z., Mikolajczyk, K., & Matas, J. (2012). Tracking-
                                                                                                             2810

