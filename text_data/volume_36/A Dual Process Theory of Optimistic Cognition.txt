UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
A Dual Process Theory of Optimistic Cognition
Permalink
https://escholarship.org/uc/item/23q9v98f
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Sunehag, Peter
Hutter, Marcus
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                                A Dual Process Theory of Optimistic Cognition
          Peter Sunehag (Peter.Sunehag@anu.edu.au) and Marcus Hutter (Marcus.Hutter@anu.edu.au)
                                                   Research School of Computer Science
                                          The Australian National University, Canberra Australia
                               Abstract                                 needs such an a priori environment, e.g. through a prior on a
   Optimism is a prevalent bias in human cognition including            hypothesis class, and that performing the mentioned compu-
   variations like self-serving beliefs, illusions of control and       tation is too hard, even approximately. We argue that the sec-
   overly positive views of one’s own future. Further, optimism         ond of these motivates optimism as a rational strategy, since
   has been linked with both success and happiness. In fact, it has
   been described as a part of human mental well-being which has        when optimizing over a too short horizon, realism leads to in-
   otherwise been assumed to be about being connected to reality.       sufficiently explorative behavior. A property that makes opti-
   In reality, only people suffering from depression are realistic.     mism particularly appealing is that as long as the outcome is
   Here we study a formalization of optimism within a dual pro-
   cess framework and study its usefulness beyond human needs           as predicted, high rewards are received. This is regardless of
   in a way that also applies to artificial reinforcement learning      the correctness of the hypothesis. Humans are often trying to
   agents. Optimism enables systematic exploration which is es-         avoid contradiction of their hypothesis and often avoid con-
   sential in an (partially) unknown world. The key property of
   an optimistic hypothesis is that if it is not contradicted when      tradicting each other’s hypothesis (Taylor and Brown, 1988).
   one acts greedily with respect to it, then one is well rewarded      Managing to enforce ones hypothesis in a group is primarily
   even if it is wrong.                                                 useful if it is optimistic, i.e. self-serving.
   Keywords: Rationality, Optimism, Optimality, Reinforcement
   Learning                                                                We are here going to discuss agents with limited resources
                                                                        within a dual process agent framework where a limited num-
                           Introduction                                 ber of hypotheses are generated by one system and the other
The optimistic bias is (together with the simplicity bias               system is making a choice by excluding implausible hy-
(Chater and Vitanyi, 2003)) perhaps the most fundamen-                  potheses and choosing optimistically/greedily among the rest.
tal and prevalent among the human cognitive biases (Taylor              Within such a framework we use normative principles such
and Brown, 1988; Sharot et al., 2007; Carver et al., 2010;              as rationality considerations as in the foundations of decision
Kahneman, 2011) It has been found to be correlated with                 theory together with the kind of performance guarantees stud-
high achievement, happiness and resilience when challenged              ied in reinforcement learning as a subfield of artificial intel-
(Carver et al., 2010) but also with dangerous risk-seeking in           ligence. Dual Process theories with an implicit and an ex-
e.g. traffic or severely underestimating costs in ruinous public        plicit part (often called system 1 and system 2) have a strong
building projects (Kahneman, 2011). Some of the drawbacks               position in cognitive science with ample empirical support
can be viewed as falling for implausible (cockeyed instead              (Evans, 2003), though still considered an approximation of a
of cautious (Wallston, 1994)) optimism where one has not                more complex reality with overlapping functionality.
learned from other examples through the base rate for the sort             In our framework, an agent consists of a decision func-
of situation one is in. Besides resilience in the face of chal-         tion and a hypothesis generating function. The hypothesis
lenges and the effect on others, the optimism bias has uses             generating function feeds the decision function a finite class
beyond the strictly human. In reinforcement learning, arti-             of environments at every time step and the decision func-
ficial agents are often equipped with an optimism bias to en-           tion chooses an action/policy given such a class. We define
able systematically explorative behavior in an unknown world            agents within this framework by combining optimistic deci-
(Szita and Lörincz, 2008). Here we mathematically study                sion functions with hypothesis generating functions defined
plausibly optimistic general reinforcement learning agents as           by enumerating a countable class and introducing new envi-
an alternative (both normative and descriptive) paradigm to             ronments from this list (which could be sampled incremen-
Bayesian models of cognition (Griffiths et al., 2008).                  tally with a simplicity bias formalized by Kolmogorov com-
   The general reinforcement learning problem in an un-                 plexity (Sunehag and Hutter, 2013)) when we are within a
known environment is an extremely challenging problem                   given error budget. We present results for generic countable
(Hutter, 2005). If one has access to a suitable (i.e. probability       classes by extending the agents introduced in Sunehag and
mass placed mainly on the actually plausible) a priori envi-            Hutter (2012a) from the finite case.
ronment, e.g. a Bayesian mixture over all environments in a                The best bounds for fully general reinforcement learning
certain hypothesis class, and the computational resources to            have a linear dependence on the number of environments in
compute the policy that maximizes the desired quality mea-              the class. Though this is easily seen to be the best one can
sure, this is the natural choice and optimal by definition. The         do in general (Lattimore et al., 2013), it is bad (exponentially
quality measure can for example be expected accumulated re-             worse) compared to what we are used to from Markov Deci-
ward during the life of the agent, or the maximum accumu-               sion Processes (MDPs) (Lattimore and Hutter, 2012) where
lated reward that is guaranteed with a certain given proba-             the linear (up to logarithms) dependence is on the size of the
bility (e.g. 0.95). The two immediate problems are that one             state space instead. We introduce environment classes that
                                                                    2949

are much more general than MDPs, while they are finitely                 we will call a decision function and a hypothesis generating
generated (by laws) in a way enabling a good bound.                      function. Within this framework, we will extend the agents
Outline. We begin by introducing background and nota-                    and analysis from the previous section to arbitrary infinitely
tion for general reinforcement learning and then we introduce            countable classes.
our agent framework and provide examples of agents that fit              Decision Functions
within it. Within this framework we then provide bounds on
                                                                         The primary component of our agent framework is a decision
the number of errors that an optimistic agent makes in the
                                                                         function f : M → A (M is the set of finite sets of environ-
case of an infinite countable class of possible environments
                                                                         ments) only depending on a class of environments M . The
which the agent includes incrementally in its environment
                                                                         decision function is independent of the history, however, the
class. Finally we extend (and improve) the results to the case
                                                                         class M fed to the decision function introduce an indirect de-
of environments generated by laws and then conclude.
                                                                         pendence. For example, the environments at time t + 1 can
General Reinforcement Learning. We will consider an                      be the environments at time t, conditioned on the new obser-
agent (Russell and Norvig, 2010; Hutter, 2005) that interacts            vation. In this setting we will often write the value function
with an environment through performing actions at from a fi-             without an argument due to this independence. Vνπ̃t = Vνπ0 (ht )
nite set A and receives observations ot from a finite set O and          if νt = ν0 (·|ht ) where the policy π̃ on the left hand side is the
rewards rt from a finite set R ⊂ [0, 1] resulting in a history           same as the policy π on the right, just after ht have been seen
ht := o1 r1 a1 , ..., ot rt . These sets can be allowed to depend on     so it starts at a later stage, meaning π̃(h) = π(ht h) where ht h
time or context but we do not write this out explicitly. Let             is a concatenation.
H := ∪n (O × R × A )n × (O × R ) be the set of histories and             Definition 1 (Rational Decision Function). Given alphabets
let ε be the empty history. A function ν : H × A → O × R
                                                                         A , O and R we say that a decision function f : M → A is
is called a deterministic environment. A function π : H → A
                                                                         a function f (M ) = a that for any class of environments M
is called a (deterministic) policy or an agent. We define the
                                                                         based on those alphabets and finite history produces an ac-
value function V based on geometric discounting (discount
                                                    i−t                  tion a ∈ A . We say that f is strictly rational for the class M
factor 0 ≤ γ < 1) by Vνπ (ht−1 ) = ∑∞         i=t γ ri where the se-     if there are ων ≥ 0, ν ∈ M , ∑ν∈M wν = 1 such that a = π(ε)
quence ri are the rewards achieved by following π from time
                                                                         for a policy
step t onwards in the environment ν after having seen ht−1 .
                                                                                               π ∈ arg max ∑ ωνVνπ .                     (1)
    Instead of viewing the environment as a function H × A →                                           π     ν∈M
O × R we can equivalently write it as a function ν : H ×
A × O × R → {0, 1} where we also write ν(o, r|h, a) for the                  Agents who are as in Definition 1 are also called admissible
function value ν(h, a, o, r) (which is not the probability of the        if wν > 0 ∀ν ∈ M since then they are Pareto optimal (Hutter,
four-tuple). It equals zero if in the first formulation (h, a) is        2005). Being Pareto optimal means that if another agent (of
not sent to (o, r) and 1 if it is. In the case of stochastic envi-       this form or not) is strictly better (higher expected value) than
ronments we instead have a function ν : H × A × O × R →                  a particular agent of this form in one environment, then it is
[0, 1] such that ∑o,r ν(o, r|h, a) = 1 ∀h, a. Furthermore, we            strictly worse in another. A special case is when |M | = 1 and
define ν(ht |π) := Πti=1 ν(oi ri |ai−1 , hi−1 ) where ai = π(hi ).       (1) then becomes
ν(·|π) is a probability measure over strings and we define                                          π ∈ arg max Vνπ
                                                                                                            π
ν(·|π, ht−1 ) by conditioning ν(·|π) on ht−1 . Vνπ (ht−1 ) :=
Eν(·|π,ht−1 ) ∑∞     i−t           ∗                    π                where ν is the environment in M . The more general case con-
               i=t γ ri and Vν (ht−1 ) := maxπ Vν (ht−1 ).
                                                                         nects to this by letting ν̃(·) := ∑ν∈M wν ν(·). The next defini-
Examples of agents: AIXI and Optimist. Given a countable                 tion defines optimistic decision functions. They only coincide
class of environments M and strictly positive prior weights              with strictly rational ones (as defined above, see Sunehag and
wν for all ν ∈ M , we define the a-priori environment ξ by let-          Hutter (2011) for details) for the case |M | = 1. However,
ting ξ(·) = ∑ wν ν(·) and the AIXI agent (in its general form)           agents based on such decision functions satisfy the looser ax-
is defined by following the policy π∗ := arg maxπ Vξπ (0).    / The      ioms that define (a weaker form of) rationality in Sunehag
above agent, and only agents of that form, satisfy the strict            and Hutter (2012b).
rationality axioms presented in Sunehag and Hutter (2011)
                                                                         Definition 2 (Optimistic Decision Function). We call a deci-
while the slightly looser version from Sunehag and Hutter
                                                                         sion function f optimistic if f (M ) = a implies that a = π(ε)
(2012b) allows for optimism. The optimist takes the decision
                                                                         for an optimistic policy π, i.e. for
                        π◦ := arg max max Vξπ (0)  /
                                    π    ξ∈Ξ                                                     π ∈ arg max max Vνπ .                   (2)
                                                                                                         π    ν∈M
for a finite set of beliefs (environments) Ξ.
                                                                         Agents Based on Decision Functions
     An agent framework with growing classes                             Given a decision function, what remains to create a complete
In this section, we introduce an agent framework that we can             agent is a hypothesis generating function g(h) = M that for
fit some existing successful agents into by a choice of what             any history h ∈ H produces a set of environments M . A
                                                                     2950

special case of a hypothesis generating function is defined           in a hypothesis generating function. In the choice of hypoth-
by combining the initial g(ε) = M0 with an update function            esis generating functions we are going to focus on what kind
ψ(Mt−1 , ht ) = Mt . An agent, i.e. a function from histories         of performance can be guaranteed in terms of how many sub-
to actions, is defined from a hypothesis generating function g        optimal decisions will be taken. First, however, we want to
and a decision function f by choosing action a = f (g(h)) af-         restrain ourselves to hypothesis generating functions that are
ter seeing history h. We discuss a number of examples below           following Epicurus’ principle that says that one should keep
to elucidate the framework and as a basis for the results we          all consistent hypotheses. In the case of deterministic envi-
present later.                                                        ronments it is clear what it means to have a contradiction be-
Example 3. Suppose that ν is a stochastic environment and             tween a hypothesis and an observation while in the stochastic
g(h) = {ν(·|h)} for all ν and let f be a strictly rational de-        case it is not. One can typically only say that the data make
cision function. This agent is a rational agent in the stricter       n hypothesis unlikely as in Example 5. We will consider the
sense . Also, if g(h) = {ν(·|h) |ν ∈ M } for all h ∈ H (same          hypothesis generating function to satisfy Epicurus if the up-
M for all h) and there are ων > 0, ν ∈ M , ∑ν∈M wν = 1 such           date function is such that it might add new environments in
that a = π(ε) for a policy                                            any way while removing environments if a hypothesis is im-
                                                                      plausible (likely to be false) in light of the observations made.
                    π ∈ arg max         ωνVνπ ,               (3)        Aside from satisfying Epicurus’ principle, we will design
                            π
                                  ∑                                   hypothesis generating functions based mainly on wanting few
                                 ν∈g(h)
                                                                      mistakes to be made. For this purpose we first define the term
then we say that we have a Bayesian agent, which can be rep-          ε-error. We are going to formulate the rest of the definitions
resented more simply in the first way by g(h) = {∑ wν ν(·|h)}.        and results in this section for γ = 0 for simplicity and brevity.
Example 4. Suppose that M is a finite class of deterministic          Definition 8 (ε-error). Given 0 ≤ ε < 1, we define the number
environments and let g(h) = {ν(·|h) | ν ∈ M consistent with           of ε-errors in history h to be
h}. If we combine g with the optimistic decision function we
                                                                               m(h, ε) = |{i ≤ `(h) | Vµai (hi ) < Vµ∗ (hi ) − ε}|
have defined the optimistic agents for classes of determinis-
tic environments from Sunehag and Hutter (2012a). We here             where µ is the true environment, `(h) is the length of h, ai is
extend the analysis to infinite classes by letting g(ht ) contain     the i:th action and Vµ∗ (h) = arg maxa Vµa (h). Each such time-
new environments that were not in g(ht−1 ).                           point is called an ε-error.
Example 5. Suppose that M is a finite class of stochastic en-            Since we consider a setting where the true environment is
vironments and that g(h) = {ν(·|h) | ν ∈ M }. If we combine           unknown, an agent cannot know if it has made an ε-error or
g with the optimistic decision function we have defined the           not. However, if one assumes that the true environment is in
optimistic AIXI agent from Sunehag and Hutter (2012b). If             the class g(ht ), or more generally that the class contains an
instead g(h) = {ν(·|h) | ν ∈ M : maxν(h)ν̃(h) ≥ z} for some z > 0     environment that is optimistic with respect to the true envi-
                                        ν̃
we have defined the optimistic agent with stochastic environ-         ronment, and if the class is narrow in total variation distance
ments from Sunehag and Hutter (2012a).                                in the sense that the distance between any pair of environ-
                                                                      ments in the class is small, then one can guarantee that an
Example 6. The Model Based Interval Estimation (MBIE)
                                                                      ε-error is not made (Sunehag and Hutter, 2012a). Since we
(Strehl et al., 2009) method for Markov Decision Processes
                                                                      do not know if this extra assumption holds for g(ht ), we will
(MDPs) defines g(h) as a set of MDPs (for a given state
                                                                      use the terms ε-confident and ε-inconfident.
space) with transition probabilities in confidence intervals
                                                                         If the value functions in the class g(ht ) differ in their pre-
calculated from h. This is combined with the optimistic de-
                                                                      dicted value by more than ε > 0, then we cannot be sure not
cision function.
                                                                      to make an ε-error even if we knew that the true environment
Example 7. Agents that switch explicitly between exploration          is in g(ht ). We call such points ε-inconfidence points.
and exploitation are typically not satisfying our (weak) ratio-       Definition 9 (ε-(in)confidence). Given 0 < ε < 1, we define
nality demand. An example is Lattimore et al. (2013) where            the number of ε-inconfidence point in the history h to be
the introduced Maximum Exploration Reinforcement Learn-
                                                                                                                         ∗     ∗
ing (MERL) agent performs certain tests when the remaining                   n(h, ε) := |{i ≤ l(h) |     max        |Vνπ1 −Vνπ2 | > ε}|
                                                                                                     ν1 ,ν2 ∈g(hi )
candidate environments are disagreeing sufficiently. This de-
cision function is not satisfying rationality while it is proven      where π∗ := arg maxπ maxν∈g(ht ) Vνπ . In the γ = 0 case studied
that MERL satisfies near-optimal sample complexity for gen-           here, we can equivalently use a∗ := arg maxa maxν∈g(ht ) Vνa
eral reinforcement learning with finite classes. Another ex-          instead of π∗ . The individual time-points are the points
ample of this kind of agent is BayesExp (Lattimore, 2013).            of ε-inconfidence and the other points are the points of ε-
                                                                      confidence.
Properties of hypothesis generating functions. After see-
ing examples of decision functions and hypothesis generating          Hypothesis generating functions with budget. We sug-
functions above, we will discuss what properties are desirable        gest defining a hypothesis generating function from a count-
                                                                  2951

able enumerated class M based on a budget function for ε-              able to have at most |M −1| contradictions. In the case where
inconfidence. The idea is simply that when the number of               environments are being added one can have errors either be-
ε-inconfidence points is below budget we introduce the next            fore the truth is added or within that many time-steps before
environment in the class. The intuition is that if the current         a contradiction or that many time-steps before the addition of
hypotheses are frequently contradictory, then one should re-           a new environment. The addition of a new environment can
solve this before adding more. The definition is also mathe-           change the optimistic policy without having encountered a
matically convenient for proving bounds on ε-errors. Besides           contradiction, the event temporarily breaks time-consistency.
the budget function we also require a criterion for excluding          Hence, every added environment after the truth has been in-
environments.                                                          cluded can add at most 2 − log1−γε(1−γ)
                                                                                                               ε-errors. In the γ = 0 case
Definition 10 (Hypothesis generation with budget and exclu-            it is only at contradictions and when the truth has not been
sion function).                                                        added that we can have errors.
Suppose we have a chosen accuracy ε > 0, an enumerated                 Theorem 11. Suppose we have a countable class of deter-
countable class of environments M , a finite initial class             ministic environments M (with a chosen enumeration and
M 0 ⊂ M a non-decreasing budget function N : N → N such                containing the true one). Also suppose we have a hypothesis
that N(t) → ∞ as t → ∞, an exclusion function (criterion)              generating function g with a finite initial class g(ε) := M 0 ⊂
φ(M̃ , h) = M̂ for M̃ ⊂ M and h ∈ H such that M̂ ⊂ M̃ .                M , budget function N : N → N, accuracy ε = 0 and suppose
Then the hypothesis generating function g with class M , ini-          that g excludes contradicted environments. π◦ is defined by
tial class M 0 , accuracy ε > 0, budget N and exclusion crite-         combining g with an optimistic decision function. The num-
rion φ is defined recursively as follows:                              ber of 0-errors m(ht , 0) is at most n(ht , 0) + C for some con-
   Let g(ε) = M 0 . If n(ht , ε) ≥ N(t), then                          stant C > 0 (which is the number of steps before the true en-
                                                                       vironment is introduced and depends on the choice of budget
 g(ht ) = {ν(·|ht ) | ν ∈ φ({ν ∈ M | ν(·|ht−1 ) ∈ g(ht−1 )}, ht )}     function N but not on t). Furthermore, ∀i ∈ N there is ti ∈ N
                                                                       such that ti < ti+1 and n(hti , 0) < N(ti ).
while if n(ht , ε) < N(t), let ν̃ be the environment in M with
the lowest index that is not in ∪t−1                                      The last claim is the most important saying that we will
                                   i=1 {ν ∈ M | ν(·|hi ) ∈ g(hi )}
(i.e. the next environment to introduce) and let g(ht ) =              always see the number of errors fall within the budget N(t)
                                                                       again (except for a constant term) even if it can be temporar-
  {ν(·|ht ) | ν ∈ {ν̃ ∪ φ({ν ∈ M | ν(·|ht−1 ) ∈ g(ht−1 )}, ht )}}.     ily above. This means that we will always introduce more
                                                                       environments and exhaust the class in the limit. If we wanted
Error Analysis                                                         the errors to always be within N(t) (except for a constant) we
We will now extend the agents described in Example 4 and               could forbid the agent from introducing more environments
Example 5 by removing the demand for the class M being                 than N(t) before time t. This is because the number of ex-
finite and analyze the effect on the number of ε-errors made.          cluded hypotheses cannot exceed the number of introduced
We will still use the optimistic decision function and apply it        hypotheses including the ones in g(ε) and we only have an
to finite classes but we will keep adding environments from            error when an environment is excluded (and not always then)
the full class to the finite working class of environments. The        or when g(ht ) is empty which it is not again after the true
resulting agents differ from agents such as the one in Exam-           environment is introduced.
ple 7 by (among other things) instead of having exploration
                                                                       Proof. Suppose that at time t, the true environment µ is in
phases as part of the decision function, it has a hypothe-
                                                                       g(ht ). Then, if we do not have a 0-inconfidence point, it fol-
sis generating function that sometimes adds an environment
                                                                       lows from optimism that
which may cause new explorative behavior if it becomes the
                                                                                               ◦
optimistic hypothesis and it deviates significantly from the                                Vµπ (ht ) = max Vµa (ht )                  (4)
                                                                                                           a
other environments. A nice point about our results is that one
chooses the asymptotic rate, however one gets a worse con-             since all the environments in g(ht ) agree on the reward for
stant the better rate one chooses. This is due to the fact that if     the optimistic action. Hence m(ht , 0) ≤ n(ht , 0) +C where C
one includes new environments at a slower rate it takes longer         is the time the true environment is introduced. However, we
until the right environment is introduced while the error rate         need to show that it will be introduced by proving that the
afterwards is better. If one knew that one had included the            class will be exhausted in the limit. If this was not the case,
right one, then one would stop introducing more.                       then there is T such that n(0, ht ) ≥ N(t) ∀t ≥ T . Since we
   We extend the agent for finite classes of deterministic envi-       have 0-inconfidence points exactly when we are guaranteed
ronments in Example 4 to the countable case and we leave the           to have a contradiction, n(0, ht ) is then bounded by the num-
extension of the stochastic case to a longer report. In the fi-        ber of environments that have been introduced up to time t
nite case with a fixed class, the proof of the finite error-bound      if we include the number of environments in the initial class.
in Sunehag and Hutter (2012a) builds on the fact that every            Hence n(0, ht ) is bounded by a finite number while (by the
ε-error must be within − log1−γε(1−γ)
                                      time-steps before a contra-      definition of budget function) N(t) → ∞ which contradicts
diction and the bound followed immediately by only being               the assumption.
                                                                   2952

Remark 12 (Extensions: γ > 0, Separable classes). As in the            Example 15. Consider an environment with a constant bi-
deterministic case, what makes the γ = 0 case simpler than             nary feature vector of length m. There are 2m such environ-
the 0 < γ < 1 case is that ε-errors can for γ > 0 also occur           ments. Every such environment can be defined by combining
within − log1−γ
              ε(1−γ)
                      time-steps before a new environment is in-       m out of a class of 2m laws. Each law says what the value of
troduced. Further, one can extend our algorithms for count-            one of the features is, one law for 0 and one for 1.
able classes to separable classes since they can be covered by            We analyze the optimistic agent from Example 4 in this
countable many balls of arbitrarily small radius.                      new setting. Every contradiction of an environment is a con-
                                                                       tradiction of at least one law and there are finitely many laws
            Environments Defined by Laws                               and this is what is needed for the finite error result from be-
In this section we will investigate classes of environments of         fore to hold but with |M | replaced by |T | (see Theorem 16
a special but generic form combining a number of laws that             below) which can be exponentially smaller. Furthermore, the
partially determine what happens next. Classes of such form            extension to countable T works the same as in Theorem 11.
have the property that one can exclude (or merge) laws and             Theorem 16 (Finite error-bound when using laws). Sup-
thereby exclude (or merge) whole classes of environments               pose that T is a finite class of deterministic laws and let
like when one learns about a state transition when working             g(h) = {ν(·|h) | ν ∈ M ({τ| τ ∈ T consistent with h})} . We
with MDPs. Our setting is, however, far more general than              define π̄ by combining g with the optimistic decision function.
MDPs and shares characteristics with what has been studied             Following π̄ with a finite class of deterministic laws T in an
in the Learning Classifier Systems literature (Holland, 1986;          environment µ ∈ M (T ),
Drugowitsch, 2007).
   We consider observations of the form of a feature vector                               |Vµπ̄ (ht ) − max Vµπ (ht )| < ε               (5)
                                                                                                         π
o = x̄ = (x j )mj=1 ∈ O = ×mj=1 O j (including the reward as one
coefficient) where x j is an element of some finite alphabet Oi .      for all but at most |T − l| − log1−γ
                                                                                                          ε(1−γ)
                                                                                                                 time steps t where l is the
Definition 13. A law is a function τ : H × A → Õ where Õ             minimum number of laws from T needed to define a complete
consists of the feature vectors from O but where some ele-             environment.
ments are replaced by a special letter ⊥ meaning that there            Proof. This is true for the same reason as the finite-error
is no prediction for this feature, i.e. Õ = ×mj=1 (O j ∪ {⊥}).        bound in Sunehag and Hutter (2012a) since there are at most
   Using a feature vector representation of the observations           |T − l| time-steps with a contradiction and errors occur only
and saying that a law predicts some of the features is a conve-        at times that are at most − log1−γ ε(1−γ)
                                                                                                                 steps before a contradic-
nient special case of saying that the law predicts that the next       tion. This is due to time-consistency of geometric discount-
observation will belong to a certain subset of the observation         ing.
space.
   We first consider deterministic laws. Each law τ predicts,
given the history and a new action, some (or none) but not             Background Environments. Further improvement in the
necessarily all of the features x j at the next time point. We         rate of errors can be achieved if we have a background en-
first define a setting where the set of laws is such that in every     vironment and the laws are only replacing the prediction of
situation and for every feature there is at least one law that         the background environment for the part they say something
makes a prediction of this feature in the given situation. We          about. Again if one formalizes this notion correctly, one can
can then directly define a set of environments by combining            prove bounds linear in the number of laws which can be much
such laws.                                                             fewer in this situation.
Definition 14. [Environments from deterministic laws] Given            Computing the optimistic decision as one planning prob-
a finite set of laws T (maps from H × A to Õ) such that O =           lem. Finding the optimistic decision with a collection of laws
× j O j have m features labelled 1, ..., m. We define the class of     results in a computation as in an auction-based multi-agent
environments                                                           planning system (as in e.g. Allard and Shekh (2012)), though
                                                                       many of the laws are making incorrect predictions. The com-
             M (T ) := {ν |∃T̃ ∈ C (T ) : ν = ν(T̃ )}                  bined choice of laws and action to use forms a larger action
where the set of coherent and complete sets of deterministic           space as in Asmuth et al. (2009), though for a much more
laws C (T ) is defined by                                              general situation. Monte-Carlo Tree Search methods (Silver
                                                                       and Veness, 2010), which are also planning methods, could
        C (T ) := {T̃ ⊂ T : ∀h, a∀ j ∈ {1, ..., m}∃τ ∈ T̃ :            be applied despite that some of the laws (which are also se-
                                                                       lected instead of only choosing actions in the search) are in-
         ν(h, a)( j) 6= ⊥ ∧ τ̃(h, a)( j) = ⊥∀τ̃ ∈ T̃ \ {τ}}            correct and contradict each other. Value predictions resulting
and for T̃ ∈ C (T ), ν(T̃ ) is the environment ν which is such         from function approximation can be very useful for guiding
that                                                                   the tree search (Silver et al., 2012). Human cognition likely
                                                                       involves both estimation of models as well as direct value es-
      ∀h, a∀ j ∈ {1, ..., m}∃τ ∈ T̃ : ν(h, a)( j) = τ(h, a)( j).       timation (Shteingart and Loewenstein, 2014).
                                                                   2953

                         Conclusions                                 Lattimore, T. and Hutter, M. (2012). PAC Bounds for Dis-
We introduced a dual process framework based on a hypoth-               counted MDPs. In Bshouty, N. H., Stoltz, G., Vayatis, N.,
esis function and a decision function. An optimistic deci-              and Zeugmann, T., editors, ALT, volume 7568 of Lecture
sion function was found to be useful for achieving optimality           Notes in Computer Science, pages 320–334. Springer.
guarantees while a simplicity bias can be useful for hypoth-         Lattimore, T., Hutter, M., and Sunehag, P. (2013). The
esis generation (Sunehag and Hutter, 2013). A key point is              sample-complexity of general reinforcement learning.
that optimism encourages exploration, which is important if             Journal of Machine Learning Research, W&CP: ICML,
one cannot optimize a strategy for one’s whole life. Further            28(3):28–36.
when acting according to an optimistic hypothesis it is only         Russell, S. J. and Norvig, P. (2010). Artificial Intelligence:
important that it is not contradicted while it does not have to         A Modern Approach. Prentice Hall, Englewood Cliffs, NJ,
be correct for other circumstances. One will still be highly            3nd edition.
rewarded.                                                            Sharot, T., Riccardi, A. M., Raio, C. M., and Phelps, E. A.
                                                                        (2007). Neural mechanisms mediating optimism bias. Na-
                          References                                    ture, pages 1–5.
Allard, T. and Shekh, S. (2012). Hierarchical multi-agent            Shteingart, H. and Loewenstein, Y. (2014). Reinforcement
   distribution planning. In AI 2012: Advances in Artificial            learning and human behavior. Current Opinion in Neuro-
   Intelligence, volume 7691 of Lecture Notes in Computer               biology, 25(0):93 – 98.
   Science, pages 755–766.                                           Silver, D., Sutton, R., and Müller, M. (2012). Temporal-
Asmuth, J., Li, L., Littman, M., Nouri, A., and Wingate, D.             difference search in computer Go. Machine Learning,
   (2009). A bayesian sampling approach to exploration in               87(2):183–219.
   reinforcement learning. In Uncertainty in Artificial Intelli-     Silver, D. and Veness, J. (2010). Monte-Carlo Planning in
   gence (UAI), pages 19–26.                                            Large POMDPs. In Advances in Neural Information Pro-
Carver, C. S., Scheier, M. F., and Segerstrom, S. C. (2010).            cessing Systems 23: 2010., pages 2164–2172.
   Optimism. Clinical Psychology Review, 30(7):879–889.              Strehl, A. L., Li, L., and Littman, M. L. (2009). Reinforce-
Chater, N. and Vitanyi, P. (2003). Simplicity: A unifying               ment learning in finite MDPs: PAC analysis. Journal of
   principle in cognitive science? Trends in Cognitive Sci-             Machine Learing Research, 10:2413–2444.
   ences, 7:19–22.                                                   Sunehag, P. and Hutter, M. (2011). Axioms for rational re-
Drugowitsch, J. (2007). Learning Classifier Systems from                inforcement learning. In Algorithmic Learning Theory,
   First Principles: A Probabilistic Reformulation of Learn-            (ALT’2011), volume 6925 of Lecture Notes in Computer
   ing Classifier Systems from the Perspective of Machine               Science, pages 338–352. Springer.
   Learning. Technical report (University of Bath. Dept.             Sunehag, P. and Hutter, M. (2012a). Optimistic agents are
   of Computer Science). University of Bath, Department of              asymptotically optimal. In Proc. 25th Australasian Joint
   Computer Science.                                                    Conference on Artificial Intelligence (AusAI’12), volume
Evans, J. S. (2003). In two minds: dual-process accounts of             7691 of LNAI, pages 15–26, Sydney, Australia. Springer.
   reasoning. Trends in Cognitive Sciences, 7(10):454 – 459.         Sunehag, P. and Hutter, M. (2012b). Optimistic AIXI. In
Griffiths, T. L., Kemp, C., and Tenenbaum, J. B. (2008).                Proc. 5th Conf. on Artificial General Intelligence (AGI’12),
   Bayesian models of cognition. In Sun, R., editor, Cam-               volume 7716 of LNAI, pages 312–321. Springer, Heidel-
   bridge handbook of computational cognitive modeling,                 berg.
   pages 59–100. Cambridge University Press, Cambridge.              Sunehag, P. and Hutter, M. (2013). Learning agents with
Holland, J. (1986). Escaping brittleness: The possibilities             evolving hypothesis classes. In Proceedings of the 6th In-
   of general purpose learning algorithms applied to parallel           ternational Conference on AGI, volume 7999 of Lecture
   rule-based systems. In Michalski, R., Carbonell, J., and             Notes in Computer Science, pages 150–159. Springer.
   Mitchell, T., editors, Machine learning: An artificial intel-     Szita, I. and Lörincz, A. (2008). The many faces of opti-
   ligence approach, volume 2, chapter 20, pages 593–623.               mism: a unifying approach. In Proceedings of the 20th In-
   Morgan Kaufmann, Los Altos, CA.                                      ternational Conference on Machine Learning, pages 1048–
Hutter, M. (2005). Universal Articial Intelligence: Sequen-             1055.
   tial Decisions based on Algorithmic Probability. Springer,        Taylor, S. E. and Brown, J. D. (1988). Illusion and well-
   Berlin.                                                              being: a social psychological perspective on mental health.
                                                                        Psychological bulletin, 103(2):193.
Kahneman, D. (2011). Thinking, fast and slow. Farrar, Straus
   and Giroux, New York.                                             Wallston, K. A. (1994). Cautious optimism vs. cockeyed op-
                                                                        timism. Psychology & Health, 9(3):201–203.
Lattimore, T. (2013). Theory of General Reinforcement
   Learning (submitted). PhD thesis, Australian National
   University.
                                                                 2954

