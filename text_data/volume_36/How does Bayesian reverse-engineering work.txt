UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
How does Bayesian reverse-engineering work?
Permalink
https://escholarship.org/uc/item/61s9n028
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Zednik, Carlos
Jakel, Frank
Publication Date
2014-01-01
Peer reviewed
  eScholarship.org                                Powered by the California Digital Library
                                                                    University of California

                                   How does Bayesian reverse-engineering work?
                             Carlos Zednik (czednik@uos.de) and Frank Jäkel (fjaekel@uos.de)
                                          Institute of Cognitive Science, University of Osnabrück
                                                          49069 Osnabrück, Germany
                                Abstract                                  articulate and test possible algorithms and implementations
                                                                          of that phenomenon.
   Bayesian models of cognition and behavior are particularly
   promising when they are used in reverse-engineering                       What role do Bayesian models play in reverse-
   explanations: explanations that descend from the                       engineering explanations? It is widely agreed that Bayesian
   computational level of analysis to the algorithmic and                 models figure at the computational level of analysis. They
   implementation levels. Unfortunately, it remains unclear               help researchers understand what a cognitive system
   exactly how Bayesian models constrain and influence these              actually does, because they describe and predict its
   lower levels of analysis. In this paper, we review and reject          behavior. Moreover, these models allow researchers to
   two widespread views of Bayesian reverse-engineering, and
   propose an alternative view according to which Bayesian
                                                                          understand why a system does what it does, because they
   models at the computational level impose pragmatic                     show that the system’s behavior is an optimal solution to a
   constraints that facilitate the generation of testable hypotheses      particular statistical inference task. But how can Bayesian
   at the algorithmic and implementation levels.                          models at the computational level of analysis be used to
   Keywords: Bayesian modeling; rational analysis; reverse-
                                                                          identify algorithms and implementations at lower levels?
   engineering; Marr’s levels; mechanistic explanation                       In what follows, we review three different answers to this
                                                                          question. The first two answers—Bayesian Realism and
                           Introduction                                   Instrumentalist Bayesianism—are well-represented in the
                                                                          literature, but are ultimately unsatisfactory. Thus, we
Bayesian models describe cognitive and behavioral                         propose a third answer—Pragmatic Bayesianism—
phenomena as a form of optimal statistical inference. Using
                                                                          according to which Bayesian models are tools for
the methodology of rational analysis (Anderson, 1990),
                                                                          hypothesis generation: they facilitate the development of
researchers attempt to specify the statistical inference task to
                                                                          novel algorithmic-level and implementation-level analyses.
which a particular phenomenon is adapted. This task is
defined formally, in terms of a cognitive system’s prior
knowledge about its environment, recent evidence collected
                                                                                               Bayesian Realism
within that environment, hypotheses being compared, and                   According to Bayesian Realism, Bayesian models at the
the relative cost or benefit of particular actions. Once the              computational level of analysis contribute to reverse-
task has been defined in this way, the mathematical                       engineering explanations because their mathematical
framework of Bayesian decision theory can be used to                      structure is reflected in the functional and physical structure
derive an optimal solution to the task: how to ideally                    of the mechanisms described at the algorithmic and
adjudicate between hypotheses using Bayes’ rule to                        implementation levels. Insofar as a particular cognitive or
combine prior knowledge with recent evidence, and how to                  behavioral phenomenon can be modeled as a (nearly-)
select actions so as to minimize cost or maximize benefit. If             optimal solution to a statistical inference task, Bayesian
the task has been specified correctly, such optimal solutions             Realism implies that the mechanisms responsible for this
often provide descriptively adequate and predictively                     phenomenon themselves perform Bayesian inference. That
powerful models of the phenomenon being investigated.                     is, they execute algorithms that invoke (or closely
   Many researchers regard the methodology of Bayesian                    approximate) Bayes’ rule to combine prior knowledge with
modeling as a way to reverse-engineer the mind. In                        new evidence, and are implemented by neural structures that
cognitive science, reverse-engineering is often associated                represent prior and posterior probability distributions, as
with David Marr (1982), who proposed that cognitive                       well as likelihood and loss functions.
systems ought to be studied at three distinct levels of                      Two arguments speak in favor of Bayesian Realism. The
analysis. At the computational level, researchers seek to                 first argument is inspired by the classic “no-miracles”
understand what a system is doing and why. At the                         argument for scientific realism in philosophy of science.
algorithmic level, they describe how the system does what it              The no-miracles argument seeks to explain the observation
does. Finally, at the implementation level, they identify                 that many well-confirmed scientific theories are exceedingly
where in a particular physical system that algorithm is                   accurate descriptive and predictive devices. Barring
realized.      Reverse-engineering          explanations      involve     miracles, the best explanation seems to be that these theories
descending “a triumphant cascade” of these three levels                   are true: their theoretical posits successfully refer, and the
(Dennett, 1987, p. 227). That is, they begin with a                       structures they describe accurately reflect the structure of
computational-level analysis of a particular cognitive or                 the world. In much the same way, Bayesian Realism is
behavioral phenomenon, and invoke that analysis to                        motivated by the desire to explain the descriptive and
                                                                          predictive successes of Bayesian models at the
                                                                          computational level of analysis. Barring miracles, the best
                                                                      666

explanation seems to be that the mathematical structures and       processes or properties that, although not yet demonstrably
processes used to describe cognition and behavior at this          related to any particular cognitive or behavioral
level are reflected in the functional processes and physical       phenomenon, are suggestive of Bayesian inference.
structures at the algorithmic and implementation levels of            These arguments for Bayesian Realism promise a bright
analysis.                                                          future for reverse-engineering explanations in cognitive
   This argument is most clearly at work in current                science. This is because, if true, Bayesian Realism can be
neuroscientific research on perception. Following a series of      used to justify inferences from the mathematical structure of
psychophysical studies demonstrating that perceptual cue-          the cognitive, perceptual or behavioral task being solved to
combination is performed with near-optimal efficiency              the functional and physical structure of the mechanisms
(Ernst & Banks, 2002), neuroscientists have sought to              solving it. If it can be shown that overt behavior is a form of
identify the neural structures and processes responsible for       optimal statistical inference that combines evidence with
this efficiency. More often than not, the observed behavioral      prior probabilities and likelihood functions, Bayesian
optimality motivates the Bayesian Coding Hypothesis (Knill         Realism implies that the neural mechanisms responsible for
& Pouget, 2004), which claims that the relevant neural             this behavior will do so as well. Even before consulting the
structures and processes represent probability distributions,      neuroscience, Bayesian Realists have a pretty good
and combine these distributions by applying Bayes’ rule.           understanding of how the brain works!
Consider:                                                             Unfortunately, empirical support for Bayesian Realism is
                                                                   weak: critics have questioned the quality of evidence
   “Recent psychophysical experiments indicate that humans         typically cited in its favor. For example, Bowers & Davis
   perform near-optimal Bayesian inference in a wide               (2012) argue that Poisson-like variability and other neural
   variety of tasks, ranging from cue integration to decision      signatures of Bayesian inference are consistent with several
   making to motor control. This implies that neurons both         (non-Bayesian) alternatives, and moreover, suggest that Ma
   represent probability distributions and combine those           et al. over-estimate the prevalence of these signatures in the
   distributions according to a close approximation to Bayes’      brain. Similarly, Maloney & Mamassian (2009) demonstrate
   rule.” (Ma, Beck, Latham, & Pouget, 2006, p. 1432,              that many different algorithms can perform optimal
   emphasis added)                                                 Bayesian inference, though not all of them invoke Bayes’
                                                                   rule and represent prior probability distributions. In
How else, if not by representing probability distributions         particular, “any observer that can combine cues linearly and
and computing over them with (close approximations to)             somehow select the correct weights for the linear
Bayes’ rule, could this kind of behavioral optimality be           combination can duplicate the performance of the Bayesian
achieved?                                                          observer”—even a suitably rigged-up lookup table
   The second argument for Bayesian Realism cites the              (Maloney & Mamassian, 2009, p. 149).
relative ease by which Bayesian inference could be                    Without empirical support, the arguments favoring
implemented in the brain. Consider the idea of probabilistic       Bayesian Realism are unsound: it is no longer clear whether
population coding. Traditionally, it is thought that a             Bayesian inference really is as easy as Ma et al. contend,
population of neurons represents (in a distributed fashion)        and it is unclear whether Bayesian Realism really is the best
exactly one value, such as the direction of perceived motion.      (as opposed to a merely possible) explanation of the
It is not hard, however, to interpret the population as            descriptive and predictive success of Bayesian models.
representing a full probability distribution over the variable
in question. Thus, the neurons that have less probable                           Instrumentalist Bayesianism
characteristic stimuli fire less than neurons that represent       Bayesian Instrumentalism is the view that Bayesian models
more probable stimuli. On the assumption that neural               at the computational level are mere descriptive and
populations encode information probabilistically in this           predictive devices, and that they are compatible with a wide
way, it is also quite easy to explain how they might be            variety of algorithms and implementations at lower levels of
combined using Bayes’ rule. For example, if population             analysis. As Colombo & Series (2012) have already
codes exhibit Poisson-like variability—i.e. the ratio of spike     observed, many proponents of Bayesian modeling seem to
count to spike variance is near 1.0—Bayes’ rule can be             adopt such an instrumentalist perspective. In one of the
applied to them by simply adding or subtracting their              original discussions of rational analysis, John Anderson
activation levels (Ma et al., 2006). Notably, it has been          suggests that this methodology “provides an explanation at a
observed that sensory neuron populations do in fact exhibit        level of abstraction above specific mechanistic proposals”
Poisson-like variability (Tolhurst, Movshon, & Dean, 1983).        (Anderson, 1991, p. 471). Similarly, Griffiths et al. (2010)
   If Bayesian inference is so easy to implement, it would         argue that “Using probabilistic models to provide a
seem surprising to find that the brain—subject to countless        computational-level explanation does not require that
evolutionary and developmental constraints—does not                hypothesis spaces or probability distributions be explicitly
actually do so. Thus, Poisson-like variability and similar         represented by the underlying psychological or neural
measures of brain activity are sometimes referred to as            processes, or that people learn and reason by explicitly
signatures of Bayesian inference in the brain: neural
                                                               667

using Bayes’ rule” (Griffiths, Chater, Kemp, Perfors, &               one can be said to have understood it completely” (Marr,
Tenenbaum, 2010, p. 362).                                             1982, p. 24)—such an association would be surprising.
   The most compelling evidence favoring Bayesian                        But there are more significant worries than the false
Instrumentalism is the formal independence of levels. In one          specter of fundamentalism. According to Instrumentalist
oft-cited passage David Marr states:                                  Bayesianism, it is unclear that systematic reverse-
                                                                      engineering is possible: it would seem exceedingly unlikely
   “The three levels are coupled, but only loosely. The               that a “triumphant cascade” can be descended in a
   choice of an algorithm is influenced, for example, by              principled way. Although research into the neuroscientific
   what it has to do and by the hardware in which it must             underpinnings of Bayesian inference might be inspired by
   run. But there is a wide choice available at each level, and       the descriptive and predictive success of Bayesian models of
   the explication of each level involves issues that are rather      cognition and behavior, such research would not be justified
   independent of the other two.” (Marr, 1982, p. 25)                 by this success. Given the formal independence of levels,
                                                                      there is no reason to believe that the mathematical structure
It is a well-known mathematical fact that every function can          of Bayesian models at the computational level of analysis is
be computed by an infinite number of non-equivalent                   reflected at lower levels. Of course, the lower levels should
algorithms. Because Bayesian models specify mathematical              somehow compute and implement the function specified by
functions, they are compatible with any number of                     the Bayesian model, by mapping stimuli onto responses as
algorithms. Thus, although the algorithmic level of analysis          the model predicts. But there is no reason to believe that e.g.
is minimally constrained insofar as only those algorithms             neural populations encode loss functions, posteriors,
come into question that actually compute the function                 likelihoods and priors, as opposed to reproducing the
specified at the computational level, there are still an infinite     modeled stimulus-response behavior in some other way.
number of algorithms to choose from. In much the same                 Thus, even if future neuroscientific research were to
way, there are innumerable ways in which any particular               eventually confirm the Bayesian Coding Hypothesis, this
algorithm might be implemented in physical hardware.                  confirmation would not result from a systematic reverse-
Thus, the formal independence of levels implies that                  engineering effort.
developers of Bayesian models at the computational level of
analysis ought to be agnostic about the kinds of algorithms                           Pragmatic Bayesianism
and implementations that can be posited at lower levels.              Bayesian Realism and Instrumentalist Bayesianism are the
   Great care must be taken not to confuse agnosticism about          two most widely-held views on how Bayesian models at the
lower levels with a rejection of their explanatory relevance.         computational level relate to the algorithmic and
In an influential recent critique, Jones & Love (2011)                implementation levels of analysis. Unfortunately, neither
outline a position they disparagingly call Bayesian                   view accounts for the possibility of reverse-engineering
Fundamentalism. Like Instrumentalist Bayesianism, this                explanations in cognitive science. Whereas the arguments
position denies that Bayesian models at the computational             favoring Bayesian Realism are as of yet inconclusive due to
level constrain the lower levels of analysis. Rather than be          lack of empirical evidence, Instrumentalist Bayesianism
agnostic about these lower levels, however, Bayesian                  makes systematic reverse-engineering impossible.
Fundamentalists deny that lower levels of analysis are                   This section introduces an alternative view. According to
explanatorily relevant: “human behavior can be explained              Pragmatic Bayesianism, Bayesian models at the
through rational analysis…without recourse to process                 computational level make reverse-engineering possible by
representation, resource limitations, or physiological or             facilitating the generation of novel hypotheses at the
developmental data” (Jones & Love, 2011, p. 170). This                algorithmic and implementation levels of analysis. Although
radical position, Jones & Love argue, smacks of                       levels of analysis may be formally independent, they are
behaviorism, and ought to be avoided: “it would be a                  pragmatically dependent. If a particular cognitive or
serious overreaction simply to discard everything below the           behavioral phenomenon can be modeled as a form of
computational level. As in nearly every other science,                Bayesian inference, it will be considerably easier to identify
understanding how the subject of study (i.e., the brain)              possible algorithms to perform this kind of inference, and to
operates is critical to explaining and predicting its behavior”       identify ways in which these algorithms might be
(Jones & Love, 2011, p. 177, original emphasis).                      implemented in physical hardware. How so? Because
   The most common response to this worry has been to                 practicing researchers are (a) guided by pragmatic
deny that proponents of Bayesian modeling in cognitive                considerations such as their interdisciplinary colleagues’
science are correctly associated with Bayesian                        previous research activity, ingenuity and communicative
Fundamentalism. In a direct response to Jones & Love’s                ability, and (b) influenced in their scientific decision-
target article, Chater et al. characterize Bayesian                   making by the conceptual and theoretical framework of
Fundamentalism as “purely a construct of Jones & Love’s               Bayesian statistical inference.
imagination” (Chater et al., 2011, p. 194). Indeed, given                An effective segue into Pragmatic Bayesianism is
their intellectual debt to David Marr—who stresses that a             Colombo & Series’ defense of Instrumentalist Bayesianism.
cognitive system must be studied at all three levels “before          Although they do not identify it as such, Colombo & Series
                                                                  668

describe one important pragmatic influence on reverse-                Researchers working in the discipline of artificial
engineering: “the predictive success of a Bayesian model in        intelligence (including machine learning and statistics),
a given psychophysical task can motivate us to investigate         have developed many different algorithms for optimally and
why this is the case” (Colombo & Series, 2012, p. 17,              efficiently computing or approximating Bayesian inference,
original emphasis). Undeniably, researchers’ motivations           only a limited number of which directly apply Bayes’ rule to
critically influence the development of algorithms and             full probability distributions. It seems natural to wonder
implementations for a particular kind of Bayesian inference.       whether algorithms already developed for theoretical
At the same time, however, Colombo & Series claim that             reasons or real-world applications might serve double-duty
“the discovery that people behave as though they were              in cognitive science. As the series of articles on
Bayesian observers does not compel us to make any specific         categorization demonstrates, describing a particular
claim at the neural level of implementation” (Colombo &            cognitive or behavioral phenomenon as a form of Bayesian
Series, 2012, p. 17). The supposed reason for this is the          statistical inference at the computational level allows
aforementioned formal independence of levels. However,             researchers in cognitive science to consider existing
although there may be no theoretical limit to the number of        artificial intelligence research not just for motivation in the
algorithms that compute a particular mathematical function,        way suggested by Colombo & Series, but for articulating
pragmatic considerations impose considerable limits on the         testable hypotheses at the algorithmic level of analysis. As
number of algorithms and implementations that will actually        is exemplified by the particle filtering algorithm advanced
be considered. Importantly, although these algorithms and          by Sanborn et al., these hypotheses need not reflect the
implementations might reflect the mathematical structure of        mathematical structure of Bayesian models at the
Bayesian models at the computational level, they need not          computational level of analysis.
do so.                                                                There is a clear sense in which any pragmatic
                                                                   consideration that contributes to the generation of testable
Constraints on algorithm-development                               hypotheses might be thought to facilitate reverse-
Consider recent attempts to develop algorithmic-level              engineering explanations in cognitive science. At the same
analyses to accompany John Anderson’s (1991) rational              time, recall that one of the worries about Instrumentalist
analysis of categorization. One such analysis is developed         Bayesianism was that, although lower-level analyses may be
by Anderson himself, and centers on “a type of iterative           inspired by Bayesian models at the computational level,
algorithm that has appeared in the artificial intelligence         they are not justified by these models. In what sense are
literature” (Anderson, 1991, p. 412). By reviewing the             psychologists justified in invoking algorithms developed by
categorization literature of the time, Anderson shows that         artificial intelligence researchers who are unconcerned with
the iterative algorithm accurately predicts qualitative and        matters of psychological and biological plausibility?
quantitative human data. Moreover, Anderson suspects (but             A useful framework for answering this question is Herbert
does not prove) that the iterative algorithm closely               Simon’s influential account of scientific discovery (Simon,
approximates the optimal assignment of objects to                  Langley, & Bradshaw, 1981). Simon views scientific
categories within the constraints of the task environment.         discovery as a form of problem-solving, in which
Sanborn et al. (2010) later demonstrate that although the          researchers are tasked with exploring the conceptual space
iterative algorithm approximates optimal Bayesian inference        of possible solutions to a particular scientific problem.
in the task environments Anderson considers, there is no           Because this space is often vast and multidimensional,
guarantee that it will do so in general. Thus, Sanborn et al.      researchers rely on heuristic strategies that highlight
present two alternative algorithms—particle filtering and          particular areas within the space to the exclusion of others,
Gibbs sampling—both of which “can approximate the                  thereby limiting the number of possible solutions they
optimal inference to any desired level of precision”               actually need to consider. Although these heuristic strategies
(Sanborn et al., 2010, p. 1145). Ultimately, by comparing all      are fallible—they might erroneously highlight an irrelevant
three candidate algorithms to experimental data, Sanborn et        area within the space or exclude a relevant one—their use is
al. propose particle filtering as the most plausible               justified insofar as they allow researchers to efficiently and
algorithmic-level analysis of human categorization.                systematically traverse the space of possible solutions to a
   Two things are worth noticing about this series of articles     particular scientific-discovery problem.
(See also: Griffiths, Vul, & Sanborn, 2012). First, each one          The appeal to existing research in artificial intelligence,
of the three proposed algorithms is adapted or                     statistics, and machine learning that is facilitated by
straightforwardly coopted from existing research in the            Bayesian models in cognitive science can be understood as
discipline of artificial intelligence. Second, although each       a heuristic strategy of this kind. Modeling a particular
one of these algorithms approximates Anderson’s model of           cognitive or behavioral phenomenon as a form of Bayesian
categorization, neither one of them requires explicit              inference is tantamount to defining a particular scientific-
representations of the full hypothesis space, prior                discovery problem: the problem of selecting, from among
probability distributions and likelihoods, nor directly            the set of algorithms that possibly perform or approximate
invokes Bayes’ rule to compute over these representations.         such inference, the algorithm that actually does so in the
                                                                   particular cognitive system being studied. Unfortunately,
                                                               669

because every function can be computed by an infinite               sampling, Fiser et al. advance an interpretation according to
number of algorithms, the solution-space is infinite in             which “a very large component of high spontaneous activity
expanse. Nevertheless, by appealing to the existing                 is probably not noise but might have a functional role in
literature in artificial intelligence, researchers can              cortical computation” (Fiser et al., 2010, p. 125). Thus,
concentrate their efforts on particular regions of the space—       because they adopt a theoretical perspective that is
those regions that have already been explored in theoretical        “colored” by a particular class of algorithms, Fiser et al.
work or real-world applications. Because only a limited             arrive at a very different way of describing particular neural
number of algorithms have actually been articulated and             structures and processes. Indeed, on their interpretation,
studied, researchers in cognitive science are able to select        spontaneous neural activity is not merely a neural signature
from (and if necessary adapt) a handful of well-understood          of Bayesian inference, but of Bayesian inference by way of
alternatives. Interestingly, this means that the reverse-           Monte Carlo sampling. Insofar as Bayesian models at the
engineering explanations in cognitive science are                   computational level suggest Monte Carlo sampling (or more
constrained in an irreducibly pragmatic way, by the research        specifically according to Sanborn et al., particle filtering) as
output of other scientific disciplines.                             a possible algorithmic-level account of behavior and
                                                                    cognition, these models also indirectly suggest particular
Constraints on implementation-description                           ways of interpreting, individuating and describing certain
Bayesian models at the computational level of analysis also         neurobiological structures and processes.
pragmatically constrain the implementation level of                    Bayesian models at the computational level may also
analysis. In order to provide an analysis of implementation,        influence the implementation level quite directly. In recent
(neuro-)scientists must identify and describe the particular        philosophical research on mechanistic explanation in
physical structures and processes which realize the                 neuroscience, Carl Craver (2013) identifies three ways in
algorithm that computes a particular mathematical function.         which neuroscientists’ decision-making is influenced by
In order to do so, they have several decisions to make: what        available characterizations of a mechanism’s function. First,
are the relevant physical structures and processes? Which           mechanisms are defined in functional terms: they are always
aspects of these structures and processes should be                 mechanisms for something. Thus, neurotransmitters are
emphasized? How should they be described? Bayesian                  “used to send signals from one cell to another” (Craver,
models at the computational level often directly influence          2013, p. 135), much like soda machines are used to dispense
the outcome of these decisions, but also influence them             cans of soda in exchange for money. Second, mechanisms
indirectly, by way of the algorithmic level.                        are typically delineated by appealing to functional
   As the previous discussion shows, Bayesian models at the         characterizations which serve to distinguish a mechanism
computational level pragmatically constrain the selection of        from its background or environment. In Craver’s words:
algorithms at the algorithmic level of analysis. In turn, the
algorithms considered at this level influence the description           “it takes considerable scientific effort, abstraction, and
of implementing neurobiological mechanisms. Consider                   idealization to distinguish components from contraband,
once again the particle filtering algorithm proposed by                activities from incidental interactions, and causes from
Sanborn et al. (2010). Particle filtering is an example of a           background conditions. And this filtering process requires
general class of algorithms known as Monte Carlo                       (essentially) fixing on some behavior, process, or function
sampling. Recently, Fiser et al. (2010) have appealed to this          for which a mechanistic explanation will be sought”
class of algorithms to interpret spontaneous neural activity           (Craver, 2013, p. 140).
in the absence of sensory stimulation:
                                                                    Third and finally, the way mechanisms are decomposed also
   “Under a sampling-based representational account,                typically relies on characterizations of function. Following
   spontaneous activity could have a natural interpretation.        Craver, such characterizations determine the particular
   In a probabilistic framework, if neural activities represent     physical structures and processes that are actually relevant
   samples from a distribution over external variables, this        to the production of the phenomenon being investigated.
   distribution must be the so-called ‘posterior distribution’.        Notably, each one of these three constraints is pragmatic
   The posterior distribution is inferred by combining              in character: it concerns influences on a researcher’s
   information from two sources: the sensory input, and the         decision-making, focus of research, and descriptive
   prior distribution describing a priori beliefs about the         emphasis. Although the neural structures and processes that
   sensory environment. Intuitively, in the absence of              compose a mechanism are real things in the world, the
   sensory stimulation, this distribution will collapse to the      particular way in which they are described is invariably tied
   prior distribution, and spontaneous activity will represent      to previously available characterizations of function. Now,
   this prior.” (Fiser et al., 2010, pp. 125–127)                   recall that Bayesian models figure at Marr’s computational
                                                                    level not just because they allow researchers to describe
The presence of spontaneous neural activity has long been           what a cognitive system actually does, but also because they
interpreted as stochastic noise (Tolhurst et al., 1983). In         help them understand why the system behaves as it does.
contrast, by appealing to the framework of Monte Carlo              Specifically, Bayesian models show that the system behaves
                                                                670

as it does because this particular behavior is an optimal          Chater, N., Goodman, N., Griffiths, T. L., Kemp, C.,
solution to the task environment within which the system is           Oaksford, M., & Tenenbaum, J. B. (2011). The
situated. Thus, Bayesian models seem ideally suited for               imaginary fundamentalists: The unshocking truth about
imposing the kinds of pragmatic constraints on                        Bayesian cognitive science. Behavioral and Brain
implementation identified by Craver.                                  Sciences, 34, 194–196.
   Consider again the work in theoretical neuroscience             Colombo, M., & Series, P. (2012). Bayes in the Brain--On
discussed in the context of Bayesian Realism above. Much              Bayesian Modelling in Neuroscience. The British
of this research is inspired by the descriptive and predictive        Journal for the Philosophy of Science, 63(3), 697–723.
success of Bayesian models in cognitive psychology and                doi:10.1093/bjps/axr043
psychophysics. Notably, this success not only motivates            Craver, C. F. (2013). Functions and Mechanisms: A
neuroscientists to look for possible neural implementations           Perspectivalist View. In P. Huneman (Ed.), Functions:
of Bayesian inference, but also regularly suggests the                Selection and Mechanisms (pp. 133–158). Dordrecht:
particular form these implementations might take: the                 Springer.
functional and physical structure of mechanisms at the             Dennett, D. C. (1987). The Intentional Stance. Cambridge,
implementation level is assumed to reflect the mathematical           MA: MIT Press.
structure of Bayesian models at the computational level.           Ernst, M. O., & Banks, M. S. (2002). Humans integrate
Thus for example, in a passage already quoted above, Ma et            visual and haptic information in a statistically optimal
al. (2006) claim that the descriptive success of Bayesian             fashion. Nature, 415(January), 429–433.
models “implies that” neurons represent probability                Fiser, J., Berkes, P., Orban, G., & Lengyel, M. (2010).
distributions and implement Bayes’ rule.                              Statistically optimal perception and learning: from
   Although this kind of research has yet to provide                  behavior to neural representations. Trends in Cognitive
conclusive evidence in favor of the Bayesian Coding                   Sciences, 14(3), 119–130. doi:10.1016/j.tics.2010.01.003
Hypothesis, it confirms Craver’s philosophical analysis.           Griffiths, T. L., Chater, N., Kemp, C., Perfors, A., &
Specifically, it shows that characterizations of function—in          Tenenbaum, J. B. (2010). Probabilistic models of
this case, Bayesian models—influence neuroscientists’                 cognition: exploring representations and inductive
decisions about how to define, delineate, and decompose               biases. Trends in Cognitive Sciences, 14(8), 357–364.
mechanisms. Thus, Bayesian models at the computational                doi:10.1016/j.tics.2010.05.004
level of analysis directly influence the implementation level      Griffiths, T. L., Vul, E., & Sanborn, A. N. (2012). Bridging
by suggesting possible ways of interpreting the activity of           Levels of Analysis for Probabilistic Models of
certain neural mechanisms, but also by suggesting which               Cognition. Current Directions in Psychological Science,
particular neural structures and processes to include in              21(4), 263–268. doi:10.1177/0963721412447619
descriptions of these mechanisms. Because Bayesian                 Jones, M., & Love, B. C. (2011). Bayesian Fundamentalism
models at the computational level pragmatically constrain             or Enlightenment? On the explanatory status and
algorithmic and implementation level analysis, they are a             theoretical contributions of Bayesian models of
viable starting point for reverse-engineering explanations in         cognition. Behavioral and Brain Sciences, 34, 169–231.
cognitive science.                                                 Knill, D. C., & Pouget, A. (2004). The Bayesian brain: the
                                                                      role of uncertainty in neural coding and computation.
                        Conclusion                                    Trends in Neurosciences, 27(12), 712–9.
Although Bayesian Realism makes reverse-engineering                Ma, W. J., Beck, J. M., Latham, P. E., & Pouget, A. (2006).
explanations easy, empirical support for this position is             Bayesian inference with probabilistic population codes.
weak. Many practicing researchers have therefore endorsed             Nature Neuroscience, 9(11), 1432–1438.
Instrumentalist Bayesianism. Unfortunately, this position          Maloney, L. T., & Mamassian, P. (2009). Bayesian decision
makes systematic reverse-engineering impossible. Unlike               theory as a model of human visual perception: Testing
these more established alternatives, Pragmatic Bayesianism            Bayesian transfer. Visual Neuroscience, 26, 147–155.
both provides a satisfying account of scientific practice and      Marr, D. (1982). Vision. New York, NY: Henry Holt & Co.
allows for systematic reverse-engineering in cognitive             Sanborn, A. N., Griffiths, T. L., & Navarro, D. J. (2010).
science.                                                              Rational Approximations to Category Learning.
                                                                      Psychological Review, 117(4), 1144–1167.
                                                                   Simon, H. A., Langley, P. W., & Bradshaw, G. L. (1981).
                         References                                   Scientific Discovery as Problem Solving. Synthese,
Anderson, J. R. (1990). The Adaptive Character of Thought.            47(1), 1–27.
    Hillsdale, NJ: Erlbaum.                                        Tolhurst, D. J., Movshon, J. A., & Dean, A. F. (1983). The
Anderson, J. R. (1991). The Adaptive Nature of Human                  statistical reliability of signals in single neurons in cat
    Categorization. Psychological Review, 98(3), 409–429.             and monkey visual cortex. Vision Research, 23(8), 775–
Bowers, J. S., & Davis, C. J. (2012). Bayesian just-so stories        785.
    in psychology and neuroscience. Psychological Bulletin,
    138(3), 389–414.
                                                               671

