UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
TRACX 2.0: A memory-based, biologically-plausible model of sequence segmentation and
chunk extraction
Permalink
https://escholarship.org/uc/item/0zv7096m
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
French, Bob
Cottrell, Gary
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

            TRACX 2.0: A memory-based, biologically-plausible model of sequence
                                           segmentation and chunk extraction
                                      Robert M. French (robert.french@u-bourgogne.fr)
                                           LEAD-CNRS UMR 5022, Université de Bourgogne
                                                             21000 Dijon, France
                                               Garrison W. Cottrell (gary@ucsd.edu)
                                                Computer Science and Engineering, UCSD
                                                       La Jolla, CA 92093-0404, USA
                             Abstract                                    predictions, they must gradually learn the probabilities of
  TRACX (French, Addyman, & Mareschal, 2011) is a
                                                                         successive events in the world. We learn that a flash of
  recursive connectionist system that implicitly extracts chunks         lightning will invariably be followed by a clap of thunder,
  from sequence data. It can account for experiments on infant           that a “hello” will usually be reciprocated, that a phone call
  statistical learning and adult implicit learning, as well as real-     will sometimes be for us, but sometimes not, that the
  world phoneme data, and an experiment using backward                   flashing light on a police car will usually be for someone
  transitional probabilities that simple recurrent networks              else, but occasionally for us, and so on.
  cannot account for. One criticism of TRACX, however, is the               This is the basis of the transitional probability (TP)
  implausibility in a connectionist model of if-then-else
  statements. In particular, one of these statements controls            theory of sequence segmentation. The idea is simple. In the
  what data is copied from the model’s internal memory into its          syllable stream that an infant hears, many multi-syllable
  input, based on a hard error threshold. We, therefore,                 words will be repeated frequently (e.g., bay-bee, mah-mee,
  developed a more biologically-plausible version of TRACX               bah-tul, and so on) and, as a result, the infant will become
  devoid of if-then-else statements, relying only on spreading           better at predicting upcoming within-word syllables
  activation and without any learning error threshold. This new          compared to upcoming between-word syllables. (The
  model, TRACX 2.0, performs essentially as well as the
                                                                         syllable pair bay-bee will be followed by the initial syllable
  original TRACX model and, in addition, has two fewer
  parameters than the original and accounts for the graded               of many different words, whereas as bay will be very
  nature of chunks.                                                      frequently followed by bee. The infant thus learns the word
                                                                         bay-bee.) Thus, low syllable-to-syllable TPs (failures to
   Keywords: chunk extraction; statistical learning; implicit
                                                                         predict) indicate word boundaries. High syllable-to-syllable
   learning; recursive autoassociative memory; autoassociators.
                                                                         TPs bind syllables together into words and facilitate their
   .                                                                     learning. An obvious connectionist candidate for this kind
                                                                         of transitional-probability based learning is the well-known
                          Introduction                                   Simple Recurrent Network (SRN, Elman, 1990).
No one disputes that individuals learn to extract structure                 While we don’t doubt that prediction is an important
from their sensory environment. There is, however, a heated              aspect of cognition, there are other plausible explanations as
debate is over just how this is done. In what follows we will            to how infants (and adults) learn to segment continuous
suggest a neurobiologically plausible, memory-based model                speech streams into words. Broadly speaking, there are four
that achieves this in the auditory domain. The model                     classes of models used to explain sequence segmentation
provides a strong hypothesis as to how people -- infants, as             and word extraction. These are:
well as adults -- might segment continuous syllable streams                 - Predictive connectionist models, most prominent
into words. The model is an improvement of a recent                           among them the SRN (Elman, 1990; Cleeremans &
connectionist       memory-based         model       of     sequence          McClelland, 1991; Servan-Schreiber, Cleeremans, &
segmentation and chunking, TRACX (French, Addyman, &                          McClelland, 1991);
Mareschal, 2011). The new model improves TRACX by                           - Chunking connectionist models, i.e., TRACX (French,
removing a crucial if-then-else statement in the model and                    et al., 2011);
replaces it with a simple connectionist mechanism.                          - Symbolic hybrid models, the best known of which are
  The mainstream view of how segmentation is done, one                        probably PARSER (Perruchet & Vinter, 1998, 2002)
that has held sway for the nearly two decades, is based on                    and the Competitive Chunker (Servan-Schreiberr &
the notion of prediction. This theory supposes that                           Anderson, 1990)
individuals, based on their previous experience with the                    - Normative statistical models (Frank, Goldwater,
world, are constantly in the process of making predictions                    Griffiths & Tenenbaum, 2010; Goldwater, Griffiths, &
about what is going to happen next in their environment. In                   Johnson, 2009; Börschinger, & Johnson, 2011).
so doing, they gradually learn to align their predictions with
what actually happens in the world. In order to make these               Recently, Kurumada, Meylan, and Frank (2013) ran a series
                                                                     2216

of tests on models from each of these classes and found that            representations of their input (hence these models are also
“computational models that implement ‘chunking’ are more                called autoencoders).
effective than ‘transition finding’ models” at reproducing                 Today, the psychological and biological plausibility of
segmentation in a context where the frequency of words                  autoassociation is widely accepted (Rolls & Treves, 1997).
followed a Zipfian distribution (e.g., words in real natural            Autoassociators have been successfully used as psycho-
language). TRACX was singled out as the model that best                 biologically plausible models in many areas of cognition.
captured human word-segmentation performance in a                       For example, Mareschal, French, & Quinn (2000) and
Zipfian context.                                                        French, Mareschal, Mermillod & Quinn (2004) developed
    However, even though French et al. (2011) criticize the             an autoassociator model of infant categorization based on
lack of neurobiological plausibility of competing non-                  the autoassociative principles of Sokolov (1963) and others.
connectionist models of sequence segmentation, one of the               Other psycho-biologically plausible models using
key features of their own model undermines its claim to                 autoassociators include models of face perception (Cottrell
neurobiological plausibility. This feature (an if-then-else             & Metcalfe, 1991), of hippocampal/episodic memory
switch) plays a crucial role in ensuring that the network can           (Metcalfe, Cottrell & Mencl, 1992; Gluck & Granger,
re-use syllable chunks that it has detected in the input. In            1993), of serial recall memory (Farrell & Lewandowsky,
what follows we show that this flaw can be overcome and                 2002), and infant habituation (Sirois & Mareschal, 2004).
develop a new, simpler implementation of the original
TRACX model, which we call TRACX 2.0. This modified                                   The Architecture of TRACX
version of TRACX not only replaces the problematic feature              The original TRACX autoassociator is constructed as
with a simple, neurobiologically sound mechanism, but also              follows. The input layer is divided into a Left-Hand Side
requires two fewer parameters than the original model. We               (LHS) and a Right-Hand Side (RHS), each with the same
also show that TRACX 2.0 produces qualitatively the same                number of units. Being an autoassociator, it, of course, has
results as the original TRACX model on five datasets for                the same number of inputs and outputs; being a RAAM, the
infants and adults.                                                     hidden layer has half as many units as the input layer, which
                                                                        allows the hidden layer to be copied back to the input layer
                           TRACX                                        and combined with the next input. Aside from the potential
The architecture of the TRACX model is explained in detail              copy-back, the network is fully feedforward. The weights
in French et al. (2011). Here we present a brief summary of             are changed by standard backpropagation based on the error
the architecture.                                                       between the input and output. Learning stops when the
    TRACX is a member of the Recursive Auto-Associative                 network is below an error threshold of 0.4. The input data
Memory (RAAM) family of connectionist architectures                     was encoded with bipolar units (-1 and 1).………...………..
(Pollack, 1990; Blank, Meeden & Marshall, 1992). It is a
three-layer         (input-hidden-output)           connectionist
autoassociator whose key ability is to learn to recognize
when it has seen pairs of input items before.
   Autoassociators gradually learn to produce output that is
identical to their input. This means that items that they have
seen frequently on input will be accurately reproduced on
output, unlike items that have not been seen by the
autoassociator before, or have only been seen infrequently.
This provides the autoassociator with a simple way of
determining whether or not it has previously encountered
the vector of values currently on its input: if the output is
very different from the input, it is novel. If it is very close, it
is known. This signal is also the error signal that drives the
weight changes, making the output more similar to the
input.                                                                   Figure 1. TRACX architecture: a 2N-N-2N autoassociator.
Plausibility of Autoassociation                                                           How TRACX Works
Autoassociators have a long history in the computational                The easiest way to explain the architecture of TRACX is by
modeling of cognition. The first model to make a lasting                means of an example. Assume we have a language that
mark was Anderson’s Brain State in a Box (BSB) model                    consists of four 3-syllable words made of distinct syllables:
(Anderson, Silverstein, Ritz and Jones, 1977). This model               abc, def, ghi, and jkl. We then present to the network a
had no hidden layer and could not learn internal                        continuous syllable stream made up of these words:
representations of its input. Ackley, Hinton, and Sejnowski                        abcjkldefghidefabcdefabcghiabcdefabc...
(1985) were the first to add a hidden layer to their                    These syllables are read by TRACX in sequential order. So,
autoassociators, thereby allowing them learn compact                    (the bipolar encoding of) a is put into the LHS and b into
                                                                    2217

the RHS. From the input layer, activation spreads forward to         two involving adults, words are learned significantly better
the output layer. The difference between the input and the           than partwords.
output determines the error signal, and the weights of the               After the model has been trained, it is tested on a stream
system are modified by backpropagation based on this error.          of data similar to the one it was trained on (as are humans
Initially, this error will be high. If the error is above            and babies). There are numerous ways in which the output
threshold (in this case, 0.4), the value in the RHS will be          error of words and partwords could have been calculated.
shifted into the LHS and the next syllable -- in this case, c --     The one we chose is as follows. An item (in this case, a 3-
will be shifted into the RHS. If, on the other hand, the             syllable word or partword), say abc (or partword, cde), is
network has seen the a-b input many times, its output will           given to the network. a and b are put on the input. This
be close to a-b, and the error will be small. The fact that a        input is fed through to the hidden layer, which produces a
                                                                     vector of hidden-unit activations. For words/partwords
                                                                     longer than two syllables, as is the case here, this hidden-
                                                                     unit vector is then put in the LHS of the network and c is put
                                                                     in the RHS. This is then fed through to the output, and the
                                                                     maximum of the absolute values of the error across all
                                                                     output nodes is the error measure for item abc.
                                                                                        Improving TRACX
                                                                     The problem with the original TRACX model is that if-
                                                                     then-else statements are not palatable in a connectionist
                                                                     context where it is unclear how such a branching behavior
                                                                     can be implemented (but such behavior can be learned - see
                                                                     Cottrell & Tsung, 1993).
                                                                         The operation of the original TRACX model cannot
                                                                     function without two conditionals in its midst. The first is
                                                                     by far the most important:
                                                                     IF        Network error is greater than the Error Threshold,
                                                                     THEN Put the element in the RHS in the LHS
                                                                     ELSE Put the hidden unit vector into the LHS.
   Figure 2. TRACX behavior at t+1 depends on error at t.
                                                                     The network then grabs the next element in the sequence
and b occur together many times in the input is another way          and puts it into the RHS and feeds what is on the input layer
of saying that a-b form a chunk. Once the network                    through to the output layer of the network. At this point the
“recognizes” that a-b is a chunk (because the                        second conditional is applied:
autoassociative error for a-b is below the error threshold),
                                                                     IF        LHS contains the previous Hidden-unit vector
its behavior changes. On the following time step, instead of
                                                                     THEN Do a backpropagation pass 25% of the time
putting b into the LHS, it puts the hidden unit
                                                                     ELSE Do a backpropagation pass.
representation of a-b (call it Hab) into the LHS. As before, it
puts the next syllable, c, into the RHS (Figure 2). Now it           This was a simple means of ensuring that the network, like
attempts to autoassociate Hab-c. Since, in fact, it will see a-      people, place less emphasis on internally generated input
b-c many times, it will eventually learn to autoassociate            compared to input from the sensory interface. In TRACX
(and chunk) Hab-c. It will not chunk further because c can be        2.0 neither of these statements is necessary.
followed by any one of four different syllables, so the error
will always be high.                                                                  TRACX without Tears
    This switch in behavior based on the error threshold is          The improved version of TRACX is based on a simple
the if-then-else we would like to eliminate in our new               observation concerning the graded nature of chunking. In
version of the model.                                                the previous version of TRACX, the two elements on the
                                                                     input were either considered by the network to be chunked
Testing TRACX: words vs. partwords                                   (if the error on output was below the Error Threshold) or
Words are syllable groups that are “bound together” as a             they were not. There was no middle ground. But this is
chunk. On the other hand, partwords are typically made up            cognitively unrealistic, since, in fact, chunks are graded. By
of the final syllable of one word and the initial syllable(s) of     this we mean that some chunks are more “chunked” than
another word. It turns out that humans - infants and adults -        others. To illustrate this, consider some chunks made, not
learn to segment words from a continuous syllable stream             from syllables, but from words. For example, almost no one
better than partwords. In the example in the previous                hears the component words “cup” and “board” in the word
section, abc, def, ghi, and jkl are words, whereas cjk, lde,         “cupboard”. The word has been with us since the late 14th
fgh, ide, cde, cgh, etc. are partwords. In all five of the           century when it meant a board on which cups and other
experiments we model below, three involving infants and              similar objects were placed. But over the course of 500
                                                                 2218

years, the two words, “cup” and “board”, have fused so              backward TPs, and French et al. (2008), Equal TPs. In all of
completely that we no longer hear them as separate entities.        these experiments with human participants, words were
On the other hand, newer compound words, such as                    learned better than partwords. This is also the case for both
“smartphone”, “mousepad”, or “congresswoman” are at the             TRACX and TRACX 2.0, but not the case for the SRN.
other extreme: these words are weakly chunked; we still
clearly hear their component words. These words are far             Saffran, Aslin & Newport (1996) This is the seminal paper
less strongly chunked than words like “breakfast”,                  in infant syllable-sequence segmentation. Six different
“football” or “cupboard”.                                           words were used, each with 3 distinct syllables from a 12-
                                                                    syllable alphabet. A random sequence of 90 of these words
                                                                    (270 syllables) with no immediate repeats or pauses
                                                                    between words was presented twice to 8-month-old infants.
                                                                    After this familiarization period, the infants heard a word
                                                                    from the familiarization sequence and a partword from that
                                                                    sequence. A head-turn preference procedure was used to
                                                                    show that infants had a novelty preference for partwords.
                                                                    The conclusion of the authors was that the infants had
                                                                    learned words better than partwords.
                                                                        We simulated this experiment with TRACX, TRACX
        Figure 3. Information transfer in TRACX 2.0.                2.0 and an SRN using the same number of words drawn
                                                                    from a 12-syllable alphabet. The familiarization sequence
    Thus, beyond the neurobiological implausibility of the          was the same length as the one the infants heard. All three
if-then-else statement in the original TRACX, the                   models learned words better than partwords, although the
dichotomous nature of chunks is dubious. We have changed            SRN is considerably farther from human performance than
this in TRACX 2.0. In the new model the contents of the             TRACX or TRACX 2.0 (Table 1).
LHS at time t+1 is a weighted sum of the hidden-layer
vector and the RHS:                                                 Aslin, Saffran & Newport (1998) In Saffran et al. (1996)
     LHS =−  (1 tanh(∆) ) * Hid + tanh(∆) * RHS                     there was a confound -- namely, words were heard three
                                                                    times as often as partwords. Aslin et al. designed an
where Δ is the absolute value of the maximum (component-            experiment that was meant to remove the unbalanced
wise) error on the output (hence it ranges from 0 to 2, and,        frequency of words and partwords. There were four 3-
therefore, tanh(Δ) ranges between 0 to 1) at time t. If Δ is        syllable words, two of which occurred twice as often in the
small (“I’ve seen these two items together in the input             familiarization sequence as the other two. Thus, the
numerous times before”), then the contribution from the             partwords spanning the two high-frequency words would
hidden layer will be large. If Δ is large, most of the              have the same overall frequency in the familiarization
contribution to the LHS will come almost exclusively from           sequence as the low-frequency words. The same head-turn
the RHS (Figure 3). This can easily be implemented via a            preference procedure showed, again, that infants had a
unit that takes Δ as input, and then multiplicatively gates the     novelty preference for partwords. The conclusion of the
connections to the LHS (positively with the RHS, and                authors was that the infants had learned words better than
negatively with a bias of 1 on the hidden layer).                   partwords.
    This weighted sum of activation sent to the LHS                     Once again, we designed a set of words exactly like
removes the problematic if-then-else statements in the              those used in Aslin et al. The length of the familiarization
original TRACX, and implements the graded notion of                 sequence was also identical to that used in Aslin et al. We
chunks. In addition, we have found that modifying the               tested TRACX, TRACX 2.0 and an SRN on words and
amount of backpropagation, as in the original TRACX                 partwords from this sequence, and found that all three
model is unnecessary. Chunks become stronger over time              networks learned words better than partwords, although the
the more they are encountered, as we know occurs in                 SRN is, again, farther from human performance than either
humans (perhaps our children do not hear “smart” and                TRACX or TRACX 2.0 (Table 1).
“phone” when they refer to their “smartphone”, but those of
us of a certain age still do).                                      Perruchet & Desaulty (2008), forward TPs This is an
                                                                    experiment on adults. Nine 2-syllable words were
                   Testing TRACX 2.0                                constructed from 12 syllables. The familiarization string
    We tested TRACX 2.0 on five of the data sets on which           was 1035 words long, and each word occurred 115 times.
the original TRACX model was tested (see French et al.,             The internal forward transitional probability between
2011). In what follows we will briefly consider each of             syllables in each word was 1. Not surprisingly, participants
these data sets and discuss the performance of TRACX,               learned words better than partwords. We simulated this
TRACX 2.0, and an SRN on this data. These experiments               experiment by using 2-syllable words drawn from a 12-
are: Saffran et al. (1996), Aslin et al. (1998), Perruchet &        syllable alphabet to construct a familiarization sequence
Desaulty (2008), forward TPs, Perruchet & Desaulty (2008),          identical in length to the one used by Perruchet & Desaulty.
                                                                2219

TRACX, TRACX 2.0 and the SRN learned words better                         We encoded the Perruchet & Desaulty vocabulary and
than partwords. Again, the performance of the SRN was the             generated sequences identical to theirs in which the word
farthest from human data (Table 1).                                   chunking cues were exclusively the backward TPs between
                                                                      the two syllables of the words. Since SRNs are sensitive
Perruchet & Desaulty (2008), Backward TPs This                        only to forward prediction, we predicted that the SRN
experiment, run on adults, is of crucial importance.                  would fail on this data set. This proved to be the case. For
Perruchet & Desaulty were the first to realize that backward          this data the SRN learned partwords significantly better than
TPs could serve as a segmentation cue. To illustrate the              words. On the other hand, both TRACX and TRACX 2.0,
contrast between backward and forward TPs, consider the               once again, recognized words better than partwords (Table
bigram qu in English. Given a q, the probability that it will         1). The reason for this is clear. Both of these models rely on
be followed by a u is, essentially, 1. However, given a u, the        the recognition of previously seen chunks. They are not
probability that it will be preceded by a q (i.e., the backward       concerned with TPs, whether forward or backward, between
TP) is only 0.01. Backward TPs can, in some cases, actually           the syllables comprising a word. They rely on remembering
be higher than forward TPs. Consider the extremely                    having seen the pairs of syllables making up words,
common suffix ez in French, as in, "Parlez-vous français?"            something that does not require TPs.
The probability that, given a z, it will be preceded by an e is
approximately 0.84, whereas the probability that an “e” will          French, Addyman & Mareschal, (2011), Equal forward
be followed by a “z” is a mere 0.027                                  TPs In this experiment, run on infants, all forward TPs
(http://www.lexique.org/listes/liste_bigrammes). Perruchet            between syllables and between the words in the language
& Desaulty created a set of 2-syllable words that made up a           were identical. Backward TPs within words were 1 and
familiarization sequence in which the first syllable of each          backward TPs between words were 0.25. Each word is
word was perfectly predicted by second syllable (i.e.,                associated with two partwords. French et al. determined by
backward TP = 1), whereas the second syllable was only                means of a head-turn preference procedure identical to the
very weakly predicted by the first syllable. They showed              one used by Aslin et al. (1998), that infants exposed to this
that under these conditions, participants still recognized            “language” learn words significantly better than partwords.
words better than partwords.
                                                                                 Words learned significantly better than Partwords?
                                 Segmentation cues        Score type                            (proportion better)
                                                                             Humans          TRACX            TRACX 2.0           SRN
Saffran et al. (1996)                                    looking time          Yes             Yes                 Yes             Yes
                                  Freq. + Fwd TPs
                                                                               0.06            0.08                0.11            0.68
Aslin et al. (1998)                                      looking time          Yes             Yes                 Yes             Yes
                                      Fwd TPs
                                                                               0.04            0.13                0.09            0.58
Perruchet & Desaulty (2008).                               % correct           Yes             Yes                 Yes             Yes
                                      Fwd TPs
Expt. 2                                                    responses           0.34            0.38                0.17            0.80
Perruchet & Desaulty (2008).                               % correct           Yes             Yes                 Yes             No
                                     Bkwd TPs
Expt. 2                                                    responses           0.22            0.32                0.05           -0.10
                                                         looking time          Yes             Yes                 Yes             Yes
Equal TP                         Freq. + Bkwd TPs
                                                                               0.13            0.50                0.06            0.05
  Table 1. Proportion of words learned better than partwords for the three models and humans on five experimental data sets.
                                                                      (See French et al., 2011, footnote 5, p. 422, for a detailed
We expected that the SRN would also learn words better                justification of this measure.) It is calculated by taking
than partwords, because of the greater frequency of words.            difference of the measures for partwords and words and
However, in the absence of forward TP information, we also            dividing this difference by the sum of these two measures.
expected it to perform far less well than it did with the                  Finally, we compared the human data from the five
Saffran et al. (1996), the Aslin et al. (1998) and Perruchet &        experiments and the average overall performance of
Desaulty (2008), forward-TP data sets. This is, indeed, what          TRACX, TRACX 2.0,and the SRN. The performance of
we observed (Table 1). Both TRACX and TRACX 2.0 also                  TRACX and TRACX2.0 are essentially equivalent over the
learn words better than partwords. However, the                       set of problems (Figure 4). Even though the performance of
performance of original TRACX model is quite far from                 TRACX is closer to human data on three of the problems in
human performance, unlike TRACX 2.0, which is much                    Table 1, TRACX 2.0 is better on two of two others, and
closer to human performance on this data set (Table 1).               over all five experiments, the performance of the two
     In Table 1, we use a “proportion better” measure to              models is similar. By contrast, the SRN's performance is
compare model results and empirical data. This is a relative-         considerably farther from human data on these 5 tasks.
difference measure that can be applied equally well to error
measures, to looking times, or to proportion-correct scores.
                                                                 2220

                                                                        Cleeremans, A. and McClelland, J. (1991). Graded state machines:
                                                                          The representation of temporal contingencies in simple recurrent
                                                                          networks. Machine Learning, 7:161-193.
                                                                        Cottrell, G.W., Metcalfe, J. (1991) EMPATH: face, gender and
                                                                             emotion recognition using holons. D. Touretzky (Ed.)
                                                                             Advances in Neural Information Processing Systems 3 (pp.
                                                                             564-571), San Mateo, CA: Morgan Kaufmann.
                                                                        Cottrell, G.W. and Tsung, F-S. (1993) Learning simple arithmetic
                                                                             procedures. Connection Science, 5(1):37-58.
                                                                        Elman, J.L. (1990) Finding structure in time. Cognitive Science,
Figure 4. The three models’ match to human performance,                      14:179-211.
averaged over 5 experiments.                                            Farrell, S. & Lewandowsky, S. (2002) An endogenous distributed
                                                                             model of ordering in serial recall. Psychonomic Bulletin and
                          Conclusion                                         Review, 9:59-79.
                                                                        Frank, M., Goldwater, S., Griffiths, T., Tenenbaum, J. (2010).
This article is not claiming that TRACX 2.0’s performance                    Modeling human performance in statistical word
is superior to that of the original TRACX. Rather, it is                     segmentation. Cognition 117(2):107-125.
sufficient for our purposes that TRACX 2.0 performs in a                French, R. M., Addyman, C., and Mareschal, D. (2011). TRACX:
qualitatively similar manner compared to the original                        A Recognition-Based Connectionist Framework for Sequence
TRACX model. What is important is that TRACX 2.0 no                          Segmentation and Chunk Extraction. Psychological Review,
longer requires the inclusion of an error-threshold                          118(4), 614-636.
parameter, nor an external world/internal representation                French, R. M., Mareschal, D., Mermillod, M. and Quinn, P. (2004)
parameter governing how often learning takes place, and                      The role of bottom-up processing in perceptual categorization
                                                                             by 3 to 4 month old infants: Simulations and data. Journal of
still performs in a manner qualitatively similar to TRACX.
                                                                             Experimental Psychology: General, 133:382-397.
The new model, like TRACX, is a recursive autoassociator                Gluck, M.A. and Granger, R. (1993) Computational models of the
but, unlike its predecessor, it makes use only of spreading                  neural bases of learning and memory. Annual Review of
activation and error backpropagation, which has been shown                   Neuroscience, 16:667-706.
to be isomorphic to the neurobiologically plausible                     Goldwater, S., Griffiths, T.L. and Johnson. M. (2009) A Bayesian
mechanism of contrastive Hebbian learning (O'Reilly &                        framework for word segmentation : Exploring the effects of
Munakata, 2000). This is a considerable improvement over                     context. Cognition, 112(1):21-54
the original model for a number of reasons, namely: i) it               Kurumada, C., Meylan, S. C., & Frank, M. C. (2013). Zipfian
considerably increases the neurobiological plausibility of                frequency distributions facilitate word segmentation in context.
                                                                          Cognition 127:439–453
the model; ii) it treats chunks in an appropriate, graded
                                                                        Mareschal, D., French, R. M., & Quinn, P. (2000). A Connectionist
manner, rather than dichotomously; iii) it reduces the                       account of asymmetric category learning in early infancy.
number of free parameters in the model by two; and, finally,                 Developmental Psychology, 36:635-645.
iv) these changes do not significantly degrade the original             Metcalfe, J., Cottrell, G.W., & Mencl, W.E. (1992). Cognitive
model's performance.                                                         Binding: A computational-modeling analysis of the
                                                                             distinction between implicit and explicit memory systems.
                     Acknowledgments                                         Journal of Cognitive Neuroscience 4(3):289-298.
                                                                        O’Reilly, R., and Munakata, Y. (2000). Computational
This work was financed in part by a grant (ANR-10-0056)                      explorations in cognitive neuroscience. MIT Press.
from the French National Research Agency to the first                    Perruchet, P. and Desaulty, S. (2008) A role for backward
author, and NSF grant SMA 1041755 to the Temporal                            transitional probabilities in word segmentation? Memory and
Dynamics of Learning Center, an NSF Science of Learning                      Cogntion, 36(7):1299-1305.
Center, to the second author.                                           Perruchet, P. and Vinter, A. (2002) The Self-Organizing
                                                                             Consciousness. Behavioral and Brain Sciences, 25:297- 330.
                                                                        Pollack, J. (1990) Recursive Distributed Representations Artificial
                          References                                         Intelligence, 46:77-105.
Ackley, D.H., Hinton, G.E., and Sejnowski, T.J. (1985) A learning       Rolls, E.T. and Treves, A. (1997) Neural Networks and Brain
   algorithm for Boltzmann machines. Cog. Science 9:147-169.                 Function, Oxford: Oxford University Press.
Anderson, J.A., Silverstein, J.W., Ritz, S.A. and Jones, R.S. (1977)    Saffran, J. R., Aslin, R. N., and Newport, E. L. (1996) Statistical
   Distinctive features, categorical perception, and probability             learning by 8-month-old infants. Science, 274:1926-1928.
   learning: Some applications of a neural model. Psychological         Servan-Schreiber, D. & Anderson, J.R. (1990) Learning artificial
   Review, 84(5):413-451.                                                    grammars with competitive chunking. JEP:LMC, 16, 592–608.
Blank, D.S., Meeden, L.A., and Marshall, J.B. (1992). Exploring         Servan-Schreiber, D., Cleeremans, A. and McClelland, J.L. (1991)
   the Symbolic/Subsymbolic continuum: A Case Study of RAAM.                 Graded state machines: The representation of temporal
   In J. Dinsmore (ed) Closing the Gap: Symbolism vs.                        contingencies in simple recurrent networks Machine Learning
   Connectionism. Mahwah: NJ: LEA, pp. 113-148.                              7(2):161-193.
Börschinger, B., & Johnson, M. (2011). A particle filter algorithm      Sokolov, E. N. (1963). Perception and the conditioned reflex.
   for Bayesian word segmentation. Proceedings of the                      Hillsdale, NJ: LEA.
   Australasian Language Technology Association, pp. 10–18.
                                                                    2221

