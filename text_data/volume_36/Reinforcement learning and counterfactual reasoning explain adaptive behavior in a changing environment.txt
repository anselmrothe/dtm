UC Merced
Proceedings of the Annual Meeting of the Cognitive Science
Society
Title
Reinforcement learning and counterfactual reasoning explain adaptive behavior in a
changing environment
Permalink
https://escholarship.org/uc/item/0994n832
Journal
Proceedings of the Annual Meeting of the Cognitive Science Society, 36(36)
Authors
Zhang, Yunfeng
Paik, Jaehyon
Pirolli, Peter
Publication Date
2014-01-01
Peer reviewed
 eScholarship.org                                 Powered by the California Digital Library
                                                                    University of California

                  Reinforcement Learning and Counterfactual Reasoning Explain
                                Adaptive Behavior in a Changing Environment
                                            Yunfeng Zhang (zywind@cs.uoregon.edu)
                               Department of Computer and Information Science, University of Oregon
                                             1202 University of Oregon, Eugene, OR 97403
                          Jaehyon Paik (jpaik@parc.com) and Peter Pirolli (pirolli@parc.com)
                                                         Palo Alto Research Center
                                               3333 Coyote Hill Rd., Palo Alto, CA 94304
                              Abstract                                     Past research suggests that animals may use
   Animals routinely adapt to changes in the environment in
                                                                        reinforcement learning to detect environmental changes
   order to survive. Though reinforcement learning may play a           (Behrens et al., 2007; Pearson et al., 2011). Reinforcement
   role in such adaption, it is not clear that it is the only           learning was shown to be a biologically plausible learning
   mechanism involved, as it is not well suited to producing            mechanism (Holroyd and Coles, 2002) and it is very similar
   rapid, relatively immediate changes in strategy in response to       to linear operators derived in optimal foraging theory to
   environmental changes. We explored the possible adaptive             track the changes of a hidden environmental variable with
   mechanisms underlying in a cognitive model of human                  probabilistic observations (McNamara and Houston, 1987).
   behavior in a change detection experiment. Besides
   reinforcement learning, the model incorporates counterfactual        Several behavioral and neuroimaging studies (Behrens et al.,
   reasoning to help learn the utility of different task strategies     2007; Nassar et al., 2010) showed that people seem to use
   under different environmental conditions. The results show           reinforcement learning to detect changes, and their
   that the model can accurately explain human data and that            performance in these tasks approaches the performance of
   counterfactual reasoning is key to reproducing the various           an ideal observer.
   effects observed in this change detection paradigm.                     Despite its dominance in the discussion of change
   Keywords: change detection, reinforcement              learning,     detection, reinforcement learning alone cannot fully explain
   counterfactual reasoning, cognitive modeling.                        how some animals often quickly switch to drastically
                                                                        different task strategies, because its error-learning rule
                          Introduction                                  suggests a gradual transition of behaviors in response to
Detecting changes in the natural environment is often vital             changes (Pearson et al., 2011). For example, reinforcement
for an organism’s survival. Animals routinely experience                learning cannot easily explain how monkeys do not just try
environmental changes across days and seasons, and                      to jump higher to reach a bunch of hanging bananas, but
sometimes more sudden and drastic changes such as flood                 know to use chairs and sticks. Such strategies cannot result
and drought. Evolution has equipped organisms with many                 from gradual updates of a single strategy, rather, they are
abilities to detect such changes, and learning is perhaps the           likely a result of evaluating a wide array of different
most powerful one of them. Studying change detection, a                 options.
problem that learning is possibly originally evolved for,                  This research proposes that counterfactual reasoning is a
may shed light on the capabilities and limitations of                   missing piece in this theoretical framework for explaining
learning.                                                               change detection behaviors. Counterfactual reasoning
   Rational analyses of change detection have been                      captures the process in which humans think about potential
developed based on optimal foraging theories (e.g.,                     or imaginary events and consequences that are alternatives
McNamara and Houston, 1987; Stephens, 1987). Stephens                   to what has occurred. This gives humans abilities to learn
(1987) derived the optimal foraging strategies for a                    the utility of a task strategy without actually applying it.
simplified, hypothetical environment that contains a variable           Neuroimaing studies (e.g., Coricelli et al, 2005) show that
food energy resource that periodically switches between a               such processes indeed exist and they seem to play a key role
poor and a rich state, and a stable food energy resource that           in decision making. Nevertheless, counterfactual reasoning
provides a medium amount of energy. It is found that to                 is somewhat overlooked as a plausible explanation for
maximize the intake of food energy, there is an optimal                 change detection behaviors.
frequency for how often the variable resource should be                    This paper presents the behavioral data collected from a
sampled (to detect its rich state). This analysis suggests that         stochastic change detection task, compares the human data
to survive in the natural world, animals need to actively               with the predictions of an ACT-R cognitive model that
explore the environment and perhaps need to do so in a                  incorporates reinforcement learning and counterfactual
particular rate to maximize the benefit and minimize the                reasoning, and compares models with and without
cost of explorations. But how does animals learn when to                counterfactual reasoning to demonstrate the importance of
explore, and what mechanisms drive them to explore rather               counterfactual reasoning in explaining human change
than to stay in a stable habitat?                                       detection performance.
                                                                    1850

                 Experimental Paradigm                              of a market-state change in each trial. Again, two levels
                                                                    were tested, one with 5% change probability and the other
The change detection experiment presented here is designed
                                                                    with 15% change probability. This factor examined how
as an investment game, in which there is a virtual market
                                                                    well people adapt to the volatility of the environment.
that the participant can invest virtual chips in. The market
                                                                       The third factor of the experiment was whether to provide
alternates between the bear state, in which the participant is
                                                                    information about the outcome of the market when “pass”
likely (> 50%) to lose the investment, and the bull state, in
                                                                    was selected. In the no-feedback-for-pass condition, the
which the participant is likely (> 50%) to profit. The change
                                                                    participant needed to guess what was happening in the
of the market state occurs at a small, constant probability
                                                                    market if pass was selected, based on the past experience
per turn. The market state is not directly observable by the
                                                                    such as how long the bear market generally lasted. This
participants, but has to be inferred from the investment
                                                                    condition simulated an environment in which one can only
outcomes (profiting or losing) of the recent trials. In
                                                                    acquire information about the choice they made. We
essence, the virtual market is designed as a hidden Markov
                                                                    expected that participants would perform worse in the no-
process to mimic the natural environment in which the
                                                                    feedback-for-pass condition than in the has-feedback-for-
underlying states, such as the amount of food in a habitat,
                                                                    pass condition.
are not directly observable, but are often similar to the states
                                                                       The discriminability factor was a within-subject variable,
of the recent past.
                                                                    and the change-frequency and the feedback factors were
Participants Forty-eight participants (26 females; mean age         between-subject variables balanced across the 48
= 36.71 years, range 21–62 years) were recruited on the             participants. In other words, each participant did both the
Amazon Mechanical Turk website. Each participant                    low discriminability and the high discriminability
received a base compensation of $3 and up to a $4 bonus for         conditions, but experienced only one change frequency and
completing the 30 min long experiment. The bonus that               one feedback condition.
participants received depended on their task performance.
                                                                    Procedure The participant clicked a link provided on an
Apparatus and Materials In each trial, two buttons labeled          Amazon Mechanical Turk webpage to navigate to the
“pass” and “10” were presented on the screen. Clicking              experiment website. Before doing the experiment, the
“pass” would skip the investment opportunity, while                 participant needed to accept a consent form, fill out a
clicking “10” would invest 10 chips to the market. If the           demographic survey, and complete a risk propensity scale
participant chose to invest, he or she would either win 15          (see Meertens and Lion, 2008). The experiment instructions
chips or lose the 10-chip investment. This investment               included that the market switches between a bull and a bear
outcome, as well as the participant’s total number of chips,        state at a constant probability per trial, but no concrete
were immediately shown to the participant after each trial. If      parameters such as the profiting probabilities were shown to
the participant finished a trial within 5 seconds (indicated by     the participants.
a count-down timer on the display), a reward of 0.05 cents             Each participant completed two low-discriminability
would be added as a bonus.                                          blocks and two high-discriminability blocks, with the
                                                                    running order randomized and balanced across participants.
Design Three factors were manipulated. The first was the            The participant was informed about the market
discriminability between the bull market and the bear               discriminability before each block. In each block, the
market. Error! Reference source not found. shows the                participant started with 100 chips, and underwent 150 trials.
probability of profiting and losing in the two                      The performance feedback, including the number of chips
discriminability conditions tested in the experiment. As can        earned and how much bonus the chips translated to, was
be seen from the table, the profiting probability of the bull       provided after each block and at the end of the experiment.
and the bear markets were set to be more similar in the low
discriminability condition than those in the high                                      Experimental Results
discriminability condition, and hence it was harder to
distinguish the two market states in the low discriminability       Figure 1 shows the overall task performance across the
condition. Manipulating this factor helped us examine how           different experimental conditions, measured as the average
the reliability of observation might affect peoples’ ability to     number of chips earned in each block. Participants earned
infer the underlying environmental states.                          more chips in the high-discriminability condition than in the
   The second factor of the experiment was the probability          low-discriminability condition (z = −11.2, p < .001)1, as is
                                                                    shown in the graph that the bars in the left column are taller
     Table 1: The probability of profiting and losing of the        than the bars in the right column. Participants also earned
       bear and the bull markets in the low and the high            more chips in the has-feedback condition than in the no-
                   discriminability conditions.                     feedback condition (z = −2.45, p < .001), as is shown in the
             Low Discriminability     High Discriminability            1
             Profiting Losing           Profiting Losing                  Multiple comparisons were done using general linear
                                                                    hypotheses tests on a linear mixed-effects model. Main effects and
      Bull     70%        30%              10%       90%
                                                                    effect sizes were obtained using a repeated measure ANCOVA,
      Bear     30%        70%              90%       10%            with the covariate being the number of profitable trials in a block.
                                                                1851

                                                 high discriminability low discriminability                                                                  Has Feedback         No Feedback
                                          1000
      Average Number of Chips Per Block
                                                                                                                                                      high discriminability   low discriminability
                                           750                                                                                                  100
                                                                                              5% change
                                           500                                                                                                   75
                                                                                                                                                                                                     5% change
                                                                                                                   Percent of Trials Invested
                                           250                                                                                                   50
                                             0                                                                                                   25
                                          1000                                                                                                    0
                                                                                                                                                100
                                                                                              15% change
                                           750
                                                                                                                                                                                                     15% change
                                                                                                                                                 75
                                           500
                                                                                                                                                 50
                                           250
                                                                                                                                                 25
                                            0
                                                                                                                                                  0
                                                        Has Feedback        No Feedback                                                                 Bear       Bull         Bear       Bull
   Figure 1: Average number of chips earned per block in                                                           Figure 2: Average percentage of trials the participants
   the different experimental conditions. Error bars show                                                         invested in when the market was in the bear and the bull
            95% confidence intervals of the mean.                                                                 states in the different experimental conditions. Only the
                                                                                                                    last 100 trials of each block were used for this graph.
graph that within each panel, the dark gray bar is usually
taller than the light gray bar. Market discriminability and the                                               Furthermore, it seems that when there was no feedback for
feedback condition had the largest effect on task                                                             the pass option, the participants were more conservative in
performance (for discriminability, F(1, 44) = 55.5, p < .001,                                                 investing in the market, as can be seen that in the bottom
!!! = .888; for feedback, F(1, 44) = 7.42, p = .009, !!! = .12),                                              right panel the no-feedback conditions has smaller
whereas change frequency did not have a significant main                                                      investment percentages than the has-feedback conditions.
effect, F(1, 44) = 3.00, p = .09, !!! = .056.                                                                    The above results suggest that the participants’ strategies
   Figure 2 reveals participants’ investment strategies and                                                   may be rational to some extent because they tried to
shows how these strategies are heavily influenced by the                                                      maximize their pay in some conditions, but their ability to
market discriminability. The investment percentages in the                                                    infer the market state from the probabilistic observations is
graph were calculated using the last 100 trials of each block                                                 perhaps limited by memory and learning mechanisms. The
because at the beginning of each block participants were                                                      next section presents a cognitive model that tries to
likely still exploring the task parameters, and the behaviors                                                 reproduce this bounded rationality using reinforcement
during the first 50 trials probably cannot represent the                                                      learning and a counterfactual reasoning strategy.
stabilized behavior. As can be seen from the graph, in the
high-discriminability conditions (left column), the                                                                                                   The Change Detection Model
investment percentages of the bear market are very different                                                  The model presented here is implemented using the ACT-R
from those of the bull market, particularly in the top left                                                   cognitive architecture (Anderson et al., 2004). ACT-R has
panel. This result suggests that the participants could                                                       many built-in constructs that directly support the modeling
somewhat accurately infer the market state and use that                                                       of this task. Particularly, it has a powerful production
information to avoid investing in the bear market and at the                                                  system that learns by reinforcement learning. In a
same time, exploit the bull market. In the low-                                                               production system, task strategies are written as production
discriminability condition, however, the investment                                                           rules, which are IF-THEN statements that execute certain
percentages are about the same across the bear and bull                                                       actions (the THEN part) when the conditions are met (the IF
markets. This suggests that the participants could not                                                        part). In ACT-R’s production system, each production rule
identify the market state and thus applied the same strategy                                                  can also be assigned a utility value, which roughly
all along, which no doubt contributed to the poor                                                             corresponds to how likely this production rule leads to the
performance in the low-discriminability conditions.                                                           successful completion of the task. In every 50-ms cognitive
   Figure 2 also shows that in the no-feedback condition, in                                                  cycle, ACT-R executes one of the production rules whose
which the market outcome was only provided if “invest” is                                                     conditions are matched, and the probability that a matched
selected, participants were less able to detect the underlying                                                rule will be selected is an increasing function of its
changes of the market. This can be seen in the left two                                                       production utility:
graphs in Figure 2 (high discriminability) where the
                                                                                                                                                                                   !!!! / !!
difference between the no-feedback conditions is less than                                                                                            !"#$%$&'&() ! =                  !! / !!
                                                                                                                                                                                                                  (1)
the difference between the has-feedback conditions.                                                                                                                                ! !!
                                                                                                           1852

where !! is the utility of the production rule i, s is a free
parameter, and the denominator is a summation over all                                          Select a
production rules whose conditions are matched. s is also                                      rule to fire
referred to as the utility noise parameter, because as s
increases the probability that a production rule will fire                       Assume                        Assume
depends less on its utility and more on the random chance.                         bull                          bear
   When a task goal is reached (or fails) and a reward (or                                                             Reward 0
penalty) is triggered, the reward (penalty) is propagated
back through the firing chain of the production rules so that          Win:       Invest    Lose:                Pass
the utility of the previously fired rules can all be updated       Reward 15             Penalize 10
accordingly by the following equation:
                                                                                                          Yes
                                                                                 Win or                           Feedback?
               !! = !!!! + !(!! − !!!! )                   (2)
                                                                                  Lose?
                                                                                                                        No
where !!!! is the utility of the production rule before the
update, !! is the utility after the update, !! is the reward,                                                  Assume
and ! is the learning rate. The production selection equation                                                     bull
and the utility updating equation are essentially the same as         Key:
the ones used in some ideal observer models (Behrens et al.,                   Production                              Reward 3
                                                                                             Decision
2007; Nassar et al., 2010) with the exception that the                             Rule                         Invest
learning parameter ! in ACT-R is set by the analyst, as
opposed to be learned on a trial-by-trial basis.                       Call      Reward Counterfactual
   Figure 3 illustrates the task strategy of the model. At the                 Propagation Reasoning
beginning of the trial, the model executes one of the two
production rules, assume-bull and assume-bear, based on            Figure 3: A flow chart showing how the model performs a
Equation 1. If assume-bull fires, the rule invest will ensue        trial of the experiment. Each trial only goes through one
because it is rational to capitalize on the bull market. Then        of the dashed lines once to complete the counterfactual
just like the experimental design, if the market returns a         reasoning (CR) process. In CR, the production utilities are
profit, a reward of 15 will be delivered and the utility of          updated the same way as in a regular learning process.
assume-bull will be updated using Equation 2; if the market
returns a loss, a penalty of 10 (!! = −10) is delivered. If       which point assume-bear is fired because its utility (which
assume-bear fires, the rule pass will be fired next, and a        stays at zero) is now larger than the utility of assume-bull.
reward of 0 will be delivered just like how the participant          To detect the change from the bear market to the bull
would neither win nor lose when selecting pass.                   market, however, requires counterfactual reasoning, which
   Note that the model does not explicitly track the              evaluates what would happen if the non-selected choice was
environmental parameters such as the profiting probabilities      selected given the newly acquired information about the
of the bull and bear markets, which might hinder its ability      environment. For the proposed model, if there is no
in making correct investment decisions. This deficiency is        counterfactual reasoning, then once assume-bear is selected,
somewhat compensated by the utility updating equation that        the model is likely to be trapped in an assume-bear state,
automatically incorporates the frequency in which the             especially when the production noise parameter s is set low.
reward and penalty occur. When the market state is stable,        This is because when assume-bear fires, assume-bull’s
the utility of assume-bull and assume-bear should, over           production utility is likely below zero. To reset its utility, it
time, tend to the expected return of the bull and bear            needs to be fired, but because assume-bear’s utility is
markets, and the production selection based on these utilities    higher, it does not have a chance to fire. With counterfactual
should lead to good investment decisions.                         reasoning, the model temporarily disables assume-bear so
   The model tracks the change of the market state by             that assume-bull has no competition and can be fired. This
heavily weighting the experience of the recent trials when        way, the model can appropriately update assume-bull’s
updating the utility of assume-bull and assume-bear. The          utility when the market changes to a bull state, which then
learning parameter ! is set to 0.5 to give equal weights to       allows the detection of the change.
the recent experience and to the last utility estimation,            The counterfactual reasoning processes used by the model
which enables the production utility to quickly respond to        are indicated in Figure 3 by the dashed-line connections. As
the change of the market. For example, considers how the          can be seen, after evaluating the made choice, the model
model would detect the change from the bull to the bear           continues to another path to evaluate and update the utility
state. Initially, the model will continue firing assume-bull      of the alternative choice. Note that the lower-right corner of
because this rule accumulated high utility from winning in        the graph specifies what to do when the current condition
the bull market. But as losing becomes more frequent after        does not provide feedback about the market outcome for the
the market changes to the bear state, the utility of assume-      pass option (No Feedback condition). In this situation,
bull is penalized and quickly drops down to below zero, at        because the model does not know what would occur if it
                                                              1853

                                                                                                                     Has Feedback
invested in the market, it needs to estimate a reward for
                                                                                                                  human         model      optimal
assume-bull. We explored a few settings for this reward
parameter and set it at 3 in the final model so that the model                                                    Has Feedback
                                                                                                       high discriminability low discriminability
                                                                                                  100 high discriminability low discriminability
generates streaks of pass choices (which are eventually
                                                                                                 100
interrupted as assume-bull’s utility surpasses assume-bear
                                                                                                                                                        5% change
                                                                                                   75
through counterfactual reasoning) that are about as long as                                       75
                                                                                                                                                            5% change
                                                                                                   50
                                                                    Percent of%Trials
                                                                                TrialsInvested
those observed in the empirical data.
   Overall, the model is a straightforward combination of                                          25
                                                                                                  50
reinforcement learning and counterfactual evaluation. As
                                                                                       Invested
                                                                                                  250
will be shown in the following section, though the model                                          100
does not perform as well as an optimal Bayesian model in
                                                                                                                                                        15% change
                                                                                                   0
                                                                                                   75
terms of the number of chips earned, it does seem to fit the                                     100
                                                                                                   50
                                                                                                                                                              15% change
human data.                                                                                       75
                                                                                                   25
                     Model Results                                                                50
                                                                                                    0
The model was run on all 28,800 trials that the participants                                      25       Bear       Bull          Bear      Bull
performed. To examine whether the model and the
participants achieved optimal performance, a Bayesian                                              0
optimal solution was developed. For every trial, this                                                     Bear       Bull Feedback
                                                                                                                     No        Bear           Bull
solution computes the posterior probability of the bull and                                             high discriminability    low discriminability
bear markets given the market outcome and the prior                                              100
probability of the two markets (which are computed from
                                                                                                  75
the previous trial using the same procedure). It then
                                                                                                                                                        5% change
                                                                    Percent of Trials Invested
calculates the expected return of investing, and if the return                                    50
is smaller than zero, pass will be selected, otherwise,
investing will be selected. Unlike our human participants,                                        25
this Bayesian model has knowledge of the underlying                                                0
market profitabilities (70%/30% or 90%/10%) and                                                  100
underlying change probabilities (5% or 15%), and can thus
                                                                                                                                                        15% change
make optimal decisions. The human data, the model                                                 75
predictions, and the optimal solutions are compared below.                                        50
   Figure 4 shows the investment percentages across the
three data sets. As can be seen, the model (light gray) match                                     25
the human data (dark gray) very well in almost all
                                                                                                   0
conditions except in the No-Feedback group’s top-left and
                                                                                                          Bear       Bull          Bear       Bull
bottom-right panel. Similar to the participants, in the high
discriminability condition, the model was able to capitalize                Figure 4: Average percentage of trials invested by
on the bull market and avoid investing in the bear market,                     the participants, the model, and the optimal
whereas in the low discriminability condition, the model                    Bayesian solution. Only the last 100 trials of each
invested at similar percentages across the two markets. In                           block were used in this analysis.
the conditions in which the model does not match the data
                                                                 even the optimal solution could not distinguish the bear and
well (No-Feedback condition’s top-left and bottom-right
                                                                 bull markets and had to adopt a uniform investment
panel), the model invested more aggressively than the
                                                                 percentage across the two markets. Unlike the participants
participants. Further examination of the payoff data shows
                                                                 and the model, however, the optimal solution invested very
that the model in fact earned more chips than the
                                                                 aggressively, almost at 100%, in both markets, whereas the
participants (by 0.5 chips per turn) in these conditions,
                                                                 model and the participants only invested in about 50% of
which suggests that the model’s strategy—always
                                                                 the trials. The reason that the model could reproduce the
performing counterfactual reasoning—is a “good enough”
                                                                 participants’ conservative strategy is perhaps that when the
strategy, and perhaps the reason that participants did worse
                                                                 environment is volatile, the model never had the chance to
is because they did not always use counterfactual reasoning.
                                                                 learn the expected return of the bull market because
   The model matched the human data well even in
                                                                 whenever the model starts investing, the frequent losses
conditions in which the participants’ strategy deviated from
                                                                 soon leads to a switch to the pass behavior. The utility of
the optimal solution. It can be seen from Figure 4 that the
                                                                 assume-bull thus remained low most of the time, which
optimal solution matches the participants’ and model’s
                                                                 resulted in a conservative behavior.
strategies in almost all conditions except in the low-
                                                                   Figure 5 illustrates how counterfactual reasoning (CR) is
discriminability and 15%-change condition. This condition
                                                                 an indispensable component of the model for explaining the
is the most difficult condition of the experiment, and indeed
                                                                 human data. The y-axis shows the average absolute
                                                             1854

                                                                                           A model sensitivity analysis that varies the percentage of
                                  35    ●
                                                                                       trials in which counterfactual reasoning is applied shows
                                                                Has Feedback           that counterfactual reasoning is key to explaining the human
          AAPE of The Predicted
                                  30                            No Feedback            data. As discussed in the introduction, counterfactual
                                        ●                                              reasoning is essentially learning by imagining the
                                                                                       interactions between the organism and the outside world.
                                  25
                                            ●
                                                        ●
                                                                    ●                  Compared to learning by actually experiencing the world, it
                                                                                       incurs almost no risks. Understandably, it might be a
         Investment Percentages
                                                ●
                                            ●       ●   ●
                                  20                            ●
                                                                                ●      powerful tool that drives animals to safely explore novel
                                                                                ●
                                                    ●       ●
                                                            ●       ●                  options in response to unusual changes of the environment.
                                                                        ●
                                  15            ●
                                                                ●
                                                                            ●
                                                                                       Our research suggests that this is likely the case, and
                                                                            ●          perhaps future theories and models of learning and decision
                                  10
                                                                        ●              making should always incorporate counterfactual reasoning.
                                        0 10 20 30 40 50 60 70 80 90 100
                                           Percent of Trials with                                         Acknowledgments
                                       Counterfactual Reasoning (CR)                   This work is supported in part by the Intelligence Advanced
     Figure 5: Average absolute percentage error (AAPE)                                Research Projects Activity (IARPA) via Department of the
     between the predicted investment percentages and the                              Interior (DOI) contract number D10PC20021. The U.S.
      observed percentages, for the 11 models that utilize                             Government is authorized to reproduce and distribute
        counterfactual reasoning (CR) at different rates.                              reprints for Governmental purposes notwithstanding any
                                                                                       copyright annotation thereon. The opinions expressed
percentage error (AAPE) between a model’s predicted                                    hereon are strictly those of the authors.
investment percentages and the observed percentages. In
this analysis, we created 11 versions of the model that
perform CR at different frequencies, ranging from 0% of the
                                                                                                               References
trials to 100% of the trials. It can be seen that if the model                         Anderson, J. R., Bothell, D., Byrne, M. D., Douglass, S.,
never uses CR (0%), its predictions are about 30% to 35%                                 Lebiere, C., & Qin, Y. (2004). An integrated theory of the
away from the observed investment percentages. As the                                    mind. Psychological review, 111(4), 1036–1060.
model utilizes CR more frequently, the predictions become                              Behrens, T. E. J., Woolrich, M. W., Walton, M. E., &
closer to the human data. The best fit is reached at                                     Rushworth, M. F. S. (2007). Learning the value of
somewhere between 80% CR and 90% CR, which suggests                                      information in an uncertain world. Nature Neuroscience,
that perhaps participants did CR most but not all of the time.                           10(9), 1214–1221.
                                                                                       Coricelli, G., Critchley, H. D., Joffily, M., O’Doherty, J. P.,
                           Discussion and Conclusions                                    Sirigu, A., & Dolan, R. J. (2005). Regret and its
                                                                                         avoidance: a neuroimaging study of choice behavior.
Our experimental results show that people can detect
                                                                                         Nature Neuroscience, 8(9), 1255–1262.
changes in a stochastic environment in which the
                                                                                       Holroyd, C. B. & Coles, M. G. H. (2002). The neural basis
observations are only imperfect indicators of the
                                                                                         of human error processing: Reinforcement learning,
environment’s underlying state. When the observations can
                                                                                         dopamine, and the error-related negativity. Psychological
be used to somewhat reliably identify the hidden states, the
                                                                                         Review, 109(4), 679–709.
participants’ performance approach optimal. When the
                                                                                       McNamara, J. M. & Houston, A. I. (1987). Memory and the
observations do not reliably identify the hidden states,
                                                                                         efficient use of information. Journal of Theoretical
participants seem to show loss aversion and to adopt a
                                                                                         Biology, 125(4), 385–395.
conservative strategy to avoid risks.
                                                                                       Meertens, R. & Lion, R. (2008). Measuring an individuals
  A cognitive model that uses reinforcement learning and
                                                                                         tendency to take risks: the risk propensity scale. Journal
counterfactual reasoning seems to accurately account for
                                                                                         of Applied Social Psychology, 38(6), 1506–1520.
participants’ performance, be it optimal or suboptimal. The
                                                                                       Nassar, M., Wilson, R., Heasly, B., & Gold, J. (2010). An
fact that the model has very few free parameters and yet it
                                                                                         approximately Bayesian delta-rule model explains the
can still predict the trends in the human data across a variety
                                                                                         dynamics of belief updating in a changing environment.
of conditions strongly suggests that reinforcement learning
                                                                                         Journal of Neuroscience, 30(37), 12366–12378.
and counterfactual reasoning might be the main mechanisms
                                                                                       Pearson, J. M., Heilbronner, S. R., Barack, D. L., Hayden,
behind decision making in such changing environment.
                                                                                         B. Y., & Platt, M. L. (2011). Posterior cingulate cortex:
Particularly, that the model reproduces participants’
                                                                                         adapting behavior to a changing world. Trends in
tendency of loss aversion in the most volatile condition
                                                                                         Cognitive Sciences, 15(4), 143–151.
suggests that perhaps loss aversion is simply a byproduct of
                                                                                       Stephens, D. W. (1987). On economically tracking a
applying reinforcement learning in a very unpredictable
                                                                                         variable environment. Theoretical Population Biology,
environment.
                                                                                         32(1), 15–25.
                                                                                    1855

